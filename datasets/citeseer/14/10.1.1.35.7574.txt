regression shrinkage selection lasso robert tibshirani department statistics division biostatistics stanford university propose new method estimation linear models 
lasso minimizes residual sum squares subject sum absolute value coefficients constant 
nature constraint tends produce coefficients exactly zero gives interpretable models 
simulation studies suggest lasso enjoys favourable properties subset selection ridge regression 
produces interpretable models subset selection exhibits stability ridge regression 
interesting relationship adaptive function estimation donoho johnstone 
lasso idea quite general applied variety statistical models extensions generalized regression models tree models briefly described 
keywords regression subset selection shrinkage quadratic programming 
consider usual regression situation data 

ip regressors response ith observation 
ordinary squares ols estimates obtained minimizing residual squared error 
reasons data analyst satisfied ols estimates 
prediction accuracy ols estimates low bias large variance prediction accuracy improved shrinking setting zero coefficients 
doing sacrifice little bit bias reduce variance predicted revision technical report title dept statistics university toronto january sabbatical leave department preventive medicine biostatistics department statistics university toronto lasso values may improve prediction accuracy 
second reason interpretation 
large number predictors determine smaller subset exhibit strongest effects 
standard techniques improving ols estimates subset selection ridge regression drawbacks 
subset selection provides interpretable models extremely variable discrete process regressors retained dropped model 
small changes data result different models selected reduce prediction accuracy 
ridge regression continuous process shrinks coefficients stable doesn set coefficients zero doesn give easily interpretable model 
propose new technique called lasso absolute shrinkage selection operator 
shrinks coefficients sets zero tries retain features subset selection ridge regression 
section define lasso look special cases 
real data example section section discuss methods estimation prediction error lasso shrinkage parameter 
bayes model lasso briefly mentioned section 
describe lasso algorithm section 
simulation studies described section 
sections discuss extensions generalized regression models problems 
results soft thresholding relation lasso discussed section section contains summary discussion 
lasso definition suppose data 

ip predictor variables responses 
usual regression setup assume observations independent conditionally independent ij assume ij standardized ij ij 
letting fi fi 
fi lasso estimate ff fi defined ff fi argmin gamma ff gamma fi ij subject jfi tuning parameter 
solution ff ff assume loss generality omit ff 
lasso computation solution quadratic programming problem linear inequality constraints 
describe efficient stable algorithms problem section 
parameter controls amount shrinkage applied estimates 
fi values cause shrinkage solutions zero coefficients may exactly equal zero 
example effect roughly similar finding best subset size 
note design matrix need full rank 
section give number data methods estimation motivation lasso came interesting proposal breiman 
breiman non negative garotte minimizes gamma ff gamma fi ij subject garotte starts ols estimates shrinks non negative factors sum constrained 
extensive simulation studies breiman shows garotte consistently lower prediction error subset selection competitive ridge regression true model small non zero coefficients 
drawback garotte solution depends sign magnitude ols estimates 
overfit highly correlated settings ols estimates behave poorly garotte may suffer result 
contrast lasso avoids explicit ols estimates 
frank friedman proposed bound norm parameters number lasso corresponds 
discuss briefly section 
orthogonal design case insight nature shrinkage gleaned orthogonal design case 
theta design matrix entry ij suppose identity matrix 
solutions easily shown fi sign fi fi gamma fl fl determined condition fi interestingly exactly form soft shrinkage proposals donoho johnstone donoho johnstone picard applied wavelet coefficients context function estimation 
connection soft shrinkage minimum norm penalty pointed donoho johnstone hoch stern non negative parameters context signal image recovery 
elaborate connection section 
lasso orthonormal design case best subset selection size reduces choosing largest coefficients absolute value setting rest zero 
choice equivalent setting fi fi fi zero 
ridge regression minimizes gamma fi ij fi equivalently minimizes gamma fi ij subject fi ridge solutions fl fi fl depends garotte estimates gamma fl fi fi shows form functions 
ridge regression scales coefficients constant factor lasso translates constant factor truncating zero 
garotte function similar lasso shrinkage larger coefficients 
simulations show differences lasso garotte large design orthogonal 
geometry lasso clear lasso produce coefficients exactly zero 
happen general non orthogonal setting 
occur ridge regression uses constraint fi jfi 
provides insight case 
criterion gamma fi ij equals quadratic function fi gamma fi fi gamma fi plus constant 
elliptical contours function shown solid curves left panel centered ols estimates constraint region rotated square indicated broken lines 
lasso solution place contours touch square occur corner corresponding zero coefficient 
picture ridge regression shown right corners contours hit zero solutions rarely result 
interesting question emerges picture signs lasso estimates different squares estimates fi variables standardized principal axes contours sigma coordinate axes show contours contact square quadrant contains fi lasso beta subset beta ridge beta lasso beta garotte dark line shows form coefficient shrinkage line technique orthogonal design case lasso estimation picture lasso left ridge regression right moderate correlation data need true 
shows example dimensions 
view right plot confirms ellipse touches constraint region different octant center lies 
garotte retains sign fi lasso change signs 
cases lasso estimate sign vector garotte presence ordinary squares estimates garotte behave differently 
model fi ij constraint written fi ij constraint fi fi example fi fi effect stretch square left panel horizontally 
result larger values fi smaller values fi favoured garotte 
predictor case suppose assume loss generality squares estimates fi positive 
show lasso estimates fi fi gamma fl lasso left panel shows example lasso estimate falls different octant squares estimate 
right panel shows overhead view 
lasso fl chosen fi fi formula holds fi fi valid predictors correlated 
solving fl yields fi fi gamma fi fi gamma fi gamma fi contrast form ridge regression shrinkage depends correlation predictors 
shows example 
generated data points model noise 
standard normal variates correlation ae 
curves show ridge lasso estimates bounds fi fi jfi jfi respectively varied 
values ae lasso estimates follow solid curve 
ridge estimates broken curves depend ae 
ae ridge regression proportional shrinkage 
larger values ae ridge estimates shrunken differentially increase little bound decreased 
pointed jerome friedman due tendency ridge regression try coefficients equal order minimize squared norm 
standard errors lasso estimate non linear non function response values fixed value difficult obtain accurate estimate standard error 
approach bootstrap fixed may optimize bootstrap sample 
fixing analogous selecting best subset squares standard error subset 
approximate closed form estimate may derived writing penalty jfi fi jfi lasso estimate fi may approximate solution ridge regression form fi gamma gamma diagonal matrix diagonal elements fi gamma denotes generalized inverse chosen jfi covariance matrix estimates may approximated gamma gamma gamma gamma oe oe estimate error variance 
difficulty formula gives estimated variance zero predictors fi 
approximation suggests iterated ridge regression algorithm computing lasso estimate turns quite inefficient 
prove useful selection lasso parameter section 
lasso beta lasso solid curve ridge broken curves predictor example 
curves show fi fi pairs bound lasso ridge parameters varied 
starting bottom broken curve moving upward correlation ae 
example prostate cancer data example prostate cancer data data comes study 
examined correlation level prostate specific antigen number clinical measures men receive radical 
factors log cancer volume log prostate weight age log benign amount seminal invasion svi log penetration lcp gleason score gleason percent gleason scores pgg 
fit linear model log prostate specific antigen standardizing predictors 
shows lasso estimates function standardized bound fi notice absolute value coefficient tends zero goes zero 
example curves decrease monotone fashion zero doesn happen general 
lack monotonicity shared ridge regression subset regression example best subset size may contain best subset size 
vertical broken line represents model optimal value selected generalized cross validation 
roughly speaking corresponds keeping just half predictors 
table shows results full squares best subset lasso procedures 
section gives details best subset procedure 
lasso gave non zero coefficients svi subset selection chose predictors 
notice coefficients scores selected predictors subset selection tend larger full model values common occurence positively correlated predictors 
lasso shows opposite effect shrinks coefficients scores full model values 
standard errors second column right estimated bootstrap resampling residuals full squares fit 
standard errors computed fixing optimal value original dataset 
table compares ridge approximation formula fixed bootstrap bootstrap re estimated sample 
ridge formula gives fairly approximation fixed bootstrap zero coefficients 
allowing vary incorporates additional source variation gives larger standard error estimates 
shows boxplots bootstrap replications lasso estimates fixed estimated value 
predictors estimated coefficient zero exhibit skewed bootstrap distributions 
central percentile intervals th percentiles bootstrap distributions contained value zero exceptions svi 
example prostate cancer data lasso shrinkage coefficients prostate cancer example 
curve represents coefficient labelled right function scaled lasso parameter fi intercept plotted 
vertical broken line represents model selected generalized cross validation 
example prostate cancer data table results prostate cancer example predictor squares subset lasso coef se score coef se score coef se score 



age 

svi 
lcp 
gleason 
pgg table standard error estimates prostate cancer example bootstrap se predictor coefficient fixed varying se approximation 



age 

svi 
lcp 
gleason 
pgg prediction error estimation age svi lcp gleason pgg boxplots bootstrap values lasso coefficient estimates predictors prostate cancer example 
prediction error estimation section describe methods estimation lasso parameter cross validation generalized cross validation analytic estimate risk 
strictly speaking methods applicable random case assumed observations drawn unknown distribution third method applies fixed case 
real problems clear distinction scenarios simply choose convenient method 
suppose ffl ffl var ffl oe mean squared error estimate defined gamma expected value taken joint distribution fixed 
similar measure prediction error pe gamma oe estimate prediction error lasso procedure fold crossvalidation described example chapter efron tibshirani 
lasso indexed terms normalized parameter fi prediction error estimated grid values inclusive 
value yielding lowest estimated pe selected 
prediction error estimation simulation results reported terms pe 
linear models fi considered mean squared error simple form fi gamma fi fi gamma fi population covariance matrix second method estimating may derived linear approximation lasso estimate 
write constraint jfi fi jfi constraint equivalent adding lagrangian penalty fi jfi residual sum squares depending may write constrained solution fi ridge regression estimator fi gamma gamma diag fi gamma denotes generalized inverse 
number effective parameters constrained fit fi may approximated tr gamma gamma letting rss residual sum squares constrained fit constraint construct gcv style statistic gcv rss gamma outline third method stein unbiased estimate risk 
suppose multivariate normal random vector mean variance identity matrix 
estimator write differential function see definition stein 
stein showed jj gamma jj jjg jj dg dz may apply result lasso estimator 
denote estimated standard error fi oe oe gamma gamma 
fi conditionally approximately independent standard normal variates equation may derive formula fi fl gamma delta fi fl max fi fl approximately unbiased estimate risk mean square error fi fl gamma fi fi fl sign fi fi gamma fl donoho johnstone give lasso bayes estimate similar formula function estimation setting 
estimate fl obtained minimizer fi fl fl argmin fl fi fl obtain estimate lasso parameter fi gamma fl derivation assumes orthogonal design may try usual non orthogonal setting 
predictors standardized optimal value roughly function noise ratio data relatively insensitive covariance 
hand form lasso estimator sensitive covariance need account properly 
simulated examples section suggest method gives useful estimate offer heuristic argument favour 
suppose xv gamma gamma columns standardized region differs region jfi shape roughly sized marginal projections 
optimal value instance 
note stein method enjoys significant computational advantage cross validation estimation experiments optimized grid values lasso parameter fold crossvalidation 
result cross validation approach required applications model optimization procedure section stein method required 
requirements gcv approach intermediate requiring application optimization procedure grid point 
lasso bayes estimate lasso constraint jfi equivalent addition penalty term jfi residual sum squares see murray gill wright chapter 
jfi proportional minus log density double exponential distribution 
result derive lasso estimate bayes posterior mode independent double exponential priors fi fi expf gamma jfi 
shows double exponential density solid curve normal density broken curve implicit prior ridge regression 
notice double exponential density puts mass near zero tails 
reflects greater tendency lasso produce estimates large zero 
lasso bayes estimate beta double exponential density solid curve normal density broken curve 
implicit prior lasso ridge regression 
algorithms finding lasso solutions algorithms finding lasso solutions fix 
problem expressed squares problem inequality constraints corresponding different possible signs fi lawson hansen provide ingredients procedure solves linear squares problem subject general linear inequality constraint theta matrix corresponding linear inequality constraints vector fi 
problem may large direct application procedure practical 
problem solved introducing inequality constraints sequentially seeking feasible solution satisfying called kuhn tucker conditions lawson hansen 
outline procedure 
fi gamma fi ij ffi 
tuples form sigma sigma 
sigma 
condition jfi equivalent ffi fi fi fi ffi fi tg fi ffi fi tg 
set equality set corresponding constraints exactly met slack set corresponding constraints equality hold 
denote ge matrix rows ffi vector ones length equal number rows ge algorithm starts fi ffi sign fi fi squares estimate 
solves squares problem subject ffi fi checks jfi computation complete violated constraint added process continued jfi outline algorithm 
start fi ffi sign fi fi squares estimate 

find fi minimize fi subject ge fi 

fi tg 
add set ffi sign fi 
find fi minimize fi subject ge fi 
procedure converge finite number steps element added set step total elements 
final iterate solution original problem kuhn tucker conditions satisfied sets convergence 
modification procedure removes elements step equality constraint satisfied 
efficient clear establish convergence 
simulations fact algorithm iterations little comfort large 
practice average number iterations required range quite acceptable practical purposes 
completely different algorithm problem suggested david gay 
write fi fi gamma fi gamma fi fi gamma non negative 
solve squares problem constraints fi fi gamma fi fi gamma way transform original problem variables constraints new problem variables fewer constraints 
show new problem solution original 
standard quadratic programming techniques applied convergence assured steps 
extensively compared algorithms examples second algorithm usually little faster algorithm 
simulations outline examples compare full squares estimates lasso non negative garotte best subset selection ridge regression 
fold cross validation estimate regularization parameter case 
best subset selection leaps procedure language fold cross validation estimate best subset size 
procedure described studied breiman spector 
breiman spector recommend fold cross validation practice 
completeness details procedure 
best subsets size original dataset call 

represents null model fitted values zero model 
denote full training set cross validation training test sets gamma 

cross validation fold find best subsets size data gamma call 
pe prediction error applied test data form estimate pe pe find minimizes pe selected model note estimating prediction error fixed models 
choosing smallest prediction error 
simulations table results example method median stand 
err 
ave zero ave squares lasso cv lasso stein lasso gcv garotte best subset ridge procedure described zhang shao lead inconsistent model selection cross validation test set grows appropriate asymptotic rate 
example example simulated datasets consisting observations model fi oe delta ffl fi ffl standard normal 
correlation ae ji ae 
set oe gave signal noise ratio approximately 
table shows mean squared errors simulations model 
lasso performs best followed garotte ridge 
estimation lasso parameter generalized cross validation perform best trend find consistent examples 
subset selection picks approximately correct number zero coefficients suffers variability shown boxplots 
table shows frequent models non zero coefficients selected lasso gcv correct model chosen time selected model contained time 
frequent models selected subset regression shown table 
correct model chosen time subset selection selected model contained time 
example example fi oe signal noise ratio approximately 
results left side table show simulations full ls lasso garotte best subset ridge full ls lasso garotte best subset ridge full ls lasso garotte best subset ridge full ls lasso garotte best subset ridge full ls lasso garotte best subset ridge full ls lasso garotte best subset ridge full ls lasso garotte best subset ridge full ls lasso garotte best subset ridge estimates coefficients example excluding intercept horizontal dotted lines indicate true coefficients simulations table frequent models selected lasso gcv example model proportion table frequent models selected subsets regression example model proportion ridge regression best margin lasso method outperform full squares estimate 
right side table shows results sample size increased 
expected performance procedures improves 
notable exception lasso shrinkage parameter chosen stein method average shrinks shrinkage needed 
example chose setup suited subset selection 
model example fi oe signal noise ratio 
results table show garotte subset selection perform best followed closely lasso 
ridge regression poorly higher mean squared error full squares estimates 
example example examine performance lasso bigger model 
simulated datasets having observations variables note best subsets regression generally considered impractical 
defined predictors ij ij ij independent standard simulations table results example method med se 
ave zeroes ave med se 
ave zeroes ave squares lasso cv lasso stein lasso gcv garotte subset ridge table results example method median stand 
err 
ave zero ave squares lasso cv lasso stein lasso gcv garotte subset ridge lasso general models normal variates 
induced pairwise correlation predictors 
coefficient vector fi 


repeats block 
defined fi delta ffl ffl standard normal 
produced signal noise ratio roughly 
results table 
show ridge regression performs best lasso gcv close second 
average value lasso coefficients blocks 
lasso produced zero coefficients average average value close true proportion zeroes 
lasso general models lasso applied wide variety models 
consider model indexed vector parameter fi estimation carried maximization function fi may log likelihood function measure fit 
apply lasso standardize predictors appropriately maximize fi constraint jfi carry maximization general non quadratic programming procedure 
alternatively consider models quadratic approximation fi leads irls iteratively reweighted squares procedure computation fi 
models include generalized linear models generalized regression models 
irls approach solve constrained problem iterative application lasso algorithm linear models irls loop 
specifically terminology generalized linear models define linear predictor ff fi maximize log likelihood constraint jfi model longer eliminate ff centering response adjusted dependent variable weights irls step center gamma similarly 
minimize gamma ij fi subject jfi simple modify algorithms section incorporate weights 
convergence procedure assured general limited experience behaved quite 
tibshirani applies idea proportional hazards model survival data 
give brief illustration logistic regression 
logistic regression illustration applied lasso logistic regression model binary data 
data analyzed hastie tibshirani chapter 
response absent predictors age number levels starting level 
extensions observations 
predictor effects known nonlinear included squared terms model centering variables 
columns data matrix standardized 
linear logistic fitted model gamma gamma gamma gamma backward stepwise deletion akaike information criterion dropped term produced model gamma gamma gamma gamma lasso chose giving model gamma gamma gamma convergence defined fl fl fl fi new gamma fi old fl fl fl gamma obtained iterations 
extensions currently exploring quite different applications lasso idea 
application tree models reported leblanc tibshirani 
prune large tree breiman cart procedure lasso idea shrink 
involves constrained squares operation parameters mean contrasts node 
set constraints needed ensure shrunken model tree 
results reported leblanc tibshirani suggest shrinkage procedure gives accurate trees pruning producing interpretable subtrees 
different application multivariate adaptive regression spline mars proposal friedman 
mars adaptive procedure builds regression surface sum products piecewise linear basis functions individual regressors 
mars builds model typically includes basis functions representing main effects interactions high order 
give adaptively chosen bases mars fit simply linear regression bases 
backward stepwise procedure applied eliminate important terms 
ongoing trevor hastie developing special lasso type algorithm dynamically grow prune mars model 
hopefully produce accurate mars models ones interpretable 
lasso idea applied ill posed problems predictor matrix full rank 
chen donoho report encouraging results lasso style constraints context function estimation wavelets 
results soft thresholding results soft thresholding consider special case orthonormal design lasso estimate form fi sign fi fi gamma fl called soft threshold estimator donoho johnstone apply estimator coefficients wavelet transform function measured noise 
back transform obtain smooth estimate function 
donoho johnstone prove optimality results estimator translate results optimality results function estimation 
interest function estimation coefficients 
give donoho johnstone results 
shows asymptotically soft threshold estimator lasso comes close subset selection performance ideal subset selector uses information actual parameters 
suppose fix ffl ffl oe design matrix orthonormal 
write fi fi oe 
consider estimation fi squared error loss risk fi fi fi gamma consider family diagonal linear projections tdp fi ffi ffi fi ffi estimator keeps kills parameter fi subset selection 
incur risk oe fi fi estimate zero 
ideal choice ffi jfi oe keep predictors true coefficient larger noise level 
call risk estimator rdp course estimator constructed fi unknown 
rdp lower bound risk hope attain 
donoho johnstone prove hard threshold subset selection estimator fi fi fi fl risk fi fi log oe rdp discussion fl chosen oe log choice giving smallest asymptotic risk 
show soft threshold estimator fl oe log achieves asymptotic rate 
results lend support potential utility lasso linear models 
important differences various approaches tend occur correlated predictors theoretical results difficult obtain case 
discussion proposed new method lasso shrinkage selection regression generalized regression problems 
lasso doesn focus subsets defines continuous shrinking operation produce coefficients exactly zero 
evidence suggests lasso worthy competitor subset selection ridge regression 
examined relative merits methods different scenarios small number large effects subset selection best lasso quite 
ridge quite poorly 
small moderate number moderate sized effects lasso best followed ridge subset selection large number small effects ridge best margin followed lasso subset selection breiman garotte little better lasso scenario little worse second scenarios 
results refer prediction accuracy 
subset selection lasso garotte advantage vs ridge regression producing interpretable submodels 
ways carry subset selection regularization squares regression 
literature far fast attempt summarize short space mention developments computational advances led interesting proposals gibbs sampling approach george mcculloch 
set hierarchical bayes model gibbs sampler simulate large collection subset models posterior distribution 
allows data analyst examine subset models highest posterior probability carried large problems 
frank friedman discuss generalization ridge regression subset selection addition penalty form jfi residual sum squares 
equivalent constraint form jfi call bridge 
lasso corresponds 
discussion suggest joint estimation fi effective strategy report results 
contours constant value jfi values depicts situation dimensions 
subset selection corresponds 
value advantage closer subset selection ridge regression smallest value giving convex region 
furthermore linear boundaries convenient optimization 
encouraging results reported suggest absolute value constraints prove useful wide variety statistical estimation problems 
study needed investigate possibilities 
software public domain splus language functions lasso available statlib archive carnegie mellon university 
functions linear models generalized linear models proportional hazards model 
obtain ftp lib stat cmu edu retrieve file lasso send electronic mail statlib lib stat cmu edu message send lasso leo breiman sharing garotte publication michael carter assistance algorithm section david andrews producing mathematica 
acknowledge enjoyable fruitful discussions david andrews chen jerome friedman david gay trevor hastie geoff hinton iain johnstone stephanie land michael leblanc brenda stephen margaret wright 
comments editors referee led substantial improvements manuscript 
supported natural sciences engineering research council canada 
breiman 
better subset selection non negative garotte technical report univ cal berkeley 
breiman friedman olshen stone 
classification regression trees wadsworth 
efron tibshirani 
bootstrap chapman hall 
friedman 
multivariate adaptive regression splines discussion annals statistics 
george mcculloch 
variable selection gibbs sampling amer 
statist 
assoc 

hastie tibshirani 
generalized additive models chapman hall 
lawson hansen 
solving squares problems prentice hall 
stein 
estimation mean multivariate normal distribution ann 
statist 

