sparse priors mixing matrix independent component analysis hyv rinen neural networks research centre helsinki university technology box fin hut finland www cis hut fi projects ica independent component analysis prior information distributions independent components weak information fact necessary succesful estimation 
contrast prior information mixing matrix usually 
considered estimation completely blind form mixing matrix 
possible nd forms prior information suoeciently general useful wide range applications 
argue prior information sparsity mixing matrix constraint general merit attention 
show computational implementation priors mixing matrix simple cases expressed conjugate priors 
property conjugate priors means essentially algorithm ordinary ica 

component analysis ica statistical model observed data expressed linear transformation latent variables nongaussian mutually independent 
classic version model expressed xn vector observed random variables vector independent latent variables unknown constant matrix called mixing matrix 
problem estimate mixing matrix realizations latent variables observations 
exact conditions model fundamental independent components nongaussian 
considerable amount research conducted estimation model see 
prior knowledge distribution independent components assumed nongaussian 
nongaussian variables roughly divided groups supergaussian variables slightly dioeerent de nitions exist 
cases assumed know independent components sub supergaussian 
case example image feature extraction components assumed supergaussian sparse 
arbitrary assumption simple consequence fact independent components estimated image data supergaussian exceptions 
hand prior knowledge mixing matrix basic ica model 
advantage giving model great generality 
application areas information form mixing matrix available 
prior information mixing matrix give better estimates matrix number data points 
great importance situations computational costs ica estimation high severely restrict amount data situations amount data restricted due nature application 
situation compared regression overlearning general phenomenon 
classical way avoiding overlearning regression regularizing priors typically penalize regression functions large curvatures lots 
possible regression methods number parameters model large compared number observed data points 
extreme theoretical case number parameters innite model estimated nite amounts data prior information 
suitable priors reduce overlearning 
example prior knowledge predates modern ica methods literature beamforming see discussion specic form mixing matrix represented small number 
investigations application ica independent components modelled classic dipole model information constrain form mixing 
problem methods may applicable data sets lose generality main factors current interest ica 
introduce form prior information mixing matrix general applications strong increase performance ica estimation 
investigate possibility simple classes priors mixing matrix prior quadratic priors 
come classes useful ica 
introduce concept sparse priors 
priors enforce sparse structure mixing matrix 
words prior penalizes mixing matrices larger number non zero entries 
form prior similar prior knowledge sparseness independent components 
fact due similarity sparse priors called conjugate priors implies estimation kind priors particularly easy ordinary ica methods simply adapted priors 
sparse priors particularly useful image feature extraction link sparsely connected networks 

background jeffreys quadratic priors assume estimator inverse mixing matrix constrained estimates independent components wx white decorrelated unit variance restriction facilitates greatly analysis 
see 
concentrate formulating priors gamma completely analogue results hold prior 
prior classical prior bayesian inference prior 
considered maximally uninformative prior indicates probably useful purpose 
shown prior form det gamma constraint whiteness wx means expressed ub constant matrix restricted orthogonal 
det det det det implies prior constant space allowed estimators decorrelating 
see prior eoeect estimator reduce overlearning 

quadratic priors regression quadratic regularizing priors common 
tempting try idea context ica 
especially feature extraction require columns features smooth sense smoothness required regression functions 
words consider column discrete approximation smooth function choose prior imposes smoothness underlying continuous function 
similar arguments hold priors dened rows lters corresponding features 
simplest class regularizing priors quadratic priors 
show quadratic regularizers simple class dene change estimator 
consider priors form log mw const rows gamma matrix dene quadratic prior 
example prior log kw alternatively include dioeerential operators prior measure sense explained 
prior manipulated algebraically yield mw tr mw tr mw quadratic priors little ica estimation 
see constrain estimates independent components white 
means wcw space allowed estimates gives algebraic manipulations gamma see mw tr mc gamma const words quadratic prior constant 
result proven quadratic prior quadratic priors little interest ica 

sparse priors 
motivation satisfactory class priors call sparse priors 
means prior information says elements row zero 
motivation considering sparse priors empirical algorithmic 
empirically observed feature extraction images obtained lter tend localized space 
implies distribution elements ij lter tends sparse elements practically zero 
similar phenomenon seen analysis source signal usually captured limited number sensors 
due spatial localization sources sensors 
algorithmic appeal priors hand fact sparse priors conjugate priors 
special class priors means estimation model prior requires simple ordinary ica algorithms 
motivation sparse priors neural interpretation 
biological neural networks known sparsely connected small proportion possible connections neurons 
exactly sparse priors model 
interpretation especially interesting ica modelling visual cortex 

measuring sparsity mixing matrix sparsity random variable say measured expectations form efg non quadratic function example measures requires variance normalized xed value mean zero 
feature extraction probably applications distribution elements zero mean due symmetry 
furthermore assume data whitened preprocessing step 
denote whitened data vector components uncorrelated unit variance 
constraining estimates wz independent components white implies orthogonal implies sum squares elements ij equal elements row considered realization random variable zero mean unit variance 
means measure rows sparsity measure form 
dene sparse prior form log ij const logarithm supergaussian density function rows gamma function log density see measure sparsity prior nice property conjugate prior 
assume independent components supergaussian simplicity assume identical distributions log density take log prior density 
write prior form log const denote canonical basis vectors th element equal zero 
posterior distribution form log const form shows posterior distribution form prior distribution fact original likelihood 
priors property called conjugate priors bayesian theory 
usefulness conjugate priors resides property prior considered correspond sample 
posterior distribution form likelihood sample size consists observed canonical basis vectors words posterior likelihood augmented whitened data sample gammat conjugate priors additional bene exactly algorithm maximization posterior ordinary maximum likelihood estimation ica 
need add virtual sample data virtual sample size dimension data 

modifying prior strength conjugate priors generalized considering family supergaussian priors log ffg const kind prior means virtual sample points weighted parameter ff 
parameter expresses degree belief prior 
large ff means belief prior strong 
parameter ff dioeerent different useful 
posterior distribution form log ffg const expression case assumed density independent components laplacian 
case ff multiply log jw gamma jw ffe const simpler algorithmic viewpoint amounts addition just virtual data vectors form ffe data 
avoids complications due dioeerential weighting sample points ensures conventional ica algorithm simply adding virtual sample data 
fact laplacian prior ordinary ica algorithms form log cosh function considered smoother approximation absolute value function 

whitening priors assumed data preprocessed whitening 
noted eoeect sparse prior dependent whitening matrix 
sparseness imposed separating matrix whitened data value matrix depends whitening matrix 
whitening matrices imposing sparseness whitening matrix may dioeerent meanings 
practice problem solved whitening matrix sparse 
imposing sparseness whitened separating matrix meaningful 
context image feature extraction sparse whitening matrix obtained zero phase whitening matrix see discussion example 
hand necessary whiten data 
data whitened meaning sparse prior somewhat dioeerent 
row constrained unit norm general data 
measure sparsity correctly measure hand developments preceding section show sum squares matrix ij ij stay constant 
means sparsity measure measuring global sparsity individual rows 

experiments performed experiments image feature extraction explore applicability sparse priors 
basic idea 
data obtained theta pixel image patches random locations monochrome photographs depicting wild life scenes animals meadows forests 
patches normalized unit norm 
data whitened zero phase whitening lter means multiplying data gamma covariance data 
see 
results shown inverse preprocessing steps performed 
sample size xed 
large window size 
estimated basis vectors shown fig 
reasons space basis vectors shown randomly selected 
prior information prior weighting factor function prior information strength ff 
suitable value ff gives sparser components ordinary ica 
parameter ff xed obtained better basis 
basis shown fig 

visually sees features better 
validate prior quantitatively computed bases corresponding dioeerent values parameter ff correspond dioeerent strengths prior information 
sparsity measured negative expectation absolute value estimated independent components essentially approximation likelihood 
sparsity measured test set separate training set learning basis vectors 
plotted fig 

values sparsity seen increase increasing ff increasing strength placed prior information 
certain value sparsity maximum starts decreasing 
natural large value ff means prior information data neglected 

introduced sparse priors mixing matrix 
argued priors may useful wide area applications 
computationally convenient conjugate priors means existing ica algorithms directly simply introducing virtual sample 
experiments show sparse priors succesfully image feature extraction 


amari cichocki yang 
new learning algorithm blind source separation 
advances neural information processing systems pages 
mit press cambridge ma 
bell sejnowski 
approach blind separation blind deconvolution 
neural computation 
bell sejnowski 
independent components natural scenes edge lters 
vision research 

cardoso laheld 
equivariant adaptive source separation 
ieee trans 
signal processing 

cardoso 
blind beamforming non gaussian signals 
iee proceedings 
cichocki unbehauen 
robust neural networks line learning blind blind separation sources 
ieee trans 
circuits systems 
comon 
independent component analysis new concept 
signal processing 
hyv rinen 
fast robust xed point algorithms independent component analysis 
ieee trans 
neural networks 
hyv rinen 
sparse code shrinkage denoising nongaussian data maximum likelihood estimation 
neural computation 
hyv rinen hoyer 
emergence phase shift invariant features decomposition natural images independent feature subspaces 
neural computation 
press 
hyv rinen oja 
fast xed point algorithm independent component analysis 
neural computation 
hyv rinen rel rio 
spikes bumps artefacts generated independent component analysis sample size 
proc 
int 
workshop independent component analysis signal separation ica pages france 
jutten herault 
blind separation sources part adaptive algorithm neuromimetic architecture 
signal processing 
knuth 
bayesian approach source separation 
proc 
int 
workshop independent component analysis signal separation ica pages france 
olshausen field 
emergence receptive eld properties learning sparse code natural images 
nature 
pajunen 
blind source separation algorithmic information theory 
neurocomputing 
rio ki inen hari oja 
independent component analysis identi cation artifacts recordings 
advances neural information processing systems pages 
mit press 
estimation image features prior information 
sample size give useful estimates 
estimation image features suitable prior information 
estimation succesful small sample size 
