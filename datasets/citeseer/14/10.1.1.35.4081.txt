ieee transactions computers vol 
xx 
month improving cache locality combination loop data transformations kandemir student member ieee ramanujam member ieee choudhary member ieee exploiting locality key realizing high levels performance modern processors 
describes compiler algorithm optimizing cache locality scientific codes uniprocessor multiprocessor machines 
distinctive characteristic algorithm considers loop data layout transformations unified framework 
approach effective reducing cache misses optimize nests optimization techniques loop transformations successful 
important special case data layouts arrays fixed changed 
show algorithm accommodate case demonstrate optimize multiple loop nests 
experiments benchmarks show techniques result substantial improvement cache performance 
keywords caches data reuse locality loop data transformations optimizing compilers 
computer systems exploiting locality key achieving high levels performance 
known increasing cache hit rates important factors reducing average memory latency 
fine tuning cache coherence protocol selecting appropriate block line size important techniques improving cache performance 
advances include gonzalez developed dual data cache selective data cache sanchez proposed static locality analysis technique oriented optimize programs exhibit high conflict ratios applied results technique new caches developed 
software techniques complement advances cache hardware organization capable delivering additional performance 
observed compiler techniques useful optimizing locality uniprocessors multiprocessors reducing number coherence related misses 
software point view programmers compiler writers attempt change access patterns program majority accesses satisfied cache memory 
efforts aimed iteration space loop transformations scheduling techniques improve locality techniques improve data locality indirectly result modifying iteration space traversal order 
supported part nsf young investigator award ccr nsf ccr 
ramanujam supported nsf young investigator award ccr nsf ccr 
kandemir department electrical engineering computer science syracuse university syracuse ny 
ramanujam department electrical computer engineering louisiana state university baton rouge la 
choudhary department electrical computer engineering northwestern university evanston il 
offer compiler approach enhance cache performance scientific codes uniprocessors multiprocessors 
unified framework approach considers modifying array layouts memory transforming loop nests suitably exploit spatial locality 
simulate rates programs order demonstrate approach effective reducing number cache misses report execution times sgi challenge shared memory multiprocessor 
conclude fixing memory layouts arrays fortran limits performance obtained programs 
contributions ffl new algorithm optimizing spatial locality characteristics nested loops 
algorithm applies data loop transformations 
data transformation part activated layouts fixed cases obtains results existing loop transformation techniques 
ffl argue known approaches consider loop transformations loop permutations tiling insufficient cases 
ffl demonstrate effectiveness approach simulation results execution time measurements show approach effective uniprocessors multiprocessors 
approach oriented optimizing spatial locality generally effective large cache block sizes 
architectural trend anyway larger block sizes higher believe approach suitable architectures 
organized follows 
section ii presents brief summary necessary background 
section iii discuss related cache locality 
section iv discusses algorithm optimizing locality single loop nest 
section extend algorithm multiple loop nests 
section vi presents set experimental results illustrate efficacy approach 
section vii presents false sharing 
section viii conclude summary discussion 
ii 
preliminaries represent iteration loop nest depth loop iteration vector value index kth loop outermost 
subscript function dimensional array loop nest assumed affine function iteration represented mapped data element ieee transactions computers vol 
xx 
month theta matrix called access matrix vector called offset vector 
linear mappings iteration spaces loop nests modeled non singular transformation matrices 
applying transformation loop index transformed loop index transformed matrix gamma similarly distance direction vector applying new distance direction vector 
transformation legal lexicographically positive 
denote gamma important characteristic algorithm entries ij derived systematically array matrices 
order obtain performance programs running machine contains sort cache memory cache locality exploited 
data brought cache reused possible replaced 
reuse data cache termed temporal locality nearby data cache line called spatial locality 
stress program may data reuse due replacement data able exploit cache locality 
scope dense matrix programs affine subscript functions affine loop bounds focus mainly self spatial reuse cases group spatial reuses bring additional reuse dimension self spatial reuse space rare 
assume memory layout dimensional array 
forms corresponding layout data memory linearly nested traversal axes predetermined order 
words data storage schemes consider expressed permutations dimensions array 
dimensional array row major layouts 
dimensional array possible storages 
layout consider fastest changing dimension innermost dimension traversal array memory 
instance row major layouts dimension fastest changing dimension 
algorithm determines fastest changing dimension relative order dimensions may important assuming large array bounds 
iii 
related loop transformations fixed layout wolf lam definitions different types reuse propose algorithm optimize locality 
algorithm evaluates subset legal loop transformations transforms loop nest locality maximized 
focus tiling innermost loops 
contrast li uses concept reuse distance 
algorithm represent reuse vectors precisely transformations operated directly reuse vectors 
mckinley offer unified optimization technique consisting loop permutation loop fusion loop distribution 
consider data space memory layout transformations 
show data space transformations difference locality properties programs 
unifying data space transformations iteration space transformations locality exploited better way possible loop data transformations 
pure loop transformations discussed anderson propose data transformation technique distributed shared memory machines 
types data transformations strip mining permutation try data accessed processor contiguous shared address space 
algorithm inherits parallelism decisions previous phase suif compiler sense approach lend direct comparison attempts come loop data transformations improve locality 
combined loop data transformations cierniak li unified approach optimize locality employs data control transformations 
notion stride vector introduced optimization strategy developed obtaining desired mapping vectors representing layouts transformation matrix 
equality obtained formulation matrix known 
algorithm tries find iteration space transformation matrix mapping vector assume 
different forms dimensional array desired stride vector 
optimization problem difficult solve heuristic assumed transformation matrix contains 
second value stride vector assumed known 
algorithm constructs matrix row row considering restricted set legal mappings 
comparison approach accurate restrict search space possible loop transformations 
approach simpler embedding compilation system require prior knowledge vector extension multiple nests different proposed cierniak li global optimization 
cierniak li data transformations optimizing java byte codes 
iv 
algorithm optimizing locality section explain algorithm automatically transforms loop nest exploit spatial locality assigns appropriate memory layouts arrays unified framework 
assume array accessed lhs access function array rhs access function loop indices transformed nest starting outermost position 
brief explanation algorithm 
technical details 
comparison techniques handle false sharing 
ffl fix layout lhs array loop transformation matrix lhs array transformed loop innermost index element array dimensions index appear dimension array 
words af ieee transactions computers vol 
xx 
month ter transformation lhs array form new innermost loop index th dimension indicates term independent means th row transformed matrix entries column th row zero 
stronger requirement necessary suitable purposes 
lhs array stored memory th dimension fastest changing dimension 
approach effectively exploits spatial locality 
notice possible values considered 
ffl fix layouts rhs arrays algorithm works rhs time 
row data matrix identical row row original matrix lhs array algorithm attempts store rhs array memory dimension fastest changing dimension 
note having row guarantee array stored memory th dimension fastest changing dimension 
ideal case rhs array row identical th row lhs array stored memory corresponding dimension fastest changing dimension 
condition stated hold rhs array means array stored memory new innermost loop index appears fastest changing dimension 
case algorithm tries transform gamma gamma affine function gamma indices indicates term independent gamma helps exploiting spatial locality second innermost loop 
transformation possible transformed loop index gamma tried 
loop indices tried unsuccessfully remaining entries set arbitrarily observing data dependences non singularity 
ffl pick best alternative transformation corresponding memory layouts recorded alternative memory layout lhs tried 
feasible solutions best chosen 
best alternative exploits spatial locality innermost loop maximum number array 
emphasized algorithm determines transformation matrix inverse memory layouts 
depending resultant access matrices selected loop transformation determine fastest changing dimension array 
points mentioned 
noted algorithm optimizes lhs array 
strictly necessary useful lhs array read written arrays read 
second special case occurs array referenced 
belong uniformly generated set matrix consider 
hand array different access matrices reasonable heuristic concentrate frequently occurring 
third method appropriate modifications completing partial matrix full non singular transformation matrix data dependences observed 
algorithm performs exhaustive search worst case 
structure lhs allows explore search space layouts controlled manner sense enumeration tree hierarchically structured search space exploring current partial configuration feasible partial configuration 
worst case complexity algorithm maximum array dimensionality number arrays number loops nest 
practice algorithm fast 
illustration algorithm shows ijk matrix multiply routine 
matrices follows sake clarity show successful steps algorithm proceeds follows theta denotes don care entry compiler tries column major layout array theta theta 
theta theta 
theta theta 
point gamma setting gamma arrays column major resulting code shown 
compiler tries alternative memory layout row major array theta theta 
theta theta 
theta theta 
point gamma setting obtain gamma notice optimized nest nest obtained earlier works 
optimized nest nest lam row major layouts 
layouts fixed row major exhaustively search possible loop transformations cases omitting temporal locality replicate results obtained pure loop oriented approaches li 
ieee transactions computers vol 
xx 
month enddo enddo enddo enddo enddo enddo enddo enddo enddo fig 

matrix multiply nest 
optimized versions 
global locality optimization multiple loop nests section address problem optimizing collection sequence loop nests accessing subset arrays program 
easy show problem finding global array layout loop order combinations satisfy nests np complete restricted case row major column major arrays considered 
heuristic problem 
locality optimization layout constraints compilation program may possible compiler due data dependences constraints able apply loop transformations change memory layouts 
fact order loops nest may partially changed may changed 
similarly compiler may able change memory layouts arrays 
information constitutes constraint compiler 
focus problem optimizing locality array layouts fixed 
note fixed layout requires innermost loop index appropriate array index position dimension depending layout form array 
example suppose memory layout dimensional array dimension fastest changing dimension dimension second fastest changing dimension third compiler try place new innermost loop index th dimension array 
possible try place th dimension 
dimensions including tried unsuccessfully gamma tried th dimension 
subsection show constrained layout algorithm important global locality optimization 
global locality optimization algorithm algorithm find memory layout array satisfies majority nests 
approach concept costly nest 
intuitively nest takes memory time optimized 
different methods adopted choose nest 
example profiling may programmer compiler directives give hints nest 
algorithm proceeds follows costly nest optimized algorithm section iv 
step memory layouts arrays determined 
remaining nests optimized approach constrained layout case previous subsection 
nest optimized new layout constraints obtained propagated optimization nest 
note order processing remaining nests may important 
number nests small aggressive approach apply heuristic considering nest turn costly nest 
formal discussion algorithm refer reader 
vi 
experimental results section presents experimental results number example programs 
demonstrate simulation results obtained enhanced version uniprocessor cache simulator 
simulate rates range cache sizes block sizes set 
matrix multiply nest execution times obtained sgi challenge multiprocessor 
machine uses snoopy write invalidate cache coherence 
node mb data cache attached 
multiprocessor experiments static scheduling employed 
due space concerns subset results 
results comparison li algorithm longer version 
matrix multiply section iv showed algorithm optimizes nest 
experimental results 
shows ratios matrix multiply nest theta double arrays direct mapped cache 
different versions program unoptimized optimized arrays column major tiled version unoptimized nest tiled version optimized nest 
thing notice tiled optimized version outperforms rest cache block sizes 
important note cases optimized nest tiling performs better tiled unoptimized version 
tile size fixed loop tiled versions 
figures shows execution times nest different input sizes single node sgi challenge gives execution times different number processors sgi challenge theta double precision arrays 
note performance improvement unoptimized nest single node 
unoptimized code unopt optimized code opt outermost loop parallelized 
example nests nas benchmarks nas parallel benchmarks set programs designed help evaluate performance parallel supercomputers 
ieee transactions computers vol 
xx 
month block size cache size block size cache size block size cache size block size cache size unopt opt unopt tiled opt tiled fig 

simulation results matrix multiply nest 
problem size unopt opt number processors unopt opt ratio dgemm blas unopt opt ratio linpack unopt opt fig 

execution times matrix multiply nest single node 
execution times matrix multiply nest multiple nodes 
rates dgemm 
rates 
utilize cache effectively benchmarks generally access data unit stride 
default layout nests 
stressed examples considered representative nests programs 
ft benchmark kernel uses simple transpose complicated transpose nests 
leftmost group bars show performance improvement obtained approach different block sizes 
notice effectiveness approach increases larger block sizes 
middle rightmost bar charts show improvement complicated transpose obtained approach tile sizes theta theta respectively 
tile size theta data innermost loops fit cache algorithm add 
emphasized complicated transpose nest specifically meant exploiting cache memory 
example shows performance blocked tiled loop nest improved proper data layout optimizations applied 
sp benchmark illustrates reduction cache misses typical loop nest sp benchmark loop distribution applied 
seen block cache line size rate optimized program unoptimized 
chose example illustrate transformations loop distribution enable applicability techniques 
lu benchmark shows performance improvement typical loop nest 
block size half misses eliminated 
mg benchmark performance improvement illustrated typical loop nest substantial 
emphasized program fragment data transformations loop transformations optimize spatial locality arrays 
combination data loop transformations derived technique crucial best performance 
additional examples shows rates dgemm routine blas 
routine performs operation fff fic matrix ff fi scalars 
unoptimized optimized versions called times different operation average rates computed 
rates normalized rate unoptimized version 
pair bars triple cache size block size associativity 
simulation theta double precision matrices 
demonstrates performance improvement routine linpack solves systems form triangular matrix order optimizing dgemm loop data transformations data transformations applied 
vii 
false sharing shared memory multiprocessors processors different data items cache block dependence false sharing may occur 
cache coherence maintained block basis processor modifies data item causes invalidation processors cache 
main causes false sharing parallelization loop carries spatial reuse 
reducing extent false sharing improve scalability parallel applications execution time 
hand larger granularity parallelism better synchronization overhead diminish increasing parallelism granularity 
get maximum benefit shared memory multiprocessors parallelism locality optimized false sharing reduced possible 
optimization problem looks hard numerous factors ieee transactions computers vol 
xx 
month block size cache size unopt opt nc nc block size ratio cache size unopt opt block size ratio cache size unopt opt block size ratio cache size unopt opt fig 

rates example nests ft 
rates example nest sp 
rates example nest lu 
rates example nest mg 
involved optimization process 
discuss algorithm extended shared memory multiprocessor environment 
start observing far algorithm derives loop data transformations optimize spatial locality innermost loops possible 
consequence cases outermost loops transformed nest carry spatial reuse 
compiler safely parallelize loops carry spatial reuse probably cause false sharing 
rule compiler refrain parallelizing loop carries spatial reuse 
focus loop permutations 
essentially want transformed loop nest generic form delta delta delta delta delta delta delta delta delta doall delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta delta loop body enddo delta delta delta enddo delta delta delta delta delta delta enddo generic form spatial reuse carried subset loops including loops loops parallelized 
single parallel loop gamma want small possible 
best possible loop nest carry spatial reuse program 
steps taken approach follows run locality optimization algorithm explained obtain sequence alternative transformed programs 
possible loop nests permutations original loop nest 
possible array layouts row major column major higher equivalents 
compiler starts handling loops time 
transformed loop checks outermost loop run parallel 
checks loop carries spatial reuse 
compiler discards alternative focuses 
process stops compiler finds set alternative loop nests fit table possible loop permutations best fastest changing dimensions denoted 
array array order generic form explained 
candidate alternatives compiler chooses outermost loop parallelism 
spatial locality tie breaker need 
alternative job 
consider example 
gamma enddo enddo enddo table shows possible loop permutations array fastest changing dimension marked permutation 
example loop order third dimension second dimension fastest changing dimensions 
application dependence analysis reveals parallelizable loop loop 
focus loop order loop order results spatial locality provided array second dimension fastest changing dimension array dimension fastest changing dimension 
notice case spatial reuse arrays carried loop 
loop parallelizable loop nest runs parallel false sharing alternative fit generic optimized loop nest explained compiler discards 
similar analysis eliminate alternative 
remaining alternatives fit generic form 
outermost loop parallelism select transformed nest 
sum second step approach ieee transactions computers vol 
xx 
month candidates third step candidates equally optimized points view parallelism locality 
viii 
summary presents new algorithm improving cache locality scientific computations 
algorithm transforms loop nests changes memory layouts multidimensional arrays unified framework 
algorithm employed combined locality optimizations tiling transformations loop fusion loop distribution 
experimental results programs provide strong evidence approach successful uniprocessors multiprocessors 
currently exploring possibility dynamically changing data layouts evaluating relative merits considering runtime overhead incurred change 
addition plan problem improving spatial locality sparse computations 
anderson amarasinghe lam 
data computation transformations multiprocessors 
proc 
th acm sigplan symp 
principles practice parallel programming july 
carr 
compiler dense matrix factorizations 
acm trans 
mathematical software september 
cierniak li 
unifying data control transformations distributed shared memory machines 
proc 
sigplan conference programming language design implementation june 
cierniak li 
optimizing java compiler 
proc 
ieee compcon san jose california february 
gannon jalby gallivan 
strategies cache local memory management global program transformations 
journal parallel distributed computing 
gonzalez 
data cache multiple caching strategies tuned different types locality 
proc 
acm international conference supercomputing pages july 
hill smith 
evaluating associativity cpu caches 
ieee trans 
computers december 
eggers 
reducing false sharing shared memory multiprocessors compile time data transformations 
proc 
th acm sigplan symp 
principles practice parallel programming july 
kandemir ramanujam choudhary 
compiler algorithm optimizing locality loop nests 
technical report tr northwestern university evanston il february 
kandemir ramanujam choudhary 
compiler algorithm optimizing locality loop nests proc 
th acm international conference supercomputing pages july 
ahmed pingali 
data centric multi level blocking 
proc 
acm sigplan conference programming languages design implementation pages june 
lam rothberg wolf 
cache performance optimizations blocked algorithms 
proc 
th int 
conf 
architectural support programming languages operating systems april 
li 
compiling numa parallel machines 
ph thesis cornell university ithaca ny 
mckinley carr tseng 
improving data locality loop transformations 
acm trans 
programming languages systems 
sanchez gonzalez 
static locality analysis cache management 
proc 
int 
conf 
parallel architectures compilation techniques pact november 

survey software solutions cache consistency maintenance shared memory multiprocessors 
ieee software fall 

tutorial cache coherency problem shared memory multiprocessors hardware solutions 
ieee computer society press los alamitos california 
torrellas lam hennessy 
false sharing spatial locality multiprocessor caches 
ieee trans 
computers june 
wilson french wilson amarasinghe anderson tjiang liao tseng hall lam hennessy 
suif infrastructure research parallelizing optimizing compilers 
acm sigplan notices december 
wolf lam 
data locality optimizing algorithm 
proc 
acm sigplan conf 
programming language design implementation pages june 
wolfe 
high performance compilers parallel computing 
addisonwesley publishing ca 
kandemir phd candidate electrical engineering computer science department syracuse university 
received bs ms computer engineering istanbul technical university 
research interests include different aspects locality improvement techniques cache memories optimizations intensive applications computer architecture 
student member ieee computer society 
ramanujam received tech 
degree electrical engineering indian institute technology madras august ph degrees computer science ohio state university columbus ohio august december respectively 
currently associate professor electrical computer engineering louisiana state university baton rouge 
research interests compilers high performance computer systems program transformations high level synthesis parallel input output systems parallel architectures algorithms 
dr ramanujam received national science foundation young investigator award 
served program committees international international conference parallel processing th international conference supercomputing conferences 
member high performance fortran forum 
taught tutorials compilers high performance computers conferences international conference parallel processing supercomputing scalable high performance computing conference international symposium computer architecture 
choudhary received ph university illinois urbana champaign electrical computer engineering university massachusetts amherst 
institute technology science india 
associate professor electrical computer engineering department northwestern university september 
associate professor ece department syracuse university assistant professor department 
worked industry computer consultants prior 
choudhary received national science foundation young investigator award 
received ieee engineering foundation award ibm faculty development award intel research council award 
main research interests high performance computing communication systems applications domains including multimedia systems information processing scientific computing 
particular interests lie design evaluation architectures software systems system software runtime systems compilers programming languages applications high performance servers high performance databases input output 
published papers various journals conferences areas 
written book book chapters topics 
research sponsored past darpa nsf nasa afosr onr doe intel ibm texas instruments 
