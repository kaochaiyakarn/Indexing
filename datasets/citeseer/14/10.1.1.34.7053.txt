draft 
appear simon haykin ed kalman filtering neural networks em algorithm identi cation nonlinear dynamical systems sam roweis zoubin ghahramani provide novel solution problem simultaneously estimating unknown parameters hidden states nonlinear dynamical system 
solution expectation maximization em algorithm iterative procedure maximum likelihood parameter estimation data sets missing hidden variables 
em applied system identi cation linear statespace models state variables hidden observer state parameters model estimated simultaneously 
generalize em algorithm estimate parameters nonlinear dynamical state space models 
expectation step extended kalman smoothing estimate state maximization step re estimates parameters uncertain state estimates 
general nonlinear maximization step dicult requires integrating uncertainty states 
gaussian radial basis function rbf approximators model nonlinearities integrals tractable maximization step solved systems linear equations 
derive online version em eks algorithm version non stationary time series 
consider identi ability expressive power nonlinear dynamical systems relate learning algorithm traditional system identi cation procedures dual joint extended kalman filtering 
demonstrate algorithm synthetic problems real time series 
learning stochastic nonlinear dynamics advent cybernetics dynamical systems important modeling tool elds ranging engineering physical social sciences 
realistic dynamical systems models essential features 
stochastic observed outputs noisy function inputs dynamics may driven unobserved noise process 
second characterized nite dimensional internal state directly observable summarizes time information past behaviour process relevant predicting evolution 
modeling standpoint stochasticity essential allow model xed parameters generate rich variety time series outputs 
explicitly modeling internal state possible decouple internal dynamics observation process 
example model sequence video images balloon oating gatsby computational neuroscience unit university college london london wc ar www gatsby ucl ac uk course completely deterministic chaotic systems property 
separate noise processes models deterministic portions dynamics observations think noises deterministic highly chaotic system depends initial conditions exogenous inputs know 
run simulations pseudo random number generator started particular seed precisely doing 
wind computationally costly directly predict array camera pixel intensities sequence arrays previous pixel intensities 
sensible attempt infer true state balloon position velocity orientation decouple process governs balloon dynamics observation process maps actual balloon state array measured pixel intensities 
able write equations governing dynamical systems directly prior knowledge problem structure sources noise example physics situation 
cases may want infer hidden state system sequence observations system inputs outputs 
solving inference state estimation problem essential tasks tracking design controllers exist known algorithms 
cases exact parameter values gross structure dynamical system may unknown 
cases dynamics system learned identi ed sequences observations 
learning may necessary precursor ultimate goal ective state inference 
learning nonlinear state models useful right explicitly interested internal states model tasks prediction extrapolation time series classi cation outlier detection lling missing observations interpolation 
chapter addresses problem learning time series models internal state hidden 
brie review fundamental algorithms form basis learning procedure 
section ii introduce algorithm derive learning rules 
section iii presents results algorithm identify nonlinear dynamical systems 
potential extensions algorithm sections iv state inference model learning remarkable algorithms developed engineering statistics form basis modern techniques state estimation model learning 
kalman lter introduced kalman bucy developed setting physical model dynamical system interest readily available goal optimal state estimation systems known parameters 
expectation maximization em algorithm pioneered baum colleagues generalized named dempster de draft 
appear simon haykin ed kalman filtering neural networks veloped learn parameters statistical models presence incomplete data hidden variables 
chapter bring algorithms order learn dynamics stochastic nonlinear systems hidden states 
goal fold develop method identifying dynamics nonlinear systems hidden states wish infer develop general nonlinear time series modeling tool 
examine inference learning discrete time stochastic nonlinear dynamical systems hidden states external inputs noisy outputs 
lowercase characters indices denote vectors 
matrices represented uppercase characters 
systems parametrized set tunable matrices vectors scalars shall collectively denote 
inputs outputs states related zero mean gaussian noise processes 
state vector evolves nonlinear stationary markov dynamics driven inputs noise source outputs nonlinear noisy stationary instantaneous functions current state current input 
vector valued nonlinearities assumed di erentiable arbitrary 
models kind examined decades systems control engineering 
viewed framework probabilistic graphical models graph theory represent conditional dependencies set variables 
graphical model diagram corresponding probabilistic generative model node possibly vector valued random variable directed arcs representing stochastic dependencies 
absent connections indicate conditional independence 
particular nodes conditionally independent non descendents parents parents children descendents de ned respect directionality arcs parent child 
capture dependencies equations compactly drawing graphical model shown gure 
graphical models helped clarify relationship dynamical systems probabilistic models hidden markov models factor analysis 
graphical models possible develop continuous time dynamical systems derivatives speci ed functions current state inputs converted discrete time systems sampling outputs zero order holds inputs 
particular continuoustime linear system sampled interval corresponding dynamics input driving matrices ax bu exp ac bc stationarity means covariance noise process depend time dynamics time invariant 
markov refers fact current state state depend past history states 
fig 

probabilistic graphical model stochastic dynamical systems hidden states inputs observables probabilistic inference algorithms vastly general kalman lter 
learning algorithm nonlinear dynamical systems combines key ideas kalman ltering em review sections 
goal develop algorithm model probability density output sequences conditional density outputs inputs nite number example time series 
crux problem hidden state trajectory parameters unknown 
knew parameters operation interest infer hidden state sequence 
uncertainty sequence encoded computing posterior distributions hidden state variables sequence observations 
kalman lter provides solution problem case linear 
hand access hidden state trajectories observables problem model tting estimating parameters noise covariances 
observations longer hidden states outputs obtained solution possibly nonlinear regression problem noise covariances obtained residuals regression 
proceed system model hidden states unknown 
classical approach solving problem treat parameters extra hidden variables apply extended kalman filtering algorithm described nonlinear system state vector augmented parameters 
stationary models dynamics parameter portion extended state vector set identity function 
approach inherently line may important certain applications 
furthermore provides estimate covariance parameters time step 
objective probabilistically speaking nd optimum joint space parameters hidden state sequences 
contrast algorithm batch algorithm discuss section iv online extensions possible attempt estimate covariance parameters 
instances em algorithm describe goal integrate uncertain estimates unknown hid draft 
roweis ghahramani learning nonlinear dynamical systems den states optimize resulting marginal likelihood parameters observed data 
extended kalman smoother eks estimate approximate state distribution step radial basis function rbf network nonlinear regression step 
important confuse extended kalman algorithm estimate just hidden state part step em described previous paragraph simultaneously estimate parameters hidden states 
kalman filter linear dynamical systems additive white gaussian noises basic models examine considering state estimation problem admit exact ecient inference 
follows call system linear state evolution function state output observation function linear nonlinear 
linear dynamics observation processes correspond matrix operations denote respectively giving classic statespace formulation input driven linear dynamical systems ax bu cx du gaussian noise vectors zero mean covariances respectively 
prior probability distribution initial states taken gaussian joint probabilities states outputs times gaussian gaussian distribution closed linear operations applied state evolution output mapping convolution applied additive gaussian noise 
distributions hidden state variables fully described means covariance matrices 
algorithm exactly computing posterior mean covariance sequence observations consists parts forward recursion uses observations known kalman lter backward recursion uses observations combined forward backward recursions known kalman rauch tung rts smoother 
key insights understanding kalman lter 
rst insight kalman lter simply method implementing bayes rule 
consider general setting prior state variable observation model yjx noisy outputs state 
bayes rule gives state inference procedure xjy yjx yjx yjx dx normalizer unconditional density observation 
need convert prior state posterior multiply likelihood observation equation renormalize 
second insight need invert output dynamics functions long easily normalizable distributions hidden states 
see applying bayes rule linear gaussian case single time step 
start gaussian belief current hidden state dynamics convert prior state condition observation convert prior posterior 
gives classic kalman ltering equations ax av jx cx jy cx kc cv 
posterior gaussian analytically tractable 
notice dynamics matrix observation matrix needed inverted 
third insight state estimation procedures implemented recursively 
posterior previous time step run dynamics model prior current time step 
convert prior new posterior current observation 
general case nonlinear system nongaussian noise state estimation complex 
particular mapping arbitrary nonlinearities result arbitrary state distributions integrals required bayes rule intractable 
methods proposed overcome intractability providing distinct approximate solution inference problem 
assuming di erentiable noise gaussian approach locally linearize nonlinear system current state estimate applying kalman lter linearized system approximate state distribution remains gaussian 
algorithms known extended kalman lters ekf 
ekf classical setting state estimation nonlinear dynamical systems basis online learning algorithms feedforward neural networks radial basis function networks 
possibility propagate set discrete samples state space re weight likelihood yjx 
algorithms general strategy known particle lters particular form known condensation notation multivariate normal gaussian distribution mean covariance matrix written 
gaussian evaluated point denoted determinant matrix denoted jaj matrix inversion symbol means distributed 
draft 
appear simon haykin ed kalman filtering neural networks 
lters called monte carlo lters bootstrap lters dynamic mixture models 
see book survey 
third approximation called unscented lter deterministically chooses set balanced points propagates nonlinearities order recursively approximate gaussian state distribution 
algorithms approximate inference learning mean eld theory variational methods 
chosen local linearization eks basis algorithms possible formulate learning algorithms approximate inference method example unscented lter 
em algorithm em expectation maximization algorithm widely applicable iterative parameter re estimation procedure 
objective em algorithm maximize likelihood observed data presence hidden variables 
denote entire sequence observed data fy hidden variables fx parameters model 
maximizing likelihood function equivalent maximizing log likelihood log log dx distribution hidden variables obtain lower bound log dx log dx log dx log dx log dx middle inequality known jensen inequality proven concavity log function 
de ne energy global con guration log lower bound negative quantity known statistical physics free energy expected energy minus entropy 
em algorithm alternates maximizing respect distribution parameters respectively holding xed 
starting initial parameters alternately apply step arg max step arg max called missing data auxiliary parameters 
easy show maximum step results exactly conditional distribution jy point bound equality 
maximum step obtained maximizing rst term entropy depend step arg max jy log dx expression associated em algorithm obscures elegant interpretation em coordinate ascent see gure 
step step change guaranteed decrease likelihood combined em step 
obviously true complete em algorithms described may true incomplete sparse variants approximations steps long goes see earlier 
example take form gradient step algorithm increase respect strictly maximize step improves bound saturating 
dynamical systems hidden states step corresponds exactly solving smoothing problem estimating hidden state trajectory observations inputs parameter values 
step involves system identi cation state estimates smoother 
heart em learning procedure idea solutions ltering smoothing problem estimate unknown hidden states observations current model parameters 
ctitious complete data solve new model parameters 
estimated states obtained inference algorithm usually easy solve new parameters 
example working linear gaussian models typically involves minimizing fig 

em algorithm coordinate ascent functional see text 
step maximizes respect xed horizontal moves step maximizes respect xed vertical moves 
draft 
roweis ghahramani learning nonlinear dynamical systems quadratic forms done linear regression 
process repeated new model parameters infer hidden states 
keep mind goal maximize log likelihood equivalently maximize total likelihood observed data respect model parameters 
means integrating summing ways model produced data hidden state sequences consequence em algorithm maximization nd needing compute maximize expected log likelihood joint data expectation taken distribution hidden values predicted current model parameters observations 
past em algorithm applied learning linear dynamical systems speci cases multiple indicator multiple cause models single latent variable state space models observation matrix known generally 
chapter extension earlier similar applying em nonlinear dynamical systems 
uses sampling step gradient steps algorithm uses rbf networks obtain computationally ecient exact step 
important advantages em algorithm classical approaches 
em algorithm provides straightforward principled method handing missing inputs outputs 
original motivation shumway sto er application em algorithm learning partially unknown linear dynamical systems 
second em generalizes readily complex models combinations discrete real valued hidden variables 
example formulate em mixture nonlinear dynamical systems 
third dicult prove analyze stability classical line approach em algorithm attempting maximize likelihood acts lyapunov function stable learning 
fourth em framework facilitates bayesian extensions learning example variational approximations 
ii 
combining eks em sections describe basic components em learning algorithm 
expectation step algorithm infer approximate conditional distribution hidden states extended kalman smoothing section ii 
maximization step rst discuss general case section ii describe particular case nonlinearities represented gaussian radial basis function rbf networks section ii 
em likelihood ascent algorithms algorithm guaranteed nd globally optimum solutions initialization key factor practical success 
typically variant factor analysis followed estimation purely linear dynamical system starting point training nonlinear models section ii 
extended kalman smoothing step system described equations step em learning algorithm needs infer hidden states history observed inputs outputs 
quantities heart inference problem conditional densities ju ju nonlinear systems conditional densities general non gaussian fact quite complex 
nonlinear systems exact inference equations written closed form 
furthermore nonlinear systems interest exact inference intractable numerically meaning principle amount computation required grows exponentially length time series observed 
intuition extended kalman algorithms approximate stationary nonlinear dynamical system non stationary time varying linear system 
particular extended kalman smoothing eks simply applies regular kalman smoothing local linearization nonlinear system 
point space derivatives vector valued functions de ne matrices respectively 
dynamics linearized mean current ltered smoothed state estimate time output equation similarly linearized 
linearizations yield noise distributions prior distribution hidden state gaussian progressively linearized system conditional distribution hidden state time history inputs outputs gaussian 
kalman smoothing linearized system infer conditional distribution illustrated gure 
notice algorithm performs smoothing words takes account observations including ones inferring state time linearization done forward direction 
re linearize backwards estimates rauch recursions 
principle approach give better results dicult implement practice requires dynamics function uniquely invertible true 
normal linear kalman smoother eks error covariances state estimates kalman gain matrices depend observed data just time index furthermore longer true system stationary kalman gain converge value smoother act draft 
appear simon haykin ed kalman filtering neural networks linearize models gaussian inputs outputs time gaussian gaussian evidence evidence evidence fig 

illustration information extended kalman smoothing eks infers hidden state distribution step algorithm 
nonlinear model linearized current state estimate time kalman smoothing linearized system infer gaussian state estimates 
optimal wiener lter steady state 
fact kalman gain need converge xed value 
learning model parameters step step em algorithm re estimates parameters model observed inputs outputs conditional distributions hidden states 
model described parameters de ne nonlinearities noise covariances mean covariance initial state 
complications arise step 
fully re estimating step may computationally expensive 
example represented neural network regressors single full step lengthy training procedure backpropagation conjugate gradients optimization method 
avoid partial steps increase maximize expected log likelihood example consisting gradient steps 
general tting procedure slower 
second complication trained uncertain state estimates output eks algorithm 
dicult apply standard curve tting regression techniques 
consider tting takes inputs outputs conditional density estimated eks gaussian space 
set data points mixture full covariance gaussians input output space gaussian clouds data 
ideally follow em framework conditional density integrated tting process 
integrating type data non trivial form simple cient approach bypass problem draw large sample gaussian clouds data samples usual way 
similar situation occurs tting output function alternative approach choose form function approximator integration easier 
show gaussian radial basis function rbf networks model allows integrals exactly eciently 
choice representation complications vanish 
fitting radial basis functions gaussian clouds general formulation rbf network clear special forms consider nonlinear mapping input vectors output vector ax bu zero mean gaussian noise variable covariance rbfs de ned 
general mapping ways represent dynamical systems depending input hidden output mappings assumed nonlinear 
examples representing substitutions representing representing substitutions 
di erent simulations different forms 
parameters coecients rbfs matrices multiplying inputs respectively output bias vector noise covariance rbf assumed gaussian space center width covariance matrix exp assume centers widths rbfs xed discuss learning locations section iv goal model data 
complication data set comes form mixture gaussian distributions 
show analytically integrate mixture distribution rbf model 
assume data set observe samples variables paired gaussian cloud data 
gaussian mean covariance matrix ax bu set parameters 
log likelihood single fully observed data point model ln jqj const values data set uncertain maximum expected log likelihood rbf mixture draft 
roweis ghahramani learning nonlinear dynamical systems gaussian data obtained minimizing integrated quadratic form min dx dz ln jqj rewrite slightly di erent notation angled brackets 
denote expectation de ning objective written min ln jqj derivatives respect premultiplying setting zero gives linear equations solve hz words expectations angled brackets optimal parameters solved set linear equations 
appendix show expectations computed analytically eciently means take full exact steps 
derivation somewhat laborious intuition simple gaussian rbfs multiply gaussian densities form new unnormalized gaussians space 
expectations new gaussians easy compute 
tting algorithm illustrated gure 
note advantages mentioned previously em algorithm ability handle missing observations generalizability extensions basic model bayesian approximations guaranteed stability lyapunov function forgo 
guarantee extended kalman smoothing increases lower bound true likelihood stability assured 
practice algorithm rarely unstable approximation worked experiments likelihood increased monotonically density models learned may desirable derive guaranteed stable algorithms certain special cases lower bound preserving variational approximations 
input dimension output dimension fig 

illustration regression technique employed step 
mixture gaussian densities required gaussian rbf networks solved analytically 
dashed line shows regular rbf centres gaussian densities solid line shows analytic rbf covariance information 
dotted lines show support rbf kernels 
ability fully integrate uncertain state estimates provides practical bene ts theoretically pleasing 
compared tting rbf networks means state estimates performing full integration derived 
means necessary introduce ridge regression weight decay parameter step penalize large coecients occur precise cancellations inputs 
ridge regression regularizer adding white noise radial basis outputs rbf kernels applied 
equivalent gaussian noise inputs covariance determined derivatives rbfs input locations 
uncertain state estimates provide exactly sort noise automatically regularize rbf step 
naturally avoids need introduce penalty large coecients improves generalization 
initialization models choosing locations rbf kernels practical success algorithm depends design choices need training procedure 
rst judiciously select placement rbf kernels representation state dynamics output function 
second sensibly initialize parameters model iterative improvement em algorithm nds local maxima likelihood function nds solution 
models low dimensional hidden states placement rbf kernel centres done state space placing kernel grid point 
scaling state variables covariance matrix state dynamics noise loss draft 
appear simon haykin ed kalman filtering neural networks generality set possible determine suitable size region statespace suitable scaling rbf kernels 
number kernels grid increased exponentially grid dimension state variables state space impractical 
cases rst simple initialization linear dynamical system infer hidden states place rbf kernels randomly chosen subset inferred state means 
set widths variances rbf kernels spacing centres attempting neighbouring kernels cross outputs half peak value 
ensures coecients set approximately equal rbf network output space 
heuristics xed assignments centres widths initialization adaptive rbf placement procedure 
section iv discuss techniques adapting positions rbf centres widths training model 
systems nonlinear dynamics approximately linear output functions initialize maximum likelihood factor analysis fa trained collection output observations conditional factor analysis models inputs 
loading matrix factor analysis solution initialize observation matrix dynamical system 
doing time independent inference factor analysis model obtain approximate estimates state time independently 
estimates initialize nonlinear rbf regressor tting estimates time step function previous time step 
iterations training purely linear dynamical system initializing nonlinear rbf network 
systems nonlinear ows embedded linear manifolds initialization estimates embedding manifold linear statistical technique fa ow nonlinear regression projections estimated manifold 
output function nonlinear dynamics approximately linear mixture factor mfa trained output observations 
systems linear ows nonlinear embedding manifold 
mfa initialization captures nonlinear shape output manifold 
estimating dynamics dicult hidden states individual mixture combined easily single order properly cover portions state space frequently require minimum distance rbf kernel centres 
practice reject centres fall close 
way see consider gaussian rbfs dimensional grid square lattice heights 
rbf centres de ne hypercube distance neighboring rbfs chosen 
centres hypercubes contributions neighboring gaussians distance nd contributes height 
height interiors approximately equal height corners 
internal state representation possible 
ideally bayesian methods control complexity model estimating internal state dimension optimal number rbf centres 
general approximate techniques cross validation variational approximations implemented practice see section iv 
currently set theses complexity parameters hand cross validation 
iii 
results tested algorithm learn dynamics nonlinear system observing system inputs outputs 
investigated behaviour simple dimensional state space problems nonlinear dynamics know weather time series problem involving real temperature data 
dimensional nonlinear state space models order able compare algorithm learned internal state representation ground truth state representation rst tested synthetic data generated nonlinear dynamics form known 
systems considered consisted inputs observables time hidden state variables 
relation state time step variety nonlinear functions followed gaussian noise 
outputs linear function state inputs plus gaussian noise 
inputs ected state linear driving function 
true learned state transition functions systems sample outputs response gaussian noise inputs internal driving noise shown gures right columns 
initialized nonlinear model linear dynamical model trained em turn initialized variant factor analysis see section ii 
dimensional state space models rbfs space uniformly spaced range automatically determined density inferred points 
dimensional state space models rbfs spaced grid uniformly range inferred states 
initialization algorithm discovered nonlinearities dynamics iterations em see gures left middle panels 
training models input output observations dynamics examined learned internal state representation compared known structure generating system 
gures show algorithm approximate solution problem getting single hidden state mfa procedure 
estimate similarity centres average separation time data points active 
standard embedding techniques multidimensional scaling mds place mfa centres euclidean space dimension 
time independent state inference observation consists responsibility weighted low dimensional mfa centres responsibilities posterior probabilities observation mfa 
draft 
roweis ghahramani learning nonlinear dynamical systems fantasy true fig 

example tting system nonlinear dynamics linear observation function 
panels show tting nonlinear system dimensional hidden state noisy outputs driven gaussian noise inputs internal state noise 
left true dynamics function line states dots generate training data 
inset histogram internal states 
right learned dynamics function states inferred training data 
inset histogram inferred internal states 
middle rst component observable time series training data 
bottom component fantasy data generated learned model scale middle plot 
recovers form nonlinear dynamics quite 
able generate fantasy data models learned exciting gaussian noise similar variance applied training 
resulting observation streams look qualitatively similar time series true systems 
quantify quality comparing log likelihood training sequences novel test sequences nonlinear model likelihood basic linear dynamical system model static model factor analysis 
presents comparison 
nonlinear dynamical system signi cantly superior likelihood training test data example systems 
notice system linear dynamical system better factor analysis strong hysteresis mode locking system 
output previous time step excellent predictor current output 
weather data example real system nonlinear output function important dynamics trained model records daily maximum minimum temperatures melbourne australia period 
model internal state variables outputs inputs 
training phase outputs minimum maximum daily temperature real valued output indicating data available world wide web australian bureau meteorology 
fantasy true fig 

multidimensional example tting system nonlinear dynamics linear observation functions 
true system piecewise linear state space 
plots show tting nonlinear system dimensional hidden state noisy outputs driven gaussian noise inputs internal state noise 
top left true dynamics vector eld arrows states dots generate training data 
top right learned dynamics vector eld states inferred training data 
middle bottom rst component observable time series training data fantasy data generated learned model 
time year month range 
model trained days temperature records just seasons 
tested remaining days showing model minimum maximum daily temperatures attempting predict time year month 
prediction performed eks algorithm state inference available observation streams 
state inference performed learned output function model predict time year 
prediction problem inherently requires information previous times static relationship temperature season ambiguous spring fall 
shows results prediction training algorithm discovered relationship hidden state observations allows perform reasonable prediction task 
show model predictions minimum maximum temperatures inferred state 
explicitly part generative model learned system implicitly parameterizes relationship time year temperature 
discover relationship evaluating nonlinear output function points state space 
evaluation yields triple month minimum maximum temperature 
triples plotted gure show model discovered southern hemisphere seasonal temperature variations 
draft 
appear simon haykin ed kalman filtering neural networks training data fa lds test data fa lds log likelihood bits sample fig 

di erences log likelihood assigned various models training test data systems gure 
adjacent group bars shows log likelihood factor analysis fa linear dynamical systems lds nonlinear dynamical systems 
letters ordinate match labels previous gures 
results training data appear left test data right taller bars represent better models 
log likelihoods set fa training data zero 
error bars represent quantile median repetitions training testing 
iv 
extensions learning means widths rbfs possible relax assumption gaussian radial basis functions xed centers widths results somewhat complicated slower tting algorithm 
derive learning rules rbf centers width matrices need consider play cost function rbf kernel 
take derivatives respect expectation cost function exchange order expectation derivative recalling clear gures nonlinearly places equation possible solve closed form 
gradient move center decrease cost corresponds partial step equation requires computing third order expectations addition rst second order expectations needed optimize similarly di erentiating cost respect gives temperature days month fig 

model maximum minimum daily temperatures melbourne australia 
left vertical line system hidden states governed linear dynamics nonlinear output function trained observation vectors dimensional time series consisting maximum minimum temperatures day real valued month year 
training points shown triangles max temp squares min temp solid line sawtooth wave 
right vertical line training system infer internal state temperature observations 
having inferred internal state predict month year missing output line 
solid lines upper plots show model prediction minimum maximum temperature inferred state time 
need fourth order expectations xm xm xm additional expectations increase storage computation time algorithm cost may compensated added advantage moving centers widths small gradient steps 
heuristic place centers widths unsupervised techniques em algorithm gaussian mixtures considers solely input density output nonlinearity 
alternatively higher order expectations approximated example hxi 
online learning major limitations algorithm batch algorithm assumes entire sequence observations estimate model parameters 
fortunately relatively straightforward derive online version algorithm updates parameters receives observations 
achieved recursive squares rls algorithm 
key observation cost minimized step algorithm quadratic function parameters 
rls simply way solving quadratic problems online 
index time step resulting draft 
roweis ghahramani learning nonlinear dynamical systems temperature month max temp min temp fig 

prediction maximum minimum daily temperatures time year 
model gure implicitly learns relationship time year minimum maximum temperature 
relationship directly invertible temporal information extended kalman smoothing correctly infers month temperature shown gure 
algorithm scalar follows hz zi ignore expectations 
initializing large easy show iterations estimates rapidly converge exact values obtained square solution 
estimate converge correct values plus bias incurred fact early estimates residuals lim recursive estimate obtained matrix inversion lemma 
important way online algorithm approximation batch em algorithm described nonlinear state space models 
expectations 
online algorithm computed running single step extended kalman lter previous parameters batch em algorithm expectations computed running extended kalman smoother entire sequence current parameter estimate 
expectations re estimate parameters smoother re run parameters re re estimated perform usual iterations em 
general expect time series non stationary parameter estimates obtained batch algorithm convergence model data better obtained online algorithm 
interestingly updates rls online algorithm described similar parameter updates dual extended kalman lter approach system identi cation discussed section 
similarity coincidental kalman lter derived generalization rls algorithm 
fact similarity exploited elegant manner derive online algorithm parameter estimation non stationary nonlinear dynamical systems 
nonstationarity handle nonstationary time series assume parameters drift gaussian random walk covariance function relating variables parameters nonlinear kernels view observation model state variable time varying output matrix dynamics observation models linear noise gaussian apply kalman lter recursively compute distribution drifting parameters hz tjt tjt tjt tjt tjt tjt tjt zi important things note 
equations describe ordinary kalman lter output output matrix jointly uncertain gaussian distribution 
second assumed output noise covariance drift introducing forgetting factor re estimation equation 
expectations computed running step ekf hidden variables derived online algorithm starting batch em algorithm ended appears identical dual extended kalman lter 
kalman lters extended ordinary running parallel estimating hidden states parameters respectively 
view online algorithm approximation bayesian posterior parameters hidden variables 
true posterior complicated distribution parameters 
recursively approximated independent gaussians 
approximated posterior mean covariance draft 
appear simon haykin ed kalman filtering neural networks bayesian methods model selection complexity control maximum likelihood procedure em algorithm described chapter potential data set nd spurious patterns noise data generalizing poorly 
implementation ridge regression weight decay regularizer parameters practice required heuristics setting regularization parameters 
mentioned previously integrating hidden variables acts sort modulated input noise ect performs ridge regression eliminate need explicit regularization 
second closely related problem faced maximum likelihood methods built procedure doing model selection 
value maximum likelihood suitable way choose di erent model structures 
example consider problems choosing dimensionality state space choosing number basis functions higher dimensions basis functions principle result higher maxima likelihood means complex models preferred simpler ones 
course leads tting 
bayesian methods provide general framework simultaneously handling tting model selection problems consistent manner 
key idea bayesian approach avoid maximization possible 
possible models structures parameters short settings unknown quantities weighted posterior probabilities predictions weighted posterior 
nonlinear dynamical system example treat parameters unknown 
model prediction output time ju ju jy jy dx ju ju rst integral line posterior distribution parameters second integral posterior distribution hidden variables 
posterior distribution parameters obtained recursively bayes rule jy ju ju ju dual extended kalman lter joint extended kalman lter non stationary online algorithm section iv coarse approximations bayesian recursions 
equations implicitly conditioned choice model structure sm dimension number basis functions 
bayesian modeling philosophy advocates averaging predictions di erent model structures necessary possible bayes rule choose model structures probabilities sm jy ju sm sm ju sn sn tractable approximations required integrals obtained ways 
highlight ideas going detail adequate solution problem nonlinear dynamical systems requires research 
rst idea markov chain monte carlo techniques sample parameters hidden variables 
mcmc methods gibbs sampling linear dynamical systems promising method nonlinear systems particle ltering 
second idea called automatic relevance determination ard 
consists zero mean gaussian prior parameter tunable variances 
optimizing variance hyperparameters results irrelevant eliminated model 
ard rbf networks center data point tipping successfully nonlinear regression name relevance vector machine analogy support vector machines 
third idea variational methods lower bound model structure posterior probabilities 
variational bayesian methods infer structure linear dynamical systems generalization nonlinear systems kind described chapter straightforward 
course principle bayesian approach advocate averaging possible choices easy see rapidly get unwieldy 
discussion identi ability expressive power saw experiments algorithm capable learning density models variety nonlinear time series 
specifying class nonlinear systems algorithm model de nes expressive power 
related question ability model principle recover actual parameters speci nonlinear systems 
question model identi ability 
questions intimately tied describe mapping actual nonlinear systems model parameter settings 
trivial degeneracies model technically able concern 
possible permute dimensions state space permuting domain output mapping dynamics corresponding fashion obtain exactly equivalent model 
second state variables rescaled fact transformed invertible linear mapping 
transformation absorbed draft 
roweis ghahramani learning nonlinear dynamical systems output dynamics functions yielding model identical input output behaviour 
loss generality set covariance state evolution noise identity matrix sets scale state space disallows certain state transformations reducing expressive power model 
third take observation noise uncorrelated state noise noises zero mean loss generality absorbed functions exist forms ability dicult overcome 
example nonlinear noise free case arbitrary invertible transformation state exist transformations result identical inputoutput behavior 
case hard detect recovered model model actual system estimated actual states appear unrelated 
clearly systems modeled assuming linear nonlinear 
similarly systems modeled assuming nonlinear linear 
example consider case observations statistically independent observation lies curved low dimensional manifold high dimensional space 
modeling require nonlinear nonlinear factor analysis 
choosing linear restricts expressive power model 
state noise covariance assuming observation noise covariance diagonal restrict expressive power model 
easy see case dimension state space small dimension observation vector large 
full covariance capture correlations observations single time step diagonal model 
nonlinear dynamical systems gaussian noise assumption restrictive may initially appear 
nonlinearity turn gaussian noise non gaussian noise 
course restricted expressive power rbf network especially means centers rbfs xed 
try appeal universal approximation theorems claim principle model nonlinear dynamical system 
misleading light noise assumptions fact nite usually small number rbfs going practice 
embedded flows ways think dynamical models investigated shown gure 
non imagine joint noise covariance nonzero hw replacing sr gives new noise process covariance sr uncorrelated leaving input output behavior invariant 
similarly non zero noise means absorbed terms functions fig 

interpretations graphical model stochastic non linear dynamical systems see text 
top markov process embedded manifold 
bottom nonlinear factor analysis time 
linear markov process ow embedded potentially projected manifold perspective function controls evolution stochastic process function speci es nonlinear embedding projection operation 
way think model nonlinear version latent variable model factor analysis possibly external inputs latent variables factors evolves time drawn independently observation 
nonlinear factor analysis model represented time evolution latent variables state space lower dimension observation space observation noise additive useful geometrical intuition applies 
cases observed ow inside embedded manifold 
observation function speci es structure shape manifold dynamics speci es ow manifold 
armed intuition learning problem looks decoupled separate stages rst nd manifold doing sort density modeling collection observed outputs ignoring time order second nd ow dynamics projecting observations manifold doing nonlinear regression time step 
intuition partly true provides basis practical ective initialization schemes tried 
crucial point far design learning algorithms concerned learning problems interact way problem easier 
know dynamics information gives prior knowledge trying learn manifold shape 
example dynamics suggest state near certain point information better naive projection locate noisy observation manifold 
conversely knowing manifold allows estimate simplify presentation ll neglect driving inputs section arguments extend systems inputs 
draft 
appear simon haykin ed kalman filtering neural networks fig 

linear nonlinear dynamical systems represents ow elds embedded manifolds 
systems linear output functions illustrated manifold hyperplane dynamics may complex 
systems nonlinear output functions shape embedding manifold curved 
dynamics ectively 
discuss separately special cases ows manifolds systems linear output functions nonlinear dynamics systems linear dynamics nonlinear output function 
output function linear dynamics nonlinear gure observed sequence forms nonlinear ow linear subspace observation space 
manifold estimation easier high levels observation noise fact shape known hyperplane 
required nd orientation character output noise 
analysis observations algorithms factor analysis excellent way initialize estimates hyperplane noises 
learning cause tilt hyperplane dynamics better conversely modify dynamics hyperplane model better 
setting expressive initially 
consider nonlinear output function invertible sense written form invertible non square matrix nonlinear output function strictly linear transform new state variable gives equivalent model purely linear output process potentially non additive dynamics noise 
nonlinear output functions paired linear dynamics observation sequence forms matrix linear ow nonlinear manifold ax manifold learning harder estimate thin curved subspace observation space presence noise 
learned manifold approximately project observations learn linear dynamics 
win comes fact locations projected dynamics look linear know bend manifold dynamics linear 
shape outputs ignoring time linearity dynamics give clues learning manifold 
stability stability key issue study dynamical system 
consider stability levels stability learning procedure stability learned nonlinear dynamical system 
step em algorithm guaranteed increase log likelihood convergence builtin lyapunov function stable learning 
pointed extended kalman smoothing step algorithm represents approximation exact step forego guarantees stability learning 
rarely problems stability learning sure depending quality eks approximation close true system dynamics boundary stability 
contrast eks approximations certain variational approximations transform intractable lyapunov function tractable preserve stability learning 
clear apply variational approximations nonlinear dynamics clearly interesting area research 
stability learned nonlinear dynamical system analyzed making linear systems theory 
know discrete time linear dynamical systems eigenvalues matrix lie inside unit circle system globally stable 
nonlinear dynamics rbf network decomposed parts cf eq linear component nonlinear component 
clearly system globally stable satisfy eigenvalue criterion linear systems 
rbf coecients bounded max jh rbf support bounded way min det max ij jc nonlinear system stable bounded input bounded output sense 
sequence bounded inputs output sequence noise free system bounded probability 
intuitively unstable behavior occur region rbf support leaves region drawn back online em learning algorithm hidden state dynamics parameter re estimation dynamics interact stability analysis quite challenging 
stability guarantee batch eks em algorithm simple form online algorithm provably stable 
draft 
roweis ghahramani learning nonlinear dynamical systems takens theorem hidden states long known linear systems equivalence called state space formulations involve hidden variables direct vector autoregressive models time series 
takens proved remarkable theorem tells deterministic nonlinear dynamical system dimensional state space state ectively reconstructed observing time lags outputs 
particular takens showed lag vector smooth embedding di true state exists 
notion nding embedding state justify nonlinear regression approach learning nonlinear dynamical systems 
suspect system nonlinear state dimensions building state space model away representing states just build autoregressive ar model directly observations nonlinearly relates previous outputs current output 
view begs question need models hidden states 
constructive realization takens theorem exists general linear systems strong results 
purely linear systems appeal cayley hamilton theorem show hidden state eliminated obtain equivalent vector autoregressive model time lags output 
furthermore construction allows conversion performed explicitly 
takens theorem offers similar guarantee elimination hidden states nonlinear dynamical systems long take output lags 
similar recipe exists explicitly converting autoregressive form 
results hidden states unnecessary 
problem view generalize realistic high dimensional noisy scenarios 
consider example mentioned 
mathematically true pixels video frame balloon oating wind highly nonlinear function pixels previous video frames modeling perspective build ar model video images 
require number parameters order number pixels squared 
furthermore noise free case takens theorem dynamics noisy optimal prediction observation depend entire history past observations 
truncation square matrix size satis es characteristic equation 
equivalently matrix power written linear combination lower matrix powers start system ax cx create dimensional lag vector holds current outputs 
write gx ci ca ca ca gaussian noise covariance 
cayley hamilton theorem assures full rank need take lags 
lag vector solve system gx write solution original observation equation times solve terms write autoregression gaussian noise tory throws away potentially valuable information unobserved state 
state space formulation nonlinear dynamical systems allows overcome limitations nonlinear autoregressive models 
allows compact representations dynamics integrate uncertain information time 
price paid requires having inference hidden state 
parameters hidden states treated di erently maximum likelihood framework em algorithm distinction parameters hidden variables attempts integrate hidden variables maximize likelihood function parameters 
leads step approach computes sucient statistics hidden variables step optimizes parameters step 
contrast fully bayesian approach learning nonlinear dynamical state space models treat hidden variables parameters unknown attempt compute approximate joint posterior distribution ect integrating 
important compare approaches system identi cation traditional ones 
highlight approaches joint ekf approaches dual ekf approaches 
joint ekf approaches augmented hidden state space constructed comprises original hidden state space parameters 
parameters hidden states interact linear dynamical systems approach results nonlinear dynamics augmented hidden states 
initializing gaussian prior distribution parameters states extended kalman lter recursively update joint distribution states parameters observations jy 
approach advantage model uncertainties parameters correlations parameters hidden variables 
fact approach treats parameters state variables completely symmetrically thought iteratively implementing gaussian approximation recursive bayes rule computations 
nonstationarity easily built giving parameters random walk dynamics 
appealing properties approach known su er instability problems reason dual ekf approaches proposed 
dual ekf approaches interacting distinct extended kalman lters operate simultaneously 
computes gaussian approximation state posterior parameter estimate observations old computes gaussian approximation parameter posterior estimated states old 
interact feeding estimate posterior means 
think dual ekf performing approximate coordinate ascent jy itera draft 
appear simon haykin ed kalman filtering neural networks tively maximizing old old assumption conditional gaussian 
interaction parameters hidden variables occurs respective means procedure avor mean eld methods physics neural networks 
methods su er dence problem parameter estimate take account uncertainty states parameter covariance overly narrow likewise states 
large systems joint dual ekf methods su er fact parameter covariance matrix quadratic number parameters 
problem pronounced joint ekf considers concatenated state space 
furthermore joint dual ekf methods rely gaussian approximations parameter distributions 
problematic consider retaining positive de niteness noise covariance matrix assumption parameters gaussian distributed 
vi 
chapter brings classic algorithms statistics systems engineering address learning stochastic nonlinear dynamical systems 
shown pairing extended kalman smoothing algorithm approximate state estimation step radial basis function learning model permits exact analytic solution step em algorithm capable learning nonlinear dynamical model data 
side ect derived algorithm training radial basis function network data form mixture gaussians 
derived online version algorithm version dealing non stationary time series 
demonstrated algorithm series synthetic realistic nonlinear dynamical systems shown able learn accurate models observations inputs outputs 
initialization model parameters placement radial basis kernels important practical success algorithm 
discussed techniques making choices provided gradient rules adapting centres widths basis functions 
belief network literature dominated methods approximate inference markov chain monte carlo variational approximations 
knowledge rst instances extended kalman smoothing perform approximate inference step em 
eks theoretical guarantees variational methods approximate monotonically optimize computable objective function learning simplicity gained wide acceptance estimation control literatures method doing inference nonlinear dynamical systems 
practical success modeling variety nonlinear time series suggests combination extended kalman algorithms em algorithm provide powerful tools learning nonlinear dynamical systems 
authors acknowledge support gatsby charitable fund nsf pdf fellowship sr appendix expectations required fit rbfs expectations need compute equation hxi hx hz starting easier ones depend rbf kernel hxi xx zz xz observe multiply gaussian rbf kernel equation get gaussian density mean covariance ij ij ij extra constant due lack normalization ij dx js jc jc ij expf ij ij ij ij ij ij ij evaluate expectations ij hx ij ij hz ij ij dx jc js jc expf dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society series vol 
pp 

chen em approach multiple indicators multiple causes model estimation latent variables journal american statistical association vol 
pp 

draft 
roweis ghahramani learning nonlinear dynamical systems shumway sto er approach time series smoothing forecasting em algorithm journal time series analysis vol 
pp 

zoubin ghahramani geo rey hinton parameter estimation linear dynamical systems tech 
rep crg tr dept computer science university toronto february 
kalman bucy new results linear ltering prediction trans 
american society mechanical engineers series journal basic engineering vol 
pp 

baum inequality applications statistical estimation probabilistic functions markov processes model ecology bulletin american mathematical society vol 
pp 

pearl probabilistic reasoning intelligent systems networks plausible inference morgan kaufmann san mateo ca 
lauritzen spiegelhalter local computations probabilities graphical structures application expert systems journal royal statistical society vol 
pp 

sam roweis zoubin ghahramani unifying review linear gaussian models neural computation vol 

ljung om theory practice recursive identi cation mit press cambridge ma 
goodwin sin adaptive ltering prediction control prentice hall 
moody darken fast learning networks processing units neural computation vol 
pp 

kalman new approach linear ltering prediction problems trans 
american society mechanical engineers series journal basic engineering vol 
pp 
march 
rauch solutions linear smoothing problem ieee transactions automatic control vol 
pp 

linear regression applied system identi cation adaptive control systems journal american institute aeronautics astronautics vol 
pp 

cox estimation state variables parameters noisy dynamic systems ieee transactions automatic control vol 
pp 

singhal wu training multilayer perceptrons extended kalman algorithm advances neural information processing systems vol 
pp 

morgan kaufmann 
niranjan function estimation approach sequential learning neural networks neural computation vol 
pp 

mclachlan lowe practical methods tracking nonstationary time series applied real world data aerosense applications science arti cial neural networks ii rogers eds 
pp 
spie proceedings 
mayne monte carlo techniques estimate conditional expectation multi stage non linear ltering international journal control vol 
pp 

isard blake condensation conditional density propagation visual tracking international journal computer vision vol 
pp 

kitagawa monte carlo lter smoother nongaussian nonlinear state space models journal computer graphics graphical statistics vol 
pp 
mar 
gordon salmond smith novel approach nonlinear non gaussian bayesian state space estimation iee proceedings radar signal processing vol 
pp 

mike west approximating posterior distributions mixtures journal royal statistical society ser 
vol 
pp 

mike west mixture models monte carlo bayesian updating dynamic models computing science statistics vol 
pp 

doucet de freitas gordon sequential monte carlo methods practice springer verlag new york 
julier uhlmann durrant whyte new approach ltering nonlinear systems proceedings american control conference seattle pp 

julier uhlmann new extension kalman lter nonlinear systems proceedings aerosense th international symposium aerospace defense sensing simulation controls orlando 
eric wan van der merwe alex nelson dual estimation unscented transformation advances neural information processing systems 
vol 
mit press 
zoubin ghahramani geo rey hinton variational learning switching state space models neural computation vol 
pp 

jordan ghahramani jaakkola saul variational methods graphical models machine learning 
baum petrie statistical inference probabilistic functions nite state markov chains annals mathematical statistics vol 
pp 

neal hinton view em algorithm justi es incremental sparse variants learning graphical models jordan ed 
pp 
kluwer academic press 
csisz ar information geometry alternating minimization procedures statistics decisions supplement issue pp 

zoubin ghahramani sam roweis learning nonlinear dynamical systems em algorithm advances neural information processing systems 
vol 
pp 
mit press 
de freitas niranjan gee nonlinear state space estimation neural networks em algorithm tech 
rep cambridge university engineering dept 
tresp fisher scoring mixture modes approach approximate inference learning nonlinear state space models advances neural information processing systems vol 

mit press 
zoubin ghahramani geo rey hinton switching statespace models tech 
rep crg tr dept computer science university toronto july 
kevin murphy switching kalman lters tech 
rep dept computer science university california berkeley august 
hinton dayan revow modeling manifolds images handwritten digits ieee transactions neural networks vol 
pp 

zoubin ghahramani geo rey hinton em algorithm mixtures factor analyzers tech 
rep crg tr dept computer science university toronto may revised feb 
torgerson multidimensional scaling theory method psychometrika vol 
pp 

eric wan alex nelson dual kalman ltering methods nonlinear prediction advances neural information processing systems 
vol 
mit press 
carter kohn gibbs sampling state space models biometrika vol 
pp 

fr bayesian model discrimination bayes factors linear gaussian state space models journal royal statistical society series vol 
pp 

mackay bayesian non linear modelling prediction competition ashrae transactions atlanta georgia vol 
pt pp 
ashrae 
neal assessing relevance determination methods delve neural networks machine learning bishop ed 
pp 
springer verlag 
tipping relevance vector machine advances neural information processing systems 
vol 
pp 
mit press 
ghahramani beal variational inference bayesian mixtures factor analysers advances neural information processing systems 
vol 
mit press 
draft 
appear simon haykin ed kalman filtering neural networks takens detecting strange attractors turbulence dynamical systems rand 
young eds proceedings warwick conference vol 
lecture notes mathematics pp 
springer verlag berlin 
hertz krogh palmer theory neural computation addison wesley redwood city ca 
neal probabilistic inference markov chain monte carlo methods tech 
rep crg tr department computer science university toronto 
draft 
roweis ghahramani learning nonlinear dynamical systems fig 

examples tting systems nonlinear dynamics linear observation functions 
rows shows tting nonlinear system dimensional hidden state noisy outputs driven gaussian noise inputs internal state noise 
left true dynamics function line states dots generate training data 
middle learned dynamics function states inferred training data 
right rst component observable time series training data top fantasy data generated learned model bottom 
nonlinear dynamics produce quasi periodic outputs response white driving noise 
