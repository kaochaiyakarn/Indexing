soccer draft caching techniques streaming multimedia internet markus hofmann eugene ng katherine guo sanjoy paul hui zhang bell laboratories carnegie mellon university corner road forbes avenue holmdel nj usa pittsburgh pa usa hofmann sanjoy bell labs com cs cmu edu existing solutions streaming multimedia internet scale terms object size number supported streams 
separate unicast streams example overload network servers 
caching standard technique improving scalability existing caching schemes support streaming media 
propose complete solution caching multimedia streams internet extending existing techniques proposing new techniques support streaming media 
include segmentation streaming objects dynamic caching self organizing cooperative caching 
consider techniques integrated fashion 
implemented complete caching architecture called soccer network simulator ns evaluate effectiveness proposed technique compare alternative caching solutions 
keywords streaming media caching proxy 
internet world wide web ubiquitous infrastructure distributing kinds data services including continuous streaming data video audio 
significant increase commercial products playback stored video audio internet occurred past years proliferation server sites support audio video contents 
existing solutions streaming multimedia efficient separate unicast stream request require stream travel server client internet request 
content provider point view server load increases linearly number receivers 
receiver point view endure high start latency unpredictable playback quality due network congestion 
isp point view streaming multimedia architecture poses serious network congestion problems 
multicast caching common techniques enhancing scalability general information dissemination systems 
directly applied support streaming media playback web 
multicast receivers assumed homogeneous synchronous 
reality receivers generally heterogeneous asynchronous 
problem solved batching multiple requests multicast session reducing server load network load 
unfortunately solution increase average start latency 
caching web objects improving latency reducing network load studied extensively starting cern httpd followed improvements hierarchical caching operative caching harvest project squid project respectively 
surprisingly little extend cache systems support streaming media 
existing caching schemes designed take advantage streaming characteristics 
example video objects usually large cached entirety 
single hour long mpeg movie instance requires gbytes disk space 
finite buffer space streams stored cache decreasing hit probability efficiency caching system 
addition transmission streaming objects needs rate regulated timing constraints need considered design caching system streaming media 
explore assess techniques enhance caching systems better support streaming media internet segmentation streaming objects dynamic caching self organizing cooperative caching 
consider techniques integrated fashion define unified streaming architecture called self organizing cooperative caching architecture soccer realize techniques allow assume transmission rate streaming objects constant bit rate cbr 
described techniques extended support streaming objects variable bit rate vbr 
soccer draft combination 
architecture caching multicast techniques takes advantage unique properties streaming media 
key components architecture called helper machines caching streaming agents inside network communicating novel scalable state distribution protocol forming dynamic meshes forwarding streaming data 
rest organized follows 
section ii discusses techniques extending caching systems better support streaming media 
section iii describes design unified streaming architecture 
evaluate various streaming techniques implemented architecture ns simulator built prototype implementation unix platform 
section iv presents simulation results 
related discussed section conclude section vi ii 
streaming extensions caching systems section discuss techniques better support streaming media caching systems 
segmentation streaming objects smart segment replacement caching entire streaming objects fashion propose promising way dividing objects smaller segments caching replacement purposes 
suppose minimal allocation unit cache disk block size streaming object segment size multiple rest discussion simply assume segment size doing segments streaming object cached replaced independently contention disk space greatly reduced disk space efficiently cache popular portions large streaming objects 
drawback independent segment caching streaming request playback time sec sec arrives cache request result partial cache hit certain parts requested data cache 
satisfying requests requires searching missing segments caches 
increases signaling cost increases probability losing synchronization 
address issue need control number missing gaps 
way accomplish increase segment size 
increases contention disk space extreme case degenerates caching 
points need large logical unit caching retains fine granularity disk allocation replacement 
chunk chunk chunk chunk uncached segments prefix cached segments streaming multimedia object fig 

segmentation chunking streaming objects propose logical unit caching called chunk 
illustrates various caching units 
chunk simply number contiguous segments object 
starting object segments form chunk 
chunk cached independently prefix caching allocation replacement policy 
basic unit caching cache replacement segment segments allocated chunk form prefix chunk segment chunk accessed segment chunk ejected cache segment chunk chosen replacement algorithm segment segments actual ejected victim 
varying chunk size achieve trade maximum number gaps segment replacement flexibility 
extreme case degenerates performing prefix caching entire object 
practice finding missing gaps system require form intelligent prefetching meet timing requirements streaming 
dynamic caching classical web caching contents cached multimedia data segments change time ejection controlled cache replacement policy 
refer static caching 
streaming nature multimedia offers new opportunities bandwidth savings performing static caching 
key observation playback requests streaming media related temporal distances 
normally playback requests require separate data streams 
hide temporal distance requests data stream needed serve requests 
observation exploited video servers video demand systems 
generalize technique caching systems name dynamic caching 
demonstrates basic dynamic caching technique 
receiver requested certain streaming object server time time soccer draft buffer time data data patch fig 

example illustrating dynamic caching requests object 

temporal distance requests 

seconds stream received notice subsequent data streamed needed satisfy request 
allocating ring buffer network cache moving window 
seconds starting playback time data stream data stream shared satisfy request 
seconds 
ring buffer essentially hidden temporal distance requests 
course obtain initial 
seconds missing data called patch cache network 
known patching 
basic techniques proposed context receivers buffering data locally order join progress multicast streaming session 
generalize techniques allow network cache perform dynamic caching receivers network caches allow stream flow number dynamic caches network 
words dynamic caches form stream distribution mesh network 
course dynamic caching enables ip multicast delivery streams caching system exactly proposed 
dynamic caching helps stream data multiple destinations efficiently complementary technique static caching 
useful data streamed caches caches 
happens static caches filled caches highly loaded 
remarkable small ring buffer deliver complete streaming object large 
choice accessing dynamic cache versus static cache interesting trade offs 
moving window nature dynamic cache ensures delivery entire streaming object 
hand static cache usually holds portions streaming object requiring complex operations order retrieve entire streaming object system 
second dynamic caches requires feeding stream dynamic cache 
possibly increases network load compared accessing static caches 
self organizing cooperative caching cooperation distributed caching systems requires ability identify caches learn current state 
optimally cache know current state caches 
allow choose suitable cache cooperate time 
impractical keep consistent date view distributed caches 
existing caching systems solve problem statically defining neighbors cache configuration file 
approach limits ability dynamically adapting changes network load system load cache contents 
furthermore support instant identification active data streams caches prerequisite sharing multicast streams multiple user requests 
reasons propose mechanism scalable state distribution enables caches learn state remote caching systems 
mechanism loosely coupled cache meshes formed forward streaming object various receivers 
call selforganizing cooperative caching 
details scalable state distribution self organizing cooperative caching section 
iii 
unified caching architecture streaming media order explore assess streaming extensions caching systems propose unified architecture called self organizing cooperative caching architecture soccer embodies techniques discussed section ii 
core elements called helper machines caching data forwarding agents inside network 
helpers serve requests streaming objects cached static dynamic data possible 
close cooperation helpers enabled scalable state distribution 
receiver interested getting certain streaming object simply sends request origin server 
request redirected receiver proxy helper transparently layer switch configuring proxy client software 
receiving request proxy helper identifies locally available caches helpers serve request static dynamic caching 
appropriate helper request forwarded directly origin server 
section iii illustrates interaction static dynamic caching 
mechanisms criteria enabling helper cooperation explained section iii iii 
soccer draft fig 

example illustrating unified architecture multicast unicast interaction static dynamic caching soccer static caching dynamic caching 
interaction static dynamic caching illustrated example shown 
receivers video server helpers data cached helper initially 
illustrates example multicast shows example unicast 
receiver requests streaming object time request directly addressed transparently redirected proxy helper step 
requested data available local cache remote helper request sent server step 
receiving request server initiates new data stream object step 
new data stream unicast multicast 
assume multicast 
time receives data stream starts caching incoming data infinitesimally small ring buffer 
starts caching data segments static cache discussed section ii 
simultaneously streams data step 
begins advertising ability deliver segments object step 
time receiver issues request streaming object request redirected reaches proxy helper time step 
state distribution advertisements knows state helper multicast stream transmitting desired object 
interesting options depending preferable exploit static dynamic caches unicast multicast techniques unicast server helper contact server request entire streaming object just 
obviously require additional buffer space increases server network load 
multicast server dynamic caching enables exploit ongoing multicast transmission 
join multicast group allocates ring buffer hold 
seconds data 
extra seconds added absorb network delays random factors 
starts buffering received multicast data step time requests patches get 
seconds object 
request patch static cache directly sender step 
receiving patch step forwards step 
soon forwarded patching data starts forwarding data ring buffer step 
unicast helper dynamic cache state distribution knows content static dynamic caches 
decide exploit dynamic cache allocates ring buffer size 
seconds sends request step 
receiving request starts forwarding data step 
request patch static cache directly sender 
rest operations proceed step 
unicast helper static cache possible serve request solely accessing static caches contacting server case necessary 
mechanisms scalable state distribution self organizing helper cooperation enabled scalable state distribution 
helpers periodically send advertise messages known multicast address indicating ability serve requests certain media objects 
state information select best helpers cooperate 
mechanism scalable state distribution strives balance conflicting goals network load kept low possible freshness state information high possible 
obviously frequent sending global advertisements improves freshness increases network load 
notice helper prefer getting support nearby helper far away helper 
reduce overhead caused global advertisements trade helper receive advertisements nearby helpers frequently helpers far away 
observations motivated design expanding ring advertisement era originally de soccer draft interval ttl table ttl values send advertisements scope frequency reception advertising helper helper receiver advertisement 
advertisement 
advertisement 
advertisement fig 

different scoping areas veloped context reliable multicast protocols 
era algorithm ttl scope restriction 
easily modified administratively scoped multicast addresses scoping mechanism 
era algorithm helpers send advertisements dynamically changing ttl values table ttl values defined scope regions current mbone 
easily adjusted conform specific network infrastructures 
scheme message sent scope local second scope regional effect scheme illustrated 
listening helpers scope helper get advertise message assuming packet losses 
distance helper receive advertisement 
th advertise message distributed worldwide 
ensures frequency advertise messages exponentially decreases increasing scope 
scheme reduces network load allowing short reaction time local scope 
ensures global visibility helpers larger time scale 
observing sequence numbers advertisements estimate number hops helpers requiring access header received ip packets 
helper selection receiving request streaming object helper decide get requested data 
data retrieved local cache directly server static dynamic cache helpers 
decision making process called helper selection 
general helper may get portions requested data multiple sources 
goal helper selection algorithm find near optimal sequence static dynamic cache accesses order serve request 
propose helper selection algorithm uses step wise algorithm find cache time finishes streaming data selected cache determining cache sequence achieving step wise local optimum 
algorithm helper selection iteration step wise algorithm helper looks best cache dynamic static 
chooses static cache gets data selected static cache start available local cache request 
chooses dynamic cache gets data clip 
algorithm outlined 
iteration cost function discussed section algorithm finds lowest cost cache currently helpers serve request 
selection restricted static caches flag set 
restrict number switches static caches algorithm requires static cache segments data considered 
cache static helper gets data possible static cache point local static cache advances iteration current request 
cache dynamic helper sends request get data tend dynamic cache 
time continues searching get required patches static caches 
dynamic cache generally involves prefetching data content dynamic cache changes time 
prefetching data selected dynamic cache suboptimal 
helper started prefetch data keeps previously selected dynamic cache 
cost function helper selection making static dynamic cache associated cost factors 
example accessing cache cause additional network load increase processing load cache dynamic cache origin server considered static cache 
soccer draft start playback time tend playback time done false false done helper static cache min cost helper cache min cost fi send request rs data min tend start available local cache rs tend done true rs sleep playback duration rs fi send request rd data tend done true tend true fi fi od fig 

helper selection algorithm requires buffer space absorb temporal distance keep complexity low reasonable define subset useful indices specify heuristic find appropriate caches 
content providers network service providers content consumers generally different conflicting optimization goals 
define possible cost function tries find balance network server helper load 
describing function specify important cost factors consider network load server helper load 
network load network distance helpers related number hops path network distance approximation network load associated data transmission host 
soccer helpers implicitly era algorithm estimate network distance sender 
classify remote helpers observed scoping level assign distance values classes table ii 
addition approximate network load receiving multicast data network distance helper multicast group defined network distance closest member group class ttl scope network distance local regional national global table ii classes network distance server helper load desirable evenly distribute load helper machines avoid overloading popular helpers 
total number incoming outgoing streams indicator current processing load server helper included advertise messages 
server helper maximum number streams serve concurrently 
number streams served far processing capacity limit serving stream little impact performance 
server helper operating close capacity serving stream degrade performance significantly 
result cost associated server helper load increase mildly low load increase sharply high load 
define follows cache local resulting stream multicast load related cost requesting stream helper server defined 
resulting stream unicast defined number streams helper maximum allowable number streams notice cost associated server helper load minimized local cache static dynamic multicast streams scenarios incur additional load helpers server 
calculating normalized cost define normalized cost function cache cost getting single segment cache 
cost factors introduced normalized cost defined network cost cost associated server helper load 
metric suitable static caches 
hand dynamic cache involves prefetching data potentially needed care taken discourage dynamic cache large temporal distance 
example helper prefetched segments user aborts transmission segments soccer draft request time dynamic buffer fig 

illustration variables normalized cost 
case normalized cost getting segments increased overhead factor general larger temporal distance larger potential waste 
illustrates estimate potential waste dynamic cache selected 
notations follows 
temporal distance segments requesting serving helper denoted 

number segments dynamic cache streaming object denoted number segments dynamic cache denoted helper received patching data starts data dynamic cache prefetched 
segments 
segments helper prefetched additional segments reaches object 
number prefetched segments segments min 

follows 

assumed 
considered cache static temporal distance 
zero overhead reduces 
time cost calculation done helper know user terminate transmission 
helper needs estimate value assuming playback lengths uniformly distributed playback average continue segments 
substituting equation estimate overhead factor 

normalized cost function helper selection algorithm 





different request length distribution give different value resulting different normalized cost function 
important note cost function minimized local static cache 
zero definition 
result local static cache available algorithm fullest 
iv 
simulation simulations conducted ns version evaluate effectiveness various techniques streaming media 
helpers configured perform proxy caching hierarchical caching dynamic cooperative caching combination techniques described sections ii iii 
state distribution ns centralized multicast feature sender trees 
option eliminates periodic flooding pruning messages dvmrp 
multicast routers ns decrement ttl value packets forwarded 
era algorithm uses ttl values ensure delivery helpers simulation described section iii 
sender fig 

mci backbone topology static hierarchy built top 
numbers link represent latency milliseconds 
parent helpers hierarchy 
perform simulation mci backbone network shown 
link capacities irrelevant consider network congestion 
helper stub router attached backbone router 
stub router simulates subnet receivers 
latency backbone router corresponding stub router ms sender attached router additional hops latency ms 
topology obtained www caida org october 
soccer draft parameter default value simulation time hours average request inter arrival time subnet minutes stream segment size mb stream chunk size mb helper static cache space gb helper dynamic cache space mb static caching dynamic caching advertisement timer seconds cost function parameter minimum static cache solution size mb cache replacement policy lfu streaming method unicast table iii default values parameters hierarchical caching construct level hierarchy level composes nodes corresponding children level nodes assigned grouping shown 
versions hierarchy 
hierarchy uses gb helper static cache space caches 
hierarchy uses gb gb level level caches respectively total cache space hierarchy 
sender contains minute long streaming media objects gb 
media stream constant bit rate udp stream kb packet size mbps playback rate 
subnet receivers follows poisson request arrival process requested object follows zipf distribution 
requests start object vary time 
bimodal distribution gives chance playing back minutes minutes minutes respectively 
table iii specifies set default simulation parameters 
applicable default values noted 
performance metrics evaluate various techniques network load sender load 
network load illustrated cumulative number bytes types packets transmitted links versus time sender load illustrated total number outgoing streams sender versus time 
general performance indicated low network sender load 
simulation results illustrate benefits various multimedia caching techniques set simulation results 
consider benefit segmenting large multimedia object varying segment size 
recall segment minimum unit data cached 
segmentation performed multimedia objects cached fashion 
shown segmentation leads highest network load server load due high contention disk space 
segment size decreases contention disk space reduced caches perform effectively leading lower load 
depending request arrival frequency continuing reduction segment size ceases provide significant additional benefit 
reducing segment size increases disk management overhead segment size set expected system load 
consider dynamic caching 
noted dynamic caching useful data streamed network caches filled caches heavily loaded 
simulate highly loaded system reducing average request inter arrival time subnet second 
simulation time reduced seconds limit execution time result data show steady state system 
static caching turned dynamic caching turned vary size dynamic cache 
points worth noting experiment 
expected network load reduces dynamic cache size increases 
second dynamic caching sender load decreased small amount extra patching streams 
benefit self organizing cooperative caching shown 
compare proposed scheme proxy caching hierarchical caching cache size configurations 
points note 
see performance hierarchical caching highly dependent cache size configuration 
hierarchy gb space helper gives similar network load proxy caching reducing sender load slightly 
hierarchy gb caches level gb caches level provides better performance 
second self organizing cooperative caching yields best performance 
reduces network load compared proxy caching hierarchical caching variant compared hierarchical caching variant 
third network load reported accounts state distribution data packets contribute total data traffic 
reality compression techniques reduce overhead 
experimented advertisement timer values seconds seconds observed network load including state distribution overhead reduces slightly advertisement messages sent frequently information helpers date 
results soccer draft simulation time sec segmentation mb segment mb segment mb segment simulation time sec caching mb dynamic cache mb dynamic cache mb dynamic cache simulation time sec proxy cache hierarchical cache hierarchical cache soccer static cache simulation time sec segmentation mb segment mb segment mb segment simulation time sec caching mb dynamic cache mb dynamic cache mb dynamic cache simulation time sec proxy cache hierarchical cache hierarchical cache soccer static cache fig 

various simulation results 
benefit segmentation 
benefit dynamic caching 
benefit selforganizing cooperative caching 
soccer draft timer seconds total data bytes state dist data bytes table iv overhead state distribution scheme server level helpers level helpers proxy max avg hier max avg hier max avg soccer max avg table server helpers load number streams summarized table iv 
note self organizing cooperative caching distribute load better schemes 
table illustrates load server level helpers level helpers various caching schemes 
helpers level exhibit similar load provide aggregate statistics level 
proxy caching server extremely overloaded 
hierarchical caching reduces server load slightly burden carried level parent caches 
level children caches effective 
contrast self organizing cooperative caching maximum average server load reduced compared proxy caching 
furthermore burden spread quite evenly caches 
result network hot spots eliminated 
related related web caching systems cern httpd harvest squid mentioned section systems designed classical web objects offer support streaming media 
furthermore static web caching architecture different self organizing cache cooperation dynamic caching defined soccer 
approaches parent caches sibling caches statically configured configuration file 
adapt changes network system loads dynamically 
body related area scalable video demand systems 
idea reduce server load grouping multiple requests time interval serving entire group single stream 
chaining patching refine basic idea define mechanisms avoid problem increased startup latency requests arrive earlier timeinterval 
proposed techniques designed video demand systems concerned wide area bandwidth constraints client buffer requirements 
fact increase clients storage requirements chaining lead increased network load wide area networks 
soccer hand aims reducing network load client storage requirements addition reducing server load 
goal achieved utilizing temporal distance network distance spatial proximity form called helper mesh 
memory requirement reduced helpers share buffers 
related done memory caching multimedia servers 
basic principle caching data different memory levels video server similarities storing data distributed caching system fundamental difference 
spatial distance different memory levels server zero 
contrast spatial distance distributed caching systems negligible considered design web cache management policies 
requirement reflected soccer helper selection cost function 
proxy caching architectures improving video delivery wide area networks 
scheme prefix caching order smooth bursty video streams playback 
prefetching storing portions stream network proxies purpose smoothing proposed 
proxy caching mechanism improving delivered quality multimedia streams 
approaches relative simple proxy architecture collaboration proxies 
complementary sense focuses design efficient collaborative proxy caching architecture 
proposed mechanisms video smoothing quality adaptation architecture additional benefits collaborative caching environment 
middleman architecture relevant soccer 
forms clusters caches cluster uses coordinator keep track state caches cluster 
centralized approach contrasts sharply soccer distributed architecture helper maintains global state information local decisions 
vi 
research step understand issues extending caching principle support streaming soccer draft dia internet 
study relevant streaming techniques generalize apply context distributed caching 
furthermore propose novel techniques including segmentation streaming objects combined dynamic static caching selforganizing cooperative caching 
integrate techniques complete solution caching streaming media internet defining self organizing cooperative caching architecture soccer 
simulations showed self organizing cooperative caching significantly reduce network server load compared proxy hierarchical caching 
turns overhead state distribution negligible proposed distribution protocol expanding ring advertisement era 
results show heavily loaded systems benefit segmentation media objects dynamic caching 
needs done evaluating assessing different cost functions helper selection better understand trade offs effectiveness complexity 
current implementation streaming helper compliance ietf standard protocols notably real time streaming protocol rtsp real time transport protocol rtp 
interoperate standard compliant streaming server client transparent way 
detailed information current implementation 
implementation evaluation production network 
internet homepage www com 
real networks internet homepage www real com 
berners lee nielsen cern httpd www org daemon status html 
bowman danzig hardy manber schwartz wessels harvest scalable customizable discovery access system tech 
rep cu cs university colorado boulder usa 
chankhunthod danzig neerdaels worrell hierarchical internet object cache proceedings usenix technical conference 
wessels icp squid cache national laboratory applied network research ircache nlanr net squid 
dan sitaram multimedia caching strategies heterogeneous application server environments multimedia tools applications vol 

hua cai sheu patching multicast technique true video demand services proceedings acm multimedia bristol england sept 
hofmann impact virtual group structure multicast performance fourth international cost workshop lisboa portugal december 
ucb lbnl vint network simulator ns 
cs berkeley edu ns 
partridge deering distance vector multicast routing protocol internet request comments november 
zipf relative determinant phonetic change reprinted harvard studies classical volume xl 
dan sitaram dynamic batching policies demand video server multimedia systems vol 
pp 
june 
aggarwal wolf yu optimal batching policies video demand storage servers proc 
international conference multimedia systems june 
sheu hua chaining generalized batching technique video demand systems procedings ieee international conference multimedia computing systems ottawa ontario canada 
dan sitaram generalized interval caching policy mixed interactive long video environments proceedings spie multimedia computing networking conference san jose ca january 
sen rexford towsley proxy prefix caching multimedia streams proceedings ieee infocom new york usa 
wang 
zhang du su network conscious approach video delivery wide area networks proxy servers proc 
ieee infocom april 
rejaie handley yu estrin proxy caching mechanisms multimedia playback streams internet proceedings th international web caching workshop san diego ca march 
acharya techniques improving multimedia communication wide area networks ph thesis cornell university dept computer science 
schulzrinne rao real time streaming protocol internet request comments april 
schulzrinne casner frederick jacobson rtp transport protocol real time applications internet request comments january 
guo hofmann paul design implementation caching system streaming media tech 
rep bl tm bell laboratories june 
