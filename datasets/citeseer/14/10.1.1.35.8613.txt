training products experts minimizing contrastive divergence tr geo rey hinton gatsby computational neuroscience unit university college london queen square london wc ar www gatsby ucl ac uk possible combine multiple probabilistic models data multiplying probability distributions renormalizing 
ecient way model high dimensional data simultaneously satis es di erent lowdimensional constraints individual expert model focus giving high probability data vectors satisfy just constraints 
data vectors satisfy constraint violate constraints ruled low probability experts 
training product experts appears dicult addition maximizing probability individual expert assigns observed data necessary experts di erent possible 
ensures product distributions small allows renormalization magnify probability data product experts model 
fortunately individual experts tractable ecient way train product experts 
way modeling complicated high dimensional data distribution large number relatively simple probabilistic models combine distributions speci ed model 
known example approach mixture gaussians simple model gaussian combination rule consists weighted arithmetic mean individual distributions 
equivalent assuming generative model data vector generated rst choosing individual generative models allowing individual model generate data vector 
combining models forming mixture attractive reasons 
easy mixtures tractable models data em gradient ascent individual models di er lot mixture better true distribution data random choice individual models 
suciently models included mixture possible approximate complicated smooth distributions arbitrarily accurately 
unfortunately mixture models inecient high dimensional spaces 
consider example manifold face images 
takes real numbers specify shape pose expression illumination face viewing conditions perceptual systems produce sharp posterior distribution dimensional manifold 
done mixture models tuned dimensional space posterior distribution sharper individual models mixture individual models broadly tuned allow cover dimensional space 
di erent way combining distributions multiply renormalize 
high dimensional distributions example approximated product onedimensional distributions 
individual distributions uni multivariate gaussians product multivariate gaussian mixtures gaussians products gaussians approximate arbitrary smooth distributions 
individual models bit complicated contain latent hidden variables multiplying distributions renormalizing powerful 
individual models kind called experts 
products experts poe advantage produce sharper distributions individual expert models 
example expert model constrain di erent subset dimensions high dimensional space product constrain dimensions 
modeling handwritten digits low resolution model generate images approximate shape digit local models ensure small image patches contain segments stroke correct ne structure 
modeling sentences expert enforce linguistic knowledge 
example expert ensure tenses agree ensure number agreement subject verb ensure strings colour adjectives follow size adjectives probable reverse 
fitting poe data appears dicult appears necessary compute derivatives parameters partition function renormalization 
shall see derivatives optimizing obvious objective function log likelihood data 
learning products experts maximizing likelihood consider individual expert models tractable compute derivative log probability data vector respect parameters expert 
combine individual expert models follows dj pm dj pm cj data vector discrete space parameters individual model pm dj probability model indexes possible vectors data space continuous data spaces sum replaced appropriate integral 
individual expert data give high probability observed data waste little probability possible rest data space 
poe data expert wastes lot probability inappropriate regions data space provided di erent experts waste probability di erent regions 
obvious way poe set observed iid data vectors follow derivative log likelihood observed vector poe 
log dj log pm dj cj log pm cj second term rhs eq 
just expected derivative log probability expert fantasy data generated poe 
assuming symbol pm simple relationship symbol lhs eq 

long pm dj positive need probability generally probability 
time series models sequence 
individual experts tractable derivative obvious diculty estimating derivative log probability data poe generating correctly distributed fantasy data 
done various ways 
discrete data possible rejection sampling expert generates data vector independently process repeated experts happen agree 
rejection sampling way understanding poe speci es probability distribution di erent causal model typically inecient 
markov chain monte carlo method uses gibbs sampling typically ecient 
gibbs sampling variable draws sample posterior distribution current states variables 
data hidden states experts updated parallel conditionally independent 
important consequence product formulation 
individual experts property components data vector conditionally independent hidden state expert hidden visible variables form bipartite graph possible update components data vector parallel hidden states experts 
gibbs sampling alternate parallel updates hidden visible variables 
get unbiased estimate gradient poe necessary markov chain converge equilibrium distribution 
unfortunately computationally feasible approach equilibrium distribution samples second serious diculty 
samples equilibrium distribution generally high variance come model distribution 
high variance derivative 
worse variance samples depends parameters model 
variation variance causes parameters regions high variance gradient zero 
understand subtle ect consider horizontal sheet tin way parts strong vertical oscillations parts motionless 
sand scattered tin accumulate motionless areas time averaged gradient zero 
learning minimizing contrastive divergence maximizing log likelihood data averaged data distribution equivalent minimizing kullback liebler divergence data distribution equilibrium distribution visible variables produced prolonged gibbs sampling generative model jjq log log jj denotes kullback leibler divergence angle brackets denote expectations distribution speci ed subscript entropy data distribution 
depend parameters model ignored optimization 
note just way writing dj 
eq 
averaged data distribution rewritten log log pm dj log pm cj simple ective alternative maximum likelihood learning eliminates computation required get samples equilibrium distribution eliminates variance masks gradient signal 
alternative approach involves optimizing di erent objective function 
just minimizing jjq minimize natural way denote data distribution imaging starting markov chain data distribution time 
di erence jjq jjq distribution step reconstructions data vectors generated full step gibbs sampling 
intuitive motivation contrastive divergence markov chain implemented gibbs sampling leave initial distribution visible variables unaltered 
running chain equilibrium comparing initial nal derivatives simply run chain full step update parameters reduce tendency chain wander away initial distribution rst step 
step closer equilibrium distribution guaranteed jjq exceeds jjq equals contrastive divergence negative 
markov chains transitions non zero probability implies contrastive divergence zero model perfect 
mathematical motivation contrastive divergence intractable expectation rhs eq 
cancels jjq jjq 
log pm dj log pm dj jjq expert chosen tractable possible compute exact values derivatives log pm dj log pm dj 
straightforward sample rst terms rhs eq 
tractable 
de nition procedure produces unbiased sample 
pick data vector distribution data 
compute expert separately posterior probability distribution latent hidden variables data vector 
pick value latent variable posterior distribution 

chosen values latent variables compute conditional distribution visible variables multiplying conditional distributions speci ed expert 

pick value visible variable conditional distribution 
values constitute reconstructed data vector third term rhs eq 
problematic compute extensive simulations see section show safely ignored small seldom opposes resultant terms 
parameters experts adjusted proportion approximate derivative contrastive divergence 
log pm dj log pm dj works practice single reconstruction data vector place full probability distribution reconstructions 
di erence derivatives data vectors reconstructions variance reconstruction procedure stochastic 
poe modelling data moderately onestep reconstructions similar data variance small 
close match data vector reconstruction reduces sampling variance way matched pairs experimental control conditions trial 
low variance feasible perform online learning data vector simulations described batch learning parameter updates summed gradients measured training set relatively large mini batches 
alternative justi cation learning algorithm eq 

high dimensional datasets data nearly lies close lower dimensional smoothly curved manifold 
poe needs nd parameters sharp ridge log probability low dimensional manifold 
starting point manifold ensuring point higher log probability typical reconstructions latent variables experts poe ensures probability distribution right local curvature provided reconstructions close data 
possible poe accidentally assign high probability distant unvisited parts data space log surface smooth height local curvature constrained data points 
possible nd eliminate points performing prolonged gibbs sampling data just way improving learning boltzmann machine learning essential part 
simple example poe data distributions factorized product lower dimensional distributions 
demonstrated gure 
experts mixture uniform single axis aligned gaussian 
tted model tight data cluster represented intersection gaussians elongated di erent axes 
conservative learning rate tting required updates parameters 
update parameters computation performed observed data vector 
data calculate posterior probability selecting gaussian uniform expert compute rst term rhs eq 

expert stochastically select gaussian uniform posterior 
compute normalized product selected gaussians gaussian sample get reconstructed vector data space 

compute negative term eq 
reconstructed vector learning population code poe ective model expert quite broadly tuned dimension precision obtained intersection large number experts 
shows happens experts type previous example tted dimensional synthetic images contain edge 
edges varied orientation position intensities side edge 
intensity pro le edge sigmoid 
expert learned variance pixel variances varied individual experts specialize small subset dimensions 
image half experts high probability picking gaussian uniform 
products chosen gaussians excellent reconstructions image 
experts top gure look edge detectors various orientations positions polarities 
experts symmetry locate edge 
di erent sets edges opposite polarities di erent positions 
dot datapoint 
data tted product experts 
ellipses show standard deviation contours gaussians expert 
experts initialized randomly located circular gaussians variance data 
unneeded experts remain vague mixing proportions gaussians remain high 
datapoints generated prolonged gibbs sampling experts tted gure 
gibbs sampling started random point range data parallel iterations annealing 
notice tted model generates data grid point missing real data 
initializing experts way initialize poe train expert separately forcing experts di er giving di erent di erently weighted training cases training di erent subsets data dimensions di erent model classes di erent experts 
expert initialized separately individual probability distributions need raised fractional power create initial poe 
separate initialization experts sensible idea simulations indicate poe far trapped poor local optima experts allowed specialize separately 
better solutions obtained simply initializing experts randomly vague distributions learning rule eq 

means dimensional gaussians product experts mixture gaussian uniform 
poe tted images contained single intensity edge 
experts ordered hand qualitatively similar experts adjacent 
poe boltzmann machines boltzmann machine learning algorithm hinton sejnowski theoretically elegant easy implement hardware slow networks interconnected hidden units variance problems described section 
smolensky introduced restricted type boltzmann machine visible layer hidden layer connections 
freund haussler realised restricted boltzmann machine rbm probability generating visible vector proportional product probabilities visible vector generated hidden units acting 
rbm poe expert hidden unit hidden unit expert speci es factorial probability distribution visible unit equally hidden unit speci es di erent factorial distribution weight connection visible unit specify log odds visible unit 
multiplying distributions visible states speci ed di erent experts achieved simply adding log odds 
exact inference tractable rbm states hidden units conditionally independent data 
learning algorithm eq 
exactly equivalent standard boltzmann learning algorithm rbm 
consider derivative log probability data respect weight ij visible unit hidden unit rst term rhs eq 
boltzmann machines products experts di erent classes probabilistic generative model intersection classes rbm log djw ij vector weights connecting hidden unit visible units expected value clamped visible units sampled posterior distribution expected value alternating gibbs sampling hidden visible units iterated get samples equilibrium distribution network hidden unit second term rhs eq 
cjw log cjw ij weights rbm expected value alternating gibbs sampling hidden visible units iterated get samples equilibrium distribution rbm 
subtracting eq 
eq 
expectations distribution data gives log ij jjq ij time required approach equilibrium high sampling variance learning dicult 
ective approximate gradient contrastive divergence 
rbm approximate gradient particularly easy compute ij jjq jjq 
expected value step reconstructions clamped visible units sampled posterior distribution reconstruction 
learning features handwritten digits real high dimensional data restricted boltzmann machine trained minimize contrastive divergence eq 
learn set probabilistic binary features model data 
test conjecture rbm hidden units visible units trained real valued images handwritten digits classes 
images training set usps cedar rom normalized highly variable style 
pixel intensities normalized lie treated probabilities eq 
modi ed probabilities place stochastic binary values data step reconstructions ij jjq jjq 
stochastically chosen binary states hidden units computing probabilities reconstructed pixels picking binary states pixels probabilities probabilities reconstructed data vector took days matlab mhz workstation perform epochs learning 
epoch weights updated times approximate gradient contrastive divergence computed mini batches size contained exemplars digit class 
learning rate set empirically quarter rate caused divergent oscillations parameters 
improve learning speed momentum method 
rst epochs parameter updates speci ed eq 
supplemented adding times previous update 
poe learned localised features binary states yielded perfect reconstructions 
image third features turned 
learned features center surround receptive elds vice versa looked pieces stroke looked gabor lters wavelets 
weights hidden units selected random shown gure 
receptive elds randomly selected subset hidden units poe trained images digits equal numbers class 
block shows learned weights connecting hidden unit pixels 
scale goes white black 
learned models handwritten digits discrimination attractive aspect poe easy compute numerator eq 
easy compute log probability data vector additive constant log log denominator eq 

unfortunately hard compute additive constant 
matter want compare probabilities di erent data vectors poe dicult evaluate model learned poe 
obvious way measure success learning sum log probabilities poe assigns test data vectors drawn distribution training data training 
alternative way evaluate learning procedure learn di erent poe di erent datasets images digit images digit 
learning test image poe poe compute log tj log log tj log respectively 
di erence log log known easy pick class test image di erence single number quite easy estimate discriminatively set validation images labels known 
shows features learned poe contains layer hidden units trained images digit 
shows previously unseen test images step reconstructions binary activities poe trained identical poe trained 
weights learned hidden units trained images digit 
scale goes white black 
note elds quite local 
local feature column row looks edge detector best understood local deformation template 
suppose active features create image di ers data having large loop top falls black part receptive eld 
turning feature top loop removed replaced line segment little lower image 
shows unnormalized log probability scores training test images model trained images digit model trained images digit 
unfortunately ocial test set usps digits violates standard assumption test data drawn distribution training data test images drawn unused portion ocial training set 
previously unseen test images scores models allow perfect discrimination 
achieve excellent separation necessary models hidden layers average scores separately trained models digit class 
digit class model units rst hidden layer second hidden layer 
model rst hidden layer second 
units rst hidden layer center row previously unseen images 
top row shows pixel probabilities image reconstructed binary activities feature detectors trained 
bottom row shows reconstruction probabilities feature detectors trained 
trained regard second hidden layer 
training rst hidden layer second hidden layer trained probabilities feature activation rst hidden layer data 
log scores models training data score model score model log scores models test data score model unnormalised log probability scores training images digits learned poe 
log probability scores previously unseen test images 
note separation classes 
shows unnormalized log probability scores images dicult classes discriminate 
discrimination perfect test images encouraging errors close decision boundary con dent misclassi cations 
dealing multiple classes di erent poe digit classes slightly obvious unnormalized scores test image discrimination 
possibility validation set train logistic regression network takes unnormalized log probabilities log scores models training data score model score model log scores models test data score model unnormalised log probability scores training images digits learned poe 
log probability scores previously unseen test images 
classes linearly separable errors close best separating line con dent errors 
poe converts probability distribution labels 
shows weights logistic regression network trained tting poe models separate digit classes 
order see second hidden layers providing useful discriminative information poe provided scores 
rst score unnormalized log probability pixel intensities poe model consisted units rst hidden layer 
second score unnormalized log probability probabilities activation rst layer hidden units poe model consisted units second hidden layer 
weights gure show second layer hidden units provides useful additional information 
presumably captures way features represented rst hidden layer correlated 
error rate compares favorably error rate simple nearest neighbor classi er training test sets best classi er elastic models digits revow williams hinton 
rejects allowed choosing appropriate threshold probability level probable class errors test images 
di erent network architectures tried digit speci poe results reported architecture best test data 
typical research learning algorithms fact test data model selection means reported results biased estimate performance genuinely unseen test images 
hinton preparation report comparative results larger mnist database www research att com yann ocr mnist careful model selection subsets training data ocial test data measure nal error rate 
approximation 
fact learning procedure eq 
gives results simulations described sections suggests safe ignore nal term rhs eq 
comes change distribution weights learned doing multinomial logistic regression training data labels outputs unnormalised log probability scores trained digit speci poe inputs 
column corresponds digit class starting digit top row biases classes 
rows weights assigned scores represent log probability pixels model learned rst hidden layer poe 
rows weights assigned scores represent log probabilities probabilities rst hidden layer model earned second hidden layer 
note weights rows smaller quite large shows scores second hidden layers provide useful additional discriminative information 
get idea relative magnitude term ignored extensive simulations performed restricted boltzmann machines small numbers visible hidden units 
performing computations exponential number hidden units exponential number visible units possible compute exact values possible measure happens jjq jjq approximation eq 
update weights amount large compared numerical precision machine small compared curvature contrastive divergence 
rbm simulations random training data random weights 
biases visible hidden units 
main result summarized follows individual weight rhs eq 
summed training cases occasionally di ers sign lhs 
networks containing units layer certain parallel update weights rhs eq 
improve contrastive divergence 
words average training data vector parameter updates rhs certain positive cosine true gradient de ned lhs 
histogram improvements contrastive divergence eq 
perform parallel weight update networks 
networks contained visible hidden units weights chosen gaussian distribution mean zero standard deviation 
smaller weights larger networks approximation eq 
better 
shows learning procedure improve log likelihood training data strong tendency 
note networks histogram improvements contrastive divergence histogram improvements data log likelihood histogram improvements contrastive divergence result eq 
perform update weights networks 
expected values rhs eq 
computed exactly 
networks visible hidden units 
initial weights randomly chosen gaussian mean standard deviation 
training data chosen random 
improvements log likelihood data networks chosen exactly way gure 
note log likelihood decreased cases 
changes log likelihood changes jjq sign reversal 
histogram 
compares contributions gradient contrastive divergence rhs eq 
term ignored 
vector weight updates eq 
contrastive divergence worse dots gure diagonal line clear networks approximation eq 
quite safe 
intuitively expect lie parameters changed move closer changes move away previous position ignored changes cause increase jjq improvement contrastive divergence 
earlier version learning rule eq 
interpreted approximate optimization contrastive log likelihood hlog log unfortunately contrastive log likelihood achieve maximum value simply making possible vectors data space equally probable 
contrastive divergence di ers contrastive log likelihood including entropies distributions see eq 
high entropy rules solution possible data vectors equiprobable 
types expert binary stochastic pixels unreasonable modeling preprocessed images handwritten digits ink background represented 
real images typically high mutual information real valued intensity pixel real valued intensities neighbors 
captured models binary stochastic pixels binary pixel bit mutual information 
possible multinomial pixels discrete values 
modeled vs unmodeled effects scatterplot shows relative magnitudes modeled unmodeled ects parallel weight update contrastive divergence 
networks gure visible hidden units weights drawn zero mean gaussian standard deviation 
horizontal axis shows jjq old old jjq old jjq new old jjq new old new denote distributions weight update 
di ers improvement contrastive divergence ignores fact changing weights changes distribution step reconstructions 
ignored term old jjq new new jjq new plotted vertical axis 
points diagonal line correspond cases weight updates caused decrease contrastive divergence 
note unmodeled ects helpful con ict modeled ects 
clumsy solution images fails capture continuity dimensionality pixel intensity may useful types data 
better approach imagine replicating visible unit pixel corresponds set binary visible units identical weights hidden units 
number active units set approximate real valued intensity 
reconstruction number active units binomially distributed replicas weights single probability controls binomial distribution needs computed 
trick allow replicated hidden units approximate real values binomially distributed integer states 
set replicated units viewed cheap approximation units weights di er viewed stationary approximation behaviour single unit time case number active replicas ring rate 
alternative replicating hidden units experts consist mixture uniform distribution factor analyser just factor 
expert binary latent variable speci es uniform factor analyser real valued latent variable speci es value factor 
factor analyser sets parameters vector factor loadings specify direction factor image space vector means image space vector variances image space experts type explored context directed acyclic graphs hinton sallans ghahramani better product experts 
alternative large number relatively simple experts expert sets parameters exactly equivalent parameters expert introduced section expert considered mixture uniform factor analyser factors 
complicated possible whilst retaining ability compute exact derivative log likelihood data parameters expert 
modeling static images example expert mixture axis aligned gaussians 
experts focus region image high variances pixels outside region 
long regions modeled di erent experts overlap possible avoid block boundary artifacts 
products hidden markov models hidden markov models hmm great practical value modeling sequences discrete symbols sequences real valued vectors ecient algorithm updating parameters hmm improve log likelihood set observed sequences 
hmm quite limited generative power way portion string generated time constrain portion string generated time discrete hidden state generator time rst part string average bits mutual information rest string hmm hidden states convey mutual information choice hidden state 
exponential ineciency overcome product hmm generator 
generation hmm gets pick hidden state time mutual information past linear number hmm 
exponentially ecient small hmm big 
apply standard forward backward algorithm product hmm necessary take cross product state spaces throws away exponential win 
products hmm practical signi cance necessary nd ecient way train 
andrew brown brown hinton preparation shown toy example involving product hmm learning algorithm eq 
works 
forward backward algorithm get gradient log likelihood observed reconstructed sequence parameters individual expert 
step reconstruction sequence generated follows 
observed sequence forward backward algorithm expert separately calculate posterior probability distribution paths hidden states 

expert stochastically select hidden path posterior observed sequence 

time step select output symbol output vector product output distributions speci ed selected hidden state hmm 
realistic products hmm trained successfully minimizing contrastive divergence far better single hmm di erent kinds sequential data 
consider example hmm shown gure 
expert concisely captures non local regularity 
single hmm model regularities strings english words capture regularity eciently ord devote entire memory capacity remembering word shut occurred string 
discussion previous attempts learn representations adjusting parameters cancel ects brief iteration recurrent network hinton mcclelland reilly seung formulated stochastic generative model appropriate objective function 
shut hidden markov model 
rst third nodes output distributions uniform words 
rst node high transition probability strings english words low probability expert 
strings contain word shut followed directly indirectly word higher probability expert 
minimizing contrastive divergence unexpected similarity learning algorithm proposed winston 
winston program compared arches blocks near misses supplied teacher di erences representations correct incorrect arches decide aspects representation relevant 
stochastic generative model dispense teacher di erences real data near misses generated model drive learning signi cant features 
logarithmic opinion pools idea combining opinions multiple di erent expert models weighted average log probability domain far new genest heskes research focussed nd best weights combining experts learned programmed separately berger della pietra della pietra training experts cooperatively 
geometric mean set probability distributions attractive property kullback liebler divergence true distribution smaller average kullback liebler divergences individual distributions jj mq wm wmp jjq wm non negative sum mq wm 
individual models identical 
di erence sides log clear bene combining experts comes fact log small disagreeing unobserved data 
tempting augment poe giving expert additional adaptive parameter wm scales log probabilities 
inference dicult teh personal communication 
consider example expert wm 
equivalent having copies expert latent states tied tying ects inference 
easier just wm allow poe learning algorithm determine appropriate sharpness expert 
comparison directed acyclic graphical models inference poe trivial experts individually tractable product formulation ensures hidden states di erent experts conditionally independent data 
relevant models biological perceptual systems able inference rapidly 
alternative approaches directed acyclic graphical models su er explaining away phenomenon 
graphical models densely connected exact inference intractable necessary resort clever slow iterative techniques approximate inference saul jordan crude approximations ignore explaining away inference rely learning algorithm nd representations inference technique damaging hinton dayan frey neal 
unfortunately ease inference poe balanced diculty generating fantasy data model 
done trivially ancestral pass directed acyclic graphical model requires iterative procedure gibbs sampling poe 
eq 
learning diculty generating samples model major problem 
addition ease inference results conditional independence experts data poe subtle advantage generative models rst choosing values latent variables generating data vector latent values 
model single hidden layer latent variables independent prior distributions strong tendency posterior values latent variables approximately marginally independent model tted data reason little success attempts learn generative models hidden layer time greedy bottom way 
poe experts independent priors latent variables di erent experts marginally dependent high mutual information fantasy data generated poe 
rst hidden layer learned greedily may lots statistical structure latent variables second hidden layer capture 
attractive property set orthogonal basis functions possible compute coecient basis function separately worrying coecients basis functions 
poe retains attractive property whilst allowing non orthogonal experts non linear generative model 
poe provide ecient instantiation old psychological idea analysis synthesis 
idea worked properly generative models selected analysis easy 
poe dicult generate data generative model model easy compute data vector generated seen relatively easy learn parameters generative model 
research funded gatsby foundation 
zoubin ghahramani david mackay david lowe yee teh guy andy brown members gatsby unit helpful discussions peter dayan improving manuscript disproving expensive speculations 
easy understand coding perspective data communicated rst specifying states latent variables independent prior specifying data latent states 
latent states marginally independent coding scheme inecient pressure coding eciency creates pressure independence 
berger della pietra della pietra 
maximum entropy approach natural language processing 
computational linguistics 
freund haussler 
unsupervised learning distributions binary vectors layer networks 
advances neural information processing systems 
moody hanson lippmann eds morgan kaufmann san mateo ca 
genest 
combining probability distributions critique annotated bibliography 
statistical science 
heskes 
bias variance decompositions likelihood estimators 
neural computation 
hinton dayan frey neal 
wake sleep algorithm self organizing neural networks 
science 
hinton sallans ghahramani 
hierarchical communities experts 
jordan ed 
learning graphical models 
kluwer academic press 
hinton mcclelland 
learning representations recirculation 
anderson editor neural information processing systems american institute physics new york 
hinton sejnowski 
learning relearning boltzmann machines 
rumelhart mcclelland editors parallel distributed processing explorations microstructure cognition 
volume foundations mit press reilly 
biologically plausible error driven learning local activation di erences generalized recirculation algorithm 
neural computation bf 
revow williams hinton 
generative models handwritten digit recognition 
ieee transactions pattern analysis machine intelligence 
saul jaakkola jordan 
mean eld theory sigmoid belief networks 
journal arti cial intelligence research 
seung 
learning continuous attractors recurrent net 
advances neural information processing systems 
jordan kearns solla eds 
mit press cambridge mass smolensky 
information processing dynamical systems foundations harmony theory 
rumelhart mcclelland editors parallel distributed processing explorations microstructure cognition 
volume foundations mit press winston 
learning structural descriptions examples 
winston 
ed 
psychology computer vision mcgraw hill book new york 

