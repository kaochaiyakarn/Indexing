lasso dual michael osborne brett june proposed tibshirani lasso absolute shrinkage selection operator estimates vector regression coe cients minimising residual sum squares subject constraint norm coe cient vector 
lasso estimator typically zero elements shares characteristics shrinkage estimation variable selection 
treat lasso convex programming problem derive dual 
consideration primal dual problems leads important new insights characteristics lasso estimator improved method estimating covariance matrix 
results develop cient algorithm computing lasso estimates usable cases number regressors exceeds number observations 
key words phrases 
convex programming dual problem partial squares quadratic programming penalised regression regression shrinkage subset selection variable selection 
centre mathematics applications australian national university canberra act australia department statistics university florida gainesville fl usa centre mathematics applications australian national university canberra act australia department statistics university adelaide adelaide sa australia centre mathematics applications cooperative research centre advanced computational systems australian national university canberra act australia 
author wishes acknowledge part carried cooperative research centre advanced computational systems established australian government cooperative research centres program 
home lasso rcs main tex revision consider usual linear regression setting data 

ij regressors response ith observation 
situation ordinary squares regression finds linear combination ij minimises residual sum squares 
large regressor variables highly correlated variances squares coe cient estimates may unacceptably high 
standard methods addressing di culty include ridge regression particularly cases parsimonious model desired subset selection 
alternative standard ridge regression subset selection techniques tibshirani proposed absolute shrinkage selection operator lasso minimises residual sum squares constraint norm coe cient vector 
lasso estimator solves optimisation problem minimise ij subject 
greater equal norm ordinary squares estimator estimator course unchanged lasso 
smaller values lasso shrinks estimated coe cient vector origin sense typically setting coe cients equal zero 
lasso combines characteristics ridge regression subset selection promises useful tool variable selection 
optimisation problem easily stated solving numerically trivial exercise 
algorithm proposed tibshirani adequate moderate values cient possible 
course ect ine cient algorithm greatly magnified techniques cross validation bootstrap choose appropriate value tibshirani estimate standard errors 
tibshirani algorithm particularly ine cient large usable severe practical limitation problems number variables larger order number observations occur frequently areas chemometrics partial squares brown employed 
fact interest lasso initially motivated problem knot selection regression splines see osborne formulated variable selection problem treat convex programming problem derive dual optimisation problem 
considering simultaneously primal problem dual develop highly cient algorithm calculating lasso estimator applicable case approach yields new insight lasso providing exact characterisation home lasso rcs main tex revision solution 
particular case characterisation suggests estimator covariance matrix lasso estimator di erent proposed tibshirani 
shall concentrate optimisation problem 
closely related optimisation problem minimise ij 
problems equivalent exists problems share solution vice versa 
optimisation problems usually referred constrained regression problems called penalised regression 
chen 
propose penalised regression context wavelet regression 
primal dual log barrier interior point algorithm solve 

propose algorithm block coordinate relaxation techniques 
possible generalisation change constraint 
investigated fu see frank friedman 
fu proposes alternative algorithm solve 
algorithm applicable starts unconstrained squares solution 
rest structured follows 
section derive dual discuss relationship primal dual problems 
section discusses theoretical properties lasso estimator including existence uniqueness solutions number non zero entries estimator 
section discuss estimation standard errors propose new approach problem duality results section 
technical details concerning standard error estimation appendix 
new cient algorithm calculate lasso estimator developed section 
section algorithm applied example tibshirani various standard error estimates compared context example 
convex duality lasso fix notation 
denote vector observed responses 
xm matrix vector 
jth column assume maximal rank 
denote null space solution unconstrained squares problem 
course home lasso rcs main tex revision unique 
dimension unique holds 
case may define min denotes norm note unique case may attain lasso equivalent ordinary squares assume sequel optimisation problem rewritten minimise subject 
vector residuals corresponding implicitly function treated fixed discussion 
continuous region feasible vectors compact solution guaranteed exist 
critical values occur outside feasible region solution lie boundary concave function region feasible values defined convex convex function clear solution set convex 
assumption ensures strictly convex case solution unique 
facts summarised lemma 
theorem existence uniqueness 
hold unique solution exists 
solution exists solution 
solutions solution 
treating convex programming problem nash lagrangian 
home lasso rcs main tex revision define sup 
minimising equivalent solving 
convex programming theory equivalent problem minimising called primal problem called primal objective function 
dual objective function defined inf dual problem maximise 
fix convex function 
minimum minimises dimensional null vector element erential osborne 
current problem erential osborne 
form 
minimises value form described denotes residual vector 
form implies follows minimises alternatively case theorem follows calculated expressions find home lasso rcs main tex revision define dual function written 
sequel shall generic notation statements hold 
note general 
tibshirani notes written quadratic programming problem 
dual function originally authors deriving dual problem quadratic programming problem 
arguments solution fulfil 
leads interesting observation concerning choice smoothing parameter penalised regression versus penalised regression ridge regression 
penalised regression typically observes finite entries non zero 
way contrast penalised regression see soon chosen solution 
see note choose holds required form components absolute value equal 
penalised regression smoothing parameter chosen adaptively cross validation search optimal parameter conveniently restricted interval zero existence finite solution dual problem guaranteed theorem 
solution 
maximum taken necessarily feasible point primal problem guaranteed fact shown global exists norm global strictly greater cases cient algorithms solving optimisation problems developed relationship primal dual problems 
case solving 
give results concerning relationship primal problem dual 
result follows directly definitions known weak duality see nash chapter 
home lasso rcs main tex revision theorem 
weak duality solution solution dual problem satisfies 
direct consequence theorem solutions 
desirable equality hold solutions allow dual gap test solution 
discussion nash chapter indicates true point satisfies saddle point condition 
theorem shows problem points exist 
lagrange multiplier corresponding defined 
theorem 
strong duality solution lagrange multiplier corresponding solution dual problem 
follows optimal primal dual function values equal 
proof 
osborne define perturbation function inf 
lemma osborne convex function ective domain dom 
lies interior dom stable osborne 
theorem follows theorem iv osborne 
dual function concave extreme maximum maxima take value 
consequence theorems solution dual problem primal feasible solution primal problem 
characteristics solutions uniqueness definitions useful proving properties solutions solution define collection vectors form equals 
entries equal zero 
set vectors denote matrix ith row note 
home lasso rcs main tex revision lies intersection hyperplanes distance origin normal vector 
suppose solution follows theorem solution constant solutions standard argument shows 
condition implies 
see note vectors 
contradiction 
geometrically argument reflects fact moving new solution stay hyperplanes lies move hyperplanes move direction define hyperplanes leave 
take convex cone defined discussion summarised theorem 
theorem 
unique solution dimensional null vector 
note condition theorem trivially fulfilled assume full rank 
theorem gives necessary su cient condition existence unique solution condition di cult verify practice 
results section enable develop useful condition 
suppose solutions 
arguing means vector constant solutions 

set indices 
solution satisfy follows solution 
solutions 
leads theorem 
theorem 
solution 
denote matrix jth column th column 
denote corresponding submatrix matrix formed vectors 
unique solution exists satisfying 
home lasso rcs main tex revision theorem easy verify solution unique assume example nn submatrix full rank 
calculating solution algorithm proposed section determine 
number elements equal solution unique case holds 
check non trivial solution 
number non zero coe cients unique solution exists vector 
interesting directions 
move direction reach hyperplane defines dimensional sphere radius fewer non zero components 
long exists move solution fewer non zero entries results proven follows full rank vertex dimensional sphere radius iterative process latest vertex reached 
hand number non zero entries increases initially move 
move direction guarantee contain vectors 
discussion motivates definition 
definition 
solution called regular 
note unique solution regular solution sense definition 
discussion preceding definition clear find regular solution multiple solutions 
obtain bound number non zero elements regular solution may define polar cone 
polar cone vectors generators 
rockafellar ch 

note dimension convex cone containing defined dimension smallest subspace containing 
home lasso rcs main tex revision lemma 
results hold rank contains linearly independent vectors 
furthermore irreducible row positive linear combination rows origin positive linear combination rows 
dimension 
dimension proof 
set 

take vectors 
elements 

elements 
easy verify vectors linearly independent vector written linear combination vectors 
proves rank 
assume exist indices 
equal 
origin 
exists th components equal equal 
equal 
case leads directly contradiction 
case components 
absolute value clear ki 

contradicts fact distinct 
irreducible 
smallest subspace contains rockafellar 
dimension number linearly independent vectors 
proof follows meyer fact irreducible 
dimension exists vector 
contradicting fact origin positive linear combination rows theorem 
regular solution non zero entries 
proof 
regular solution zero non zero components 
lemma dimensional subspace definition 
course assumed full rank dimension consider dimensional space entries 
clear 
su ciently small solution norm home lasso rcs main tex revision contradicting theorem 
span follows equivalently standard errors lasso estimates solution satisfy 
section showed depend particular solution true 
combining fact follows rank matrix 
denote identity matrix write rr shows rank equal rank equal rank covariance matrix estimates may approximated var estimate error variance 
contrasted suggestion tibshirani approximate closed form estimate may derived writing penalty 
lasso estimate may approximate solution ridge regression form diagonal matrix diagonal elements denotes generalized inverse chosen covariance matrix estimates may approximated estimate error variance 
quoted section slightly altered fix typographical error agree closely notation 
claim formula yield appropriate estimate covariance matrix support claim appendix show motivated sequences smooth approximations sequence approximations leading breaks original problem approached 
tibshirani notes gives estimated variance predictors 
view inappropriate note yields positive standard error coe cient estimates 
distribution individual lasso coe cient home lasso rcs main tex revision estimates typically condensation probability zero may far normally distributed 
suggests summarising uncertainty standard errors may appropriate topic deserves investigation 
section issues examined context reanalysis prostate cancer data tibshirani 
algorithms section derive algorithms calculating solutions 
algorithm duality theory compute lasso estimator setting 
second algorithm simple orthogonal design case 
implementations algorithms available request 
general case iterative algorithm propose solve local linearisation current value 
step ith component non zero index set updated various stages algorithm 
represent permutation matrix collects non zero components components write sign entry corresponding entry positive 
step algorithm feasible ensured algorithm initial set chosen appropriately 
obtain iterate current solve amounts local linearisation current minimise subject constraint active karush kuhn tucker conditions nash problem written solution max home lasso rcs main tex revision sign say sign feasible 
sign feasible proceed follows clark osborne 

move new zero component direction find smallest corresponding set 
possibilities 
set recompute solving new computed descent direction compatible revised proceed stage algorithm 
update deleting resetting accordingly feasible recompute revised problem 

iterate sign feasible obtained 
sign feasible obtained test optimality verifying 
calculate construction solution 
proceed follows 

determine violated condition find maximal absolute value 

update adding update appending zero element appending sign 
set solve iterate 
justification calculations case sign feasible 
note current optimal restricted problem portion algorithm skipped 
descent direction objective reduced step 
cycling procedure converge 
procedure finite finitely possible configurations 
convergence process contradicted final sign feasible 
justification calculations case sign feasible 
optimal augmented vector suboptimal augmented problem updated adding augmented solution say hs augmented problem descent direction augmented problem long primal feasibility maintained 
fact implies algorithm converge requires choose properly 
home lasso rcs main tex revision justify choice note implies hs descent direction augmented problem 
hand feasibility requires 
multiplying adding yields see choosing sign yields sign implies linearised constraint augmented problem equivalent norm constraint small displacements direction hs equivalent hs small 
turn insures primal feasibility maintained algorithm 
solving 
solution readily ciently computed stage algorithm maintaining qr factorisation factorisation easily updated changed 
starting iteration 
iteration started component add determined part algorithm 
starting problem advantages emphasises building optimal starting small base pruning large permits computation proceed time building mentioned 
lasso estimate calculated values say solve starting 
values take starting point solution situation occurs chosen say generalised cross validation tibshirani 
advantages tibshirani algorithms 
suggests primary advantages current algorithm proposed tibshirani 
algorithm starts small base build optimal solution tibshirani algorithm starts solution unconstrained problem 
tibshirani approach infeasible large larger ine cient small medium sized values home lasso rcs main tex revision lasso coe cient estimates typically equal zero 
similarly lasso estimate calculated ordered values algorithm allows solution starting point calculating solution connexion penalised regression 
algorithm proposed fu solve 
algorithms may larger discussed specific context wavelet regression chen 


principle algorithm solve 
case necessary find value corresponding lagrange multiplier equal smoothing parameter 
done loop performing grid search newton raphson algorithm 
note solution value convenient starting point algorithm bound changed grid search implemented ciently 
connexion subset selection techniques 
way algorithm calculates solution illustrates interesting connexions lasso known subset selection techniques 
see assume regressor variables centred rescaled sample mean zero sample variance 
standardisation ith entry proportional sample correlation ith regressor variable vector residuals 
stage index added set index variable maximal correlation residual vector constrained subproblem 
dissimilar forward variable selection miller chapter 
index added solve new subproblem solving new problem may happen indices variables deleted 
backward deletion practically built lasso argue behaves stepwise regression miller chapter 
lasso driven overarching optimality criterion ad hoc stepwise regression procedure 
osborne 
develop homotopy method constraint homotopy parameter obtain complete characterisation solutions approach gives insight relationship lasso stepwise regression 
orthogonal design case orthogonal design case diagonal matrix assume loss generality course tibshirani notes case solution sign max 
solution unconstrained problem chosen relationship lasso estimator calculated easily give simple home lasso rcs main tex revision algorithm purpose 
letting positive part note ordered values 
max 
clearly 
max easily computed 
example section prostate cancer data tibshirani 
data come study 
examined correlation level prostate specific antigen number clinical measures men receive radical 
regressor variables log cancer volume log prostate weight age log benign amount seminal invasion svi log penetration lcp gleason score gleason percentage gleason scores pgg 
tibshirani standardised regressor sample mean zero sample variance 
standardisation allows incorporate intercept term parameter part penalty 
fitting model minimise 
response variable log prostate specific antigen built standardised regressors mentioned 
due standardisation immediately see tibshirani home lasso rcs main tex revision estimated standard errors estimated tibshirani moore penrose predictor coe cients age svi lcp gleason pgg table coe cient standard error estimates prostate cancer example calculating may standardise transform problem 
tibshirani presents results case result unconstrained squares fit chosen generalised cross validation 
algorithm described section fit obtaining parameter estimates table 
estimates identical tibshirani reproduced software available tibshirani statlib archive carnegie mellon university 
error tibshirani routine evaluates gcv 
routine centres regressors neglects centre response variable 
residual sum squares gcv function wrongly calculated 
error corrected optimal gcv 
find optimal values gcv function evaluated grid values evenly spaced 
di cult reproduce standard error estimates tibshirani 
examining tibshirani programs firstly searches interval data chosen 
secondly calculating diag zero entries somewhat arbitrarily set facts able reproduce standard errors table column labelled 
standard errors reproduced tibshirani software standard error table tibshirani 
calculating indicating surprisingly correct value lagrange multiplier 
correct value enforcing constraint 
value tibshirani method estimating covariance matrix yields values corresponding column table 
home lasso rcs main tex revision corrected standard error estimates produced quite small course zero coe cients estimated zero 
comparison standard errors calculated non zero considerably larger constrained non zero coe cient estimates 
interesting note matrix chosen tibshirani generalised inverse matrix consider example moore penrose inverse see rao 
moore penrose inverse places zero diagonal elements correspond parameters estimated zero 
estimated standard errors obtains moore penrose inverse column table 
case surprisingly estimated standard errors parameter estimated zero yielded moore penrose inverse similar obtained 
estimated standard errors non zero parameter smaller 
estimates similar obtained tibshirani results discussion section appendix believe preferred way estimating standard errors lasso estimates 
authors grateful iain johnstone permission prostate cancer data fu providing 
grateful bill number helpful comments discussions 
associate editor referees comments significant improvements presentation 
smooth approximations lasso section concentrate case show optimisation problem approximated smoothly 
done approximating function smooth functions 
changing manifold unconstrained ordinary leastsquares estimator projected sphere smooth di erentiable manifold 
calculations assume enforced 
note case probability arbitrarily close large distribution clearly mixture distribution 
analysis motivates covariance matrices discussed section 
shown matrices stem di erent approximations projections di erent smooth manifolds 
approximation obtained follows 
consider family densities form normalisation constant du max 
set dx dt home lasso rcs main tex revision primitive satisfying smoothly approximated small smoothness approximation controlled obtain huber functions 
approximation consider minimise subject 
known approximation absolute function see koch 
leads second smooth approximation constraint 
ii follows quantity associated smooth optimisation problems indicated subscript shall additional subscripts ii respectively distinction approximations necessary 
noted value values quantities depend approximation 
function concave region minimise convex 
assume full rank minimising strictly convex function convex region unique solution smooth problem exist 
easy show solution 
kuhn tucker conditions smooth problem ith component ii 
note form similar components ii strictly 
fact follows follows quantities converge counterparts 
look matrix equation induced approximation picture emerges 
approximation calculations similar section home lasso rcs main tex revision obtain 
may approximate variance matrix see gallant chapter formulae similar converge quantities equations tends zero 
second approximation obtain ii ii ii diag ii resulting approximation covariance matrix ii ii ii form proposed tibshirani 
note problem arises approximation tends zero solution zero entry 
matrix ii non singular case limit singular elements diagonal ii correspond zero entries tending infinity 
sense approximation breaks 
regular behaviour approximation suggests preference estimating covariance matrix brown 

measurement regression calibration clarendon press oxford 
chen donoho saunders 

atomic decomposition basis pursuit siam journal scientific computing 
url www stat stanford edu donoho reports ps clark osborne 

linear restricted interval squares problems ima journal numerical analysis 
frank friedman 

statistical view chemometrics regression tools discussion technometrics 
fu 

penalized regression bridge versus lasso journal computational graphical statistics 
home lasso rcs main tex revision gallant 

nonlinear statistical models john wiley sons new york 

eds 
statistical modelling latent variables elsevier north holland new york amsterdam 
koch 

asymptotic performance median smoothers image analysis nonparametric regression annals statistics 
meyer 

extension mixed primal dual bases algorithm case constraints dimensions 
unpublished manuscript 
miller 

subset selection regression vol 
monographs statistics applied probability chapman hall london 
nash 

linear nonlinear programming mcgraw hill new york 
osborne 

finite algorithms optimization data analysis wiley series probability mathematical statistics john wiley sons chichester 
osborne 

knot selection regression splines lasso ed dimension reduction computational complexity information vol 
computing science statistics interface foundation north america fairfax station va pp 

url www stats adelaide edu au people interface ps gz osborne 

new approach variable selection squares problems ima journal numerical analysis 
consideration 
rao 

linear statistical inference applications edn john wiley sons new york 
rockafellar 

convex analysis vol 
princeton mathematical series princeton university press princeton new jersey 
bruce tseng 

block coordinate relaxation methods nonparametric signal denoising wavelet dictionaries journal computational graphical statistics 
appear 
johnstone yang 

prostate specific antigen diagnosis treatment prostate ii 
radical treated patients journal 
tibshirani 

regression shrinkage selection lasso journal royal statistical society series 

