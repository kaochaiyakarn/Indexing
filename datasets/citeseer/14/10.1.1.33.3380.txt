statistical language modeling cmu cambridge toolkit philip clarkson prc eng cam ac uk cambridge university engineering department street cambridge cb pz uk 
cmu statistical language modeling toolkit released order facilitate construction testing bigram trigram language models 
currently academic government industrial laboratories countries 
presents new version toolkit 
outline conventional language modeling technology implemented toolkit describe extra ciency functionality new toolkit provides compared previous software task 
give example toolkit constructing testing simple language model 

language modeling continues fruitful research topic standard language modeling techniques large vocabulary recognition systems changed little past years 
improvements traditional models normally come combining new model conventional trigram model 
cmu statistical language modeling cmu slm toolkit set unix software tools facilitating construction testing conventional bigram trigram language models 
version released currently academic government industrial laboratories countries 
release version ground shifted little language modeling 
larger corpora available powerful computers available process 
interest grown moving trigram language models gram gram models 
furthermore version tolerable dealing small corpora real problem dealing hundreds millions words 
version toolkit developed order address shortcomings 
give brief outline conventional language modeling technology implemented version toolkit 
describe theoretical implementational improvements version provide example toolkit 

conventional language modeling theory 
smoothing conventional language models bigram trigram model 
largest corpora available ronald rosenfeld roni cmu edu school computer science carnegie mellon university forbes avenue pittsburgh pa usa 
contain fraction possible trigrams necessary smooth data order provide better estimates infrequent unseen events maximum likelihood estimate probability event occurred times possible sparse sample maximum likelihood estimate biased high observed events biased low unobserved ones 
correct bias redistribute probability mass observed events unseen ones discounting counts coe cient dr modi ed count rdr revised probability estimate remaining probability mass assigned unseen events 
gram wn observed training data wn wn estimated lower order model wn wn process known backing 
frequently backing discounting combined scheme devised katz combines turing discounting backing approach data smoothing implemented version toolkit 

cuto order reduce size language model infrequent grams removed model 
counts grams discarded referred cuto table shows bigram trigram cuto ect size perplexity trigram language model 

context cues language data viewed toolkit stream words interspersed context cues 
markers indicate events paragraph article boundaries 
provide useful information language model may ect prediction predicted model 

vocabulary types toolkit supports types vocabulary handle vocabulary oov words di erent ways 
closed vocabulary model provision oovs 
words appear training test data cause error 
type model command control environment context trigram language model example event refers unigram bigram trigram 
cuto size bigram trigram model perplexity mb mb mb mb mb mb mb mb table 
ect cuto size perplexity trigram language model 
results generated training test sets broadcast news evaluation 
vocabulary restricted number commands system understands guarantee oovs occur training test data 
open vocabulary model allows oovs occur vocabulary words mapped symbol 
types open vocabulary model implemented toolkit 
rst type treats oov symbol way word vocabulary 
second model cover situations oovs occurred training data wish allow situation occur test data 
situation occur example limited amount training data able choose vocabulary provides coverage training set 
case arbitrary proportion probability mass reserved oov words 

enhancements version 
multiple discounting methods ways de ne discount coe cient dr equation 

turing discounting de ne nr number events occur times turing discounting scheme implemented version de nes dr rnr nk typically dr higher counts belief events occurring times estimated maximum likelihood 
approach disadvantages 
example require dr puts constraints relative values nk 
general constraints satis ed naturally occurring data may data way example boosting counts subset grams 
turing discounting remains default version discounting schemes implemented su er problem produce superior results 


linear discounting linear discounting quantity proportional count subtracted count 
set dr case select linear discounting assigns probability unseen events turing discounting dr number words training data 

absolute discounting absolute discounting involves subtracting constant counts 
dr shown setting approxi mately optimal terms maximising log likelihood function leaving 

witten bell discounting discounting scheme refer witten bell discounting referred type rst applied language modeling 
discounting ratio dependent event count number distinct events followed particular context 
bigram number distinct bigrams form model 
dr 
grams arbitrary size new version toolkit longer limited construction testing bigram trigram models supports gram models value 
flexible handling context cues version symbols art xed context cues 
version adopts exible approach allowing user specify subset vocabulary context cues 

cient memory usage data structures store grams version compact version result language model construction memory intensive task 
example trigram language model version required bytes bigram bytes trigram 
version requires bytes bigram bytes trigram 

interactive language model evaluation tool evaluate language models run interactively 
language model read commands read standard input 
language model tested cient fashion version language model read time new test performed 

testing arpa format language models version possible perform tests language models written toolkit binary format 
version allows user evaluate performance language models standard arpa format may supplied third party need generated toolkit 

forced back tool evaluating language models allows user specify set forced back parameters 
may items vocabulary especially context cues unknown symbol back time 
example see word string context cue indicating sentence boundary predicting probability ofb full context may wish disregard information sentence boundary 
want back bigram distribution inclusive forced backo unigram distribution exclusive forced back 
version supports types forced back arbitrary vocabulary items 

linear interpolation models order facilitate combination language models toolkit includes program appendix calculating maximum likelihood weights set models expectation maximisation em algorithm 
models described output common set items probability streams program receives input 
may generated models created toolkit software 
example may generated cache probabilities interpolated probabilities trigram model created toolkit 

cient pre processing tools tools version simple lters convert data stream slightly di erent format 
tools relied heavily unix tools sort awk result great deal disk incurred large les processed 
version sacri ces certain amount simplicity modularity predecessor favour cient approach 
tools better grab large amount ram sorting reading writing disk little possible 

example usage section provide example toolkit de ning language model vocabulary generating language model large corpora training text nally evaluating language model performance held test text 

creating vocabulary preliminary stage constructing language model de ne model vocabulary 
tool text outputs number occurrences word input text vocab turns list vocabulary le case containing common words 
cat training text text vocab top training vocab context cues le generated echo training ccs 
constructing language model rst step turn training text list id grams grams word mapped integer id zero oovs 
cat training text text vocab training vocab buffer temp usr tmp training id gram text text vocab vocab test text text perplexity 
toolkit id gram lm language model option indicates building gram model buffer option tells program ram grab mb temp option allows specify store temporary les 
second step convert id gram stream binary language model le 
lm training id gram vocab training vocab binary training gram cutoffs witten bell context training ccs speci ed witten bell discounting cuto bigrams trigrams grams 
stage specify vocabulary type various parameters 
write language model standard arpa format 

evaluating language model tool interactive evaluation language model 
binary training gram perplexity text test text computing perplexity language model respect text test text perplexity entropy bits computation words 
number grams hit number grams hit number grams hit number grams hit oovs context cues removed calculation 

development development toolkit may include 
support class grams 
currently classes supported implicitly mapping training set set classes building standard model class vocabulary 
results model form cn cn combined estimate wi ci 
variants class grams wn cn currently supported 
general model allow word position distinct class vocabulary 

porting run microsoft windows toolkit accessible larger community users 

smoothing strategy described backing distributions optimized back model give improvement conventional methods 
technique implemented version toolkit 

exible backing idea merely language model unable provide reliable estimate event probability general language model 
need take form merely backing gram gram 
want back speci task dependent language model language model trained general text word gram model class gram model 

new toolkit construction testing statistical language models 
toolkit represents signi cant improvement existing publicly available software terms functionality computational ciency 
described techniques standard current language modeling theory implemented toolkit 
details toolkit including details download latest version svr www eng cam ac uk prc toolkit html philip clarkson supported epsrc advanced studentship 
bahl brown de souza mercer 
tree statistical language model natural language speech recognition 
ieee transactions pattern analysis machine intelligence 
bahl jelinek mercer 
maximum likelihood approach continuous speech recognition 
ieee transactions pattern analysis machine intelligence 
clarkson robinson 
language model adaptation mixtures exponentially decaying cache 
proceedings ieee icassp 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal society statistics 

population frequencies species estimation population parameters 
biometrika 
jelinek 
self organized language models speech recognition 
waibel lee editors readings speech recognition pages 
morgan kaufman publishers 
jelinek 
trigrams 
struggle improved language models 
proceedings eurospeech 
katz 
estimation probabilities sparse data language model component speech recognizer 
ieee transactions acoustics speech signal processing 
kneser ney 
improved backing gram language modeling 
proceedings ieee icassp 
kuhn de mori 
cache natural language model speech reproduction 
ieee transactions pattern analysis machine intelligence 
kuhn de mori 
corrections cache natural language model speech reproduction 
ieee transactions pattern analysis machine intelligence 
lau rosenfeld roukos 
trigger language models maximum entropy approach 
proceedings ieee icassp 
ney essen kneser 
structuring probabilistic dependencies stochastic language modelling 
computer speech language 
schwartz fung nguyen 
estimation powerful language models small large corpora 
proceedings ieee icassp 
rosenfeld 
adaptive statistical language modeling maximum entropy approach 
phd thesis school computer science carnegie mellon university april 
published technical report cmu cs 
rosenfeld 
cmu statistical language modeling toolkit arpa csr evaluation 
arpa spoken language technology workshop austin tx january 
witten bell 
zero frequency problem estimating probabilities novel events adaptive text compression 
ieee transactions information theory july 
