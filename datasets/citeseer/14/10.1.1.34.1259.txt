investigation linguistic features clustering algorithms topical document clustering vasileios hatzivassiloglou luis gravano department computer science columbia university amsterdam avenue new york ny usa gravano cs columbia edu investigate hierarchical clustering methods single link complete link average single pass linguistically motivated text features noun phrase heads proper names context document clustering 
statistical model combining similarity information multiple sources described applied darpa topic detection tracking phase tdt data 
model log linear regression alleviates need extensive search order determine optimal weights combining input features 
extensive series experiments documents multiple news sources modalities establish choice clustering algorithm additional features impact clustering performance 
apply optimal combination features tdt test data obtaining partitions documents compare favorably results obtained participants official tdt competition 
clustering plays crucial role organizing large document collections 
notable example clustering structure query results providing users overview results easier understand process flat list documents see 
form basis processing documents organized topical groups summarization 
clustering key component darpa ongoing topic detection tracking tdt initiative completed second phase tdt early 
goal tdt initiative provide benchmarks comparing systems see www itl nist gov tdt tdt htm 
address specific tasks relating manipulating organizing broadcast news newswire stories 
stream incoming news articles topic detection task grouping articles correspond topic topic defined seminal event activity directly related events activities 

topic detection clustering task group documents topic 
study document clustering applied tdt topic detection problem 
investigate alternatives crucial components clustering strategy clustering algorithm document features guide clustering 
specifically study performance popular hierarchical clustering algorithms single link complete link average single pass clustering 
consider evaluation expensive non hierarchical clustering techniques efficiency concerns 
single pass clustering irrevocable clustering assignments document soon document inspected 
techniques considered single pass best suited tdt topic detection task requires systems clustering assignments line soon new document received 
investigate limitations line algorithm experimentally compare performance single pass clustering algorithms mentioned 
component clustering strategy explore document features guide clustering 
typically document clustering techniques words appear documents define distance function determines final clustering 
additional linguistically informed sets features attempt limit input features important ones facilitating task learning clustering algorithm 
investigate sets automatically identified features matched noun phrase heads additional premodifiers excluded proper names single nouns phrases categorized people place organizations names 
conduct large scale experimental evaluation involving real documents tdt initiative data set 
results show expected average outperforms hierarchical clustering algorithms limited range clustering thresholds performance surprisingly close computationally cheaper line single pass method 
establish linguistically motivated features increase clustering performance conjunction full word vectors traditionally employed clustering 
results compare favorably obtained official participants tdt competition 
remainder review clustering algorithms linguistic features experimented sections respectively 
combining features important step usually approached approximate computationally expensive exhaustive search methods section statistical model combination task trainable log linear model 
section contains detailed presentation experimental results discussion significance 
clustering algorithms part goal experiments reported explore effect chosen clustering algorithm quality document clustering clustering text features implemented major hierarchical clustering technique discussed literature 
techniques variants ir systems perform clustering thousands documents 
briefly review algorithms discuss strengths weaknesses 
see complete discussion hierarchical clustering techniques 
set documents fd similarity function output reasonable partition collection clusters cm function pre selected threshold techniques single link clustering complete link clustering share algorithmic steps differ criterion apply determine clusters merged 
general procedure places document separate cluster initially iteratively examines pairs clusters merges pair clusters satisfy method test merging repeats cycle mergeable clusters reducing iteration 
merging criterion distinguishes methods single link clustering clusters merged pair documents complete link clustering clusters merged pairs satisfy average adopts middle position extremes 
clusters mergeable average similarity pairs equal greater threshold produced clustering depends pair clusters merged iteration multiple candidates 
single link clustering unaffected selection mergeable point continue mergeable additional elements included clusters 
complete link average chose systems select merging pair clusters largest minimum average similarity respectively 
fourth technique experimented single pass clustering unique irrevocable clustering assignments soon sees element 
single pass clustering especially suitable large collections documents situations new documents arrive continually different points time case online news source 
algorithm proceeds maintaining initially empty increasing set clusters 
new document arrives average similarity members existing cluster calculated 
cluster average exceeds matches threshold assign document cluster 
new cluster containing just arrived document formed 
choose multiple clusters satisfy similarity criterion selecting maximum average similarity document consideration 
methods single complete link reduce numerical similarity information minimum maximum set 
methods expected perform run faster average technique takes account information pairs clusters assesses merging 
comparison single pass clustering operates information step document placed final cluster arrives 
anticipate single pass clustering average 
methods adopt greedy approach clustering justified large data sets 
alternative class algorithms collectively referred non hierarchical clustering methods spend time clustered element order improve quality partitioning 
defining merit function entire partition iteratively optimized hierarchical techniques discussed revise prior decisions moving elements cluster originally assigned 
reason increased performance increased computational complexity techniques 
see comprehensive discussion non hierarchical techniques including means medians exchange simulated annealing methods 
experiments reported worked tens thousands documents collections hierarchical methods take hours cluster include optimization methods comparative analysis 
linguistic features document clustering task received considerable attention ir community ongoing topic detection tracking effort highlighted issues involved large collections documents partitioned 
techniques improving basic algorithms described previous section considered including multiple clustering stages varying thresholds probabilistic delaying decisions fixed number elements possible include delay implementation :10.1.1.21.3098
mixture models word vector distributions 
learning method depends selection informative input features producing high quality output 
tdt effort clustering information indexing retrieval purposes words document dominated sole features clustering 
case suitability full collection words basis determining document similarity eventually clusters 
unfiltered words contain information humans perform task 
keeping words appropriately weighed scheme tf idf information available clustering algorithm avoids hard choices limiting similarity features specific words classes words 
tasks informed selection features prove beneficial injecting external linguistic knowledge kinds words important classification enabling learning algorithm zoom significant input features 
example classifying images basis captions sable hatzivassiloglou established keeping sentence image captions specific parts speech significantly improves classification accuracy 
types linguistic knowledge useful information retrieval tasks nominal compounds syntactic constraints collocations 
explore linguistically motivated restrictions set words clustering noun phrase heads proper names 
possible complete accuracy word classes document belong specific topical class noun phrases proper nouns carry information document indirectly location time frame events discussed 
assumption justified news articles documents describe specific events opposed generic discussions topic 
limiting input features grammatical categories construct additional feature vectors place addition traditional word vectors clustering 
external tools extract features automatically text 
identifying noun phrase heads linkit tool developed columbia university purpose identifying significant topics documents indexing text collections 
linkit uses part speech information automatically assigned alembic toolkit developed mitre simple finite state grammar locate maximal non recursive noun phrases text 
phrases head final noun sequence 
manner phrases bill clinton president clinton clinton mapped clinton providing means addressing hard problem definite coreference 
unfortunately hillary clinton demonstrating approach introduce errors 
experiments measure positive contribution collapsing related terms canonical form linkit basic implementation outweighs errors 
second tool nominator developed ibm 
uses capitalization punctuation information contextual model large knowledge base identify proper nouns context 
proper names recognized classified categories person place org standing organization 
allows experiment different versions proper name vectors document including different categories definition really proper name 
considered versions version referred simply takes words phrases labeled proper names nominator 
second version exclude words phrases labeled unknown term 
words phrases nominator unable confidently characterize proper names identifies fixed terms respectively cases probability proper name lower 
examples words tagged tdt corpus include internet chapter examples private school study 
consider words phrases marked person place org version 
classes include information participants location event consider central document placement appropriate cluster 
combining features previous section ways calculate vectors linguistically motivated features complement basic vector containing words appear document 
having extracted vectors calculate similarity documents applying tf idf normalization vector cosine vectors correspond documents 
manner obtain different similarity matrices functions depending set input features chosen 
similarity matrices individually drive clustering algorithm produce partition document set 
interesting consider combinations similarity values assigned pair documents different feature models 
combine similarity values hard problem involves steps deciding form combining function second assigning values parameters form 
frequently chosen form linear weighted sum weights estimated search regime calculates final similarity set weights clusters documents evaluates produced clustering repeating process iteratively different sets weights 
complexity approach exponential number weights consequently parameters 
exhaustive search limited range resolution weights considered approximated gradient descent decomposition techniques 
versions distinguish occurrences word assigned different labels different contexts ford person ford org 
avoid problems consider mathematical model selecting optimal values weights slightly modified formulation weighted sum approach 
vectors similarity values vk feature words linkit value ij corresponding th pair documents assume combining function best approximate values vector ranging pairs documents documents th pair cluster values obtained training set documents optimal cluster assignments available case tdt training data experiments 
fit log linear regression model predictors response 
model calculates linear internal predictor weighted sum 
relates final response logistic transformation note log linear model quite similar linear regression successfully combining features text classification 
log linear extension guarantees final response lie interval endpoints associated outcomes 
appropriate straightforward weighted sum technical reasons relating statistical assumptions inherent modeling fact variance binomial distribution appropriately models dependent mean constant assumed linear regression model see 
modest assumptions distribution optimal set weights calculated efficiently iterative reweighted nonlinear squares algorithm 
approach aims optimize final similarity function evaluation metrics clustering function leads 
possible different set weights lead higher scores effects particular clustering algorithm factored 
experimental analysis section weights assigned procedure case better produced limited exhaustive search exceptional case difference final performance negligible 
time log linear model greatly simplifies task combining similarity values principled manner allows experiment models words plus linkit data words plus versions nominator data practical 
experimental evaluation previous sections described possible choices clustering algorithms text features produce document clusters 
evaluate clustering feature selection choices experimentally section text corpora evaluation metrics developed context tdt initiative section 
addition reporting results single pass algorithm satisfies tdt online requirement results clustering algorithms section algorithms fit strictly tdt setting inspect documents repeatedly clustering 
tdt corpora metrics tdt corpus experiments consists newswire articles associated press new york times voice america public radio international cnn abc 
training corpus consisted articles tdt training corpus evaluation test corpus consisted documents tdt evaluation test corpus topic detection 
tdt metric evaluating performance topic detection systems cost detection cdet combines pm false alarm fa errors single number cdet cm 
pm 
pt fa 
fa 
pt cm fa costs false alarm respectively equal tdt evaluation pt training set specific priori target probability story discussing topic equal tdt evaluation topic detection 
note pm fa related traditional metrics recall precision respectively specifically pm equal minus recall fa fallout generally low precision high 
cdet offers way combine usually competing factors number rankings different systems different versions system different input features 
results report cdet metric computed evaluation software produced nist tdt 
training test corpora evaluation metric enables directly compare results obtained tdt participants large scale evaluation 
discusses details tdt evaluation methodology including way system produced clusters aligned clusters model 
tdt experiments evaluation scores reported separately micro averaging scores calculated document averaged documents macro averaging scores calculated cluster model averaged clusters tdt terminology story weighted topic weighted respectively 
results training corpus tf vs tf idf weights experiment studied performance tf tf idf weighting schemes tdt training cor tdt data included additional documents development set participants free training 
documents experiments reported 
subset training corpus available tdt participants see footnote 
topic story weighted cdet function threshold words tf idf weights 
topic story weighted cdet function threshold tf idf weights average clustering 
pus discussed 
surprisingly tf idf weights results slightly better lower values topic weighted cdet metric 
rest experimental evaluation tf idf weights 
subsequent experiments adopted standard cosine metric calculate similarity feature vectors 
choice clustering algorithm compares performance clustering algorithms discussed section words representation documents tf idf weights 
expected theoretical grounds average performs significantly better complete link single link better single pass clustering 
results consistent feature representations documents tried section 
consequently focus clustering rest 
report results single pass clustering 
effect technique performs slightly worse average clustering training data important advantage line technique section comparable methods tdt 
furthermore reveals appropriate range thresholds single pass performs average clustering 
surprising fact justifies single pass clustering accurate estimation suitable value performed training data may offer explanation relatively small improvement obtained tdt systems delayed clustering decisions documents 
analysis individual document features discussed section choice features represent documents collection 
shows cdet values obtain choices isolation clustering 
see words representation performs best significantly lower values topic story weighted cdet linkit representations 
result may indicate discarding non nouns results significant loss information may due limitations specific tools linkit nominator 
example cases slightly different forms proper names microsoft microsoft matched current techniques methods perform matching shown useful information retrieval 
case failure additional linguistic features improve feature combination intercept words linkit nominator words linkit words words words words linkit words linkit words linkit table combinations features weights estimated corresponding log linear model 
topic story weighted cdet function threshold tf idf weights average clustering optimal log linear combination words features 
formance mean contribute better clusterings original word vectors show paragraph 
choice document feature combinations combine similarity values obtained methods applied log linear model section 
selected documents training part tdt corpus topic assignment known manually annotated linguistic data consortium 
pairs documents randomly selected pairs training models included different combinations input features 
combinations weights assigned similarity vector shown table 
note interesting observations table 
line empirical results preceding paragraph weights words invariably larger features weights nominator larger absolute values assigned linkit 
indicates current extraction techniques words remain important feature information provided nominator useful simple noun phrase head matching performed linkit 
second linkit vector assigned negative weights mean noun heads useful matching feature indicates information words proper nouns additional confirmed separate analysis variance study 
matches noun phrases evidence document dissimilarity 
surprising result form basis detailed analysis matching noun phrases 
negative intercept confirms absence information low expected similarity documents far documents belong different clusters cluster 
note automatic modeling procedure explored range weights usually covered techniques unable detect negative correlation linkit similarities document similarity 
contrasts performance best single feature words models involving additional linguistically motivated features shows combined features slightly outperform words feature extend range threshold values cdet curve remains low important method applied unseen data different optimal may apply 
comparisons tdt results having explored different clustering algorithms contributions features feature combinations part tdt training set selected best feature combination average similarity words linkit accordingly model combining features predicts documents cluster time 
optimized story weighted optimized topic weighted story weighted cdet topic weighted cdet story weighted cdet topic weighted cdet average clustering training test single pass clustering training test table training test detection costs average single pass clustering tf idf weights optimal log linear combination words linkit 
optimized story weighted optimized topic weighted story weighted cdet topic weighted cdet story weighted cdet topic weighted cdet average clustering training test single pass clustering training test table training test detection costs average single pass clustering tf idf weights words single feature 
organization cdet cdet average bbn ibm dragon umass upenn cmu cidr uiowa system system table cdet values test corpus tdt participants system 
rows show test results obtained single pass clustering best parameters learned training lowest cdet lowest topic weighted cdet 
words plus linkit plus fixed threshold parameter basis training set 
applied average single pass methods tdt test set 
results shown table reveal system stable applied different set unseen documents generally improves cost detection test set lower cost test set training set 
note table separate entries threshold values minimized training set story weighted micro averaged cdet topic weighted macro averaged cdet 
table shows original feature words fares compared combination features cdet scores table 
observe combination features continues slightly outperform words test set case training set 
cases tables combined features approach performs better just words 
compare performance system features single pass technique ensure fair comparison test set performance tdt participants set documents table 
system places middle range participants terms micro averaged detection cost optimized micro macro averages performs especially terms macro averaged detection cost places second optimized micro averaging optimized macro averaging 
analysis established chosen clustering algorithm input features difference performance system separates documents topical groups 
confirmed average performs best tdt setting discovered single pass offer cheaper close second clustering threshold carefully selected 
addition information matching noun phrase heads proper nouns help improve clustering quality 
addressed hard problem combining similarity values multiple input features different similarity functions proposing theoretically justified statistical model shown practice perform better exhaustive search 
course improvement offered linguistic features relatively small fact believe due part errors extraction tools part generality classes considered 
motivation including noun phrase head proper name vectors place focus input features documents 
noun phrase heads person organization names approximation explore additional techniques proper names nouns selected major participants event described document 
similarly plan refine technique identifying location event incorporate time information success systems 
acknowledgments stefan participated earlier formed precursor experiments reported 
kathy mckeown shi fu chang members stim research group columbia university provided valuable feedback design reported experiments 
reported funded part national science foundation stimulate iri 
opinions findings recommendations authors necessarily reflect views nsf 
aberdeen burger day hirschman robinson vilain 
mitre description alembic system muc 
proceedings sixth message understanding conference muc 
bates watts 
nonlinear regression analysis applications 
wiley new york 
cohen 
integration heterogeneous databases common domains queries textual similarity 
proceedings acm international conference management data sigmod june 
fiscus doddington garofolo martin 
nist topic detection tracking evaluation tdt 
proceedings darpa broadcast news workshop pages herndon virginia february march 
frakes baeza yates editors 
information retrieval data structures algorithms 
prentice hall englewood cliffs new jersey 
gay croft 
interpreting nominal compounds information retrieval 
information processing management 
marti hearst jan pedersen 
reexamining cluster hypothesis scatter gather retrieval results 
proceedings th annual international acm conference research development information retrieval sigir august 
kaufman rousseeuw 
finding groups data cluster analysis 
wiley new york 
mark liberman 
topic detection tracking principal investigators meeting 
stephen lowe 
beta binomial mixture model application tdt tracking detection 
proceedings darpa broadcast news workshop pages herndon virginia february march 
mckeown klavans hatzivassiloglou barzilay eskin 
multidocument summarization reformulation progress prospects 
proceedings th national conference artificial intelligence aaai pages orlando florida july 
national institute standards technology 
topic detection tracking phase tdt evaluation plan 
version august rd 
available www itl nist gov tdt doc tdt eval plan pdf 
ron papka james allan victor lavrenko 
umass approaches detection tracking tdt 
proceedings darpa broadcast news workshop pages herndon virginia february march 
sable hatzivassiloglou 
text approaches categorization images 
proceedings rd european conferenceon digital libraries ecdl paris france 
salton buckley 
term weighting approaches automatic text retrieval 
information processing management 
salton smith 
application syntactic methodologies automatic text analysis 
proceedings twelfth annual international acm sigir conference research development information retrieval sigir 
duffy 
statistical analysis discrete data 
springer verlag new york 
alan smeaton 
progress application natural language processing information retrieval tasks 
computer journal 
wacholder 
simplex nps clustered head method identifying significant topics document 
proceedings coling acl workshop computational treatment nominals pages montreal canada october 
wacholder ravin choi 
disambiguation proper names text 
proceedings th acl conference applied natural language processing anlp pages washington april 
yang liu 
re examination text categorization methods 
proceedings nd acm international conference research development information retrieval sigir pages berkeley california 
yang pierce carbonell :10.1.1.21.3098
study retrospective line event detection 
proceedings st annual international acm sigir conference research development information retrieval sigir pages melbourne australia august 
