system video surveillance monitoring robert collins alan lipton takeo kanade david david hasegawa peter burt lambert wixson cmu ri tr robotics institute carnegie mellon university pittsburgh pa sarnoff princeton nj year video surveillance monitoring vsam project robotics institute carnegie mellon university cmu sarnoff developed system autonomous video surveillance monitoring 
technical approach uses multiple cooperative video sensors provide continuous coverage people vehicles cluttered environment 
final report presents overview system technical accomplishments achieved 
fl carnegie mellon university funded darpa image understanding contract office naval research 
thrust cmu research darpa video surveillance monitoring vsam project cooperative multi sensor surveillance support battlefield awareness 
vsam integrated feasibility demonstration ifd contract developed automated video understanding technology enables single human operator monitor activities complex area distributed network active video sensors 
goal automatically collect disseminate real time information battlefield improve situational awareness commanders staff 
military federal law enforcement applications include providing perimeter security troops monitoring peace movements unmanned air vehicles providing security airports suspected drug terrorist hide outs collecting time stamped pictures entering exiting building 
automated video surveillance important research area commercial sector 
technology reached stage mounting cameras capture video imagery cheap finding available human resources sit watch imagery expensive 
surveillance cameras prevalent commercial establishments camera output recorded tapes rewritten periodically stored video archives 
crime occurs store car stolen investigators go back fact see happened course late 
needed continuous hour monitoring analysis video surveillance data alert security officers burglary progress suspicious individual parking lot options open avoiding crime 
keeping track people vehicles interactions urban battlefield environment difficult task 
role vsam video understanding technology achieving goal automatically parse people vehicles raw video determine insert dynamic scene visualization 
developed robust routines detecting tracking moving objects 
detected objects classified semantic categories human human group car truck shape color analysis labels improve tracking temporal consistency constraints 
classification human activity walking running achieved 
labeled entities determined image coordinates wide baseline stereo overlapping camera views intersection viewing rays terrain model monocular views 
computed locations feed higher level tracking module tasks multiple sensors variable pan tilt zoom cooperatively continuously track object scene 
resulting object hypotheses sensors transmitted symbolic data packets back central operator control unit displayed graphical user interface give broad overview scene activities 
technologies demonstrated series yearly demos testbed system developed urban campus cmu 
final report year vsam ifd research program 
emphasis results published 
older appeared print briefly summarized relevant technical papers 
report organized robotics institute cmu vsam final report follows 
section contains description vsam ifd testbed system developed testing ground new video surveillance research 
section describes basic video understanding algorithms demonstrated including moving object detection tracking classification simple activity recognition 
section discusses geospatial site models aid video surveillance processing including calibrating network sensors respect model coordinate system computation geolocation estimates graphical display object hypotheses distributed simulation 
section discusses coordination multiple cameras achieve cooperative object tracking 
section briefly lists milestones achieved vsam demos performed pittsburgh rural bushy run site second third held urban cmu campus concludes plans research 
appendix contains published technical papers cmu vsam research group 
vsam testbed system built vsam testbed system demonstrate automated video understanding technology described sections combined coherent surveillance system enables single human operator monitor wide area 
testbed system consists multiple sensors distributed campus cmu tied control room located planetary robotics building prb 
testbed consists central operator control unit ocu control room vsam testbed system campus carnegie mellon university 
close main rack 
receives video ethernet data multiple remote sensor processing units see 
ocu responsible integrating symbolic object trajectory information accumulated geometric site model presenting results user map graphical user interface gui 
logical component testbed system architecture described briefly 
robotics institute cmu vsam final report dis ocu gui vis sensor fusion site model schematic overview vsam testbed system 
sensor processing units spu acts intelligent filter camera vsam network 
function analyze video imagery presence significant entities events transmit information symbolically ocu 
arrangement allows different sensor modalities seamlessly integrated system 
furthermore performing video processing possible spu reduces bandwidth requirements vsam network 
full video signals need transmitted symbolic data extracted video signals 
vsam testbed handle wide variety sensor spu types 
list ifd sensor types includes color ccd cameras active pan tilt zoom control fixed field view monochromatic low light cameras thermal sensors 
logically spu combines camera local computer processes incoming video 
convenience video signals testbed system sent fiber optic cable computers located rack control room 
exceptions spu platforms move van mounted relocatable spu suo portable spu airborne spu 
computing power board results sent ocu relatively low bandwidth wireless ethernet links 
addition ifd house focussed research effort fre sensor packages integrated system columbia lehigh hemispherical field view texas instruments indoor surveillance system 
pre specified communication protocol see section fre systems able directly interface vsam network 
logical system architecture treated identically 
difference hardware level different physical connections cable wireless ethernet may required connect ocu 
relocatable van airborne spu warrant discussion 
relocatable van spu consists sensor pan tilt head mounted small tripod placed vehicle roof stationary 
video processing performed board vehicle results object detection tracking assembled symbolic data packets transmitted back operator control workstation radio ethernet connection 
major research issue involved demonstrating van unit involves rapidly calibrate sensor pose object detection tracking results integrated vsam network computation geolocation display operator control console 
robotics institute cmu vsam final report types sensors incorporated vsam ifd testbed system color ptz thermal relocatable van airborne 
addition fre sensors successfully integrated columbia lehigh texas instruments indoor activity monitoring system 
robotics institute cmu vsam final report airborne sensor computation packages mounted norman twin engine aircraft operated army night vision electronic sensors directorate 
equipped flir systems ultra turret degrees freedom pan tilt global positioning system gps measuring position attitude heading system measuring orientation 
continual self motion aircraft introduces challenging video understanding issues 
reason video processing performed sarnoff specially designed video processing engine 
operator control unit ocu shows functional architecture vsam ocu 
accepts video processing results integrates information site model database known objects infer activities interest user 
data sent gui visualization tools output system 
info site model info arbiter sensor trigger ocu functional model spu geolocation triggers classification recognition tracking mtd behaviour analysis footprint activity db footprint target db site model db target dynamic analysis trajectory maintenance modeling multi tasking handoff control sensor target info sensor idle spu tracking detection car park monitoring monitoring hvi definition trigger user gui functional architecture vsam ocu 
key piece system functionality provided ocu sensor arbitration 
care taken ensure outdoor surveillance system limited sensor assets 
sensors allocated surveillance tasks way user specified tasks get performed sensors multiple sensors assigned track important objects 
time ocu maintains list known objects sensor parameters set tasks may need attention 
tasks explicitly indicated user gui may include specific objects tracked specific regions watched specific events detected person near particular doorway 
sensor robotics institute cmu vsam final report arbitration performed arbitration cost function 
arbitration function determines cost assigning tasks 
costs priority tasks load spu visibility objects particular sensor 
system performs greedy optimization cost determine best combination spu tasking maximize system performance requirements 
ocu contains site model representing vsam relevant information area monitored 
site model representation optimized efficiently support vsam capabilities ffl object geolocation intersection viewing rays terrain 
ffl visibility analysis predicting portions scene visible sensors sensors efficiently tasked 
ffl specification geometric location extent relevant scene features 
example directly task sensor monitor door building look vehicles passing particular intersection 
graphical user interface gui operator console located control room 
shown laptop portable operator console 
close view visualization node display screen 
technical goals vsam project demonstrate single human operator effectively monitor significant area interest 
keeping track multiple people vehicles interactions complex urban environment difficult task 
user obviously shouldn looking dozen screens showing raw video output 
amount sensory overload virtually guarantees information ignored requires prohibitive amount transmission bandwidth 
approach provide interactive graphical user interface gui uses vsam technology automatically place dynamic agents representing people robotics institute cmu vsam final report vehicles synthetic view environment 
approach benefit visualization scene events longer tied original resolution viewpoint single video sensor 
gui currently consists map area overlaid object locations sensor platform locations sensor fields view 
addition low bandwidth compressed video stream sensors selected real time display 
gui sensor suite tasking 
interface operator task individual sensor units entire testbed sensor suite perform surveillance operations generating quick summary object activities area 
lower left corner control window contains selection controls organized tabbed selections 
allows user move fluidly different controls corresponding entity types objects sensors regions interest 
ffl object controls 
track directs system actively tracking current object 
tracking terminates active tracking tasks system 
trajectory displays trajectory selected objects 
error displays geolocation error bounds locations trajectories selected objects 
ffl sensor controls 
show fov displays sensor fields view map position marker drawn 
move triggers interaction allowing user control pan tilt angle sensor 
request imagery requests continuous stream single image currently selected sensor imagery terminates current imagery stream 
ffl roi controls panel contains controls associated regions interest system 
tasks focus sensor resources specific areas session space 
create triggers creation roi specified interactively user polygon boundary points 
user selects set object types human vehicle trigger events roi set event types enter pass considered trigger events roi 
communication nominal architecture vsam network allows multiple linked controlling multiple 
ocu supports exactly gui user related command control information passed 
data dissemination limited single user interface accessible series visualization nodes vis 
independent communication protocols packet structures supported architecture carnegie mellon university packet architecture distributed interactive simulation dis protocols 
designed low bandwidth highly flexible architecture relevant vsam information compactly packaged robotics institute cmu vsam final report gui spu spu gui gui spu spu vis vis vis vis vis spu spu spu ocu vis ocu ocu spu nominal architecture expandable vsam networks 
event block event block target target position block header roi roi block block event comm 
sensor block sensor comm 
block comm 
block block comm 
block bounding block target imagery box template image sensor block imagery packet structure 
header describes sections 
section multiple data blocks 
data block describe information 
redundant overhead 
concept packet architecture hierarchical decomposition 
data sections encoded packet command sensor image object event region interest 
short packet header section describes sections packet 
section possible represent multiple instances type data instance potentially containing different layout information 
level short describe contents various blocks packets keeping wasted space minimum 
communication guis compatible 
protocol specification document accessible www cs cmu edu vsam 
vis nodes designed distribute output vsam network needed 
provide symbolic representations detected activities overlaid maps imagery 
information flow vis nodes unidirectional originating ocu 
communication uses dis protocol described detail 
important benefit keeping vis nodes dis compatible allows easily interface synthetic environment visualization tools modsaf section 
robotics institute cmu vsam final report current testbed infrastructure section describes vsam testbed campus carnegie mellon university fall see 
vsam infrastructure consists cameras distributed campus 
cameras connected vsam operator control room planetary robotics building prb connected fiber optic lines prb wired directly spu computers portable small unit operations suo unit connected wireless ethernet vsam ocu 
done vsam concentrated increasing density sensors wean prb area 
overlapping fields view area campus enable conduct experiments wide baseline stereo object fusion sensor cuing sensor handoff 
monochrome color smith wean prb placement color monochrome cameras current vsam testbed system 
shown additional cameras flir suo portable system moved different places needed 
backbone cmu campus vsam system consists sony evi color zoom cameras installed prb smith hall newell simon hall wean hall roberts hall porter hall 
units mounted directed perception pan tilt heads 
camera newell simon mounted technologies pan tilt head 
rugged outdoor mount evaluated better performance specifications longer term usage 
stationary fixed fov color cameras mounted peak prb side robotics institute cmu vsam final report pan tilt zoom color camera located 
prb left right sensors added facilitate activity analysis classification sensor cuing 
stationary fixed fov monochrome cameras mounted roof wean hall close proximity pan tilt zoom color cameras 
connected operator control room single fiber video multiplexor 
monochrome cameras vertical resolution tv lines perform fairly night available street lighting 
mounting bracket installed cameras temporary installation thermal flir sensor 
fourth stationary fixed fov monochrome camera mounted prb pointing back 
suo portable unit built allow software development research cmu support suo program 
unit consists hardware delivered fort georgia november 
operator control room prb houses spu ocu gui development workstations nineteen computers total 
pentium iii mhz computers 
single compound spu quad xeon mhz processor computer purchased conduct research classification activity analysis digitization simultaneous video streams 
included list machines silicon graphics origin develop video database storage retrieval algorithms designing user interfaces handling vsam video data 
auto tracking tps installed corner prb hardwired data processing computer linked vsam ocu 
system allows real time automatic tracking objects obtain ground truth evaluating vsam geolocation sensor fusion algorithms 
data displayed real time vsam gui 
office naval research provided funds thermal sensors quad xeon processor computer origin sgi infinite reality engine surveying systems 
video understanding technologies keeping track people vehicles interactions complex environment difficult task 
role vsam video understanding technology achieving goal automatically parse people vehicles raw video determine automatically insert dynamic scene visualization 
developed robust routines detecting moving objects tracking video sequence combination temporal differencing template tracking 
detected objects classified semantic categories human human group car truck shape color analysis labels improve tracking temporal consistency constraints 
classification human activity walking running achieved 
labeled entities determined image coordinates wide baseline stereo overlapping camera views intersection viewing rays terrain model monocular views 
computed robotics institute cmu vsam final report provide higher level tracking capabilities tasking multiple sensors variable pan tilt zoom cooperatively track object scene 
results displayed user real time gui archived web object event database 
moving object detection detection moving objects video streams known significant difficult research problem 
aside intrinsic usefulness able segment video streams moving background components detecting moving blobs provides focus attention recognition classification activity analysis making processes efficient moving pixels need considered 
conventional approaches moving object detection temporal differencing background subtraction optical flow see excellent discussion :10.1.1.47.9503:10.1.1.54.7209
temporal differencing adaptive dynamic environments generally poor job extracting relevant feature pixels 
background subtraction provides complete feature data extremely sensitive dynamic scene changes due lighting extraneous events 
optical flow detect independently moving objects presence camera motion optical flow computation methods computationally complex applied full frame video streams real time specialized hardware 
vsam program cmu developed implemented methods moving object detection vsam testbed 
combination adaptive background subtraction frame differencing section 
hybrid algorithm fast surprisingly effective primary algorithm majority vsam system 
addition new prototype algorithms developed address shortcomings standard approach 
mechanism maintaining temporal object layers developed allow greater disambiguation moving objects occluded objects resume motion section 
limitation affects method standard algorithm static cameras stare mode pan tilt cameras 
overcome limitation second extension developed allow background subtraction continuously panning tilting camera section 
clever accumulation image evidence algorithm implemented real time conventional pc platform 
fourth approach moving object detection moving airborne platform developed subcontract sarnoff 
approach image stabilization special video processing hardware 
described section 
robotics institute cmu vsam final report car moves hole left background model parked car long term detection car moves problems standard mtd algorithms 
background subtraction leaves holes stationary objects move 
frame differencing detect entire object hybrid algorithm moving object detection developed hybrid algorithm detecting moving objects combining adaptive background subtraction technique frame differencing algorithm 
discussed major drawback adaptive background subtraction stationary objects scene start move 
usually detected leave holes newly exposed background imagery differs known background model see 
background model eventually adapts holes generate false alarms short period time 
frame differencing subject phenomenon generally effective method extracting entire shape moving object 
overcome problems combined methods 
frame differencing operation performed determine regions legitimate motion followed adaptive background subtraction extract entire moving region 
consider video stream stationary stabilized camera 
represent intensity value pixel position time frame differencing rule suggests pixel legitimately moving intensity changed significantly current image frame current image frame 
pixel moving ji gamma gamma ji gamma gamma threshold describing statistically significant intensity change pixel position described 
main problem frame differencing pixels interior object uniform intensity aren included set moving pixels 
clustering moving pixels connected region interior pixels filled applying adaptive background subtraction extract moving pixels region bounding box represent current background intensity value pixel learned observation time 
blob filled pixels significantly different background model fx ji gamma rg robotics institute cmu vsam final report background model difference threshold statistical properties pixel intensities observed sequence images fi initially set image initially set pre determined non zero value 
updated time ae gamma non moving moving ae gamma theta ji gamma non moving moving time constant specifies fast new information old observations 
note value changed pixels determined non moving part stationary background 
non moving pixel position considered time series analogous local temporal average intensity values analogous times local temporal standard deviation intensity computed infinite impulse response iir filter 
shows result detection algorithm frame 
result detection algorithm 
original image 
detected motion regions 
temporal layers adaptive background subtraction robust detection system able recognize objects stopped disambiguate overlapping objects functions usually possible traditional motion detection algorithms 
important aspect derives observation legitimately moving objects scene tend cause faster transitions changes due lighting meteorological diurnal effects 
section describes novel approach object detection layered adaptive background subtraction 
robotics institute cmu vsam final report detection algorithm layered detection processes pixel analysis region analysis 
purpose pixel analysis determine pixel stationary transient observing intensity value time 
region analysis deals agglomeration groups pixels moving regions stopped regions 
graphically depicts process 
observing intensity transitions pixel different intensity layers connected transient periods postulated 
pixel color region spatial time mo mo background moving object object layer transient transient layer concept combining pixel statistics region analysis provide layered approach motion detection 
transient regions stationary regions image buffer pixel analysis region analysis video stream time diff 
spatiotemporal analysis layer management creation ordering deletion pixel diff 
selective adaptation moving target stopped target lm layer images background motion different similar architecture detection process 
temporal analysis pixel basis determine pixels transient stationary 
transient pixels clustered groups assigned spatio temporal layers 
layer management process keeps track various background layers 
shows architecture detection processes 
key element algorithm needs observe behavior pixel time determining pixel undergoing transition 
observed pixel intensity value displays characteristic profiles depending occurring scene pixel location ffl legitimate object moving pixel displays profile exhibits step change robotics institute cmu vsam final report ambient illumination changes human moves pixel human stops pixel characteristic pixel intensity profiles common events 
moving objects passing pixel cause intensity profile step change followed period instability 
object passes pixel intensity returns normal 
object stops intensity settles new value 
variations ambient lighting exhibit smooth intensity changes large steps 
intensity followed period instability step back original background intensity 
shows profile 
ffl legitimate object moving pixel stopping displays profile exhibits step change intensity followed period instability settles new intensity object stops 
shows profile 
ffl changes intensity caused lighting meteorological effects tend smooth changes don exhibit large steps 
shows profile 
capture nature changes pixel intensity profiles factors important existence significant step change intensity intensity value profile stabilizes passing period instability 
interpret meaning step change object passing stopping leaving pixel need observe intensity curve re stabilizing step change 
introduces time delay process 
particular current decisions pixel events frames past 
implementation set correspond second video 
pixel intensity time occurring frames past 
functions computed motion trigger just prior frame interest stability measure computed frames time 
motion trigger simply maximum absolute difference pixel intensity value previous frames max phi ji gamma gamma psi robotics institute cmu vsam final report stability measure variance intensity profile time gamma gamma point transience map defined pixel possible values background transient stationary 
stationary background threshold transient transient stabilized intensity value background intensity background stationary non background pixels transience map clustered regions nearest neighbor spatial filter clustering radius process similar performing connected components segmentation gaps distance pixels tolerated component 
choice depends scale objects tracked 
spatial region analyzed algorithm transient pixels labeled transient moving object elseif stationary pixels labeled stationary remove pixels assigned layer 
left new layer new layer stopped object contains mixture transient stationary pixels perform spatial clustering 
region sr produced spatial clustering sr transient sr moving object robotics institute cmu vsam final report sr stationary new layer sr sr stopped object sr stationary transient sr moving object regions consist stationary pixels added layer background 
layer management process determine stopped objects resume motion occluded moving stationary objects 
stationary layered regions scene background updated iir filter described section accommodate slow lighting changes noise imagery compute statistically significant threshold values 
detection results shows example analysis occurs single pixel 
video sequence contains activities pixel 
vehicle drives pixel stops 
second vehicle occludes stops 
person getting second vehicle occludes pixel 
person returning vehicle occludes pixel 
second car drives away 
car drives away seen steps clearly visible pixel intensity profile algorithm correctly identifies layers accumulate 
shows output region level layered detection algorithm 
detected regions shown surrounded bounding boxes note overlapping objects independently detected 
stopped car depicted temporary background layer person determined moving foreground region overlayed 
pixels belonging car person disambiguated 
robotics institute cmu vsam final report background stationary frame transient intensity variance trigger layer layer example pixel analysis scene shown 
car drives stops 
second car stops front 
person gets returns 
second car drives away followed shortly car 
layer layer stopped stopped moving detection result 
stopped vehicle partially occludes person moving foreground 
displayed right layers corresponding stopped vehicles moving foreground person bitmaps denoting pixels occluded layer 
robotics institute cmu vsam final report background subtraction continuously panning camera pan tilt camera platforms maximize virtual field view single camera loss resolution accompanies wide angle lens 
allow active tracking object interest scene 
moving object detection background subtraction directly applicable camera panning tilting image pixels moving known camera pan tilt approximately described pure camera rotation apparent motion pixels depends camera motion scene structure 
respect problems associated panning tilting camera easier camera mounted moving vehicle traveling scene 
ultimately seek generalize adaptive background subtraction handle panning tilting cameras representing full spherical background model 
algorithmic tasks need performed background subtraction camera pans tilts different parts full spherical model retrieved subtracted reveal independently moving objects 
background updating camera revisits various parts full field view background intensity statistics areas updated 
tasks depend knowing precise pointing direction sensor words mapping pixels current image corresponding pixels background model 
read current pan tilt angles encoders pan tilt mechanism information reliable camera stationary due unpredictable communication delays precisely know pan tilt readings image camera moving 
solution problem register image current spherical background model inferring correct pan tilt values camera rotating 
set background images panning tilting camera 
maintaining background model larger camera physical field view entails representing scene collection images 
case initial background model constructed methodically collecting set images known pan tilt settings 
example view set robotics institute cmu vsam final report shown 
approach building background model images spherical cylindrical mosaic set images directly determining appropriate distance pan tilt space 
warping transformation current image nearby image simple planar projective transform 
main technical challenge register incoming video frames appropriate background image real time 
image registration techniques difficult implement real time special video processing hardware 
developed novel approach registration relies selective integration information small subset pixels contain information state variables estimated projective transformation parameters 
dramatic decrease number pixels process results substantial speedup registration algorithm point runs real time modest pc platform 
details 
results sample frame registration background subtraction shown 
results background subtraction panning tilting camera 
left right current video frame closest background image warp current frame image coordinates absolute value difference warped frame background image 
object tracking build temporal model activity individual object blobs generated motion detection tracked time matching frames video sequence 
systems object tracking kalman filters 
pure kalman filter approaches limited unimodal gaussian densities support simultaneous alternative motion hypotheses 
extend basic kalman filter notion maintain list multiple hypotheses handle cases matching ambiguity multiple moving objects 
object trajectories analyzed help reduce false alarms distinguishing legitimate moving objects noise clutter scene 
iteration basic tracking algorithm predict positions known objects associate predicted objects current objects robotics institute cmu vsam final report tracks split create new tracking hypothesis tracks merge merge tracking hypotheses update object track models reject false alarms object frame represented parameters position image coordinates dp position uncertainty image velocity uncertainty velocity object bounding box image coordinates image intensity template numeric confidence measure numeric salience measure 
predicting object positions computational simplicity accurate tracking important estimate position object iteration tracker 
estimated position cull number moving regions need tested 
object position image estimated typical manner 
time interval dt samples position extrapolated dt uncertainty position assumed original position uncertainty plus velocity uncertainty grown function time dp dp dt values choose candidate moving regions current frame 
done extrapolating bounding box object dt growing dp moving region centroid falls predicted bounding box considered candidate matching 
object matching object region current frame determine best match frame performed image correlation matching computed convolving object intensity template candidate regions new image 
evaluate potential object displacement image region accumulate weighted sum absolute intensity differences pixel region corresponding pixel frame yielding correlation function ji gamma jjw jj weighting function described shortly jjw jj normalization constant jjw jj robotics institute cmu vsam final report negative correlation surface normalised correlation typical correlation surface inverted easier viewing graphically results offsets thought correlation surface minimum provides position best match measure quality match 
position best match argmin correlation surface min refined sub pixel accuracy bi quadratic interpolation new position object corresponding match new velocity estimate dt quality match value minc 
due real time processing constraints vsam testbed system basic correlation matching algorithm modified ways improve computational efficiency 
correlation computed moving pixels 
achieve setting weighting function zero pixels moving performing computation pixels 
moving pixels radial linear weighting function gamma max radial distance pixels center region max largest radial distance effect putting weight pixels center object 
second significantly imagery dynamically sub sampled ensure constant computational time match 
matching theta size image template computation rapidly unwieldy large templates 
notion fix threshold size image sub sampled 
furthermore treat dimensions separately data lost dimension small efficient matching 
case threshold set pixels determined empirically provide reasonable quantity data correlation matching stressing computational engine 
algorithm robotics institute cmu vsam final report sub sample direction sub sample direction course physically sub sampling imagery computationally expensive correlation matching implemented counting number times sub sampling performed direction selecting pixels spacing correlation process 
example theta image sub sampled twice direction direction making theta image 
correlation process th pixel direction nd pixel direction chosen matching 
loss resolution sub pixel accuracy method 
method ensures computational complexity matching process 
complexity matching function shown 
computational complexity correlation matching algorithm threshold 
clearly complexity bounded 
hypothesis tracking updating tracking objects video largely matter matching 
idea frame match known objects frame moving regions 
simple scenarios arise ffl moving region exists match known object 
case new object hypothesized confidence set nominal low value 
ffl object match moving region 
object left field view occluded detected 
case confidence measure object reduced 
confidence drops threshold object considered lost 
robotics institute cmu vsam final report ffl object matches exactly moving region 
best case tracking 
trajectory object updated information new moving region confidence object increased 
ffl object matches multiple moving regions 
occur object breaks independent objects group people breaking person getting car detection algorithm cluster pixels object correctly 
case best region indicated correlation matching value chosen new object position confidence value increased moving regions considered new object hypotheses updated accordingly 
ffl multiple objects match single moving region 
occur objects occlude objects merge group people coming erroneously split object clustered back 
case special exception 
analysis done object trajectories determine update object hypotheses 
objects merging single moving region tracked separately 
trajectories analyzed 
share velocity period time merged single object 
tracked separately 
allows system continue track objects occluding merge ones form single object 
object parameters updated parameters matched new observations moving regions 
updated position object position calculated sub pixel accuracy correlation matching process new velocity estimate calculated matching filtered iir filter provide gamma new velocity uncertainty estimate generated iir filter way gamma gamma cases template object taken template moving region confidence increased 
multiple objects matched single moving region templates updated 
objects come occluding template corrupted updated 
philosophy decision hopefully occluding objects change appearance greatly occlusion tracking possible occlusion finished 
note multiple objects may match moving region necessarily get position estimate correlation matching process match different parts region 
object matched maintains position velocity estimates current image template 
confidence reduced 
confidence object drops certain threshold considered lost dropped list 
high confidence objects ones robotics institute cmu vsam final report tracked reasonable period time persist frames object momentarily occluded reappears tracker reacquire 
results system shown 
tracking objects simultaneously 
false alarm rejection serious issue moving object tracking disambiguation legitimate objects motion clutter trees blowing wind moving shadows noise video signal 
cue legitimacy object track persistence intermittent contact valid object persistent 
cue salience trajectory trees blowing wind tend exhibit oscillatory motion people vehicles tend move purpose 
tracking scheme described automatically deals persistence objects special consideration salience objects 
motion salience algorithm cumulative flow technique due wixson 
optic flow moving objects accumulated time 
flow changes direction accumulation set zero 
way motion blowing trees accumulates significantly purposeful motion car driving road accumulates large flow 
optic flow computationally expensive short cut 
iteration displacement computed correlation matching process taken average flow object 
initially parameters frame count cumulative flow sum maximum max set zero 
algorithm determining motion salience cumulatively add displacements frame cumulative flow increment frame count 
frame cumulative displacement falls maximum value indicating change direction set zero 
objects displacements accumulate frames considered salient 
algorithm displayed robotics institute cmu vsam final report sum sum sum max max sum theta max max threshold salient salient moving object salience algorithm 
object type classification ultimate goal vsam effort able identify individual entities truck pm bus oakland fred smith 
object classification algorithms developed 
uses view dependent visual properties train neural network classifier recognize classes single human human group vehicles clutter section 
second method uses linear discriminant analysis determine provide finer distinction vehicle types van truck sedan colors section 
method successfully trained recognize specific types vehicles ups trucks campus police cars 
classification neural networks vsam testbed classifies moving object blobs general classes humans vehicles viewpoint specific neural networks trained camera 
neural network standard layer network 
learning network accomplished backpropagation algorithm 
input features network mixture image object parameters image blob perimeter area pixels image blob area pixels apparent aspect ratio blob bounding box camera zoom 
output classes human vehicle human group 
teaching network input blob human outputs set human set 
classes trained similarly 
input fit classes tree blowing wind outputs set 
results neural network interpreted follows robotics institute cmu vsam final report input layer hidden layer output layer teach pattern area vehicle single human multiple human aspect ratio reject target single human zoom magnification target camera neural network approach object classification 
class samples classified human human group vehicle false alarms total table results neural net classification vsam data output threshold classification maximum nn output classification reject neural network classification approach fairly effective single images advantages video temporal component 
exploit classification performed blob frame results classification kept histogram 
time step class label blob chosen described 
results classification scheme summarized table 
experimented features disambiguate human vehicle classes 
incorporated neural network classification expense having perform extra feature computation 
geolocation object estimated image location terrain map see section actual width height meters estimated image projection 
simple heuristic ratio values performs robotics institute cmu vsam final report surprisingly human group vehicle reject promising classification feature moving object determine rigid non rigid examining changes appearance multiple frames 
useful distinguishing rigid objects vehicles non rigid walking humans animals 
describe approach local computation optic flow boundaries moving object region 
gross displacement moving blob calculated section flow field computed pixels blob possible determine velocity pixels relative body motion simply subtracting gross motion gamma find residual flow 
expected rigid objects little residual flow non rigid object human exhibit independent motion 
average absolute residual flow pixel kr calculated magnitude value provides clue rigidity object motion time periodicity 
rigid objects vehicles display extremely low values moving objects humans display significantly residual flow periodic component 
rigidity frame number human vehicle average magnitude residual flow person top curve car bottom curve plotted time 
clearly human higher average residual flow frame rigid 
curve exhibits periodicity non rigid human gait 
robotics institute cmu vsam final report classification linear discriminant analysis developed method classifying vehicle types people linear discriminant analysis 
method sub modules classifying object shape determining color 
sub module computes independent discriminant classification space calculates class space weighted class nearest neighbor nn method 
calculate discriminant spaces linear discriminant analysis lda 
lda statistical tool discriminating groups clusters points multidimensional space 
lda called supervised clustering 
lda feature vectors computed training examples different object classes considered labeled points high dimensional feature space 
lda computes set discriminant functions formed linear combinations feature values best separate clusters points corresponding different object labels 
lda desirable properties reduces dimensionality data classes lda space separated possible meaning variance spread points class minimized variance classes spread cluster centroids maximized 
lda calculations proceed follows 
calculate average covariance matrix points class different classes ic gamma ic gamma gamma gamma number object classes number training examples class ic feature vector ith example class centroid vector class compute eigenvalues eigenvectors separation matrix gamma solving generalized eigenvalue problem gamma theta 
assume loss generality eigenvalues sorted delta delta delta dimensionality feature space 
eigenvector associated eigenvalue provides coefficients ith discriminant function maps feature vector coordinate discriminant space 
dimensionality reduction achieved considering largest eigenvalues eigenvectors mapping dimensional feature vector dimensional vector theta theta theta practice choose integer robotics institute cmu vsam final report line classification feature vector measured detected object transformed point discriminant space 
determine class object distance point points representing labeled training example examined closest labeled examples chosen 
nearest neighbors nn classification rule labels nearest neighbors provide votes label class new object distance provides weight vote 
class chosen class receives highest weighted vote 
due disparity numbers training samples class normalize number votes received class total number training examples class 
shape classification line learning process supervised learning process object type classification shape performed steps 
human operators collect sample shape images assign class labels 
experiment specify shape classes human single group sedan including wd van truck mule golf carts transport physical plant workers mainly noise 
labeled special objects vans ups vans police cars 
figures pages show sample input image chips object types 
total collected approximately sample shape images 

system calculates area center gravity width height motion blob sample image 
system calculates st nd rd order image moments blob axis axis images 
features comprise dimensional sample vector calculated image features 

system calculates discriminant space shape classification lda method described 
shape classification line classification process line classification phase system executes steps automatically 

system calculates area center gravity width height input image st nd rd order image moments axis axis forming dimensional vector motion blob 

corresponding point discriminant space computed linear combination feature vector values 

votes class determined consulting nearest neighbor point labels discriminant space described 
robotics institute cmu vsam final report color classification line learning process addition type object determined blob shape dominant color object classified lda 
observed color varies scene lighting conditions reason discrete set color classes chosen fairly invariant class wise outdoor lighting changes variation class learned lda 

human operators segment color samples training images divide classes red orange yellow green blue white silver gray black 
collected approximately images fine weather conditions images cloudy conditions 

system samples rgb intensity values pixels sample image 
system maps sampled rgb values color space values equations theta gamma theta theta gamma gamma theta system averages calculated values get single dimensional color feature vector image 

system calculates discriminant space color classification lda method described 
color classification line classification process line classification phase system executes steps automatically 

system measures rgb samples pixels axes input motion blob 

rgb values converted color space 

corresponding points discriminant space computed linear combination feature vector values euclidean distance color class summed 

votes class determined consulting nearest neighbor point labels discriminant space described 

color class associated shortest total euclidean distance chosen output color class 
robotics institute cmu vsam final report table cross validation results lda classification 
human sedan van truck mule total errors human sedan van truck mule avg 
results pages show sample training image chips different object types sample output classification procedure 
table shows cross validation evaluation objects columns classified results rows 
recognition accuracy roughly sunny cloudy weather conditions 
currently system raining interfere measured rgb values images 
reason system early mornings late due non representativeness lighting conditions 
system specular reflection vehicle bodies windows 
open problems solved 
robotics institute cmu vsam final report trucks left trucks right sample images lda learning trucks robotics institute cmu vsam final report vans right vans left vans left sample images lda learning vans robotics institute cmu vsam final report right left left sample images lda learning robotics institute cmu vsam final report sample images lda learning robotics institute cmu vsam final report ups ups police cars sample images lda learning special objects robotics institute cmu vsam final report final results classification process 
robotics institute cmu vsam final report activity analysis detecting objects classifying people vehicles determine objects doing 
opinion area activity analysis important open areas video understanding research 
developed prototype activity analysis procedures 
uses changing geometry detected motion blobs perform gait analysis walking running human beings section 
second uses markov model learning classify simple interactions multiple objects people meeting vehicle driving scene dropping section 
gait analysis detecting analyzing human motion real time video imagery viable algorithms pfinder 
algorithms represent step problem recognizing analyzing humans drawbacks 
general detecting features hands feet head tracking fitting prior human model cardboard model ju 
developed star skeletonization procedure analyzing human gaits 
key idea simple fast extraction broad internal motion features object employed analyze motion 
simple method employed robustly detect extremal points boundary object produce star skeleton 
star skeleton consists centroid motion blob local extremal points recovered traversing boundary blob see 
distance border position dft lpf inverse dft star skeleton shape centroid boundary unwrapped distance function centroid 
function smoothed extremal points extracted 
shows star skeletons extracted various objects 
clear form skeletonization provides sparse set points classify analyze robotics institute cmu vsam final report motion different types moving object 
human vehicle polar bear video image motion detection skeleton skeletonization different moving objects 
clear structure rigidity skeleton significant analyzing object motion 
technique analyze motion gait individual cyclic motion individual joint positions 
implementation person may fairly small blob image individual joint positions determined real time fundamental cyclic analysis performed 
cue gait object posture 
metric star skeleton possible determine posture moving human 
shows properties extracted skeleton 
uppermost skeleton segment assumed represent torso lower left segment assumed represent leg analyzed cyclic motion 
determination skeleton features 
angle left cyclic point leg vertical angle torso vertical 
shows human skeleton motion sequences walking running values cyclic point 
data acquired video frame rate hz 
comparing average values figures shows posture running person easily robotics institute cmu vsam final report frame sec skeleton motion walking person skeleton motion running person frame rad frame rad leg angle running person leg angle walking person frame frame rad rad torso angle running person torso angle walking person skeleton motion sequences 
clearly periodic motion provides cues object motion mean value distinguished walking person angle torso segment guide 
frequency cyclic motion leg segments provides cues type gait 
activity recognition multiple objects markov models developed prototype activity recognition method estimates activities multiple objects attributes computed low level detection tracking subsystems 
activity label chosen system maximizes probability observing attribute sequence 
obtain markov model introduced describes probabilistic relations attributes activities 
tested functionality method synthetic scenes human vehicle interaction 
test system continuous feature vector output low level detection tracking algorithms quantized discrete set attributes values tracked blob robotics institute cmu vsam final report ffl object class human vehicle ffl object action appearing moving stopped disappearing ffl interaction near activities labeled human entered vehicle human got vehicle human exited building human entered building vehicle parked human rendezvous 
train activity classifier conditional joint probabilities attributes actions obtained generating synthetic activity occurrences simulation measuring low level feature vectors distance velocity objects similarity object class category noise corrupted sequence object action classifications 
shows results scenes joint probability calculation 
results markov activity recognition synthetic scenes 
left person leaves building enters vehicle 
right vehicle parks person gets 
web page data summarization developed web data logging system see 
high traffic area data dozens people collected just minutes observation 
observation consists color thermal video multiple cameras best view image chips collateral information date time weather conditions temperature estimated subject trajectory camera acquisition parameters object classification results 
addition storing data evaluation debugging data logging system necessary vsam systems site monitoring operation 
data logging prototype observations explored web browsing cgi server vsam researchers access data 
ways view object activity information 
shows example activity report 
activity report shows labeled events car parked human entered building robotics institute cmu vsam final report target detection target classification activity monitoring tracker am human got vehicle am vehicle parked am human exited building human red vehicle sedan blue sedan red human yellow link activity database vsam database super spu camera server vsam ri cmu edu user web browser remote target database cgi camera web page data summarization system 
sorted time 
user wants detail hypertext link brings page showing image chip object class color information 
shows example object report 
objects seen system activities related shown page sorted time observation 
cut information overload user select specific subsets object classes view 
user selects object system automatically brings page showing objects class having similar color features 
way possible user detect vehicle person observed different places times surveillance site 
click detail activity report web page 
robotics institute cmu vsam final report click search object report web page 
airborne surveillance fixed ground sensor placement fine defensive monitoring static facilities depots warehouses parking lots 
cases sensor placement planned advance get maximum usage limited vsam resources 
battlefield large constantly shifting piece real estate may necessary move sensors order maximize utility battle unfolds 
airborne sensor platforms directly address concern self motion aircraft introduces challenging video understanding issues 
years program sarnoff developed surveillance technology detect track individual vehicles moving aircraft keep camera turret fixated ground point multitask camera separate geodetic ground positions 
airborne object tracking object detection tracking difficult problem moving sensor platform 
difficulty arises trying detect small blocks moving pixels representing independently moving object objects image shifting due self motion 
key success airborne sensor characterization removal self motion video sequence pyramid vision technologies real time video processor system 
new video frames stream processor registers warps new frame chosen image resulting cancelation pixel movement leading stabilized display appears motionless seconds 
stabilization problem moving object detection moving platform ideally reduced performing vsam stationary camera sense robotics institute cmu vsam final report moving objects readily apparent moving pixels image 
object detection tracking performed frame differencing image alignment register frame gamma frame gamma performed frames sec 
sample results shown 
circumstances remaining residual pixel motion due parallax caused significant scene structure trees 
removing parallax effects subject going research vision community 
detection small moving objects moving airborne sensor 
camera fixation aiming known human operators fatigue rapidly controlling cameras moving airborne ground platforms 
continually adjust turret keep locked stationary moving object 
additionally video continuously moving reflecting camera 
combination factors leads operator confusion 
sarnoff built image alignment techniques stabilize view camera turret automate camera control significantly reducing strain operator 
particular real time image alignment keep camera locked stationary moving point scene aim camera known geodetic coordinate imagery available 
details 
shows performance stabilization fixation algorithm ground points aircraft traverses approximate ellipse 
field view examples ffi aircraft took approximately minutes complete orbit 
air sensor multi tasking occasionally single camera resource track multiple moving objects fit single field view 
problem particularly relevant high altitude air platforms narrow field view order see ground objects reasonable robotics institute cmu vsam final report fixation target point fixation target point fixation target points 
images shown taken seconds fixation started 
large center cross hairs indicate center stabilized image point fixation resolution 
sensor multi tasking employed switch field view periodically target areas monitored 
process illustrated described detail 
footprints airborne sensor autonomously multi tasked disparate geodetic scene coordinates 
site models calibration geolocation automated surveillance system benefit greatly scene specific knowledge provided site model 
vsam tasks supported accurate site model ffl computation object geolocation section robotics institute cmu vsam final report ffl visibility analysis predicting portions scene visible cameras allow effective sensor tasking ffl geometric focus attention example task sensor monitor door building specify vehicles appear roads ffl suppression false alarms areas foliage ffl prediction visual effects shadows ffl visualization scene enable quick comprehension geometric relationships sensors objects scene features ffl simulation planning best sensor placement debugging algorithms ffl landmark camera calibration 
scene representations illustrates wide variety scene representations vsam testbed system past years 
variety due year effort bootstrapped representation bushy run site largely hand 
second third years project performed campus cmu compact terrain data base model campus ended supporting algorithmic needs 
usgs 
united states geological survey usgs produces digital mapping products create initial site model 
include digital quarter quad nadir looking image site look orthographic projection 
result image scene features appear correct horizontal positions 
digital elevation model dem image pixel values denote scene elevations corresponding horizontal positions 
grid cell usgs dem shown encompasses meter square area 
digital topographic map digital version popular usgs topo maps 
digital line graph dlg vector representations public cartographic features 
ordered directly usgs eros data center web site located url cr usgs gov 
ability existing mapping products usgs national imagery mapping agency nima bootstrap vsam site model demonstrates rapid deployment vsam systems monitor trouble spots globe feasible goal 
custom dem 
robotics institute autonomous helicopter group mounted high precision laser range finder remote control yamaha helicopter create high resolution half meter grid spacing dem bushy run site vsam demo 
raw radar returns collected respect known helicopter position orientation board data form cloud points representing returns surfaces scene 
points converted dem projecting horizontal coordinate bins computing mean standard deviation height values bin 
robotics institute cmu vsam final report variety site model representations vsam ifd testbed system usgs custom dem aerial mosaic vrml model site model spherical representations 
robotics institute cmu vsam final report mosaics 
central challenge surveillance sensor information human operator 
relatively narrow field view sensor difficult operator maintain sense context outside camera immediate image 
image mosaics moving cameras overcome problem providing extended views regions swept camera 
displays aerial mosaic demo bushy run site 
video sequence obtained flying demo site panning camera turret back forth keeping camera tilt constant 
vsam ifd team demonstrated coarse registration mosaic usgs projective warp determine approximate mapping mosaic pixels geographic coordinates 
feasible technology lead automated methods updating existing information fresh imagery fly 
example seasonal variations fresh case vsam demo integrated 
vrml models 
shows vrml model bushy run buildings surrounding terrain 
model created factorization method applied aerial ground video sequences 
compact terrain data base 
years vsam testbed system compact terrain data base model campus primary site model representation 
originally designed represent large terrain context advanced distributed simulation optimized efficiently answer geometric queries finding elevation point real time 
terrain represented grid elevations triangulated irregular network tin hybrid data bases containing representations allowed 
represents relevant cartographic features top terrain skin including buildings roads bodies water tree canopies 
shows small portion park cmu campus 
important benefit site model representation vsam processing allows easily interface synthetic environment simulation visualization tools provided modsaf 
spherical representations 
second year vsam testbed spu microsoft windows nt operating system supported software 
reason explored spherical lookup tables fixed mount spu 
seen stationary camera represented surface viewing sphere 
true camera allowed pan tilt focal point zoom image pan tilt zoom setting essentially discrete sample bundle light rays impinging camera focal point 
idea store spherical lookup table containing locations surface material types points intersection camera viewing rays site model 
third year changed windows linux operating system variant unix directly spu 
spherical lookup tables obsolete 
geospatial site coordinate systems interchangeably vsam testbed 
geodetic coordinate system provides frame standard unambiguous global true sense word 
unfortunately simple computations robotics institute cmu vsam final report distance points complicated function latitude longitude elevation 
reason site specific cartesian coordinate systems typically established handle bulk geometric model computations performed 
local vertical coordinate system origin base prb operator control center representing camera positions providing operator display map coordinate system 
model campus universal transverse mercator utm coordinates provide alternative cartesian coordinate system related rotation translation 
conversion geodetic utm coordinates straightforward interchangeably system 
camera calibration vsam system full geometric site model requires calibrating cameras respect model 
developed set calibration procedures specifically designed situ meaning place camera calibration 
believe cameras calibrated environment resembles actual operating conditions 
philosophy particularly relevant outdoor camera systems 
cameras get transport installation changes temperature humidity affect camera intrinsic parameters 
furthermore impossible recreate full range zoom focus settings useful outdoor camera system confines indoor lab 
amount site calibration necessary determining extrinsic parameters location orientation camera placement 
unfortunately outdoors ideal environment careful camera calibration 
cold rainy unpleasant 
simple calibration methods needed performed minimal human intervention 
gps landmarks measurements extrinsic camera calibration cmu campus 
robotics institute cmu vsam final report developed methods fitting projection model consisting intrinsic lens extrinsic pose parameters camera active pan tilt zoom control 
intrinsic parameters calibrated fitting parametric models optic flow induced rotating zooming camera 
calibration procedures fully automatic require precise knowledge scene structure 
extrinsic parameters calculated sighting sparse set measured landmarks scene see 
actively rotating camera measure landmarks virtual hemispherical field view leads conditioned exterior orientation estimation problem 
details calibration procedures 
model geolocation video understanding techniques described section operate primarily image space 
large leap terms descriptive power transforming image blobs measurements scene objects descriptors 
particular determination object location scene allows infer proper spatial relationships sets objects objects scene features roads buildings 
furthermore believe computation spatial geolocation key coherently integrating large number object hypotheses multiple widely spaced sensors 
elev ku kv kw projection ray ray vertical estimating object intersecting backprojected viewing rays terrain model 
traversal algorithm determines dem cell contains intersection viewing ray terrain 
regions multiple sensor viewpoints overlap object locations determined accurately wide baseline stereo triangulation 
regions scene institute cmu vsam final report viewed multiple sensors small percentage total area regard real outdoor surveillance applications desirable maximize coverage large area finite sensor resources 
determining object locations single sensor requires domain constraints case assumption object contact terrain 
contact location estimated passing viewing ray bottom object image intersecting model representing terrain see 
sequences location estimates time assembled consistent object trajectories 
previous uses ray intersection technique object localization surveillance research restricted small areas planar terrain relation image pixels terrain locations simple homography 
benefit camera calibration required determine back projection image point scene plane provided mappings coplanar scene points known 
large outdoor scene areas may contain significantly varied terrain 
handle situation perform geolocation ray intersection full terrain model provided example digital elevation map dem 
calibrated sensor image pixel corresponding assumed contact point object terrain viewing ray ku kv kw constructed sensor location unit vector designating direction viewing ray emanating sensor arbitrary distance 
general methods determining viewing ray intersects scene example ray tracing quite involved 
scene structure stored dem simple geometric traversal algorithm suggests known algorithm drawing digital line segments 
consider vertical projection viewing ray dem grid see 
starting grid cell containing sensor cell ray passes examined turn progressing outward elevation stored dem cell exceeds component viewing ray location 
component view ray location computed gamma gamma depending direction cosine larger 
approach viewing ray intersection localizes objects lie boundaries single dem grid cell 
precise sub cell location estimate obtained interpolation 
multiple intersections terrain required algorithm generate order increasing distance sensor cut distance 
see details 
geolocation evaluation evaluated geolocation accuracy cameras prb wean cmu campus laser tracking generate ground truth 
experiment run having person carry prism loops prb parking lot system logged time stamped horizontal positions estimated 
robotics institute cmu vsam final report system simultaneously tracked person prb wean cameras logging time stamped geolocation estimates camera 
ground truth trajectory overlaid geolocation estimates prb camera wean camera average prb wean estimates 
scales meters 
shows ground truth trajectory curve overlaid geolocation estimates prb camera wean camera average prb wean camera estimates corresponding time stamps 
cameras track trajectory fairly 
prb camera geolocation estimates large errors lower portions upper right arc loop person feet occluded parked vehicles walking areas 
higher elevation direction view wean camera allowed see person feet lower portion loop trajectory correctly followed 
error occurs top right loop person comes close vehicles reflected shiny surfaces 
pulls bounding box person feet causes underestimation position 
geolocation estimates averaged points time stamps agreeing small threshold far fewer points shown 
effect averaging smooth high variance portions curves distance accuracy noticeably improve 
geolocation estimates computed point located center lowest side bounding box enclosing moving blob 
system maintains running estimate variance point variance high position shape bounding box changes greatly tracking 
system computes internal estimate horizontal geolocation error projecting error box standard deviation image point estimation intersects terrain providing bound error geolocation estimate 
subsampling set error boxes shown cameras 
interesting note portions trajectory errors large due occlusions reflections system aware variance geolocation estimate high 
determine actual geolocation accuracy time stamp geolocation estimate compared list ground truth time stamps find suitably close correspondence 
cases corresponding ground truth point horizontal displacement error plotted prb camera wean camera average geolocation computed prb wean 
mean covariance point cloud estimated major robotics institute cmu vsam final report geolocation error boxes computed system trajectory estimates prb camera wean camera 
scales meters 
compare figures minor axes covariance ellipse overlaid plot length axes scaled represent times standard deviation point spread direction 
numeric standard deviations axis displayed table 
geolocation estimates max std meters min std meters prb wean avg prb wean numbers confirm observation averaging geolocation estimates cameras improving accuracy 
getting slightly worse 
referring see center error point spread 
averaging biased geolocation estimates camera noise estimate cancelling properly intensifying 
removing geolocation bias sensor necessary achieve accurate results averaging 
possible sources error camera calibration parameters terrain model small biases time stamps produced spu 
standard deviation geolocation estimates camera roughly order meters axis maximum spread roughly meters minimum 
confirmed axis maximum error camera oriented direction vector camera object observed 
model human computer interface keeping track people vehicles interactions chaotic area battlefield difficult task 
commander obviously shouldn looking dozen screens showing robotics institute cmu vsam final report plotted covariances horizontal displacement errors estimate ground truth locations corresponding time stamps 
prb camera wean camera average prb wean estimates corresponding time stamps 
scales meters 
raw video output amount sensory overload virtually guarantees information ignored requires prohibitive amount transmission bandwidth 
suggested approach provide interactive graphical visualization battlefield vsam technology automatically place dynamic agents representing people vehicles synthetic view environment 
approach benefit visualization object longer tied original resolution viewpoint video sensor synthetic replay dynamic events constructed high resolution texture mapped graphics perspective 
particularly striking amount data compression achieved transmitting symbolic object information back operator control unit raw video data 
currently process ntsc color imagery frame size pixels frames second pentium ii computer data streaming system sensor rate roughly mb second sensor 
vsam processing detected object hypotheses contain information object type location velocity measurement statistics time stamp description sensor current pan tilt zoom example 
object data packet takes roughly bytes 
sensor tracks objects second frames second ends transmitting bytes back ocu reduction data bandwidth 
ultimately key comprehending large scale multi agent events full immersive visualization allows human operator fly environment view dynamic events unfolding real time viewpoint 
envision graphical user interface cartographic modeling visualization tools developed synthetic environments se community 
site model model vsam processing visualization represented compact terrain database 
objects inserted dynamic agents site model viewed distributed interactive simulation clients modular semi automated forces modsaf program associated immersive viewer 
demonstrated proof concept idea battle space battle robotics institute cmu vsam final report lab simulation center fort georgia part april vsam workshop 
april researchers cmu set portable vsam system mobile operations urban terrain training site 
camera set corner building roof geodetic coordinates measured previous survey height camera known location measured 
camera mounted pan tilt head turn mounted leveled tripod fixing roll tilt angles pan tilt sensor assembly zero 
yaw angle horizontal orientation sensor assembly measured sighting digital compass 
processing troop exercises log files containing camera calibration information object hypothesis data packets sent ftp back cmu processed determine time stamped list moving objects 
week information brought back simulation center assistance colleagues bdm played back vsam workshop attendees custom software broadcast time sequenced simulated entity packets network display modsaf 
processed vsam video data screen dumps resulting synthetic environment shown 
sample synthetic environment visualizations data collected site 
automated tracking people 
modsaf orthographic map display estimated 
tracking soldier walking town 
immersive texture mapped visualization event seen user specified viewpoint 
demonstrated visualization process form basis real time immersive visualization tool 
ported object geolocation computation vsam spu platforms 
allowed estimates object geolocation computed frame frame tracking process transmitted data packets back ocu 
ocu incoming object identity geolocation data distributed interactive simulation dis packets understood modsaf clients re broadcast multicast network 
point objects detected viewable short lag context full site model viewer 
robotics institute cmu vsam final report real time visualization objects detected classified vsam testbed system transmitted dis packets network 
sensor coordination complex outdoor scenes impossible single sensor maintain view object long periods time 
objects occluded environmental features trees buildings sensors limited effective fields regard 
promising solution problem network video sensors cooperatively track objects scene 
developed demonstrated methods sensor coordination vsam testbed 
objects tracked long distances occlusion handing cameras situated object trajectory 
second wide angle sensors keeping track objects large area task active pan tilt zoom sensors get better view selected objects process known sensor 
multi sensor handoff little done autonomously coordinating multiple active video sensors cooperatively track moving object 
approach matsuyama controlled indoor environment cameras lock particular object moving floor 
approach problem generally object geolocation computed section determine sensor look 
pan tilt zoom closest sensors controlled bring object fields view viewpoint independent cost function determine moving objects find specific object interest 
steps described 
assume time sensor pan tilt value tasked track particular object ground location velocity function converts ground coordinate pan tilt point determined camera calibration object location converted desired sensor pan tilt value 
behavior pan tilt unit robotics institute cmu vsam final report approximated linear system infinite acceleration maximum velocity sigma sigma sigma gamma sigma gamma substituting desired sensor pan tilt left hand side equation solving gamma yields prediction acquisition time long take pan tilt device point object current location 
object moved trajectory time 
new object position estimated gamma predicted object position converted new desired sensor pan tilt procedure iterates time increments gamma small convergence start increase divergence 
algorithm guarantees converges sensor able reacquire object 
appropriate camera zoom setting determined directly desired size object projection image 
knowing classification object determined section employ heuristic humans approximately feet tall vehicles approximately feet long 
position object sensor range object angle subtended image object approximately ae tan gamma human tan gamma vehicle knowing focal length sensor function zoom determined camera calibration appropriate zoom setting easily chosen 
sensor pointing right direction right zoom factor moving objects extracted compared specific object interest see match 
need re acquire specific object key feature necessary multi camera cooperative surveillance 
obviously viewpoint specific appearance criteria useful new view object may significantly different previous view 
recognition features needed independent viewpoint 
criteria object scene trajectory determined geolocation normalized color histogram object image region 
candidate motion regions tested applying matching cost function manner similar described section 
example multi sensor hand track vehicle travels campus shown 
diagram shows continuous autonomous tracking single object distance approximately time approximately minutes 
sensors cooperatively track object 
time shown object occluded sensor visible sensor continues track 
object moves occlusion area sensor automatically track shown 
robotics institute cmu vsam final report object sensor sensor sensor object sensor sensor sensor object sensor sensor sensor object sensor sensor sensor cooperative multi sensor tracking see text description 
robotics institute cmu vsam final report object moves field regard sensors third sensor automatically tasked continue surveillance shown 
automatically managing multiple redundant camera resources vehicle continuously tracked complex urban environment 
sensor second form sensor cooperation sensor 
term sensor denote wide field view camera control second active camera zoom actively follow subject generate better view 
motivation keep track objects scene simultaneously gathering high resolution views selected objects 
camera system master camera slave camera 
master camera set global view scene track objects extended areas simple tracking methods adaptive background subtraction 
object trajectory generated master camera relayed slave camera real time 
slave camera highly zoomed follow trajectory generate close imagery object 
relatively simple exercise cameras calibrated respect local terrain model 
shown section person trajectory determined reasonable accuracy roughly meter error person meters away intersecting backprojected viewing rays terrain 
estimating location person camera viewpoint easy matter transform location pan tilt command control second camera 
shows example camera 
person detected automatically wide angle view shown left image second camera tasked move slightly ahead person estimated trajectory shown right image 
cameras located far apart geographically obvious need camera calibration accurate site model 
developed sensor method works closely located cameras 
method requires image computations geolocation computation extrinsic camera calibration 
furthermore intrinsic parameters needed slave camera determine pan tilt angles needed point pixel image 
basic idea form mosaic warping master camera view pixel coordinate system slave camera view 
image trajectories objects detected master view transformed trajectories overlaid slave camera view 
slave camera compute pan tilt angles necessary keep object zoomed field view 
robotics institute cmu vsam final report example camera 
left wide angle view person detected 
right better view second camera tasked intercept person estimated path 
images taken slave camera master camera respectively 
shows master camera view warped pixel coordinate system slave camera view form image mosaic 
image pixels averaged directly overlapping region 
robotics institute cmu vsam final report years vsam milestones current vsam ifd testbed system suite video understanding technologies result year evolutionary process 
impetus evolution provided series yearly demonstrations 
tables provide succinct synopsis progress years areas video understanding technology vsam testbed architecture sensor control algorithms degree user interaction 
program vsam ifd testbed continues provide valuable resource development testing new video understanding capabilities 
directed achieving goals ffl better understanding human motion including segmentation tracking articulated body parts ffl improved data logging retrieval mechanisms support system operations ffl bootstrapping functional site models passive observation scene activities ffl better detection classification multi agent events activities ffl better camera control enable smooth object tracking high zoom ffl acquisition selection best views eventual goal recognizing individuals scene 
robotics institute cmu vsam final report table progression video understanding technology video understanding demo results demo results demo results ground moving target detection mtd tracking multiple target detection single target step stare tracking temporal change adaptive template matching multi target mtd trajectory analysis motion salience temporal consistency adaptive background subtraction layered adaptive background subtraction robust detection mtd panning tilting zooming optic flow image registration target tracking multi hypothesis kalman filter airborne mtd tracking stabilization temporal change correlation real time camera pointing motion plus appearance drift free fixation ground target geolocation ray intersection dem ray intersection seeds model geolocation uncertainty estimation kalman filtering domain knowledge airborne target geolocation video image registration fine aiming video image registration real time target recognition temporal salience predicted trajectory spatio temporal salience color histogram classification target patterns spatio temporal signature target classification technique aspect ratio motion skeletonization neural network spatio temporal salience patterns inside image chips spurious motion rejection model recognition linear discriminant analysis target classification categories human vehicle human human group vehicle human human group sedan van truck mule van ups van police car target classification accuracy percentage correctly identified vehicle human small sample large sample large sample activity monitoring motion individual target behaviors multiple target behaviors parking lot monitoring getting cars entering buildings ground truth verification line line target geolocation accuracy meters meters meter camera calibration tens pixels pixels ones pixels domain knowledge elevation map hand drawn road network seeds model generate ray occlusion tables line parking area road network occlusion boundaries robotics institute cmu vsam final report table progression vsam architecture goals vsam architecture demo results demo results demo results number types sensors standard video camera fixed focal length standard video camera zoom static color cameras color video cameras pan tilt zoom thermal types spu vsam nodes slow relocatable airborne fast relocatable fixed mount airborne visualization clients super spu handling multiple cameras web vis node system coverage rural km area ground km airborne coverage university campus km area ground airborne coverage km urban area dense coverage university campus km ground area interest architecture dedicated ocu spu variable packet protocol table progression vsam sensor control sensor control demo results demo results demo results ground sensor aiming hand multitasking predetermined handoff regions coordinates signatures epipolar constraints occlusion footprint databases camera camera handoff wide angle air sensor aiming video image registration landmark points video image registration landmark points ground air interaction human directed predetermined locations ocu directed target geolocation spu behavior single supervised task track target primitive unsupervised behavior look target single task supervision track activity unsupervised behavior detection multi task supervision activity monitoring complex unsupervised behavior parking lot monitoring robotics institute cmu vsam final report table progression user interaction user interaction demo results demo results demo results site model usgs dem lidar real time mosaics compact terrain database spherical mosaics aerial mosaic improved model site model function visualization geolocation line demo scenario planning action review algorithm evaluation ground truth verification 
line relocatable sensor planning geolocation occlusion analysis target geolocation line sensor placement planning virtual spu scenario perturbation analysis 
line ground truth verification dynamic visualization system tasking system tasking user user interface point click camera control sensor tasking point click camera control region target tasking 
tracked object specification interactive activity event tasking visualization overlay current target sensor positions live video feeds target sensor information shown gui display computer live video feeds visualization sensor network video archiving replaying significant events java visualization nodes indexed web access activity report live internet access vsam network web visualization nodes robotics institute cmu vsam final report acknowledgments authors army night vision electronic sensors directorate lab team davison ft virginia help airborne operations 
chris kearns andrew assistance fort site steve joe bdm tec help site model distributed simulation visualization software 
anderson peter burt van der wal 
change detection tracking pyramid transformation techniques 
proceedings spie intelligent robots computer vision volume pages 
american society photogrammetry asp 
manual photogrammetry 
fourth edition american society photogrammetry falls church 
barron fleet beauchemin 
performance optical flow techniques 
international journal computer vision 
bergen anandan hanna hingorani 
hierarchical model motion estimation 
proceedings european conference computer vision 
bradshaw reid murray 
active recovery motion trajectories prediction 
ieee transactions pattern analysis machine intelligence march 
collins miller lipton 
dem determine geospatial object trajectories 
proceedings darpa image understanding workshop pages november 
collins 
calibration outdoor active camera system 
proceedings conference computer vision pattern recognition pages 
ieee computer society june 
dellaert collins 
fast image tracking selective pixel integration 
iccv workshop frame rate applications september 
bannon 
autonomous scene monitoring system 
proc 
th annual joint government industry security technology symposium 
american defense preparedness association june 
lipton 
real time human motion analysis image skeletonization 
proceedings workshop applications computer vision 
robotics institute cmu vsam final report geometric geodesy branch 
geodetic survey 
publication phase ii interim terrain data fort georgia may 
hansen anandan dana van der wal burt 
real time scene stabilization mosaic construction 
proc 
workshop applications computer vision 
haritaoglu larry davis harwood 




real time system tracking people 
fgr 
isard blake 
contour tracking stochastic propagation conditional density 
proceedings european conference computer vision pages 
institute simulation training ist 
standard distributed interactive simulation application protocols version 
university central florida division sponsored research march 
ju black yacoob 
cardboard people parameterized model articulated image motion 
proceedings international conference face gesture analysis 
kanade collins lipton anandan burt 
cooperative multisensor video surveillance 
proceedings darpa image understanding workshop volume pages may 
kanade collins lipton burt wixson 
advances cooperative multisensor video surveillance 
proceedings darpa image understanding workshop volume pages november 
koller nagel 
model object tracking monocular image sequences road traffic scenes 
international journal computer vision june 
lipton patil 
moving target detection classification realtime video 
proceedings workshop applications computer vision 
alan lipton 
local application optic flow analyse rigid versus non rigid motion 
iccv workshop frame rate applications september 
matsuyama 
cooperative distributed vision 
proceedings darpa image understanding workshop volume pages november 
sawhney hsu kumar 
robust video mosaicing topology inference local global alignment 
proc 
european conference computer vision 
sawhney kumar 
true multi image alignment application mosaicing lens distortion 
proc 
ieee conference computer vision pattern recognition 
robotics institute cmu vsam final report tomasi kanade 
shape motion image streams factorization method 
international journal computer vision 
toyama krumm brumitt meyers 
wallflower principles practice background maintenance 
proc 
international conference computer vision pages 
wixson hansen mishra 
image alignment precise camera fixation aim 
proc 
ieee conference computer vision pattern recognition 
wixson selinger 
classifying moving objects rigid non rigid 
proc 
darpa image understanding workshop 
wren azarbayejani darrell alex pentland 
pfinder real time tracking human body 
ieee transactions pattern analysis machine intelligence 
robotics institute cmu vsam final report 
