ensemble methods machine learning thomas dietterich oregon state university corvallis oregon usa tgd cs orst edu www home page www cs orst edu tgd 
ensemble methods learning algorithms construct set classifiers classify new data points weighted vote predictions 
original ensemble method bayesian averaging algorithms include error correcting output coding bagging boosting 
reviews methods explains ensembles perform better single classifier 
previous studies comparing ensemble methods reviewed new experiments uncover reasons adaboost overfit rapidly 
consider standard supervised learning problem 
learning program training examples form xm ym unknown function 
values typically vectors form hx components discrete real valued height weight color age 
called features notation ij refer th feature situations drop subscript implied context 
values typically drawn discrete set classes kg case classification real line case regression 
chapter consider classification 
training examples may corrupted random noise 
set training examples learning algorithm outputs classifier 
classifier hypothesis true function new values predicts corresponding values 
denote classifiers hl ensemble classifiers set classifiers individual decisions combined way typically weighted unweighted voting classify new examples 
active areas research supervised learning study methods constructing ensembles classifiers 
main discovery ensembles accurate individual classifiers 
necessary sufficient condition ensemble classifiers accurate individual members classifiers accurate diverse hansen salamon 
accurate classifier error rate better random guessing new values 
classifiers diverse different errors new data points 
see accuracy diversity imagine ensemble classifiers fh consider new case classifiers identical diverse wrong wrong 
errors classifiers uncorrelated wrong may correct majority vote correctly classify precisely error rates hypotheses equal errors independent probability majority vote wrong area binomial distribution hypotheses wrong 
shows simulated ensemble hypotheses having error rate 
area curve hypotheses simultaneously wrong error rate individual hypotheses 
number classifiers error fig 

probability exactly hypotheses error assuming hypothesis error rate errors independently hypotheses 
course individual hypotheses uncorrelated errors rates exceeding error rate voted ensemble increase result voting 
key successful ensemble methods construct individual classifiers error rates errors somewhat uncorrelated 
formal characterization problem intriguing address question possible practice construct ensembles 
fortunately possible construct ensembles 
fundamental reasons 
reason statistical 
learning algorithm viewed searching space hypotheses identify best hypothesis space 
statistical problem arises amount training data available small compared size hypothesis space 
sufficient data learning algorithm find different hypotheses give accuracy training data 
constructing ensemble accurate classifiers algorithm average votes reduce risk choosing wrong classifier 
top left depicts situation 
outer curve denotes hypothesis space inner curve denotes set hypotheses give accuracy training data 
point labeled true hypothesis see averaging accurate hypotheses find approximation statistical computational representational fig 

fundamental reasons ensemble may better single classifier second reason computational 
learning algorithms performing form local search may get stuck local optima 
example neural network algorithms employ gradient descent minimize error function training data decision tree algorithms employ greedy splitting rule grow decision tree 
cases training data statistical problem absent may difficult computationally learning algorithm find best hypothesis 
optimal training neural networks decisions trees np hard rivest blum rivest 
ensemble constructed running local search different starting points may provide better approximation true unknown function individual classifiers shown top right 
third reason representational 
applications machine learning true function represented hypotheses forming weighted sums hypotheses drawn may possible expand space representable functions 
bottom depicts situation 
representational issue somewhat subtle learning algorithms principle space possible classifiers 
example neural networks decision trees flexible algorithms 
training data explore space possible classifiers people proved asymptotic representation theorems hornik stinchcombe white 
finite training sample algorithms explore finite set hypotheses searching find hypothesis fits training data 
consider space effective space hypotheses searched learning algorithm training data set 
fundamental issues important ways existing learning algorithms fail 
ensemble methods promise reducing eliminating key shortcomings standard learning algorithms 
methods constructing ensembles methods constructing ensembles developed 
review general purpose methods applied different learning algorithms 
bayesian voting enumerating hypotheses bayesian probabilistic setting hypothesis defines conditional probability distribution yjx 
new data point training sample problem predicting value viewed problem computing 
rewrite weighted sum hypotheses view ensemble method ensemble consists hypotheses weighted posterior probability 
bayes rule posterior probability proportional likelihood training data times prior probability learning problems possible completely enumerate compute normalization evaluate bayesian committee 
furthermore true function drawn bayesian voting scheme optimal 
bayesian voting primarily addresses statistical component ensembles 
training sample small hypotheses significantly large posterior probabilities voting process average marginalize away remaining uncertainty training sample large typically hypothesis substantial posterior probability ensemble effectively shrinks contain single hypothesis 
complex problems enumerated possible approximate bayesian voting drawing random sample hypotheses distributed 
markov chain monte carlo methods neal seeks develop set tools task 
idealized aspect bayesian analysis prior belief 
prior completely captures knowledge obtain definition better 
practice difficult construct space assign prior captures prior knowledge adequately 
chosen computational convenience known inadequate 
cases bayesian committee optimal ensemble methods may produce better results 
particular bayesian approach address computational representational problems significant way 
manipulating training examples second method constructing ensembles manipulates training examples generate multiple hypotheses 
learning algorithm run times time different subset training examples 
technique works especially unstable learning algorithms algorithms output classifier undergoes major changes response small changes training data 
decision tree neural network rule learning algorithms unstable 
linear regression nearest neighbor linear threshold algorithms generally stable 
straightforward way manipulating training set called bagging 
run bagging presents learning algorithm training set consists sample training examples drawn randomly replacement original training set items 
training set called bootstrap replicate original training set technique called bootstrap aggregation term bagging derived breiman 
bootstrap replicate contains average original training set training examples appearing multiple times 
training set sampling method construct training sets leaving disjoint subsets training data 
example training set randomly divided disjoint subsets 
overlapping training sets constructed dropping different subsets 
procedure employed construct training sets fold crossvalidation ensembles constructed way called committees munro doyle 
third method manipulating training set illustrated adaboost algorithm developed freund schapire 
bagging adaboost manipulates training examples generate multiple hypotheses 
adaboost maintains set weights training examples 
iteration learning algorithm invoked minimize weighted error training set returns hypothesis weighted error computed applied update weights training examples 
effect change weights place weight training examples misclassified weight examples correctly classified 
subsequent iterations adaboost constructs progressively difficult learning problems 
final classifier constructed weighted vote individual classifiers 
classifier weighted accuracy weighted training set trained 
research schapire singer shown adaboost viewed stage wise algorithm minimizing particular error function 
define error function suppose training example labeled gamma corresponding positive negative examples 
quantity positive correctly classifies negative 
quantity called margin classifier training data 
adaboost seen trying minimize exp gammay negative exponential margin weighted voted classifier 
viewed attempting maximize margin training data 
manipulating input features third general technique generating multiple classifiers manipulate set input features available learning algorithm 
example project identify volcanoes venus trained ensemble neural networks 
networks different subsets available input features different network sizes 
input feature subsets selected hand group features different image processing operations principal component analysis fast fourier transform 
resulting ensemble classifier able match performance human experts identifying volcanoes 
tumer ghosh applied similar technique sonar dataset input features 
deleting input features hurt performance individual classifiers voted ensemble perform 
obviously technique works input features highly redundant 
manipulating output targets fourth general technique constructing ensemble classifiers manipulate values learning algorithm 
dietterich bakiri describe technique called error correcting output coding 
suppose number classes large 
new learning problems constructed randomly classes subsets input data re labeled original classes set derived label original classes set derived label 
relabeled data learning algorithm constructs classifier repeating process times generating different subsets obtain ensemble classifiers hl new data point classify 
answer classify class receives vote 
class receives vote 
classifiers voted class highest number votes selected prediction ensemble 
equivalent way thinking method class encoded bit codeword bit th learned classifier attempts predict bit codewords 
classifiers applied classify new point predictions combined bit string 
choose class codeword closest hamming distance bit output string 
methods designing errorcorrecting codes applied choose codewords equivalently subsets 
dietterich bakiri report technique improves performance decision tree algorithm backpropagation neural network algorithm variety difficult classification problems 
schapire shown adaboost combined error correcting output coding yield excellent ensemble classification method calls adaboost 
oc 
performance method superior ecoc method bagging essentially quite complex algorithm called adaboost 
main advantage adaboost oc implementation simplicity learning algorithm solving class problems 
ricci aha applied method combines error correcting output coding feature selection 
learning classifier apply feature selection techniques choose best features learning classifier 
obtained improvements tasks approach 
injecting randomness general purpose method generating ensembles classifiers inject randomness learning algorithm 
backpropagation algorithm training neural networks initial weights network set randomly 
algorithm applied training examples different initial weights resulting classifier quite different kolen pollack 
common way generating ensembles neural networks manipulating training set may effective 
study munro doyle compared technique bagging fold cross validated committees 
cross validated committees worked best bagging second best multiple random initial weights third best synthetic data set medical diagnosis data sets 
decision tree algorithm easy inject randomness kwok carter dietterich 
key decision choose feature test internal node decision tree 
internal node applies criterion known information gain ratio rank order various possible feature tests 
chooses top ranked feature value test 
discrete valued features values decision tree splits data subsets depending value chosen feature 
real valued features decision tree splits data subsets depending value chosen feature chosen threshold 
dietterich implemented variant chooses randomly equal probability top best tests 
compares performance single run ensembles classifiers different data sets 
data set point plotted 
point lies diagonal line ensemble lower error rate 
see nearly points lie line 
statistical analysis shows randomized trees statistically significantly better single decision tree data sets statistically remaining data sets 
ali pazzani injected randomness foil algorithm learning prolog style rules 
foil works somewhat ranks possible conditions add rule information gain criterion 
ali pazzani percent error fig 

comparison error rate ensemble decision trees constructed injecting randomness uniform vote 
computed candidate conditions scored top ranked candidate applied weighted random choice algorithm choose 
compared ensembles classifiers single run foil statistically significant improvements tasks statistically significant loss performance task 
obtained similar results fold cross validation construct training sets 
raviv intrator combine bootstrap sampling training data injecting noise input features learning algorithm 
train member ensemble neural networks draw training examples replacement original training data 
values training example perturbed adding gaussian noise input features 
report large improvements synthetic benchmark task medical diagnosis task 
note markov chain monte carlo methods constructing bayesian ensembles injecting randomness learning process 
uniform vote randomized decision trees hypothesis receives vote proportional posterior probability 
comparing different ensemble methods experimental studies performed compare ensemble methods 
largest studies bauer kohavi dietterich 
table summarizes results dietterich study 
table shows adaboost gives best results 
bagging randomized trees give similar performance randomization able better cases bagging large data sets 
table 
pairwise combinations ensemble methods 
cell contains number wins losses ties algorithm row algorithm column 
adaboost bagged random bagged adaboost data sets study little noise 
artificial classification noise added domains bagging adaboost gave different performance results shifted radically shown table 
conditions adaboost overfits data badly bagging shown presence noise 
randomized trees 
table 
pairwise combinations adaboosted bagged randomized domains synthetic class label noise 
cell contains number wins losses ties algorithm row algorithm column 
adaboost bagged random bagged adaboost key understanding results return shortcomings existing learning algorithms statistical support computation representation 
decision tree algorithm problems arise 
decision trees essentially partition input feature space rectangular regions sides perpendicular coordinate axes 
rectangular region corresponds leaf node tree 
true function represented small decision tree ensemble 
true function correctly represented large decision tree need large training data set order find fit statistical problem arise 
computational problem arises finding best smallest decision tree consistent training data computationally intractable series decisions greedily 
decisions incorrectly training data incorrectly partitioned subsequent decisions affected 
highly unstable small changes training set produce large changes resulting decision tree 
representational problem arises rectangular partitions input space 
true decision boundaries orthogonal coordinate axes requires tree infinite size represent boundaries correctly 
interestingly voted combination small decision trees equivalent larger single tree ensemble method construct approximation diagonal decision boundary small trees 
shows example 
left side plotted decision boundaries constructed decision trees uses internal nodes 
right boundary results simple majority vote trees 
equivalent single tree internal nodes accurate individual trees 
class class class class fig 

left shows true diagonal decision boundary staircase approximations kind created decision tree algorithms 
right shows voted decision boundary better approximation diagonal boundary 
consider algorithms adaboost bagging randomized trees 
bagging randomization construct decision tree independently 
bagging accomplishes manipulating input data randomization directly alters choices 
methods acting somewhat bayesian voting sampling space possible hypotheses bias hypotheses give accuracy training data 
consequently main effect address statistical problem lesser extent computational problem 
directly attempt overcome representational problem 
contrast adaboost constructs new decision tree eliminate residual errors properly handled weighted vote previously constructed trees 
adaboost directly trying optimize weighted vote 
making direct assault representational problem 
di rectly optimizing ensemble increase risk overfitting space ensembles usually larger hypothesis space original algorithm 
explanation consistent experimental results 
low noise cases adaboost gives performance able optimize ensemble overfitting 
high noise cases adaboost puts large amount weight mislabeled examples leads overfit badly 
bagging randomization noisy noise free cases focusing statistical problem noise increases statistical problem 
understand large datasets randomization expected better bagging bootstrap replicates large training set similar training set learned decision tree diverse 
randomization creates diversity conditions risk generating low quality decision trees 
despite plausibility explanation important open question concerning adaboost 
adaboost aggressively attempts maximize margins training set doesn overfit 
part explanation may lie stage wise nature adaboost 
iteration training examples constructs new hypothesis chooses weight hypothesis 
backs modifies previous choices hypotheses weights compensate new hypothesis 
test explanation conducted series simple experiments synthetic data 
true classifier simple decision rule tests just feature feature assigns example class feature class gamma feature 
construct training testing examples generating feature vectors length random follows 
generate feature important feature random 
generate features randomly agree feature probability disagree 
assign labels training example true function random classification noise 
creates difficult learning problem simple decision rules kind decision stumps features correlated class 
large ensemble able problem voting separate decision stumps feature 
constructed version adaboost works aggressively standard adaboost 
new hypothesis constructed weight assigned version performs gradient descent search minimize negative exponential margin equation 
algorithm weights learned hypotheses new hypothesis added 
training examples reflect revised hypothesis weights 
shows results training training set size 
plot confirms explanation 
aggressive adaboost initially higher error rates test set standard adaboost 
gradually improves 
standard adaboost initially obtains excellent performance test set overfits classifiers added ensemble 
limit ensembles representational properties minimizing function equation 
see exceptionally performance standard adaboost problem due stage wise optimization process slow fit data 
iterations adaboost standard adaboost aggressive adaboost fig 

aggressive adaboost exhibits worse performance standard adaboost challenging synthetic problem ensembles established method obtaining highly accurate classifiers combining accurate ones 
provided brief survey methods constructing ensembles reviewed fundamental reasons ensemble methods able perform single classifier ensemble 
provided experimental results elucidate reasons adaboost performs 
open question discussed concerns interaction adaboost properties underlying learning algorithm 
learning algorithms combined adaboost algorithms global character algorithms learn relatively lowdimensional decision boundary 
interesting see local algorithms radial basis functions nearest neighbor methods profitably combined adaboost yield interesting new learning algorithms 
bibliography ali pazzani 

error reduction learning multiple descriptions 
machine learning 
bauer kohavi 

empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
blum rivest 

training node neural network npcomplete extended 
proceedings workshop computational learning theory pp 
san francisco ca 
morgan kaufmann 
breiman 

bagging predictors 
machine learning 


human expert level performance scientific image analysis task system combined artificial neural networks 
chan 
ed working notes aaai workshop integrating multiple learned models pp 

available www cs fit edu 
dietterich 

experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning 
dietterich bakiri 

solving multiclass learning problems error correcting output codes 
journal artificial intelligence research 
freund schapire 

decision theoretic generalization line learning application boosting 
tech 
rep bell laboratories murray hill nj 
freund schapire 

experiments new boosting algorithm 
proc 
th international conference machine learning pp 

morgan kaufmann 
hansen salamon 

neural network ensembles 
ieee trans 
pattern analysis machine intell 
hornik stinchcombe white 

universal approximation unknown mapping derivatives multilayer feedforward networks 
neural networks 
rivest 

constructing optimal binary decision trees np complete 
information processing letters 
kolen pollack 

back propagation sensitive initial conditions 
advances neural information processing systems vol 
pp 
san francisco ca 
morgan kaufmann 
kwok carter 

multiple decision trees 
levitt lemmer 
eds uncertainty artificial intelligence pp 

elsevier science amsterdam 
neal 

probabilistic inference markov chain monte carlo methods 
tech 
rep crg tr department computer science university toronto toronto ca 
munro doyle 

improving committee diagnosis resampling techniques 
touretzky mozer 
eds advances neural information processing systems vol 
pp 
cambridge ma 
mit press 
raviv intrator 

bootstrapping noise effective regularization technique 
connection science 
ricci aha 

extending local learners error correcting output codes 
tech 
rep naval center applied research artificial intelligence washington schapire 

output codes boost multiclass learning problems 
proceedings fourteenth international conference machine learning pp 
san francisco ca 
morgan kaufmann 
schapire freund bartlett lee 

boosting margin new explanation effectiveness voting methods 
fisher 
ed machine learning proceedings fourteenth international conference 
morgan kaufmann 
schapire singer 

improved boosting algorithms confidence rated predictions 
proc 
th annu 
conf 
comput 
learning theory pp 

acm press new york ny 
tumer ghosh 

error correlation error reduction ensemble classifiers 
connection science 
