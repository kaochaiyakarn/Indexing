massachusetts institute technology artificial intelligence laboratory memo february memo statistical models occurrence data thomas hofmann jan puzicha publication retrieved anonymous ftp publications ai mit edu 
modeling predicting occurrences events fundamental problem unsupervised learning 
contribution develop statistical framework analyzing occurrence data general setting elementary observations joint occurrences pairs objects nite sets 
main challenge statistical models context overcome inherent data sparseness estimate probabilities pairs rarely observed unobserved sample set 
considerable interest extract grouping structure nd hierarchical data organization 
novel family mixture models proposed explain observed data nite number shared aspects clusters 
provides common framework statistical inference structure discovery includes proposed models special cases 
adopting maximum likelihood principle em algorithms derived model parameters 
develop improved versions em largely avoid tting problems overcome inherent locality em optimization 
broad variety possible applications information retrieval natural language processing data mining computer vision chosen document retrieval statistical analysis noun adjective occurrence unsupervised segmentation textured images test evaluate proposed algorithms 
copyright massachusetts institute technology report describes research accomplished center biological computational learning arti cial intelligence laboratory massachusetts institute technology university bonn cape cod 
thomas hofmann supported faculty discretionary fund 
jan puzicha supported german research foundation dfg bu 
ultimate goal statistical modeling explain observed data probabilistic model 
order serve useful explanation model reduce complexity raw data er certain degree simpli cation 
sense statistical modeling related information theoretic concept minimum description length 
model explanation data encoding model describing data conditioned model yields signi cant reduction encoding complexity compared direct encoding data 
complexity considerations particular relevant type data investigated best described term occurrence data cod 
general setting follows suppose nite sets fx xng fy objects arbitrary labeling 
elementary observations consider pairs xi yj joint occurrence object xi object yj 
data numbered collected sample set lg arbitrary ordering 
information completely characterized su cient statistics nij jf xi yj measure frequency occurrence xi yj 
important special case cod histogram data object xi xis characterized distribution measured features yj 
obvious partitioning subsets si component sample set si represent empirical distribution jji jji nij ni ni 
occurrence data distinctive application 
important example information retrieval may correspond collection documents set keywords 
nij denotes number occurrences word yj document xi 
consider application computational linguistics sets correspond words part binary syntactic structure direct objects nouns corresponding adjectives :10.1.1.14.7663
computer vision may correspond image locations discretized categorical feature values 
local histograms jji image neighborhood xi utilized subsequent image segmentation 
examples data mining molecular biology preference analysis enumerated stress analyzing occurrences events fact general fundamental problem unsupervised learning 
contribution general statistical framework cod 
rst glance may statistical models cod trivial 
consequence arbitrariness object labeling sets purely nominal scale ordering properties frequencies nij capture know data 
intrinsic problem cod data sparseness known zero frequency problem 
large majority pairs xi yj small probability occurring counts nij typically zero signi cantly corrupted sampling noise ect largely independent underlying probability distribution 
normalized frequencies predicting events large number occurrences observed judged impossible data sparseness problem urgent inthe case higher order cod triplets quadruples observed 
domain natural language processing large text corpora available rarely data completely avoid problem 
typical state art techniques natural language processing apply smoothing techniques deal zero frequencies unobserved events 
prominent techniques example back method simpler lower order models model interpolation held data 
class methods similarity local smoothing techniques proposed essen steinbiss dagan :10.1.1.14.7663
empirical comparison smoothing techniques 
information retrieval essentially proposals overcome sparseness problem 
rst class methods relies cluster hypothesis suggests inter document similarities order improve retrieval performance :10.1.1.36.2325
prohibitive compute pairwise similarities documents methods typically rely random comparisons random fractionation 
second approach focuses index terms derive improved feature representation documents 
far popular technique category salton vector space model di erent variants proposed di erent word weighting schemes 
variant known latent semantics indexing performs dimension reduction singular value decomposition 
related methods feature selection proposed text categorization term strength criterion 
contrast propose model statistical approach family nite mixture models way deal data sparseness problem 
mixture class models combined models goal orthogonal standard interpolation techniques 
mixture models advantage provide sound statistical foundation calculus probability theory powerful inference mechanism 
compared unconstrained table count model mixture models er controllable way reduce number free model parameters 
show signi cantly improves statistical inference generalization new data 
canonical way complexity control vary number components mixture 
introduce di erent technique avoid tting problems relies annealed generalization classical em algorithm 
argue annealed em additional advantages making important tool tting mix word gram models examples higher order occurrence data 
ture models 
mixture models natural framework unifying statistical inference clustering 
particularly important interested discovering structure typically represented groups similar objects pairwise data clustering :10.1.1.130.3511
major advantage clustering cod compared similarity clustering fact require external similarity measure exclusively relies objects occurrence statistics 
models directly applicable occurrence histogram data necessity pairwise comparisons avoided altogether 
probabilistic models cod investigated titles class gram models distributional clustering aggregate markov models natural language processing :10.1.1.13.9919
approaches recovered special cases cod framework clarify relation approach sections 
particular discuss distributional clustering model major stimulus research section 
rest organized follows section introduces mixture model corresponds probabilistic grouping object pairs 
section focuses clustering models strict sense models partitioning set objects asymmetric models sets simultaneously symmetric models 
section presents hierarchical model combines clustering abstraction 
discuss improved variants standard em algorithm section nally apply derived algorithms problems information retrieval natural language processing image segmentation section 
separable mixture models basic model maximum likelihood principle rst specify parametric model generates cod try identify parameters assign highest probability observed data 
rst model proposed separable mixture model smm 
introducing classes smm generates data scheme 
choose class distribution 
select object xi class speci conditional distribution ij 
select object yj class speci conditional distribution jj note steps 

carried independently 
xi yj conditionally independent class joint probability distribution smm mixture separable component distri butions parameterized pij xi yj kx xi yjj kx ij jj number independent parameters smm 
minfn mg signi cantly complete table nm entries 
complexity reduction achieved restricting distribution linear combinations separable component distributions 
fitting smm optimally smm data maximize log likelihood nx mx nij log kx ij jj respect model parameters 
overcome di culties maximizing log sum set unobserved variables introduced corresponding em algorithm derived 
em general iterative technique maximum likelihood estimation iteration composed steps expectation step estimating unobserved data generally averaging complete data log likelihood maximization step involves maximization expected log likelihood computed step iteration 
em algorithm known increase likelihood step converges local maximum mild assumptions cf 
:10.1.1.33.2557
denote rr indicator variable represent unknown class observation xi yj generated 
set indicator variables summarized boolean matrix rr kx rr denotes space boolean assignment matrices 
ectively partitions sample set classes 
treating additional unobserved data complete data log likelihood lx kx rr log log log estimation problems decouple posterior parameter estimate step computed exploiting bayes rule general obtained hrr rr sj rr rr joint probability model starting point distributional clustering algorithm authors fact restricted investigations asymmetric clustering model cf 
section 
smm hrr iteration hrr pk step obtained di erentiation estimate rr imposing normalization constraints method lagrange multipliers 
yields normalized summation respective posterior probabilities ij jj lx hrr hrr hrr iterating step parameters converge local maximum likelihood 
notice unnecessary store posteriors step ciently interleaved 
distinguish clearly di erent models proposed sequel representation terms directed graphical models belief networks utilized 
formalism random variables parameters represented nodes directed acyclic graph cf 
general semantics graphical models 
nodes observed quantities shaded number observations represented frame number corner indicate number observations called plate 
graphical representation smm fig 

rr yj graphical model representation symmetric parameterization separable mixture model smm 
asymmetric formulation smm speci cation data generation procedure joint probability distribution symmetric favor interpretation classes terms clusters objects classes correspond groups pair occurrences call aspects 
see comparison cluster approaches section di erent hard assignment objects clusters di ers probabilistic clustering objects 
di erent observations involving xi yj explained di erent aspects objects particular distribution aspects occurrences 
stressed smm help bayes rule pi nx ij ji ij pi corresponding generative model fact equivalent smm illustrated graphical model fig 

generates data scheme 
select object xi probability pi 
choose class conditional distribution ji 
select object yj class speci conditional distribution joint probability distribution smm parameterized pij xi yj speci conditional distribution jji de ned associated object xi understood linear combination prototypical conditional distributions jj weighted probabilities ji cf 

notice ji de nes probabilistic assignment objects classes probabilities induced uncertainty hidden class membership object xi typically case mixture models 
special case smm equivalent word clustering model saul pereira developed parallel 
comparing graphical models fig 
fig simply corresponds arc reversal 
interpreting smm terms cross entropy achieve better understanding smm consider cross entropy kullback leibler divergence empirical conditional distribution jji nij ni yj xi conditional jji implied model jji mx mx jji log jji jji jji log jji mx ni nij log jj note rst entropy term depend parameters 
ni log rr graphical model representation separable mixture model smm asymmetric representation 
rewrite observed data log likelihood nx ni jji nx ni log pi estimation pi ni carried independently remaining parameters obtained optimizing sum cross entropies conditional distributions weighted ni frequency appearance xi minimizing cost function nx nid jji symmetry smm equivalent decomposition obtained interchanging role sets product space mixture model classes tted smm correspond aspects observations pair occurrences directly interpreted probabilistic grouping data space 
enforce simultaneous interpretation terms groups probabilistic clusters sets objects may re ect prior belief part task 
achieved imposing additional structure set labels enforce product decomposition aspects kg element uniquely identi ed 
resulting product space mixture model pmm joint distribution pij kx ij jj probability generate observation speci pair combination clusters di erence smm pmm reduction number conditional distributions ij jj reduces model complexity 
smm conditional distributions class pmm imposes additional constraints ij ij jj jj illustrated graphical model fig 

ir rr jr graphical representation product space mixture model pmm 
pmm kx classes ky classes constrained smm kx ky classes 
number independent parameters pmm reduces kx ky kx ky advantage unconstrained smm depends speci data generating process 
di erence tting procedure compared smm occurs step substituting ij jj hrr hrr step adapted respect modi ed labeling convention 
clustering models grouping structure inferred smm corresponds probabilistic partitioning observation space conditional probabilities ji jj interpreted class membership probabilities objects precisely correspond object speci distributions aspects 
depending application hand natural assume typically unknown de nitive assignment objects clusters particular main interest lies extracting grouping structure case exploratory data analysis tasks 
models object assigned exactly cluster referred clustering models strict sense treated models right 
demonstrate advantage reduce model complexity compared aspect smm approach 
asymmetric clustering model modi cation smm leads asymmetric clustering model acm 
original formulation data generation process smm assumption observation xi yj generated class class speci distribution ij jj equivalently conditional distribution jji linear combination probabilities jj weighted distribution ji 
restrict choice object xi single class 
implies yj occuring observations xi yj involving object xi assumed generated identical conditional distribution jj introduce indicator variable ii class membership allows specify probability distribution xi pi kx ii jj acm understood smm ii replacing ji 
model introduces asymmetry clustering set objects tting class conditional distributions second set obviously interchange role may obtain distinct models 
sample set log likelihood nx ni log pi nx kx ii mx maximum likelihood equations ii jj nij log jj pi ni arg min jj ii nij ii ni nx ii ni pn ik nk class conditional distributions jj linear superpositions empirical distributions objects xi cluster eq 
simple centroid condition distortion measure cross entropy kullback leibler divergence 
contrast maximum likelihood estimate smm minimizes cross entropy tting ij cross entropy serves distortion measure acm 
update scheme solve likelihood equations structurally similar means algorithm calculate assignments centroids nearest neighbor rule recalculate centroid distributions alternation 
acm fact similar distributional clustering model proposed minimization nx kx ii jj distributional clustering kl divergence distortion measure distributions motivated fact centroid equation satis ed stationary points dropping independent pi parameters data dependent constant arrive nx ni kx ii jj showing choice kl divergence simply follows likelihood principle 
point non negligible di erence distributional clustering cost function likelihood weights object speci contributions empirical frequencies ni 
implies objects large sample sets si larger uence optimization data partitioning account observations opposed constant uence distributional clustering model 
em algorithm probabilistic acm interpreting cluster memberships ii model parameters may consider unobserved variables 
fact interpretation consistent common mixture models preferred context statistical modeling particular scales consider complete data distribution ij sji ij ij ny ii speci es prior probability hidden variables ii unobservable variables ii yields em scheme replaces arg min evaluation posterior probabilities hii nij jj pk jj nij exp nid jj exp nid step equivalent posteriors replacing boolean variables 
additional mixing proportions estimated step nx jj hii notice crucial di erence compared smm posteriors indicator variables rr belonging xi identi ed likelihoods observations si collected product suitably normalized 
illustrate di erence consider example 
fraction observed fact unique property kl divergence satis ed euclidean distance 
pairs involving xi best explained assigning xi remaining fraction data best explained assigning fitting smm model approximately results ji ji irrespective number observations posteriors hir additively collected 
acm contributions rst enter huge product 
particular ni posteriors hii approach boolean values automatically result hard partitioning compared original distributional clustering model proposed maximum likelihood approach naturally includes additional parameters mixing proportions notice model refer probabilistic acm di erent observations involving object xi independent conditioned parameters 
consequence considering predictive probability additional observation xi yj requires condition sample set precisely subset si yields sjs kx pi ii kx jj hii posterior probabilities addition parameters order de ne predictive probability distribution occurrence pairs 
corresponding graphical representation fig 
stresses fact observations identical objects coupled hidden variable ii ii xi graphical representation asymmetric clustering model acm 
notice ni interpreted random variable treated quantity 
symmetric clustering model classify models discussed far way model ji arbitrary probability distribution smm ii unobserved hidden variable ji hii probabilistic acm iii boolean variable ji ii hard acm 
hand model imposes restrictions conditional distributions introduces asymmetry acm ji ij restricted jj 
indicates way derive symmetric clustering models imposing constraints conditional distributions ij jj unfortunately naively modifying smm object sets simultaneously result reasonable model 
introducing indicator variables ii jj replace ji jj yields joint probability pij kx ii jj normalized yield valid probability distribution zero pairs xi yj ii jj results zero probabilities occurrence pairs belonging cluster pmm corresponding clustering model interesting 
introduce cluster association parameters de ne joint probability distribution symmetric clustering model scm pij ii jj impose global normalization constraint nx mx pij nx mx ii jj sequel add constraints break certain invariances multiplicative constants restrictions enforced 
introducing lagrange multiplier results augmented log likelihood xx ii jj log pi log qj log nij ii jj corresponding stationary equations maximum likelihood estimators pi qj ni ii mj jj piii jj piii mj nij obtained di erentiation respect substituting right hand side results equation xx ii jj nij inserting gives explicit expression depends substituting expression back yields pi ni ii equivalent expression qj 
observing fraction right hand side depend speci index self consistency equations ii straightforward written pi ni verify choice constants ir gives valid solution ii solution corresponds local maximum simultaneous re scaling pi ii compensated reciprocal change obvious leaving joint probability una ected 
break scale invariance imposing additional conditions piii xp yp parameters 
advantage result simple estimators pi ni qj mj respectively 
proposed choice decouples estimation parameters 
supports interpretation pi qj terms marginal probabilities correspond occurrence frequencies objects particular cluster 
constraints nal expression maximum likelihood estimates nx mx nij ii jj interpreted estimate joint probability cluster pair 
notice auxiliary parameters related marginalization maximum likelihood estimate quotient joint frequency objects clusters product respective marginal cluster frequencies 
treat functions insert results term represents average mutual information random events xi yj maximizing maximizes mutual information isvery satisfying gives scm precise interpretation terms information theoretic concept 
similar criterion mutual information proposed brown class gram model :10.1.1.13.9919
precisely model special case hard clustering scm formally ii ji bigram model implies word coupled means equations set discrete variables obtained maximizing augmented likelihood deduce ii hi arg max hi jj nij log expression hi simpli es jj ni ni ni constant independent cluster index dropped maximization :10.1.1.13.9919
stressed manipulations fact variables appearing identi ed estimates consistent plausible verifying consistent parameters marginalization pij pi holds independently speci choice automatically ensures global normalization pi turn explains global constraint ect optimal choices 
similar equations obtained jj arg maxp nij ii log nature simultaneous clustering suggests alternating minimization procedure partition optimized xed partition vice versa 
update step partition estimators updated order ensure validity update sequence guaranteed increase likelihood step 
notice cluster association parameters ectively decouple interactions assignment variables ii ik di erent xi xk assignment variables jj jl di erent yj yl 
principle insert expression directly likelihood derive local maximization algorithm cf 
result complicated stationary conditions :10.1.1.13.9919
decoupling ect important probabilistic scm derived section 
probabilistic scm acm approach preferred statistics treat discrete hidden variables 
situation essentially acm 
complete data distribution probabilistic scm jj yx ii jj nij classes implicitly utilized di erent ways classes predicting predicted 
obvious choice 
mean depends formula arbitrary partitioning space 
ni ii mj jj predictive probability xi yj involves joint posteriors sjs hii jj ic step equations obtained replacing products boolean variables posteriors 
estimates parameters hii coupling exact computation posteriors step intractable 
preserve tractability procedure propose apply factorial approximation called mean eld approximation cf 
appendix hii jj hii results approximations marginal posterior probabilities hii exp xx nij log similar equation mean eld equations intuitively understood soft max versions hard clustering equations additional priors alternatively may apply markov chain monte carlo mcmc methods approximate required correlations 
mean eld approximation advantage cient due deterministic nature 
notice mean eld conditions form highly non linear coupled system equations 
solution xed point iteration alternates update posterior marginals update continuous parameters hii sequence 
optimizes common objective function step maintains valid probability distribution 
overview altogether derived di erent model types cod summarized systematic scheme table 
seen models span large range model imposing constraints class conditional distributions ij jj hierarchical clustering model model speci cation section hierarchical generative model called 
assume tree topology em approach scm problems started random initialization posteriors typically approach uniform distributions 
remedy applied utilize solution acm initialize arbitrarily selected set hidden variables 
ignore variants obtained asymmetric models reversing role simplicity 
model ij jj smm 

pmm ij ij hard acm ii pi 
probabilistic acm hii ipi 
hard scm probabilistic scm ii pi hii ipi ji qj table systematic overview cod models 
clusters complete binary tree 
clusters identi ed terminal nodes conditional probabilities jj attached leaves inner nodes tree 
involves stages 
rst step similar acm case object xi assigned component cluster represented unobserved variable ii generating ni observations conditional distribution jj second probabilistic sampling step involved selecting resolution abstraction level 
rst step selection horizontal mixture possible paths object xi second step probabilistic selection component vertical mixture nodes selected path observation 
model vertical selection introduce second set hidden variables vr encode resolution level th observation 
notice di erent nature sets variables variables represent partitioning space similar acm partitions occurrence space smm 
obviously hidden variables independent ful ll set constraints induced xx vr denotes nodes nodes path constraints ensure xi assigned cluster observation si restricted generated abstraction nodes path pictorial representation fig 
object xi assigned choices abstraction levels observations xi yj restricted active highlighted vertical path 
tension model imposed classes objects abstraction levels objects partitioning abstraction constraints ii vr opposed independent choices rr smm 
complete speci cation specify prior probabilities vr general choice condition priors terminal node selected variables xi introduce parameters assumes object speci distribution abstraction levels 
simplicity constraints incorporated prior setting de nitions complete data log likelihood lc xxxx nij ii vr log si ni log pi xx ii log corresponding representation terms graphical model shown fig 

may think mixture model horizontal mixture clusters vertical mixture abstraction levels 
horizontal component isa mixture vertical components path root vertical components shared di erent horizontal components tree topology 
general acm allows di erent abstraction level observation 
class conditional probability jj modeled mixture ering additional degrees freedom 
fact acm retrieved special case setting hand constrained smm mixing component densities jj restricted nodes single path tree 
restriction precisely expresses obtain hierarchical clustering organization objects simultaneously organization objects dual organization particular interesting information retrieval cluster documents hierarchical fashion simultaneously assign abstraction levels di erent keywords 
scheme data generation 
em algorithm skipping derivation maximum likelihood equations hard clustering case directly continue probabilistic em version 
step need calculate posterior probabilities hi vr values clustering variables ii restrict admissible values vr wemay compute joint posterior probabilities chain rule vr js vr js js summarizes continuous parameters 
conditional posterior probabilities vr marginal posteriors de nition computed hii yields fv hii ii si completing step 
step parameters updated set equations hii hii si hv large scale applications requires closer investigation computational complexity pi ii vr graphical representation hierarchical asymmetric clustering model 
em model tting procedure 
major computational burden algorithm re calculation posterior probabilities hv step 
accelerate em algorithm exploit fact hierarchical organization critically depend parameters mean setting const result reasonable model 
fact learn xi speci distribution vertical mixtures ne tuning may improve model performance essential structure identi ed 
intermediate choice worked experiments set introduce priors shared objects xi belonging cluster simpli ed step advantage hv depend 
reduces model complexity may prevent tting 
simpli ed model computing posteriors observations su ces compute posterior probabilities objects 
result signi cant acceleration example natural language modeling size vocabulary compared size corpus word occurrences typically di er orders magnitude 
const additional speed results simpli ed propagation posteriors tree 
simulation pursued stage strategy degrees freedom incrementally increased 
acm straightforward check predictive probabilities xi yj sjs pi ji ji hii hierarchies abstraction hierarchical mixture models proposed supervised unsupervised learning shares organization clusters tree structure :10.1.1.136.9119
extracts hierarchical relations clusters breaks permutation symmetry cluster labeling 
important capable perform statistical abstraction 
observations common clusters subtree rooted inner node preferably captured level generality represented node 
observations highly speci explained terminal level 
inner nodes represent coarser view data obtained combination distributions successor nodes expected hierarchical model 
vertical mixtures perform specialization terms level generality adequate observations 
incorporates novel notion hierarchical modeling di ers multiresolution approaches hierarchical concepts unsupervised learning 
ers new possibilities data analysis information retrieval tasks extracting resolution dependent meaningful keywords subcollection documents gives satisfying solution problem cluster summarization cf 
explicitly nds characteristic terms super cluster documents 
additional problems solved arrive complete algorithm 
important concerns speci cation procedure obtain tree topology explaining heuristic important concept annealing 
improved em variants annealed em far mainly focused modeling problem de ning mixture models cod 
standard model tting procedure em algorithm hard clustering variants 
discuss important problems naturally occur context 
rst problem avoid unfavorable local maxima log likelihood 
second important problem avoid tting maximize performance unseen data 
framework allows improve em procedures aspects known deterministic annealing 
deterministic annealing applied clustering problems including vectorial clustering pairwise clustering context cod distributional clustering :10.1.1.130.3511
key idea introduce temperature parameter replace minimization combinatorial objective function substitute known free energy 
details topic appendix annealing methods statistical physics 
consider general case maximum likelihood estimation em algorithm 
step de nition computes posterior average complete data log likelihood maximized step 
annealed step temperature performs average distribution obtained generalizing bayes formula likelihood contribution taken power amounts fact important develop hierarchical generalizations kind 
focus speci cod 
increasing ect prior turn result larger entropy annealed posteriors 
example case acm annealed step generalizing hii exp ni exp ni jj hard clustering applications deterministic annealing utilized usual limit 
guarantee deterministic annealing general nds global minimum independent empirical studies indicate typical solutions obtained signi cantly better corresponding optimization 
due fact annealing homotopy method expected likelihood cost function smoothed large recovered limit 
addition xed annealed step performs regularization entropy posterior probabilities minimize generalized free energy balances expected costs relative entropy cf 
appendix 
reason annealed em reduces sensitivity minima controls ective model complexity 
potential improve generalization tting models 
advantages deterministic annealing investigated experimentally section 
addition annealed em algorithm ers way generate tree topologies 
known adaptive vector quantization starting high value successively lowering leads sequence phase transitions 
phase transition ective number distinguishable clusters grows maximal number reached annealing stopped 
suggests heuristic procedure start single cluster recursively split clusters 
course annealing keeps track splits uses phase diagram tree topology note merely tree topology successively grown data partition obtained speci temperature may drastically change annealing process 
summarize annealed em solves problems 
avoids unfavorable local minima applying temperature continuation method modi ed likelihood convex high temperature eq 
di ers original formula pereira scales temperature frequency ni includes mixing proportions 
pointed naturally obtained ml framework distributional clustering cost function weights ni considered 
canceling weights may reasonable approach limit ect frequently observed objects xi organization clusters 
statistical viewpoint implausible observations automatically sharpen posterior distributions temperature level jj 
avoids tting discounting likelihood contribution step 
ers physically motivated heuristic produce meaningful tree topology 
experiments statistical models utilized annealed variant 
predictive em modi cation step improve generalization worth considering 
notational simplicity focus step smm 
parameters eliminated substituting current step estimators terms posteriors arrive hrr 
eq 
reveals estimating hrr step old estimator appear right hand side ect systematically overestimate high posterior probabilities small posteriors underestimated 
positive feedback posteriors may lead substantial tting phenomena 
illustrate problem consider extreme case ni xi yl sis observation xi 
stationary condition step hrr hrr hrr jj hrr iq jj hri ful lled hrr stable solution arg max jj 
sparse data diagonal contribution dominant positive feedback bears risk tting 
order overcome problems propose variant em refer predictive em 
modi cation exclude th observation recalculating posteriors hrr class membership th observation predicted remaining samples 
smm implies diagonal contributions excluded 
obvious proposed correction minor uence computational complexity tting procedure 
despite heuristic avor caused modifying algorithmic step predictive em motivated strict optimization principles 
details convergence considerations appendix stress fact positive feedback occurs em algorithms severe cod models inherent sparseness problem 
furthermore error measures squared error data vector cluster centroid far sensitive type problems cross entropy involving logarithms small probabilities 
may result unde ned posterior probabilities due zeros example 
case assume posterior uniform 
accelerated em em algorithms important advantages gradient methods problems convergence speed em may restrict applicability large data sets 
simple way accelerate em algorithms overrelaxation step 
discussed early context mixture models rediscovered title em 
method useful accelerating tting procedure discussed models 
essentially estimator generic parameter step modi ed step estimate usual step 
choosing guarantees convergence typically choice speed convergence 
case constraint violated performing step parameter set projected back admissible parameter space 
overview elaborated acceleration methods em refer 
multiscale optimization multiscale optimization approach accelerating clustering algorithms topological structure exists object space 
image segmentation example natural assumption adjacent image sites belong high probability cluster image segment 
fact exploited signi cantly accelerate estimation process maximizing suitable nested sequence variable subspaces coarse ne manner 
achieved temporarily tying adjacent sites joint assignment variable 
notational convenience restrict presentation acm extensions scm probabilistic variants straightforward 
formally coarsening hierarchy nested sequence equivalence relations xi xi xi xg 
context image analysis equivalence relations typically correspond multi resolution pixel grids obtained subsampling 
log likelihood minimized coarsening level imposing constraints form ii ij xi xj models consideration ectively amounts reducing number indicator functions set variables equivalence class preserving functional form likelihood enabling highly accelerated optimization coarser levels 
maximization procedure resolution level converged optimization proceeds level solution subset de ned initializing optimization level solution level probabilistic version multiscale optimization amounts multiscale optimization current form applicable smm pmm 
smm acm scm cran penn table perplexity results di erent models smm acm scm data sets cran predicting words conditioned documents penn predicting nouns conditioned adjectives fold cross validation scm 
modifying step em algorithm computing posteriors reduced sets indicator functions 
emphasize contrast multiresolution optimization schemes multiscale optimization advantage maximize original log likelihood resolution levels 
set hidden variables ectively reduced imposing constraints set hidden variables applied multiscale optimization image analysis experiments resulting typical accelerations factors compared single level optimization 
results information retrieval information retrieval large databases key topics data mining 
problem severe cases query formulated precisely natural language interfaces documents image databases 
typically obtain entries best match query similarity measure 
di cult reliably estimate similarities query may contain information possibly relevant keywords occur query documents 
applies cluster hypothesis entry relevant query similar entries may relevant query may possess high similarity query due small 
clustering provides way pre structuring database purpose improved information retrieval cf 
overview 
types clustering approaches set documents keywords proposed literature 
frequently methods context linkage algorithms single linkage complete linkage wards method cf 
hybrid combinations ag average log likelihood test set performance number em iterations average log likelihood training set performance number em iterations generalization performance training likelihood annealed em di erent temperatures cran eld collection 
average log likelihood test set performance number em iterations average log likelihood training set performance number em iterations training likelihood generalization performance annealed predictive di erent temperatures cran eld collection 
centroid methods probabilistic interpretation number disadvantages 
contrast cod mixture models provide sound statistical basis overcome fundamental sparseness problem proximity clustering 
particular hierarchical clustering model additional features ita suitable candidate interactive coarse ne information retrieval 
performed experiments information retrieval di erent collections abstracts 
index terms dataset automatically extracted documents help standard word stemmer 
list words utilized exclude frequently words 
words occurrences eliminated 
documents identi ed set objects index terms correspond rst series experiments investigated di erent models perform predicting occurrences certain words con text particular document 
set word occurrences divided training test set 
statistical point view canonical goodness measure average log likelihood test set 
context natural language processing customary utilize perplexity related average test set log likelihood exp 
annealed em algorithm validation set utilized determine optimal choice computational temperature 
comparative results discussed models standard test collection cran summarized table 
experiments performed fold cross validation 
main lowest perplexity obtained smm 
performs better constrained acm scm 
terms perplexity linear mixture models pre performance pmm comparable smm displayed 
level level level level level level level aa level ab level ba level bb ion increas atom atom format contrast grain dynamic network loop atom dimer glass loop relax conform cell layer channel grown cluster state defect system test gener size target model observ distribu studi matrix control case complex perform wavelength file dynamic cloud dwarf lsb field car lipid transition robust plane perturb root function author scale faint blue redshift alpha qso event time seismic region sub nonlinear simul distribu univers mass halo cdm cluster result model data approach point genet map imag spatial allow power scale spectrum fractal jump background tau moment peak theta weak omega disc epsilon poisson spectrum fault mpc descrip cdm stress power video visual index shot partition method algorithm size constraint environ line circuit file matrix cloud dwarf lsb field cluster object problem algorithm design process model applic processor graph partition system object access algorithm method cluster propos learn cell format task parallel schedul linear neural part network pattern similar classif descrip data algorithm classif cluster learn input vector center function estim equal vector filter glvq mountain network neural learn control kohonen cost membership bayesian segment object estim approach base speaker speech train continu recogni model chip phone hmm imag region segment iter color ellipsoid concept shell pitch phrase map scale brain patient activ reson adapt codebook design pixel contour motion spatial upper part cluster hierarchy levels cluster dataset generated annealed em 
node described index words yj highest probability jj clustering models 
optimal temperature smm consistently standard em algorithm 
clustering models optimal generalization performance requires higher temperature expected 
temperature complexity control clearly better restricting number components 
smm components su ers tting 
stress advantages proposed em variants investigated ect temperature regularization detail 
fig 
shows log likelihood curves typical runs annealed em algorithm di erent temperatures 
tting phenomenon clearly visible smm test set performance degrades iterations 
notice annealing performs better early stopping 
comparison predictive em variant standard em smm depicted fig 

demonstrates presumably simulations performed cod cran collection 
qualitatively similar results obtained data sets 
slight modi cation avoid positive feedback improves test set performance 
em variant proven valuable tool simulations typical acceleration factor 
facilitate assessment extracted structure investigated dataset documents containing abstracts papers clustering title word cluster 
data presumably amenable interpretation reader standard text collections 
top levels cluster hierarchy generated visualized fig 

demonstrate ability identify abstraction levels hierarchy wehave visualized distribution responsibility observations involving index word yj particularly interesting examples fig 

rst tree word cluster shows expected occurrences cluster documents explained common feature documents occurrences assigned root 
word decision level node indicating typical word algorithmically oriented documents assigned nodes subtree left branch papers physics astronomy 
index term robust occurs di erent meanings rst highly spe cluster robust decision segment channel glass exemplary relative word distributions nodes cluster dataset keywords cluster decision glass robust segment channel 
ci meaning context stability analysis plane perturb eigenvalue root broad meaning sense robust methods algorithms 
word segment occurs mainly documents computer vision language processing signi cant larger extend rst eld 
glass speci term solid state physics lowest level hierarchy 
channel ambivalent context physics communication theory 
bimodal distribution clearly captures fact 
experiments carried dataset documents abstracts journals neural computation neural networks nn 
solution clusters visualized fig 
node described index words highest probability 
examples demonstrate hierarchical organization obtained able extract interesting structure cooccurrence data 
detailed investigation full potential context information retrieval scope pursued 
computational linguistics computational linguistics statistical analysis word occurrences lexical structures adjective noun verb direct object received considerable degree attention :10.1.1.14.7663
potential applications methods word sense disambiguation problem occurs di erent linguistic tasks ranging parsing tagging machine translation 
data utilized test di erent models consists adjective noun pairs extracted tagged version penn treebank corpus lob corpus performance results penn dataset reported second half table 
results qualitatively similar ones obtained cran document collection application quite di erent information retrieval 
result simultaneous hard clustering lob data scm reported fig 

visualization matrix reveals groups space preferably combined mainly group complementary space 
example adjective group holy divine human occurrences exclusively nouns cluster life nature 
groups indi erent respect groups corresponding set adjective group headed small big suitable 
singular plural forms identi ed 
level level level level level level level aa level ab level ba level bb net state neuron continu function number hidden bound continu art univers delta valu associ threshold circuit node theta transfer sub alpha epsilon matrix citi list path effect tsp function weight learn error method error gener learn perform algorithm minima rate network algorithm learn train problem model regular term order optim neuron process jacobian fam joint selec bayesian data gaussian shown hybrid nonlinear smooth perform network process data model classif train target classifi imag classifi constraint map decision function control neural design inform network model nonlinear statist optim linear neocognitron ann recogn expert rule filter local digit detect gabor artmap art node cost transform hint processor dfa determin network neural function result gener neuron activ spike burst fire cell spike rate record time robot domain level algorithm cmac bound echo target bat sonar chang elem circuit local spatial electr optim pyramid rat threshold center ltp conduct phase connec dynamic time model synaptic neuron network neural learn algorithm dynamic control motor problem robot limb object order target cell extern chaotic search solution loop net oper class group analog interneuron model neuron process author inform learn algorithm rule network map visual imag motion cell process field orient map transform nonlinear term object recogni perform segment map maxim kohonen format shape human region pattern line transform associ pattern learn pattern node new motion select flow overlap rule optim size hebbian vector kei set regular estim give bag error valu upper part cluster hierarchy levels nn dataset generated annealed em 
node described index words yj highest probability jj unsupervised texture segmentation unsupervised segmentation textured images challenging partially solved problems low level computer vision 
numerous approaches texture segmentation proposed past decades stage scheme 
modeling stage characteristic features extracted textured input image range spatial frequencies mrf models occurrence matrices fractal indices 

clustering stage features grouped homogeneous segments homogeneity features formalized mathematical notion similarity 
widely features interpreted vectors euclidean space segmentation obtained minimizing means criterion sums square distances feature vectors assigned group speci prototype feature vectors 
means clustering understood statistical mixture model isotropic gaussian class distributions 
occasionally grouping process pairwise similarity measurements image sites similarity measured non parametric statistical test applied feature distribution surrounding neighborhood 
agglomerative techniques rigorously optimization approaches developed applied grouping similarity data texture segmentation context 
pairwise similarity clustering provides indirect way group discrete feature distributions reducing information distribution mean 
cod mixture models especially acm model formalize grouping feature distribution direct manner 
contrast pairwise similarity clustering er sound generative model texture class description utilized subsequent processing stages edge localization 
furthermore need compute large matrix pairwise similarity scores image sites greatly reduces processing time memory requirements 
compared mixture gaussian model acm provides signi cantly exibility distribution modeling 
especially texture segmentation application class features exhibit non gaussian multi voice face body head light party country system sense market deal friend example night bird man time people year day complete natural official separate growing nuclear western apparent atomic potential slight vertical severe awful long poor happy golden beautiful idea book job manner word power line war impression size statement test style weapon society experience position family reason course court result unit child knowledge trade community feature control land music metal blow stone value level rate price cost great fine bad close human divine holy continued similar single strong regular obvious red cold black dark hot way thing hand area side smile frame crowd baby phrase strange thin soft bitter plain behaviour place floor field trip person leave section companion study expression world desire town atmosphere space note authority government piece shape council sound committee wind effect quality solution picture colour special important particular various basic electric massive gentle cool cheap heavy literary familiar pure solid sudden sharp powerful friendly curious method movement view soil spot iron ship drink life nature spirit name water eye hair fish milk number amount scale report majority interest service part condition school form problem meeting class case activity agreement type true hard excellent ancient famous right wide left opposite large common small certain foreign big suitable total open annual medical light double national huge general occasional social firm private full deep military daily free quick musical square different real original possible simple local white central legal eastern old young little early short tiny perfect native distant violent new modern serious existing worker demand informant acid glance attitude thought tradition programme division change term property action effort feeling love letter silence heart high final low ordinary average empty brilliant endless clever royal extended closed reciprocal interesting political usual personal successful economic remarkable individual historical industrial clustering lob scm visualization matrix characterization clusters probable words 
modal distribution main reason success pairwise similarity clustering approaches acm compared standard means 
acm applicable feature extraction process 
example exploit gabor lter image representation 
speci cally experiments modulus bank gabor lters orientations scales octave spacing resulting dimensional feature vector associated image site 
experiments features discretized separately dimension bins 
statistical independence feature channels assumed simplicity 
feature generation process modeled occurrence image site xi measured gabor feature occurrence channel yj rj kj main contribution novel class statistical models analysis occurrence data proposed evaluated 
introduced discussed di erent models enumerating alternative approaches distinguishing systematic point view 
criterion di erentiate models way hidden variables introduced ectively imposes constraints component distributions mixture 
proposed statistical models turned special cases 
models sound statistical foundation de ne gen rj denotes gabor channel kj denotes index discretized feature 
sample set si site xi consists gabor responses window centered xi size window proportional lter scale 
image location xi ectively characterized dimensional histograms gabor coe cients 
distribution tted approximate em algorithm 
models method choice problem crucially depends modeling goal 
required detect groups structure hierarchical representations 
situations may willing sacri ce precision terms statistical accuracy perplexity reduction extract structure interest 
proposed framework models derived extract group structure object spaces acm texture model hierarchical dependencies clusters 
strongly tion algorithm collection textured images 
fig 
believe proposed framework exible shows exemplary results images ran adapted di erent tasks 
generality generated brodatz texture collection developed methods stressed discussing micro textures 
fig 
shows similar results mixtures aerial images 
detailed benchmark study bene ts context broad range potential applications 
novel segmentation algorithm including comparisons addition modeling problem ad state art techniques appear computational issues particular focusing coming 
improved variants basic em algorithm 
im mixture images containing textures 
image segmentation obtained acm 
experiments underline possible advantages annealed version em fruitful combination ideas methods statistics statistical physics 
acknowledgment authors wish michael jordan peter dayan tishby joachim buhmann helpful comments suggestions 
authors grateful carl de marcken joshua goodman sharing expertise data natural language processing du buf providing image data depicted fig 

appendix establish important relationship log likelihood quantity known free energy statistical physics 
consider data log likelihood log sj function discrete hidden states xed parameters de ne cost function hidden variable space 
minimizing probability distributions subject constraint entropy yields quantity isknown free energy statistical physics 
generalized non uniform priors xing respect prior distribution 
introducing lagrange parameter arrive objective function probability distributions discrete space ft pjs ep ep log solution minimization problem associated generalized free energy tilted gibbs distribution exp step amounts discounting likelihood compared prior th power 
step performs minimization ep ft pjs respect xed gibbs distribution necessarily correspond true posterior 
notice convergence annealed em guaranteed ft necessarily likelihood lyapunov function 
exact calculation posterior step intractable typically higher order correlations hidden variables scm optimization problem restricted factorial dis rr pr tributions suggestive notation hrr pr stress variational parameters pr thought approximation posterior marginals 
variational technique known mean eld approximation successfully applied optimization problems computer vision inference graphical models :10.1.1.130.3511
general solutions mean eld approximation ful ll stationary conditions hrr exp hh rr expectations taken respect 
notice expected costs appear exponent expectation taken respect hidden variables rr xed 
order obtain convergent iteration procedure general case replace synchronous step update sequential update 
scm coupling hidden variables restricted pairs variables ii jj nij allows recompute posteriors variables posteriors sweep vice versa 
appendix sj order preserve strict optimization principles exactly posterior probability posterior minimizes ft 
annealed em algorithm generalization de ned predictive step variant implemented slightly careful naively eliminating diagonal contributions 
inserting corrected estimates arbitrary choice temperature 
complete data log likelihood function mixture images containing textures extracted aerial images 
image segmentation obtained acm 
obtain cost function lx kx rr hr hr log ru log ru log ru refer predictive likelihood 
minimizing free energy corresponding mean eld approximation yields direct contribution proportional exp hr additional terms result indirect ect rr hs variables rs omit details derivation purely technical 
consequence utilize sequential update guarantee convergence predictive em 
reasons ciency wehave ignored indirect ects computation posterior probabilities experiments empirically turned 
bauer koller singer 
update rules parameter estimation bayesian networks 
proceedings thirteenth annual conference uncertainty arti cal intelligence pages 
snyder 
mean eld approximation minimizes relative entropy 
journal optical society america 
breiman friedman olshen stone 
classi cation regression trees 
wadsworth intern 
group belmont california 
brown desouza mercer della pietra lai :10.1.1.13.9919
class gram models natural language 
computational 
buhmann 
vector quantization complexity costs 
ieee transactions information theory july 
chaudhuri sarkar 
texture segmentation fractal dimension 
ieee transactions pattern analysis machine intelligence 
chen goodman 
empirical study smoothing techniques language modeling 
proceedings th annual meeting acl pages 
cutting karger pedersen 
constant interaction time scatter gather browsing large document collections 
sixteenth annual international acm sigir conference development information retrieval pittsburgh pa usa pages 
dagan lee pereira 
similaritybased estimation word probabilities 
proceedings association computational linguistics 
dagan lee pereira 
similaritybased methods word sense disambiguation 
proceedings association computational linguistics 
dayan hinton neal zemel 
helmholtz machine 
neural computation 
deerwester dumais landauer furnas harshman 
indexing latent semantics analysis 
journal american society information science 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
essen steinbiss 
cooccurrence smoothing stochastic language modeling 
proceedings ieee conference signal processing pages 
geiger federico girosi 
coupled markov random elds mean eld theory 
advances neural information processing systems pages 
geman geman gra dong 
boundary detection constrained optimization 
ieee transactions pattern analysis machine intelligence 
gilks richardson spiegelhalter editors 
markov chain monte carlo practice 
chapman hall 

estimation probabilities 
research monograph 
mit press cambridge ma 

population frequencies species estimation population parameters 
biometrika 
gri ths willett 
similarity information document retrieval systems 
journal american society information science 
heitz perez bouthemy 
multiscale minimization global energy functions visual recovery problems 
cvgip image understanding 
hindle 
noun classi cation predicate argument structures 
proceedings association computational linguistics pages 
hofmann buhmann :10.1.1.130.3511
pairwise data clustering deterministic annealing 
ieee transactions pattern analysis machine intelligence 
hofmann puzicha buhmann 
deterministic annealing unsupervised texture segmentation 
proceedings international workshop energy minimization methods computer vision pattern recognition volume lecture notes computer science pages may 
jain 
unsupervised texture segmentation gabor lters 
pattern recognition 
jain dubes 
algorithms clustering data 
prentice hall englewood cli nj 
jelinek 
development experimental discrete dictation recogniser 
proceedings ieee 
jelinek mercer 
interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice 
jordan jacobs 
hierarchical mixtures experts em algorithm 
neural computation 
katz 
estimation probabilities sparse data language model component speech recogniser 
assp 
laine fan 
frame representations texture segmentation 
ieee transactions image processing 
lauritzen editor 
graphical models 
clarendon press oxford university press 
manjunath chellappa 
unsupervised texture segmentation markov random eld models 
ieee transactions pattern analysis machine intelligence 
mao jain 
texture classi cation segmentation multiresolution simultaneous autoregressive models 
pattern recognition 
mclachlan basford 
mixture models 
marcel dekker new york basel 
mclachlan krishnan 
em algorithm extensions 
wiley new york 
miller rose 
hierarchical unsupervised learning growing phase transitions 
neural computation 
neal hinton 
new view em algorithm justi es incremental variants 

submitted 

unsupervised texture segmentation feature distributions 
technical report car tr center automation research university maryland 
healey 
markov random eld models unsupervised segmentation textured color images 
ieee transactions pattern analysis machine intelligence 
pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann publishers san mateo ca 

minimum property free energy 
physical review 
pereira tishby lee 
distributional clustering english words 
proceedings association computational linguistics pages 
peters walker 
iterative procedure obtaining maximum likelihood estimates parameters mixture normal distribution 
siam journal applied mathematics 
peters walker 
numerical evaluation maximum likelihood estimates subset mixture proportions 
siam journal applied mathematics 
puzicha buhmann 
multiscale annealing real time unsupervised texture segmentation 
technical report iai institut fur informatik iii 
rissanen 
universal coding information prediction estimation 
ieee transactions information theory 
rissanen 
stochastic complexity modeling 
annals statistics 
rose gurewitz fox 
statistical mechanics phase transitions clustering 
physical review letters 
rose gurewitz fox 
vector quantization deterministic annealing 
ieee transactions information theory 
salton 
experiments automatic thesaurus construction information retrieval 
proceedings ifip congress ta pages 
salton 
developments automatic text retrieval 
science 
salton buckley 
term weighting approaches automatic text retrieval 
information processing management 
saul pereira 
aggregate mixed order markov models statistical language processing 
proceedings second conference empirical methods natural language processing 
association computational linguistics 
saul jaakkola jordan 
mean eld theory belief networks 
journal arti cial intelligence research 
bigun 
hierarchical image segmentation multi dimensional clustering orientation adaptive boundary re nement 
pattern recognition 
shi malik 
normalized cuts image segmentation 
proceedings ieee conference computer vision pattern recognition cvpr pages 
sparck jones 
automatic keyword classi cation information retrieval 
butterworths london 
titterington smith makov 
statistical analysis finite mixture distributions 
john wiley sons 
ueda nakano 
deterministic annealing variants em 
advances neural information processing systems pages 
van den bout miller 
graph partitioning annealed neural networks 
ieee transactions neural networks 
van rijsbergen 
information retrieval 
butterworths london boston 
willett 
trends hierarchical document clustering critical review 
information processing management 
witten bell 
zero frequency problem estimating probabilities novel events adaptive text compression 
ieee transactions information theory 
won derin 
unsupervised segmentation noisy textured images markov random elds 
cvgip graphical models image processing july 
yang 
corpus statistics remove redundant words text categorization 
journal american society information science 
zhang 
mean eld theory em procedures blind markov random elds 
ieee transactions image processing 

