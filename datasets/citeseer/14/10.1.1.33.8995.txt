structural risk minimization data dependent hierarchies john shawe taylor royal holloway university london peter bartlett australian national university robert williamson australian national university martin anthony london school economics neurocolt technical report series nc tr october produced part esprit working group neural computational learning neurocolt neurocolt coordinating partner department computer science egham surrey tw ex england information contact john shawe taylor address email neurocolt dcs rhbnc ac uk department computer science royal holloway university london egham tw ex uk 
email jst dcs rhbnc ac uk department systems engineering australian national university canberra australia 
email peter bartlett anu edu au department engineering australian national university canberra australia 
email bob williamson anu edu au department mathematics london school economics houghton street london wc ae uk 
email anthony lse ac uk received september introduces generalizations vapnik method structural risk minimisation srm 
making explicit details srm provides result allows trade errors training sample improved generalization performance 
considers general case hierarchy classes chosen response data 
result generalization performance classifiers large margin 
theoretically explains impressive generalization performance maximal margin hyperplane algorithm vapnik workers basis support vector machines 
concludes general result terms luckiness functions provides quite general way exploiting serendipitous simplicity observed data obtain better prediction accuracy small training sets 
examples functions including vc dimension measured sample 
keywords learning machines maximal margin support vector machines probable smooth luckiness uniform convergence vapnik chervonenkis dimension fat shattering dimension computational learning theory probably approximately correct learning 
standard probably approximately correct pac model learning considers fixed hypothesis class required accuracy ffl confidence gamma ffi theory characterises target function learned examples terms vapnik chervonenkis dimension measure flexibility class specifies sample sizes required deliver required accuracy allowed confidence 
cases practical interest precise class containing target function learned may known advance 
learner may hierarchy classes delta delta delta delta delta delta told target lie sets structural risk minimization srm copes problem minimizing upper bound expected risk hypothesis classes 
principle curious order algorithm necessary theoretical bound generalization performance 
formal statement method section 
linial mansour rivest studied learning framework allowing learner seek consistent hypothesis subclass turn drawing extra examples stage ensure correct level accuracy confidence consistent hypothesis 
addresses shortcomings linial approach 
requirement draw extra examples seeking richer class 
results appeared 
may unrealistic assume examples obtained cheaply time foolish examples available start 
suppose fixed number examples allowed aim learner bound expected generalization error high confidence 
second drawback linial approach clear adapted handle case errors allowed training set 
situation need trade number errors complexity class class complex result worse generalization error fixed number examples allowing extra errors restricted class 
model consider allow precise bound error arising different classes reliable way applying structural risk minimisation principle introduced vapnik 
results reported sections implicit cited treatment serves introduce main results sections explicit assumptions inherent presentations 
lugosi zeger considers standard srm provides bounds true error hypothesis lowest empirical error class 
theorem gives error bound decreases twice empirical error roughly linearly ratio vc dimension number examples give error bound decreases empirical error square root ratio 
section onwards address shortcoming srm method vapnik page highlights srm principle structure defined priori training data appear 
algorithm maximally separating hyperplanes proposed vapnik workers violates principle hierarchy defined depends data :10.1.1.103.1189:10.1.1.15.9362
section prove result shows achieves correct classification training data class valued functions thresholded values real valued functions training points away zero bound generalization error better obtained vc dimension thresholded class 
section apply case considered vapnik separating hyperplanes large margin 
section introduce general framework allows large class methods measuring luckiness sample sense large margin lucky 
section show vapnik maximum margin hyperplanes fit general framework allows radius set points estimated data 
addition show function measures vc dimension set hypotheses sample points valid un luckiness function 
leads bound generalization performance terms measured dimension worst case bound involves vc dimension set hypotheses input space 
approach interpreted general way encoding bias prior assumptions possibly advantage happen standard srm training errors correct 
case fixed hierarchy expect target close approximation class small maximal separation case expect target consistent classifying hyperplane large separation examples 
corresponds collusion probability distribution target concept impossible exploit standard pac distribution independent framework 
assumptions happen correct training data confident accurate hypothesis small data set expense small penalty incorrect 
commonly studied related problem model order selection see example briefly remarks relationship 
assuming hierarchy hypothesis classes aim identify best class index 
best literature simply means correct sense fact target hypothesis sample size grows infinity selection procedure probabilistic sense pick methods complexity regularization seen solve similar problems 
see example :10.1.1.48.9258
aware methods apart srm explicit finite sample size bounds performance available 
furthermore exception methods discussed methods take form minimizing cost function comprising empirical risk plus additive complexity term depend data 
denote logarithms base log natural logarithms ln 
set jsj denotes cardinality 
state measurability conditions needed arguments hold 
assume discussion function classes involved see appendix section 
standard srm training errors initial example consider hierarchy classes ae input space assume vcdim rest section 
recall vc dimension class functions size largest subset domain restriction class subset set valued functions see 
hierarchy classes called decomposable concept class linial 
assume fixed number labelled examples vector learner xm target function lies subclasses learner uses algorithm find value contains hypothesis consistent sample require function ffl ffi give learner upper bound generalization error confidence gamma ffi 
theorem gives suitable function 
standard srm training errors jfi gj denote number errors er pfx denote expected error xm drawn independently follows write target obvious context 
theorem sequence hypothesis classes mapping vcdim probability distribution set positive numbers satisfying probability gamma ffi independent examples drawn learner finds consistent hypothesis generalization error bounded ffl ffi ln em ln ln ffi provided proof proof uses standard bound generalization error classes divides confidence classes giving proportion class show pfx er ffl ffi ffi showing pfx er ffl ffi ffi applying union bound 
probability left inequality bounded usual analysis see pi hd exp gammaffl ffi pi hd growth function maximum number dichotomies implementable points applying sauer lemma substituting value ffl ffi gives required bound ffi role numbers may little counter intuitive appear able bias estimate adjusting parameters 
numbers specified advance represent confidence different points failure occur 
sense arguments function ffl ffi 
deliberately omitted dependence different status learning framework 
vapnik implicitly assumes corresponding term gamma ln represents overestimate ffl ffi arising prior uncertainty class 
information classes srm training errors arise improve bias 
example know class occur choose recover standard pac model 
hand probabilities function falling different classes known expected overestimate ffl ffi increase bound suitable ffl obtained argument knew true advance gammaq ln know probabilities setting gives expected overestimate entropy distribution srm training errors framework established previous section wish consider possibility errors training sample 
result vapnik slightly improved version due anthony shawe taylor 
note result expressed terms quantity denotes number errors hypothesis sample usual proportion errors 
theorem ffl fl 
suppose hypothesis space functions input space probability measure theta 
probability respect er ffl gamma fl er pi exp gamma fl aim double stratification ffi class number errors sample dk 
generalization error function size sample index class number errors sample confidence ffi theorem sequence hypothesis classes mapping having vc dimension probability measure theta dk sets positive numbers satisfying dk probability gamma ffi independent identically distributed examples learner finds hypothesis generalization error bounded ffl ffi ln dk ffi ln em srm training errors provided proof bound required probability failure fz er ffl ffi ffi showing fz er ffl ffi ffi dk apply theorem value ensure value fl fl dk case 
appropriate value fl dk gamma ffi ensures er ffl ffi gamma fl dk ffl ffi gamma fl dk er required application theorem 
sauer lemma implies fz er ffl ffi ffi dk em exp gammafl dk ffl ffi ffi dk ln em ln dk ffi gamma ffl ffi ffl ffi ln dk ffi ln em ignoring term 
result follows 
choice prior dk different affect resulting trade complexity accuracy 
view expectation penalty term choosing large class probably overestimate reasonable give correspondingly large penalty large numbers errors 
possibility exponentially decreasing prior distribution dk gamma rate decrease varied classes 
assuming choice incremental search optimal value reduction number classification errors class ln em note tradeoff errors sample generalization error discussed :10.1.1.15.9362
classifiers large margin classifiers large margin standard methods structural risk minimization require decomposition hypothesis class chosen advance seeing data 
section introduce variant srm effectively decomposition data seen 
main tool dimension introduced problems learning 
show classifier correctly classifies training set large margin fat shattering function scale related margin small generalization error small 
formally stated theorem 
definition set real valued functions 
say set points fl shattered real numbers indexed binary vectors indexed function satisfying fl gamma fl fat shattering dimension fat set function positive real numbers integers maps value fl size largest fl shattered set finite infinity 
denote threshold function ff iff ff 
fix class valued functions 
interpret function class classification function considering thresholded version ffi result implies real valued function class maps training examples correct side large margin misclassification probability thresholded version function depends fat shattering dimension class scale related margin 
result special case corollary applied generally arbitrary real valued target functions 
application classification problems described 
theorem set valued functions defined set fl 
positive constant function probability distribution probability gamma ffi sequence xm examples chosen independently jh gamma gamma fl satisfies pr jh gamma ffl provided ffl log ffi log fl ffl fat fl 
classifiers large margin clearly implies misclassification probability ffl conditions theorem implies jh gamma 
remainder section improvement result 
advantage fact target values fall finite set fact behaviour near threshold functions important remove ffl factor log factor bound 
improve constants obtained argument proof theorem 
quote lemma need definition 
definition pseudo metric space subset ffl 
set ffl cover exists ffl 
ffl covering number ffl minimal cardinality ffl cover finite cover defined 
idea finite approximate respect pseudometric distance finite sample xm pseudo metric space functions dx max jf gamma write ffl ffl covering number respect pseudometric dx quote lemma alon 
lemma alon class functions distribution choose ffl fat ffl 
ffl ffl log em dffl expectation taken sample drawn corollary class functions distribution choose ffl fat ffl 
ffl gamma ffl log em gammaa dffl expectation samples drawn proof scale functions affine transformation mapping interval create set functions clearly fat fl fat fl gamma ffl ffl gamma result follows 
classifiers large margin order motivate introduce notation come apply 
aim transform problem observing large margin observing maximal value taken set functions 
folding functions threshold 
hat operator implements folding 
define mapping thetaf 
gamma gamma fixed real 
set functions define fg 
idea mapping function corresponding maps input classification output value provided classification obtained thresholding correct 
lemma suppose set functions map finite fat shattering dimension bounded function 
continuous right 
distribution xy max ff fl gamma fl jfi ffl ffi ffi ffl ffi log em log log ffi 
proof standard permutation argument may fix sequence xy bound probability uniform distribution swapping permutations permuted sequence satisfies condition stated 
fl fl kg 
notice minimum defined continuous right fl fl 
fl satisfying fl fl fl probability greater xy fl fl fl fl event max ff fl ffi points note fl 
ff ff gamma fl ff gamma fl ff fg 
consider minimal fl cover pseudo metric dxy exists gamma fl xy 
classifiers large margin definition gamma fl gamma fl fl gamma fl fl gamma fl ffl ffi points fl fl gamma fl max reduces separation output values conclude event occurs 
permutation argument fixed gammaffl ffi sequences obtained swapping corresponding points satisfy conditions points largest values remain right hand side occur 
union bound xy fl fl fl gammaffl ffi expectation xy drawn fl fat fl fat fl set points fl shattered fl shattered furthermore class functions mapping set interval gamma fl 
corollary setting gamma fl ffl fl fl xy gamma fl fl log em flk dfl fat fl fat fl log em gammaffl ffi ffi provided ffl ffi gamma log em log log ffi delta required 
function fl theorem fat fl continuity property ensure fl assume fat fl continuous right 
avoid requirement give error estimate directly terms fat introduce worse constant argument fat practice works continuous upper bounds fat fl fl floor value critical question bound strict equal 
provided fat strictly continuous bound corresponding floor function continuous right 
addition arbitrarily small constant continuous function allow substitution strict inequality 
lemma set real valued functions fl fat fl fat fl classifiers large margin proof realises dichotomy xm margin fl output values realises dichotomy phi margin fl output values gamma gamma lemma form due vapnik page 
lemma set system sets probability measure define jx aj ffl ae sup gamma ffl oe ae xy sup gamma ffl oe denote threshold function 
ff iff ff 
class functions ft fg 
theorem consider real valued function class having fat shattering function bounded function 
continuous right 
fix learner correctly classifies independently generated examples er fl min jf gamma confidence gamma ffi expected error bounded ffl ffi log em log log ffi fl 
proof making lemma move double sample stratify union bound suffices show ffi fxy er fl fl min jf gamma ery ffi largest value need consider shatter greater number points xy 
sufficient ffi ffi large margin hyperplanes consider note lemma function fl bounds fat fl 
probability distribution theta second component determined target value component 
note point misclassified maxf xg fl theta maxf xg fl gamma fl fi fi fi phi psi fi fi fi ffi replacing fl fl lemma lemma obtain ffi ffl ffi log em log log ffi condition lemma satisfied linking ffl substituting ffi gives result 
related result gives bounds misclassification probability thresholded functions terms error estimate involving margin corresponding real valued functions 
result bounds fat shattering dimension sigmoidal neural networks gives bounds generalization performance networks depend size parameters independent number parameters 
large margin hyperplanes consider particular case results previous section applicable class linear threshold functions euclidean space 
vapnik page suggested choosing maximal margin hyperplane hyperplane maximises minimal distance points assuming correct classification improve generalization resulting classifier :10.1.1.103.1189:10.1.1.15.9362
give evidence indicate generalization performance frequently significantly better predicted vc dimension full class linear threshold functions 
section show large margin help case give explicit bound generalization error terms margin achieved training sample 
bounding appropriate fat shattering function applying theorem 
margin arises proof perceptron convergence theorem see example page alternate motivation large margin noise immunity 
margin occurs winnow algorithms variants developed littlestone :10.1.1.130.9013
connection uses explored 
large margin hyperplanes consider hyperplane defined weight vector threshold value 
subset euclidean space limit point hyperplane min wi say hyperplane canonical form respect min wi delta denote euclidean norm 
theorem basis argument 
theorem vapnik suppose subset input space contained ball radius point 
consider set hyperplanes canonical form respect satisfy kwk class corresponding linear threshold functions sgn hx wi restriction points vc dimension bounded minfr ng argument terms theorem need bound fat shattering dimension class hyperplanes 
argument concerning level fat shattering dimension defined 
definition set real valued functions 
say set points level fl shattered level fl shattered choosing level fat shattering dimension set function positive real numbers integers maps value fl size largest level fl shattered set finite infinity 
level fat shattering dimension scale sensitive version dimension introduced vapnik 
scale sensitive version introduced alon 
lemma set linear functions unit weight vectors restricted points ball radius fx 
hw xi kwk level fat shattering function bounded fl minfr fl ng large margin hyperplanes proof set points fx level fl shattered value dichotomy realised weight vector threshold hw fl gamma fl 
min xi gamma rj fl 
consider hyperplane defined gamma 
canonical form respect points satisfies kw kw dk realises dichotomy set points shattered subset canonical hyperplanes satisfying kw fl 
result follows theorem 
corollary set defined linear functions unit weight vectors restricted points ball dimensions radius origin thresholds fat shattering function bounded fat fl minf fl proof suppose points lying ball radius origin fl shattered relative 
kwk jr 
create extended vector 
jr kx 
parameter vector hyperplane realizes dichotomy set gamma 
show points level fl shattered level fw hw hw gamma hw fl hw gamma fl 
fl gamma fl gamma fl gamma fl kw 
set points level fl shattered level dim lemma fat fl minf fl 
theorem suppose inputs drawn independently distribution support contained ball centered origin radius succeed correctly classifying inputs canonical hyperplane kwk fl confidence gamma ffi generalization error bounded ffl fl log em log log ffi fl luckiness general framework decomposing classes proof firstly note restrict consideration subclass point fl shattered required achieve dichotomy different signs 
points lie ball shatter hyperplane intersect ball 
kwk means may achieve greater margin zero dichotomy choosing larger value dichotomies achieve larger fl 
bound may weak special case classification training set true 
position apply theorem value theorem taken 
fat fl fl fl fl substituting bound theorem gives required bound 
section give analogous result special case general framework derived section 
sample size bound result weaker additional log factor allow cope slightly general situation estimating radius ball knowing advance 
fact bound theorem depend dimension input space particularly important light vapnik ingenious construction support vector machines :10.1.1.15.9362
method implementing quite complex decision rules defined polynomials neural networks terms linear hyperplanes dimensions 
clever part technique algorithm dual space maximizes margin training set 
vapnik algorithm bound theorem allow posteriori bounds generalization error range applications 
important note explanation performance maximum margin hyperplanes different vapnik page 
whilst result theorem theorem presents explanation bound expected generalization error terms number support vectors 
small number support vectors gives bound 
construct examples combinations small large margin support vectors occur 
explanation 
terminology section margin reciprocal number support vectors luckiness functions determine bounds performance 
luckiness general framework decomposing classes standard pac analysis gives bounds generalization error uniform hypothesis class 
decomposing hypothesis class described luckiness general framework decomposing classes section allows bias generalization error bounds favour certain target functions distributions hypothesis low hierarchy accurate approximation 
results section show possible decompose hypothesis class basis observed data cases terms margin attained 
section introduce general framework subsumes standard pac model framework described section recover slightly weaker form results section special case 
general decomposition hypothesis class sample allows bias generalization error bounds favour general classes target functions distributions correspond realistic assumptions practical learning problems 
order allow decomposition hypothesis class depend sample need better information provided sample 
standard pac analysis structural risk minimisation fixed decomposition hypothesis class effectively discard training examples function defined hypothesis class induced training examples 
additional information exploit case sample decompositions hypothesis class encapsulated luckiness function 
main idea fix advance assumption target function distribution encode assumption real valued function defined space training samples hypotheses 
value function indicates extent assumption satisfied sample hypothesis 
call mapping luckiness function reflects fortunate assumption satisfied 
function theta measures luckiness particular hypothesis respect training examples 
convenient express relationship inverted way function theta turns ordering luckiness functions impose hypotheses important 
define level function relative function gj gj defined terms matter convenience quantity plays central role follows 
denote xy concatenation xm 
luckiness general framework decomposing classes examples example consider hierarchy classes introduced section define minfd follows sauer lemma bound em 
notice xy em observation prove useful investigate luckiness sample infer luckiness subsequent sample 
show section hyperplane margin section luckiness function satisfies technical restrictions introduce 
fact terms function defined formally convenience 
definition linear threshold function separating hyperplane defined canonical form respect sample define max im kx kwk probable smoothness luckiness functions introduce technical restriction luckiness functions required theorem 
definition ff subsequence vector vector obtained deleting fraction ff coordinates 
write ff partitioned vector xy write ff xy 
luckiness function defined function class probably smooth respect functions ffi oe ffi targets distribution fxy xy oe ffi ffi ffi 
definition probably smooth identical replaced 
intuition arcane definition captures luckiness estimated half sample luckiness general framework decomposing classes high confidence 
particular need ensure dichotomies double sample 
probably smooth luckiness function hypothesis luckiness points know high confidence proportion ffi points double sample growth function class functions lucky small oe ffi 
theorem suppose positive numbers satisfying luckiness function function class probably smooth respect functions oe ffi 
target function distribution probability gamma ffi independent examples chosen learner finds hypothesis oe ffi generalization error satisfies er ffl ffi ffl ffi log ffi ffi log proof lemma phi er oe ffi er ffl ffi psi phi xy oe ffi ery ffl ffi psi provided ffl ffi follows definition ffl ffi fact ffi 
suffices show ffi ffi event phi xy oe ffi ery ffl ffi psi event fxy xy oe ffi ffi 
follows ffi suffices show ffi 
subset fxy xy ery ffl ffi gamma jyj gamma jy psi jy denotes length sequence consider uniform distribution group permutations mg swap elements sup xy foe xy oe rg luckiness general framework decomposing classes oe oe oe fix xy subsequence xy oe denote corresponding subsequence permuted version xy similarly oe oe 
foe xy oe rg phi oe xy oe er oe er oe ffl ffi gamma jyj gamma jy psi phi oe oe er oe er oe ffl ffi gamma jyj gamma jy psi fixed subsequence xy define event inside sum partition group permutations number equivalence classes class permutations map fixed value contains clearly equivalence classes equal probability pr ajc pr sup pr ajc sum supremum equivalence classes equivalence class oe permutation write pr ajc pr gamma oe er oe er oe ffl ffi gamma jyj gamma jy fi fi delta sup oe fi fi oe fi fi sup pr gamma er oe er oe ffl ffi fi fi delta second supremum subset oe clearly fi fi oe fi fi probability ffi jm combining results jm gammam ffl ffi jm ffi ffi ffl ffi jm log jm log ffi theorem follows 
examples probably smooth luckiness functions examples probably smooth luckiness functions section consider examples luckiness functions show probably smooth 
example example simplest case luckiness depends hypothesis independent examples second example luckiness depends examples independent hypothesis 
third example allows predict generalization performance maximal margin classifier 
case luckiness clearly depends examples hypothesis 
example luckiness function function data hypothesis 
fourth example concerns vc dimension class functions restricted particular sample available 
example consider example function clearly probably smooth choose oe ffi em ffi ffi bound generalization error obtain theorem identical theorem 
second example second example consider involves examples lying hyperplanes 
definition define function linear threshold function dim dimension vector space spanned vectors proposition class linear threshold functions defined function definition probably smooth respect oe ffi em ffi ln em ln ffi proof recognition dimensional subspace learning problem indicator functions subspaces 
vc dimension applying hierarchical approach theorem obtain error bound number examples second half sequence lying outside subspace 
probability gamma ffi gamma subsequence points lying subspace 
sequence growth function bounded oe ffi 
example useful distribution highly concentrated subspace small probability points lying outside 
conjecture possible relax assumption probability distribution concentrated exactly subspace take advantage examples probably smooth luckiness functions situation concentrated subspace classifications compatible perpendicular projection space 
data classification decide luckiness 
third example position state result concerning maximal margin hyperplanes 
proposition function definition probably smooth oe ffi em ffi log em log log log ffi uc 
proof definition function maximal margin hyperplane margin fl satisfying fl max im kx proof works allowing sets points excluded second half sample making value ignoring points probability gamma ffi remaining points ball radius origin correctly classified maximal margin hyperplane margin fl 
provided case function oe ffi gives bound growth function double sample hyperplanes larger margins 
remains show probability gamma ffi exists fraction ffi points double sample removal leaves subsequence points satisfying conditions 
consider class ff ae jae ae kxk ae class vc dimension permutation argument probability gamma ffi fraction second half sample outside ball centered origin containing radius log log ffi growth function bh 
consider permutation argument applied points double sample contained estimate examples probably smooth luckiness functions closer hyperplane fl incorrectly classified 
involves application lemma fl substituted fl folding argument introduced just lemma 
corollary fat fl minf fl set linear threshold functions unit weight vector restricted points ball radius origin 
probability gamma ffi fraction second half sample correctly classified margin fl hyperplane log em log log ffi fl uc 
result follows adding numbers excluded points expressing result fraction double sample required 
combining results theorem proposition gives corollary 
corollary suppose positive numbers satisfying 
suppose ffi probability distribution probability gamma ffi independent examples chosen learner finds hypothesis satisfies generalization error ffl ffi log em log log ffi uc log em uc log log log function definition 
compare corollary theorem extra log factor arises fact consider possible permutations omitted subsequence general proof necessary direct argument fat shattering 
additional generality obtained support probability distribution need known may derive advantage observing points small norms giving better value fl obtained theorem priori bound 
fourth example final example generic nature indicate luckiness function computed estimated 
vary particular representation 
class functions write jx fh jx hg 
examples probably smooth luckiness functions definition consider hypothesis class define function function vcdim jx motivation example number different sources 
sontag showed result smoothly parametrized classes functions mild conditions sets general position size equal vc dimension class shattered vc dimension bounded half number parameters 
implies vc dimension super linear number parameters sets points 
fact shows nonempty open sets samples shattered 
consider hypothesis space multi layer sigmoidal neural network vc dimension quadratic number parameters possible vc dimension restricted particular sample linear number parameters 
learning results standard kind take advantage result get appropriately small sample size bounds conditions theorem hold 
luckiness function take advantage sontag result implicitly sense detect situation sontag predicts occur fact occurred 
exploit give better bounds generalization error 
motivation seen distribution dependent learning described shown classes infinite vc dimension may learnable provided distribution sufficiently concentrated regions input space set hypotheses low vc dimension 
problem analysis apparent way checking priori distribution concentrated way 
probable smoothness function definition shows effectively estimate distribution sample learn successfully witnesses region low vc dimension 
addition motivations approach mirrors closely taken lugosi pint er 
divide original sample part generate covering set functions hypothesis class metric derived function values points 
choose function cover minimises empirical error second half sample 
bound error function terms size cover derived set points 
size cover bounded vc dimension set hypotheses restricted points 
generalization effectively bounded terms vc dimension estimate derived sample 
bound obtain difficult compare directly expressed terms expected size cover 
addition estimator build potentially large empirical cover function class 
lugosi nobel extended examples probably smooth luckiness functions number ways particular general regression problems 
bounds terms expected size covers 
technical lemma analyses probabilities swapping group permutations argument 
group sigma consists permutations exchange corresponding points second halves sample mg 
lemma sigma swapping group permutations sample points xy 
consider fixed set points 
probability uniform distribution permutations exactly points half sample bounded proof result immediate pair corresponding positions opposite halves sample expression counts fraction permutations leave exactly points half 
rest proof concerned showing pairs occur opposite positions probability reduced 
probability pairs matched way 
case permutation points half number gamma trials succeed gamma trial having probability 
gamma gamma gamma note gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma result follow show relevant values function attains maximum value quadratic function negative coefficient square term maximum range interest strictly gamma gamma gamma gamma gamma range interest gamma gamma gamma gamma gamma gamma examples probably smooth luckiness functions result follows 
problem arise case view equation case single linked pair introduced 
point automatically half 
need consider 
equation worst case 
easily verified required 
proposition function definition probably smooth respect oe ffi ffi oe ffi em ffu ffu ff ln ffi proof ff ff ffi proposition statement 
result follow show high probability ratio vc dimensions obtained restricting single double samples greater ff 
formally expressed sufficient show phi xy ff vcdim jx ffi vcdim jx vcdim psi ffi oe gives bound growth function set functions vc dimension ffu vc dimension measured half sample 
argument bound probability 
vc dimension double sample consider points xy shattered stratify bound considering case points left hand side permutation 
lemma probability occurs bounded provided 
having points half violate condition ff ffi points left hand side vcdim jx ff ffi monotonically increasing bound probability condition violated summing probabilities examples probably smooth luckiness functions ff ffi satisfy equation ff ffi ffu suffices show buc buc ffi bound follows ed eff ffu ffi provided ff ffi ff log ffe log ffi lemma holds provided ff ln gamma ln ffi holds ff ffi ln ffi required 
corollary suppose ffi probability distribution probability gamma ffi independent examples chosen learner finds hypothesis satisfies addition bounds quantity vcdim jx generalization error ffl ffi ae ln ffi log em log ffi oe proof apply proposition theorem choosing 
observe corollary interpreted result effective 
similar notion introduced precise definition 
corollary result aware gives theoretical performance bound terms quantities determined empirically albeit potentially large computational cost 
aim show structural risk minimisation performed specifying advance stratification hypothesis class 
new inductive framework subclass resulting hypothesis depends relation observed data just predefined partition functions 
luckiness function data hypothesis captures stratification implicit approach probable smoothness property required ensure luckiness observed sample reliably infer lucky generalization 
shown vapnik maximal margin hyperplane algorithm example implementing strategy luckiness function ratio maximum size input vectors maximal margin observed 
lower bounds exist priori estimates generalization derived vc dimension bounds better generalization bounds result non random relation probability distribution target hypothesis 
evident maximal margin hyperplane case distribution concentrated away separating hyperplane 
different avenues pursued application ideas practical learning algorithms allows practitioners take advantage intuitions structure particular problems 
encapsulating ideas appropriate luckiness function potentially derive algorithms generalization bounds significantly better normal worst case pac estimates intuitions correct 
analytic point view questions raised 
corresponding lower bounds help place theory tighter footing help resolve role additional log factor introduced luckiness framework 
alternatively may possible refine proof definition probable smoothness eliminate apparent looseness bound 
exciting prospect theoretical angle possibility linking posteriori bounds generalization 
notable example bounds provided bayesian approach volume weight space consistent hypothesis treated manner luckiness function see example :10.1.1.51.7418
size maximal margin viewed way bounding volume weight space consistent hyperplane classification 
weight space volume estimators considered true volume probably smooth accurate estimation true volume requires sample points 
bayesian estimates placed framework role prior distribution source criticism approach transparent status bounds distribution independent 
vladimir vapnik useful discussions workshop artificial neural networks learning generalization statistics centre de recherches math ematiques universit de montr eal results 
carried part whilst john shawe taylor martin anthony visiting australian national university whilst robert williamson visiting royal holloway bedford new college university london 
supported australian research council esprit working group neural computational learning neurocolt nr 

martin anthony visit australia partly financed royal society 
done whilst authors icnn organizers providing opportunity 
noga alon shai ben david nicol cesa bianchi david haussler scale sensitive dimensions uniform convergence learnability proceedings conference foundations computer science focs 
appear journal acm 
martin anthony peter bartlett function learning interpolation technical report 
extended appeared computational learning theory proceedings nd european conference eurocolt pages ed 
paul vitanyi lecture notes artificial intelligence springer verlag berlin 
martin anthony norman biggs john shawe taylor learnability formal concepts pages proceedings third annual workshop computational learning theory rochester morgan kaufmann 
martin anthony john shawe taylor result vapnik applications discrete applied mathematics 
andrew barron approximation estimation bounds artificial neural networks machine learning 
andrew barron complexity regularization applications artificial neural networks pages ed 
nonparametric functional estimation related topics kluwer academic publishers 
andrew barron thomas cover minimum complexity density estimation ieee transactions information theory 
peter bartlett sample complexity pattern classification neural networks size weights important size network technical report department systems engineering australian national university may 
peter bartlett philip long prediction learning uniform convergence scale sensitive dimensions preprint department systems engineering australian national university november 
peter bartlett philip long robert williamson learnability real valued functions journal computer system sciences 
michael manfred opper perceptron learning largest version space neural networks statistical perspective 
proceedings ctp workshop theoretical physics world scientific 
available brain ac kr compressed ps bernhard boser isabelle guyon vladimir vapnik training algorithm optimal margin classifiers pages proceedings fifth annual workshop computational learning theory pittsburgh acm 
kevin kumar learning canonical smooth estimation part simultaneous estimation ieee transactions automatic control 
corinna cortes vladimir vapnik support vector networks machine learning :10.1.1.15.9362
thomas cover joy thomas elements information theory wiley new york 
richard duda peter hart pattern classification scene analysis john wiley sons new york :10.1.1.15.9362
girosi michael jones tomaso poggio regularization theory neural networks architecture neural computation pages 
leonid pascal koiran approximation learning convex superpositions pages paul vitanyi ed proceedings eurocolt lecture notes artificial intelligence springer berlin 
fundamentals artificial neural networks mit press cambridge ma 
david haussler decision theoretic generalizations pac model neural net learning applications information computation 
donald horne lucky country australia sixties penguin books victoria 
isabelle guyon vladimir vapnik bernhard boser leon bottou sara solla structural risk minimization character recognition pages john moody 
eds 
advances neural information processing systems morgan kaufmann publishers san mateo ca 
michael kearns robert schapire efficient distribution free learning probabilistic concepts pages proceedings st symposium foundations computer science ieee computer society press los alamitos ca 
pascal koiran eduardo sontag neural networks quadratic vc dimension appear nips journal computer system sciences available neurocolt technical report ftp ftp dcs rhbnc ac uk pub neurocolt tech reports 
kumar kevin learning canonical smooth estimation part learning choice model complexity ieee transactions automatic control 
nathan linial mansour ronald rivest results learnability vapnik chervonenkis dimension information computation 
nick littlestone learning quickly irrelevant attributes abound new linear threshold algorithm machine learning 
nick littlestone mistake driven bayes sports bounds symmetric learning algorithms technical report nec research center new jersey 
nick littlestone chris relative winnow preprint nec research center new jersey 
lennart ljung system identification theory user prentice hall ptr upper saddle river new jersey 
lugosi andrew nobel adaptive model selection empirical complexities preprint department mathematics computer science technical university budapest hungary 
lugosi pint er data dependent skeleton estimate learning pages proceedings ninth annual workshop computational learning theory association computing machinery new york 
lugosi kenneth zeger nonparametric estimation empirical risk minimization ieee transactions information theory 
lugosi kenneth zeger concept learning complexity regularization ieee transactions information theory 
david mackay bayesian model comparison backprop nets pages john moody 
eds 
advances neural information processing systems morgan kaufmann publishers san mateo ca 
david mackay probable networks plausible predictions review practical bayesian methods supervised neural networks preprint cavendish laboratory cambridge 
david pollard convergence stochastic processes springer new york 
john shawe taylor martin anthony norman biggs bounding sample size vapnik chervonenkis dimension discrete applied mathematics 
john shawe taylor peter bartlett robert williamson martin anthony framework structural risk minimization pages proceedings th annual conference computational learning theory association computing machinery new york 
eduardo sontag shattering sets points general position requires gamma parameters rutgers center systems control report neurocolt technical report nc tr ftp ftp dcs rhbnc ac uk pub neurocolt tech reports 
aad van der jon wellner weak convergence empirical processes springer new york 
vladimir vapnik estimation dependences empirical data springer verlag new york 
vladimir vapnik principles risk minimization learning theory pages john moody 
eds 
advances neural information processing systems morgan kaufmann publishers san mateo ca 
vladimir vapnik nature statistical learning theory springerverlag new york 
vladimir vapnik ja 
chervonenkis uniform convergence relative frequencies events probabilities theory probability applications 
vladimir vapnik ja 
chervonenkis ordered risk minimization ii automation remote control 
