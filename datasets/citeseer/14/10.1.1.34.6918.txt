dimensionality reduction unsupervised data dash liu yao department information systems computer science national university singapore kent ridge singapore nus sg dimensionality reduction important problem efficient handling large databases 
feature selection methods serve purpose supervised data record attached label 
little done dimensionality reduction unsupervised data class information available 
principal component analysis pca 
pca creates new features principal components functions original features 
difficult obtain intuitive understanding data new features 
concerned problem determining choosing important original features unsupervised data 
method observation removing irrelevant feature feature set may change underlying concept data 
propose entropy measure ranking features conduct extensive experiments show method able find important features class testing ranked features different classifiers compares similar method relief requires class information 
dimensionality reduction applications class labels readily known 
keywords data preprocessing unsupervised data entropy 
real world applications size database usually large 
largeness due excessive number features variables huge number instances records 
data mining tools efficiently try reduce data size 
focus reducing size decreasing number features 
approach determine relative importance features select subset important features 
achieved various ways example feature selection feature extraction 
number feature selection methods determine relative importance features selecting subset important ones 
examples relief poe acc :10.1.1.51.6297
typical feature selection method tries choose subset features original set ideally necessary sufficient describe target concept 
target concepts denoted class labels 
data class labels called supervised data data class labels unsupervised 
original features selected evaluation function methods require class labels 
feature selection methods largely fail unsupervised data 
feature extraction methods create new features uncorrelated retain variation possible database 
commonly known method principal component analysis pca 
pca need class labels extracting features principal components pcs usually values original features needed order calculate pcs pc function original features 
difficult get intuitive understanding data extracted features 
practice encounter databases large class information 
data normally collected organizational book keeping purposes data mining 
example transactional data contain class information huge number features 
desirable preprocess data applying knowledge discovery tool 
dimensionality reduction creating new feature offers effective solution decreasing data size 
pca feature selection methods help type dimensionality reduction due reasons 
new problem surfaces need data mining arises 
specifically deal new problem problem determining choosing important original features unsupervised data 
entropy measure determines importance original features introduced section 
measure works nominal continuous data types 
section sequential backward selection algorithm problem determining threshold number features discussed 
experimental study section shows proposed algorithm able find important features databases known important features 
shows performance proposed algorithm class labels close better relief generalized version relief multiple class labels ranks original features class labels concludes section 
entropy measure continuous nominal data types section introduce entropy measure determining relative importance variables 
measure applicable nominal continuous data types need class information evaluate variables entropy measures example information gain measure id 
dimension dimension class class class class class class dimension dimension class class class class class class iris data different variable dimensional spaces 
physical significance consider instance dataset described set variables 
data distinct clusters instance belong cluster instance separated instances close 
irrelevant variables influence relevant ones forming distinct clusters 
method observation removing irrelevant variable variable set may change underlying concept data 
consider set instances dimensional hyper space instance represented vector values im 
removing variable original set variables decreasing dimensionality hyper space project data gamma dimensional hyper space 
distinctness clusters remain projecting instances gamma dimensions variable important describing underlying structure data 
example iris data summarized table variables total 
show instances dimensional space subset 
removing variable equivalent projecting data dimensional plane subset removing variable equivalent projecting data dimensional plane subset 
examining figures notice data subset displays distinct clusters data subset 
implies variable important variable clusters subset roughly distinct clusters subset variable may removed variables allowed 
provide measure capture difference subsets comparing importance values 
measure unsupervised data class information priori information clusters classes 
need entropy measure evaluates distinctness underlying clusters subset variables 
say data orderly configurations distinct clusters chaotic configurations 
entropy theory know entropy probability orderly configurations configurations simple reason orderly configurations compared configurations 
entropy low close close distant pairs instances high close instances separated distance close mean distance 
similarity measure distance assumes small value close close pairs instances probably fall single cluster large value close distant pairs instances probably fall different clusters 
idea remove variables possible maintain level distinctness variables removed 
instances entropy measure gammas log gamma gamma log gamma assumes maximum value minimum value 
data set instances entropy measure gamma ij theta log ij gamma ij theta log gamma ij ij similarity value normalized instances variables numeric ordinal similarity value instances ij gammaff thetad ij ij distance instances ff parameter 
plot similarity vs distance curve bigger curvature larger ff 
experiments various values ff suggest robust kinds data sets just certain data sets 
ff calculated automatically assigning maximum value similarity formula 
fact produces results tested data sets 
mathematically ff gamma ln average distance instances hyper space 
ff determined data calculated easily 
euclidean distance measure calculate distance ij instances multi dimensional space defined ij ik gammax jk max max min maximum minimum values th dimension 
interval th dimension normalized dividing maximum interval max gamma min calculating distance 
similarity nominal variables measured hamming distance concept 
similarity value instances ij jx ik jk jx ik jk ik equals jk number variables subset consideration 
mixed data numeric nominal variables discretize numeric values applying measure 
algorithm find important variables sequential backward selection algorithm determine relative importance variables unsupervised data sud 
algorithm number variables originally data set 
sud original variable set gamma iteratively remove variables time variable determine variable removed gamma calculate tv eqn 
variable minimizes tv gamma remove important variable output iteration entropy calculated removing variable set remaining variables 
variable removed important variable set variable gives entropy 
continues till importance variables determined 
obtain ordered list variables reversing order removed 
variable important important 
interested dimensionality reduction naturally want know variables keep task 
know application needs variables simply choose variables 
automated selection vote zoo features sud error rate features sud typical trends improved performance increasing number variables 
complicated 
investigated issue experiments number data sets class labels 
ran sud removing class labels determined importance levels variables 
performed fold cross validation subset consisting important variables varies obtain average error rates 
results shown 
clearly show increasing may improve performance chosen classifier 
cases performance ceases improve certain point varies data set case parity data performance falls sharply variables added 
order determine data having class information may perform experiments choose number variables error rate fall 
possible may windowing technique find including variables performance improve extra variables 
experimental studies want show sud find important variables continuous nominal data performance comparable method requires class information sud 
method part test sud number data sets having continuous nominal variables important variables known 
part select relieff run sud class variable relief class variable total data sets having class variables class variables required run classifier find variables ordered importance compare sud relief performing fold cross validation oc subset consisting important variables varies data sets summary data sets table 
parity data modified version original parity data extra variables redundant irrelevant 
column table important variables shown data sets iris chemical plant non linear corral monk parity 
corral monk parity data sets target concepts data set data type classes parity nominal corral nominal monk nominal vote nominal nk mushroom nominal nk breast cancer nominal nk lymphography nominal nk zoo nominal nk iris continuous ionosphere continuous nk bupa continuous nk glass continuous nk pima diabetes continuous nk new thyroid continuous nk wine continuous nk chemical plant continuous non linear continuous table summary data sets 
important variables class variable continuous nk known defined earlier 
data set important variables decreasing order importance iris plant non linear monk corral parity table sud able find important variables data sets known important variables 
data decreasing order importance sud relief par corral monk vote breast zoo iris sphere bupa glass pima thyroid wine table order variables sud relief 
known 
important variables iris chemical plant results researchers 
iris data chiu liu setiono conclude petal length petal width important variables 
chemical plant non linear data sets taken sugeno 
chemical plant data conclude variables important 
nonlinear data output class variable input variables defined gamma variables data set irrelevant 
data sets specified machine learning repository university california irvine 
results produce experimental results tables figures 
table shows sud able find important variables data sets 
verified comparing list known literature 
class variable included data set 
sud assigns high importance redundant variables parity corral 
table shows order importance sud relieff data sets discrete class variables plant non linear data sets continuous class variables left 
compares sud relieff performing fold cross validation inducing axis parallel decision trees oc producing oblique decision trees subset consisting important variables run oc continuous data suggested run data 
graphs average error rates plotted axis number variables starting important variable axis 
shows performance sud class variable close par corral monk error rate features relief sud features relief sud error rate features relief sud vote breast error rate features relief sud features relief sud error rate features relief sud zoo iris error rate features relief sud features relief sud error rate features relief sud ionosphere bupa glass error rate features relief sud features relief sud error rate features relief sud pima thyroid wine error rate features relief sud features relief sud error rate features relief sud error rates fold cross validation variables selected sud class variable close lower variables selected relief class variable 
iris bupa glass error rate features relief sud features relief sud error rate features relief sud pima thyroid wine error rate features relief sud features relief sud error rate features relief sud error rate fold cross validation oc variables selected sud class variable close lower variables selected relief class variable 
better relief class variable 
focus solving problem reducing number original features unsupervised data problem concerns data mining large size data 
feature selection methods pca solves problem directly 
entropy measure proposed measure importance variable 
measure applicable numeric nominal data types 
sequential backward selection algorithm sud implemented determine relative importance features 
issue choosing important features discussed 
carried sets experiments show sud able find important features compares feature ranking algorithm requires class variables 
chiu 
method software extracting fuzzy classification rules subtractive clustering 
proceedings north american fuzzy information processing society conf 
june 
devijver kittler 
pattern recognition statistical approach 
prentice hall 
johan fast 
entropy significance concept entropy applications science technology chapter statistical significance entropy concept 
eindhoven philips technical library 
jolliffe 
principal component analysis 
springer verlag 
kira rendell 
feature selection problem traditional methods new algorithm 
proceedings ninth national conference ai 
george klir tina 
fuzzy sets uncertainty information chapter ch 
uncertainty information 
prentice hall international editions 
kononenko 
estimating attributes analysis extension reli ef 
proceedings european conference machine learn ing pages 
liu setiono 
chi feature selection discretization numeric attributes 
proceedings th ieee international conference tools artificial intelligence tat pages november 
sugeno 
fuzzy logic approach qualitative modeling 
ieee transactions fuzzy system vol february 

comparison techniques choosing subsets pattern recognition 
ieee transactions computers september 
sreerama murthy simon kasif steven salzberg 
system induction oblique decision trees 
journal artificial intelligence research 
quinlan 
induction decision trees 
machine learning pages 
morgan kaufmann 
quinlan 
programs machine learning 
morgan kaufmann 
uthurusamy 
data mining discovery current challenges directions 
fayyad piatetsky shapiro smyth uthurusamy editors advances knowledge discovery data mining pages 
aaai press mit press 
dubes jain 
critical evaluation intrinsic dimensionality algorithms 
gelsema kanal editors pattern recognition practice pages 
morgan kaufmann publishers 

