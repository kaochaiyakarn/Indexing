empirical comparison initialization methods means algorithm pe na lozano larra mail si ehu es www sc ehu es isg intelligent systems group dept computer science artificial intelligence university basque country box san spain aim compare empirically initialization methods means algorithm random forgy macqueen kaufman 
algorithm known robustness widely reported literature performance depends key points initial clustering instance order 
conduct series experiments draw terms mean maximum minimum standard deviation probability distribution square error values final clusters returned means algorithm independently initial clustering instance order initialization methods 
results experiments illustrate random kaufman initialization methods outperform rest compared methods means effective independent initial clustering instance order 
addition compare convergence speed means algorithm initialization methods 
results suggest kaufman initialization method induces means algorithm desirable behaviour respect convergence speed random initialization method 
keywords means algorithm means initialization partitional clustering genetic algorithms 
basic problems arises great variety fields including pattern recognition machine learning statistics called clustering problem 
fundamental data clustering problem may defined discovering groups data grouping similar objects 
groups called cluster region density objects locally higher regions 
data clustering viewed data partitioning problem 
approaches find groups database developed focus means algorithm iterative partitional clustering algorithms may initialize expensive clustering algorithms em algorithm 
known means algorithm suffers initial starting conditions effects initial clustering instance order effects 
main purpose compare classical initialization methods means algorithm criteria quality final clustering performed means algorithm concrete initialization method effectiveness sensitivity means algorithm initialization method initial starting conditions robustness 
secondary objective compare speedup convergence means algorithm concrete initialization method efficiency 
order reach set initialization method draw mean deviation probability distribution square error values final clusters means algorithms returns approach extremes genetic algorithms compute number iterations means algorithm needs converge 
remaining part laid follows 
section describe means algorithm point drawbacks motivate 
furthermore take look genetic algorithms 
section introduces initialization methods databases involved comparison 
experimental results summarized section 
section draw 
background means algorithm partitional clustering methods idea optimizing function referred clustering criterion hopefully translates intuitive notions cluster reasonable mathematical formula 
function value usually depends current partition database fc ck pk omega gamma ir pk omega gamma set partitions database omega fw wm nonempty clusters 
database omega dimensional vector 
concretely means algorithm finds locally optimal solutions clustering criterion sum distance element nearest cluster center centroid 
criterion referred square error criterion 
follows fc ck kw ij gamma number clusters number objects cluster ij th object th cluster centroid th cluster defined ij seen pseudo code means algorithm provided initial partition database centroids step select initial partition database clusters fc ck step calculate cluster centroids ij step database instance order step reassign instance closest cluster centroid moved kw gamma kw gamma step recalculate centroids clusters step cluster membership stabilized go step pseudo code means algorithm 
initial clusters calculated 
instances database relocated cluster represented nearest centroid attempt reduce square error 
relocation instances done instance order 
instance relocation step step changes cluster membership centroids clusters square error recomputed 
process repeated convergence square error reduced means instance changes cluster membership 
drawbacks means algorithm despite wide array applications means algorithm exempt drawbacks 
drawbacks extensively reported literature 
important listed ffl clustering methods means algorithm assumes number clusters database known obviously necessarily true real world applications ffl iterative technique means algorithm especially sensitive initial starting conditions initial clusters instance order ffl means algorithm converges finitely local minima 
running algorithm defines deterministic mapping initial solution final 
overcome lack knowledge real value database input parameter adopt rough usual approach try clustering values problem initial starting conditions exclusive means algorithm shared clustering algorithms hill climbing strategy deterministic behaviour leads local minima dependent initial solution instance order 
guarantee achieving global minima convergence means algorithm ensured 
milligan shows strong dependence means algorithm initial clustering suggests final cluster structures obtained ward hierarchical method provide means algorithm initial clusters 
fisher propose creating initial clusters constructing initial hierarchical clustering 
higgs suggest maxmin algorithm order select subset original database initial centroids establish initial clusters 
meila heckerman experimental results instance em algorithm reminiscent means different initialization methods hierarchical agglomerative clustering method 
initialization methods mentioned constitute initialization methods 
clustering methods means algorithm result hybrid clustering algorithm 
initialization methods suffer problem means algorithm provided initial clustering 
remaining part focus simpler inexpensive initialization methods constitute initialization complex clustering method see section 
reason motivates bradley fayyad develop algorithm refining initial seeds means algorithm 
overcome possible bad effects instance order fisher procedure order instances database 
show ordering instances consecutive observations dissimilar lead clusterings 
roure propose local strategy reduce effect instance ordering problem 
focus incremental clustering procedures strategy coupled particular procedure may adapted means algorithm 
genetic algorithms part main objective aim find best worst set initial starting conditions approach extremes probability distributions square error values 
due computational expense performing exhaustive search tackle problem genetic algorithms 
roughly speaking say genetic gas kinds evolutionary algorithms probabilistic search algorithms simulate natural evolution 
gas solve combinatorial optimization problems rules natural selection natural genetics 
survival fittest string structures structured randomized information exchange 
working way certain conditions gas evolve global optima probability arbitrarily close 
dealing gas search space problem represented collection individuals 
individuals represented character strings 
individual coding solution problem 
addition individual associated fitness measure 
part space examined called population 
purpose ga find individual search space best genetic material 
shows pseudo code ga 
initial population chosen fitness individuals determined 
iteration parents selected population 
parental couple produces children aga choose initial population random evaluate initial population convergence criterion select parents current population produce children selected parents mutate children evaluate children replace worst individual population best child output best individual aga 
pseudo code genetic algorithm 
probability near zero mutated hereditary distinctions changed 
evaluation children worst individual population replaced fittest children 
process iterated convergence criterion satisfied 
operators define children production process mutation process crossover operator mutation operator respectively 
operators applied different probabilities play different roles ga mutation needed explore new areas search space helps algorithm avoid local optima 
crossover aimed increase average quality population 
choosing adequate crossover mutation operators appropriate reduction mechanism probability ga reaches near optimal solution reasonable number iterations increases 
experimental results mentioned main purpose classify classical initialization methods criteria quality final clustering returned means algorithm initialization methods sensitivity means algorithm initialization method initial starting conditions 
addition main objective interested convergence speed means algorithm compared initialization methods 
initialization methods compare ffl random divide database partition clusters random 
usual initialization method ffl forgy approach fa proposed forgy see choose instances database seeds random assign rest instances cluster represented nearest seed step select seed centrally located instance step instance step instance calculate ji max gamma ji ji kw gamma min sj selected seeds step calculate gain selecting ji step select selected instance maximizes ji step selected seeds go step step having clustering assign instance cluster represented nearest seed 
pseudo code ka initialization method 
ffl macqueen approach ma proposed macqueen choose instances database seeds random 
assign instance order rest instances cluster nearest centroid 
assignment recalculation centroids carried ffl kaufman approach ka proposed kaufman rousseeuw 
case initial clustering obtained successive selection representative instances instances 
representative instance centrally located instance database 
rest representative instances selected heuristic rule choosing instances promise higher number rest 
see pseudo code 
interesting differences initialization methods ka deterministic ii random fa generate initial partition independently instance order iii ma generates initial partition dependently instance order 
obvious differences computational expenses initialization methods exist 
carry experiments known real world databases machine learning repository ffl iris database instances attributes clusters ffl ruspini database instances attributes clusters ffl glass database instances attributes 
clusters grouped bigger clusters 
said disadvantages means algorithm assumes number clusters known input 
iris database ruspini database glass database 
drawing probability distributions note means algorithm initialization method completely deterministic concrete initial starting conditions 
objective measure effectiveness robustness means algorithm proposed initialization method independently concrete initial starting conditions aim draw mean maximum minimum standard deviation probability distributions square error values marginalizing influence initial partition instance order 
due enormous size space initial starting conditions possible carry marginalizing process reasonable time 
propose sampling space initial starting conditions initialization method order approach probability distributions 
sampling process carried different way initialization method dedicate subsections explain sampling methods 
sampling process random fa random fa generate initial partition independently instance order carried sample space initial starting conditions steps 
firstly sampled space initial partitions random obtaining fp pm secondly sampled space instance orders individual sample space initial partitions random obtaining fo th individual initial partitions fp pm doing constructed sample space initial starting conditions om initial partitions generated random follows random clusters generated random fa seeds generated random fa performed 
means run individual sample space initial starting conditions ij marginalized influence instance order running means algorithm starting approaching ij mean square error values starting independently instance order ij square error value starting instance order ij probability distribution square error values approached values number initial partitions number instance orders 
repeated process different values different databases referred 
sampling process ma generate initial partition ma independently instance order random fa 
initialization method generated delta instance orders random 
divided sample groups instance orders random resulting fom gg 
ma initialization method applied group fo seeds generated random obtaining set initial partitions fp respectively able compose sample space initial starting conditions set om mn means algorithm run individual sample space initial starting conditions ij ij marginalized influence instance order running means algorithm starting seeds generate fp approaching ij ij mean square error values starting seeds generate fp independently instance order ij ij value starting ij instance order ij probability distribution square error values approach values number initial partitions number instance orders 
repeated process different values different databases referred 
sampling process ka ka returns initial partition number clusters database needed sample space initial starting conditions dimension space instance orders 
generated sample space initial starting conditions initial partition instance order generated random 
probability distribution square error values approached values square error value starting instance order number instance orders 
repeated process different values different databases referred previous section 
results show shape drawn probability distributions 
figures histograms illustrates hit counts different square error values obtained means algorithm initialization methods concrete database concrete value note presence local optima histograms 
table summarizes probability distributions sample mean sample standard deviation gamma 
table conclude random ka initialization methods outperform rest compared initialization methods 
addition induce effective behaviour means algorithm initialization methods random ka means algorithm exhibit robust behaviour note sample standard deviation shorter random ka 
feature illustrating low sensitivity means algorithm initial starting conditions random ka 
similar result random initialization method 
despite fact ka performs slightly better random cases ruspini database hard choose ka random 
random fa ma ka histograms iris database 
hit counts axis different square error values axis initialization methods 
random fa ma ka histograms iris database 
hit counts axis different square error values axis initialization methods 
random fa ma ka histograms ruspini database 
hit counts axis different square error values axis initialization methods 
random fa ma ka histograms ruspini database 
hit counts axis different square error values axis initialization methods 
random fa ma ka histograms glass database 
hit counts axis different square error values axis initialization methods 
random fa ma ka histograms glass database 
hit counts axis different square error values axis initialization methods 
random fa ma ka histograms glass database 
hit counts axis different square error values axis initialization methods 
defer selection ka random evidence appears 
conducted nonparametric tests kruskal wallis variance analysis concrete database considered value 
purpose tests see independent samples space final clustering values corresponding initialization methods database concrete value population 
results nonparametric tests performed distributions significantly similar ff 
looking extremes probability distributions samples space initial starting conditions previous section considerable size sure best worst set initial starting conditions included samples 
order information extremes probability distributions square error values aim find best worst set initial starting conditions 
due computational expense performing exhaustive search ga 
ga works population individuals 
encoding instance order individual permutation mg set instances database 
initial population generated random 
fitness individual population square error value returned running means algorithm 
instance order encoded individual 
initial partition generated initialization iris ruspini glass random gamma fa gamma ma gamma ka gamma table experimental results mean sample standard deviation sn gamma gammaf gamma 
method extremes looking 
parental couple selected means biased range selection process 
produce children order crossover operator 
mutation probability mutation operator swapping couple elements permutation encoded children strings 
best children strings added population obtaining intermediate population 
worst individuals removed intermediate population obtaining new population 
iterate process improvement iterations 
obtain results resume table run ga described times combination databases considered values database compared initialization methods 
entry table resumes best square error value ga reaches positive optimization left extreme probability distribution 
order complete table follow process time want worst square error value negative optimization right extreme probability distribution table shows initialization methods means algorithm able reach supposed best final partition 
table summarizes worst square error values ga reaches 
initial partition obtained fa ma reached random 
random behave bad fa ma 
conclude table known extremes random quite difficult fall worst extreme random initialization method ga able 
table shows ka worst case better rest means algorithm unable reach bad final partitions rest methods 
property ka assuming ga unable reach real extreme happens random initial partition reached fa ma reached ka assume ka bad ma fa 
iris ruspini glass random fa ma ka table best square error values ga entry left extremes probability distributions best instance order 
iris ruspini glass random fa ma ka table worst square error values ga entry right extremes probability distributions worst instance order 
convergence speed table reveals ma initialization method means algorithm reach earlier convergence 
hand random initialization method induces means algorithm slowest convergence speed 
interested comparing random ka initialization methods induce means algorithm best performance 
conclude table ka means algorithm need number iterations converge random cases 
cases means algorithm initialized random may need iterations average converge initialized ka 
hand remaining cases random induces means algorithm efficient behaviour ka ka implies need iteration average means algorithm converge random 
ka preferred random respect convergence speed means algorithm 
widely reported means algorithm suffers initial starting conditions effects 
keeping mind idea main purpose compare empirically classical initialization methods means algorithm 
comparison done effectiveness robustness convergence speed means algorithm initialization methods 
concluded random ka initialization methods outperformed methods respect effectiveness robustness means algorithm initialization methods 
addition ka exhibited desirable behaviour means algorithm unable reach bad partitions rest iris ruspini glass random ite sd ite fa ite sd ite ma ite sd ite ka ite sd ite table mean ite sample standard deviation sd ite number iterations means algorithm initialization method 
compared methods 
study convergence speed induced initialization methods able conclude ka showed capability induce means algorithm desirable behaviour random 
note random usually considered standard initialization method means algorithm empirical comparison shows election performance 
similar result respect random majority databases machine learning repository 
acknowledgments authors anonymous referees useful comments related 
research supported spanish de cultura ap 
anderberg 

cluster analysis applications 
academic press new york ny 
banfield raftery 

model gaussian non gaussian clustering 
biometrics 
bishop 

neural networks pattern recognition 
oxford university press oxford 
bradley fayyad 

refining initial points means clustering 
proceedings fifteenth international conference machine learning 
morgan kaufmann publishers san francisco ca 
pinson 

analyse 
th applications 
masson paris 
cheeseman stutz 

bayesian classification autoclass theory results 
advances knowledge discovery data mining 
aaai press menlo park ca 
davis 

applying adaptive algorithms epistatic domains 
proceedings international joint conference artificial intelligence 
fisher 

knowledge acquisition incremental conceptual clustering 
machine learning 
fisher xu 

ordering effects clustering 
proceedings ninth international conference machine learning 
morgan kaufmann publishers san mateo ca 
fisher 

iterative optimization simplification hierarchical clusterings 
journal artificial intelligence research 
forgy 

cluster analysis multivariate data efficiency vs interpretability classifications 
biometrics 
fukunaga 

statistical pattern recognition 
academic press san diego ca 
goldberg 

genetic algorithms search optimization machine learning 
addison wesley massachussets 
hartigan 

clustering algorithms 
john wiley sons canada 
higgs watson 

experimental designs selecting molecules large chemical databases 
chem 
inf 
comput 
sci 
holland 

adaptation natural artificial systems 
university michigan press michigan 
jain dubes 

algorithms clustering data 
prentice hall englewood cliffs 
kaufman rousseeuw 

finding groups data 
cluster analysis 
john wiley sons canada 
kruskal wallis 

ranks criterion analysis variance 
amer 
statist 
assoc errata ibid 
macqueen 

methods classification analysis multivariate observations 
proc 
symp 
math 
probability th berkeley ad 
university california press berkeley ca 
mclachlan basford 

mixture models 
marcel dekker new york ny 
meila heckerman 

experimental comparison clustering initialization methods 
proceedings fourteenth conference uncertainty artificial intelligence 
morgan kaufmann publishers san francisco ca 
merz murphy aha 

uci repository machine learning databases 
department information computer science university california irvine 
www ics uci edu mlearn mlrepository html 
milligan 

examination effect types error perturbation fifteen clustering algorithms 
psychometrika 
roure 

robust incremental clustering bad instance orderings new strategy 
proceedings sixth conference artificial intelligence 
coelho eds lisbon 
ismail 

means type algorithms generalized convergence theorem characterization local optimality 
ieee transactions pattern analysis machine intelligence 
willet 

comparison algorithms dissimilarity compound selection 
mol 
graphics modelling 
tou gonz alez 

pattern recognition principles 
addison wesley reading ma 
ward 

hierarchical grouping optimize objective function 
journal american statistical association 

