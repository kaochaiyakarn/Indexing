learning imbalanced data sets comparison various strategies japkowicz faculty computer science university university halifax nova canada mail nat cs dal ca majority concept learning systems previously designed usually assume training sets balanced assumption necessarily correct 
exists domains class represented large number examples represented 
purpose demonstrate experimentally case connectionist systems class imbalances hinder performance standard classifiers compare performance approaches previously proposed deal problem 

field machine learning rapid transition status academic discipline applied science myriad new issues previously considered machine learning community coming light 
issue class imbalance problem 
class imbalance problem corresponds domains class represented large number examples represented 
class imbalance problem crucial importance encountered large number domains great environmental vital commercial importance shown certain cases cause significant bottleneck performance attainable standard learning methods assume balanced distribution classes 
example problem occurs hinders classification applications diverse detection oil spills satellite radar images kubat holte matwin detection fraudulent telephone calls fawcett provost flight helicopter fault monitoring japkowicz myers gluck 
point attempts dealing class imbalance problem paz danny silver helpful comments draft 
consider case 
discussion applies multi class problems 
japkowicz myers gluck ling li kubat matwin fawcett provost kubat holte matwin attempts conducted isolation 
particular date systematic strive link specific types imbalances degree inadequacy standard classifiers 
furthermore comparison various methods proposed remedy problem performed 
purpose address concerns attempt unify research conducted problem 
part concentrates finding type imbalance damaging standard classifier expects balanced class distributions second part implementations categories methods previously proposed tackle problem tested compared domains part 
remainder divided sections 
section statement specific questions asked study 
section describes part study focusing types class imbalance problems create difficulties standard classifier 
section describes part study designed compare categories approaches previously attempted considered problems section 
sections conclude 

questions interest study thought step investigation questions question types imbalances hinder accuracy performance standard classifiers 
question approaches dealing class imbalance problem appropriate 
questions important answers may suggest fruitful directions research 
particular may help researchers focus inquiry particular type solution promising particular characteristics identified application domain 
question raises issue class imbalances damaging 
studies previously mentioned identified specific domains imbalance shown hurt performance certain standard classifiers discuss questions imbalances damaging extent different types imbalances affect classification performances 
takes global stance answers questions context classifier series artificial domains spanning large combination characteristics 
question considers categories approaches previously proposed independent researchers tackling class imbalance problem 
methods class represented small data set gets sampled match size class ling li 

methods class represented large data set sized match size class kubat matwin 

methods ignore little classes altogether recognition discrimination inductive scheme japkowicz myers gluck kubat holte matwin 
quest part study aimed finding approaches appropriate certain specific domain conditions 
order answer question scheme implemented closely related methods various versions discrimination recognition mlp networks attempt limit amount bias introduced different unrelated learning paradigms 
schemes tested artificial domains previously generated answer question 
advised test systems hypotheses interest real world domains desirable study 
study intended suggest new directions research purpose artificial domains best suited allow various domain characteristics controlled 
refers standard multi layer perceptron trained associate output value instances positive class output value instances negative class rumelhart hinton williams 
concentrates domains balanced imbalance imbalance affects subcluster small class extent 
lack space interesting issue imbalanced imbalances left research 
discussed section japkowicz myers gluck 
complexity class class backbone model complexity 
class imbalance matter 
order answer question series artificial concept learning domains generated varies different dimensions degree concept complexity size training set level imbalance classes 
standard classifier system tested domain simple system described rumelhart hinton williams 
section discusses domain generation process followed report results obtained various domains 
domain generation experiments section domains created various combinations concept complexity training set size degree imbalance 
generation method inspired schaffer designed similar framework testing effect overfitting avoidance sparse data sets 
data generation schemes number differences 
detail generated domains dimensional inputs range associated classes 
input range divided number regular intervals intervals size associated different class value 
contiguous intervals opposite class values degree concept complexity corresponds number alternating intervals domain 
actual training sets generated backbone models sampling points random uniform distribution intervals 
number points sampled interval depends size domain degree imbalance 
example backbone model shown 
different complexity levels considered level corresponds backbone model composed regular intervals 
example domains generated complexity level point input range associated class value point input range associated class value complexity level points intervals associated class value intervals associated class value regardless size training set degree imbalance 
training set sizes considered size corresponds training set size round 
training set size includes regular intervals domain regular interval fact represented round training points imbalance factor considered 
example size level complexity level imbalance taken consideration intervals represented examples size complexity level intervals contains training examples levels class imbalance considered level corresponds situation sub interval class represented data normally entitled sub interval class contains th rounded normally entitled data 
means sub intervals class represented round training examples 
example interval represented examples represented represented examples represented reported results number testing points representing sub interval kept fixed 
means domains complexity level tested positive negative examples domains complexity level tested positive negative examples results results displayed plots error obtained combination concept complexity training set size imbalance level 
plot represents plot obtained different size 
leftmost plot corresponds smallest size progresses rightmost plot corresponds largest 
plots cluster bars represent concept complexity level 
leftmost cluster corresponds simplest concept progresses rightmost corresponds complex 
cluster bar corresponds particular imbalance level 
leftmost bar corresponds imbalanced level progresses rightmost bar corresponds balanced level imbalance 
height bar represents average percent error rate obtained runs different domains generated backbone complexity varied single simple dimension 
sophisticated models order obtain finer grained results 
model domain bar represents 
please note graphs indicate large amount variance results despite fact results averaged different trials 
derived graphs reflect general trends specific results 
scaling different graph necessarily lines drawn percent error marks order facilitate interpretation results 
performance depends number hidden units uses experimented hidden units reported results obtained optimal network capacity 
default values kept fixed networks trained levenberg marquardt optimization method learning rate set networks trained maximum epochs performance gradient descended gamma threshold discrimination classes set 
means results reported posteriori checking possible network capacities best results reported 
fact experiment re ran times believed posteriori view sufficient especially systems tested conditions 
results indicate points interest 
matter size training set linearly separable domains domains complexity level appear sensitive amount imbalance 
related observation fact degree concept complexity increases point problem obtains acceptable accuracy domain balanced complexity levels particular case system sensitivity imbalances 
gap different imbalance levels increase degree concept complexity increases plots 
observed size training set appear factor size error rate gap balanced imbalanced data sets 
suggests imbalance problem relative problem depends proportion imbalance experienced domain problem intrinsic training set size meaningless say system perform poorly domain contains negative training examples specifying size positive class 
research aimed existing discrimination tools developed balanced training sets exploring possibility learning recognition concentrate finding ways decrease complexity imbalanced domains re balancing imbalanced domains means decreasing size note small class size inherently harmful issue separate considered 
size size size size size graph displays classification error levels obtained different training set size function difficulty concept cluster bars imbalance level bar cluster different training set size 
scaling different graph necessarily lines drawn percent error marks order facilitate interpretation results 
graphs show degree concept complexity increases sensitivity class imbalances 
training set sizes affect result 
training set 

comparison various strategies having identified domains class imbalance impair accuracy regular classifier section proposes compare methodologies proposed deal problem 
various schemes comparison described followed report performance 
comparing specific methods study compares various kinds methods 
methods implemented connectionist paradigm closely related minimize differences performance caused phenomena particular methodology 
schemes dealing class imbalances sampling sampling method considered category rand consists resampling small class random contains examples class 
sizing sizing method closely related sampling method considered category rand downsize consists eliminating random elements sized class matches size class 
learning recognition method considered category autoassociation classification approach described japkowicz myers gluck 
approach consists training multi layer perceptron designed reconstruct input output layer reconstruct class domain output layer 
training achieved classification relying idea network generalize novel instance reconstruct input output layer accurately instance class trained generalization fails reconstruction error large instance class 
training scheme represented class domain recog 
domain threshold discriminating recognized non recognized examples set comparing accuracy obtained different threshold values regularly generated function mean deviation reconstruction errors obtained training set retaining yielding optimal classification performance 
results results rand rand downsize recog reported figures respectively 
results indicate methodologies generally help improve results 
homogeneously 
detail rand rand downsize effective especially concept complexity gets large larger 
methods obtain comparable results small sized domains rand downsize gains advantage rand small training set size increases 
hand performance recog generally results accurate rand rand downsize 
complexity concept reaches assume problem recognizing class simpler discriminating classes recog slightly accurate rand rand downsize 
results displayed figures suggest methods worth exploring help improve accuracy standard classifier designed classify balanced data sets 
just discussed rand rand downsize approaches worth studying quite effectively help improve classification class size size size size size rand size size size size size rand downsize size size size size size recog imbalanced data sets 
recog method prove accurate domains tested expected effective rand rand downsize methods amount data class drastically limited 
recognition methods may prove effective autoassociation classification worth exploring 
related experiments conducted fully reported space limitations 
reported japkowicz 
experiments consisted sampling smaller data set focused manner concentrating data located close boundaries sizing larger data set concentrating saving points located near boundaries training minority majority class 
results obtained experiments indicate simple artificial domains clear advantage sophisticated re sampling sizing schemes 
hand results indicate clearly better learn recognize majority class minority 

purpose unify research conducted isolation problem class imbalance guide research area 
concerned issues class imbalance problem matter 
various categories methods attempted solve problem different realizations compare 
concluded standard multi layer perceptron sensitive class imbalance problem applied linearly separable domains sensitivity increases complexity domain 
size training set appear factor 
showed sampling minority class sizing majority class effective methods dealing problem sizing approach works better sampling approach large domains 
recognition approach shown potential help current realization needs improvement 
additional study reported japkowicz showed sophisticated sampling sizing methods simple uniformly random approach appears unnecessary case feedforward neural networks simple artificial domains approach works definitely better applied majority class 

directions left explore 
mentioned footnote useful test different types imbalances far balanced imbalances considered 
imbalanced imbalances different subclusters class different numbers examples representing surveyed 
second issue type classifier 
study feedforward neural networks considered 
results reported may consequently closely linked particular technology worthwhile check performance problems section standard classifiers nearest neighbours 
useful explore performance various sampling sizing schemes re sample data point re sample twice times recognition approaches especially incorporating counter examples kubat holte matwin 
category methods proceeds biasing directly takes consideration class imbalances see pazzani example tested compared methods considered 
fawcett provost 
adaptive fraud detection 
data mining knowledge discovery 
japkowicz myers gluck 
novelty detection approach classification 
proceedings fourteenth joint conference artificial intelligence 
japkowicz 
class imbalance problem significance strategies 
proceedings international conference artificial intelligence 
kubat matwin 
addressing curse imbalanced data sets sided sampling 
proceedings fourteenth international conference machine learning 
morgan kauffmann 
kubat holte matwin 
machine learning detection oil spills satellite radar images 
machine learning 
ling li 
data mining direct marketing problems solutions 
kdd 
pazzani merz murphy ali hume brunk 
reducing misclassification costs 
proceedings eleventh international conference machine learning 
morgan kaufmann 
rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland eds parallel distributed processing 
cambridge ma mit press 

schaffer 
overfitting avoidance bias 
machine learning 
