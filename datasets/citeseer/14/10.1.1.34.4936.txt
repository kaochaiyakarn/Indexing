variational learning switching state space models zoubin ghahramani geo rey hinton gatsby computational neuroscience unit university college london queen square london wc ar uk email zoubin gatsby ucl ac uk submitted neural computation introduce new statistical model time series iteratively segments data regimes approximately linear dynamics learns parameters linear regimes 
model combines generalizes widely stochastic time series models hidden markov models linear dynamical systems closely related models widely control econometrics literatures 
derived extending mixture experts neural network jacobs fully dynamical version expert gating networks recurrent 
inferring posterior probabilities hidden states model computationally intractable exact expectation maximization em algorithm applied 
variational approximation maximizes lower bound log likelihood forward backward recursions hidden markov models kalman lter recursions linear dynamical systems 
tested algorithm arti cial data sets natural data set respiration force patient sleep 
results suggest variational approximations viable method inference learning switching state space models 
commonly probabilistic models time series descendants hidden markov models hmm stochastic linear dynamical systems known state space models ssm 
hidden markov models represent information past sequence single discrete random variable hidden state 
prior probability distribution state derived previous hidden state stochastic transition matrix 
knowing state time past observations statistically independent 
markov independence property gives model name 
state space models represent information past real valued hidden state vector 
conditioned state vector past observations statistically independent 
dependency state vector previous state vector speci ed dynamic equations system noise model 
equations linear noise model gaussian state space model known linear dynamical system kalman lter model 
unfortunately real world processes characterized purely discrete purely linear gaussian dynamics 
example industrial plant may multiple discrete modes behavior approximately linear dynamics 
similarly pixel intensities image translating object vary approximately linear dynamics subpixel translations image moves larger range dynamics change signi cantly nonlinearly 
addresses models dynamical phenomena characterized combination discrete continuous dynamics 
introduce probabilistic model called switching state space model inspired divide conquer principle underlying mixture experts neural network jacobs 
switching state space models natural generalization hidden markov models state space models dynamics transition discrete manner linear operating regime 
large literature models kind econometrics signal processing elds harrison stevens chang athans hamilton shumway sto er bar shalom li 
extend models allow multiple real valued state vectors draw connections elds relevant literature neural computation probabilistic graphical models derive learning algorithm parameters model structured variational approximation rigorously maximizes lower bound log likelihood 
organized follows 
section review background material statespace models hidden markov models hybrids 
section describe generative model probability distribution de ned observation sequences switching state space models 
section describe learning algorithm switching state space models structured variational approximation em algorithm 
section simulation results arti cial domain assess quality approximate inference method natural domain 
conclude section 
background state space models state space model de nes probability density time series real valued observation vectors fy assuming observations generated sequence hidden state vectors fx particular state space model speci es hidden state vector time step observation vector time step statistically independent observation vectors hidden state vectors obey markov independence property 
joint probability sequences states observations factored fx jx jx jx conditional independencies speci ed equation expressed graphically form 
simplest commonly models kind assume transition output functions linear time invariant distributions state observation variables multivariate gaussian 
term state space model refer simple form model 
models state transition function ax state transition matrix zero mean gaussian noise dynamics covariance matrix assumed gaussian 
equation ensures gaussian 
output function cx output matrix zero mean gaussian output noise covariance matrix jx gaussian jx jrj exp cx cx dimensionality vectors 
observation vector divided input predictor variables output response variables 
model input output behavior system conditional probability output sequences input sequences linear gaussian ssm modi ed state transition function ax bu input observation vector xed input matrix 
table describing variables notation provided appendix de ne state ax bu directed acyclic graph dag specifying conditional independence relations state space model 
node conditionally independent non descendents parents output conditionally independent variables state conditionally independent gure gures shaded nodes represent observed variables unshaded nodes represent hidden variables 
problem inference state estimation state space model known parameters consists estimating posterior probabilities hidden variables sequence observed variables 
local likelihood functions observations gaussian priors hidden states gaussian resulting posterior gaussian 
special cases inference problem considered ltering smoothing prediction anderson moore goodwin sin 
goal ltering compute probability current hidden state sequence inputs outputs time fug 
recursive algorithm perform computation known kalman lter kalman bucy 
goal smoothing compute probability sequence inputs outputs time kalman lter forward direction compute probability fy fug similar set backward recursions complete computation accounting observations time rauch 
refer combined forward backward recursions smoothing kalman smoothing recursions known rts rauch tung smoother 
goal prediction compute probability states observations observations upto time fug computed model simulated forward direction equations inputs compute probability density state output time problem learning parameters state space model known engineering system identi cation problem general form assumes access sequences input output observations 
focus maximum likelihood learning single locally optimal value parameters estimated bayesian approaches treat parameters random variables compute approximate posterior distribution parameters data 
distinguish line line approaches learning 
line recursive algorithms favored real time adaptive control applications obtained computing gradient second derivatives log likelihood ljung om 
similar gradient methods obtained line methods 
alternative method line learning expectation maximization em algorithm dempster 
procedure iterates step xes current parameters computes posterior probabilities hidden states observations step maximizes expected log likelihood parameters posterior distribution computed step 
linear gaussian state space models step exactly kalman smoothing problem de ned step simpli es linear regression problem shumway sto er digalakis 
details em algorithm state space models ghahramani hinton original shumway sto er 
hidden markov models hidden markov models de ne probability distributions sequences observations fy distribution sequences obtained specifying distribution observations time step discrete hidden state probability transitioning hidden state 
markov property joint probability sequences states observations factored notation fy short hand sequence exactly manner equation place fs js js js similarly conditional independencies hmm expressed graphically form 
state represented single multinomial variable take discrete values kg 
state transition probabilities js speci ed transition matrix 
observables discrete symbols values observation probabilities js fully speci ed observation matrix 
continuous observation vector js modeled di erent forms gaussian mixture gaussians neural network 
hmms applied extensively problems speech recognition juang rabiner computational biology baldi fault detection smyth 
hmm known parameters sequence observations algorithms commonly solve di erent forms inference problem rabiner juang 
rst computes posterior probabilities hidden states recursive algorithm known forward backward algorithm 
computations forward pass exactly analogous kalman lter ssms computations backward pass analogous backward pass kalman smoothing equations 
noted bridle personal communication smyth heckerman jordan forward backward algorithm special case exact inference algorithms general graphical probabilistic models lauritzen spiegelhalter pearl 
observation holds true kalman smoothing recursions 
inference problem commonly posed hmms compute single sequence hidden states 
solution problem viterbi algorithm consists forward backward pass model 
learn maximum likelihood parameters hmm sequences observations known baum welch algorithm baum 
algorithm special case em uses forward backward algorithm infer posterior probabilities hidden states step 
uses expected counts transitions observations re estimate transition output matrices linear regression equations case observations gaussian distributed 
statespace models hmms augmented allow input variables model conditional distribution sequences output observations sequences inputs cacciatore nowlan bengio frasconi meila jordan 
hybrids burgeoning literature models combine discrete transition structure hmms linear dynamics ssms developed elds ranging econometrics control engineering harrison stevens chang athans hamilton shumway sto er bar shalom li deng 
models known alternately hybrid models state space models switching jump linear systems 
brie review literature including related neural network models 
shortly kalman bucy solved problem state estimation linear gaussian state space models attention turned analogous problem switching models fu 
chang athans derive equations computing conditional mean variance state parameters linear state space model switch arbitrary markovian dynamics 
prior transition probabilities switching process assumed known 
note models sets parameters observation length exact conditional distribution state gaussian mixture components 
conditional mean variance require far computation summary statistics 
shumway sto er consider problem learning parameters state space models single real valued hidden state vector switching output matrices 
probability choosing review state space models hmms related simpler statistical models pca factor analysis mixture gaussians vector quantization independent components analysis ica roweis ghahramani 
directed acyclic graphs specifying conditional independence relations various switching statespace models 
shumway sto er output matrix equation switches independently xed number choices time step 
setting represented discrete hidden variable bar shalom li output equation dynamic equation switch switches markov kim fraser outputs states observed 
shown simple case output depends directly current state previous state previous output 
particular output matrix pre speci ed time varying function independent previous choices 
pseudo em algorithm derived step exact form require computing gaussian mixture components approximated single gaussian time step 
bar shalom li sections review models state dynamics output matrices switch switching follows markovian dynamics 
di erent methods approximately solving state estimation problem switching models discuss parameter estimation models 
methods referred generalized gpb interacting multiple models imm idea collapsing gaussian mixture gaussians results considering settings switch state time step 
avoids exponential growth mixture components cost providing approximate solution 
sophisticated computationally expensive methods collapse gaussians gaussians derived 
kim derives similar approximation closely related model includes observed input variables 
furthermore kim discusses parameter estimation model making em algorithm 
authors markov chain monte carlo methods state parameter estimation switching models carter kohn related dynamic probabilistic networks dean kanazawa kanazawa 
hamilton section describes class switching models real valued observation time depends observations times discrete states time precisely gaussian mean linear function binary indicator variables discrete states system seen th order hidden markov model driving th order auto regressive process tractable small number discrete states hamilton models closely related hidden filter hmm fraser 
discrete real valued states 
real valued states assumed observed known deterministic function past observations embedding 
outputs depend states previous outputs form dependence switch randomly 
time step hidden variable switch state exact inference model carried tractably 
resulting algorithm variant forward backward procedure hmms 
kehagias petridis pawelzik 
variants model 
elliott 
section inference algorithm hybrid markov switching systems separate observable switch state estimated 
true switch states represented unit vectors estimated switch state vector unit square elements corresponding estimated probability switch state 
real valued state approximated gaussian estimated switch state forming linear combination transition observation matrices di erent ssms weighted estimated switch state 
derive control equations hybrid systems discuss applications change measures whitening procedure large family models 
regard literature neural computation model generalization mixture experts neural network jacobs jordan jacobs related mixture factor analyzers hinton ghahramani hinton 
previous dynamical generalizations mixture experts architecture consider case gating network markovian dynamics cacciatore nowlan meila jordan 
limitation generalization entire past sequence summarized value single discrete variable gating activation system experts convey average log bits information past 
models consider experts gating network markovian dynamics 
past summarized state composed cross product discrete variable combined real valued state space experts 
provides wider information channel past 
advantage representation real valued state contain componential structure 
attributes position orientation scale object image naturally encoded independent real valued variables accommodated state exponential growth required discretized hmm representation 
important place context literature just reviewed 
hybrid models state space switching jump linear systems described assume single real valued state vector 
model considered generalizes multiple real valued state vectors 
models described hamilton fraser current dynamical extensions mixtures experts model real valued state vectors hidden 
inference algorithm derive making structured variational approximation entirely novel context switching state space models 
speci cally method approximate methods reviewed tting single gaussian mixture gaussians computing mean covariance mixture 
derive learning algorithm parameters model including markov switching parameters 
algorithm maximizes lower bound log likelihood data heuristically motivated approximation likelihood 
algorithm simple intuitive avor decouples forward backward recursions hidden markov model kalman smoothing recursions state space model 
states hmm determine soft assignment observation state space model prediction errors state space models determine observation probabilities hmm 
note state vectors concatenated large state vector factorized block diagonal transition matrices cf 
factorial hidden markov model ghahramani jordan 
obscures decoupled structure model 
classes methods seen minimizing kullback liebler kl divergences 
kl divergence asymmetrical variational methods minimize direction methods merge gaussians minimize direction 
return point section 
graphical model representation switching state space models 
discrete switch variable real valued state vectors 
switching state space model depicted generalization mixture experts 
light arrows correspond connections mixture experts 
switching state space model states experts gating network depend previous states dark arrows 
generative model switching state space models sequence observations fy modeled specifying probabilistic relation observations hidden state space comprising real valued state vectors discrete state vector discrete state modeled multinomial variable take values mg reasons obvious refer switch variable 
joint probability observations hidden states factored fs js 
jx jx corresponds graphically conditional independencies represented 
conditioned setting switch state observable multivariate gaussian output equation state space model notice index real valued state variables value switch state 
probability observation vector jx jrj exp dimension observation vector observation noise covariance matrix output matrix state space model cf 
equation single linear gaussian state space model 
real valued state vector evolves linear gaussian dynamics state space model di ering initial state transition matrix state noise equation 
simplicity assume state vectors identical dimensionality generalization algorithms models di erent size state spaces immediate 
switch state evolves discrete markov transition structure speci ed initial state probabilities state transition matrix js 
exact analogy mixture experts architecture modular learning neural networks gure jacobs 
state space model linear expert gaussian output noise model linear gaussian dynamics 
switch state gates outputs state space models plays role gating network markovian dynamics 
possible extensions model shall consider obvious straightforward ones ex di ering output covariances state space model ex di ering output means state space model model allowed capture observations di erent operating range ex conditioning sequence observed input vectors fu learning ecient learning algorithm parameters switching state space model derived generalizing expectation maximization em algorithm baum dempster 
em alternates optimizing distribution hidden states step optimizing parameters distribution hidden states step 
distribution hidden states fs combined state state space models de ne lower bound log probability observed data log fy gj log fs fs gj dfx log fs fs fs gj fs dfx fs fs log fs gj fs dfx denotes parameters model jensen inequality cover thomas establish 
steps em increase lower bound log probability observed data 
step holds parameters xed sets posterior distribution hidden states parameters fs fs maximizes respect distribution turning lower bound equality easily seen substitution 
step holds distribution xed computes parameters maximize distribution 
log fy gj start step step ect log steps combined decrease log change parameters produced step distribution produced previous step typically longer optimal procedure iterated 
unfortunately exact step switching state space models intractable 
related hybrid models described section posterior probability real valued states gaussian mixture terms 
seen semantics directed graphs particular separation criterion pearl implies hidden state variables marginally independent conditionally dependent observation sequence 
induced dependency ectively couples real valued hidden state variables discrete switch variable consequence exact posteriors gaussian mixtures exponential number terms 
order derive ecient learning algorithm system relax em algorithm approximating posterior probability hidden states 
basic idea expectations respect intractable setting fs fs step tractable distribution approximate results em learning algorithm maximizes lower bound log likelihood 
di erence bound log likelihood kullback liebler kl divergence cover thomas kl qkp fs fs log fs fs dfx intractability step smoothing problem simpler single state switching model noted fu chang athans bar shalom li complexity exact inference approximation determined conditional independence relations parameters choose tractable structure graphical representation eliminates dependencies structure parameters varied obtain tightest possible bound minimizing 
algorithm alternates optimizing parameters distribution minimize step optimizing parameters distribution hidden states step 
exact em steps increase lower bound log likelihood equality reached step 
refer general strategy parameterized approximating distribution variational approximation refer free parameters distribution variational parameters 
completely factorized approximation statistical physics provides basis simple powerful mean eld approximations statistical mechanical systems parisi 
theoretical arguments motivating approximate steps neal hinton originally technical report 
saul jordan showed approximate steps maximize lower bound log likelihood proposed powerful technique structured variational approximations intractable probabilistic networks 
key insight saul jordan judicious approximation exact inference algorithms tractable substructures intractable network 
general tutorial variational approximations jordan 

parameters switching state space model fa state dynamics matrix model output matrix state noise covariance mean initial state covariance initial state tied output noise covariance prior discrete markov process js discrete transition matrix 
extensions ex ex readily implemented substituting adding means input matrices possible approximations posterior distribution hidden variables learning inference switching state space models focus fs zq unnormalized probabilities call potential functions de ne soon zq normalization constant ensuring integrates 
written terms potential functions conditional probabilities corresponds simple graphical model shown 
terms involving switch variables de ne discrete markov chain terms involving state vectors de ne uncoupled state space models 
mean eld approximations approximated stochastically coupled system removing couplings original system 
speci cally removed stochastic coupling chains results fact observation time depends hidden variables time retain coupling hidden variables successive time steps couplings handled exactly forward backward kalman smoothing recursions 
approximation structured sense variables uncoupled 
discrete switching process de ned variational parameters distribution 
parameters scale probabilities states switch variable time step plays exactly role observation probability js play regular hidden markov model 
soon see minimizing kl qkp results equation supports intuition 
uncoupled state space models approximation de ned potential functions related probabilities original system 
potentials prior transition probabilities graphical model representation structured variational approximation posterior distribution hidden states switching state space model 
multiplied factor changes potentials try account data jx jx jx variational parameters vector plays role similar switch variable component range 
posterior probability depend observation time posterior probability includes term assumes state space model generated call responsibility assigned state space model observation vector di erence deterministic parameter stochastic random variable 
maximize lower bound log likelihood kl qkp minimized respect variational parameters separately sequence observations 
de nition switching state space model equation approximating distribution minimum kl satis es xed point equations variational parameters see appendix exp 
denotes expectation distribution 
intuitively responsibility equal probability state space model generated observation vector unnormalized gaussian function expected squared error state space model generated compute necessary sum variables including done eciently forward backward algorithm switch state variables playing exactly role observation probability associated setting switch variable 
related prediction error model data intuitive interpretation switch state associated models smaller expected prediction error particular observation favored time step 
forward backward algorithm ensures nal responsibilities models obtained considering entire sequence observations 
compute necessary calculate expectations see expanding equation exp hx tr hx initialize parameters model 
repeat bound log likelihood converged step repeat convergence kl qkp compute prediction error state space model observation compute forward backward algorithm hmm observation probabilities run kalman smoothing recursions data weighted step re estimate parameters state space model data weighted re estimate parameters switching process baum welch update equations 
learning algorithm switching state space models 
tr matrix trace operator tr ab tr ba 
expectations computed eciently kalman smoothing algorithm state space model model time data weighted responsibilities parameters depend parameters vice versa process iterated iteration involves calls forward backward kalman smoothing algorithms 
iterations converged step outputs expected values hidden variables nal step computes model parameters optimize expectation log likelihood equation appendix function expectations hidden variables 
switching ssms parameter re estimates computed analytically 
example derivatives expectation respect setting zero get hs hx hs hx weighted version re estimation equations ssms 
similarly re estimation equations switch process analogous baum welch update rules hmms 
learning algorithm switching state space models structured variational approximation summarized 
deterministic annealing kl divergence minimized step variational em algorithm multiple minima general 
way visualize minima consider space possible segmentations observation sequence length segmentation mean discrete partition sequence state space models 
ssms possible segmentations weighting data equivalent running kalman smoother unweighted data time varying observation noise covariance matrix sequence 
segmentation inferring optimal distribution real valued states ssms convex optimization problem real valued states conditionally gaussian 
diculty kl minimization lies trying nd best soft partition data 
combinatorial optimization problems possibility getting trapped local minima reduced gradually annealing cost function 
employ deterministic variant annealing idea making simple modi cations variational xed point equations exp temperature parameter initialized large value gradually reduced 
equations maximize modi ed form bound entropy multiplied ueda nakano 
merging gaussians approximate inference methods described literature switching state space models idea merging time step mixture gaussians gaussian 
merged gaussian obtained simply setting mean covariance equal mean covariance mixture 
brie describe alternative variational approximation methods derived traditional gaussian merging procedure applied model de ned 
switching state space models described section di erent ssms possibly di erent state space dimensionalities inappropriate merge states gaussian 
possibly apply gaussian merging technique considering ssm separately 
ssm hidden state density produces time step mixture gaussians case merge gaussians weighted current estimates mjy mjy respectively 
merged gaussian obtain gaussian prior time step 
implemented forward pass version approximate inference scheme analogous imm procedure described bar shalom li 
procedure nds time step best gaussian current mixture gaussians ssm 
denote approximating gaussian mixture approximated best de ned minimizing kl pkq 
furthermore gaussian merging techniques greedy best gaussian computed time step immediately time step 
gaussian kl pkq local minima easy nd optimal computing rst moments inaccuracies greedy procedure arise estimates jy single merged gaussian real mixture 
contrast variational methods seek minimize kl qkp local minima 
methods greedy sense iterate forward backward time obtaining locally optimal simulations experiment variational segmentation deterministic annealing goal experiment assess quality solutions variational inference algorithm ect deterministic annealing solutions 
generated sequences length simple model switched ssms 
ssms switching process de ned data sequences length true segmentations 
segmentations switch states represented dark light dots respectively 
notice dicult correctly segment sequences knowing dynamics processes 
switch state chosen priors transition probabilities 
sequences data set shown true state switch variable 
compared di erent inference algorithms variational inference variational inference deterministic annealing section inference gaussian merging section 
sequence initialized variational inference algorithms equal responsibilities ssms ran iterations 
non annealed inference algorithm ran xed temperature annealed algorithm initialized temperature decayed iterations decay function eliminate ect model inaccuracies gave inference algorithms true parameters generative model 
segmentations non annealed variational inference algorithm showed little similarity true segmentations data 
furthermore non annealed algorithm generally underestimated number switches converging solutions switches 
annealed variational algorithm gaussian merging method segmentations similar true segmentations data 
comparing percent correct segmentations see annealing substantially improves variational inference method gaussian merging annealed variational methods perform comparably 
average performance annealed variational method better gaussian merging 
experiment modelling respiration patient sleep switching state space models prove useful modelling time series dynamics characterized di erent regimes 
illustrate point examined physiological data set patient tentatively diagnosed sleep medical condition patients intermittently breathing sleep 
data obtained repository time series data sets associated santa fe time series analysis prediction competition weigend gershenfeld described di erent sequences length segmentations shown sequences light dark dots corresponding ssms generating data 
rows segmentations variational method annealing variational method deterministic annealing gaussian merging method true segmentation 
inference algorithms give real valued hard segmentations obtained thresholding nal values 
rst sequences ones shown 
detail 

respiration pattern sleep characterized regimes breathing breathing induced re ex arousal 
furthermore patient periods normal rhythmic breathing 
trained switching state space models varying random seed number components mixture dimensionality state space component data set consisting consecutive measurements chest volume 
controls trained simple state space models varying dimension state space simple hidden markov models varying number discrete hidden states 
simulations run convergence iterations whichever came rst convergence assessed measuring change likelihood bound likelihood consecutive steps em 
likelihood simple ssms hmms calculated test set consisting data available web www cs colorado edu andreas time series santafe html 
samples training testing 
percent correct segmentation histograms percent correct segmentations control segmentation variational inference annealing variational inference annealing gaussian merging 
percent correct segmentation computed counting number time steps true estimated segmentations agree 
time respiration time respiration chest volume respiration force patient sleep non continuous time segments night measurements sampled hz 
training data 
characterized extended periods small variability chest volume followed bursts 
see behaviour followed normal rhythmic breathing 
test data 
segment nd instances approximately rhythmic region 
thick lines bottom plot explained main text 
switching switching switching switching hmms log likelihood nats observation test data total runs simple state space models switching state space models di ering numbers components hidden markov models 
iterations em learning curves state space model switching state space model 
tive measurements chest volume 
switching ssms likelihood intractable calculated lower bound likelihood simple ssms modeled data poorly performance values 
large majority runs switching state space model resulted models higher likelihood simple 
consistent exception noted values switching ssm performed identically simple ssm 
exploratory experiments suggest cases single component takes responsibility data model ectively 
may local minimum problem result poor initialization heuristics 
looking learning curves simple switching state space models easy see plateaus solutions simple component ssms switching ssm get caught 
likelihoods hidden markov models comparable best switching state space models 
purely terms coding eciency switching ssms little advantage hmms data 
useful contrast solutions learned hmms solutions learned switching ssms 
thick dots bottom figures show responsibility assigned components fairly typical switching ssm components state size 
component clearly specialized modeling data periods component models periods rhythmic breathing 
switching components provide intuitive model data discrete components needed hmm comparable coding eciency 
discussion main draw rst series experiments correct model parameters problem segmenting switching time series components dicult 
combinatorially alternatives considered energy surface su ers local minima local optimization approaches variational method limited quality initial conditions 
deterministic annealing thought sophisticated initialization procedure hidden states nal solution temperature provides initial conditions 
annealing improved quality segmentations 
rst experiment indicates simpler gaussian merging method performs comparably annealed variational inference 
gaussian merging methods advantage time step cost function minimized local minima 
may account perform relative non annealed variational method 
hand variational methods advantage assumptions constrain model continuity real valued hidden state switch times possible obtain better performance data 
iteratively improve approximation posterior de ne lower bound likelihood 
results suggest may fruitful gaussian merging method initialize variational inference procedure 
furthermore possible derive variational approximations switching models described literature combination gaussian merging variational approximation may provide fast robust method learning inference models 
second series experiments suggests real data set believed switching dynamics switching state space model uncover multiple regimes 
captures regimes generalizes test set better simple linear dynamical model 
similar coding eciency obtained hidden markov models due discrete nature state space model nonlinear dynamics 
doing hidden markov models discrete states solutions interpretable 
variational approximations provide powerful tool inference learning complex probabilistic models 
seen applied switching state space model incorporate single framework known exact inference methods kalman smoothing forward backward algorithm 
variational methods applied classes intractable switching models described section 
training complex models apparent importance methods model selection initialization 
summarize switching state space models dynamical generalization mixture experts neural networks closely related known models econometrics control combine representations underlying hidden markov models linear dynamical systems 
domains priori belief multiple approximately linear dynamical regimes switching state space models provide natural modeling tool 
variational approximations provide method overcome single dicult problem learning switching ssms inference step intractable 
deterministic annealing improves solutions variational method 
notation symbol size description variables observation vector time fy sequence observation vectors state vector state space model ssm time km entire real valued hidden state time switch state variable represented discrete variable values mg vector model parameters state dynamics matrix ssm output matrix ssm state noise covariance matrix ssm initial state mean ssm initial state noise covariance matrix ssm output noise covariance matrix initial state probabilities switch state state transition matrix switch state variational parameters responsibility ssm related expected squared error ssm generated miscellaneous matrix transpose jx matrix determinant hxi expected value distribution dimensions size observation vector length sequence observation vectors number state space models size state vector state space model derivation variational xed point equations appendix derive variational xed point equations learning algorithm switching state space models 
plan 
write probability density de ned switching state space model 
convenience express probability density log domain associated energy function hamiltonian probability density related hamiltonian usual boltzmann distribution temperature 

normalization constant required 
integrates unity 
expressing probabilities log domain ect resulting algorithm 
similarly express approximating distribution hamiltonian hq obtain variational xed point equations setting zero derivatives kl divergence respect variational parameters joint probability observations hidden states switching state space model equation fs js jx jx proceed dissect expression constituent parts 
initial probability switch variable time represented vector switch state state 
probability transitioning switch state time switch state time js initial distribution hidden state variable state space model gaussian mean covariance matrix jq exp probability distribution state state space model time state time gaussian mean covariance matrix jx jq exp write jx jrj exp terms exponent equal vanish product 
combining negative logarithm obtain hamiltonian switching state space model ignoring constants log jq log jq log jrj log log hamiltonian approximating distribution analogously derived de nition equation fs zq potentials initial switch state switch state transitions potential initial state state space model jx potential state time state time jx jx hamiltonian obtained combining terms negative logarithm hq log jq log jq log jrj log log log comparing hq see interaction variables eliminated introducing sets variational parameters responsibilities bias terms discrete markov chain order obtain approximation maximizes lower bound log likelihood minimize kl divergence kl qkp function variational parameters kl qkp fs fs log fs fs dfx hh hq log zq log 
denotes expectation approximating distribution zq normalization constant de ne distributions exponential family 
consequence zeros derivatives kl respect variational parameters obtained simply equating derivatives hhi respect corresponding sucient statistics ghahramani hi hs hi hx hi hp hx hx covariance terms cancel subtract hamiltonians hq log derivatives obtain hi hs log hi hx hs hx hi hs get xed point equation satis ed hs fact hs get 
fu 

state estimation switching environments 
ieee transactions automatic control ac 
anderson moore 

optimal filtering 
prentice hall englewood cli nj 


likelihood evaluation state estimation nonlinear state space models 
ph thesis graduate group managerial science applied economics university pennsylvania philadelphia pa baldi chauvin mcclure 

hidden markov models biological primary sequence information 
proc 
nat 
acad 
sci 
usa 
bar shalom li 

estimation tracking 
artech house boston ma 
baum petrie soules weiss 

maximization technique occurring statistical analysis probabilistic functions markov chains 
annals mathematical statistics 
bengio frasconi 

input output hmm architecture 
tesauro touretzky leen editors advances neural information processing systems pages 
mit press cambridge ma 
cacciatore nowlan 

mixtures controllers jump linear non linear plants 
cowan tesauro alspector editors advances neural information processing systems pages 
morgan kaufmann publishers san francisco ca 
carter kohn 

gibbs sampling state space models 
biometrika 
bishop ghosh 

mixture experts framework adaptive kalman ltering 
ieee trans 
systems man cybernetics 
chang athans 

state estimation discrete systems switching parameters 
ieee transactions aerospace electronic systems aes 
cover thomas 

elements information theory 
wiley new york 
dean kanazawa 

model reasoning causation 
computational intelligence 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
royal statistical society series 
deng 

stochastic model speech incorporating hierarchical nonstationarity 
ieee trans 
speech audio processing 
digalakis rohlicek ostendorf 

ml estimation stochastic linear system em algorithm application speech recognition 
ieee transactions speech audio processing 
elliott moore 

hidden markov models estimation control 
springer verlag new york 
fraser 

forecasting probability densities hidden markov models states 
wiegand gershenfeld editors time series prediction forecasting understanding past sfi studies sciences complexity proc 
vol 
xv pages 
addison wesley reading ma 
ghahramani 

structured variational approximations 
technical report crg tr www gatsby ucl ac uk zoubin papers struct ps gz department computer science university toronto 
ghahramani hinton 

parameter estimation linear dynamical systems 
technical report crg tr www gatsby ucl ac uk zoubin papers tr ps gz department computer science university toronto 
ghahramani hinton 

em algorithm mixtures factor analyzers 
technical report crg tr www gatsby ucl ac uk zoubin papers tr ps gz department computer science university toronto 
ghahramani jordan 

factorial hidden markov models 
machine learning 
goodwin sin 

adaptive ltering prediction control 
prentice hall 
hamilton 

new approach economic analysis nonstationary time series business cycle 
econometrica 
hamilton 

time series analysis 
princeton university press princeton nj 
harrison stevens 

bayesian forecasting discussion 
royal statistical society 
hinton dayan revow 

modeling manifolds images handwritten digits 
submitted publication 
jacobs jordan nowlan hinton 

adaptive mixture local experts 
neural computation 
jordan ghahramani jaakkola saul 

variational methods graphical models 
jordan editor learning graphical models 
kluwer academic publishers 
jordan jacobs 

hierarchical mixtures experts em algorithm 
neural computation 
juang rabiner 

hidden markov models speech recognition 
technometrics 


recursive estimation dynamic modular rbf networks 
touretzky mozer hasselmo editors advances neural information processing systems pages 
mit press 
kalman bucy 

new results linear ltering prediction 
journal basic engineering asme 
kanazawa koller russell 

stochastic simulation algorithms dynamic probabilistic networks 
besnard hanks editors uncertainty arti cial intelligence 
proceedings eleventh conference pages 
morgan kaufmann publishers san francisco ca 
kehagias 

time series segmentation predictive modular neural networks 
neural computation 
kim 

dynamic linear models markov switching 
econometrics 
lauritzen spiegelhalter 

local computations probabilities graphical structures application expert systems 
royal statistical society pages 
ljung om 

theory practice recursive identi cation 
mit press cambridge ma 
meila jordan 

learning ne motion markov mixtures experts 
touretzky mozer hasselmo editors advances neural information processing systems 
mit press 
neal hinton 

new view em algorithm justi es incremental sparse variants 
jordan editor learning graphical models 
kluwer academic press 
parisi 

statistical field theory 
addison wesley redwood city ca 
pawelzik kohlmorgen uller 

annealed competition experts segmentation classi cation switching dynamics 
neural computation 
pearl 

probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann san mateo ca 
rabiner juang 

hidden markov models 
ieee acoustics speech signal processing magazine 
rauch 

solutions linear smoothing problem 
ieee transactions automatic control 
goldberger moody mark 

multi channel physiological data description analysis 
weigend gershenfeld editors time series prediction forecasting understanding past sfi studies sciences complexity proc 
vol 
xv pages 
addison wesley reading ma 
roweis ghahramani 

unifying review linear gaussian models 
neural computation 
saul jordan 

exploiting tractable substructures intractable networks 
touretzky mozer hasselmo editors advances neural information processing systems 
mit press 
shumway sto er 

approach time series smoothing forecasting em algorithm 
time series analysis 
shumway sto er 

dynamic linear models switching 
amer 
stat 
assoc 
smyth 

hidden markov models fault detection dynamic systems 
pattern recognition 
smyth heckerman jordan 

probabilistic independence networks hidden markov probability models 
neural computation 
ueda nakano 

deterministic annealing variant em algorithm 
tesauro touretzky alspector editors advances neural information processing systems pages 
morgan kaufmann 
weigend gershenfeld 

time series prediction forecasting understanding past 
sfi studies sciences complexity proc 
vol 
xv 
addison wesley reading ma 

