journal artificial intelligence research submitted published evolutionary algorithms reinforcement learning david moriarty moriarty isi edu university southern california information sciences institute admiralty way marina del rey ca alan schultz schultz aic nrl navy mil navy center applied research artificial intelligence naval research laboratory washington dc john grefenstette ib gmu edu institute biosciences bioinformatics biotechnology george mason university va distinct approaches solving reinforcement learning problems searching value function space searching policy space 
temporal difference methods evolutionary algorithms known examples approaches 
kaelbling littman moore provided informative survey temporal difference methods 
article focuses application evolutionary algorithms reinforcement learning problem emphasizing alternative policy representations credit assignment methods problem specific genetic operators 
strengths weaknesses evolutionary approach reinforcement learning survey representative applications 

kaelbling littman moore sutton barto provide informative surveys field reinforcement learning rl 
characterize classes methods reinforcement learning methods search space value functions methods search space policies 
class exemplified temporal difference td method evolutionary algorithm ea approach 
kaelbling focus entirely set methods provide excellent account state art td learning 
article intended round picture addressing evolutionary methods solving reinforcement learning problem 
kaelbling clearly illustrate reinforcement learning presents challenging array difficulties process scaling realistic tasks including problems associated large state spaces partially observable states rarely occurring states nonstationary environments 
point approach best remains open question sensible pursue parallel lines research alternative methods 
scope article address better general search value function space policy space hope highlight strengths evolutionary approach reinforcement learning problem 
reader advised view fl ai access foundation morgan kaufmann publishers 
rights reserved 
moriarty schultz grefenstette article ea vs td discussion 
cases methods provide complementary strengths hybrid approaches advisable fact survey implemented systems illustrates ea reinforcement learning systems include elements 
section spells reinforcement learning problem 
order provide specific anchor discussion section presents particular td method 
section outlines approach call evolutionary algorithms reinforcement learning earl provides simple example particular earl system 
sections focus features distinguish eas rl eas general function optimization including alternative policy representations credit assignment methods rl specific genetic operators 
sections highlight strengths weaknesses ea approach 
section briefly surveys successful applications ea systems challenging rl tasks 
final section summarizes presentation points directions research 

reinforcement learning reinforcement learning methods share goal solve sequential decision tasks trial error interactions environment barto sutton watkins grefenstette ramsey schultz 
sequential decision task agent interacts dynamic system selecting actions affect state transitions optimize reward function 
formally time step agent perceives state selects action system responds giving agent possibly zero numerical reward changing state ffi 
state transition may determined solely current state agent action may involve stochastic processes 
agent goal learn policy maps states actions 
optimal policy defined ways typically defined policy produces greatest cumulative reward states argmax cumulative reward received state policy ways compute 
approach uses discount rate fl discount rewards time 
sum computed infinite horizon fl reward received time step alternatively computed summing rewards finite horizon agent state descriptions usually identified values returned sensors provide description agent current state state evolutionary algorithms reinforcement learning world 
sensors give agent complete state information state partially observable 
reinforcement learning intelligent agents designed paradigms notably planning supervised learning 
briefly note major differences approaches 
general planning methods require explicit model state transition function ffi 
model planning algorithm search possible action choices find action sequence guide agent initial state goal state 
planning algorithms operate model environment backtrack undo state transitions enter undesirable states 
contrast rl intended apply situations sufficiently tractable action model exist 
consequently agent rl paradigm actively explore environment order observe effects actions 
planning rl agents normally undo state transitions 
course cases may possible build action model experience sutton enabling planning experience accumulates 
rl research focuses behavior agent insufficient knowledge perform planning 
agents trained supervised learning 
supervised learning agent examples state action pairs indication action correct incorrect 
goal supervised learning induce general policy training examples 
supervised learning requires oracle supply correctly labeled examples 
contrast rl require prior knowledge correct incorrect decisions 
rl applied situations rewards sparse example rewards may associated certain states 
cases may impossible associate label correct incorrect particular decisions agent subsequent decisions making supervised learning infeasible 
summary rl provides flexible approach design intelligent agents situations planning supervised learning impractical 
rl applied problems significant domain knowledge unavailable costly obtain 
example common rl task robot control 
designers autonomous robots lack sufficient knowledge intended operational environment planning supervised learning regime design control policy robot 
case goal rl enable robot generate effective decision policies explores environment 
shows simple sequential decision task example 
task agent grid world move state state selecting actions right 
sensor agent returns identity current state 
agent starts state receives reward indicated visiting state 
task continues agent moves grid world action state 
goal learn policy returns highest cumulative rewards 
example policy results sequences actions starting state gives optimal score 
moriarty schultz grefenstette simple grid world sequential decision task 
agent starts state receives row column current box sensory input 
agent moves box selecting moves right agent score increased payoff indicated box 
goal find policy maximizes cumulative score 
policy space vs value function space reinforcement learning problem described previous section address main topic find optimal policy consider main approaches involves search policy space involves search value function space 
policy space search methods maintain explicit representations policies modify variety search operators 
search methods considered including dynamic programming value iteration simulated annealing evolutionary algorithms 
focuses evolutionary algorithms specialized reinforcement learning task 
contrast value function methods maintain explicit representation policy 
attempt learn value function returns expected cumulative reward optimal policy state 
focus research value function approaches rl design algorithms learn value functions experience 
common approach learning value functions temporal difference td method described section 

temporal difference algorithms reinforcement learning stated comprehensive comparison value function search direct policy space search scope 
useful point key conceptual differences typical value function methods typical evolutionary algorithms searching policy space 
common approach learning value function rl problems temporal difference td method sutton 
evolutionary algorithms reinforcement learning td learning algorithm uses observations prediction differences consecutive states update value predictions 
example consecutive states return payoff prediction values respectively difference suggests payoff state may overestimated reduced agree predictions state updates value function achieved update rule ff gamma ff represents learning rate immediate reward 
difference predictions gammav consecutive states measure prediction error 
consider chain value predictions consecutive state transitions prediction containing non zero reward environment 
iterations sequence update rule adjust values state agree successors eventually reward received 
words single reward propagated backwards chain value predictions 
net result accurate value function predict expected reward state system 
mentioned earlier goal td methods learn value function optimal policy optimal action computed equation argmax ffi course stated rl state transition function ffi unknown agent 
knowledge way evaluating 
alternative value function compute called function watkins watkins dayan 
function value function represents expected value action state acting optimally ffi represents immediate reward received state function actions optimal policy directly computed equation argmax table shows function grid world problem 
table representation function associates cumulative payoffs state action pair system 
letter number pairs top represent state row column represent actions right respectively 
td method adjusts values decision 
selecting action agent considers effect action examining expected value state transition caused action 
function learned td update equation ff max gamma moriarty schultz grefenstette table function simple grid world 
value associated state action pair 
essentially equation updates current reward predicted reward actions selected optimally 
watkins dayan proved updates performed fashion value explicitly represented estimates asymptotically converge correct values 
reinforcement learning system values select optimal action state 
learning widely known implementation temporal difference learning qualitative comparisons evolutionary approaches sections 

evolutionary algorithms reinforcement learning earl policy space approach rl searches policies optimize appropriate objective function 
search algorithms survey focuses evolutionary algorithms 
brief overview simple ea rl followed detailed discussion features characterize general class eas rl 
design considerations evolutionary algorithms evolutionary algorithms eas global search techniques derived darwin theory evolution natural selection 
ea iteratively updates population potential solutions encoded structures called chromosomes 
iteration called generation ea evaluates solutions generates offspring fitness solution task environment 
substructures genes solutions modified genetic operators mutation recombination 
idea structures associated solutions mutated combined form better solutions subsequent generations 
canonical evolutionary algorithm shown 
wide variety eas developed including genetic algorithms holland goldberg evolutionary programming fogel owens walsh genetic programming koza evolutionary strategies rechenberg 
eas general purpose search methods applied variety domains including numerical function optimization combinatorial optimization adaptive control adaptive testing machine learning 
reason widespread success eas relatively requirements application 
appropriate mapping search space space chromosomes 
appropriate fitness function 
evolutionary algorithms reinforcement learning procedure ea initialize evaluate structures termination condition satisfied select alter structures evaluate structures 
pseudo code evolutionary algorithm 
example case parameter optimization common represent list parameters vector real numbers bit string encodes parameters 
representations standard genetic operators mutation cut splice crossover applied straightforward manner produce genetic variations required see 
user decide large number control parameters ea including population size mutation rates recombination rates parent selection rules extensive literature studies suggest eas relatively robust wide range control parameter settings grefenstette schaffer caruana eshelman das 
problems eas applied relatively straightforward manner 
applications eas need specialized problem domain grefenstette 
critical design choice facing user representation mapping search space knowledge structures phenotype space space chromosomes genotype space 
studies shown effectiveness eas sensitive choice representations 
sufficient example choose arbitrary mapping search space space chromosomes apply standard genetic operators hope best 
mapping subject continuing research general consensus candidate solutions share important phenotypic similarities exhibit similar forms building blocks represented chromosomes holland 
follows user ea carefully consider natural way represent elements search space chromosomes 
necessary design appropriate mutation recombination operators specific chosen representation 
result design process representation genetic operators selected ea comprise form search bias similar biases machine learning meth moriarty schultz grefenstette parent parent offspring offspring genetic operators fixed position representation 
offspring generated crossing selected parents 
operation shown called point crossover 
offspring inherits initial segment parent final segment parent 
second offspring inherits pattern genes opposite parents 
crossover point position chosen random 
second offspring incurred mutation shaded gene 
ods 
proper bias ea quickly identify useful building blocks population converge promising areas search space 
case rl user needs major design decisions 
space policies represented chromosomes ea 
second fitness population elements assessed 
answers questions depend user chooses bias ea 
section presents simple earl adopts straightforward set design decisions 
example meant provide baseline comparison elaborate designs 
simple earl remainder shows ways eas search space rl policies 
section provides concrete example simple earl call earl pseudo code shown 
system provides ea counterpart simple table td system described section 
straightforward way represent policy ea single chromosome policy single gene associated observed state 
earl gene value allele biological terminology represents action value associated corresponding state shown 
table shows part earl population policies sample grid world problem 
number policies population usually order 
fitness policy population reflect expected accumulated fitness agent uses policy 
fixed constraints fitness individual policy evaluated 
world deterministic sample grid world 
ways exploit problem specific knowledge eas include heuristics initialize population hybridization problem specific search algorithms 
see grefenstette discussions methods 
evolutionary algorithms reinforcement learning procedure earl initialize population policies evaluate policies termination condition satisfied select high payoff policies policies update policies evaluate policies 
pseudo code evolutionary algorithm reinforcement learning system 
policy table policy representation 
observed state gene indicates preferred action state 
representation standard genetic operators mutation crossover applied 
fitness policy evaluated single trial starts agent initial state terminates agent reaches terminal state falls grid grid world 
non deterministic worlds fitness policy usually averaged sample trials 
options include measuring total payoff achieved agent fixed number steps measuring number steps required achieve fixed level payoff 
fitness policies population determined new population generated steps usual ea 
parents selected reproduction 
typical selection method probabilistically select individuals relative fitness pr fitness fitness represents individual total number individuals 
selection rule expected number offspring policy proportional policy fitness 
example policy average fitness single offspring moriarty schultz grefenstette policy fitness table ea population decision policies sample grid world 
simple policy representation specifies action state world 
fitness corresponds payoffs accumulated policy grid world 
policy twice average fitness offspring 
offspring formed cloning selected parents 
new policies generated applying standard genetic operators crossover mutation clones shown 
process generating new populations strategies continue indefinitely terminated fixed number generations acceptable level performance achieved 
simple rl problems grid world earl may provide adequate approach 
sections point ways earl exhibits strengths complementary td methods rl 
case td methods earl methods extended handle challenges inherent realistic rl problems 
sections survey extensions organized specific biases distinguish eas reinforcement learning earl generic eas policy representations fitness credit assignment models genetic operators 

policy representations earl critical feature distinguishes classes eas representation 
example eas function optimization simple string vector representation eas combinatorial optimization distinctive representations permutations trees graph structures 
likewise eas rl distinctive set representations policies 
range potential policy representations unlimited representations earl systems date largely categorized discrete dimensions 
policies may represented condition action rules neural networks 
second policies may represented single chromosome representation may distributed populations 
single chromosome representation policies rule policies rl problems practical interest number observable states large simple table representation earl impractical 
large scale state 
parent selection rules explored grefenstette 
evolutionary algorithms reinforcement learning policy ik ik rule policy representation 
gene represents condition action rule maps set states action 
general rules independent position chromosome 
conflict resolution mechanisms may needed conditions rules allowed intersect 
policy simple parameter representation weights neural network 
fitness policy payoff agent uses corresponding neural net decision policy 
spaces reasonable represent policy set condition action rules condition expresses predicate matches set states shown 
early examples representation include systems ls smith ls schaffer grefenstette followed samuel grefenstette 
neural net representation policies td rl systems earl systems employ neural net representations function approximators 
simplest case see neural network agent decision policy represented sequence real valued connection weights 
straightforward ea parameter optimization optimize weights neural network belew mcinerney schraudolph whitley dominic das anderson yamauchi beer 
representation requires modification standard ea 
turn distributed representations policies earl systems 
distributed representation policies previous section outlined earl approaches treat agent decision policy single genetic structure evolves time 
section addresses earl approaches decompose decision policy smaller components 
approaches potential advantages 
allow evolution detailed level task specific subtasks 
presumably evolving solution restricted subtask moriarty schultz grefenstette message list sensors classifiers evolutionary algorithm decision rewards holland learning classifier system 
easier evolving monolithic policy complex task 
second decomposition permits user exploit background knowledge 
user base decomposition subtasks prior analysis performance task example known certain subtasks mutually exclusive learned independently 
user decompose complex task subtasks certain components explicitly programmed components learned 
terms knowledge representation earl alternative single chromosome representation distribute policy population elements 
assigning fitness individual elements policy evolutionary selection pressure brought bear detailed aspects learning task 
fitness function individual individual rules individual neurons 
general approach analogous classic td methods take approach extreme learning statistics concerning state action pair 
case single chromosome representations partition distributed earl representations rule neural net classes 
distributed rule policies known example distributed rule approach earl learning classifier systems lcs model holland reitman holland wilson 
lcs uses evolutionary algorithm evolve rules called classifiers map sensory input appropriate action 
outlines holland lcs framework holland 
sensory input received posted message list 
left hand side classifier matches message message list right hand side posted message list 
new messages may subsequently trigger classifiers post messages invoke decision lcs traditional forward chaining model rule systems 
lcs chromosome represents single decision rule entire population represents agent policy 
general classifiers map set observed states set messages may interpreted internal state changes actions 
example evolutionary algorithms reinforcement learning condition action strength table lcs population grid world 
don care symbol allows generality conditions 
example rule says turn right column 
strength rule conflict resolution parent selection genetic algorithm 
environment lcs lcs lcs level hierarchical system 
lcs learns specific behavior 
interactions rule sets pre programmed 
learning agent grid world sensors column row population lcs appear shown table 
classifier matches state column recommends action classifier statistic called strength estimates utility rule 
strength statistics conflict resolution action recommended fitness genetic algorithm 
genetic operators applied highly fit classifiers generate new rules 
generally population size number rules policy kept constant 
classifiers compete space policy 
way earl systems distribute representation policies partition policy separate modules module updated ea 
dorigo colombetti describe architecture called complex reinforcement learning task decomposed subtasks learned separate lcs shown 
provide method called behavior analysis training bat manage incremental training agents distributed lcs architecture 
single chromosome representation extended partitioning policy multiple evolving populations 
example cooperative evolution model potter agent policy formed combining chromosomes independently evolving populations 
chromosome represents set rules rules address subset performance task 
example separate populations evolve policies different components complex task moriarty schultz grefenstette population ea evolutionary algorithm merge domain model collaboration fitness individual evaluated ea ea ea representative representative representative representative cooperative coevolutionary architecture perspective th ea instance 
ea contributes representative merged representatives form collaboration policy agent 
fitness representative reflects average fitness collaborations 
address mutually exclusive sets observed states 
fitness chromosome computed fitness agents employ chromosome part combined chromosomes 
combined chromosomes represent decision policy called collaboration 
distributed network policies distributed earl systems neural net representations designed 
potter de jong separate populations neurons evolve evaluation neuron fitness collaboration neurons selected population 
sane moriarty miikkulainen separate populations maintained evolved population neurons population network blueprints 
motivation sane comes priori knowledge individual neurons fundamental building blocks neural networks 
sane explicitly decomposes neural network search problem parallel searches effective single neurons 
neuron level evolution provides evaluation recombination neural network building blocks population blueprints search effective combinations building blocks 
gives overview interaction populations 
individual blueprint population consists set pointers individuals neuron population 
generation neural networks constructed combining hidden neurons specified blueprint 
blueprint receives fitness corresponding network performs task 
neuron receives fitness top networks participates perform task 
aggressive genetic selection recombination strategy quickly build propagate highly fit structures neuron blueprint populations 
evolutionary algorithms reinforcement learning network blueprint population neuron population overview populations sane 
member neuron population specifies series connections connection labels weights neural network 
member network blueprint population specifies series pointers specific neurons build neural network 

fitness credit assignment earl evolutionary algorithms driven concept natural selection population elements higher fitness leave offspring generations influencing direction search favor high performance regions search space 
concept fitness central ea 
section discuss features fitness model common earl systems 
specifically focus ways fitness function reflects distinctive structure rl problem 
agent model common features earl fitness models fitness computed respect rl agent 
policy represented ea converted decision policy agent operating rl environment 
agent assumed observe description current state select action consulting current policy collect reward provided environment 
earl systems td systems agent generally assumed perform little additional computation selecting action 
approach limits agent strict stimulus response behavior usually assumed agent perform extensive planning reasoning acting 
assumption reflects fact rl tasks involve sort control activity agent respond dynamic environment limited time frame 
moriarty schultz grefenstette policy level credit assignment shown previous section meaning fitness earl systems may vary depending population elements represent 
single chromosome representation fitness associated entire policies distributed representation fitness may associated individual decision rules 
case fitness reflects accumulated rewards received agent course interaction environment specified rl model 
fitness may reflect effort expended amount delay 
worthwhile considering different approaches credit assignment td ea methods 
reinforcement learning problem payoffs may sparse associated certain states 
consequently payoff may reflect quality extended sequence decisions individual decision 
example robot may receive reward movement places goal position room 
robot reward depends previous movements leading point 
difficult credit assignment problem exists rewards sequence decisions individual decisions 
general ea td methods address credit assignment problem different ways 
td approaches credit reward signal explicitly propagated decision agent 
iterations payoffs distributed sequence decisions appropriately discounted reward value associated individual state decision pair 
simple earl systems earl rewards associated sequences decisions distributed individual decisions 
credit assignment individual decision implicitly policies prescribe poor individual decisions fewer offspring generations 
selecting poor policies evolution automatically selects poor individual decisions 
building blocks consisting particular state action pairs highly correlated policies propagated population replacing state action pairs associated poorer policies 
illustrates differences credit assignment td earl grid world 
learning td method explicitly assigns credit blame individual state action pair passing back immediate reward estimated payoff new state 
error term associated action performed agent 
ea approach explicitly propagate credit action associates fitness entire policy 
credit assigned implicitly fitness evaluations entire sequences decisions 
consequently ea tend select policies generate third sequences achieve lower fitness scores 
ea implicitly selects action state example bad sequences sequences 
credit assignment implicit credit assignment performed building blocks earl systems addressed credit assignment problem directly 
shown section individuals earl system represent entire policies components policy component rule sets individual decision rules individual neurons 
distributed representation fitness explicitly assigned individual components 
evolutionary algorithms reinforcement learning max max max max max max max max max max max max max max max max fitness td explicit credit assignment ea implicit credit assignment explicit vs implicit credit assignment 
learning td method assigns credit state action pair immediate reward predicted rewards 
ea method assigns credit implicitly associating fitness values entire sequences decisions 
cases policy represented explicit components different fitness functions associated different evolving populations allowing implementer shape policy evolving specific subtasks dorigo colombetti potter de jong grefenstette 
ambitious goal allow system manage number evolving species form interactions potter 
exciting research early stage 
example lcs model classifier decision rule strength updated td method called bucket brigade algorithm holland 
bucket brigade algorithm strength classifier bid classifiers right post messages 
bids subtracted winning classifiers passed back classifiers posted enabling message previous step 
classifier strengths reinforced classifier posts message triggers classifier 
classifier invokes decision lcs receives strength reinforcement directly environment 
bucket brigade bid passing mechanism clearly bears strong relation method temporal differences sutton 
bucket brigade updates classifier strength strength classifiers fire direct result activation 
td methods differ slightly respect assign credit strictly temporal succession take account causal relations steps 
remains unclear appropriate distributing credit 
single chromosome representations td methods adopted earl systems 
samuel gene decision rule maintains quantity called strength resolve conflict rule matches agent current sensor readings 
payoff obtained terminating trial strengths moriarty schultz grefenstette rules fired trial updated grefenstette 
addition resolving conflicts rule strength plays role triggering mutation operations described section 

rl specific genetic operators creation special genetic operators provides avenue imposing bias eas 
specialized operators earl systems appeared holland called triggered operators responsible creating new classifiers learning agent classifier existing population matched agent current sensor readings 
case high strength rule explicitly generalized cover new set sensor readings 
similar rule creation operator included early versions samuel grefenstette 
versions samuel included number mutation operators created altered rules agent early experiences 
example samuel specialization mutation operator triggered low strength general rule fires episode results high payoff 
case rule conditions reduced generality closely match agent sensor readings 
example agent sensor readings range bearing original rule range bearing set turn strength new rule range bearing set turn strength episode triggering operator resulted high payoff suspect original rule generalized new specific version lead better results 
strength new rule initialized payoff received triggering episode 
considered lamarckian operator agent experience causing genetic change passed offspring 
samuel uses rl specific crossover operator recombine policies 
particular crossover samuel attempts cluster decision rules assigning offspring 
example suppose traces previous evaluations parent strategies follows denotes th decision rule policy trace parent episode 

high payoff 
low payoff 
jean developed evolutionary theory stressed inheritance acquired characteristics particular acquired characteristics adapted surrounding environment 
course theory superseded darwin emphasis stage adaptation undirected variation followed selection 
research generally failed substantiate lamarckian mechanisms biological systems gould 
evolutionary algorithms reinforcement learning 
trace parent 

low payoff 
high payoff 
possible offspring fr motivation rules fire sequence achieve high payoff treated group recombination order increase likelihood offspring policy inherit better behavior patterns parents 
rules fire successful episodes randomly assigned offspring 
form crossover lamarckian triggered experiences agent directly related structure rl problem groups components policies temporal association decision rules 

strengths earl ea approach represents interesting alternative solving rl problems offering potential advantages scaling realistic applications 
particular earl systems developed address difficult challenges rl problems including ffl large state spaces ffl incomplete state information ffl non stationary environments 
section focuses ways earl address challenges 
scaling large state spaces early papers rl literature analyze efficiency alternative learning methods toy problems similar grid world shown 
studies useful academic exercises number observed states realistic applications rl preclude approach requires explicit storage manipulation statistics associated observable state action pair 
ways earl policy representations help address problem large state spaces generalization selectivity 
policy generalization earl policy representations specify policy level abstraction higher explicit mapping observed states actions 
case rule representations rule language allows conditions match sets states greatly reducing storage moriarty schultz grefenstette table approximated value function population table 
table displays average fitness policies select state action pair reflects estimated impact action fitness 
tiny population size example estimates particularly accurate 
note question marks states actions converged 
policies select alternative action population statistics impact actions fitness 
different simple td methods statistics actions maintained 
required specify policy 
noted generality rules policy may vary considerably level rules specify action single observed state way completely general rules recommend action regardless current state 
likewise neural net representations mapping function stored implicitly weights connections neural net 
case generalized policy representation facilitates search policies grouping states action required 
policy selectivity earl systems selective representations policies 
ea learns mappings observed states recommended actions usually eliminating explicit information concerning desirable actions 
knowledge bad decisions explicitly preserved policies decisions selected evolutionary algorithm eventually eliminated population 
advantage selective representations attention focused profitable actions reducing space requirements policies 
consider example simple earl operating grid world 
population evolves policies normally converge best actions specific state selective pressure achieve high fitness levels 
example population shown table converged alleles actions states 
converged state action pairs highly correlated fitness 
example policies converged action state 
action state achieves higher expected return action vs table 
policies select action state achieve lower fitness scores selected 
simple earl snapshot population table provides implicit estimate corresponding td value function table distribution biased profitable state actions pairs 
evolutionary algorithms reinforcement learning green blue red blue environment incomplete state information 
circles represent states world colors represent agent sensory input 
agent equally start red state green state dealing incomplete state information clearly favorable condition reinforcement learning occurs agent observe true state dynamic system interacts 
complete state information available td methods efficient available feedback associating reward directly individual decisions 
real world situations agent sensors provide partial view may fail disambiguate states 
consequently agent unable completely distinguish current state 
problem termed perceptual aliasing hidden state problem 
case limited sensory information may useful associate rewards larger blocks decisions 
consider situation agent act complete state information 
circles represent specific states world colors represent sensor information agent receives state 
square nodes represent goal states corresponding reward shown inside 
state agent choice actions 
assume state transitions deterministic agent equally start state red green sensor readings 
example different states return sensor reading blue agent unable distinguish 
actions blue state return different rewards 
function applied problem treats sensor reading blue observable state rewards action averaged blue states 
blue blue converge respectively 
reward blue higher alternatives observable states red green agent policy learning choose enter observable state blue time 
final decision policy learning shown table 
table shows optimal policy respect agent limited view world 
moriarty schultz grefenstette value function policy optimal policy red green blue expected reward table policy expected reward returned converged function compared optimal policy sensory information 
words policy reflects optimal choices agent distinguish blue states 
associating values individual observable states simple td methods vulnerable hidden state problems 
example ambiguous state information td method mistakenly combines rewards different states system 
confounding information multiple states td recognize advantages associated specific actions specific states example action top blue state achieves high reward 
contrast ea methods associate credit entire policies rely net results decision sequences sensor information may ambiguous 
example evolutionary algorithm exploits disparity rewards different blue states evolves policies enter blue state avoid bad 
agent remains unable distinguish blue states evolutionary algorithm implicitly distinguishes ambiguous states rewarding policies avoid bad states 
example ea method expected evolve optimal policy current example existing ambiguous state information 
policies choose action sequence starting red state achieve highest levels fitness selected reproduction ea 
agents policies placed green state select action receive lowest fitness score subsequent action blue sensors returns negative reward 
policies achieve high fitness started red state selected choose green state 
course generations policies choose action green state maximize fitness ensure survival 
confirmed hypotheses empirical tests 
learner single step updates table representation converged values table run 
evolutionary algorithm consistently converged population optimal policy 
shows average percentage optimal policy population function time averaged independent runs 
simple ea methods earl appear robust presence hidden states simple td methods 
refined sensor information helpful 
previous example ea policies achieve better average reward td policy evolved policy remains unable 
binary tournament selection policy population crossover probability mutation rate 
evolutionary algorithms reinforcement learning generation optimal policy distribution hidden state problem evolutionary algorithm 
graph plots percentage optimal policies population averaged runs 
rewards blue states 
rewards realized agent separate blue states 
method generates additional features disambiguate states presents important asset ea methods 
kaelbling 
describe promising solutions hidden state problem additional features agent previous decisions observations automatically generated included agent sensory information chrisman lin mitchell mccallum ring 
methods effective disambiguating states td methods initial studies research required determine extent similar methods resolve significant hidden state information realistic applications 
useful develop ways methods augment sensory data available ea methods 
non stationary environments agent environment changes time rl problem difficult optimal policy moving target 
classic trade exploration exploitation pronounced 
techniques encouraging exploration td rl include adding exploration bonus estimated value state action pairs reflects long agent tried action sutton building statistical model agent uncertainty dayan sejnowski 
simple modifications standard evolutionary algorithms offer ability track nonstationary environments provide promising approach rl difficult cases 
fact evolutionary search competition population policies suggest immediate benefits tracking non stationary environments 
extent population maintains diverse set policies changes environment bias moriarty schultz grefenstette selective pressure favor policies fit current environment 
long environment changes slowly respect time required evaluate population policies population able track changing fitness landscape alteration algorithm 
empirical studies show maintaining diversity population may require higher mutation rate usually adopted stationary environments cobb grefenstette 
addition special mechanisms explored order eas responsive rapidly changing environments 
example grefenstette suggests maintaining random search restricted portion population 
random population elements analogous populations uncorrelated fitness landscapes 
maintaining source diversity permits ea respond rapidly large sudden changes fitness landscape 
keeping randomized portion population population impact search efficiency stationary environments minimized 
general approach easily applied earl systems 
useful algorithms developed ensure diversity evolving include fitness sharing goldberg richardson crowding de jong local mating collins jefferson 
goldberg fitness sharing model example similar individuals forced share large portion single fitness value shared solution point 
sharing decreases fitness similar individuals causes evolution select individuals niches 
earl methods employ distributed policy representations achieve diversity automatically suited adaptation dynamic environments 
distributed representation individual represents partial solution 
complete solutions built combining individuals 
individual solve task evolutionary algorithm search complementary individuals solve task 
evolutionary pressures prevent convergence population 
moriarty miikkulainen showed inherent diversity specialization sane allow adapt quickly changes environment standard convergent evolutionary algorithms 
learning system detect changes environment direct response possible 
anytime learning model grefenstette ramsey earl system maintains case base policies indexed values environmental detectors corresponding environment policy evolved 
environmental change detected population policies partially reinitialized previously learned policies selected basis similarity previously encountered environment current environment 
result environment changes cyclic population immediately seeded policies effect occurrence current environment 
having population policies approach protected kinds errors detecting environmental changes 
example spurious environmental change mistakenly detected learning unduly affected part current population policies replaced previously learned policies 
zhou explored similar approach lcs 
evolutionary algorithms reinforcement learning summary earl systems respond non stationary environments techniques generic evolutionary algorithms techniques specifically designed rl mind 

limitations earl ea approach rl promising growing list successful applications outlined section number challenges remain 
online learning distinguish broad approaches reinforcement learning online learning offline learning 
online learning agent learns directly experiences operational environment 
example robot learn navigate warehouse moving physical environment 
problems earl situation 
require large number experiences order evaluate large population policies 
depending quickly agent performs tasks result environmental feedback may take unacceptable amount time run hundreds generations ea evaluates hundreds thousands policies 
second may dangerous expensive permit agent perform actions actual operational environment cause harm environment 
policies ea generates bad policies 
objections apply td methods 
example theoretical results prove optimality learning require state visited infinitely obviously impossible practice 
likewise td methods may explore undesirable states acceptable value function 
td earl practical considerations point offline learning rl system performs exploration simulation models environment 
simulation models provide number advantages earl including ability perform parallel evaluations policies population simultaneously grefenstette 
rare states memory record observed states rewards differs greatly ea td methods 
temporal difference methods normally maintain statistics concerning stateaction pair 
states revisited new reinforcement combined previous value 
new information supplements previous information information content agent reinforcement model increases exploration 
manner td methods sustain knowledge bad state action pairs 
pointed previously ea methods normally maintain information policies policy components 
knowledge bad decisions explicitly preserved policies decisions selected evolutionary algorithm eventually eliminated population 
example refer table shows implicit statistics population table 
note question moriarty schultz grefenstette marks states actions converged 
policies population select alternative action ea statistics impact actions fitness 
reduction information content evolving population disadvantage respect states rarely visited 
evolutionary algorithm value genes real impact fitness individual tends drift random values mutations tend accumulate genes 
state rarely encountered mutations may freely accumulate gene describes best action state 
result evolutionary algorithm learns correct action rare state information may eventually lost due mutations 
contrast table td methods permanently record information state action pairs may robust learning agent encounter rare state 
course td method uses function approximator neural network value function suffer memory loss concerning rare states updates frequently occurring states dominate updates rare states 
proofs optimality attractive features td methods learning algorithm proof optimality watkins dayan 
practical importance result limited assumptions underlying proof hidden states state visited infinitely satisfied realistic applications 
current theory evolutionary algorithms provide similar level optimality proofs restricted classes search spaces vose wright 
general theoretical tools available applied realistic rl problems 
case ultimate convergence optimal policy may important practice efficiently finding reasonable approximation 
pragmatic approach may ask efficient alternative rl algorithms terms number reinforcements received developing policy tolerance level optimal policy 
model probably approximately correct pac learning valiant performance learner measured learning experiences samples supervised learning required converging correct hypothesis specified error bounds 
developed initially supervised learning pac approach extended td methods general ea methods ros 
analytic methods early stage development research lines may day provide useful tools understanding theoretical practical advantages alternative approaches rl 
time experimental studies provide valuable evidence utility approach 

examples earl methods take look significant examples earl approach results rl problems 
attempt exhaustive survey selected earl systems representative diverse policies representations outlined section 
samuel represents class single chromosome rule earl systems 
example distributed rule earl method 
genitor single chromosome neural net system sane distributed neural net system 
brief survey evolutionary algorithms reinforcement learning provide starting point interested investigating evolutionary approach reinforcement learning 
samuel samuel grefenstette earl system combines darwinian lamarckian evolution aspects temporal difference reinforcement learning 
samuel learn behaviors navigation collision avoidance tracking herding robots autonomous vehicles 
samuel uses single chromosome rule representation policies member population policy represented rule set gene rule maps state world actions performed 
example rule range bearing set turn strength high level language rules offers advantages low level binary pattern languages typically adopted genetic learning systems 
easier incorporate existing knowledge acquired experts symbolic learning programs 
second easier transfer knowledge learned human operators 
samuel includes mechanisms allow coevolution multiple behaviors simultaneously 
addition usual genetic operators crossover mutation samuel uses traditional machine learning techniques form lamarckian operators 
samuel keeps record experiences allow operators generalization specialization covering deletion informed changes individual genes rules experiences 
samuel successfully reinforcement learning applications 
briefly describe examples learning complex behaviors real robots 
applications samuel learning performed simulation reflecting fact initial phases learning controlling real system expensive dangerous 
learned behaviors tested line system 
schultz grefenstette schultz schultz grefenstette samuel learn collision avoidance local navigation behaviors nomad mobile robot 
sensors available learning task sonars infrared sensors range bearing goal current speed vehicle 
samuel learned mapping sensors controllable actions turning rate translation rate wheels 
samuel took human written rule set reach goal limited time hitting obstacle percent time generations able obtain percent success rate 
schultz grefenstette robot learned herd second robot 
task learning system range bearing second robot heading second robot range bearing goal input sensors 
system learned mapping sensors turning rate steering rate 
experiments success measured percentage times robot maneuver second robot goal limited amount time 
second robot implemented random walk plus behavior avoid nearby obstacles 
robot learned exploit achieve goal moving second robot goal 
moriarty schultz grefenstette samuel initial human designed rule set performance percent generations able move second robot goal percent time 
grefenstette samuel ea system combined case learning address adaptation problem 
approach called anytime learning grefenstette ramsey learning agent interacts external environment internal simulation 
anytime learning approach involves continuously running interacting modules execution module learning module 
execution module controls agent interaction environment includes monitor dynamically modifies internal simulation model observations actual agent environment 
learning module continuously tests new strategies agent simulation model genetic algorithm evolve improved strategies updates knowledge base execution module best available results 
simulation model modified due observed change agent environment genetic algorithm restarted modified model 
learning system operates indefinitely execution system uses results learning available 
samuel shows ea method particularly suited anytime learning 
previously learned strategies treated cases indexed set conditions learned 
new situation encountered nearest neighbor algorithm find similar previously learned cases 
nearest neighbors re initialize genetic population policies new case 
grefenstette reports experiments mobile robot learns track robot dynamically adapts policies anytime learning encounters series partial system failures 
approach blurs line online offline learning online system updated offline learning system develops improved policy 
fact offline learning system executed board operating mobile robot 
described previously dorigo colombetti distributed rule ea supports approach design autonomous systems called behavioral engineering 
approach tasks performed complex autonomous systems decomposed individual behaviors learned learning classifier systems module shown 
decomposition performed human designer fitness function associated lcs carefully designed reflect role associated component behavior autonomous system 
furthermore interactions modules preprogrammed 
example designer may decide robot learn approach goal threatening predator near case robot evade predator 
architecture set behaviors set evasion behavior higher priority goal seeking behavior individual lcs modules evolve decision rules optimally performing subtasks 
develop behavioral rules number behaviors autonomous robots including complex behavior groups chase feed escape evolutionary algorithms reinforcement learning dorigo colombetti 
approach implemented tested simulated robots real robots 
exploits human design earl methods optimize system performance method shows promise scaling realistic tasks 
genitor genitor whitley whitley aggressive general purpose genetic algorithm shown effective specialized reinforcement learning problems 
whitley 
demonstrated genitor efficiently evolve decision policies represented neural networks limited reinforcement domain 
genitor relies solely evolutionary algorithm adjust weights neural networks 
solving rl problems member population genitor represents neural network sequence connection weights 
weights concatenated realvalued chromosome gene represents crossover probability 
crossover gene determines network mutated randomly perturbed crossover operation recombination network performed 
crossover gene modified passed offspring offspring performance compared parent 
offspring outperforms parent crossover probability decreased 
increased 
whitley refer technique adaptive mutation tends increase mutation rate populations converge 
essentially method promotes diversity population encourage continual exploration solution space 
genitor uses called steady state genetic algorithm new parents selected genetic operators applied individual evaluated 
approach contrasts generational gas entire population evaluated replaced generation 
steady state ga policy evaluated just retains fitness value indefinitely 
policies lower fitness replaced possible fitness noisy evaluation function may undesirable influence direction search 
case pole balancing rl application fitness value depends length time policy maintain balance randomly chosen initial state 
fitness random variable depends initial state 
authors believe noise fitness function little negative impact learning policies difficult poor networks obtain fitness networks copies population survive occasional bad fitness evaluation 
interesting general issue earl needs analysis 
genitor adopts specific modification rl applications 
representation uses real valued chromosome bit string representation weights 
consequently genitor policies weight definitions reducing potentially random disruption neural network weights result crossover operations occurred middle weight definition 
second modification high mutation rate helps maintain diversity promote rapid exploration policy space 
genitor uses unusually small populations order discourage different competing neural network species forming population 
moriarty schultz grefenstette ley 
argue speciation leads competing conventions produces poor offspring dissimilar networks recombined 
whitley 
compare genitor adaptive heuristic critic anderson ahc uses td method reinforcement learning 
different versions common pole balancing benchmark task genitor comparable ahc learning rate generalization 
interesting difference whitley genitor consistent ahc solving pole balancing problem failure signals occurs wider pole bounds problem harder 
ahc preponderance failures appears cause states failure 
contrast ea method appears effective finding policies obtain better performance success uncommon 
difference ea tends ignore cases pole balanced concentrate successful cases 
serves example advantages associated search policy space policy performance paying attention value associated individual states 
sane sane symbiotic adaptive neuro evolution system designed efficient method building artificial neural networks rl domains possible generate training data normal supervised learning moriarty miikkulainen 
sane system uses evolutionary algorithm form hidden layer connections weights neural network 
neural network forms direct mapping sensors actions provides effective generalization state space 
sane method credit assignment ea allows apply problems reinforcement sparse covers sequence decisions 
described previously sane uses distributed representation policies 
sane offers important advantages reinforcement learning normally implementations neuro evolution 
maintains diverse populations 
canonical function optimization ea converge population single solution sane forms solutions population 
different types neurons necessary build effective neural network inherent evolutionary pressure develop neurons perform different functions maintain different types individuals population 
diversity allows recombination operators crossover continue generate new neural structures prolonged evolution 
feature helps ensure solution space explored efficiently learning process 
sane resilient suboptimal convergence adaptive changes domain 
second feature sane explicitly decomposes search complete solutions search partial solutions 
searching complete neural networks solutions smaller problems neurons evolved combined form effective full solution neural network 
words sane effectively performs problem reduction search space neural networks 
sane shown effective different large scale problems 
problem sane evolved neural networks direct focus minimax game tree search moriarty evolutionary algorithms reinforcement learning miikkulainen 
selecting moves evaluated game situation sane guides search away misinformation search tree effective moves 
sane tested game tree search othello evaluation function world champion program bill lee mahajan 
tested full width minimax search sane significantly improved play bill examining subset board positions 
second application sane learn obstacle avoidance behaviors robot arm moriarty miikkulainen 
approaches learning robot arm control learn hand eye coordination supervised training methods examples correct behavior explicitly 
unfortunately domains obstacles arm intermediate joint rotations reaching target generating training examples extremely difficult 
reinforcement learning approach require examples correct behavior learn intermediate movements general reinforcements 
sane implemented form neuro control networks capable maneuvering oscar robot arm obstacles reach random target locations 
camera visual infrared sensory input neural networks learned effectively combine target reaching obstacle avoidance strategies 
related examples evolutionary methods learning neural net control systems robotics reader see cliff harvey husbands husbands harvey cliff yamauchi beer 

summary article began suggesting distinct approaches solving reinforcement learning problems search value function space search policy space 
td earl examples complementary approaches 
approaches assume limited knowledge underlying system learn experimenting different policies reinforcement alter policies 
approach requires precise mathematical model domain may learn direct interactions operational environment 
td methods earl methods generally base fitness performance policy 
sense ea methods pay attention individual decisions td methods 
glance approach appears efficient information may fact provide robust path learning policies especially situations sensors inadequate observe true state world 
useful view path practical rl systems choice ea td methods 
tried highlight strengths evolutionary approach shown earl td complementary approaches means mutually exclusive 
cited examples successful earl systems samuel explicitly incorporate td elements multilevel credit assignment methods 
practical applications depend kinds multi strategy approaches machine learning 
listed number areas need particularly theoretical side 
rl highly desirable better tools predicting amount experience needed learning agent reaching specified level moriarty schultz grefenstette formance 
existing proofs optimality learning ea extremely limited practical predicting approach perform realistic problems 
preliminary results shown tools pac analysis applied ea td methods effort needed direction 
serious challenges remain scaling reinforcement learning methods realistic applications 
pointing shared goals concerns complementary approaches hope motivate collaboration progress field 
anderson 

learning control inverted pendulum neural networks 
ieee control systems magazine 
barto sutton watkins 

learning sequential decision making 
gabriel moore 
eds learning computational neuroscience 
mit press cambridge ma 
belew mcinerney schraudolph 

evolving networks genetic algorithm connectionist learning 
farmer langton rasmussen taylor 
eds artificial life ii reading ma 
addison wesley 
chrisman 

reinforcement learning perceptual aliasing perceptual distinctions approach 
proceedings tenth national conference artificial intelligence pp 
san jose ca 
cliff harvey husbands 

explorations evolutionary robotics 
adaptive behavior 
cobb grefenstette 

genetic algorithms tracking changing environments 
proc 
fifth international conference genetic algorithms pp 

collins jefferson 

selection massively parallel genetic algorithms 
proceedings fourth international conference genetic algorithms pp 
san mateo ca 
morgan kaufmann 
dayan sejnowski 

exploration dual control 
machine learning 
de jong 

analysis behavior class genetic adaptive systems 
ph thesis university michigan ann arbor mi 
dorigo colombetti 

robot shaping experiment behavioral engineering 
mit press cambridge ma 


efficient reinforcement learning 
proceedings seventh annual acm conference computational learning theory pp 

association computing machinery 
fogel owens walsh 

artificial intelligence simulated evolution 
wiley publishing new york 
evolutionary algorithms reinforcement learning goldberg 

genetic algorithms search optimization machine learning 
addison wesley reading ma 
goldberg richardson 

genetic algorithms sharing multimodal function optimization 
proceedings second international conference genetic algorithms pp 
san mateo ca 
morgan kaufmann 
grefenstette 

optimization control parameters genetic algorithms 
ieee transactions systems man cybernetics smc 
grefenstette 

incorporating problem specific knowledge genetic algorithms 
davis 
ed genetic algorithms simulated annealing pp 
san mateo ca 
morgan kaufmann 
grefenstette 

credit assignment rule discovery system genetic algorithms 
machine learning 
grefenstette 

genetic algorithms changing environments 
manner manderick 
eds parallel problem solving nature pp 

grefenstette 

robot learning parallel genetic algorithms networked computers 
proceedings summer computer simulation conference pp 

grefenstette 

genetic learning adaptation autonomous robots 
robotics manufacturing trends research applications volume pp 

asme press new york 
grefenstette 

proportional selection sampling algorithms 
handbook evolutionary computation chap 

iop publishing oxford university press 
grefenstette 

rank selection 
handbook evolutionary computation chap 

iop publishing oxford university press 
grefenstette ramsey 

approach anytime learning 
proc 
ninth international conference machine learning pp 
san mateo ca 
morgan kaufmann 
grefenstette ramsey schultz 

learning sequential decision rules simulation models competition 
machine learning 
holland 

adaptation natural artificial systems introductory analysis applications biology control artificial intelligence 
university michigan press ann arbor mi 
holland 

escaping brittleness possibilities general purpose learning algorithms applied parallel rule systems 
machine learning artificial intelligence approach vol 

morgan kaufmann los altos ca 
moriarty schultz grefenstette holland 

genetic algorithms classifier systems foundations directions 
proceedings second international conference genetic algorithms pp 
hillsdale new jersey 
holland reitman 

cognitive systems adaptive algorithms 
pattern directed inference systems 
academic press new york 
husbands harvey cliff 

circle round state space attractors evolved sighted robots 
robot 
autonomous systems 
kaelbling littman moore 

reinforcement learning survey 
journal artificial intelligence research 
koza 

genetic programming programming computers means natural selection 
mit press cambridge ma 
lee mahajan 

development world class othello program 
artificial intelligence 
lin mitchell 

memory approaches reinforcement learning nonmarkovian domains 
tech 
rep cmu cs carnegie mellon university school computer science 
mccallum 

reinforcement learning selective perception hidden state 
ph thesis university rochester 
moriarty miikkulainen 

evolving neural networks focus minimax search 
proceedings twelfth national conference artificial intelligence aaai pp 
seattle wa 
mit press 
moriarty miikkulainen 

efficient reinforcement learning symbiotic evolution 
machine learning 
moriarty miikkulainen 

evolving obstacle avoidance behavior robot arm 
animals animats proceedings fourth international conference simulation adaptive behavior sab pp 
cape cod ma 
moriarty miikkulainen 

forming neural networks efficient adaptive evolution 
evolutionary computation 
potter 

design analysis computational model cooperative coevolution 
ph thesis george mason university 
potter de jong 

evolving neural networks collaborative species 
proceedings summer computer simulation conference ottawa canada 
potter de jong grefenstette 

coevolutionary approach learning sequential decision rules 
eshelman 
ed proceedings sixth international conference genetic algorithms pittsburgh pa evolutionary algorithms reinforcement learning rechenberg 

cybernetic solution path experimental problem 
library translation 
royal aircraft establishment aug 
ring 

continual learning reinforcement environments 
ph thesis university texas austin 
ros 

probably approximately correct pac learning analysis 
handbook evolutionary computation chap 

iop publishing oxford university press 
schaffer caruana eshelman das 

study control parameters affecting online performance genetic algorithms function optimization 
proceedings third international conference genetic algorithms pp 

morgan kaufmann 
schaffer grefenstette 

multi objective learning genetic algorithms 
proceedings ninth international joint conference artificial intelligence pp 

morgan kaufmann 
schultz 

learning robot behaviors genetic algorithms 
intelligent automation soft computing trends research development applications pp 

tsi press albuquerque 
schultz grefenstette 

genetic algorithm learn behaviors autonomous vehicles 
proceedings aiaa guidance navigation control conference hilton head sc 
schultz grefenstette 

shepherd learning complex robotic behaviors 
robotics manufacturing trends research applications volume pp 

asme press new york 
smith 

flexible learning problem solving heuristics adaptive search 
proceedings eighth international joint conference artificial intelligence pp 

morgan kaufmann 
sutton 

integrated architectures learning planning reacting approximate dynamic programming 
machine learning proceedings seventh international conference pp 

sutton 

learning predict methods temporal differences 
machine learning 
sutton barto 

reinforcement learning 
mit press cambridge ma 
valiant 

theory learnable 
communications acm 
vose wright 

simple genetic algorithms linear fitness 
evolutionary computation 
moriarty schultz grefenstette watkins 

learning delayed rewards 
ph thesis university cambridge england 
watkins dayan 

learning 
machine learning 
whitley 

genitor algorithm selective pressure 
proceedings third international conference genetic algorithms pp 
san mateo ca 
morgan kaufman 
whitley 

genitor different genetic algorithm 
proceedings rocky mountain conference artificial intelligence pp 
denver whitley dominic das anderson 

genetic reinforcement learning problems 
machine learning 
wilson 

zeroth level classifier system 
evolutionary computation 
yamauchi beer 

sequential behavior learning evolved dynamical neural networks 
adaptive behavior 
zhou 

csm computational model cumulative learning 
machine learning 

