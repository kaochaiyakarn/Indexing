appear proc 
workshop learning statistical models relational data 
aaai 
learning stochastic logic programs stephen muggleton department computer science university york york yo dd united kingdom 
stochastic logic programs slps shown generalisation hidden markov models hmms stochastic context free grammars directed bayes nets 
stochastic logic program consists set labelled clauses interval rst order range restricted de nite clause 
summarises syntax distributional semantics proof techniques slps discusses standard inductive logic programming ilp system progol modi ed support learning slps 
resulting system nds slp uniform probability labels de nition near maximal bayes posterior probability alters probability labels increase posterior probability 
stage implemented di ers previous versions progol allowing user de ned evaluation functions written prolog 
shown maximising bayesian posterior function involves nding slps short derivations examples 
search pruning bayesian evaluation function carried way previous versions 
system demonstrated worked examples involving learning probability distributions sequences learning simple forms uncertain knowledge 
representations uncertain knowledge divided procedural descriptions sampling distributions 
stochastic grammars lari young hidden markov models hmms declarative representations uncertain statements 
probabilistic logics fagin halpern relational bayes nets jaeger 
stochastic logic programs slps muggleton introduced originally way lifting stochastic grammars type representations level rst order logic programs lps :10.1.1.31.1630:10.1.1.35.951
cussens cussens showed slps represent undirected bayes nets type representations 
slps presently muggleton de ne distributions sampling inductive logic programming ilp muggleton :10.1.1.31.1630
copyright american association arti cial intelligence www aaai org 
rights reserved 
previous papers describing slps concentrated procedural sampling interpretation 
rst summarises semantics proof techniques slps 
describes method learning slps examples background knowledge 
organised follows 
section introduces standard de nitions lps 
syntax semantics proof techniques slps section incomplete slps shown multiple consistent distributional models 
section introduces framework learning slps discusses issues involved construction underlying lp estimation probability labels 
overview ilp system progol muggleton section section describes mechanism allows ned evaluation functions progol derives user de ned function learning slps :10.1.1.31.1630
worked examples learning slps section section concludes discusses 
lps summarises standard syntax semantics proof techniques lps see lloyd 
syntax lps variable denoted upper case letter followed lower case letters digits 
predicate function symbols denoted lower case letter followed lower case letters digits 
variable term function symbol immediately followed bracketed tuple terms term 
case zero function symbol constant written brackets 
term function symbols variable constant 
predicate symbol immediately followed tuple terms called atomic formula atom 
negation symbol 
literals atom 
case called positive literal called negative literal 
clause nite set literals treated universally quanti ed disjunction literals 
clause said unit contains exactly atom 
nite set clauses called clausal theory treated conjunction clauses 
literals clauses clausal appear proc 
workshop learning statistical models relational data 
aaai 
theories true false formed formulas 
term said ground contains variables 
horn clause clause containing positive literal 
de nite clause clause containing exactly positive literal written positive literal head negative literals constitute body clause 
de nite clause variables head appear body called range restricted 
non de nite horn clause called goal written horn theory clausal theory containing horn clauses 
de nite program clausal theory containing de nite clauses 
range restricted de nite program de nite program clauses range restricted 
semantics lps fv vn said substitution variable term distinct greek lowercase letters denote substitutions 
said ground ground 
term fv vn substitution 
instantiation written formed replacing occurrence instance clause subsumes clause exists substitution theta rst order language set formed xed nite set predicate symbols function symbols variables 
set ground literals called interpretation simply interpretation case contains ground atom interpretation de nite clause said model simply model ground instance implies model horn theory model clause said satis able model unsatis able 
suppose chosen smallest rst order language involving constant predicate function symbols horn theory case interpretation called herbrand interpretation ground atomic subset called herbrand base called herbrand model horn theory herbrand model herbrand theorem satis able herbrand model 
say entails model model proof lps inference rule states rewritten say exists series applications transform said sound implies complete implies said refutation complete complete restricted false 
substitution said uni er atoms 
general uni er mgu uni ers exists substitution resolution inference rule follows 
nf said resolvent clauses common variables mgu suppose de nite program goal 
resolution linear restricted clauses resolvent linear resolution 
resolvent linear resolution goal 
assuming literals clauses ordered linear resolution sld literal chosen resolve rst sld refutation sequence sld linear resolutions represented dp hg cn resolvent empty clause 
false 
answer substitution substitution corresponding resolution involving dp range restricted ground 
sld resolution known sound refutation complete de nite programs 
range restricted de nite program ground atom shown showing sld false 
negation failure nf inference rule says sld false implies sldnf slps syntax slps slp set labelled clauses probability 
number range rst order range restricted de nite clause subset clauses predicate symbol head called de nition de nition sum probability labels 
said complete incomplete 
represents de nite program consisting clauses labels removed 
example unbiased coin 
slp complete represents coin comes heads tails probability 
coin head coin tail simple example sampling distribution cussens cussens considers restricted de nition slps 
section provides complex sampling distribution language attaching probability labels productions grammar 
grammar encoded range restricted de nite program 
appear proc 
workshop learning statistical models relational data 
aaai 
example pet example 
slp incomplete 
likes pet pet cat mouse shows statements form pr jq encoded slp case pr likes 
proof slps stochastic sld refutation sequence pn cn goal hg cn sld refutation 
refutation represents repeated application inference rule 
takes goal labelled clause produces labelled goal pq sld resolvent answer probability incomplete probability ground atom respect ajs 
state ajs ajs ajs represents conditional probability incomplete probabilities 
ground atom predicate symbol de nition slp incomplete ajs proof 
suppose probability labels clauses pn ajs pn sum products 
ajs pn semantics slps section introduce normal semantics slps 
suppose rst order language probability distribution ground atoms vector consisting called distributional interpretation simply interpretation 
atom predicate symbol interpretation probability suppose chosen smallest rst order language involving constant predicate function symbols horn theory 
case interpretation called distributional herbrand interpretation simply herbrand interpretation 
de nition interpretation distributional model simply model slp ajs ground atom model herbrand respect distributional herbrand model simply herbrand model 
unreasonable de ne semantics terms proofs way 
noted ajs represents potentially nite summation probabilities individual derivations 
analogous de ning satis ability rst order formula terms nite boolean expression derived truth tables connectives example models 
js js 
predicate symbols constant model model suppose slps 
usual write model model learning slps bayes function section describes framework learning complete slp examples maximising bayesian posterior probability 
assumed consists ground unit clauses 
posterior probability expressed bayes theorem follows 
ejs represents prior probability distribution slps 
suppose normal chosen randomly independently distribution instance space ejs js 
assume js js see section 
normalising constant 
probabilities involved bayes function tend small sense re express equation information theoretic terms applying negative log transformation follows 
log log log js log viewed expressing size number bits quantity log js viewed sum sizes number bits derivations constant representing log 
note approach similar described muggleton di ering de nition js 
approach muggleton uses js favour lp hypotheses low generality equation favours slp hypotheses low mean derivation size 
surprisingly bayes function learning slps appropriate nding lps low time complexity respect examples 
instance function prefer slp underlying lp represented quick sort underlying lp represented insertion sort mean proof lengths lower 
appear proc 
workshop learning statistical models relational data 
aaai 
search strategy previous subsection leaves open question hypotheses constructed search ordered 
approach taken involves stages 

lp construction 
choose slp uniform probability labels de nition near maximal posterior probability respect 
parameter estimation 
vary labels increase posterior probability respect progol implement search stage 
stage implemented algorithm assigns label clause laplace corrected relative frequency involved proofs positive examples limitations strategy strategy sub optimal ways implementation stage approximate involves greedy clause clause construction slps implementation stage optimal case positive example unique derivation 
overview progol ilp systems take lps representing background knowledge examples attempt nd simplest consistent hypothesis holds 
section brie describes mode directed inverse entailment approach progol muggleton :10.1.1.31.1630
equation equivalent 
assuming ground conjunction ground literals true models 
true model contain subset ground literals 

set solutions considered progol restricted number ways 
firstly assumed contain positive literal nite number negative literals 
set negative literals determined mode declarations statements concerning input output nature predicate arguments types user de ned restrictions depths variable chains 
progol uses covering algorithm repeatedly chooses example forms associated clause searches clause maximises information compression bounded sub lattice 
hypothesised clause added clause base examples covered removed 
algorithm terminates examples covered 
original version progol muggleton search clause involves maximising compression function number positive negative examples covered number literals minimum number additional literals required complete input output variable chains computed considering variable chains :10.1.1.31.1630
versions progol function reduce degree greediness search 
function estimates global compression expected nal hypothesised set clauses extrapolated local coverage size properties clause construction 
hypothesised clause pruned speci re nements exists previously evaluated clause acceptable solution covers noise threshold negative examples input output variable chains complete associated associated user de ned evaluation progol user de ned evaluation functions progol implemented allowing rede nition prolog equation 
shows convention names progol built user de ned functions variables 
approach allowing de nition evaluation function indirect means general criteria progol pruning search see inequalities applied unaltered long user pos cover user neg cover monotonically decrease user hyp size monotonically increases downward re nement addition body literals hypothesised clause 
learning slps functions derived 
appear proc 
workshop learning statistical models relational data 
aaai 
variable built user de ned pos cover user pos cover neg cover user neg cover hyp size user hyp size hyp rem user hyp rem built user de ned predicates variables equation 
equation rewritten terms information function js log degree compression achieved hypothesis computed subtracting eje posterior information hypothesis consisting returning examples 
eje ejs log compression induced respect simply di erence equations follows 
log js log jh equation extrapolation positive examples covered hypothesised clause comparing equations user de ned functions follows represent built functions represent user de ned counter parts 
log jh worked examples source code progol input les worked examples obtained ftp ftp cs york ac uk pub progol animal taxonomy shows examples background knowledge example set involves learning taxonomic descriptions animals 
stage sec examples class dog mammal 
class trout sh 
background covering dog hair 
knowledge legs dolphin 
examples background knowledge animal taxonomy 
examples man walks dog 
dog walks man 
background np det noun 
knowledge noun 
examples background knowledge simple english grammar 
tion slp constructed uniform probability labels follows class reptile legs eggs 
class mammal milk 
class fish 
class reptile legs habitat land 
class bird covering feathers 
stage labels altered follows re ect distribution class types training data 
class reptile legs eggs 
class mammal milk 
class fish 
class reptile legs habitat land 
class bird covering feathers 
simple english grammar shows examples background knowledge example set involves learning simple english grammar 
stage learned slp follows 
np vp np 
example value equation increased factor achieve positive compression appear proc 
workshop learning statistical models relational data 
aaai 
np verb np prep np 
describes method learning slps examples background knowledge 
method approximate bayes map maximum posterior probability algorithm 
implementation progol ecient produces meaningful solutions simple domains 
pointed section method nd optimal solutions 
author views method described rst attempt hard problem 
believed improvements search strategy 
interesting topic research 
author believes learning slps potential interest domains ilp success muggleton :10.1.1.31.1630
domains believed slps advantage lps producing predictions attached degrees certainty 
case multiple predictions probability labels allow relative ranking 
particular importance natural language domains general application bioinformatics muggleton :10.1.1.31.1630
author wray buntine david page furukawa james cussens discussions topic stochastic logic programming 
due wife daughter clare support happiness give supported partly esprit rtd project project epsrc closed loop machine learning epsrc protein structure prediction development benchmarking machine learning algorithms epsrc machine learning natural language computational logic framework 
cussens 
loglinear models rst order probabilistic reasoning 
proceedings th annual conference uncertainty arti cial intelligence 
san francisco kaufmann 
fagin halpern 
uncertainty belief probability 
proceedings ijcai 
san mateo ca morgan kau man jaeger 
relational bayesian networks 
proceedings thirteenth annual conference uncertainty arti cial intelligence 
san francisco ca kaufmann 
lari young 
estimation stochastic context free grammars algorithm 
computer speech language 
lloyd 
foundations logic programming 
berlin springer verlag 
second edition 
muggleton 
inverse entailment progol 
new generation computing 
muggleton 
stochastic logic programs 
de raedt ed advances inductive logic programming 
ios press 

muggleton 
inductive logic programming issues results lll challenge 
arti cial intelligence 
muggleton 
scienti knowledge discovery inductive logic programming 
communications acm 
muggleton 
learning positive data 
machine learning 
accepted subject revision 
