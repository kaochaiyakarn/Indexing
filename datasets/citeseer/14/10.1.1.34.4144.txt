unified bias variance decomposition zero squared loss pedro domingos department computer science engineering university washington seattle washington cs washington edu www cs washington edu homes bias variance decomposition useful widely tool understanding machine learning algorithms 
originally developed squared loss 
years authors proposed decompositions zero loss significant shortcomings 
particular decompositions intuitive relationship original squared loss 
define bias variance arbitrary loss function show resulting decomposition specializes standard squared loss case close relative kong dietterich zero case 
decomposition applies variable misclassification costs 
show number interesting consequences unified definition 
example schapire notion margin expressed function zero bias variance making possible formally relate classifier ensemble generalization error base learner bias variance training examples 
experiments unified definition lead insights 
better part decades machine learning research concentrated mainly creating flexible learners powerful representations 
time simple learners perform experiments better sophisticated ones holte domingos pazzani 
years reason clear predictive error components powerful learners reduce bias increase variance optimal point trade varies application application 
parallel development researchers learning ensembles models outperforms learning single model bauer kohavi 
complex ensembles outperform simple single models contradicted existing intuitions relationship simplicity accuracy 
finding apparently odds value simple learners easier understand light copyright american association artificial intelligence www aaai org 
rights reserved 
bias variance decomposition error allowing intensive search single model liable increase variance averaging multiple models reduce 
result developments bias variance decomposition error cornerstone understanding inductive learning 
machine learning research mainly concerned classification problems zero loss main evaluation criterion bias variance insight borrowed field regression main criterion 
result authors proposed bias variance decompositions related zero loss kong dietterich breiman kohavi wolpert tibshirani friedman :10.1.1.48.4661:10.1.1.32.9399
decompositions significant shortcomings 
particular clear relationship original decomposition squared loss 
source difficulty decomposition squared loss purely additive loss bias variance proved difficult obtain result zero loss definitions bias variance intuitively necessary properties 
take position forcing bias variance decomposition purely additive defining bias variance happen preferable start single consistent definition bias variance loss functions investigate loss varies function bias variance case 
lead insight clearer picture collection unrelated decompositions 
easier extend bias variance decomposition loss functions 
intuitively bias variance trade exists generalization problem possible useful apply bias variance analysis reasonable loss function 
believe unified decomposition propose step goal 
proposing unified definitions bias variance showing squared loss zero loss variable misclassification costs decomposed 
followed derivation number properties new decomposition particular relating previous results 
describe experiments new decomposition discuss related 
unified decomposition training set 
learner produces model test example model produces prediction 
sake simplicity fact function remain implicit 
true value predicted variable test example loss function measures cost predicting true value commonly loss functions squared loss absolute loss zero loss 
goal learning stated producing model smallest possible loss model minimizes average examples example weighted probability 
general nondeterministic function sampled repeatedly different values seen 
optimal prediction example prediction minimizes subscript denotes expectation taken respect possible values weighted probabilities optimal model model general model non zero loss 
case zero loss optimal model called bayes classifier loss called bayes rate 
learner general produce different models different training sets function training set 
dependency removed averaging training sets 
particular training set size important parameter learning problem want average training sets size 
set training sets 
quantity interest expected loss ed expectation taken respect training sets respect predictions produced example applying learner training set 
bias variance decompositions decompose expected loss terms bias variance noise 
standard decomposition exists squared loss number different ones proposed zero loss 
order define bias variance arbitrary loss function need define notion main prediction 
definition main prediction loss function set training sets argmin ed 
danger ambiguity represent simply ym expectation taken respect training sets respect predictions produced learning training sets multiset predictions 
specific prediction appear produced training set 
words main prediction value average loss relative predictions minimum prediction differs predictions 
main prediction squared loss mean predictions absolute loss median zero loss mode frequent prediction 
example possible training sets size learn classifier classifiers predict class predict main prediction zero loss class 
main prediction necessarily member example main prediction squared loss 
define bias variance follows 
definition bias learner example ym 
words bias loss incurred main prediction relative optimal prediction 
definition variance learner example ed 
words variance average loss incurred predictions relative main prediction 
bias variance may averaged examples case refer average bias average variance 
convenient define noise follows 
definition noise example 
words noise unavoidable component loss incurred independently learning algorithm 
definitions intuitive properties associated bias variance measures 
ym measure central tendency learner 
central means depends loss function 
measures systematic loss incurred learner measures loss incurred fluctuations central tendency response different training sets 
bias independent training set zero learner optimal prediction 
variance independent true value predicted variable zero learner prediction regardless training set 
necessarily case expected loss ed loss function decomposed bias variance defined 
approach propose decomposition show applies different loss functions 
exhibit loss functions apply 
cases may worthwhile investigate expected loss expressed function 
consider example true prediction consider learner predicts training set certain loss functions decomposition ed holds ed ym ed multiplicative factors take different values different loss functions 
showing decomposition reduces standard squared loss 
theorem equation valid squared loss 
proof 
substituting ym ed equation ed ed ed standard decomposition squared loss derived example geman 

shown geman 
minimized making 
authors kohavi wolpert refer ed term bias squared :10.1.1.48.4661
follow convention geman 
simply refer bias 
sense goal unified bias variance decomposition square ed simply consequence square squared loss 
show decomposition applies zero loss class problems reflecting fact noisy examples non optimal prediction correct reflecting variance increases error biased examples decreases biased ones 
pd probability training sets learner predicts optimal class theorem equation valid zero loss problems pd ym 
proof 
represents zero loss proof 
showing equation trivially true 
assume classes implies vice versa 
proving equation 
show similar manner ed ym ed ym ym ym equation trivially true 
ym ym implies vice versa ed pd pd pd ed ym ed ym ed proving equation 
equation ed ed ed depend ed ed ed ed pd pd pd obtain equation equation 
decomposition zero loss closely related kong dietterich 
main differences kong dietterich ignored noise component defined variance simply difference loss bias apparently unaware absolute value difference average loss incurred relative frequent prediction 
side effect kong dietterich incorporate definition variance negative 
kohavi wolpert criticized fact variance squared loss positive 
decomposition shows subtractive effect variance follows self consistent definition bias variance zero squared loss variance remains positive 
fact variance additive unbiased examples subtractive biased ones significant consequences 
learner biased example increasing variance decreases loss 
behavior markedly different squared loss obtained definitions bias variance purely result different properties zero loss 
helps explain highly unstable learners decision tree rule induction algorithms produce excellent results practice limited quantities data 
effect zero loss evaluation criterion higher tolerance variance bias variance decomposition purely additive increase average loss caused variance unbiased examples partly offset offset decrease biased ones 
average loss examples sum noise average bias termed net variance ed averaging equation test examples factor see equation points key difference zero squared loss 
squared loss increasing noise increases error 
zero loss training sets test examples increasing noise decreases error high noise level principle beneficial performance 
decomposition applies general case multiclass problems correspondingly generalized coefficients theorem equation valid zero loss multiclass problems pd pd ym pd ym 
proof 
proof similar theorem key difference longer imply ym ym longer imply implies equation 
equation trivially true 
similar treatment applies equation leading pd ym ym ed pd pd obtain theorem 
theorem means multiclass problems variance biased examples contributes reducing loss training sets ym loss reduced 
leads interesting insight zero loss evaluation criterion tolerance variance decrease number classes increases things equal 
ideal setting bias variance trade parameter learner number neighbors nearest neighbor may direction high variance problems fewer classes 
classification problems zero loss inappropriate evaluation measure misclassification costs asymmetric example classifying patient healthy costly reverse 
consider class case cost making correct prediction nonzero real values decomposition applies case appropriate choice theorem class problems equation valid real valued loss function pd pd ym ym ym 
omit proof interests space see domingos 
theorem essentially shows loss reducing effect variance biased examples greater smaller depending asymmetric costs direction greater 
decomposition applies multiclass case open problem 
apply case decomposition contains additional term corresponding cost correct predictions 
properties unified decomposition main concepts breiman explain bagging ensemble method reduces zero loss order correct learner 
definition breiman learner order correct example iff pd pd :10.1.1.32.9399
breiman showed bagging transforms order correct learner nearly optimal 
order correct learner unbiased definition theorem learner order correct example iff zero loss 
proof immediate definitions considering ym zero loss frequent prediction 
schapire 
proposed explanation boosting ensemble method works terms notion margin 
algorithms bagging boosting generate multiple hypotheses applying learner multiple training sets definition margin stated follows 
definition schapire class problems margin learner example pd pd :10.1.1.31.2869
positive margin indicates correct classification ensemble negative error 
intuitively large margin corresponds high confidence prediction 
set training sets learner applied 
example rounds boosting carried 
algorithms boosting different training sets corresponding predictions different weights sum pd 
computed weights 
definitions apply unchanged situation 
effect generalized notions bias variance apply training set selection scheme simply traditional possible training sets size equal weights 
schapire 
showed possible bound ensemble generalization error zero loss test examples terms distribution margins training examples vc dimension base learner 
particular smaller probability low margin lower bound generalization error 
theorem shows margin closely related bias variance defined 
theorem margin learner example expressed terms zero bias variance positive sign negative sign 
proof 
pd pd pd 
ym pd ym 


demonstration similar pd pd 
conversely possible express bias variance terms margin sign positive sign negative sign 
relationship margins bias variance expressed theorem implies schapire theorems stated terms bias variance training examples 
bias variance decompositions relate learner loss example bias variance example 
knowledge time generalization error related bias variance training examples 
theorem sheds light breiman schapire 
success ensemble methods bagging boosting best explained 
breiman argued bias variance explanation schapire argued explanation 
theorem shows faces coin helps explain bias variance explanation fail applied boosting 
maximizing margins combination reducing number biased examples decreasing variance unbiased examples increasing biased ones examples reverse 
differentiating effects hard understand boosting affects bias variance 
unfortunately loss functions decomposition equation apply 
example apply arbitrary particular apply absolute loss 
see domingos 
important direction determining general properties loss functions necessary sufficient equation apply 
show long loss function metric bounded simple functions bias variance noise 
theorem inequalities valid metric loss function ed ed max proof 
recall function arguments metric iff minimality symmetry triangle inequality 
triangle inequality ym ym expected value equation respect simplifying produces upper bound 
triangle inequality symmetry ym ym ym rearranging terms expectation wrt simplifying leads ed 
remaining components lower bound obtained similar manner 
experiments bias variance decomposition zero loss proposed numerous experiments large suite benchmark datasets blake merz 
space limitations preclude full description experiments see domingos main observations surprisingly varying pruning parameter quinlan minor effect bias variance 
varying maximum number levels decision trees produces interesting results 
bias typically decreases rapidly levels stabilizes 
net variance increases steadily slowly largely variance biased examples significantly offsets variance unbiased ones 
minimum loss extreme level unlimited levels 
boosting tends slightly reduce bias strongly reduce variance 
bulk bias reduction occurs rounds bias stabilizes 
variance curves irregular 
nearest neighbor bias increase dominates variance reduction 
increasing ideal effect reducing variance unbiased examples increasing biased ones 
compared results kohavi wolpert decomposition variance typically smaller contributor error 
largely traced conflicting effects variance biased unbiased examples 
exceptions previous observations 
general case variance increases bias decreases vary monotonically bias variance parameter 
related bias variance decomposition zero loss proposed kong dietterich 
proposed purely ad hoc manner applied ensemble learner artificial noise free domain results show fact founded useful decomposition incomplete 
breiman proposed decomposition average zero loss examples leaving bias variance specific example undefined 
tibshirani points breiman definitions bias variance undesirable properties artificially constructed produce purely additive decomposition 
tibshirani definitions suffer problems hand variance decomposing zero loss bias unrelated quantity calls aggregation effect 
kohavi wolpert defined bias variance terms quadratic functions pd 
resulting decomposition purely additive suffers serious problem assign zero bias bayes classifier 
kohavi wolpert emphasize fact definition bias restricted values binary valued bias natural consequence binary valued loss function 
practice kohavi wolpert method produces biased estimates bias variance estimators obscures meaning example corrected bias negative 
friedman studied relationship zero loss bias variance class probability estimates 
emphasized effect bias variance strongly non additive increasing variance reduce error 
obtain similar results directly terms bias variance class predictions friedman restrictive assumptions classes gaussian probabilities 
proposed general definitions bias variance applicable loss function derived corresponding decompositions squared loss zero loss variable misclassification costs 
showed margins expressed function zero bias variance simple relationship loss bias variance exists metric loss function 
experiments benchmark datasets illustrated utility decomposition 
directions include applying decomposition loss functions conducting experiments 
functions implementing decomposition proposed available www cs washington edu homes acknowledgments research partly supported praxis xxi 
author grateful provided datasets experiments 
bauer kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
blake merz 
uci repository machine learning databases 
machine readable data repository department information computer science university california irvine irvine ca 
www ics uci edu mlearn mlrepository html 
breiman 
bagging predictors 
machine learning 
breiman 
bias variance arcing classifiers 
technical report statistics department university california berkeley berkeley ca 
breiman 
arcing edge 
technical report statistics department university california berkeley berkeley ca 
domingos pazzani 
optimality simple bayesian classifier zero loss 
machine learning 
domingos 
unified bias variance decomposition 
technical report department computer science engineering university washington seattle wa 
friedman 
bias variance loss curse dimensionality 
data mining knowledge discovery 
geman bienenstock doursat 
neural networks bias variance dilemma 
neural computation 
holte 
simple classification rules perform commonly datasets 
machine learning 
kohavi wolpert 
bias plus variance decomposition zero loss functions 
proceedings thirteenth international conference machine learning 
bari italy morgan kaufmann 
kong dietterich 
error correcting output coding corrects bias variance 
proceedings twelfth international conference machine learning 
tahoe city ca morgan kaufmann 
quinlan 
programs machine learning 
san mateo ca morgan kaufmann 
schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods 
proceedings fourteenth international conference machine learning 
nashville tn morgan kaufmann 
tibshirani 
bias variance prediction error classification rules 
technical report department preventive medicine biostatistics department statistics university toronto toronto canada 
