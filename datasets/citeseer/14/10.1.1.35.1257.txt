kaz man tex unsupervised learning word segmentation rules genetic algorithms inductive logic programming kazakov kazakov cs york ac uk suresh manandhar suresh cs york ac uk university york york yo dd uk 
article presents combination unsupervised supervised learning techniques generation word segmentation rules raw list words 
language bias word segmentation introduced simple genetic algorithm search segmentation corresponds best bias value 
second phase words segmented genetic algorithm input order decision list learner clog 
result set order rules segmentation unseen words 
applied training data unseen data rules produce segmentations linguistically meaningful large degree conforming annotation provided 
keywords unsupervised machine learning inductive logic programming natural language word segmentation 
word segmentation task splitting words number constituents morphemes sleep ing dis member ed order compare words study differences 
word segmentation related word morphology tokenization lexical constituents identified text 
word segmentation important subtask morphological analysis range applications hyphenation identification morphosyntactic categories text speech conversion 
instance computational lexicons natural language processing nlp complex just exhaustive list words grammatical properties 
lexical lookup system operates morphological analysis certainly segment sequence characters word morphemes ritchie 

methods word segmentation methods word segmentation theory morphology laid expert ritchie 
attempt data text corpora words annotated grammatical properties learn segment words 
fl kluwer academic publishers 
printed netherlands 
kaz man tex purpose corpus approaches employ eager learning derive single theory tool subsequently applied training unseen data rumelhart mcclelland mooney califf 
lazy learning methods training data directly segmenting word deligne 
comparison hand crafted tools word segmentation learnt data shows sophisticated approaches generative phonology chomsky halle level morphology koskenniemi word constituent surface level projection morpheme concept actual spelling pronunciation obtained application number rules 
approach allows word constituents appear data mapped single morpheme related set grammatical properties 
instance es kicks passes surface level realisations single plural morpheme indicating grammatical property name 
corpus word segmentation explicit theory learnt neural networks rumelhart mcclelland lazy learning derived theories sophisticated abstractions word constituents data brill mikheev 
described adds relatively small growing number methods natural language processing little annotation collins singer riloff jones thompson 

supervised ilp learning morphology inductive logic programming ilp lavrac dzeroski proved feasible way learn linguistic knowledge domains morphological analysis blockeel part speech tagging cussens parsing zelle mooney kazakov 
statistical connectionist approaches learnt theory easily understood modified human expert 
ilp applied learning word morphology occasions 
cases main task learn relationship word morphosyntactic features effectively involves segmenting words making constituents produced mooney califf manandhar muggleton bain 
ilp proved efficient area morphology learning english past tense shows mooney califf muggleton kaz man tex bain 
trying predict past tense form english verb current ilp approaches outperforms best application connectionist approach rumelhart mcclelland propositional decision trees ling known far 
theories learnt come flavours order decision trees mooney califf manandhar pure order logic clauses muggleton bain 
approaches product eager learning halfway house lazy eager learning 
lazy learning approach selected muggleton bain extract single theory corpus 
analysed word derives theory relevant word provides explanation decisions 
ilp morphology learning extends concept language order logic 
order logic mainly convenient way representing operations strings variable length lists possibly structured features 
ilp statistical propositional learning allows background knowledge easily integrated expected gap theories acquired ilp hand crafted theories gradually closed augmenting set background concepts currently simple string operations increasingly complex linguistic concepts rule templates theories kazakov 
order decision lists mooney califf manandhar excellent representation language stating morphological rules 
rules describe regular behaviour exceptions rules exceptions exceptions decision lists easily represent ordered exceptions regular hierarchies 
earlier mooney califf shown morphological rules decision list notation learnt suitable annotated input 
particular clog decision list learner shown perform efficiently learning morphological rules manandhar 
example ilp potential learning word morphology case agreement vowels successive syllables turkish known vowel harmony matthews 
supplying concepts vowel front vowel back vowel allow ilp learner describe agreement specifying pair vowels type explicitly enumerating correct pairs vowels propositional decision tree learner quinlan 
kaz man tex 
unsupervised learning word segmentation annotated corpora greatly facilitates learning 
situations interested unsupervised learning ul unannotated corpora 
motivation ul vary purely pragmatic high cost unavailability annotated corpora theoretical language modelled communication code framework information theory shannon weaver 
unsupervised learning set pillars data representation framework language bias bias introducing order possible representations representational bias search technique looking best representation data respect representational bias 
distinguish lazy ul merely segments words provided eager ul theory potentially applicable data extracted 
look ilp shows order logic allows easy implementation data representation 
best theory learnt ilp learner usually selected built criterion imposing trade generality complexity 
learnt theory seen new representation data criterion selection seen built representational bias 
representational biases default implemented 
issue search decides ilp suitable unsupervised learning 
exhaustive search usually employed ilp inefficient tasks exact algorithm finding optimal solution exists 
complexity word segmentation task means case may forced reduce search imposing constraints ceiling length number word constituents 
instance multigram approach represents text concatenation limited set variable length strings 
representational bias aims maximise likelihood set strings underlying data data respect set strings deligne deligne bimbot 
search best segmentation implemented search minimal cost path directed acyclic graph 
method requires priori selection maximal length constituents 
corpus statistics requires sufficient amount data 
shortcoming approach results reused data set repeating learning step 
algorithms unsupervised learning word segmentation corpus statistics known harris 
notion analogy 
man tex tions derived text speech conversion rules 
eager learning approach aiming segmentation words constituents uses minimal description length mdl representational bias described brent 

approach theoretical interest treats word morphology general point view information theory 
authors search method described proved inadequate 

unsupervised ga ilp learning described combines advantages existing methods unsupervised learning word segmentation expressiveness ilp framework 
practical tools aiming word morphology limit length word constituents brill mikheev deligne bimbot goal allow unlimited lengths 
frequently assumption maximum constituents word brent having clear limitations adopted keep data representation simple main interest focussed search technique subsequent learning order logic segmentation rules 
naive theory morphology ntm representational bias borrowed previous kazakov seen simplified version minimal description length brent 

simplification allows implementation efficient search algorithm kazakov 
firstly ntm bias define fitness function genetic algorithm ga applied search best segmentation list words 
list segmented words obtained genetic algorithm input decision list learning algorithm clog manandhar 
result logic program decision list representation segment unseen words 
generality underlying principle notion minimal description length mdl theoretical interest instance compare quality different representations word paradigm 
practical point view application considered areas single split word desirable 
machine learning point view article introduces combination unsupervised supervised learning methods novel interest domains natural language processing 
kaz man tex decision list exceptions words learner decision list segmented rules ga ntm bias training set words test set words words segmented 
ga ilp learning word segmentation rules selection data motivated need clear generally accepted gold standard segmentation evaluation phase 
language corpus contain word constituents sufficient length test potential advantages framework unlimited length constituents 
corpus conjugated french verb forms english past tense data satisfied requirements experiments 

word segmentation word morphology word segmentation usually initial stage detailed morphological analysis type segmentation required depends application model word morphology 
section starts formal definition concept morpheme 
presents models represent word morphology discusses relationship number constituents word allowed model complexity search required learn model areas possible application 

morpheme usually assumed word segmentation results number constituents cover entire word overlap follow 
fact condition generally valid indo european languages hold arabic matthews 
role word constituents varied history linguistics 
considered classical approach morphology basic assumptions kaz man tex 
minimal word constituents morphemes morphology operates 

word morphemes follow 

set rules produce actual pronunciation spelling morpheme context 
instance said word truthful morphemes truth ful 
want say word studied consists morphemes study ed special rule handles change final study context ed 
different meanings morpheme 
sequence characters grammatical lexical meaning 

invariant lexical grammatical unit realised sequences characters 
definitions matthews sequence characters substituted configuration phonological units lot nlp research aims provide segmentation morphological analysis text speech languages spelling directly reflect pronunciation 
change terminology may raise objection possible split single phonological unit represented characters text 
requirement sequence characters grammatical lexical meaning eliminates cases 
morpheme second sense represents certain kind abstraction number variants called morphs 
imagine word represented levels 
lexical level contains morphemes included word surface level shows actual word formed concatenation corresponding morphs 
instance speak pairs morphemes dog plural morpheme boss plural morpheme leaf plural morpheme negative morpheme surface level produce dogs leaves undo respectively 
distinction usually inflectional derivational morphology 
inflectional morphology concerned productive paradigms different forms verb walk walks walked different forms noun horse horses part speech pos 
noun verb adjective change 
morphosyntactic features tense person change 
hand derivational morphology concerned transformations changing kaz man tex word meaning pos 
instance adding able verb produce adjective related different meaning 
combining words form new known compounding blackboard 
different types morphemes model types morphology 
detailed representation usually include root modified affixes turn divided prefixes suffixes infixes little family languages 
instance un ing non deriv ion highlighted roots preceded prefix followed suffixes 

paradigm representation practical point view important represent word forms word corresponding set morphosyntactic features way compact exhaustive enumeration 
compact theory word formation generative power enabling prediction new word forms known ones analysis unseen words 
certain level complexity required theory reflect subtleties described language 
detailed description language morphology costly practice complexity model chosen trade cost performance 
european languages english romance languages simple efficient representations inflections lexical entry obtained combination stem walk set endings ing 
stems beg ed corresponding different parts paradigm simplify representation decreasing number endings 
paradigms words lexemes merged larger clusters called inflectional classes 
classes 
determine form inflection take paradigm matthews 
instance verbs walk talk belong inflectional class forming various forms adding ed ing 
choice representation varies purpose 
instance second language studied practical reasons put forward matthews words language learnt mixture rote learning rules practice job language teacher combination efficient 
kaz man tex 
lexical vs surface level level morphology simpler human learner nlp developer alike cover cases exceptions simply complex set rules needed 

generative phonology level morphology approaches morphology notion morphemes 
formalisms words formed firstly combining morphemes lexical level applying rules derive word surface level representation 
framework generative phonology chomsky halle initial lexical level sequence morphemes modified sequential application rewriting generative rules word go intermediate stages takes final surface level form 
criticism approach related procedural character rules applied certain order operate underlying surface direction 
objections addressed koskenniemi level morphology 
approach lexicons containing morphemes roots affixes set morphological rules 
rules establish sequence characters surface level appears text correspond sequence symbols represent morphemes lexicon 
morphological rules implemented finite state transducers fst 
fst automata bands alphabets composed pairs symbols 
instance past tense copy represented lexical level combination morphemes copy ed 
corresponding string surface level ed 
pairs corresponding symbols levels aligned possibly making kaz man tex nil symbols 
case participle mapping copy ing copy ing 
possible rule may postulate symbol corresponds letter right hand side rhs context rule matching rhs context theoretical point view aspects level approach interesting 
morphemes close surface level need intermediary descriptions directly related empirical observations 
furthermore approach defended supporters declarative allowing separate conditions governing linguistic phenomenon application 
level rules bi directional operate underlying surface direction generation mode surface underlying direction recognition mode 
related approach worth mentioning kaplan kay multilevel morphology 

vs constituents constraints imposed number length constituents requirement follow overlap number possible segmentations word length gamma follows immediately fact segmentation represented binary vector length gamma marks segment boundary 
number word constituents vary models depends particular application 
context corpus learning higher number complex task 
tradeoff number constituents maximal length increase compensated decrease vice versa 
may wish segment word constituents different morphosyntactic feature assigned 
unachievable table shows 
french verb ons indicates third person plural 
attempt find separate morphemes indicating verb tense mood fail 
increasing number constituents word brings issues related generative power model 
model clearly designed limit number spurious words generate 
achieved introducing different types morphemes restrict way combined 
hard learn model unsupervised learning kaz man tex table words morphosyntactic features word form tense mood person number aim ons indicative third plural aim ons past indicative third plural aim er ons indicative third plural aim er ons conditional third plural er ons conditional third plural framework mean learning morphemes classes ilp lingo learning new predicates parallel 
word segmentation limited single split word constituents belong different classes 
avoid awkward descriptions classes called prefix lhs suffix right 
terms usual linguistic meaning 
depending application correspond stem application uses type morphological lexicon kazakov 
alternatively prefix suffix correspond concatenation morphemes applications aiming text speech conversion unknown words 
case trying copy pronunciation word constituents appear known words 
looking segmentation minimal number constituents approach reliable dis honest better guess di st constituents word model standard teach inflectional morphology languages french czech 
model suited combined description inflectional derivational morphology expect deriv ion fire similar segmentations constituents 
usefulness detailed segmentation depends application 
task predict part speech unknown word concatenation derivational inflectional morphemes nation 
may informative inflectional morpheme ed red 
kaz man tex 
related section offer insight existing approaches unsupervised word segmentation 

segmentation techniques analogy course general linguistics de saussure describes principle analogy long term word forms language change tend form patterns pref suf pref suf pref suf pref suf example words obey pattern ing sleep sleep ing 
word segmented prefix suffix find corpus words satisfying pattern 
analogy principle helps filter spurious segmentations generated simply matching pairs words 
instance prefix appears words onyx ontology segment yx corpus contain pair words pref yx pref pref 
situations principle analogy fail obvious segmentation hand 
example shows instance segmented word pref suf missing data 
pref suf pref suf pref suf pref suf pref suf pref suf brill mikheev describe methods finding set pos tags unknown word 
segmentation method uses just row column analogy principle look words sharing prefix pref satisfying suf suffix suf satisfying affixes rules selected ones generate correct pos tag largest number words 
brill algorithm attempts derive rules pos tag prediction affix data 
computationally expensive search pruned limiting maximal length instance nnp proper noun jj adjective det determiner kaz man tex affix 
threshold increased cost considerable reprogramming decision priori mean blind shot require knowledge language corpus 

harris approach harris describes unsupervised approach segmentation utterances phonetically 
approach counts number different phonemes br prefix appear utterance language initial sequence prefix phonemes 
notation br prefix length context utterance 
utterance available left substrings prefix produced counts br prefix computed 
utterance segmented function br reaches local maxima 
possible adapt method segmentation words utterances replacing phonemes letters 
mentioned harris method efficiently implemented creating trie utterances 
trie labelled tree labels correspond single character knuth 
usually employed provide fast efficient method storing dictionaries 
tree unique root marking words leaves labelled character marking word complete paths root leaf correspond valid words 
function br prefix gives number branches path prefix name 
gives example trie words cut cuts bread spot spots spotted 
segment word algorithm looks number branches br prefix path prefix root downwards 
gives value br prefix list words 
word segmented initial substring length br reaches local maximum greater br gamma br 
gives examples maxima indicated character ffl 
plateau case shown nodes plateau considered segmentation points provided downhill slope follows 
criterion obtains cut cut spot spot spot ted 
bread segmented case shown 
alternative approach suggested harris focus word utterance suffixes 
starting word trie constructed words reverse 
word entered trie 
kaz man tex 
trie structure list words cut cut bread spot spot spot ted 
values br prefix letter prefix 
segmentation criterion 
harris claims modification technique performs better english 
modification criterion segmentation th letter br br 
difficult formalise procedure number vaguely defined constraints kind br value 

approaches information theory deligne thesis describes segmentation long single string possibly containing sentences delimiters words 
adapt method task word segmentation introducing firm boundaries text corresponding word borders 
method looking maximise likelihood data model time 
possible segmentations represented directed graph start node 
edge man tex br br br br br br 
segmentation points various shapes br responds sequence letters forming segment 
path start node represents particular segmentation text 
edge graph assigned weight representing probability segment 
probability estimated simply relative frequency string text search best segmentation maximises product segments probabilities reformulated search minimal cost path graph 
initial segmentation obtained re estimate probability segment segment text 
process stops segmentation stable 
obvious virtue method previous knowledge language required 
complexity method led deligne limit maximal length segments experiments 
context segmenting lists words continuous text removing restriction may cause problems number constituents exponential respect word length 
additionally multiplied complexity search minimal cost path number nodes graph easily issue especially languages turkish words considerable length 
kaz man tex word segmentation method information theory employed brent 

article describes binary encoding list words lexicons word constituents table describing constituents combined form words 
encoding requiring minimal number bits assumed describe optimal segmentation list words 
search space possible encodings large approach limits number constituents word 
suffixes righthand side constituents share suffix allowed 
search technique iterative trying reduce binary encoding size alternately adding new suffixes lexicon removing suffixes lexicon addition new suffixes result shorter representations 
results summed authors words method promises useful tool refining making explicit linguistic intuitions 
bad news search 
non trivial linguistic theories degrees freedom exhaustive search impossible 
promises keep field natural language interesting time come 
brent describes unsupervised method recovering word boundaries text word delimiters spaces removed 
method estimates probabilities potential word sequences original text reconstructed 
brent method tested successfully speech corpora 
needed test brent approach adapted successfully task studied 

french verb morphology data set measure performance method produced specially created data set consisting entire paradigms french verbs 
section provides short overview french verb morphology data set conjugated french verbs experiments representation segment word model 

french verb morphology verbs french complex conjugation see table ii combines synthetic single word forms analytical ones making auxiliary verbs 
instance kaz man tex person singular simple ai aim person singular past tense formed auxiliary 
purposes research synthetic forms verb considered 
main french verb 
subdivision bring number classes 
conjugation consists verbs infinitives er person singular indicative form 
second conjugation includes verbs infinitives ir participle 
cover vast majority french verbs 
small number verbs form third conjugation 
characteristic conjugation verbs irregular forms predicted conjugation verbs 
result conjugation irregular verbs described separately 
paradigm verbs considered regular represented produced concatenation stem set endings 
table ii stem printed bold face 
remaining part word forms verb endings described summary french 
irregular verb paradigms belonging third conjugation stem may change completely word forms stems share single letter phoneme 
annotation word forms paradigms problematic 
standard segmentation endings mentioned summary detailed paradigm description recognised authority missing opted single gold standard verbs third conjugation 
verbs non quantitative evaluation segmentation produced learning algorithm carried individual basis 
regular verbs spelling stem word constituent left defined endings table ii undergo changes 
instance mang mange ons 
cases spelling pronunciation stem change ed ed ons er er ons es 
peculiarities split sub cases conjugation regular case plus exception second stem appropriately shortened remaining part man pla complex templates introduced man tex table ii 
second french short stem bold face conjugation second conjugation indicative subjunctive indicative subjunctive 
pers 
sing 
aim aim fin finiss 
pers 
sing 
aim es aim es fin finiss es 
pers 
sing 
aim aim fin finiss 
pers 
pl aim ons aim ions finiss ons finiss ions 
pers 
pl aim ez aim iez finiss ez finiss iez 
pers 
pl aim ent aim ent finiss ent finiss ent imperfect imperfect imperfect imperfect 
pers 
sing 
aim ais finiss ais finiss 
pers 
sing 
aim ais es finiss ais finiss es 
pers 
sing 
aim ait aim finiss ait fin 
pers 
pl aim ions ions finiss ions finiss ions 
pers 
pl aim iez iez finiss iez finiss iez 
pers 
pl aim ent finiss finiss ent imperative imperative simple past simple past 
pers 
sing 
aim ai fin 
pers 
sing 
aim aim fin fin 
pers 
sing 
aim fin 
pers 
pl aim ames aim ons fin finiss ons 
pers 
pl aim ates aim ez fin finiss ez 
pers 
pl aim erent fin conditional conditional simple simple 
pers 
sing 
ai ais ai ais 
pers 
sing 
ais ais 
pers 
sing 
ait ait 
pers 
pl ons ions ons ions 
pers 
pl ez iez ez iez 
pers 
pl ont ont participle infinitive participle infinitive aim ant finiss ant past past masc 
sing 
aim fini masc 
pl aim fini fem 
sing 
aim fini fem 
pl aim es fini es kaz man tex ing part verb paradigm fill consonant set endings slot missing fill consonant 
segmentation schemes evaluation approach word assumption 
formalisms distinguish lexical surface level level morphology koskenniemi single stem mang lexical level realised written forms surface level mang mange operate directly alterations stem 
word form stem alteration considered correct called short stem ss 
forms simple conditional infinitive short stem regular verbs immediately followed suffix er ir 
similarly form imperfect subjunctive combine short stem alternation second person singular simple past ass conjugation corresponding suffix second conjugation forms iss 
past participle french adjective 
base form masculine singular adjective formed adding resp 
short stem 
word forms masculine plural feminine singular feminine plural formed adding endings es respectively 
observations section representations french verb paradigm chosen 
short stem representation forms verb split short stem defined corresponding 
short stem shown table ii bold face 
long stem representation longest constituents ss ss ass ss er ss resp 
ss ss iss ss ir ss stem 
table ii long stems separated blank space endings 
exception general principle singular indicative third person plural simple past ii conjugation short stem ss ss ir 
amendment representation conform commonly french taught foreign language 

french verb data set data set experiments described article consists parts 
part contains paradigms regular kaz man tex table iii 
regular verb conjugation classes represented data set class conjugation short stem alterations number verbs placer manger mang mange pes ese eder ed ed jet appel ii total second conjugation verbs verbs different synthetic forms enumerated 
regular verbs grouped classes changes short stem undergoes 
classes shown table iii class represented infinitive verbs belonging class 
table shows number verbs classes 
irregularity verbs second part data set shows higher number different word forms paradigm compared regular case see table iv 
words part data set annotated short stem long stem segmentations defined previous section 
table iv 
data set statistics data set word forms different different word forms word forms paradigm regular verbs irregular verbs regular irregular data set generated combining endings verb paradigm corresponding stems 
result stored predicate fv 
arguments fv clause contain word followed suffix stem 
information argument contains general kaz man tex class ii iii representative finer grain template generate segmentation 
data fv comes flavours segmentation short long stem 
sample data set shown 
second part data set comprises paradigms irregular iii conjugation verbs go etre examples templates verbs verbs verbs verbs verbs verbs verbs verbs 
total irregular verb paradigms data set 
predicate fv argument experiments 
speaking templates context claimed irregular verbs sound confusing case able define stem suffix comparing conjugation verbs template 
case 
fact verbs template derived example 
paradigms show little difference template limited single letter 
example situation english verbs gun pun 
claim verb stem consists single letter misleading 
compare regular irregular verb forms sharing morphosyntactic features find common 
irregular verbs forms factoring common leave empty stem 
tu es indicative second person singular clearly infeasible 
irregular verb forms identify constituent playing role inflectional morpheme exact form constituent 
instance suffixes ens ns play inflectional morpheme role conjugated forms verbs 
tempted provide unambiguous annotation forms irregular verbs purposes quantitative evaluation clearly possible 
personal communication number french linguists shown consensus individual paradigm reached 
chosen single segmentation correct case meant position discussion french verb morphology far home ground 
opted providing segmentations irregular verbs pro esslli summer school kaz man tex duced method food discussion 
segmentations obtained request 
fv word suffix gr template 
fv gr manger 
fv gr manger 
fv gr manger 
fv gr manger 

data set segmented words 
unsupervised segmentation genetic algorithm section describes bias employed rank putative segmentations set words 
section shows genetic algorithm ga bias fitness measure employed search best segmentation set words respect bias 

ntm bias word segmentation consider segmentation list words word framework 
segment boundary position word represented length constituent integer zero word length 
segmentation words list represented vector integers 
word list arbitrary vector kind introduces naive theory morphology ntm kazakov 
theory defines lexicons prefixes resp 
suffixes enumerated repetition see 
quality theory estimated number characters prefix lexicon suffix smaller number better theory 
upper bound max measure number characters word list 
case reached prefix suffix generated twice theory 
word segmentation bias formulated 
ntm word segmentation bias set naive theories word morphology select lowest number characters corresponding pair lexicons 
kaz man tex ais ai tait er length lexicon lexicon suffixes word prefixes boundary segment chan ta number characters list words 
naive theory word morphology note ntm bias similarly harris approach see section take account frequencies words actual texts 
believe rightly manner word segmented independent word 
ntm bias hypothesis substrings composed real morphemes occur words frequency higher left right substrings 
way segmentation low produce lexicons prefixes suffixes correspond single morphemes combinations 
word list stored list pairs indices prefix suffix 
lexicons affixes bias described seen shortest representation encode lexicon 
ntm bias provides ordering hypothesised segmentations list words 
need search algorithm search space plausible segmentations find segmentation optimal respect bias 
ntm bias global measure computed segmentation entire word list known lend easily greedy search techniques gradient information hill climbing search 
employ genetic algorithm search space possible segmentations ntm bias fitness measure kazakov 
explored possible way divide word constituents multiple constituents repeated ga derived lexicons input kazakov 
section describe ga implementation word segmentation task 
presumption limited languages main operator derivational inflectional morphological rules concatenation 
kaz man tex 
ga implementation genetic algorithms ga goldberg search technique tasks large search space greedy search methods get stuck local maxima 
genetic algorithms inspired darwinian evolution 
ga maintains set candidate solutions called chromosomes individuals applies natural selection operators crossover mutation successively generate new candidate solutions existing ones 
fitness function employed rank individuals determine goodness 
chromosomes represented sequence letters alphabet 
crossover operation constructs new child chromosomes splicing parent chromosomes points 
mutation operator creates new chromosome single parent randomly changing single letter parent chromosome 
chromosomes mutated mutation probability known mutation rate 
version ga employed experiments known simple genetic algorithm sga shown 
chromosome procedure simple genetic algorithm 
initialisation create random population candidate solutions individuals popsize size 
evaluate individuals fitness function 
store best evaluated individual best individual 

generation selection sample individuals fitness resulting mating pool higher fitness appear repeatedly higher probability 
apply crossover probability crossover rate 
apply mutation probability mutation rate 
evaluate individuals fitness function 
update best individual 

stopping condition satisfied provide best individual solution 
go step 

simple genetic algorithm sga codes segmentation entire word list 
chromosome vector integers 
integer index represents segment point word word list 
example kaz man tex sequence chromosome encoding segmentation shown 
fitness chromosome calculated max gamma fitness function defined way take negative values sga searches individuals maximal fitness 
mutation sga defined shift morpheme boundary position left right random choice new boundary position allowed interval 
mutation rate set user increased generations best individual 
higher mutation rate kept better individual produced changed back original value 
point crossover applied 
fittest individual kept generations result learning certain number generations 
characteristics typical ga 
nondeterministic 
provide solution may optimal 
running sga training data proved time consuming split training data chunks certain length 
separately ran sga put back segmented words chunks 
technique proved feasible trade input data size time needed find ntm high quality 
described decomposition allows individual sga runs run parallel 
technique related splitting training data separate lists sort alphabetical order list words splitting 
technique assumption list containing word forms paradigm suitable learning difference maximal value max ntm bias value number characters lexicons best ntm great 
difference increased chunks list words provided language represented data set differences word forms paradigm concentrated suffixes 
data set artificially produced 
corpus contains conjugated verbs allow clear evaluation framework 
restriction imposed part speech possible obtain exhaustive list word forms unannotated corpus words corpus listed duplicates removed 
increasing corpus size word list converge exhaustive lexicon word forms 
kaz man tex 
learning segmentation rules ga ilp value segmentations obtained ga limited factors 
firstly cases segmentations optimal segmentations seen incorrect suboptimal respect ntm bias shows sample ga output 
secondly segmentations represent theory applicable examples training data 
problem addressed certain extent application simple iterative statistical procedure similar deligne 
instance word prefix suffix lexicons containing relative frequency items select segmentation maximises product relative frequencies prefix suffix seg req pref req suf done words lexicons computed step repeated segmentation stable kazakov 
criterion needs prefix suffix best segmentation lexicons 
ga ilp approach adopted output ga form clauses predicate seg word suffix 
example representation seg 
clauses training set positive examples ilp learner clog 
theory word segmentation obtained result order decision list consisting parts exceptions rules order 
exception fact training example unchanged segmentation just word training set 
exceptions impact segmentation unseen words removed decision list 
cases exceptions correspond segmentations incorrect respect ntm bias 
segmentation rules exceptions removed applied training set words general result better segmentation generated ga summarises ga ilp approach 
rest section introduces clog describes learning word segmentation rules 

decision list learning clog clog described system learning order decision lists 
clog shares fair amount similarity foidl mooney kaz man tex table sample ga output erez ee ent es ra eb erez es ez ait ez ons ions iez ees ent ons ait er califf 
foidl clog learn order decision lists positive examples desirable property language learning 
plural 
plural 
plural 
plural 
plural 
plural 
plural 
plural 
plural 
plural 
plural 
plural 

sample training data set clog 
input clog list examples 
instance examples describe plural common english nouns 
training examples interested learning ordered set rules 
example appropriate background knowledge examples clog produce set rules exceptions 
clause exception 
plural 
plural mate 
plural mate 
plural mate 

rules induced clog data 
general rule english change ies kaz man tex clog able learn rule example training set showing pattern 
second rule states word ends plural form replaces en see definition mate predicate 
rule default rule apply previous rules fail 
default rule add word 
clog algorithm clause time sequential cover algorithm mitchell learns decision list starting clause 
clog employs bottom guidance consider generalisations relevant example 
similar strategy progol muggleton 
currently generalisations generated user defined predicate example clauses 
example consider scheme manandhar 

note definition generate clauses different experiments described section 
example provided example ease understanding 
generate clauses plural clauses bagof plural mate mate clauses 
mate true exists stm stm stm denotes concatenation 
intuitively plural prefix prefix suffix suffix share common stem stm 
mate split 
mate split 
mate split split 
mate split split split split 
split 
split split 
query mate solutions obtained plural clauses give kaz man tex clauses plural mate plural mate plural mate plural mate clauses generalisations example plural 
efficiency clog comes fact generate clause evaluate strategy existing learners foidl 
clog employs generate evaluate strategy computes single iteration heuristic goodness clauses 
results significant gain efficiency 
previous example training set simultaneous evaluation clauses determine best respect gain function see 
clog lower memory requirements train large data sets keeps single training example memory time 
remaining training examples stored files 
contrasts ilp learners keep training examples memory 
set generalisations example produced predicate generate clauses called generalisation set 
denoted gc 
clog cycles input example generalisation set single iteration checking candidate generalisation covers example positively negatively 
process completed best candidate generalisation chosen 
example set pruned candidate cycle repeats 
notation gamma qp denotes number examples covered correctly gamma qn denotes number examples incorrectly covered gamma sp denotes number previously covered examples covered correctly gamma qn denotes number previously covered examples incorrectly covered 
contrast foidl clog learns rules sequentially training examples changing ordering examples may result different ordering learnt rules 
gain function currently clog user defined 
segmentation problem chose simple gain function kaz man tex ptc positive examples covered cpe set covered positive examples initially empty dl decision list learnt initially empty ptc empty arbitrary example ptc gc clause covers example ptc sp sn qp qn gc covers positively sp sn qp qn sp sn qp qn covers negatively sp sn qp qn sp sn qp qn endif endif example cpe sp sn qp qn gc covers positively sp sn qp qn sp sn qp qn covers negatively sp sn qp qn sp sn qp qn endif endif best gc gain best maximum example ptc best covers positively cpe cpe feg ptc ptc gamma feg endif example cpe best covers negatively cpe cpe gamma feg ptc ptc feg endif add best top dl enddo 
clog algorithm gain qp gamma sn gamma number literals clause body qp sn defined 
experiments manandhar 
show clog significantly efficient foidl task learning morphology rules 
task analysis plural english nouns running times foidl clog table vi times sun sparc 
kaz man tex table vi 
running times clog foidl task learning rules analysis english nouns 
training set size clog foidl earlier segmentation french verbs kazakov manandhar running times clog data set training examples seconds respectively 
foidl seconds respectively data sets 
experience clog indicates ideally suited applications user clear idea type clauses generalised example total size generated clauses large roughly clauses example current hardware 

setting clog learning segmentation rules section describes coding background predicates needed clog induce decision list rules word segmentation 
sga output list known prefixes suffixes generated stored prefix prefix resp 
suffix suffix 
seg generate prefix suffix 
restrict segmentation points clog considered hypothesised rules 
task learning segmentation rules coded definition generate clauses took input example generated output clauses body contained append literals covered example 
generated clauses included possible variable binding patterns consistent mode declarations covered example 
predicate append deterministic modes append pref suf append pref suf 
defined standard way append 
append append 
furthermore values pref suf prefix resp 
suffix 
instance provided prefix prefix suffix suffix kaz man tex table vii 
output query generate clauses 
seg append 
seg append 
seg append 
seg append 
seg append append 
seg append append 
seg append append 
seg append append 
seg append append 
seg append append 
seg append append 
seg append append 
seg append append 
seg append append 
seg append append 
seg append append 
seg append append 
seg append append 
seg append append 
seg append append 
segmentations generated sga query generate clauses seg produces output table vii 
kaz man tex 
experiments results french verb data set described section learn order word segmentation rules help ga naive theory morphology bias clog described previously 
setting experiments specified values parameters data set regular irregular verbs regular verbs 
ga input chunk size represents number words chunks input list words split 
ga applied separately described 
values words 
training data size entire data set split random way halves 
halves unseen data 
different experiments remaining half training data 
precisely training half data split lists length application ga second ga segmented lists learn segmentation rules clog 
data evaluation annotation provided irregular verbs quantitative evaluation performance rules learnt carried regular verbs data 
regular verbs training unseen half data set regular verbs training unseen 
order clog training examples learning examples seg clauses segmented words alphabetical order marked ff tables results introduced ga input compared setting clog input sorted length endings marked words shortest endings processed longest 
ordering clog training examples avoids problems situation 
legal endings right substring ons ions correct decision produce longer decision list learnt clog rule deriving shorter rule kaz man tex deriving longer rule learnt 
opposite happens rule looking longer ions added decision list add segmentation rule fired presence shorter ons mean misclassify longer stem examples 
practice conflict leads specific rules stem suffix 
suggested ordering example longer seen subsequently rule longer derived examples shorter processed learner 
word lists repetitions removed learning 
evaluation done full list verb paradigms word forms appeared 
reason related case long stem paradigms gold standard evaluation 
case may happen word form annotated different segmentations depending morphosyntactic category corresponds fin second person singular indicative fini masculine plural past participle 
fact words data set allow multiple segmentation 

varying ga input length tests sample different word forms data set including regular irregular verbs training data experiments ga input chunk size resp 
words 
results shown table viii 
columns shows ratio nmax nmax number characters words number characters stem lexicons corresponding list segmented words generated ga clog rules exceptions applied training data cf 

certain number words covered clog rules ratio computed twice clog 
firstly ratio covered covered successfully segmented words see column cov 

secondly ratio computed words number characters uncovered words added size stem suffix lexicons words segmented see column 
results show subset words covered clog rules really corresponds segmentations lower segmentation bias value average segmentation words produced ga kaz man tex table viii 
performance function ga input chunk size total ga input nmax evaluation annotated regular verbs chunk ga clog ss cov 
acc prec acc prec acc prec acc prec words words table ix 
impact order examples rules learnt clog order ss training examples acc prec acc prec acc prec acc prec alphabetical ff shortest endings remaining part table shows results application segmentation rules set regular verbs annotation available 
parameters evaluation 
accuracy standard meaning percentage words segmentation produced rules identical provided annotation 
precision computes score accuracy take account words segmentation generated rule covers 
accuracy precision shown different cases 
case column generation long stem considered correct 
second case count correct segmentations deriving long short stem 
cases common errors counted interest linguists 
segmented words suffix er ir belonging long stem split part common attached 
fact correct segmentation verbs iii conjugation separate case 
case table accounts segmentations common part ii conjugation suffixes ass iss usually considered part long stem attached 
experiment ga applied shorter word chunks word list subsequently learnt order rules able produce words segmentation long stem representation column table 
words application rules resulted kaz man tex segmentation contained long short stem column table 
words segmented rules considered score case raises 
comparison accuracy precision achieved experiments shows accuracy case remains unchanged 
applying ga longer lists words vs results balanced distribution long short stem segmentations increasing number words partially compensates bias data introduced alphabetical sort 

impact training examples ordering results learning word sample regular irregular verbs compared cases clog provided ga output alphabetical order ff ga output sorted words shortest endings came 
annotated regular verbs evaluation 
results reported table ix 
impact ordering positive segmentation rules applicable words acc rec words segmented paradigm representation long stem accuracy increases case goes 
larger coverage rules paid decrease precision case 

performance unseen data performance segmentation rules learnt samples regular irregular verbs evaluated test data sets unseen regular verbs regular verbs see table 
case words evaluation learning phase 
test data set includes unseen words inflected forms regular verbs learning 
comparison shows applied unseen data segmentation rules achieve slightly lower accuracy precision long stem case 
case accuracy precision better unseen data data containing training examples 
possible explanation increase accuracy precision certain number fitting rules produce incorrect segmentation training words fired case unseen data 
kaz man tex table performance unseen vs data training test ss examples data set acc prec acc prec acc prec acc prec ff unseen reg 
ff reg 
unseen reg 
reg 

impact irregular verbs training data annotation provided case regular verbs may question reasons including conjugated forms irregular verbs training data 
fact authors original expectation examples act noise decrease performance rules evaluated task segmentation regular verbs 
results table xi confirm hypothesis 
adding examples corresponding different conjugation increases scores cases 
reasons account 
firstly additional irregular verb examples endings corresponding examples ii conjugation 
result verb paradigm irregular individual word forms support segmentation conforms regular 
secondly fact irregular verbs take completely different stems paradigm decreases coverage rules making stems favouring way learning rules french indo european languages general ones 

learning training data sets different size having explored influence parameters learning evaluation phase learning algorithm performance usual comparison results learning training samples different size 
table xii shows results learning word forms regular irregular verbs input ga clog alphabetical order ga input split lists words 
table shows relatively low impact training data set size samples chosen 
increasing number training examples precision decreases accuracy case increases 
kaz man tex experiments described section ga run generations number individuals equal times higher number words input ga lists 
crossover rate set mutation rate 
parameters selected number undocumented experiments 
populations individuals average time run pentium mhz platform min 
single run clog largest training sample examples took hours running sicstus prolog single thread sgi origin mhz mb ram processor 
table xi 
learning regular verbs vs learning verbs tr 
examples ss type size order acc prec acc prec acc prec acc prec reg 
ff regular ff table xii 
accuracy precision function training data size tr 
examples ss number order acc prec acc prec acc prec acc prec ff ff ff 
samples segmented words rules shows sample decision lists learnt clog 
experiments rules learnt corresponded basic patterns 
displays representatives patterns obtained experiment second row table ix learning rules examples 
rules type look particular left substring word data set usually stem word 
rules limited scope number word forms stem verb paradigm 
rules second type looking particular 
french languages kaz man tex word morphology operates closed set inflectional suffixes 
result representative data set rules suffixes cover stable percentage examples absolute number words covered increase approximately linearly size list words available 
rule patterns result insufficient number examples noise imperfect segmentation ga output learning larger noisy data result rules types 
rules real interest identify substrings morphemes derived ga setting 
instance example shown third pattern covers words alteration allows distinguish indicative hand imperfect indicative subjunctive 
similarly example fifth pattern looks right substring ira producing 
difference ir strings suffix derive ii conjugation long stem short simple conditional 
case learning rules regular irregular verb word forms order decision list learnt contained exceptions rules 
rules corresponded second rule pattern unconditionally deriving word 
examples segmentation entire paradigm verbs shown table xiii 
stem produced separated hyphen stem defined section set bold face 
paradigms verbs manger belong conjugation 
segmentation produced long stem word forms short remaining cases 
short stem invariably preferred past participle producing endings ee es ees 
case manger segmentation rules produced alterations mang mange short stem 
third paradigm shown second conjugation verb 
word forms segmentation produced long short stem fact cases handled special amendment definition long stem paradigm representation 
amendment dropped result conform long stem representation cases 
representation scheme convention adopted convenience fact learning unannotated data results certain paradigm representation may considered serious support wider 
kaz man tex seg 
seg 
seg 
seg append append 
seg append 
seg append append 
seg append 
seg append append 
seg append 
seg append 
seg append 
seg append 
seg append 
seg append 
seg append 
seg append 
seg append 
seg append 
seg append 
seg append 
seg append 

sample decision list rules learnt clog 
seg append 

seg append 

seg append append 

seg append append 

seg append append 

types rules learnt kaz man tex table xiii 
segmentation manger conj ii conj iii conj 
aim mang fini ais aim es mang es fini aim mang fini aim ons mange ons finiss ons ons aim ez mang ez finiss ez ez aim ent mang ent finiss ent ont aim ais mange ais finiss ais ais aim ais mange ais finiss ais ais aim ait mange ait finiss ait ait aim ions mang ions finiss ions ions aim iez mang iez finiss iez iez aim mange finiss aim ai mange ai fini ai aim mange fini aim mange fini aim ames mange ames fin ames aim ates mange ates fin ates aim erent mang erent ent erent aim manger ai ai ir ai aim manger ir aim era manger ir ons manger ons ons ir ons aim erez manger ez ez ir ez ont manger ont ont ir ont aim mang finiss aim es mang es finiss es es aim mang finiss aim ions mang ions finiss ions ions aim iez mang iez finiss iez iez aim ent mang ent finiss ent ent finiss es es finiss es asses aim mange fin ions ions finiss ions iez iez finiss iez ent ent finiss ent ais manger ais ais ir ais ais manger ais ais ir ais aim manger ait ait ir ait ions manger ions ions ir ions aim manger iez iez ir iez manger ir aim mang fini aim es mang es fini es aim ee mang ee fini ee aim ees mang ees fini es ees aim ant mange ant finiss ant ant aim er manger er kaz man tex table xiii shows paradigm irregular verb 
segmentations correspond short long stem representation 
segmentations contain stems considered alterations short stem 
grammar book iii conjugation endings simple rai ras verb question leaves merely short stem 
long stem producing exactly endings regular conjunctions ir derived rules 

comparison ga harris harris segmentation technique compare relative merits ga preprocessing step ilp learner algorithm 

experimental comparison experiment algorithms applied training data set containing regular irregular verb forms 
number short long stems produced counted subset regular verb forms annotation available cf 
row table xi 
harris method trie reversed words rl trie yielded consistently better results 
avoid comparing methods produce different number segments ga harris method modification suggested 
local maxima function br global maximum defined segment boundary 
instance br mange br multiple segmentation mange ass es result single segmentation mange asses 
modification reduced number segments word cases 
words global maxima different segmentations segments produced output 
results evaluated separately class stem long short 
recall precision score equations evaluation score defined geometric mean recall precision see table xiv 
measure accuracy experiments ga clog specific case recall stem word produced 
recall correct stems words precision correct stems segmentations produced kaz man tex score theta recall theta precision recall precision results table xiv show standard harris method best segments word sought recall important precision strong preference consistent long short stem verb paradigm 
fact method generates segments word unsuitable preprocessing step setup decision list learner programmed split word point expects training data type 
comparison ga modified version harris algorithm shows superiority ga respects 
firstly ga performs better generation single type stem long 
intuitively higher ratio long versus short stems generated ga mean clog confused concurrent patterns rules learns produce type stem 
expectation confirmed results table xv 
table compares performance clog rules learnt training data set words set training unseen regular verbs 
versions harris method trie built starting lr trie rl trie word letter compared 
better terms accuracy precision consistently generates type stem 
combination ga ilp clog clearly best easily outperforms harris clog irrespective standard lr reversed rl procedure employed 
table xiv 
ga vs harris algorithm long stem short stem rec 
prec 
score rec 
prec 
score ga harris rl split harris rl split 
stem alteration handling harris method particularly prone errors stem alteration occurs 
alterations reflect phonetic changes required spelling conventions 
number french verb stem alterations shown table iii see 
seen stem changes kaz man tex table xv 
harris vs ga preprocessing step clog algorithm ss acc prec acc prec acc prec acc prec ga clog harris lr clog harris rl clog concentrated cases consist adding extra letter replacing final 
instance stem verb manger mang letter added consonant hard vowel follows 
forms manger training corpus segmentation harris ga compared table xvi 
number suffixes table start letter cases belong stem mang 
words marked asterisk harris erroneously produces stem mange 
harris method ga segmentation verb contains correct short long stem 
possible generalise observation number patterns standard multi segment harris approach fail 
consider part trie shown successors stem subtrees starting letter subtrees starts letter required definition 
subtrees path stem harris method produce stem including cases stem correct stem 
equivalent statement word stem suf segmented immediately stem single character segmented stem word stem suf segmented stem 
opposite case stem stem produced letter follows case manger shown table xvi 
serious shortcoming harris approach related distance applied right left word segmented 
farther robust method number words sharing letters kaz man tex table xvi 
comparison harris ga segmentations harris segmentation short long ga segmentation ga stem mange ai mange ai mange ais mange ais mange ant mange ant mange ass es es mange ass ez iez mange ass ons ions mange ons mange ons mange mang er mange ra manger mange rai manger ai mange mang mange manger ait mange ras manger mange ron manger ons mange ron manger ont mange mang ez mange ames mange ames mange ates mange ates mang iez mang iez mang ions mang ions mang erent mang erent mang mang mang ee mang ee mang ees mang ees decreases 
extreme word length certain word corpus sharing letters word segmented letter gamma morpheme boundary 
instance correct segmentation noun pl possible left right mode singular form corpus 
harris original problem avoided prompting human operator provide appropriate example clearly infeasible large data sets 
kaz man tex tr ltr ltr ltr ltr tr tr 
tr stem 
stem alteration handled harris look data shows small number words segmentation generated harris method words clearly contain morphemes number branches trie rises peak instance erent ga usually able handle completely regular morphology words proper way 

hybrid approach unsupervised learning word segmentation rules described efficient combination ga ilp 
provides approximation concept learnt reduces dramatically search space 
learns rules employed segment unseen words 
results tests show set rules word segmentation learnt limited number unannotated words opposed approaches large tagged corpora 
size corpora order words average statistical nlp 
theoretical comparison learning strategies shows small subsets data derive microtheories cf 
analogy principle independent rest data 
hand best segmentation choice integral criterion mdl brent constituent relative frequency data deligne 
naive theory morphology ntm bias chooses single segmentation related segmentation words 
limit words prefix suffix provided ntm bias analogy principle kaz man tex able produce segmentation 
ntm need impose limitation length affixes approaches brill deligne 
clearly distinguish ntm bias word segmentation ga convenient simplification mdl principle takes account size model theory 
ga easy implement algorithm search best ntm 
ga seen search technique mdl framework word segmentation far appear exploited 
ntm bias shows feasibility number cases approaches fail 
case demonstrated harris method 
case shown equation section handled de saussure analogy usually easily managed ntm bias 
limitations approach computational complexity ga search best ntm constituents word model 
possibility modify ntm bias brent 
brent take account amount information needed reconstruct data set ntm 
shift encoding paradigm modification mdl bias required possible ga search segmentations elements 
time efficiency clog provided premise experimentation larger data sets bootstrapping techniques considered generation list words clog learn rules 
explore possibility approach preprocessing phase morphology learning tasks instance prediction pos unknown words 
ability obtain approximation best ntm representation lexicon important contribution nlp far theoretical interest 
machine learning point view probably important contribution described combination unsupervised supervised learning novel potential range applications nlp area 
authors grateful anonymous referees detailed highly useful comments pointing harris 
wish stephen muggleton james cussens kaz man tex thoughtful remarks 
author indebted olga step number improvements suggested francois discussions 
supported esprit long term research action inductive logic programming ii part research carried prague 
level phonology 
notes linguistics pp 


paris 
blockeel application inductive logic programming natural language processing 
master thesis katholieke universiteit leuven belgium 
brent efficient probabilistically sound algorithm segmentation word discovery 
machine learning 
brent murthy discovering suffixes case study minimum description length induction 
proc 
fifth international workshop artificial intelligence statistics 
brill advances transformation part speech tagging 
proc 
twelfth national conference artificial intelligence 
pp 
aai press mit press 
chomsky halle sound patterns english 
new york harper row 
collins singer unsupervised models named entity classification 
emnlp 
cussens part speech tagging progol 
proc 
seventh international workshop inductive logic programming 
pp 

van den bosch weijters trees compression classification lazy learning algorithms 
artificial intelligence review 
de saussure course general linguistics 
new york philosophical library edition 
deligne mod eles de equences de variables application au traitement du langage de la parole 
ph thesis enst paris france 
deligne bimbot inference variable length linguistic acoustic units multigrams 
free speech journal 
approche deux en les de la 
traitement automatique des 
goldberg genetic algorithms search optimization machine learning 
addison wesley 
harris phoneme morpheme 
language 

prague czechoslovakia spn 
kaplan kay regular models phonological rule systems 
computational linguistics 
kazakov modul pro em 
master thesis czech technical university prague 
kaz man tex kazakov inductive approach natural language parser design 
somers eds proceedings 
ankara pp 
bilkent university 
kazakov unsupervised learning naive morphology genetic algorithms 
daelemans bosch weijters eds workshop notes ecml workshop empirical learning natural language processing tasks 
prague czech republic pp 

kazakov achievements prospects learning word morphology inductive logic programming 
cussens dzeroski eds proceedings learning language logic workshop lll 
forthcoming 
kazakov natural language applications machine learning 
ph thesis czech technical university prague czech republic 
kazakov manandhar hybrid approach word segmentation 
page ed proc 
eighth international conference inductive logic programming 
madison wisconsin pp 
springer verlag 
knuth art computer programming sorting searching vol 

reading massachusetts addison wesley 
koskenniemi level morphology general computational model word form recognition production 
finland university helsinki dept general linguistics 
lavrac dzeroski inductive logic programming techniques applications 
chichester ellis horwood 
ling learning past tense english verbs symbolic pattern vs connectionist models 
journal artificial intelligence research 
manandhar dzeroski learning multilingual morphology clog 
page ed proc 
eighth international conference inductive logic programming 
madison wisconsin pp 

matthews morphology theory 
cambridge university press edition 
matthews concise oxford dictionary linguistics 
oxford university press 
mikheev automatic rule induction unknown word guessing 
computational linguistics 
mitchell machine learning 
new york mcgraw hill 
mooney califf induction order decision lists results learning past tense english verbs 
journal artificial intelligence research 
muggleton inverse entailment progol 
new generation computing 
muggleton bain analogical prediction 
proc 
ninth international workshop inductive logic programming 
bled slovenia springer verlag 
morphology analogy machine translation 
ph thesis university uk 
quinlan induction decision trees 
machine learning 
riloff jones learning dictionaries information extraction multi level bootstrapping 
aaai 
orlando fl pp 

ritchie russel black pulman computational morphology practical mechanisms english lexicon 
london mit 
kaz man tex rumelhart mcclelland parallel distributed processing vol 
ii chapt 
learning past tense english verbs pp 

cambridge ma mit press 
shannon weaver mathematical theory communication 
urbana university illinois press 
czech lexicon level morphology 
volz eds proceedings second european seminar language applications multilingual europe 
ids mannheim pp 

thompson califf mooney active learning natural language parsing information extraction 
icml 
pp 

paradigmatic cascades linguistically sound model pronunciation analogy 
proceedings th annual meeting association computational linguistic acl 
madrid 
zelle mooney learning semantic grammars constructive inductive logic programming 
proceedings aaai 
pp 
aai press mit press 
kaz man tex kaz man tex 
