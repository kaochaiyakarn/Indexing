chapter optimization approaches semi super learning demiriz kristin bennett department decision sciences engineering systems department mathematical sciences rensselaer polytechnic institute 
troy ny rpi edu examine mathematical models semi supervised support vector machines vm 
training set labeled data working set unlabeled data vm constructs support vector machine training working sets 
vm solve transductive inference problem posed vapnik 
transduction task estimate value classification function points working set 
contrasts inductive inference estimates classification function possible values 
propose general vm model minimizes misclassification error function capacity available data 
depending poorly estimated unlabeled data penalized different mathematical models result 
examine practical algorithms solving model 
approach utilizes vm model norm linear support vector machines converted program mip 
global solution mip integer programming solver 
second approach uses quadratic program 
variations block coordinate descent algorithms find local solutions problem 
mip local learning algorithm produced best results 
experimental study statistical learning methods indicates incorporating working data improve generalization 

focus mathematical programming approaches semisupervised learning classification tasks 
main idea semi supervised learning construct classifier training set labeled data working set unlabeled data 
labels known problem clustering 
labels known applications algorithms complementarity inductive learning problem classification 
practical domains unlabeled data abundant labeled data expensive generate relatively scarce medical diagnosis web search drug design database marketing 
training data consist relatively labeled data points high dimensional space done prevent classification regression function overfitting training data 
key idea exploiting unlabeled data hope able provide additional information problem improve accuracy data unknown labels generalization 
including unlabeled data testing set semi supervised learning perform transductive learning typical inductive learning 
induction task construct discriminant function valid 
function fixed applied test data 
transduction labeled training data unlabeled testing data discriminant function constructed available data 
learning task predict labels specific test data points possible points 
simpler task result theoretically better bounds generalization error reducing amount required labeled data generalization 
semi supervised support vector machine approach illustrated simple example 
consider class problem shown 
labeled training sets linearly separable exists infinite number possible separating planes correctly classify sets 
intuitively best linear classifier middle plane shown separates sets greatest margin 
margin sum distances closest points support vectors set plane equivalently distance supporting planes set 
supporting planes shown dotted lines 
statistical learning theory proves misclassification optimization approaches semi supervised learning transductive learning error maximizing margin separation minimizes bound expected misclassification error unseen data 
maximizing margin reduces capacity function fit data 
intuitively fat plane wide margin capacity fit data skinny 
svm optimal plane quadratic linear programming depending metric measure margin distance 
consider additional unlabeled test data shown 
svm performs poorly particular test set terms classification accuracy testing data 
note resulting margin combined labeled training data unlabeled testing data small 
construct svm margin correctly classifies training data achieves widest margin data results semi supervised svm significantly improved preferable plane shown 
results statistical learning theory show fixed misclassification error maximizing margin data train test lead better bounds expected generalization error 
semi supervised svm consider possible labels test data assign labels produce best svm maximum margin available data labeled unlabeled 
purpose limit discussion linear svm methods extended nonlinear support vector machines standard svm approach including kernel functions 
section 
review support vector machines 
section 
provide general framework viewing semisupervised support vector machine problem 
depending penalize unlabeled data appearing margin problem formulated linear convex quadratic program additional equilibrium constraints constraints nonconvex objective terms 
section 
examine psfrag replacements applications algorithms complementarity induction train induction test transduction psfrag replacements traditional svm versus semi supervised svm practical approaches linear mixed integer program mip formulation introduced 
incorporating mip local learning approach performance greatly enhanced 
section 
examine practical algorithms nonconvex quadratic formulation 
conclude brief summary discussion optimization issues semisupervised learning 
researchers reported favorable results semi supervised methods web text classification problems example em expectation maximization training bayesian networks transductive version svm light :10.1.1.20.9305
ismail propose augmented error components labeled unlabeled data 
provide analytical solution case linear noisy targets linear hypothesis functions 
show results non linear case 
theoretical results exist relative value labeled unlabeled data 
psfrag replacements optimization approaches semi supervised learning 
review svm class class optimal plane maximizes margin underlying problem interest estimate classification function input output training data classes function correctly correctly classify unseen examples generated underlying probability distribution training data 
limit discussion linear classification functions 
points linearly separable exist vector scalar equivalently optimal separating plane furthest closest points classes 
geometrically equivalent maximizing separation margin distance parallel planes see 
margin separation euclidean distance norm 
maximize margin minimize subject constraints 
structural risk minimization fixed empirical misclassification rate larger margins lead better generalization prevent overfitting high dimensional attribute spaces 
classifier called support vector machine solution depends points called support vectors located supporting planes general classes linearly separable generalized optimal plane problem 
slack term added applications algorithms complementarity point point misclassified formulation svm qp quadratic programming fixed penalty parameter 
capacity control provided margin maximization greatly improve generalization 
typically dual form solved practice robust linear programming approach svm identical svm qp margin term changed norm norm problem robust linear program svm rlp rlp formulation useful variation svm nice characteristics 
norm weight reduction provides capacity control 
results show minimizing corresponds maximizing separation margin infinity norm 
statistical learning theory potentially extended incorporate alternative norms 
major benefit svm rlp svm qp dimensionality reduction 
svm rlp svm qp minimize magnitude weights rlp forces weights due properties norm 
results dimensionality reduction variables weights removed model 
benefit svm rlp svm qp solved linear programming quadratic programming 
svm easily generalized nonlinear discriminants kernel functions 
basic idea data mapped optimization approaches semi supervised learning nonlinearly higher dimensional space linear svm constructed transformed space corresponding nonlinear classifier original space 
limit formulation linear classification problem leave computational studies approaches extended kernels 

semi supervised svm basic idea semi supervised support vector machines want best support vector machine labeled data unlabeled points margin 
want penalize support vector machine unlabeled points fall margin 
specifically define semi supervised support vector machine problem vm fixed misclassification penalty parameter margin penalty function unlabeled data 
question define hard margin approach unlabeled points allowed margin margin penalty function defined unlabeled point falls outside margin considered classified penalty incurred 
transform hard margin problem linear quadratic program additional equilibrium constraint 
start svm formulation add constraints point working set 
constraint calculates misclassification error point class constraint calculates misclassification error point class add constraint forces misclassification errors point zero 
produces mathematical programming problem equilibrium constraints applications algorithms complementarity margin penalty functions requirement unlabeled points may fall margin may strong 
natural relaxation problem move equilibrium constraint objective margin penalty function results nonconvex quadratic optimization problem close examination choice error function shows attractive properties 
unlabeled point falls outside margin error associated point 
point falls margin support vector machine 
piecewise quadratic margin penalty function produced see construction natural choice margin penalty function calculates minimum possible misclassification errors 
final class point corresponds results smallest error 
transductive idea proposed vapnik 
advantage correct labels resulting svm identical produced optimization approaches semi supervised learning points known 
minimum error formulation resulting margin penalty function shown experimental study practical methods solving problems focused minimum error formulation 
say formulations possible preferable 
sections explore different approaches practically solving problem 

mixed integer programming formulation integer programming exactly solve vm 
basic idea add decision variable point working set 
variable indicates class point 
point class point class results mixed integer program vm mip constant chosen sufficiently large feasible optimal likewise norm objective 
globally optimal solution problem cplex commercial mixed integer programming codes provided computer resources sufficient problem size 
mathematical programming modeling language ampl able express problem approximately lines code plus data file solve cplex 
practical limitation approach capacity mip solver 
cplex sun ultra mb ram practical include unlabeled data points due cpu time limitation 
applications algorithms complementarity local semi supervised support vector machines get practical restriction number integer variables unlabeled data handled mip solver utilized vm mip part local learning algorithm 
local learning point classified points neighborhood 
example nearest neighbor algorithm nn nearest neighbors point euclidean distance metric point assigned majority class nearest neighbors 
local learning methods called memorybased methods training examples kept memory classify new points 
local models fewer training examples takes computational time optimize local vm train global expense local models 
previous empirical studies shown generalization ability local methods exceeds global ones local models include points related query point interested unlabeled data learning task 
variations exist selecting neighborhoods determining output class neighbor 
example discriminant adaptive nearest neighbor uses local discriminant analysis estimate class nn classification 
lawrence local neural network models function approximation 
see survey approaches 
local vm experimental results local vm application vm local neighborhood unlabeled point determined nn algorithm euclidean distance 
neighborhood includes labeled unlabeled examples 
order labeled examples neighborhood arbitrarily pick available data points 
study needed best select neighborhood point 
summarize method local vm classify unlabeled point steps 
find nn unlabeled point 

labeled points neighborhood class label unlabeled point class 
continue 

solve vm mip neighborhood 

label point result vm advantages local vm single global vm 
transduction data need construct new model 
fact local vm compute new model point optimization approaches semi supervised learning table dataset summary statistics data set dim points test size bright cancer diagnostic dim heart housing ionosphere musk sonar pima true transductive algorithm 
models unlabeled points solve local vm computational time algorithm including time find local neighborhood generally global vm algorithm 
fewer unlabeled points local model means fewer binary variables model 
having fewer binary variables results running time local model 
advantage classification function local vm nonlinear piecewise linear exact linear vm locally 
determining nearest neighbors point problematic large datasets 
consider appropriate metric method find nn 
datasets relatively small dimensions euclidean distance combined partial sort algorithm find local neighborhood 
mentioned outlines algorithm unlabeled point related data file created vm model solved ampl 
output ampl analyzed find label point 
computational study vm consisted trials realworld data sets described table bright dim galaxy sets basic properties datasets summarized table 
dataset sampled randomly times working set composed data bright dim pima datasets size working set set points rest data training set 
formula pick penalty parameter continuous response variable housing dataset categorized applications algorithms complementarity table average error results inductive transductive svm methods data set svm rlp local svm local nn bright cancer diagnostic dim heart housing ionosphere musk sonar pima size training set size working set 
average working set errors reported table 
best result different models underlined dataset 
columns table provide comparison inductive linear norm support vector machine svm rlp transductive linear norm svm optimized mixed integer programming vm mip 
datasets transductive vm mip results slightly better significantly different inductive results svm rlp 
note parameters formulations identical difference formulations unlabeled data transductive case 
formulation unlabeled data help hurt generalization 
columns table compare inductive version local svm transductive version local vm study neighborhoods points local svm local vm identical 
testing set point optimization problem solved local vm identical solved local svm terms involving unlabeled data removed 
done ensure unlabeled data change experiment 
fact means unlabeled data determine effective size neighborhood local svm form transduction 
column table gives results nearest neighbor algorithm 
done examine improvements occur simply switching local algorithm 
local vm outperformed local svm datasets supporting transductive hypothesis 
improvements simply attributed optimization approaches semi supervised learning local learning strategy nn worse local svm vm datasets 
local vm consistently best best experiments 
vm local vm obtained best results datasets dim housing datasets 
results indicate labeled unlabeled points transduction model improve accuracy 
local vm resulted better accuracy vm datasets 
noteworthy point cases sonar musk housing bright local vm improved accuracy notably 
cancer diagnostic heart ionosphere fact vm performed best indicates neighborhood local vm increased local vm perform better 
best method choosing neighborhoods local methods open question 
proposed algorithm section takes consideration unlabeled point time 
unlabeled points neighborhood algorithm returns results test point interest 
results points basically discarded 
extension keeping results final vote algorithm 
case assign probability class membership certain point 
results point starting points improve solution time local vm nearby points 

nonconvex quadratic approach alternative approach solving minimum error vm problem convert nonconvex quadratic program 
adapt approach previously handle classification labels bilinear separability global tree optimization problems 
decision variable introduced point optimality predicted class predicted class 
resulting problem vm qp intuitively simple approach adapt block coordinate descent algorithm alternates fixing estimating svm weights dependent variables optimizing svm variables fixed 
shown class problems includes vm qp norm approach converge applications algorithms complementarity finite number iterations solution satisfying minimum principal necessary optimality conditions 
linesearch required 
proof require subproblem solved optimality condition relaxed require strict decrease objective function 
global tree optimization problem block coordinate descent algorithm prone local minima tabu search method 
applied transduction simple algorithm prone local minima report results 
improve results developed heuristic variation block coordinate descent algorithm 
introduce algorithm section 
descent algorithm transductive svm essential idea heuristic approach start heavily penalizing solutions points falling margin relax requirement order find solutions wider margin 
just basic block coordinate descent method estimate labels current estimate svm solve vm qp fixed 
note practice easy nonlinearity kernels solve dual problem fixed reduces usual dual svm problem eq 
tailored transduction local minimum reached 
weight misclassification error decreased allowing wider margins 
order escape local minima algorithm switches labels unlabeled data close separating hyperplane necessary 
purpose check consecutive solutions process repeated track local minima 
consecutive solutions assign opposite labels points satisfying local minima points classified class 
case algorithm restarts initial conditions reduced margin penalty parameter unlabeled data 
empirically picked performed cases 
ensure starting solution initial label assignments optimization approaches semi supervised learning closest class center unlabeled point 
resulting algorithm summarized follows algorithm vm find class centers training points assign labels initialization working set closest class center 
fix solve problem dual find 

fix 
check convergence criteria 
solve problem solution previous counter counter exists point margin assign opposite labels points satisfying solution class reassign initial conditions benchmark transductive svm report results svm light proposed joachims :10.1.1.20.9305
transductive svm light viewed block coordinate descent algorithm alternates estimating class labels optimizing svm labels 
transductive svm light inner outer loop 
outer loop adjusts penalty parameters misclassification errors 
different errors unlabeled data estimated class labels 
initial inductive iteration algorithm starts low penalty terms unlabeled data 
penalty terms fying unlabeled point class class object respectively 
uniformly increases influence unlabeled data user defined penalty level 
phase algorithm tunes penalty terms way satisfy user defined bias data 
inner loop optimizes svm penalties 
inner loop switches labels points action reduces error 
vm svm light alternates transductive svm light classi applications algorithms complementarity table average error results transductive inductive methods data set svm qp svm light vm heart housing ionosphere sonar labels avoid local minima 
primary difference svm light changes signs points time 
difference svm light uses different margin penalty parameters class class objects 
addition vm qp starts lower values margin penalty parameters 
details svm light successful results large datasets :10.1.1.20.9305
default parameter options experiments svm light 
vm results section compare vm svm qp eq 
transductive svm light 
datasets previous section 
due long computational times vm transductive svm light limit experiments heart housing ionosphere sonar datasets 
linear kernel functions methods section 
results table show unlabeled data case datasets heart ionosphere affects generalization ability slightly difference best transductive result svm qp eq 
statistically significant 
cases housing sonar best transductive method outperforms svm qp significantly 
datasets vm performs significantly better transductive svm light case housing difference methods statistically significant 
indicated results vm svm light inconclusive 
algorithms expensive inductive versions 
results mixed integer programming approaches know transduction improve learning 
speculate reason improvements vm svm light optimization problem difficult methods failing find global minima 
know prior experiments little room improvement specific learning tasks 
local minima lead better generalization 
vm mip local version finding globally optimal solutions better 
results svm optimization approaches semi supervised learning light reported know larger problems text categorization transductive inference svm light lead significant improvements :10.1.1.20.9305
different learning tasks vm may perform better 
speculate problems local minima improve generalization essential global minimum 
studies needed identify methods find globally solutions sufficient 
note nonlinear kernels result better generalization 

examined mathematical models semi supervised support vector machines vm 
proposed general vm model minimizes misclassification error function capacity available data 
different functions penalizing unlabeled points falling margin discussed 
computational investigation focused minimum error formulation transductive inference problem 
converted problem mixed integer program exactly solved commercial integer programming packages 
mip formulation local learning algorithm powerful scalable transductive inference method created 
computational experiments local learning method effective 
studies needed determine best select neighborhoods choose parameters local vm mip 
addition efficient computational methods local vm mip needed 
possibility estimated labels models point starting point points 
examined quadratic optimization approach vm 
computational studies conclusive approach 
best optimization approach solving problem open question 
acknowledgments partially supported nsf iri nsf iis nsf dms 
referees scott helpful comments 
atkeson moore schaal 
locally weighted learning 
artificial intelligence review 
bennett 
global tree non greedy decision tree algorithm 
computing science statistics 
bennett 
combining support vector mathematical programming methods classification 
sch lkopf burges smola applications algorithms complementarity editors advances kernel methods support vector machines pages cambridge ma 
mit press 
bennett bredensteiner 
geometry learning 
web manuscript rensselaer polytechnic institute www rpi edu geometry ps 
accepted publication geometry editors maa press 
bennett demiriz 
semi supervised support vector machines 
cohn kearns solla editor advances neural information processing systems pages cambridge ma 
mit press 
bennett mangasarian 
robust linear programming discrimination linearly inseparable sets 
optimization methods software 
bennett mangasarian 
bilinear separation space 
computational optimization applications 

nonlinear programming 
scientific cambridge ma 
blue 
hybrid tabu search local descent algorithms applications artificial intelligence 
phd thesis rensselaer polytechnic institute troy ny 
blum mitchell 
combining labeled unlabeled data training 
proceedings conference computational learning theory madison wi 
acm bredensteiner bennett 
feature minimization decision trees 
computational optimization applications 
castelli cover 
exponential value labeled samples 
pattern recognition letters 
ismail 
incorporating test inputs learning 
proceedings advances neural information processing systems cambridge ma 
mit press 
cortes vapnik 
support vector networks 
machine learning 
cplex optimization incorporated incline village nevada 
cplex callable library 
gay kernighan 
ampl modeling language mathematical programming 
boyd ma 
hastie tibshirani 
discriminant adaptive nearest neighbor classification 
ieee pami 
optimization approaches semi supervised learning joachims 
text categorization support vector machines learning relevant features 
european conference machine learning ecml 
joachims :10.1.1.20.9305
transductive inference text classification support vector machines 
international conference machine learning 
lawrence tsoi back 
function approximation neural networks local methods bias variance smoothness 
peter bartlett anthony robert williamson editors australian conference neural networks pages 
australian national university 
mangasarian 
arbitrary norm separating plane 
operations research letters 
mangasarian 
generalized support vector machines 
smola bartlett sch lkopf schuurmans editors advances large margin classifiers pages cambridge ma 
mit press 
ftp ftp cs wisc edu math prog tech reports ps 
mccallum nigam 
employing em pool active learning text classification 
proceedings th international conference machine learning icml 
murphy aha 
uci repository machine learning databases 
department information computer science university california irvine california 
musser 
stl tutorial guide programming standard template library 
addison wesley 
nigam mccallum thrun mitchell 
learning classify text labeled unlabeled documents 
proceedings th national conference artificial intelligence aaai 
pennington humphreys 
automated star galaxy discrimination neural networks 
astronomical journal 
vapnik 
estimation dependencies empirical data 
springer new york 
english translation russian version 
vapnik 
nature statistical learning theory 
springer verlag new york 
vapnik 
statistical learning theory 
wiley inter science 
vapnik ja 
chervonenkis 
theory pattern recognition 
nauka moscow 
russian 
