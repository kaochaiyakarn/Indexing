authorship attribution support vector machines joachim diederich jorg kindermann leopold gerhard gmd forschungszentrum sankt augustin explore text mining methods identification author text 
time apply support vector machine svm problem 
able cope half inputs requires feature selection process frequency vector words text 
performed number experiments texts german newspaper 
nearly perfect reliability svm able reject authors detected target author cases 
second experiment ignored nouns verbs adjectives replaced grammatical tags bigrams 
resulted slightly reduced performance 
author detection svm full word forms remarkably robust author wrote different topics 
economic intellectual activities world wide web medium scrutiny authenticity document relevant 
example internet plagiarism 
article discuss advanced statistical text mining methods authorship attribution 
authorship attribution considered categorization problem 
contrast classification tasks clear features text classify author 
decades text attributes proposed area information retrieval careful tuning able solve cases disputed authorship 
apply support vector machine svm authorship attribution time 
currently classification approaches neural networks decision trees allows process hundreds thousands features 
offers opportunity words text inputs carefully selected characteristic words 
similar text classification problems aiming thematic categorization svm turned quite effective joa 
svm able classify text respect content 
framework author attribution clear specific topic addressed author structural stylistic features authors language lead successful classification 
achieve content invariance investigated content free summary text 
took counts grammatical tags combined bigrams capture details language patterns 
section give overview linguistic features analysis author style 
section discusses different statistical methods authorship assessment 
starts statistical hypothesis tests includes semi parametric approaches support vector machine 
preprocessing step text classification raw frequency vectors transformed normalized discussed fourth section 
fifth section describes numerical experiments authorship attribution corpus german newspaper 
section summarizes results 
features stylometry statistical analysis style stylometry assumption author style certain features inaccessible conscious manipulation 
considered provide reliable basis identification author 
style author may variable differences topics genre personal development author time 
may changed explicit imitation literary styles 
ideally stylometry identify features invariant effects expressive discriminate author writers 
early stylometric studies introduced idea counting features text applied word lengths sentence lengths men 
yule reported wider variation sentence lengths word lengths 
differences sentence length distributions author depending time genre text gan 
parallel differences word length distributions prose verse author 
features counts words vowel counts words specific lengths hol 
powerful criterion stylometry richness diversity author vocabulary 
zipf observed number words ff exactly times ff fl fl zip 
conjectured parameter fl depends age intelligence author zip 
sic able fit successfully family compound poisson distributions word frequencies number authors works different languages 
order remove dependency vocabulary size text length alternate features proposed 
range simple type token ratio complex measures zipf size orl 
interesting feature comparison number words occur exactly times training data number words occur exactly times new text thi 
efron estimated size shakespeare vocabulary asking new words shakespeare write play 
studies distinct differences vocabulary size different authors differences texts author gan 
features limited value authorship attribution 
clear collection different features vocabulary size different word fields knowledge specific words larger discriminatory power 
obviously word usage highly depends topic text 
discrimination purposes need function words 
seminal mw counted words discriminate possible authors 
burrows bur developed idea sets common high frequency words conducted version principal component analysis data 
technique successfully applied classical federalist papers problem promises large gains computing power available 
smith bs frequency occurrence prepositions distinguish oscar wilde plays essays 
word counts directly employ features derived words 
example syntactic class words bvt 
compared syntax word easily influenced choices conscious control author 
discourse structure texts author corresponding vocabulary quite different syntax features reliable purpose authorship attribution 
cha ff discusses techniques enhancing statistical language processing syntactic information 
rud approximately style markers isolated 
clearly agreement significant style markers 
text categorization nearly words contain information 
joachims joa ranked word stems large corpus information gain respect classification 
turned model features ranks performed nearly best features top similar feature set 
features ranked lowest contain considerable information somewhat relevant 
section discuss statistical techniques authorship attribution 
conventional techniques rely carefully selected features newly developed approaches allow thousands inputs features alleviate need careful selection 
statistical techniques authorship attribution statistical approaches authorship attribution start assumption composition texts produced author characterized probability distribution 
considering population authors attribution text author considered statistical hypothesis test classification problem 
usually requires distinct steps feature selection identification possible features discrimination texts discussed 
model selection selection suitable distributions models describing feature values 
learning estimation free parameters different authors available data 
classification selection potential author new text 
sections discuss aspects detail 
hypothesis test hypothesis tests assume probability distributions known type different authors 
words different types text counted natural distribution multinomial distribution 
vector word frequencies texts corresponding relative frequency vector true probability vector 
statistics may goodness fit test counts bfh pearson chi square gamma log likelihood ratio statistic nq log nq freeman tukey goodness fit statistic ft nq gamma generated null hypothesis statistics asymptotic chi square distribution gamma degrees freedom 
distribution mean value gamma variance gamma 
means values larger gamma gamma indicate significant deviation 
sample versions statistics compare distribution independent samples 
application tests authorship attribution hampered factors approximation underlying test valid expected frequency larger usually hold high fraction words text 
second distribution produced author changes genre topic contradicts assumption texts follow multinomial distribution 
number tests developed checking different distributional features 
starting characteristic th sentence author text length cusum test goe detects significant mean value establishes bounds test quantity gamma valid specific assumptions independence terms gamma 
bounds allow nice graphical representation 
number court cases received significant public attention far 
number independent investigations method unreliable hol stability characteristics multiple texts warranted 
test developed efron thi analyses diversity author vocabulary 
generate sets words occur exactly times training corpus 
subsequently determine number words corpus sample text propose various significance test assumption word selection author poisson process 
tests results mixed 
val applied tests works shakespeare marlowe consistency shakespeare plays poor consistency shakespeare poems plays marlowe plays 
semi parametric models classification advent powerful computers initiated development machine learning techniques larger flexibility 
regression models yc naive bayesian models mn text classification structural limitations number versatile procedures applied text categorization inductive rule learning mrg bayesian probabilistic networks th multilayer perceptrons radial basis function networks lm decision trees nearest neighbor classification lh support vector machines joa 
models universal approximators able approximate functional relation arbitrarily 
model underlying distribution potentially infinite number parameters selected way predictive performance optimal 
approach strength weaknesses corresponds representational bias ability represent specific structures economical way 
naive bayesian probabilistic classifiers joint probabilities words text categories estimate probability categories text 
naive assumption occurrence word conditionally independent words category known 
resulting algorithm efficient 
extended mixture multinomials successfully applied text categorization mn 
multi layer perceptrons tweedie attribute authorship disputed federalist papers 
normalized frequency eleven common function words text input network 
neural network hidden output nodes 
trained conjugate gradient tested fold cross validation 
network unambiguously classified disputed federalist papers madison consistent results authors methods 
problem neural networks text classification high computational effort training 
perceptrons semi parametric models pose problem user offer little insight process arrived result general totality knowledge embedded 
number techniques explain networks developed adt 
radial basis function rbf networks start number prototype feature vectors class assume feature vector new exemplar close prototype class 
distance prototype measured common euclidean distance generalized version weighting specific features 
rbf networks model style author directly mixture different styles may depend topic genre 
especially suited stylometric analysis 
prior knowledge initialize weight vectors 
important relatively small data sets situation easily arise authorship attribution 
rbf networks lm stylometric analysis 
frequency function words features normalized zero mean unit variance 
discriminate plays shakespeare fletcher total samples author 
trained rbf networks produced classifications agreement conventional scholarship application computational methods multi layer perceptrons 
nearest neighbor classification knn similar rbf networks uses distance prototypes criterion 
constructing synthetic prototypes instances training set employed 
new test document algorithm finds nearest neighbors training documents 
resulting classification weighted majority vote categories neighbors 
details voting mechanism depends specific procedure lh 
top performing methods benchmark reuters corpus 
class class 
hyperplane hyperplane maximal margin generated linear svm decision trees sequentially partition input space single dimension 
corresponding variable selected step lookahead greedy search heuristic measure classification quality qui 
splits training set parts procedure starts 
size resulting tree limited cross validation 
boosting version applied text categorization results 
support vector machines support vector machines svms gained popularity learning community vap 
simplest linear form svm hyperplane separates set positive examples set negative examples maximum distance margin 
shows hyperplane associated margin 
formula output linear svm normal vector hyperplane input vector 
margin defined distance hyperplane nearest positive negative examples 
maximizing margin expressed optimization problem minimize kwk subject th training example gamma correct output svm th training example 
note hyperplane determined training instances margin support vectors 
course problems linearly separable 
cortes vapnik cv proposed modification optimization formulation allows penalizes examples fall wrong side decision boundary 
support vector machines structural risk minimization principle vap computational learning theory 
idea find model guarantee lowest true error 
limits probability model error unseen randomly selected test example 
svm finds model minimizes approximately bound true error controlling model complexity vc dimension 
avoids fitting main problem semi parametric models 
svm extended nonlinear models mapping input space high dimensional feature space chosen priori 
space optimal separating hyperplane constructed vap 
phi mapping phi dim 
phi linear classification parameter may learned 
note algorithm uses dot products 
high dimensional values calculated dot products phi phi required determined input space 
examples common kernels polynomial xy sigmoid tanh xy theta rbf exp gammafl kx gamma yk new vector classified class decision function value sgn ff ff corresponding support vectors training set examples margin different 
rewrite sgn select fl ffi distributions generating data inequality holds probability gamma ffi training patterns test error fl log log ffi kxk kwk term fraction training samples margin wx fl large constant 
inputs comparable length kxk 
training svm requires solution quadratic programming qp problem qp optimization method learn ff basis training examples 
qp methods slow large problems 
method implemented joachims joa especially suitable text classification uses sparse representation inputs 
weights learned new items classified computing defined 
distinctive advantage svm text categorization ability process different inputs 
opens opportunity words text directly features 
word number times occurrence recorded 
typically corpus contains different words text covering small fraction 
joachims joa svm classification text different topic categories 
features uses word stems 
establish statistically significant features requires feature occurs times text 
empirical evaluation done test collections reuter news agency data set covering different topics ohsumed corpus william hersh describing diseases 
features case svm versions polynomial rbf performed substantially better currently best performing conventional methods naive bayes rocchio decision trees nearest neighbor 
joa transductive svm text categorization able exploit information unlabeled training data 
dumais linear svms text categorization accurate fast 
times faster train accurate decision tree tested classifiers 
applied svms reuter collection emails web pages 
drucker classify emails spam non spam 
find boosting trees svms similar performance terms accuracy speed 
svms train significantly faster 
transformations frequency vectors normalization length known fact frequency distribution words large texts fairly skewed 
zipf law original version zip frequency term rank text positive parameters 
seen distribution frequencies terms texts extremely uneven 
units occur rule thumb half terms called hapax occur 
unfortunately especially seldom units contain highly specific information content text 
formula generalized various authors summary see cb 
zipf explained equation principle effort 
generalization zipf law fl gamma known zipf mandelbrot law man 
contains equation special case 
order compare documents different length term frequency vectors normalized standard length 
standpoint performance svm best normalization rule kd kl kxk lp jx norm delta kl denotes euclidean norm 
commonly transformation term frequencies svm applied text classification 
examine effect spontaneous emergence new lexical items 
stated number types jv text grows text length jn formula zipf mandelbrot law deduce enlarging text frequencies lexical units increases factor 
increase delta positive constant smaller 
quantity calculated follows rank frequency distribution text length jn measured running words vocabulary size jv text contains ae holds corresponding vocabularies ae delta rank frequency distribution means texts structure 
conditions jn jv jn jv jv jv jv delta jv jv effect increasing vocabulary norm term frequency vector calculated follows kn jv jv jv inserting zipf mandelbrot law equation yields kn kn jv jv fl fl sum denominator converges vocabulary size tends infinity 
diverges fl 
material term frequency spectrum exhibits fl norm norm equally justified 
empirical tests show normalization respect yields better results compared larger fl 
particular metric equivalent incorporating prior knowledge solution problem 
increasing weight larger values feature space 
suggests favor text classification tasks 
hand known svm best input vectors normalized unit length respect euclidean norm 
considered normalization rules 
ffl normalization unit length respect ffl normalization unit length respect transformation frequencies lexical units scale different orders magnitude larger documents interesting examine term frequency information mapped quantities efficiently processed svm 
empirical tests different transformations type frequencies raw frequencies logarithmic frequencies 
simplest frequency transformation term frequencies type document frequencies multiplied importance weights described normalized unit length 
raw frequencies importance weight idf see section normalized respect norm joa 
tested combinations example raw frequencies importance weight normalized respect norm defined rel number types document second transformation logarithm 
consider transform common approach quantitative linguistics consider logarithms linguistic quantities quantities 
normalize unit length respect define log log combine log different importance weights normalize resulting vector respect importance weights importance weights order reduce dimensionality learning task 
feature extraction text retrieval thought terms reduction dimensionality input space 
common importance weights inverse document frequency idf originally designed identifying index words sm 
svm capable manage large number dimensions 
reduction dimensionality necessary importance weights quantify important specific type number percent average topic tokens types texts text length politics economy local affairs table summary statistics berliner corpus documents length 
documents text collection 
type evenly distributed document collection low importance weight judged specific documents occurs type documents 
importance weight type multiplied transformed frequency occurrence 
different importance weights combined frequency transformations described 
examined performance combinations ffl importance weight ffl inverse document frequency idf ffl redundancy inverse document frequency idf commonly svm applied text classification 
idf defined idf log number documents collection contain term number documents collection 
intuitively inverse document frequency word low occurs documents highest word occurs document 
idf forms fixed vector importance weights comprising types occurring training set 
idf value types new document occur training data set 
scheme possible process documents open source alleviating restrictions imposed fixed vocabulary training data 
idf advantage easy calculate 
disadvantage disregards frequencies term documents 
term occurs documents occurs times remaining document judged important classification documents highly specific document 
contrast term occurs times documents exception specific document useful training testing classification procedure 
effects mainly high frequency terms collections larger documents 
empirical studies show utilization idf cases lead improved performance sb 
redundancy quantifies skewness probability distribution 
consider empirical distribution type different documents define importance weight mmax gamma log nd log frequency occurrence term document frequency occurrence term training set number documents training set 
forms fixed vector importance weights comprising types occurring training set 
define value types occur training data 
measure distribution term various documents deviates uniform distribution 
rank author number documents rank order number documents written authors 
rank type number occurences type rank order frequency different types words corpus 
numerical experiments consider task decide text previously unknown belongs author 
plausible scenario registered external student delivers written exam teacher wants know text written student 
specific setup reflected loss function specifies loss incurs decision selected true class 
loss mis classification extraneous author target author set severe identify target author 
case may recognize correct author additional investigation case faked exam passes unnoticed 
data experiments texts berliner bz daily newspaper berlin 
issue downloaded www de wissen berliner 
material dec feb 
articles subdivided twelve topics 
studied politics economy local affairs 
table shows number words sub categories 
percentage words occurring types large 
reduce computational effort utilized texts words target authors words authors 
training corpus consists documents mio 
running words tokens different words types including word forms 
shows number documents written authors logarithmic scale 
authors contributed single document 
little information available alternative authors discrimination task difficult 
exploit training set documents better way cross testing procedure 
randomly divided subsets nearly equal size 
different svms determined training set size test set size 
numbers correctly wrongly classified documents added yielding effective test set documents 
authors target author percent author transformation kernel ok false ok false loss prec 
recall word forms linear cubic rbf linear fuchs linear rbf fl redundancy rel linear neumann rel rbf fl neumann rel 
linear cubic rbf linear cubic rbf bigrams word lengths linear fuchs linear linear linear neumann linear rel rbf fl rel 
table best result author terms loss svm word forms combined bigrams word lengths cross testing 
discussed assume cost assigning extraneous text target author times worse identifying genuine text 
tar tar respectively denote number correctly incorrectly classified documents target author oth oth figures authors alternative class 
corpus texts originate target author 
assume new texts classified arrive ratio prior probability tar 
loss classification test set tar tar tar tar gamma tar oth oth oth allow comparisons report loss new classifications 
addition precision prc tar tar oth prc tar recall rec tar tar tar describe result experiment 
precision probability document predicted genuine truly belongs class 
recall probability genuine document classified class 
tradeoff exists large recall large precision 
adjusting parameter recall may increased cost decreasing precision vice versa 
derived standard measure retrieval performance prc rec prc rec proposed van 
precision recall reflect actual loss function main criterion 
transformation relative frequencies rel logarithm relative frequencies 
rel weighted inverse document frequencies called tfidf sections 
transformations rel alternatively weighted redundancy weight 
full forms transformation full forms transformation kernel rel rel tfidf rel rel tfidf 



linear 
cubic rbf fl rbf fl rbf fl table loss full word forms experiment different transformations svm kernels averaged authors 
experiments word forms experiment selected authors documents combined areas politics local affairs december february 
number texts authors range 
set runs counts lower case word forms document inputs svm 
experiments program svm light written thorsten joachims joa available www ai informatik uni dortmund de forschung verfahren svmlight svmlight eng html 
investigated kernels linear quadratic cubic rbf different widths fl 
training set computed svm models number combinations kernels transformations 
transformations relative frequency rel relative frequency redundancy factor rel logarithmic relative frequency logarithmic relative frequency redundancy factor inverse document frequency tfidf conjunction linear quadratic cubic rbf kernel fl 
furthermore resulting frequency vectors normalized norm 
combinations author 
svm models estimated combinations training sets 
table shows details best result author terms loss 
precision recall values displayed 
multiple entries author cases combinations frequency transformations svm kernel functions showed identical performance respect loss 
impressive nearly documents authors attributed target author 
yields precision 
hand number cases target author classified correctly 
recall ranges 
author result better 
document assigned target author pretty sure allocation correct opposite case check result means 
situation corresponds intended application 
table shows averaged loss authors experiment full word forms 
turns frequency transformations choice svm kernel function little effect performance terms loss 
apart observation logarithmic relative frequencies normalization best performance 
rbf kernel sensitive choice fl parameter cases 
considered adequate 
bigrams tags function words second set inputs aimed content information structural data 
corpus lemmatizer 
designed freely available web 
lemmatization produced different categories 
example shown table 
excluded nouns sub verbs ver adjectives word categories 
types taken function words little content 
tags comprised auxiliary verbs articles lemmas word forms grammatical tags tags function words der der art def nom sin mas der art def nom sin mas sub nom sin mas sub nom sin mas vor vor prp dat vor prp dat der der art def dat sin fem der art def dat sin fem def dat sin fem def dat sin fem sub dat sin fem sub dat sin fem haben hat ver sin haben ver sin sich sich ref sin sich ref sin adv adv adv adv um um prp um prp zan zan kilometer kilometer sub plu mas sub plu mas ern ert ver pa ver pa table example tagged word forms tagged function words 
conjunctions punctuation marks adverbs prepositions 
ignoring function words appearing times corpus contained total function word types comprising tokens 
remaining types non function words 
combined function words tags yielding total types function words play syntactic roles 
result shown column table 
note taggers usually commit errors 
errors considered part preprocessing process 
remainder section call combined tags function words 
input vector svm comprised sub vectors ffl frequency words different lengths 
words longer letters combined category 
ffl frequency 
ffl frequency bigrams 
document borders marked special delimiter 
maximal number possible bigrams occurred corpus 
comprehensive vector counts transformations applied discussed 
obviously resulting input vector sparse 
result cross testing shown table 
time restricted range frequency normalized ones 
respect precision results outperform full word forms 
case extraneous text wrongly assigned target author 
performance nearly optimal 
respect recall bigrams reliable univariate word forms 
average recall percentage points worse word forms 
bigrams yield high information content exploited svm authorship attribution 
table shows average loss combination transformation kernel authors 
cases linear kernel logarithmic relative frequencies minimal loss 
quadratic kernel performs nearly cubic rbf kernel adequate 
simple relative frequencies worse logarithmic version 
summary bigrams surprisingly account content information dropped 
bigrams transformation kernel rel tfidf linear quadratic cubic rbf fl rbf fl rbf fl table loss experiment different transformations svm kernels averaged authors 
authors target author percent author transf 
kernel ok false ok false loss prec 
recall word forms emmerich linear rel linear neumann cubic richter tfidf cubic rel cubic word length bigrams emmerich tfidf cubic tfidf linear rho neumann tfidf linear rho richter 
tfidf cubic table best result author terms loss svm word forms bigrams trigrams trained local affairs tested politics economy 
testing different topic function words selected minimize content related information 
perform author changes topic 
tested training svm data local affairs topic testing data economy politics 
selected target authors simultaneously contributed areas texts training set 
naturally number test cases far previous experiment 
normalized frequency transformations tested 
results shown table 
configurations authors number misclassifications extraneous texts zero 
precision cases 
respect recall word forms perform best 
values range contrast bigrams 
experimented trigrams worse bigrams 
explanation high errors may bigrams occur text 
intelligent aggregation procedure required retains discriminating information increases statistical significance 
note results give impression number test cases low 
comparison classifiers performed preliminary experiments alternative classification procedures decision trees multilayer perceptrons 
analysis section trained texts dec february 
author fuchs single training set size positive examples test set size positive examples 
impossible frequencies word forms inputs 
aggregated category authors target author percent method transformation ok false ok false loss precision recall mlp tree svm table results multilayer perceptron decision tree svm tagged aggregated function words 
contained tokens training set 
produced types inputs 
tested frequency transformations earlier including redundancy factors 
best results shown table 
result mlp depends starting values weights show average runs 
decision trees worse mlps quite catch svm 
results compared svms full number input features shown table 
author fuchs find section table loss value precision recall 
preliminary experiments indicate svms superior author identification willing code large number features 
discusses prior author identification introduces support vector machines new approach 
especially suited task feature spaces high dimensions features carry important information data specific instances sparse 
experimental results show svms consistently achieve performance identification tasks 
need select specific features may simply take word frequencies 
addition preprocessing weighting features critical approaches lead nearly identical results 
svms authorship attribution text mining process documents significant length databases large number texts 
svm technology firmly grounded computational learning theory training times compare favorably methods neural networks 
svms currently method choice authorship attribution 
second problem investigated question function words sufficient author identification 
turned bigrams function words perform word forms show performance superior classifiers handle reduced set features 
procedure tested texts authors topic performance significantly reduced full forms performing better function word bigrams 
function word bigrams type investigated carry significantly information author full function words 
remains explored research bigrams tags structural features content free characteristics author identification 
adt andrews diederich 
survey critique techniques extracting rules trained artificial neural networks 
knowledge systems 
apte weiss 
text mining decision rules decision trees 
proc 
conf 
automated learning discovery workshop learning text web 
bfh bishop fienberg holland 
discrete multivariate analysis theory practice 
mit press cambridge mass 
boser guyon vapnik 
training algorithm optimal margin classifiers 
haussler editor proc 
th acm workshop computational learning theory pages 
acm press 
bs smith 
bridge statistics literature graphs oscar wilde literary genres 
applied statistics 
bur burrows 
word patterns story shapes statistical analysis narrative style 
literary linguistic computing 
bvt baayen van halteren tweedie 
outside cave shadows syntactic annotation enhance authorship attribution 
literary linguistic computing 
cb baayen 
word frequency distributions 
altmann editors quantitative text analysis pages 
trier 
cha charniak 
statistical language learning 
mit press cambridge ma 
cv cortes vapnik 
support vector networks 
machine learning 
dumais platt heckerman sahami 
inductive learning algorithms representations text categorization 
th international conference information knowledge managment 
drucker wu vapnik 
support vector machines spam categorization 
ieee transactions neural networks 
far jill 
analysing authorship guide cusum technique 
university wales press cardiff 
gan 
literature statistics 
kotz johnson editors encyclopedia statistical sciences volume pages 
wiley 
goe goel 
cumulative sum control charts 
kotz johnson editors encyclopedia statistics volume pages 
wiley 

advanced theory language choice chance 
springer berlin 
hol holmes 
evolution stylometry humanities scholarship 
literary linguistic computing 
joa joachims 
making large scale svm learning practical 
technical report uni dortmund 
joa joachims 
text categorization support vector machines learning relevant features 
nedellec rouveirol editors european conference machine learning ecml 
joa joachims 
transductive inference text classification support vector machines 
int 
conf 
machine learning icml 
lh lam ho 
generalized instance set automatic text categorization 
th ann 
int 
acm sigir conference research development information retrieval sigir pages 
lm lowe matthews 
shakespeare vs fletcher stylometric analysis radial basis functions 
computers humanities 
rapp 
freely available morphological analyzer disambiguator context sensitive lemmatizer german 
proc 
coling acl 
program available psycho uni paderborn de 
man mandelbrot 
word frequencies related markovian models discourse 
proceedings symposia applied mathematics xii 
men 
characteristic curves composition 
science ix 
mn mccallum nigam 
comparison event models naive bayes text classification 
aaai workshop learning text categorization 
mrg moulinier ganascia 
text categorization symbolic approach 
proc 
fifth symp 
document analysis information retrieval 
mw mosteller wallace 
inference disputed authorship federalist 
addisonwesley reading ma 
ng goh low 
feature selection perceptron learning usability case study text categorization 
th ann 
int 
acm sigir conference research development information retrieval sigir pages 
orl 
ein modell der des 
editors studies zipf law pages 
bochum 
qui quinlan 
inferno cautious approach uncertain inference 
computer journal pages 
rud 
state authorship attribution studies problems solutions 
computers humanities 
sb salton buckley 
term weighting approaches automatic text retrieval 
information processing management 
sic 
distribution law word frequencies 
statistical association 
sm salton mcgill 
modern information retrieval 
mcgraw hill new york 
th tzeras hartmann 
automatic indexing bayesian inference networks 
th ann 
int 
acm sigir conference research development information retrieval sigir pages 
thi efron 
shakespeare write newly discovered poem 
biometrika 
tweedie singh holmes 
neural network applications stylometry federalist papers 
computers humanities 
val 
efron authorship tests valid 
computers humanities 
van van rijsbergen 
information retrieval 
butterworths london 
vap vapnik 
statistical learning theory 
wiley new york 
yc yang chute 
example mapping method text categorization retrieval 
acm transaction information systems 
yule 
sentence length statistical characteristic style prose application cases disputed authorship 
biometrika 
zip george zipf 
human behaviour principle effort 
human ecology 
houghton mifflin boston 
zip george zipf 
selected studies principle relative frequency language 
harvard university press cambridge ma 
zip george zipf 
observations possible effects mental age frequency distribution words viewpoint dynamic 
psychology 

