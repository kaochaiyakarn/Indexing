extreme attraction benefits corner attractors david garrison cottrell department computer science engineering university california san diego la jolla ca usa cs ucsd edu fred department computer science university california santa barbara santa barbara ca usa cs ucsb edu april connectionist attractor networks played central role cognitive models involving associative memory soft constraint satisfaction 
early attractor networks step activation functions permitting construction attractors binary bipolar patterns focused networks continuous sigmoidal activation functions 
incorporation sigmoidal processing elements allows expressive real vector representations attractor networks 
empirical studies reported reveal learning performance sigmoidal attractor networks best general real vectors avoided training patterns explicitly placed extreme corners network activation space 
binary bipolar patterns produces benefits number attractors learnable network accuracy learned attractors amount training required 
benefits persist conditions sparse patterns 
furthermore experiments show advantages patterns solely effects large separation training patterns afforded corner attractors 
background computational models cognitive processes typically require encoding knowledge appropriate data structures 
general knowledge best represented highly flexible manner allowing expression subtle nuances 
expressive power sole virtue knowledge representation scheme 
desirable properties knowledge structures odds goal flexibility 
reached number researchers artificial intelligence machine learning 
increasing expressive power language knowledge encoded difficult perform valid inference reasoning levesque brachman hinder learning concepts haussler 
kinds tradeoffs representational power tractability computation appear design artificial neural networks 
exact manner information represented networks way system inputs outputs encoded vectors real numbers profound impact success system 
report focuses representational tradeoff context attractor neural networks 
attractor networks studied extensively played central roles connectionist models cognitive processes hopfield amit 
networks incorporate recurrent connections processing elements allowing current activation state network directly influence activation state 
activation state recurrent network evolves systematically time tracing trajectory space possible activation values 
attractor networks leverage shape trajectories perform computations 
example activation state network enters cyclic trajectory may generate rhythmic repetitive actions 
attractor networks focused trajectories asymptotically approach particular activation states 
points activation space called stable fixed point attractors 
network activation state comes close fixed points natural evolution activity network cause state forever approach fixed point 
region activation space drawn fixed point attractor called basin attraction attractor 
attractor networks may contain fixed point attractors algorithms exist learning attractors examples hopfield rumelhart almeida williams zipser pineda 
wide variety cognitive models connectionist net works kind 
notably attractor networks associative memories hopfield encoding memory trace fixed point activation space network 
initializing state network pattern activity similar stored memory trace results retrieval trace 
networks solve optimization problems soft constraint satisfaction problems hopfield tank rumelhart 
played central roles cognitive models lexical access kawamoto reading aloud plaut visual object recognition mjolsness consciousness mathis mozer 
addition useful functional properties attractor networks exhibit interesting dynamic behavior activation states unfold time 
allows attractor network models predictions time course cognitive acts way non recurrent connectionist models 
early studies attractor networks processing elements step activation functions 
restricted possible activation level unit values 
patterns activity represent things memory traces forced design network binary vectors 
modern connectionist models widely abandoned step activation functions favor continuous logistic sigmoid 
shift processing elements real valued outputs probably driven existence powerful learning algorithms time bptt rumelhart required differentiable activation functions 
continuous outputs opens door network training techniques 
continuous outputs allow continuous target patterns 
patterns represent mental states longer need discrete binary vectors 
reasons patterns real valued elements preferred binary vectors 
real vectors allow representational schemes expressive power 
relative richness real vector representations seen individual patterns relationships patterns 
real vectors elements individual pattern may directly encode meaningful scalar values similarity measures probability levels 
comparison binary elements express presence absence feature 
binary vectors variety possible relationships patterns limited 
example number possible distinct distances patterns finite equal dimensionality vectors 
real vector elements number possible relationships patterns infinite restrictions weak demands metric spaces 
short move binary real pattern elements expressive representations possible 
reason prefer real vector representations involves number fixed point attractors instantiated network 
modeling cognitive capacities memory desirable networks contain attractors 
step activation functions number possible attractors clearly bounded number corners activation space number possible binary vectors dimensionality 
vector dimensionality exactly corners space 
real vectors immediately clear limit number possible fixed points 
turns limited computational power networks certainly restrict number stable fixed point attractors may instantiated real valued attractor network limits appear constraining real valued activation functions 
example theoretical calculations number distinct stable fixed point attractors may embodied attractor network demonstrated advantage continuous activation functions step activation functions factor amit 
attractors needed network model turning real valued patterns reasonable avenue explore 
number cognitive models proposed especially benefit real vector patterns stored attractor network 
example involves modeling lexicon process retrieving representation word meaning representation surface form 
efforts construct real vector representations word semantics word occurrence statistics collected large text corpora lund burgess 
approach encoding word meaning advantage constructing semantic representation empirical data linguistic intuitions researchers 
potential real vectors construction target patterns attractor network model lexicon clouse cottrell 
network model needs maintain distinct fixed point attractor word large vocabulary 
distances pairs word patterns reflect semantic similarity words flexibility general real vector representation desirable 
surprisingly researchers models opted abandon expressive power real vector representation lexical semantics favor binary vector representation 
researchers 
connectionist models involve instantiation real valued fixed points attractor network appear rarely literature 

goal study show general real valued patterns attractor networks 
message report working attractor networks apparent advantages general real vector representations illusory target attractors learned standard learning algorithms 
demonstrate attractor networks usual sort continuous activation function learn construct stable fixed point attractors successfully polarized vector targets general real vectors 
simulation results show patterns placed extreme corners activation space may learned greater number may learned greater accuracy may learned faster patterns non extreme element values 
short corner attractors provide special benefits learnability attractor network models 
unfortunately context attractor networks appears tradeoff expressive power representation learnability target patterns 
important note focuses issue learning stable fixed point attractors standard training techniques 
separate issue network capacity issue attractors may possibly instantiated network generating interference patterns 
extensive done theoretical capacity typical attractor networks little direct investigation capacity limits encountered networks trained standard learning algorithms amit 
study aims show learning large collection attractors target patterns extreme valued elements best 
developing intuition order understand attractor networks learn fail learn different kinds target patterns useful obtain intuitions concerning role connection weights play dynamic behavior network 
consider attractor network consisting collection completely interconnected processing elements employing logistic sigmoid activation function 
activity flows network approaching stable fixed point attractor activation level interesting note theoretical involved binary bipolar target patterns 
network rest bias weighted net input gamma gamma gamma gamma behavior unit part shows schematic single unit attractor network viewed 
plots show current output sigmoidal function previous output gamma 
slope line shown 
note points intersection sigmoid slope line fixed points stable unstable dynamics unit 
plots shown various parameter values value recurrent connection weight sum bias weighted net input unit 
unit approach constant value 
consider processing element depicted point time unit essentially reached constant state 
weight changes affect possible activation states unit 
address question reduce number parameters interest 
value recurrent connection weight sum bias weight weighted net input rest network 
output single processing element time labeled may expressed simply gamma gamma note parameter determines gain sigmoidal relationship gamma value gamma determines sigmoid centered gamma gamma value exactly 
gamma sigmoid centered shown 
gamma stable fixed point attractors exist unit equidistant medial activation value gain decreased attractors merge located middle range shown 
alternatively gain may kept high value gamma may modified shifting location stable attractors longer symmetric shown 
continue change ratio stable attractors disappears leaving 
moral story fixed input rest network single unit stable fixed point attractor states 
finding hand possible show network constant input activity stable fixed point attractors number units network 
means absolute maximum number attractors possibly instantiated attractor network exceed number corners activation space network sigmoidal activation function 
words general real vectors target patterns allow creation attractors binary patterns 
analysis speak issue learning 
corner attractors easier learn patterns middle activation space 
question investigated looking error surface single processing element 
attractor networks trained gradient descent techniques error measure typically squared error rumelhart 
target activity levels single unit squared deviation targets various parameter values may plotted 
plot targets various positive values negative values shown 
valley error surface centered gamma line valley looking steep sided canyon high values point minimal error floor canyon value slightly greater 
see error surface changes targets targets error surfaces single unit error behavior single processing element plotted positive parameter negative parameter value connection weight unit self link parameter sum unit bias weighted input receives outside 
error shown sum squared deviation target value location stable fixed point attractor basin encompasses target 
plot shows error surface targets 
plot involves extreme targets 
target patterns placed closer corners activation space examining 
plot displays squared error target activation levels 
extreme targets cause canyon wider importantly move point minimum error larger values clearly depicted top row graphs show cross sections error surfaces various target values gamma line 
note point minimal error appears higher values targets approach extreme values zero region relatively low error minimum grows longer 
general appears region parameter space corresponds relatively low error larger targets near corners activation space 
area may seen bottom row graphs 
graphs display slices error surfaces cross sections parallel axis error surface cross sections single unit various target values graphs show dimensional slices error surface plots 
ordinate axis graph labeled sum squared error values 
column displays target values compute sum squared error cross section error surface gamma line abscissa labeled values cross section error surface line value point minimal error abscissa labeled values 
pass point minimal error 
words show wide canyon point error minimum 
note canyon gets wider target activation levels approach extreme values 
note smoothly descending contour error surface transformed sharp cliff 
short geometry error surfaces suggest learning corner attractors 
lack smooth error gradient may difficult network find approximate neighborhood parameter space produces desired attractors 

network correct neighborhood parameters may vary widely significantly impacting performance 
second property may help formation large number corner attractors 
recall parameter includes weighted input rest network 
means individual unit enters region parameter space produces desired dynamic behavior significant changes activity levels rest network upset behavior 
words exploring weight values rest network tend destroy learned unit 
large region slack parameter space pursuing corner targets may facilitate learning 
examination provides general insight learning algorithms modify weight values respond various distributions target patterns 
conjectured larger slack region parameter space provided corner targets benefit learning shown true 
demonstrated target patterns extreme corners activation space really facilitate process learning stable attractors 
remainder report focuses empirical study aimed revealing benefits corner attractors 
experiment order verify alleged benefits corner attractors number artificial neural networks trained success constructing stable fixed points monitored training 
training sets various sizes importantly different training sets consisted patterns sampled randomly different distributions target vectors 
networks trained patterns sampled corners activation space expected successful networks trained patterns sampled target distributions 
network consisted single layer completely interconnected processing elements 
addition weighted inputs units processing element received input adaptable self connection adaptable bias input 
unit sigmoidal activation function hyperbolic tangent scaled gamma 
network sizes investigated units 
networks trained backpropagation time bptt learning algorithm rumelhart 
connection weights initialized small random values gamma 
networks trained epochs epoch involved single exposure target patterns training set 
exposure activation state network initialized result adding zero mean gaussian noise standard deviation target vector 
activity network allowed propagate time steps 
squared error objective function error signal applied network time step 
error signals backpropagated time full time steps resulting recommended change connection weight 
weights immediately updated 
batch training 
recommended weight changes summed training epoch weights modified epoch 
small learning rate 
training sets consisted number target patterns ranging 
training set sizes investigated multiple inclusive 
smaller networks experienced training set containing patterns number corners activation spaces 
training patterns randomly sampled different distributions 
network exposed distributions target vectors ffl corners element target vector took extreme value gamma 
training patterns sampled uniformly random discrete set containing corners activation space 
ffl uniform element target vector sampled uniformly random continuous range gamma inclusive 
words training patterns sampled uniformly random activation space 
ffl hypersphere training patterns sampled uniformly random surface hypersphere inscribed activation space hypersphere centered origin unit radius vectors generated relatively fast algorithm watson 
distribution interesting property producing training patterns constant distance center activation space 
target vectors normalized length uncommon artificial neural network pattern recognition literatures 
ffl projected corners activation space radially projected hypersphere inscribed activation space hypersphere centered origin unit radius 
training patterns sampled uniformly random discrete set containing projected points 
distribution contains vectors hypersphere maximally close corner 
particular interest networks performed distribution compared hypersphere distribution especially distribution 
ffl rotated entire training set patterns sampled manner identical projected distribution common transformation applied vectors set network 
particular random rotation origin applied training patterns 
rotation sampled uniformly random space possible rotations hypersphere 
utility distribution similarity projected distribution 
note pattern training set received exactly rotation 
average distance training patterns identical distributions average distance training patterns nearest corners tend quite different 
relationship results distributions help tease apart respective contributions intra target separation proximity activation space corners 
goal examining distributions demonstrate dominance corners distribution provide clues corner attractors easier learn 
epochs training network tested presence stable fixed point attractors 
large size networks activation spaces especially larger networks fine grained examination entire activation space network deemed intractable 
searching attractors close targets called spurious attractors amit opted hunt attractors neighborhoods training patterns 
simple empirical method 
training pattern activation state network initialized point close pattern point units center activation space 
activity network allowed propagate time steps resulting activation state network time recorded 
furthermore problem sampling rotations uniformly trivial solution 
approach starts noting problem randomly selecting rotation formally identical randomly selecting new set coordinate axes origin 
dimensional space new axes may selected procedure axis selected uniformly random radii dimensional hypersphere remaining axes determined recursive application procedure gamma dimensional subspace orthogonal chosen axis 
axis determined right handed nature coordinate system 
activation state network ceased changing tolerance vector element time steps resulting point activation space identified stable fixed point attractor 
attractors shared multiple target patterns training set recorded 
networks size trained training set size distributions target vectors 
words distinct combination training parameters applied networks 
difference trials numerical seed pseudo random number generator initialize connection weight values generate training sets 
measures network performance report means trials standard error values 
obvious goal randomization process avoid anomalous conditions 
learning capacity benefit fraction training sets networks successfully construct stable fixed point attractors 
answer question may 
note target distributions plotted graph 
note training set sizes shown networks generally failed learn single stable attractor cases 
plots number appropriate attractors formed measured different ways 
graphs left side count stable fixed point neighborhood training pattern success 
initializing network near training pattern resulted arrival stable fixed point attractor counted success closer training pattern basin attraction 
words discovered attractor counted 
graphs left show total number stable fixed point attractors joint neighborhoods training patterns 
generous measure success 
graphs right side stringent criterion 
plots stable attractor counted success addition previous requirements euclidean distance training pattern target half distance vector nearest neighboring training vector 
words attractor counted success relatively close corresponding training pattern 
note expected graphs right show generally worse performance fraction learned stable attractors fraction learned fraction learned training set size fraction learned close attractors training set size corners uniform hypersphere projected rotated fraction training set learned function training set size means random trials plotted standard error bars 
graphs left count training pattern learned pattern basin unique stable fixed point attractor 
graphs right count training pattern learned resulting fixed point half distance nearest neighboring training pattern 
row network size set number units increasing top bottom 
note abscissa scales differ rows 
note corners performance consistently sizes 
left 
result evident graphs dominance corners distribution 
training sets consisting corner patterns consistently performed training set distributions 
especially evident larger networks 
result implies corner targets facilitate learning attractors 
short larger training sets learnable training patterns placed extreme corners activation space 
clear loser training set distributions appears hypersphere distribution performs consistently poorly 
possible explanations poor performance come mind 
learning targets difficult simply surface hypersphere far corners activation space especially larger networks 
may result error surface large relatively flat region near point minimal error 
processing elements driven linear portion sigmoidal activation function slight changes weights profound changes network error 
hyper sensitivity weight modifications difficult learn patterns 
alternative explanation involves average distance target patterns 
difficulty learning hypersphere targets stems relatively close proximity training vectors case 
distance neighboring target vectors distance targets corners probably play role successful learning attractors clear hypersphere data factor influential 
possible way sort contributions factors involves examining projected rotated results 
distributions produce training sets average distance neighboring patterns projected distribution patterns consistently closer corners activation space 
interesting note smaller networks projected distribution performs better rotated distribution 
suggest small distance corners activation space important large distance training patterns 
unfortunately things simple 
notable separation curves vanishes size network increases 
projected performance drops away corners curve eventually roughly matches rotated performance 
results clearly show significant advantage near corners activation space clear large advantage merging projected rotated perfor mance curves larger networks interpretations high dimensional geometry 
simply result relative shrinking hypersphere network size increases causing patterns relatively closer 
alternatively degradation projected performance result increasing distance corners surface hypersphere network size increases 
words explanations distance neighboring patterns distance extreme corners activation space deeply confounded dimensionality activation space increases 
high dimensional spaces way widely separate patterns place near corners 
summary examined attractor networks able learn larger training sets training patterns sampled extreme corners activation space 
clear examination part benefit direct result close corners activation space solely product wide separation training patterns 
factors confounded network size grows possible discern experiment relative contribution target separation learning attractors large networks 
accuracy benefit addition facilitating learning large training sets extreme targets result learning stable attractors closer euclidean distance corresponding training patterns 
demonstrated results 
graphs left show distances target patterns resulting activation states averaged patterns training set acquired stable attractors 
graphs right averaged stable attractors 
note smaller networks mean distance consistently lower corners distribution 
corner targets produce lower distances larger networks range successfully learned training set see 
interesting 
appears case corner targets successfully learned patterns kept close respective training patterns 
patterns learned drawn attractors side activation space 
comparison activation states networks trained distributions training patterns tend drawn middle activation space 
mean distance patterns corners uniform hypersphere projected rotated mean distance mean distance training set size mean distance stable attractors training set size distance training pattern network activation function training set size means random trials plotted standard error bars 
graphs left display means patterns training set 
graphs right show means patterns resulted unique stable fixed point attractors 
row network size set number units increasing top bottom 
note abscissa ordinate scales differ rows 
distributions average distance target vector resulting activation state stays low 
effect seen fairly constant separation uniform distribution curve hypersphere curves graphs larger networks 
uniform distribution produces mix patterns close center activation space far away hypersphere distributions bound region close center 
targets learned drawn middle space hypersphere distributions tend average stay closer targets uniform distribution case 
worth noting projected distribution outperforms hypersphere distributions networks small advantage disappears network size increases 
mirrors seen reinforces analysis learning capacity networks 
producing learning capacity benefit placing training patterns near corners activation space shows accuracy benefit entirely explained terms mean separation targets 
issues confounded size network increased 
summary corner patterns successfully learned resulting fixed point attractors tend located closer corresponding targets training patterns drawn distributions 
patterns successfully learned tend pulled far target values 
learning speed benefit far demonstrated corner targets allow larger training sets learned result accurately located attractors 
addition benefits corner attractors learned faster patterns sampled distributions 
shown 
graphs figures display learning curves resulted training relatively small training sets 
plots left show results training sets size plots right involve size sets 
learning curves display fraction training set learned measured generous way left side graphs function focusing small training set sizes ones successfully mastered distribution training patterns 
see 
learning speed findings hold larger training set sizes 
fraction learned training patterns fraction learned fraction learned training epochs fraction learned corners uniform hypersphere projected rotated training patterns training epochs fraction training set learned function amount training means random trials plotted standard error bars 
graphs left show results training patterns training set graphs right involve training set size 
row network size set number units increasing top bottom 
training pattern counted learned pattern basin unique stable fixed point attractor 
training epoch consists presentation training pattern 
amount training measured epochs 
note corners distribution consistently produces faster learning compared training pattern distributions 
advantage small epochs large epochs 
short corner attractors learned rapidly targets middle activation space 
sparsity connectionist cognitive models high dimensional vector representations tend sparse 
means large fraction vector elements usually take common null value gamma 
hopes demonstrating benefits corner targets persist conditions sparse patterns simulations conducted 
networks processing elements trained manner smaller networks training set distributions corners uniform 
previous experiments sparsity training patterns systematically controlled 
selected level sparsity expressed density probability vector element non null take value gamma 
words smaller values density parameter produced target vectors null elements 
case corners distribution non null vector elements set high 
case uniform distribution non null vector elements set uniform random value interval gamma 
different density probabilities examined 
training process identical previous simulations purposes tractability training limited training epochs 
resulting network performance statistics shown 
main thing note corner targets continue dominate conditions sparse patterns 
shown simulations exhibited persistence accuracy benefits learning speed benefits seen non sparse training sets 
advantages corner attractors appear sustained domain sparse patterns 
closer examination interesting note surprisingly performance degrades sparsity increases density parameter decreases 
finding appears run counter theoretical results demonstrated network capacity store vector patterns increases pattern sparsity hertz 
fraction learned stable attractors fraction learned fraction learned training set size fraction learned close attractors training set size corners uniform fraction training set learned function training set size means random trials plotted standard error bars 
graphs left count training pattern learned pattern basin unique stable fixed point attractor 
graphs right count training pattern learned resulting fixed point half distance nearest neighboring training pattern 
row level training pattern density set density decreasing top bottom 
apparent discrepancy highlights difference study previous theoretical 
focusing learnability patterns capacity networks principle 
turns certain aspects bptt learning algorithm may deemed responsible results 
trained sparse training patterns networks quickly learn pattern element null 
bias weights rapidly grow large negative values 
results saturation sigmoidal activation function units turn slows learning process crawl 
way learning training set hindered sparse patterns 
examination attractors formed simulated networks shows single stable fixed point attractor vicinity vector null elements entirety activation space 
improved learning may modifying learning procedure 
learning rate lower lower online training updating weights training pattern presentation epoch techniques teacher forcing employed williams zipser 
discussion empirical results report clearly show advantages placing attractor network training patterns extreme corners activation space advantage learning capacity advantage learning accuracy advantage learning speed 
extensive experience attractor networks results may surprising 
non linearity processing element activation function multiple stable fixed point attractors possible placing target patterns linear range activation function bound hinder formation attractors 
known proof superiority corner targets learning attractors examples empirical investigation important reasoned intuitions 
despite success demonstrating benefits corner targets thing fully uncovered 
deep understanding exactly advantages exist 
benefit definitely appears accrue having targets relatively flat portion processing element activation function providing large flat region similar networks linear activation functions capable sustaining stable fixed point 
error surface point minimal error 
demonstrated difference performance seen projected rotated distributions training patterns smaller networks 
benefit corner attractors arise large separation results distribution 
data bearing question shown 
plots left show mean distance target pattern nearest corner activation space various network sizes 
graphs right show mean separation target pattern nearest neighbor training set 
notice measures projected distribution results closest corners distribution results smaller networks projected loses uniform network size increases 
matches performance findings 
notice projected rotated distribution results consistently separated left side graphs regardless network size essentially identical measuring intra pattern distances 
network performance numbers indicate distributions produce significantly different results small networks approximately poor results larger networks 
distance nearest corner measure explain performance results intra pattern distance candidate explaining happening larger networks 
short difficult ascertain relative contributions features training sets factors inherently interact particularly large networks 
important note results reported apply training sets consisting patterns particular kind 
specifically results extended address learnability individual patterns diverse training set 
analysis performed matter data collected 
examined degree successful formation stable fixed point attractor target predicted features target vector including distance target nearest corner distance target center activation space distance target nearest neighbor training set mean derivative activation function target point 
measures job discriminating successfully learned patterns members training set 
short benefits corner attractors fully realized training set patterns corners activation space 
ways report provide bad news mean distance nearest corner corners uniform hypersphere projected rotated mean distance mean distance training set size mean distance mean min separation patterns mean min separation mean min separation training set size mean min separation various euclidean distances function training set size means random trials plotted standard error bars 
graphs left display distances training patterns nearest corners activation space 
graphs right show distances training patterns nearest neighboring training pattern 
row network size set number units increasing top bottom 
note abscissa scales differ rows ordinate scales differ columns 
tractor network models cognitive processes 
general connectionist networks excel manipulating rich representations real vectors 
unfortunate expressive representations fail fixed point attractors 
excellent cognitive models fabricated real vector representations benefit inclusion attractor dynamics 
networks help explain semantic priming effects lexical retrieval high dimensional real vectors model word semantics lund burgess 
episodic memory models real vector representations memory traces charm model metcalfe attractor dynamics remove noise retrieved traces noise results interference stored traces 
real vector representations attractor networks useful natural unfortunately 
point researchers choose abandon expressive nature real valued vector elements utility attractor dynamics 
fortunately middle way 
binary representations may constructed beneficial properties real vector encodings 
general requires increasing dimensionality vector space size attractor networks employed projects may worthwhile cost 
short information encoded single real vector elements may encoded collection polarized elements 
artificial neural network literature includes variety useful methods encoding scalars portions binary vectors 
include codes binary element captures small range scalar values thermometer codes binary element represents passing threshold gray codes capture adjacency information circular scales 
algorithms developed directly translate collections real vectors polarized form manner preserves possible distances pairs patterns clouse cottrell 
algorithms allow researcher encode salient features pattern similarity degree intra pattern separation vector space representational scheme appropriate attractor networks 
summary simulation shown vector representations place patterns extreme corners activation space facilitate learning stable fixed point attractors standard attractor networks 
facilitation takes form ability learn larger training sets accurate placement learned attractors faster learning 
completely clear benefits arise clear entirely artifact large intra pattern separation afforded corner targets 
moderately rich representational schemes built large binary vectors schemes recurrent attractor networks preference real vector schemes 
way models may leverage benefits corner attractors 
acknowledgments due members ucsd cse artificial intelligence research group 
comments preliminary presentation november quite helpful 
authors extend particular gratitude tim bailey suggestions helped establish criteria successful learning attractors 
welcome sanity checks insights provided dan clouse jeanne eric mjolsness adam taylor 
lastly grateful minute proofing efforts smith 
simulations reported executed crbp modified version bp simulator mcclelland rumelhart 
report prepared top diagrams generated mathematica xfig 
almeida 

learning rule asynchronous perceptrons feedback combinatorial environment 
butler editors proceedings ieee international conference neural networks volume pages new york 
ieee 
amit 

modeling brain function world attractor neural networks 
cambridge university press 
clouse cottrell 

lexical access internet semantics 
high dimensional semantic spaces derived large text corpora symposium proceedings th annual conference cognitive science society pages 
clouse cottrell 

discrete multi dimensional scaling 
cottrell editor proceedings th annual conference cognitive science society pages la jolla 
lawrence erlbaum 
haussler 

quantifying inductive bias 
artificial intelligence 
hertz krogh palmer 

theory neural computation volume lecture notes santa fe institute studies sciences complexity 
addison wesley redwood city 
hopfield 

neural networks physical systems emergent collective computational abilities 
proceedings national academy sciences volume pages usa 
hopfield tank 

computing neural circuits model 
science 
kawamoto 

nonlinear dynamics resolution lexical ambiguity 
journal memory language 
levesque brachman 

fundamental tradeoff knowledge representation reasoning 
brachman levesque editors readings knowledge representation chapter pages 
morgan kaufmann san mateo 
lund burgess 

producing high dimensional semantic spaces lexical occurrence 
behavior research methods instrumentation computers 
mathis mozer 

computational utility consciousness 
tesauro touretzky leen editors advances neural information processing systems pages denver 
mit press 
metcalfe 

composite holographic associative recall model 
psychological review 
mjolsness 

bayesian inference visual grammars neural nets optimize 
technical report tr yale university computer science department 
pineda 

recurrent backpropagation dynamical approach adaptive neural computation 
neural computation 
plaut mcclelland seidenberg patterson 

understanding normal impaired word reading computational principles quasi regular domains 
psychological review 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland pdp research group editors parallel distributed processing explorations microstructure cognition chapter 
mit press cambridge 
rumelhart smolensky mcclelland hinton 

schemata sequential thought processes pdp models 
rumelhart mcclelland pdp research group editors parallel distributed processing explorations microstructure cognition chapter 
mit press cambridge 
watson 

statistics spheres volume university lecture notes mathematical sciences 
wiley new york 
williams zipser 

learning algorithm continually running fully recurrent neural networks 
neural computation 

