page printer opaque chapter cluster weighted modeling probabilistic time series prediction characterization synthesis bernd schoner neil gershenfeld cluster weighted modeling mixture density estimator local models framework analysis prediction characterization non linear time series 
architecture model estimation characterization formalisms introduced 
characterization tools include estimator uncertainty predictor uncertainty correlation dimension data set 
second part chapter framework extended synthesize audio signals applied model violin data driven input output approach 
list time series worthwhile forecast long rst unsuccessful attempts 
helpful know heart beating weather tomorrow stock market going crash 
unfortunately examples share property common don familiar categories system dynamics theory 
non linear non gaussian non stationary essentially non 
linear systems theory yielded multitude results widely applied practically engineering scienti disciplines 
majority signal processing system engineering control characterization techniques rely linear assumptions theory matured decades research implementations 
limitations linear theory clear non linear behavior kind handled 
reconstruction embedding theorem hand provides author correspondence 
bernd schoner neil gershenfeld theoretical means handle highly non linear behavior arbitrary physical systems hidden dynamics 
shows system state space mapped di space constructed observable system characterize data respect dimensionality dynamic behavior reconstructed space 
reconstruction theorem detects low dimensional structure high dimensional data space lets space described ective degrees freedom system example violin countless mechanical degrees freedom 
unfortunately turns dicult reconstructed state space predict output complex system 
low dimensional systems tractable fig models easily unstable complicated state space arbitrary prediction horizon 
driven systems easier handle autonomous systems 
model dimensionality driven system signi cantly bigger input output observables need considered time 
presence noise practically real world system complicates embedding task 
due problems fairly small number examples embedding despite theoretical promise applied successfully predict signal 
linear highly non linear systems large class systems easily classi ed combine characteristics worlds 
bow string interaction violin example strongly non linear transforms slow actions player fast audio signal 
time ect violin body eciently described linear lter little non linear ects bridge body dynamics 
violin combines linear non linear processing 
chapter introduces cluster weighted modeling cwm modeling tool allows characterize predict systems arbitrary dynamic character 
framework density estimation gaussian kernels contain simple local models describing system dynamics data subspace 
extreme case kernel framework collapses simple model linear coef cients 
opposite extreme allows embed forecast data may non gaussian discontinuous high dimensional chaotic 
cwm covers multitude models characterized di erent local model state representation 
create globally nonlinear models transparent local structures embedding past practice mature techniques general non linear framework 
exception famous wolf tone tone periodically collapses despite constant 
phenomenon particularly strong cello caused non linear coupling string body mode 

cluster weighted modeling limitations arti cial neural networks anns apparent quickly modeling power networks take long converge coecients meaningful context entire model failure success architecture unpredictable 
new family networks developed interpret data probabilistically represented graphical networks 
meta class models graphical models conceptually unbounded 
unify existing network architectures example classical anns single theory provide new insights extensions conventional networks open new application domains 
graphical models referred independence networks graphical representation really describes dependence independence random variables 
called bayesian belief networks dependencies variables expressed terms conditional probability functions implicit explicit prior beliefs built 
furthermore named uence diagrams causal dependences variables clearly illustrated 
uence meant probabilistically contains deterministic causality special case 
unfortunately graphical models lack systematic search algorithm maps problem network architecture 
network parameters trained new data architecture needs redesigned node node scratch 
cluster weighted modeling special case probabilistic model gives generality graphical models favor ease minimal number hyper parameters fast parameter search 
designed architecture general reasonably possible speci particular application necessary 
tool allows statistical time series analysis physicist perspective time allows solve complicated engineering problems example design digital musical instruments 
opposed anns provides transparent local structures meaningful parameters allows identify analyze data subspaces converges quickly 
rst part chapter provides basic architecture estimation characterization tools cwm 
second part concerned problem building data driven input output model violin 
violin complex driven device socio cultural artistic physical subtlety hardly matched human artifact 
time violin provides clear error metric model just sounds 
non linear dynamics statistics viewpoint violin paradigmatic object shows non linear linear stochastic deterministic behavior time 
bernd schoner neil gershenfeld cluster weighted modeling architecture cluster weighted modeling cwm input output inference framework probability density estimation joint set input feature output target data 
similar mixture experts type architectures interpreted exible transparent technique approximate arbitrary function 
conventional kernel techniques cwm requires hyper parameter xed provides data parameters length scale bandwidth local approximation output input algorithm 
start set discrete real valued input features may measured features components time lagged embedding space discrete real valued output target vector joint inputoutput set xn general model infers joint density data set conditional quantities expected expected covariance hp jxi derived 
expand joint density clusters labeled contains input domain uence local model output distribution 
rst step joint density separated unconditioned cluster probability conditional probability data cluster expanded input domain uence output distribution xjc yjx xjc problems require distinction slowly varying state variables describing global boundary conditions state system fast varying variables describing fast dynamics system 
case decompose obtain density yjx jc may identical overlapping dimensions completely distinct 

cluster weighted modeling input distribution taken gaussian distribution xjc jp 

pm cluster weighted covariance matrix feature space 
reduced variances dimension computational complexity issue 
output distribution taken yjx jp dy 

mean value output gaussian replaced function unknown parameters diagonal terms output covariance matrices neglected needed 
understand form consider conditional forecast expected yjx dy dy yjx dy xjc xjc xjc xjc observe predicted superposition local functionals weight contribution depends posterior probability input point generated particular cluster 
denominator assures sum weights contributions equals unity 
likewise compute conditional error terms expected covariance hp jxi yjx dy yy yjx dy yy yjx dy xjc xjc xjc xjc bernd schoner neil gershenfeld equals expected variance single output dimension considered jxi xjc xjc parameters determined number clusters form local models control model resources versus tting 
trade complexity local models complexity global architecture nicely illustrated case local polynomial expansion equ locally constant models large number clusters predictive power determined number gaussian kernels 
alternatively high order polynomial model single kernel model reduces global polynomial model 
choice local models depends application 
general expresses prior beliefs nature data insights mechanics system functions regularizer model 
machine learning architectures estimation algorithms typically depend global regularizers handle prior beliefs model 
problematic global statements may apply locally 
example maximum entropy principle handling discontinuities notion local smoothness integrated curvature enforcing local smoothness rounds discontinuities 
approach model constrained local architecture may enforce local smoothness time allows discontinuities needed 
model estimation non linear function tting uses models linear coecients nonlinear basis functions example polynomial expansion models coecients inside nonlinearities example neural network 
case generalized linear model equ single matrix pseudo inverse needed nd set coecients yielding minimum mean square error 
number 
cluster weighted modeling coecients equ exponential dimension model non linear coecients equ expressive power reduce number coecients needed approximation error linear dimension 
non linear parameters equ require iterative search 
cwm uses simple local models satisfy create globally powerful models described combines ecient estimation bene ts models 
local model parameters matrix inversion local covariance matrix nd remaining cluster parameters charge global weighting variant expectation maximization em algorithm 
em iterative search maximizes model likelihood data set initial conditions 
pick set starting values cluster parameters enter iterations expectation step 
step assume current cluster parameters correct evaluate posterior probabilities relate cluster data point 
posteriors interpreted probability particular data generated particular cluster normalized responsibility cluster point jy xjc xjc xjc sum clusters denominator causes clusters interact ght points specialize data best explain 
step assume current data distribution correct nd cluster parameters maximize likelihood data 
new estimate unconditioned cluster probabilities jy dy dx jy xn idea integral density approximated average variables drawn density 
compute expected input mean cluster bernd schoner neil gershenfeld estimate new cluster means xjc dx xjc dy dx jy dy dx xn jy xn xn jy xn jy xn apparently formal density variable integrated important result cluster parameters respect joint input output space 
clusters get pulled data explained model explains data 
similar way de ne cluster weighted expectation function xjc dx xn jy xn xn jy xn jy xn lets update cluster weighted covariance matrices pm ij lets compute matrices needed update local models 
model parameters derivative log total likelihood function respect parameters log yn xn considering single output dimension single coecient 
cluster weighted modeling get log yn xn xn yn xn yn xn xn np cm jy xn xn xn plugging obtain expression update hf ji 
am matrix inverse done singular value decomposition avoid numerical problems singular covariance matrices 
considering full set model parameters get 
am bm ij hf 
am ij hy 
output covariance matrices associated model estimated 
clusters initialized arbitrarily algorithm guaranteed terminate local likelihood maximum 
initializing clusters places close nal position saves time don walk way data set 
method performs empirically choose initial cluster probabilities 
bernd schoner neil gershenfeld pick randomly points training set clusters initialize cluster input means cluster output mean points 
set remaining output coecients zero 
size data set space dimension initial cluster variances 
idea normalize training data zero mean unit variance arbitrary data values may cause probabilities small 
summarize model estimation process pick initial conditions evaluate probability data xjc nd posterior probability clusters jy update cluster weights cluster weighted expectations input means new variances new covariances new maximum likelihood model parameters new nally output variances ew go back total data likelihood increase anymore 
error estimation characterization probability density training data set error estimates statistics derived provides useful insights self consistency check model 
density indicates model uncertainty expect obtain valid model data density low 
certainty model estimate proportional data density subspace 
conditional covariance hand indicates prediction uncertainty input related characterizations uncertainty entropy lyapunov exponents 
di erential entropy gaussian process log 
di erences di erential entropy matter ignore additive consider log 
asymptotic rate growth entropy time equal source entropy turn equal sum positive lyapunov exponents times time lag samples assuming prediction errors roughly gaussian asymptotic value log output width input dimension increased provides local estimate source entropy system 
sum negative exponents similarly analyzing time series reverse order exchanging positive negative exponents 
clusters nd subspace occupied data cluster parameters nd dimension data set high dimensional space 
intuitively number signi cant eigenvalues local covariance matrices provides estimate dimensionality data manifold 
example obtain signi cant eigenvalues lorenz attractor embedded dimensions fig 
quantify eigenvalues local covariance matrices em 
cluster weighted modeling 
plot shows lorenz set embedded dimensional lag space 
dense dots show embedded data 
cluster means covariances derived input density estimate forecasting surface shaded conditional uncertainty showing maxima associated orbit re injection 
fe evaluate radial correlation integral cm xd dx erf 
erf bernd schoner neil gershenfeld sorted cluster eigenvalues test set mean square error training set mean square error test set data likelihood training set data likelihood 
fitting lorenz set 
top data likelihood function iterations 
middle mean square error function iteration bottom sorted eigenvalues local covariance matrices 
turn lets compute cluster correlation dimension log cm log erf limit dimension equal dimension space locally curvature clustered space seen 
evaluated max max direction contribution direction variance max contribution drops expected dimension data set nally expectation conventional calculation dimension data set inter point pairs clusters nd signi cant places evaluate dimension appropriate length scale test scaling 

cluster weighted modeling application build digital mimic synthesis musical instruments tries infer models behave sound original instrument ideally extend original model indistinguishable 
general goal variety di erent modeling approaches 
global sampling example particularly successful commercial keyboard synthesizers 
single note piano recorded di erent volume levels varying duration sounds replayed synthesis 
memory cheap little interpolation samples required sound quality close original recordings 
method works instruments low dimensional control space keyboard instruments 
model know instrument internal state reuses seen notion control part player 
successful synthesis technique physical modeling 
rst principles analysis acoustics instrument implemented numerical methods 
method provides lot exibility example allows create new instruments derived physical mechanisms implemented physically 
approach serious limitations 
current computers barely run full scale model violin shown simple calculation nite element approximation violin 
assumed body modes body axis nite element nodes cycle get nodes violin plate order nodes instrument 
multiply cd quality sample rate khz roughly giga instructions second needed run model real time 
fundamental problem physical models systematic parameter search model structure instrument family 
basic model violin way nd parameters distinguish instrument trying combinations parameters high dimensional space 
method presenting lies conceptually sampling physical modeling approach best described physics sampler 
infer model recorded data stored samples create model exibility physical model synthesize physics instrument sound 
time doing computational compression data physical device represented ecient description 
mentioned mechanics violin playing involve stochastic behavior 
stochastic aspects clear considers player instrument jointly 
partially controls instrument 
idea spectral characteristics wants achieve means hear control phase produced signal 
naturally causal relationship player action bernd schoner neil gershenfeld spectral content sound phase di erent partials random unpredictable 
fortunately phase perceived discriminating feature typical playing situation may pick arbitrarily long avoid discontinuities signal components 
general lesson learn need model process instantiation particular process 
predict deterministic aspects signal stochastic behavior needs summarized appropriate statistics power spectrum 
violin musical instruments characterized slowly varying boundary conditions map fast audio signal 
non linear interaction bow string causes slow player motion turned famous helmholtz motion contains frequency components nal audio signal 
slow fast elements describe di erent times scales mixed confuse 
fast slow dynamics corresponding state variables need treated di erently 
cwm provides means implement distinction slowly varying boundary conditions select domain operation cluster con guration space equ fast dynamics handled local models associated state variables equ 
previous section introduced cwm machine learning framework allows predict characterize arbitrary input output data 
inference tool need consider second important aspect data analysis prediction data representation 
linear transforms fourier wavelet transforms change information content data considerable di erence domain try predict 
cwm lets embed variety speci local representations 
section discuss cluster weighted spectral modeling cluster weighted sampling examples local implementations cwm 
introduce ways higher order factorization show cwm structure included hidden markov model explicitly encoding timing model 
cluster weighted spectral modeling goal build input output model violin data set contains physical input features measured bow nger board synchronized audio data 
training process network learns mapping physical input sound 
training network knows generate appropriate audio new input 
decompose audio training signals spectral frames frame rate equals sampling rate slowly varying physical input 
frame contains coecients short term fourier transform 
cluster weighted modeling stft applied xed number audio samples weighted hamming window 
underlying assumption player operates spectral composition sound spectral characteristics change faster actual control 
frames retain harmonic partials violin signal 
amplitudes harmonic partials taken magnitude power spectrum frequency bin precise frequency estimates obtained phase di erence closely spaced sample windows 
total partials output vector components 
input vector consists physical input data bow velocity pressure nger position bow bridge position 
driven belief past input conditions current state instrument input vector augmented respect past input data 
adding time lagged input samples balance need include past burden big input space 
model scales linearly output dimension sensitive large input spaces required amount training data increases exponentially input data dimension 
model sensitive tting bigger input space 
training set vector pairs xn train cwm input output model simple linear local models form 
synthesis process vector spectral information predicted new input data spectral data compute time domain audio data sinusoidal additive synthesis phase amplitude partials taken predicted components linearly interpolating frames 
nal signal obtained summing di erent components 
cluster weighted sampling global sampling successful synthesis technique instruments low dimensional control space piano 
technique appropriate instruments continuous complex control violin 
violin case amount data required cover possible playing situations prohibitive control possibilities essentially unlimited 
overcome problem parameterize available sample material ecient way 
cwm learns select appropriate samples predict parameters needed reassemble sound raw material 
clusters multiple output models covering sample selection amplitude prediction pitch prediction 
rst expert pointer sample space 
cluster generated control data takes sequence samples stand particular playing situation 
cluster replayed cluster takes samples 
come back issue sequencing time domain sound samples 
bernd schoner neil gershenfeld time time 
cluster weighted sampling overlapping samples string signal 
input output model bottom bow position bow speed nger position predicted samples amplitude solid sampled amplitudes dashed predicted samples pitch solid sampled pitch dashed doubled dashed lines indicate overlapping sample windows old window slowly faded new window faded way total weight data adds unity moment 
second output model pitch predictor 
control input typically includes left hand nger position nger board local linear model predicts appropriate pitch moment time 
samples selected synthesis certainly won match desired pitch exactly 
re sampled respect predicted target pitch 
resampling done real time 

minff minff gt stored sampling frequency target sampling frequency 
sample pitch target di er big pitch shifts results audible artifacts 
resampling easily 
cluster weighted modeling compensate ects vibrato 
hope record possible vibrato sequence frequency choose desired vibrato behavior sampled material 
third output model predicts sound volume moment time simple locally linear predictors 
selected samples re scaled respect target volume 
strong modi cations sample volume avoided order correct timbre altered 
approach requires number preprocessing steps extract high level properties audio data 
need pitch volume label parameterize correct audio data moment time 
properties easier obtain may 
pitch extraction problem solved full generality turns surprisingly simple approach 
measuring physical input data estimate pitch start 
certain nger position possible pitch small frequency interval practically impossible pitch tracker get confused audio analysis 
important detail sequencing pieces audio looping sample interval change cluster occurs 
choose match samples minimizing square error old new samples 
additionally fade old sound fade new sound hamming window overlap add 
sine re sample audio material anyway increase resolution allowing non integer alignment sounds increasing complexity synthesis algorithm 
success depends length permissible fading interval character sound 
fig shows overlap highly phase coherent pieces string signal violin describing helmholtz motion 
case partials line nicely fundamental discontinuities problem 
sound signal loses regularity ltering bridge resonant body instrument harder deal 
higher order factorization hierarchical mixture models hidden markov models demonstrated network structure easily applied problems suciently complex applications 
cases additional hierarchical structure helpful crucial 
identical models may want reused di erent areas con guration space systems may long term temporal dependences 
introduce mixture models arbitrary hierarchical depth 
similarly add higher level factorization describing global states system 
bernd schoner neil gershenfeld time 
hidden markov model bottom cluster model input space clusters state state probabilities predicted samples amplitude measured dashed predicted line predicted samples pitch measured dashed predicted line 
measured predicted data visibly di erent reconstructed audio sounds similar original audio data spectral characteristics basic characteristics sound envelope preserved 
example top level state violin model distinguish global playing conditions playing particular string 
probability density expanded model previous sections time lags input signal encode temporal structure memory system 
way stating dependence say current state depends past state current input 
hidden markov models hmms developed precisely implement dependence probabilistic framework 

cluster weighted modeling embed cwm hmm structure obtain input output synthesis model explicit time dependence built 
hmms typically de ned terms number distinct states state transition probability matrix fa denotes probability state follows state emission probability denotes probability system generates observation state replace discrete emission probabilities continuous probability density function form means cluster probabilities ectively time dependent conditioned past system states 
cluster represents speci state set possible states likelihood sequence input output observations fx 
qt qt 
qt qi emission probability pair state probability densities may simple clusters sum clusters qi yjx 
xjc 
probability distributions identical 
model estimation complicated probabilistic ideas shown earlier 
hmms typically trained forward backward procedure special implementation em estimation problem tractable 
synthesis model evaluated forward procedure output generated causally 
output sequence moment time taken expected value estimated past states current observed input 

jq 

jq hy jx 
particular sequence states re ects sequence input gestures internal states violin 
fig illustrates state sequence bernd schoner neil gershenfeld simple 
follow note attack sustained part bow change 
summary valuable insights possible signals complex systems penetrated routine data analysis engineering practice algorithms limited applicability reliability 
modeling framework course solve problems handle nonlinearity stochasticity transparent fashion provides clear connection past practice domain choice local models just single hyperparameter number clusters 
natural extension exists problems require internal states model needing incur architectural uncertainty general graphical probabilistic networks 
valuable consequences probabilistic setting range statistics derived underlying model 
impose cost function learning algorithm outset prediction questions answered directly density estimate 
possible reasonable amounts data estimate constrained local models 
possible kinds characterization data done reliably context able predictions data including internal consistency checks predicting model errors 
resulting models ecient storage computation model resources allocated data describe sample generalization limited reasonable behavior local models 
features point possibility broadly applicable physics sampling building phenomenological models driven systems space ective internal degrees freedom enabling new applications literally sound great 
acknowledgments grateful support mit media lab things think consortium 
amari 
information geometry em em algorithms neural networks 
neural networks 

cluster weighted modeling andrew barron 
universal approximation bounds superpositions sigmoidal function 
ieee transactions information theory 
buntine 
guide literature learning probabilistic networks data 
ieee transactions knowledge data engineering 
martin casdagli 
dynamical systems approach modeling input output systems 
casdagli eubank editors nonlinear modeling forecasting santa fe institute studies sciences complexity pages redwood city 
addison wesley 
cleveland devlin 
regression analysis local tting 
statist 
assoc 
lothar cremer 
physics violin 
mit press cambridge massachusetts 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
statist 
soc 

neil gershenfeld 
nature mathematical modeling 
cambridge university press new york 
heckerman wellman 
bayesian networks 
communications association machinery 

jordan jacobs 
hierarchical mixtures experts em algorithm 
neural computation 
michael jordan editor 
learning graphical models 
mit press cambridge massachusetts 
dana 
sampling synthesis 
mark brandenburg editors applications digital signal processing audio acoustics pages 
kluwer academic publishers 

speech analysis synthesis sinusoidal representation 
ieee transactions acoustics speech signal processing assp 

fundamentals string dynamics 

radford neal 
bayesian learning neural networks 
springer new york 
radford neal geo rey hinton 
new view em algorithm justi es incremental variants 
lawrence rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
schoner cooper douglas gershenfeld 
data driven modeling acoustical instruments 
journal new music research 
xavier serra julius smith 
spectral modeling synthesis sound analysis synthesis system deterministic plus stochastic decomposition 
computer music journal 
smith 
exible sampling rate conversion method 
acoustics speech signal processing san diego volume 
julius smith 
physical modeling digital 
computer music journal 
takens 
detecting strange attractors turbulence 
rand young editors dynamical systems turbulence volume lecture notes mathematics pages new york 
springer verlag 
