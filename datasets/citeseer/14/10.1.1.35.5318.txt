advances neural information processing systems solla leen 
muller eds pp mit press probabilistic methods support vector machines peter department mathematics king college london strand london wc ls email peter kcl ac uk describe framework interpreting support vector machines svms maximum posteriori map solutions inference problems gaussian process priors 
provide intuitive guidelines choosing svm kernel 
assign evidence maximization optimal values parameters noise level determined unambiguously properties map solution cross validation error illustrate simple approximate expression svm evidence 
determined error bars svm predictions obtained 
support vector machines probabilistic framework support vector machines svms subject intense research activity neural networks community tutorial introductions overviews developments see :10.1.1.117.3731
open questions remains set tunable parameters svm algorithm methods choosing width kernel function noise parameter controls closely training data fitted proposed see effect shape kernel function remains imperfectly understood :10.1.1.117.3731
error bars class probabilities svm predictions important safety critical applications example difficult obtain 
suggest probabilistic interpretation svms tackle problems 
shows svm kernel defines prior functions input space avoiding need think terms high dimensional feature spaces 
allows define quantities evidence likelihood set hyperparameters kernel amplitude 
give simple approximation evidence maximized set hyperparameters 
evidence sensitive values individually contrast properties cross validation error deterministic solution depends product ck assign unambiguous value error bars derived 
focus class classification problems 
suppose set training examples binary outputs sigma corresponding classes 
basic svm idea map inputs vectors oe high dimensional feature space ideally feature space problem linearly separable 
suppose true 
decision hyperplanes deltaoe separate training examples obey deltaoe set training inputs svm solution chosen largest margin largest minimal distance training examples 
equivalently specifies margin minimizes squared length weight vector jjwjj subject constraint delta oe problem linearly separable slack variables introduced measure margin constraints violated writes delta oe gamma control amount slack allowed penalty term added objective function jjwjj penalty coefficient training examples delta oe incur penalty contribute gamma delta oe :10.1.1.117.3731
gives svm optimization problem find minimize jjwjj delta oe shifted hinge loss gamma theta gamma 
interpret svms probabilistically regard defining negative log posterior probability parameters svm training set term gives prior exp gamma jjwjj gamma gamma 
gaussian prior components uncorrelated unit variance 
chosen gaussian prior variance flat prior implied recovered letting 
latent variable values delta oe individually appear second data dependent term sense express prior directly distribution 
joint gaussian distribution components covariances oe delta delta oe oe deltaoe svm prior simply gaussian process gp functions covariance function oe delta oe zero mean 
correspondence svms gps noted number authors 
second term negative log likelihood define probability obtaining output sigma jx exp set exp gamma ensure probabilities sigma add value larger 
likelihood complete data set dj jx input distribution remains essentially arbitrary point 
likelihood function normalized jx gamma jx exp gamma probabilistic setting sense keep finite small training sets equal nonzero probability 

remedy write actual probability model dj posterior probability jd dj independent normalization factor construction map value svm solution 
simplest choice normalizes independent dx conceptually corresponds procedure sampling sample gp prior 
data point sample 
assign outputs sigma probability yjx respectively remaining probability gamma don know class probability restart process sampling new 
smallest inside gap functions values gap survive dataset required size built 
reflected dependent factor effective prior follows 
correspondingly likelihood yjx yjx xj normalized sigma input density influenced function reduced uncertainty gaps 
summarize eqs 
define probabilistic data generation model map solution argmax jd data set identical standard svm 
effective prior gp prior modified data set size dependent factor likelihood defines just conditional output distribution input distribution relative arbitrary 
relevant properties feature space encoded underlying gp prior covariance matrix equal kernel 
log posterior model ln jd gamma dx dx gamma gamma const just transformation 
differentiating non training inputs sees maximum standard form ff ff ff ff respectively 
call training inputs group marginal form subset support vectors ff 
sparseness svm solution number support vectors comes fact hinge loss constant 
contrasts uses gp models classification see likelihood sigmoidal logistic transfer function nonzero gradient 
noise free limit sigmoidal transfer function step function map values tend trivial solution 
illuminates alternative point view margin shift hinge loss important svms 
probabilistic framework main effect kernel svm classification change properties underlying gp prior true ln 
smaller higher gap model intuitive sense 
samples svm priors input space unit square plots samples underlying gaussian process prior :10.1.1.117.3731
greyscale plots represent output distributions obtained likelihood model greyscale indicates probability black white 
exponential ornstein uhlenbeck kernel covariance function exp gamma giving rough decision boundaries 
length scale 
reduced amplitude note sample prior corresponding new kernel grey uncertainty gaps roughly regions definite outputs black white widened 
row squared exponential rbf kernel exp gamma gamma yielding smooth decision boundaries 
changing holding fixed new sample shows parameter sets typical length scale decision regions 
polynomial kernel delta 
absence clear length scale widely differing magnitudes bottom left top right corners square kernel plausible probabilistic point view 

fig 
illustrates samples different types kernels 
effect kernel smoothness decision boundaries typical sizes decision regions uncertainty gaps clearly seen 
prior knowledge properties target available probabilistic framework provide intuition suitable choice kernel 
note samples fig 
effective prior 
finds dependent factor change properties prior qualitatively evidence error bars providing intuition svm kernels probabilistic framework discussed possible apply bayesian methods svms 
example define evidence likelihood data model specified hyperparameters parameters defining 
follows dj factor naive evidence derived unnormalized likelihood model correction factor ensures normalized data sets 
crucial order guarantee optimization log evidence gives optimal hyperparameter values average opper private communication clearly general depend separately 
actual svm solution hand map values seen depend product ck 
properties deterministically trained svm test cross validation error determine resulting class probabilities unambiguously 
outline simple approximation naive evidence derived 
integral log integrand additive constant 
integrating gaussian distributed intractable integral remains 
progress expanding log integrand maximum 
non marginal training inputs equivalent laplace approximation terms expansion quadratic deviations maximum give simple gaussian integrals 
remaining leading terms log integrand vary linearly near maximum 
couplings appear quadratic order discarding terms integral factorizes evaluated 
result calculation ln gamma ff gamma gamma ln gamma gamma ln det terms represent maximum log integrand ln dj comes integration fluctuations 
note contains information marginal training inputs km corresponding submatrix lm diagonal matrix entries quantitative changes arise function values discouraged large tends increase size decision regions narrow uncertainty gaps 
verified comparing samples 
ln ln ln toy example evidence maximization 
left target latent function solid line 
svm rbf kernel exp gamma gamma ck trained dashed line training examples circles 
keeping ck constant evidence top right evaluated function 
note normalization factor shifts maximum larger values naive evidence 
bottom right class probability jx target solid prediction evidence maximum dashed 
target generated 
ff gamma ff sparseness svm solution matrices reasonably small making determinants amenable numerical computation estimation 
eq 
diverges ff marginal training inputs approximation retaining linear terms log integrand breaks 
adopt simple heuristic replacing det det prevents spurious singularities identity matrix 
choice keeps evidence continuous training inputs move set marginal inputs hyperparameters varied 
fig 
shows simple application evidence estimate 
data set evidence evaluated function kernel amplitude varied simultaneously ck svm solution remained unchanged 
data set generated artificially probability model true value known spite crude approximation maximum full evidence identifies quite close truth 
approximate class probability prediction jx value plotted fig 
overestimates noise target somewhat 
note yjx obtained simply inserting map values 
proper bayesian treatment average posterior distribution jd course taken leave 
normalization factor estimated assumed uniform input density example sampling gp prior 
unknown empirical training input distribution proxy samples multivariate gaussian covariance matrix 
gave similar values ln example subset training inputs 
summary described probabilistic framework svm classification 
gives intuitive understanding effect kernel determines gaussian process prior 
importantly allows properly normalized evidence defined optimal values hyperparameters noise parameter corresponding error bars derived 
include comprehensive experimental tests simple estimate naive evidence comparison approaches 
include variational methods experiments gaussian approximation posterior jd example promising 
improvement possible dropping restriction factor analysed covariance form 
easily shows optimal gaussian covariance matrix gamma gamma parameterized diagonal matrix 
interesting compare laplace gaussian variational results evidence cavity field approach 
pleasure tommi jaakkola manfred opper matthias seeger chris williams ole winther interesting comments discussions royal society financial support dorothy hodgkin research fellowship 
burges :10.1.1.117.3731
tutorial support vector machines pattern recognition 
data mining knowledge discovery 
smola scholkopf 
tutorial support vector regression 

neuro colt technical report tr available svm gmd de 
scholkopf burges smola 
advances kernel methods support vector machines 
mit press cambridge ma 
scholkopf bartlett smola williamson 
shrinking tube new support vector regression algorithm 
nips 
cristianini campbell shawe taylor 
dynamically adapting kernels support vector machines 
nips 
seeger 
bayesian model selection support vector machines gaussian processes kernel classifiers 
submitted nips 
wahba 
support vector machines reproducing kernel hilbert spaces randomized 
technical report university wisconsin 
jaakkola haussler 
probabilistic kernel regression models 
proceedings th international workshop artificial intelligence statistics 
appear 
smola scholkopf muller 
connection regularization operators support vector kernels 
neural networks 
opper winther 
gaussian process classification svm mean field results leave estimator 
advances large margin classifiers 
mit press 
appear 

probabilistic interpretation bayesian methods support vector machines 
submitted icann 
williams 
prediction gaussian processes linear regression linear prediction 
jordan editor learning inference graphical models pages 
kluwer academic 
