support vector method hierarchical clustering asa ben hur faculty management technion haifa israel david horn school physics astronomy tel aviv university tel aviv israel siegelmann faculty management technion haifa israel vladimir vapnik labs research schultz dr red bank nj usa novel method clustering support vector machine approach 
data points mapped high dimensional feature space support vectors define sphere enclosing 
boundary sphere forms data space set closed contours containing data 
kernel parameter varied contours fit data tightly splitting contours occurs 
contours interpreted cluster boundaries points disconnected contour defined cluster 
cluster boundaries take arbitrary geometrical shapes clusters separated valleys underlying probability distribution 
sv algorithms outliers dealt introducing soft margin constant leading smoother cluster boundaries 
hierarchical structure data explored varying parameters 
investigate dependence method parameters apply data sets 
clustering ill defined problem exist numerous methods 
parametric models non parametric criteria 
parametric algorithms usually limited expressive power certain cluster structure assumed including number clusters 
examples non parametric hierarchical methods model temperature serving hierarchy parameter magnetic regions defining clusters approach successive normalization dissimilarity matrix pairs points 
papers suggested support vector sv approach learning labeled data data exploration unlabeled data :10.1.1.39.912
propose algorithm characterizing support high dimensional distribution 
product algorithm compute set contours enclose data points 
contours interpreted cluster boundaries formulate approach svm language 
shape contours regulated width parameter kernel function 
points inside separate piece interpreted cluster 
hierarchical organization data explored varying width parameter soft margin constant 
identify clusters valleys probability distribution data 
uses paradigm identify clusters 
describing cluster boundaries support vectors section algorithm describing support probability distribution represented finite data set forms basis clustering algorithm 
fx data set points ir input space 
nonlinear transformation high dimensional feature space look smallest enclosing sphere radius described constraints jj ajj jj 
jj euclidean norm center sphere 
outliers allowed incorporating soft constraints jj ajj 
solve problem introduce lagrangian jj ajj lagrange multipliers constant penalty term 
minimization respect leads kkt complementarity conditions jj ajj 
outlier point 
equation states outliers point inside surface sphere feature space 
follows kkt conditions point surface sphere 
defined support vectors 
eliminate variables turning lagrangian wolfe dual form 
variables don appear lagrangian may replaced constraints sv minimization problem solving 
follow sv method represent dot products 
mercer kernel 
noted kernels yield tight contour representations cluster 
gaussian kernel jj laplacian kernel jjx jj width parameters lagrangian written invariant kernel gaussian minimization problem equivalent standard sv optimization problem 
point define distance feature space image center sphere jj ajj view definition kernel radius sphere fr support vector contour encloses points data space set fx rg shape contour governed shows increased enclosing contour forms tighter fit data number svs increases 
fixed decreased number svs decreases ignoring outliers gives smoother shape 
discussed section 
denote sv number support vectors outliers respectively note results consequence constraints proposition sv discuss generalization ability algorithm data points generated probability distribution original data set probability error expected fraction points contained high dimensional sphere 
estimated error sv standard leave argument addition outliers 

outliers number outliers function characterize dependence sv parameters dependence empirically generated data sets points sampled uniform distribution unit square 
data set sv computed number supports number supports function averaged data sets 
similar behavior probability distributions 
observation number outliers depends weakly satisfies max function linear dependence linear behavior continues sv find sv functions hierarchical clustering section go set examples demonstrate clustering ability algorithm 
data set separation clusters seen allowing outliers 
seen increased shape boundary curve data space varies 
values enclosing contour splits forming increasing number connected components 
regard component representing single cluster 
data set contains points 
gaussian kernel 
define adjacency matrix ij pairs points ij line segment connecting clusters defined connected components graph induced labeling procedure justified empirical observation line segment connecting points different components contain points outside sphere line connecting close neighbors component lies inside sphere 
checking line segment implemented sampling number points value numerical experiments 
outliers left unclassified labeled cluster closest choose 
role sub section demonstrate importance allowing outliers setting value 
clusters data distinguished outliers separated 
consider task separating mixture gaussians 
outliers allowed small single point clusters formed main clusters distinguished 
demonstrate clustering solution value higher value allows outliers 
example 
outliers allowed smallest upper figures data point mixture gaussians 
laplacian kernel 
laplacian kernel cn 
lower figures inner sphere composed pts components gaussian distribution concentric rings contain pts uniform angular distribution gaussian radial distribution 
gaussian kernel 
gaussian kernel cn 
cluster distinguished regardless value nc separation occurs iterative process types hierarchical clustering algorithms agglomerative algorithms divisive ones 
cluster identification algorithm ways agglomerative manner starting large value point different cluster decreasing value single cluster divisive approach start ing small value increasing 
efficient meaningful clustering solutions usually relatively small number clusters 
describe qualitative schedule varying parameters 
start value cluster occurs increase detect cluster splitting 
single point clusters start break large number support vectors obtained overfitting increased 
decision dividing approach described clustering partition data sets sizable overlap perform clustering smaller data sets compute average overlap clustering solutions number partitions 
validation performed 
approach halt fraction outliers support vectors exceeds threshold 
justified relation fraction generalization 
alternatively relations equations pick values advance iterative process 
complexity performance quadratic programming problem equation solved smo algorithm proposed efficient tool svm training 
minor modifications required adapt problem solve 
benchmarks reported show algorithm converges kernel evaluations depending type data parameters 
complexity labeling part algorithm complexity 
note memory requirements smo algorithm low implemented memory cost decrease efficiency algorithm useful large data sets 
compare performance algorithm hierarchical algorithms ran iris data set standard benchmark pattern recognition literature 
obtained uci repository 
data set contains instances containing measurements iris flower 
types flowers represented instances 
clusters linearly separable easily separated low values higher values remaining clusters separated laplacian kernel gaussian kernel 
case clustering solution perfect points misclassified gaussian laplacian kernels respectively 
misclassifications outliers 
compared outliers super clustering algorithm misclassifications iterative algorithm 
discussion algorithm described finds clustering solutions discovering gaps probability distribution data 
clusters sizable overlap distinguished algorithm 
advantage algorithm represent clusters arbitrary shape algorithms geometric representation limited 
respect reminiscent method 
algorithm distinct advantage algorithm kernel method explicit calculations feature space avoided leading higher efficiency 
wish stress possibility method deal outliers 
algorithms minimize error criteria square error criterion outliers alter clustering solution affect clustering solution obtained algorithm chosen large 
estimates numbers outliers support vectors functions designed best fit clustering problem 
empirical results indicate cluster description algorithm obtain better theoretical understanding svms general number support vectors outliers defined dependence kernel soft margin parameters 
addition interesting differences cluster descriptions produced laplacian gaussian kernels numerical experiments performed domains generated laplacian kernel didn holes observed gaussian kernel leading better generalization performance cases 
problems local features gaussian kernel perform better 
jain dubes 
algorithms clustering data 
prentice hall englewood cliffs nj 
duda hart 
pattern classification scene analysis 

fukunaga 
statistical pattern recognition 
academic press san diego ca 
domany 
data model magnet 
neural computation 
el yaniv gdalyahu tishby 
new nonparametric pairwise clustering algorithm 
machine learning 
scholkopf platt shawe taylor smola williamson 
estimating support high dimensional distribution 
proceedings annual conference neural information systems nips 
mit press 
tax duin 
support vector domain description 
pattern recognition letters 
ben hur horn siegelmann vapnik 
support vector clustering method 
lipson siegelmann 
clustering irregular shapes high order neurons 
neural computation 
vapnik 
nature statistical learning theory 
springer verlag 
scholkopf 
support vector learning 
oldenburg verlag 
scholkopf burgess editors 
advances kernel methods support vector learning 
mit press 
saitoh 
theory reproducing kernels applications 
longman scientific technical 
platt 
fast training svms sequential minimal optimization 
scholkopf burges smola editors advances kernel methods support vector learning pages 
mit press cambridge ma 
fisher 
multiple measurements taxonomic problems 
annual part ii 
blake merz 
uci repository machine learning databases 
