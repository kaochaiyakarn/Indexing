acting uncertainty discrete bayesian models mobile robot navigation cassandra kaelbling kurien department computer science brown university providence rhode island cs may acting uncertainty discrete bayesian models mobile robot navigation anthony cassandra arc cs brown edu leslie pack kaelbling cs brown edu department computer science brown university james kurien jmk cs brown edu discrete bayesian models model uncertainty mobile robot navigation question actions chosen remains largely unexplored 
presents optimal solution problem formulated partially observable markov decision process 
solving optimal control policy intractable general goes explore variety heuristic control strategies 
control strategies compared experimentally simulation runs robot 
robot delivers items performs office environment needs able navigate robustly 
able overcome errors perception action worst getting lost period time able recover re localizing continuing task 
bayesian framework particularly appropriate modeling robot belief location generally state world 
supplies founded method robot current beliefs combining uncertain information gained sensing acting 
bayesian framework long time robotics results 
develop level architecture bayesian modeling done top level 
addition explore sub optimal control strategies bayesian belief state 
standard process applying bayesian framework modeling mobile robot uncertainty ffl build typically manually detailed metric cartesian map robot environment including positions perceptible features ffl represent true state robot position orientation pose global coordinates ffl represent belief state robot continuous parametric probability distribution typically gaussian poses ffl model uncertainty robot actions observations continuous parametric distributions typically gaussians ffl predict features observed match actual observations kalman filter update belief state actions matched observations ffl choose actions appropriate mean mode belief state 
tasks environments necessary know robot pose detail 
robust low level routines example local sensors drive door necessary know robot region outside door serves precondition low level operator 
cases coarse grained uncertainty model may appropriate 
pursue approach navigation process ffl develop set robust low level perception action routines driving corners detecting doors ffl build manually topological model environment nodes corresponding regions pose space grouped preconditions postconditions action routines ffl represent true state robot node topological map ffl represent belief state robot discrete probability distribution nodes ffl model uncertainty robot actions observations discrete distributions ffl bayes rule update belief distribution actions observations ffl instantaneous rewards discounted optimization criterion specify objective ffl take control actions robot belief state 
advantage approach applied just easily situations robot initially knows location situations robot initial belief state uniform idea 
robot begins take actions observations belief distribution change reflect new information need distribution continuous unimodal 
may case example robot know corner building 
means project discrete belief models 
project stanford university topological map combined robust low level behaviors 
belief state update heuristic model observational error error due actions 
xavier project carnegie mellon university full belief state update 
projects fairly ad hoc action strategies planning paths domain deterministic 
major contribution outline optimal control strategy number heuristic approximations comparing performance simulation runs real robot 
describing architecture system 
introduce optimal belief state update control strategies describe number heuristic control strategies 
experimental results comparing control strategies simulated experiments mobile robot 
system architecture system described level software architecture 
higher level referred navigator uses bayesian model environment robot actions observations 
context model navigator receives observations maintains belief distribution chooses actions 
role lower level software called pilot bridge gap actions observations model motions percepts underlying robot 
navigator layer navigator implemented assumes running office type environment corridors rooms oriented approximately orthogonal axes 
general approach described applied environment easily characterized network states operations move 
street networks underground systems 
provide navigator rough metric information office environment generates state map 
state consists location orientation compass directions 
locations discrete square areas size environment experiments square meter discretization 
map describes underlying connectivity states specifies type room corridor location 
location types help define ideal observations state 
example robot corridor type location looks direction adjacent location happens room type location ideally see doorway 
actions robot actions move forward turn left turn right op declare goal 
declare goal action robot indicate achieved objective 
action model specifies state action probability state result 
due regularities environment compute action model connectivity map action error model 
probabilities obtained informal experimentation fairly simple extension learn experiences 
table shows action probabilities experiments 
define actions terms action standard outcomes probabilities move forward turn left turn right op declare goal action noisy outcomes probabilities move forward turn left turn right op declare goal table action probabilities actions 
possible actual outcomes 
example action means probability robot move forward rotate degrees left 
rotating degrees right notated movement 
outcome occur particular state world example going forward wall front robot left state impossible outcome 
standard outcomes simulations robot noisy column simulation experiments explore effects increasing noise 
observations state robot able observation 
perceive nominal directions front left right doorway wall open undetermined 
observation combination percepts direction 
possible observations 
observation model specifies state action probability particular observation 
action model entire table need specified observation probabilities computed ideal observations specified map error model 
table shows conditional probabilities observations 
observations depend action taken robot move get additional useful sensor readings 
implementation observation probabilities turn left turn right action op results new observational information 
modeling observations way strong independence assumptions 
assume individual directional percepts time independent assume observations instant independent observations instant 
actual implementation observations pilot layer assumptions justified 
adversely affect results 
pilot layer pilot layer supports navigator model real robot case real world interface oo oo ideal observed standard noisy wall wall wall open wall doorway wall undetermined open wall open open open doorway open undetermined doorway wall doorway open doorway doorway doorway undetermined undetermined undetermined table conditional observation probabilities construct observation noise model 
mobile robot 
wheeled cylindrical synchro drive base ultrasonic sensors infrared sensors evenly distributed circumference 
infra red sensors fairly reliable short range proximity sensors emergency obstacle detection avoidance 
ultrasonic sensors give longer range proximity information due higher order specular reflections readings combined order get true picture surfaces world 
combined local occupancy grid basis high level feature detection 
occupancy grid pilot fuses ultrasonic measurements occupancy grid simplified variant algorithm moravec elfes 
pilot responsible solely local navigation occupancy grid maps features distance robot small range 
robot moves forward occupancy grid maintains small map information derived old ultrasonic data constantly scrolling grid trailing boundary 
short range ultrasonic readings considered simple certainty model place complex probabilistic model involving spread ultrasonic wave distance 
restricting grid local region allows numerous simplifications global occupancy grid methods rhino 
robot operating narrow corridors preprocessing ultrasonic data eliminate higher order specular reflection 
ultrasonic reading greater meters relevant grid discarded eliminating majority troublesome readings 
second grid robot centric data persists meters motion 
requirement robot dead reckoning accurate small intervals eliminating need error estimates correction 
third resolution ultrasonic sensors deliver small map requires relatively little time space maintain 
resolution enabled robust feature detection grid 
supporting observations pilot percepts actions highly dependent detecting features environment doors direction world axis 
features determined processing local occupancy grid specialized feature detection algorithms 
orientation closest world axis estimated integrating world axis estimates local occupancy grids robot travels 
current grid local estimate wall direction looking long parallel line segments occupancy grid image 
robot turns existing world axis estimate rotated degrees serves seed new estimate axis current direction travel 
world axis estimated simple search doors openings walls robot vicinity left front right robot 
rough line segment hough transform occupancy grid parallel perpendicular estimated axis wall observed 
similarly occupancy grid largely clear axis open observation results 
door observed ad hoc detector searches door indentations openings classified wall segment 
percepts left front right robot returned pilot action combined single observation 
supporting actions pilot layer supports actions move forward turn right op 
note pilot layer declare goal action effect op action 
turn left turn right actions attempt reorient robot independent current orientation axis orthogonal current 
simplify modeling location navigator discretizes world locations uniform distance apart 
result model specifies takes robot distance unit currently meter interesting features doors start opening separated multiples distance unit 
resulting semantics move forward action slightly complicated 
pilot detect new feature simply attempts move forward unit estimated axis travel 
obstacles encountered pilot attempt guide robot past objects approximately unit travel requested direction accomplished 
new feature encountered door new opening forward motion completed unit traveled 
robot appears travel number distance units features supporting discretized model navigator requires 
reliability important note pilot system completely reliable 
example occasionally mistakes hallway open door knows world axes 
occasionally mis estimates axis traveling diagonally world short distances 
informal experiments pilot deterministic slight proved disastrous 
navigator explicitly models abstraction pilot provides current level competence adequate navigation tasks attempted 
pomdp model section briefly describe class models known partially observable markov decision processes pomdps 
model developed operations research community introduced artificial intelligence researchers 
start describing simpler class markov decision processes 
markov decision processes mdp defined tuple hs ri finite set environment states reliably identified robot finite set actions state transition model environment function mapping elements theta discrete probability distributions reward function mapping theta specifies instantaneous reward robot derives action state 
write probability environment transition state state action taken write immediate reward robot action state process markov current state action provide information available predicting state 
policy mapping specifying action taken situation 
policy reward function value state expected value sums rewards received time step discounted far occur 
fl reward received tth step executing policy starting state closely related quantity value state action executed policy followed 
discount factor fl controls influence rewards distant 
fl value state determined entirely rewards received step generally interested problems longer horizon set fl near 
due properties exponential definition rewritten fl pr say policy dominates better 
policy optimal dominated policy 
markov decision process value fl possible compute optimal policy fairly efficiently 
shall refer optimal policy mdp express optimal value functions 
adding partial observability state completely observable add model observations 
includes finite set possible observations observation function mapping theta discrete probability distributions write probability making observation state having taken action belief state discrete probability distribution set environment states representing state robot belief currently occupying state 
set belief states 
write probability value assigned environment state belief state decompose problem acting partially observable environment components state estimator takes input belief state action observation returns updated belief state policy maps belief states actions 
state estimator constructed straightforward application bayes rule 
output state estimator belief state represented vector probabilities environmental state sums 
component corresponding state written se determined previous belief state previous action current observation follows se pr normalizing factor 
resulting function ensures current belief state accurately summarizes available knowledge 
constructing optimal policies key finding optimal policies partially observable case problem cast completely observable continuous space mdp 
state set belief mdp action set current belief state action joj possible successor belief states new state transition function defined os se pr pr defined 
reward function ae constructed expectations belief state ae belief mdp markov having information previous belief states improve choice action 
importantly agent adopts optimal policy belief mdp resulting behavior optimal partially observable process 
remaining difficulty belief process continuous established algorithms finding optimal policies mdps finite state spaces existing exact pomdp solution procedures computationally intractable 
number algorithms exist solving belief mdp efficient solve problems states observations 
heuristic control strategies computationally intractable compute optimal pomdp control strategy simplest environments consider number simple heuristic control strategies fairly ad hoc principled motivations 
belief replanning implemented strategy replan slight variation strategy 
algorithm starts assuming world state planning path goal deterministic idealization domain 
addition generates sequence predicted world states traversed nominal trajectory followed 
robot plan updating belief state checking step world state belief distribution equal predicted state 
cycle begins planning current state 
main difference implementation original approach computing bayesian belief state original approach model observational uncertainty action uncertainty compute certainty factors world states 
mdp algorithms pomdp model mdp model probabilistic observations 
mdp algorithms section solution underlying mdp obtained quickly large models 
state mls policy finds world state highest probability executes action optimal state mdp mls argmax 
real difference method belief replan method optimal mdp policy plan simplified deterministic version environment 
may similar current domain potentially dangerous consequences actions falling staircase mdp take account 
voting method start computing probability belief distribution action optimal indicator function value argument true 
choose action optimal vote argmax 
similar action choice method xavier 
difference deterministic planning algorithm choose best action world state finding best action underlying mdp 
qmdp method refined version voting method votes state actions value mdp argmax delta contrast winner take behavior voting method 
technique optimal agent uncertainty existed single step 
degree uncertainty general useful robot actions depend degree uncertainty 
optimal policy pomdp model seamlessly integrates concerns acting order reduce uncertainty acting order achieve goal 
infeasible solve pomdp directly algorithms section explicitly trade concerns 
methods consider myopic strategy actions gain information 
entropy probability distribution gamma log log 
lower value certain distribution 
robot confused entropy high reasonable take actions reduce entropy belief state 
define expected entropy belief state action ee summation possible belief states 
possible consider expected entropy resulting longer sequences action quickly quite expensive compute 
simple strategy follow mdp policies long entropy belief state high act explicitly reduce entropy myopic strategy action reduces expected entropy belief state 
cases benign entropy belief state confusion states require action 
cases reason try reduce entropy 
reason compute entropy distribution 
entropy threshold oe genuine confusion action take seek reduce 
action entropy policy ae ae argmax oe argmin ee entropy weighting ew approach exclusively pursuing goal exclusively trying reduce entropy policy weights aims normalized form entropy 
define normalized entropy belief state uniform belief state maximum entropy factor control relative weighting entropy 
yields measure 
robot certain knowledge world state maintain certainty goal exp start east exp start east exp start north east synthetic office environment goal south exp start north exp start north south synthetic office environment long term value take appropriate actions 
note delta upper bound value belief state hand robot completely confused value considerably lower 
way estimate consider value state resulting performing fixed sequence actions designed disambiguate state performing optimally 
values may optimistic provide picture value confused 
sequence length consisted repetition actions followed single turn left action 
sequence homing sequence version domain 
define weighted entropy value ev delta gamma delta associated value eq ae fl ev policy ew argmax eq 
simulation experiments tested control strategies simulation variety synthetic office environments easier run extensive experiments 
confirmed results running experiments model real office environment 
ae algorithm oe ew algorithm 
synthetic environments figures show layouts synthetic environments 
location represented states pomdp model major orientation 
dark locations rooms connected corridors doorways 
conducted separate sets experiments synthetic environment different start goal state sets see figures 
set experiments robot knows state started second set small set similar looking states robot started 
set experiments robot starting information starting state 
start state experiment robot starts randomly states belief state initialized uni exp goal exp goal exp goal south south west exp start west exp start north exp start east synthetic office environment goal east exp start west exp start south exp start east synthetic office environment form distribution set possible start states 
domains reward performing action declare goal goal state state action pairs 
alternative give reward entering goal state 
navigation rarely useful sake robot perform actions arrives goal succeeded arrived goal knows believes high degree 
trials terminated declare goal action executed steps performance measured discounted sum rewards fl starting initial state 
results reported averages trials 
table shows results different methods synthetic office environments starting state known 
addition heuristics include results omniscient agent knows true state times 
gives coarse measure hard problem confused achievable optimal policy 
performed test determine significant differences algorithms 
best performance outlined significantly worse lightly outlined 
variability methods part 
approximation method particular circumstances fail 
data points methods appear significantly worse examples circumstances 
mls method fairly robust environments showing information belief state needed algorithm mls voting mdp replan ew ae omniscient table experiment known starting state 
algorithm mls voting mdp replan ew ae omniscient table experiment multiple possible start states 
algorithm mls voting mdp replan ew ae omniscient table experiment uniform starting belief 
exceedingly situation 
situation address agent certain starting state 
experiments shown table possible starting states possible states office similar immediate surroundings 
case see methods performing poorly 
mdp voting methods able reach goal office experiment 
cycle set actions making progress goal 
cycling behavior seen performance environments 
fact mdp method significantly better office experiment particular configuration starting states goal state allows behave nearly omniscient method 
problem selecting voting mdp methods easy know priori particular environment bring best worst algorithms 
mls method performs situations prefered choice known particular problem instance 
final situation explore difficult robot equally start states 
situation initial belief distribution uniform states 
table shows results applying algorithms situation 
results show mdp voting methods usually perform uncertainty high 
mls method quite robust suggests moderately noisy environments best choice different types environment layouts starting beliefs 
results show ae method useful enhancement voting method 
significantly worse voting method outperform methods 
believe entropy methods merit action set contains explicit information gathering actions 
unfortunately models experiments rich set actions action provide significantly information 
noisier synthetic environments navigation problem harder noise action observation increases 
order gauge effects noise various methods repeated experiments noisy action observation probabilities shown previously tables 
space precludes inclusion full data set merely touch results 
mdp method universally bad configurations starting beliefs 
trials declare goal 
direct result mdp assumption completely disambiguated step 
noise high confident belief goal 
prefer delay declaring goal doing action hopes knowing action 
belief replan method bad mdp inferior mls voting algorithms 
mls method best choice environments 
note entropy methods help noisy environments 
actions observations fair amount noise action significantly reduce entropy 
real office environment shows layout robot environment pomdp states 
layout standard noise model simulation actual robot 
experiments different starting belief state configurations 
experiments goal state location facing east 
experiments known starting location east east south roughly corresponding differing physical distances goal 
experiments starting belief locations previous starting states added state east east north respectively roughly distance similar immediate observations 
final experiment conducted uniform starting belief states 
table shows simulation results trials 
results consistent synthetic office layout simplified version real environment 
entropy methods significantly better methods runs robot quite time consuming ew ae methods simulated actual robot experiments 
robot experiments order verify simulation results duplicated experiments table exception experiment real robot running unmodified computer science department 
due time required robot complete experimental trial trials run 
trials algorithm run experiments real office environment 
experiment alg 

mls voting mdp replan 
table simulations robot office environment 
trials starting location run algorithm experiments 
average discounted reward algorithm shown table 
simulation algorithms proved adequate starting state known 
reached goal directly trial despite occasional errors observation action 
experiments starting belief state bimodal distribution mls reached goal trial 
experiments mdp replan issued declare goal action state near similar goal fact goal 
larger number trials reveal mls 
experiment appropriate initial actions possible starting states odds algorithms spend half required actions collapsing belief state actual state making direct run goal 
simulation experiment tested performance uniform belief state randomly chosen start states algorithm 
practical robot informal runs ensure mls succeed reaching goal state uniform initial belief state 
voting performed significantly worse reality simulation suggested 
voting lead cycles belief states actions progress goal 
behavior experiment alg 
mls voting mdp replan table experiments robot 
pronounced simulation persistent cycles 
simulator models pilot navigator action observation errors determined stationary probability distribution independent robot real pose world 
cycles belief states actions soon disrupted failed action erroneous observation 
reality likelihood errors robot dependent current pose environment 
configurations sets actions robot extremely reliable allowing action observation cycle associated belief state cycle persist indefinitely 
initial configurations experiments reliably cause voting algorithm enter cycle place turns persists hundreds actions 
simulator strict adherence simplified probability model cause simulated omniscient controller encounter errors leading worse performance simple tasks algorithms running robot 
accepting inevitable errors low level piloting account recover robust high level navigation strategy 
optimal control strategy easily computed motivated heuristic strategies perform better ad hoc ones 
preliminary study great deal remains done 
control strategies explored essentially myopic respect uncertainty 
able take long string actions order disambiguate belief state 
apply sophisticated methods machine learning literature find approximations true value function pomdp 
expect improve performance starting location extremely uncertain 
simple domains explored exercise abilities pomdp models control active perception 
domain robot perceptual actions associated cost appropriately controlled model 
extend set actions include visual operations provide better information 
big shortcomings modeling environment pomdp models dependence world static 
system cope dynamic events incorporating events noise model deal transient changes 
fundamental change layout occurs door closes robot performance deteriorate rapidly especially changes 
line learn world model experience techniques adapted hidden markov models currently pursued learning probabilities topology koenig simmons 
acknowledgments shatkay implemented algorithm find homing sequences 
supported part nsf iri iri 
buhmann burgard cremers fox hofmann thrun 
mobile robot rhino 
ai magazine summer 
anthony cassandra leslie pack kaelbling michael littman 
acting optimally partially observable stochastic domains 
proceedings twelfth national conference artificial intelligence seattle wa 
te cheng 
algorithms partially observable markov decision processes 
phd thesis university british columbia british columbia canada 
richard duda peter hart 
hough transform detect lines curves pictures 
graphics image processing 
sven koenig reid simmons 
unsupervised learning probabilistic models robot navigation 
proceedings ieee international conference robotics automation 
zvi kohavi 
switching finite automata theory 
mcgraw hill new york 
leonard hugh durrant whyte 
localization tracking geometric beacons 
ieee transactions robotics automation 
michael littman anthony cassandra leslie kaelbling 
learning policies partially observable environments scaling 
machine learning proceedings twelfth international conference pages san francisco ca 
morgan kaufmann 
michael littman anthony cassandra leslie pack kaelbling 
efficient algorithm dynamic programming partially observable markov decision processes 
technical report cs brown university providence rhode island 
william lovejoy 
survey algorithmic methods partially observed markov decision processes 
annals operations research 
george monahan 
survey partially observable markov decision processes theory models algorithms 
management science 
moravec elfes 
high resolution maps wide angle sonar 
proceedings ieee conference robotics automation pages 
nourbakhsh rob powers stan birchfield 
office navigating robot 
ai magazine pages summer 
christos papadimitriou john tsitsiklis 
complexity markov decision processes 
mathematics operations research 
ronald parr stuart russell 
approximating optimal policies partially observable stochastic domains 
proceedings international joint conference artificial intelligence pages 
morgan kaufmann 
martin puterman 
markov decision processes 
john wiley sons new york 
reid simmons sven koenig 
probabilistic navigation partially observable environments 
fourteenth international joint conference artificial intelligence pages montreal canada 
morgan kaufmann 
richard smallwood edward sondik 
optimal control partially observable markov processes finite horizon 
operations research 
edward sondik 
optimal control partially observable markov processes 
phd thesis stanford university stanford california 
chelsea white iii 
partially observed markov decision processes survey 
annals operations research 
