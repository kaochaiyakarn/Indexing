ieee transactions neural networks vol 
september general framework adaptive processing data structures paolo frasconi member ieee marco gori senior member ieee alessandro sperduti structured organization information typically required symbolic processing 
hand connectionist models assume data organized relatively poor structures arrays sequences 
framework described attempt unify adaptive models artificial neural nets belief nets problem processing structured information 
particular relations data variables expressed directed acyclic graphs numerical categorical values coexist 
general framework proposed regarded extension recurrent neural networks hidden markov models case acyclic graphs 
particular study supervised learning problem problem learning transductions input structured space output structured space transductions assumed admit recursive hidden statespace representation 
introduce graphical formalism representing class adaptive transductions means recursive networks cyclic graphs nodes labeled variables edges labeled generalized delay elements 
representation possible incorporate symbolic subsymbolic nature data 
structures processed unfolding recursive network acyclic graph called encoding network 
doing inference learning algorithms easily inherited corresponding algorithms artificial neural networks probabilistic graphical model 
index terms graphical models graphs learning data structures problem solving recurrent neural networks recursive neural networks sequences syntactic pattern recognition 
integration symbolic subsymbolic systems fundamental research topic development intelligent efficient systems capable dealing tasks nature purely symbolic subsymbolic 
common opinion scientific community quite wide variety real world problems require hybrid solutions solutions combining techniques neural networks fuzzy logic genetic algorithms probabilistic networks expert systems symbol techniques 
popular view hybrid systems numerical data processed subsymbolic module structured data processed symbolic counterpart system 
manuscript received march revised november 
supported part italian murst italian national research council 
frasconi dipartimento di sistemi informatica universit di firenze firenze italy 
gori dipartimento di ingegneria dell informazione universit di siena siena italy 
sperduti dipartimento di informatica universit di pisa pisa italy 
publisher item identifier 
ieee unfortunately different nature numerical structured representations tight integration different components difficult 
approaches hybrid systems role prior knowledge providing partial specification transduction learned 
interesting promising approaches incorporation symbolic knowledge adaptive models neural networks inherently limited complementary role played learning symbolic knowledge symbolic rules injected harder learning 
propose different view hybrid systems incorporation symbolic knowledge involve primarily desired input output transduction nature data 
number different application domains data strongly structured processing ignore topological information expressing relations different portions data 
times real word problems data significantly structured composing features subsymbolic nature 
put forward section adaptive models typically processing arrays sequences adequate process complex data structures 
show structured information represented processed framework amenable neural belief networks 
possibility represent process structures neural probabilistic fashion greatly increases potential integration subsymbolic symbolic components hybrid system 
remainder section argue relevance structured information application domains motivate formulation general framework adaptive computation data structures 
section ii formalize structured learning domains means directed acyclic graphs numerical categorical values coexist 
similar data organization common number different application domains briefly sketched section 
section iii introduce deterministic probabilistic structural transductions particular recursive state space representations associated graphical models 
section iv classes adaptive models structural processing deterministic setting temporal processing takes place recurrent neural networks extended case graphs connec frasconi general framework adaptive processing data structures fig 

typical chemical compound naturally represented undirected graph 
models refer recursive neural networks setting hidden markov models extended hidden recursive models 
applications adaptive recursive processing reviewed section guidelines development theory proposed outlined section vi 
learning structured information application domains application domains information relevant solving problem encoded implicitly relationships basic entities 
example chemistry chemical compounds usually represented undirected graphs 
node graph atom group atoms arcs represent bonds atoms see fig 

fundamental problem chemistry prediction biological activity chemical compounds 
quantitative structure activity relationship attempt face problem relying compound structures 
biological activity drug fully determined interaction active molecules 
unfortunately discovering hard expensive 
assumption direct correlation activity structure compound approach way approaching problem comparing structure known active compounds inactive compounds focusing similarities differences 
aim discover substructure set substructures characterize activity generalize knowledge new compounds 
example software engineering important example application uses structured information certainly software engineering 
major goals software engineering evaluate quality software 
evaluation usually metrics correlated properties interest 
number metrics see mccabe complexity developed try codify properties portion program numerically 
features usually detailed recurrent neural networks recursive neural networks reduce model domain restricted sequences 
historical reasons shall name recurrent neural networks referring models operating sequences 
fig 

portion software code corresponding flowgraph 
metrics software evaluation turn functions acting graph domains 
intermediate representation advantage sense independent specific language preserving essential static dynamic aspects program 
example intermediate representation dependence graphs 
dependence graph statements represented nodes directed edges represent statement ordering implied dependencies source program 
depending specific application different kinds dependence graphs control flow graphs control dependence graphs data dependence graphs instance dependence graphs 
commonly accepted procedural languages expressed flowgraph number basic elements decision node junction node node 
set flowgraphs derived possible programs see fig 

software metric function estimate complexity portion software 
aim indicator quality testability reusability maintainability program 
example problem solving artificial intelligence rich source applications structured information related problem solving artificial intelligence 
perform search tree typically gives rise combinatorial explosion search space 
examples systems expensive search theorem provers deductive databases expert systems 
systems search solution proof exploring branch search tree defined problem hand see fig 
section 
exhaustive search guarantees completeness solution finite time 
brute force approach practical feasible problems small ieee transactions neural networks vol 
september fig 

directed acyclic graph representing logical term ay aaa 
dimensions 
facing larger problems commonly recognized heuristics aimed identifying promising paths search tree speed search significantly 
known search algorithm guided heuristics uses dynamic programming principle seeking optimal cost path 
heuristic evaluation function computes ratings inference steps states 
ratings select inference step performed state explored 
order useful heuristics simple majority cases understood approximation complete perfect search guiding strategy oracle 
unfortunately heuristics expensive devised typically summarize knowledge expert domain 
specific slight change domain may require new heuristic devised scratch 
way overcome problems learn heuristic supervised fashion data samples states inference steps ratings obtained solutions 
tasks performed learning control heuristic finding rating applicable rules current context selecting subgoal regarded problems learning classification logical terms 
fact positive samples states inference steps solution paths search tree negative samples states inference steps failure paths 
terms order logic easily represented directed acyclic graphs shown fig 

vertices labeled function names edges link functions arguments 
constants considered functions zero arity leaf vertices 
example explain concepts definitions intuitively proposed computational scheme 
example pattern recognition pattern recognition source applications may interested adaptive processing data structures 
recognized early syntactic structural pattern recognition premise structure entity important classification description 
fig 

logo corresponding representation symbolic subsymbolic information 
fig 
shows logo corresponding structural representation tree nodes components properly described terms geometrical features 
representation invariant respect translations naturally incorporate symbolic numerical information 
course extraction robust representations patterns minor problem 
presence significant amount noise significantly affect representations strongly symbols 
depending problem hand structured representation derive emphasize symbolic subsymbolic information 
example logo shown fig 
significantly corrupted noise unreasonable recognize word sum case just consider words subsymbolic information collected single node 
motivations related approaches common feature shared application domains sketched previous section required predictions simple arrays features sequences dynamic data structures incorporating numerical information 
argue machine learning models conceived dealing sequences straightforwardly adapted process data structures 
instance processing binary trees recurrent neural networks hidden markov models take place sequential representations traversing trees 
approach major drawbacks 
number nodes grows exponentially height trees short trees give rise long sequences making learning hard 
second sequential mapping data structures break nice regularities representation nested parenthesis way creating sequential representation possible reconstruct tree 
way providing unique sequential representation binary trees considering inorder preorder visits 
recurrent neural networks known problem learning long term dependencies 
frasconi general framework adaptive processing data structures inherently associated data structure making generalization hard 
years interesting approaches representation processing structured information proposed field connectionist models 
hinton introduced concept distributed reduced descriptors order allow neural networks represent compositional structures 
concrete examples distributed reduced descriptors recursive autoassociative memory raam pollack holographic reduced representations plate 
labeling raam model proposed extension raam advances access content capabilities discussed 
possible carry synthesis distributed reduced descriptors fixed outdegree directed labeled graphs 
field natural language processing results classification distributed representations syntactical trees devised typology dialogue acts obtained 
ii 
definitions background topics structured domains instances learning domain structured pieces information described annotated directed ordered acyclic graphs doag 
doag mean dag vertex set vert edge set edg vertex total order edges leaving defined 
example case graphs representing logical terms see fig 
order outgoing edges immediately induced order function arguments 
problems structure classification shall require doag empty possess supersource vertex vertex vert reached directed path starting reasons requirement related processing scheme defined section iii 
note doag possess supersource possible define convention adding extra vertex minimal number outgoing edges supersource expanded doag 
doag denote set parents set children de set descendants pa set ancestors indegree cardinality set pa outdegree cardinality set ch shall denote class doag having maximum indegree maximum outdegree logical terms example see fig 
maximum outdegree corresponds maximum arity functions considered 
generic class doag bounded unspecified indegree outdegree simply denoted 
graphs storing structured information marked labeled sense vertices edges contain sets domain variables called labels 
vertices edges containing empty set variables said unlabeled 
assume labels graph disjoint sets 
domain variables contained labels called attributes 
general attributes numerical take continuous values categorical take discrete symbolic values 
presence edge marked graph indicates variables contained related way 
edge labeled variables characterize relationship variables variables graphs edge labels reduced graphs having labels nodes 
straightforward method reducing structures labeled edges structures unlabeled edges move label attached edge leaving node label attached node assume additional equivalence relation defined domain variables variables equivalence class type semantics 
case say labels similar contains element equivalence class intersect subset equivalence classes 
graph uniformly labeled labels similar 
example logo recognition problem described fig 
labels vertex variables perimeter area shape measured corresponding image element 
perimeter area shape equivalence classes set domain attributes 
case tree uniformly labeled 
note attributes numerical categorical 
explicitly stated assume graphs uniformly labeled 
assumption may take representative equivalence class form set representatives domain variables correspond set attribute names shape perimeter area logo example 
labels fixed size tuples attributes 
general notation labels defined follows 
number equivalence classes 
context called label size 
assume conventional order denote representative th equivalence class order chosen really matters notation distinguishing attribute names 
set representatives denoted standard notation shall uppercase letters variables lowercase letters realizations 
categorical variable set admissible states alphabet denoted numerical variable shall assume realizations real numbers set possible realizations labels 
set referred label space label domain 
label spaces denoted calligraphic letters 
size label space denoted uniformly labeled doag denoted boldface uppercase letter corresponding label space graph 
example denotes doag labels labels accessed vertex subscripts denotes label attached vertex ordered set vertices denotes ordered set labels attached vertices ieee transactions neural networks vol 
september fig 

example io isomorph transduction logical terms domain 
data structure doag obtained ignoring node labels referred skeleton denoted skel clearly data structures distinguished different skeletons skeleton different node labels 
class data structures defined local universe domain skeleton denoted give examples structured domains briefly recall learning problems domains approached 
turns machine learning tools easily deal general classes data structures 
example trivial graphs trivial graph single node edges 
class class trivial data structures 
data kind referred trivially structured 
models techniques dealing data largely predominant machine learning literature 
instance feedforward neural networks decision trees bayesian classifiers assume instance described simple tuple predictive attributes encoded fixed size arrays real numbers 
models kind referred unstructured relation defined variables characterize instances learning domain 
example strings sequences consider class linear chains finite alphabet 
topological order linear chains total order associated serial order strings 
class corresponds set strings finite length alphabet free monoid known kleene closure topological order linear chains associated temporal order discrete time process exists bijection nodes linear chains natural numbers 
words node linear chain unambiguously associated discrete time index way continuous space class describes set finite sequences continuous vectors 
early attempts reported dealing serially ordered data adaptive models trivially structured data 
known example nettalk feedforward neural network trained map english text sequence serially ordered characters sequence phonetic acoustic parameters speech synthesis 
difficulty similar approaches variable length sequences transformed fixed width vectors 
typically achieved choosing appropriate moving window fixed size time delay neural networks 
significant drawback approach depth temporal dependencies fixed advance learned data 
difficulties recognized early connectionist community see adaptive models dealing sequentially ordered data known 
significant examples neural architectures recurrent neural networks rnn probabilistic models hidden markov models hmm input output hmm iohmm 
example binary trees consider class binary trees making predictions data structures context node split separate ordered pieces information left right context 
principle unstructured models adaptively process binary tree data structures 
fact rely extension moving window approach sequences tree encoded fixed size vectors subsequently input unstructured model making predictions 
approach motivated fact unstructured models feedforward neural networks fixed number input units trees variable size 
encoding process defined priori 
temporal windows degree freedom involves window length encoding binary trees fixed size vectors involves arbitrary structural choices requires selection fixed size subgraph 
generalized shift operators discrete time operator applied temporal variable essentially symbolic transformation maps expression involving variable measured different time steps 
particular shift operator applied returns variable time graphical framework finite length temporal sequence corresponds linear chain 
supersource head chain corresponds time step sequence time indexes decrease direction arrows 
thought operator gives access unique child node sequence graphically represented linear chain 
shift operators composed neutral operator defined general doag considered ordered set generalized shift operators defined associated discrete time operators introduced temporal domain 
example gamma operator defined ic aa constant zero 
review discrete time operators 
frasconi general framework adaptive processing data structures ordered set children 
denote shift operator associated th child node 
class class linear chains subscript omitted 
special classes graphs descriptive subscript notations 
example considering class binary trees denote operators associated left child right child respectively 
considering labeled graph local universe domain expression denotes label th child node example fig 
denoting supersource node labeled label child label second child 
note nonempty label shift operators composed expressions involving composition shift operators specify directed paths doag 
note composition shift operators commutative 
example fig 
composition yields label composition yield empty label node labeled children 
considering class ary trees inverse shift operators defined probabilistic graphical models shall describe general graphical formalism data structure processing 
formalism largely inspired causal semantics commonly attached probabilistic graphical models known belief networks causal networks briefly review models section 
belief conditional independence networks popular artificial intelligence tool reasoning probabilistic expert systems 
general belief networks effectively statistics representing manipulating complex probability distributions 
matter fact learning systems boltzmann machines multilayered perceptrons input output hidden markov models just mention easily regarded particular graphical models :10.1.1.133.6544
belief network annotated graph nodes represent random variables universe discourse missing edges encode set conditional independence statements variables 
particular state knowledge semantics belief networks determine collecting evidence set variables modifies knowledge set variables 
specifically denote universe discourse disjoint subsets variables said conditionally independent denoted conditional independence model collection triplets holds true 
graph nodes associated variables independency map independence model nodes associated graphically separated subset nodes triplet graphical separation criteria verifying conditional independence defined undirected graphs known markov networks directed acyclic graphs dag known bayesian networks chain graphs 
criteria referred separation separation respectively 
belief networks limited qualitatively encoding conditional independencies quantitatively specify parameters probability distribution universe discourse 
particular case bayesian networks bn shown table universe discourse factored denotes parents bn specified dag local density models attached node set parameters local density 
case categorical variables simplest choice unrestricted multinomial model local densities conditional probability table cpt entries case numerical variables common choice local densities gaussian model 
theoretical results graphical models particular results concerning learning hold general case local densities belonging exponential family 
directed belief networks immediate interesting interpretation terms probabilistic causal relationships 
set parents variable subset universe discourse direct causal impact notion causality need restricted probabilistic relationships directed graphical models extended include deterministic causal relationships 
shall consider deterministic nodes computing deterministic function state parents 
formally iff denotes dirac function artificial neural networks best known examples graphical models involving deterministic nodes 
shall tacitly assume neurons connectionist networks modeled deterministic nodes graphical model 
iii 
general aspects structural processing generally speaking problem learning data structures consists making predictions knowledge labeled graph 
framework assumed essentially probabilistic 
setting unsupervised learning problem formulated estimation density supervised learning problem formulated estimation conditional distribution output doag input doag local universe domains generally distinct 
shall introduce distinct related classes models supervised learning 
detailed classes rely hidden state space representation states ieee transactions neural networks vol 
september random variables 
relationships input output state variables may deterministic probabilistic 
kind computation typical artificial neural networks neural activities deterministic functions random variables response output neurons interpreted position parameter conditional density output input conditional expectation kind computation typical bayesian networks nodes labeled random variables directly obtained running probabilistic inference algorithm having entered evidence input nodes 
distinction classes models matter interpretation 
particular neural networks interpreted fully probabilistic directed graphical models logistic function weighted sums read conditional density units parents 
problem interpretation neural networks densely connected graphs exact inference algorithms quickly intractable moderately large networks 
structural transductions order describe general properties models supervised learning data structures useful abandon temporarily probabilistic setting assume data generated deterministic transduction structured spaces 
generally speaking deterministic transduction binary relation defined structured spaces 
relations functions considered 
learning general functions challenging open research problem 
section restrict learning domain characterize subclass transductions reasonably easy build adaptive models 
particular general function modify skeleton structure processed may skel case sequences allowing transduction modify skeleton essentially means allow input output sequences different lengths 
sequential transductions called synchronous time step input label consumed output label emitted inputs outputs share time scale 
concept synchronism generalized transductions structures asking input output structures share skeleton 
specifically transduction io isomorph restrict attention io isomorph transductions 
io isomorph transduction algebraic depends clearly problem learning algebraic transductions reduced conventional learning problem input output instance spaces 
contrast predictions structural transductions algebraic depend contextual information possibly stored input data structure 
example transductions operated temporal dynamical systems recurrent neural networks algebraic 
io isomorph structural transduction causal depends subgraph induced causal transductions algebraic need kind memory store information input labels descendants node example consider class linear chains 
case memory device stores information past events output time depend input time past inputs sequential transductions kind realized dynamical systems memory systems normally associated concept internal state 
causality dynamical systems necessary sufficient condition existence internal state 
issues generalized extended data organized form doag give example definitions consider logical terms domain suppose output label space binary alphabet indicating class positive negative terms 
shown fig 
transduction io isomorph input output graphs share skeleton 
label generic node thought class subterm rooted node class subgraph induced label supersource clearly class term 
looking top fig 
notice case algebraic 
fact example output label subterm depends context associated labels nodes algebraic outputs supersource second child supersource equal 
looking bottom fig 
see causal 
fact class subterm second child supersource depends contextual information supersource 
causal class term depended bottom context associated arguments say io isomorph transduction admits recursive state representation exists structure space exist functions fixed size array labels sets vari frasconi general framework adaptive processing data structures ables attached children internal state doag 
definition referred state transition function referred output function 
observed special case sequences node corresponds discrete time point ch contains single node corresponds previous time point 
case sequences representation corresponds usual state space representation dynamical systems example control systems 
intuitively data generated transductions admit recursive representation explained hidden structure skeleton matches input output skeletons 
representation characterizing models recursive neural networks deterministic relations input state output variables 
context assumed functions depend trainable parameters connection weights 
states updated recursive message passing scheme state label updated state labels corresponding children serial computer achieved traversing graph skel order defined reversed topological sort nodes skel parallel computer states simultaneously updated propagating frontier set nodes supersource 
note fixed size array elements maximum outdegree input doag 
vertex children order apply necessary specify states associated missing children elements associated indexes ranging 
particular leaf children missing 
states associated missing children accessed basis induction terminates recursive traversal doag 
concept basis recursion direct correspondence concept initial state classic dynamical systems recurrent neural networks deal sequentially ordered data 
initial states models may assumed fixed may learned data 
case general doag necessary specify set state variables associated basis recursion 
set referred frontier label 
frontier labels correspondence missing children data structure processed 
words array filled frontier state label children missing 
equations interpreted terms causal dependencies input state output variables 
causal dependency deterministic models related conditional independency probabilistic belief network 
main difference variables functions topological sort dag linear order vertices edge general setting conceived different frontier states associated different children 
example case binary trees left right frontier states take different values parents conditionally independent rest remaining variables parents 
belief networks general case functions may thought degenerate conditional densities probability mass concentrated single value variable conditionally independent rest markov blanket set nodes formed joining children parents parents children 
note noncausal transductions admit recursive state representation causality property global map explained terms local causal dependencies recursive representation 
state transition function output function dependent causal io isomorph transduction said stationary functions independent node stationarity defined way generalizes concept time invariance applies dynamical systems operating class linear chains 
cases structural processors produce outputs structured 
example problem data structure classification single categorical variable commonly associated input structure 
remarkable type prediction give specialized definition transductions output space structured 
supersource transduction function defined recursive representation denotes supersource input graph 
supersource transductions map input structure output structure skeleton single node trivial graph 
alternatively supersource transductions thought io isomorph transductions output labels empty sets variables label attached supersource worth mentioning stationary transductions admit recursive representation compute function 
example consider doag shown fig 

supersource stationary transduction described necessarily map graphs output regardless form functions fact seen state variables leaves equal propagating state frontier easily seen state variables supersource equal predicted output 
example tree automata best known stationary transductions operate structures complex linear chains realized tree automata briefly recall 
sake simplicity limit discussion binary tree automata 
introduce accepting automata 
frontier root automaton fra tuple transition function maps triple ieee transactions neural networks vol 
september fig 

different doag necessarily mapped output causal stationary supersource transduction 
state symbol shift operators pointing left right child respectively 
frontier state 
computation automaton proceeds external nodes root binary tree accepted iff set accepting states 
accepting tree automata fit state space representation realize supersource transductions 
translating tree machines easily defined introducing output alphabet replacing output function maps pair translating tree machines defined way clearly realize causal io isomorph transductions fact pair functions specifies recursive representation transductions 
probabilistic transductions probabilistic transduction joint density defined concepts defined qualifying deterministic transductions easily generalized probabilistic transductions constraining shall say probabilistic transduction generic property io isomorphism causality exists deterministic transduction property example probabilistic transduction io isomorphic similarly say io isomorph probabilistic transduction admits recursive state representation exists structure space exists state transition densities emission densities 
case stationary transductions densities node deterministic transductions set assumed filled frontier states missing children 
probabilistic transduction said stationary state transition emission densities independent vertex supersource probabilistic transductions defined assuming trivial graph supersource shown section identical factorization bayesian networks universe discourse contains input state output variables 
graphical models structural processors give graphical notation recursive representation causal transductions 
proposed formalism describes probabilistic deterministic transductions 
io isomorph causal stationary transduction canonical recursive network directed possibly cyclic labeled graph follows 
defined nodes node labeled distinct variable chosen sets input state output variables 
edges describe causal dependencies input state output variables recursive representation edges labeled shift operators 
generic variables 
edge labeled neutral shift operator indicates causally dependent similarly edge indicates causally dependent edges incident output nodes labeled neutral shift operator 
edges incident output state nodes leave output nodes 
node labeled input variables zero indegree 
cycles edges labeled neutral shift operator allowed 
noncanonical recursive networks defined labeling edges composition shift operators abbreviated set variables involved processing data structure graphically represented recursive network associated transduction 
essentially recursive network template unfolded expanded skeleton input data structure io isomorphism assumption matches skeleton output structure 
resulting labeled graph function recursive network input output data structures called encoding network transduction denoted nodes edges constructed follows 
vertex set cartesian product vertex sets denotes th input variable node denotes frasconi general framework adaptive processing data structures fig 

pair input output doag skeletons belong class 
recursive network io isomorph transduction 
example state space components iy py 
encoding network input output doag recursive network 
sake simplicity edges leaving frontier state variables drawn incident nodes located bottom left portion encoding network 
th state variable node denotes th output variable node edge set obtained follows 
denote vertices directed edge edge labeled expression algorithm builds skeleton encoding network associated recursive network skeleton input output doag algorithm build encoding network vert edg foreach vert foreach vert vert vert foreach edg foreach vert edg edg edg edg return ieee transactions neural networks vol 
september fig 

recursive network hrm 
example encoding network construction shown fig 

note arrows encoding networks reversed respect direction arrows input data structure 
computation recursive causal transductions proceeds frontier supersource 
shift operators applied nodes encoding network 
particular returns state label attached th child skeleton th parent encoding network 
straightforward recognize case probabilistic transductions encoding network constructed algorithm bayesian network density factored 
detailed section transduction implemented means recursive neural networks encoding network feedforward neural network 
iv 
models adaptive structure processing hidden recursive models hidden recursive models hrm class probabilistic models structure processing hidden discrete states 
recursive network general hrm shown fig 

input output nodes recursive network contains single node corresponding hidden discrete state variable node model class skeletons node recursive connections self loops labeled denote hidden state space 
size state space usually design choice 
alternatively model selection techniques may employed learning data 
moore model edges input variables parents parent output variables mealy model additional edges see fig 

encoding network associated hrm bayesian network conditional probability tables shared replicas basic cell 
example hidden markov models hidden markov models hmm known device learning extensions include multiple state variables conceived described 
fig 

recursive networks standard hmm iohmm respectively 
encoding networks standard hmm iohmm respectively 
fig 

binary tree corresponding hrm dark square nodes denote frontier states 
distributions temporal sequences linear chains framework 
standard hmm parametric model stochastic process generated underlying finite state markov chain output distribution associated state state transition 
state variables hmm satisfy markov conditional independency graphically represented fig 

fig 
shows recursive network standard hmm input output hmm iohmm extension hmm supervised learning temporal domains 
main difference recursive network iohmm shown fig 
contains node labeled input variable 
hmm iohmm correspond simplest form hrm dealing class linear chains intuitively iohmm recurrent neural networks relationship hrm recursive neural networks described section iv 
network employed classification structures output node supersource input structure fig 
shows example encoding network hrm classifies binary trees 
hmm depicted different graphical form known state transition diagram 
state transition diagram directed possibly cyclic graph nodes labeled states contrast recursive nets nodes labeled variables absence arc state state indicates probability making transition zero 
frasconi general framework adaptive processing data structures fig 

recursive diagram binary tree hrm 
slice unfolded network variables involved shown 
shaded variables receive evidence learning 
important difference respect recursive neural networks hrm naturally deal supervised unsupervised learning problems 
case unsupervised learning input variables model generative form hidden states causally affect labels observed structure interesting advantage hrm respect recursive neural networks missing input data dealt simply entering evidence encoding network labels observed 
evidence normally entered visible nodes encoding network 
case supervised learning input output nodes visible training output labels play role targets 
probabilistic inference assesses probability hidden states observed data solving structured credit assignment problem 
making predictions input nodes instantiated probabilistic inference yields case unsupervised learning input nodes evidence entered output nodes 
prediction assessing probability evidence frontier variables may instantiated known state alternatively frontier state distributions may thought additional parameters learned data 
parameters parameters bayesian network specify generic node conditional probabilities simplest statistical model conditional probabilities unrestricted multinomial model 
case parameter table specifies probabilities state configuration parents 
particular probability denotes th configuration parents 
particular topology described nodes form recursive structure locally connected pattern 
feature essentially depends fact network obtained unfolding basic triplet skeleton simple consequence conditional distributions size similar meanings 
stationarity assumption tables independent achieve significant reduction terms model complexity 
noticed simple case hmm iohmm stationarity common assumption 
inference theory inference learning hrm relatively simple recognizes hrm just special case bayesian networks topology known 
research concerning inference bayesian networks dates back principal concern construction probabilistic expert systems relatively mature making available general understood algorithms 
computing resources concern straightforwardly calling algorithms subroutines may appropriate case hrm basic distinction inference algorithms singly connected dag polytrees general multiply connected dag case inference performed directly bayesian network local message passing algorithm referred propagation 
case dag compiled new structure called junction tree nodes contain clusters variables inference relies local message propagation algorithm nodes junction trees 
description model fixed topology encoding network bayesian network inference performed changes training example 
different junction tree needs constructed training example 
maintaining junction tree example may costly terms memory 
recompiling junction tree time new example network may computationally costly discussed training algorithms presence hidden variables iterative presentations example needed convergence 
interesting solution consists merging training examples optimally compressed supergraph done junction tree built entire training set 
alternatively ad hoc propagation algorithms class doag considered may derived 
example case binary trees section iv 
problem standard inference algorithms intractable densely connected networks 
resort approximate methods dealing complex classes doag interesting solution approximate inference relies mean field theory statistical physics shown accurate density connections increases 
learning topology network deterministically known learning problem simply reduced estimating parameters model dataset 
problem complicated presence hidden variables exact full bayesian methods intractable 
common approach learning presence missing data estimate parameters maximum posteriori map principle 
map framework parameters supposed obey probability distribution tends ieee transactions neural networks vol 
september delta function centered value maximizes learning learn single value pretend negligible prior parameters 
large datasets effect prior negligible approximated maximum likelihood ml value way learning cast optimization procedure applied likelihood function common approaches solving optimization problem gradient ascent expectation maximization em algorithm applied provided local conditional distributions belong exponential family 
methods iterative certain regularity conditions converge local maximum likelihood 
specializations hrm case sequentially structured data hmm iohmm typically trained em algorithm 
em iteratively fills missing values data effectively assigning credit hidden state variables observed data 
initial value assigned parameters 
generic th iteration em consists expectation step followed maximization step 
expectation step consists computing expected sufficient statistics parameters observed data previous parameters case unrestricted multinomial distributions sufficient statistics simply counts times generic variable state parents th configuration 
intuition em counts available state hidden variables unknown expectations computed probabilities easily computed solving probabilistic inference problem evidence instantiating label nodes target nodes data structure propagating evidence hidden nodes unfolded network parameters maximization step consists updating parameters expected sufficient statistics inference binary tree hrm model consider section generalization iohmm processing binary trees categorical variables 
recursive network model slice unfolded network shown fig 

derive evidence propagation algorithm specialized topology 
algorithm require compilation unfolded network junction tree seen specialization pearl propagation 
denote generic node data structure denote hidden variable input variable output variable node respectively drop node subscripts simplify notation 
assuming root node variable depending left right child parent input variable connected output variable connected denote evidence entered input output nodes 
aim probabilistic inference assess table generic hidden node denote evidence connected ancestors evidence connected descendants 
introduce inward table outward table tables recursively computed inward outward equations generalize known baum welch forward backward propagation hmm parameters transition probabilities parameters emission probabilities 
expected sufficient statistics recursively computed bayes theorem interesting note formal resemblance recurrences inside outside algorithm learning stochastic context free grammars scfg 
scfg intended learning sequential domains tree structures scfg correspond admissible parse trees explain string generated grammar 
recursive neural networks adaptive processing data structures takes place introduce parametric representation weights estimated examples 
section show io isomorph transductions naturally implemented recursive neural networks generalized form recurrent networks parameters learned examples gradient descent algorithms 
recursive neural network recursive state transition function output function approximated feedforward neural networks leading parametric representation connection weights 
note special case linear chains equations exactly frasconi general framework adaptive processing data structures fig 

encoding network associated doag 
recursive network unfolded structure doag 
correspond general state space equations recurrent neural networks 
representation stationary independent node nonstationary transductions obtained example node variant connection weights state variables recursive representation continuous neural networks coding discrete entities 
parametric representations implemented number different feedforward neural network architectures 
significant architectures 
order recursive networks maximum outdegree doag dependence node children ch expressed means weight matrices similarly information attached nodes propagated weight matrix parameters adaptive model state updated vectorial sigmoidal function 
equation quite straightforward extension order recurrent neural networks difference generalized form processing place dimension note weight matrices node resulting transduction stationary 
output node obtained directly state neurons identity map placing layer top state neurons 
fig 
pictorial representation computation place recursive neural network 
graphical formalism developed section iii graph mapped output associated encoding network 
fig 
recursive neurons represented layer belong dark squares denote frontier states 
recursive radial basis functions recurrent radial basis functions extended general computation needed process structures relying parametric representation chosen exponential function 
equation denote output th radial basis function unit position parameters 
complex model obtained adding dispersion parameters adaptively control widths radial functions 
state vector obtained additional layer sigmoidal units top radial basis function units weight matrix see details 
matrix notation parameters controlling state transition function high order recursive recurrent neural networks high order neural networks proposed mainly giles associates static networks recurrent networks interesting models especially dealing symbolic tasks 
easily conceive high order networks processing data structure extension second order recurrent networks 
instance special case binary trees introduce third order networks follows extension general case dimensional networks straightforward 
processing data structures scheme defined section iii connectionist specification functions possible calculate computation outputs case stationary supersource transductions simply needs compute producing unstructured output value general function calculated produces output graph skeleton neural networks approximating functions possible carry computation learning process effectively 
instance functions regarded multilayer perceptrons having sigmoidal locally tuned processing units 
formulation supervised learning adopted case sequences recurrent network error function created gives measure fitting examples training set 
matter architecture considering learning parameters associated recursive function carried relying encoding network associated graph 
formalism graphical models developed section iii possible regard adaptive computation case ieee transactions neural networks vol 
september graphs natural extension time unfolding process recurrent neural networks 
connectionist model function optimization learning parameters hard practice error function contains local minima 
detailed analysis optimal convergence issues proposed 
examples applications discuss examples applications proposed framework fruitfully applied 
recall standard feature approach processing structures discuss drawbacks approach 
give closer look learning heuristics tree grammars inference 
specifically summarize existing context proposed framework applications motivate proposed framework applied considering applications proposed framework applied systematically 
feature approach classification structures consider structured domain training set representing classification task 
standard feature approach encodes graph fixed size vector predefined features 
examples trivial features graphs number vertices number edges mean number outgoing edges 
features usually determined experts application domain input classifier feedforward neural network 
encoding process defined priori depend classification task 
example molecular biology chemistry chemical compounds represented labeled graphs encoding process performed definition topological indexes designed expensive trial error approach 
problem selecting relevant features partially solved special selection criterion devised classification task hand 
definition criterion difficult priori knowledge problem available useless candidate features encoding relevant information correct classification input graphs graphs classified differently represented feature vector 
summarizing conclude priori definition encoding process main drawbacks 
definition candidate features independent classification task 
second selection relevant features priori knowledge 
means general scheme definition selection features devised 
fact relevance different features graphs may change dramatically different classification tasks encoding process classification task implies general scheme assign unique feature vector graph making classification task difficult 
advantage framework propose general encoding process parametric apart general assumptions causality stationarity fully depends specific classification task hand 
fact proposed adaptive processing scheme specific encoding procedure learned training data adapting set parameters example classification problem structure supersource encoding vector representing state vector depends information contained data structure means application domain examples desired function available need explicit priori knowledge encoding dynamic data structure array containing relevant features 
encoding process performed recursive state transition function parameters tuned available training examples 
course quality resulting encoding depend amount training data 
learning heuristics mentioned serious problem symbolic problem solving systems combinatorial explosion search space 
problem usually faced devising heuristics selection promising part search tree 
context heuristic mean evaluation function returns cost branch search tree 
cost expected representative computational burden usefulness associated exploration part search tree 
aim reach solution leaf soon possible 
unfortunately heuristics domain specific heuristic effective domain usually useless domain 
cases heuristics known advance difficult formalize automated system usually works level abstraction different expert devising heuristic 
solution problems machine learning techniques extract control information devising specifically done context supervised learning collecting positive negative examples searches search space training inductive system set examples 
depicted fig 
search space symbolic problem solving systems represented tree nodes correspond search states edges correspond inference steps 
learning heuristic functions problem learning real valued function having set admissible search states domain 
applications search states conveniently represented static form problems automated reasoning yield search states contain dynamically structured information 
example states model elimination provers prolog systems consist logical expressions suitable labeled doag representation explained example 
cases evaluation function depends frasconi general framework adaptive processing data structures fig 

example search tree 
shaded nodes represent positive training data white nodes constitute negative training data 
dashed nodes training 
note node tree contains set subgoals represented trees 
structured information see fig 

fact finding rating applicable rules current context selecting subgoal realizing generalizing lemma failure store regarded problems learning function symbolic structures arbitrary size 
defined reason recursive neural networks hrm natural candidates performing inductive task 
context neural networks deal performed setheo system 
setheo theorem prover full order logic 
uses model elimination proof calculus 
details setheo 
attempt learn evaluation function setheo performed training backpropagation network feature vectors representing predefined features structures encountered search applicable inference steps 
examples static features features computed compilation time number literals clause number distinct variables head clause 
examples uniform dynamical features features computed run time uniform branches logical current depth search tree number instantiated variables calling subgoal 
distinct dynamical features depend actual clause occurrence run time current total number uses clause current search number variables calling clause instantiated 
results obtained network standard models improved search time order magnitude clear discussion section examples features encoding structural information fixed size vector able capture relevant information gathered note feature different number instantiated variables calling subgoal subgoal clause unification tried 
symbolic structures arbitrary size 
fact important include selection relevant features part learning task 
preliminary move direction reported networks successfully perform classification symbolic recursive structures encoding logical terms 
refinement led definition backpropagation structure algorithm 
experimental comparison approach backpropagation structure algorithm reported shows algorithm obtains slightly better results examples solvable approach smaller networks training times 
backpropagation structure algorithm able successfully learn classification tasks solved reasonable amount time networks 
improvement direction development cascade correlation network structure generalization recurrent cascade correlation sequences obtained better results respect networks subset classification problems reported 
advantage cascade correlation network structure backpropagation structure networks necessary number hidden units automatically determined learning algorithm 
integration backpropagation structure networks setheo system discussed great detail sophisticated formulation learning goal 
basic idea heuristic enables system find single solution long length path leading solution reasonable respect length shortest path leading solution 
merging new formulation learning goal neural networks structures resulted seed search discovering solution problems solution reasonable amount computation 
inference tree grammars classical grammatical inference learner set labeled strings requested infer set rules define formal language 
stated classical way domain grammatical inference consists learning sets strings sets sequentially ordered pieces information 
particular assuming language associated grammar regular set grammatical inference consists identifying possibly small finite accepting automaton 
past years research carried generalize conventional automata theory changing type inputs outputs strings labeled trees 
numerous researchers approached grammatical inference adaptive models recurrent neural networks iohmm 
known recurrent neural networks simulate finite state automata turing machine real time 
ability representing finite automata sufficient guarantee regular grammar learned examples 
detailed discussion language identification problem connectionist grammatical inference systems 
ieee transactions neural networks vol 
september section discuss relationships proposed framework inference tree grammars 
tree grammar defined tuple grammar alphabet nonterminals terminals ranked alphabet productions form trees finite set starting trees denotes set trees nodes labeled elements tree grammar expansive form productions form tree automata generalize finite state automata known see example languages trees generated tree grammars expansive form recognized frontier root tree automata fra defined example 
computational results different recursive neural network models structures elman style networks cascade correlation networks neural trees reported 
specifically shown elman style networks simulate fra cascade correlation networks structures neural trees structures simulate fra 
results observing neural trees structures generalizations neural trees sequences shown neural trees sequences simulate finite state machine 
note results derived structures true sequences sequences special cases labeled graphs 
means fully developed theory structures automatically cover sequences 
observed hrm equivalent fra limit case probabilities case state transition function fra simply obtained arguments nonzero parameters cpt associated hidden state variables hrm 
fact experimental results reported showing binary tree hrm able learn regular grammars binary tree domains 
vi 
framework discussed opens novel way adaptively dealing pieces information relations expressed form directed acyclic graphs 
formalism intuitively explained extending models serially ordered sequences models capable dealing partial orders variable described doag quite natural expect theoretical analyzes recurrent networks hmm generalized recursive models 
particular interesting issue symbolic prior knowledge adaptive structural processors 
prior knowledge injection algorithms introduced recurrent networks assuming available prior knowledge expressed terms state transition rules finite automaton see detailed theoretical analysis 
known transitions injected recurrent architecture unknown rules supposed filled data driven learning 
approach aims reduce complexity learning improve generalization 
learning extract rules trained network order complete refinement process symbolic domain 
strict relations recurrent recursive networks pointed methods adapted incorporate extract symbolic knowledge case linear lists extended case graphs 
instance natural relationship automata second order recurrent neural networks extended relationship ary tree automata th order recursive networks 
recurrent networks process sequences partial specification transduction involve information attached nodes list case recursive networks graph topology additional rich source prior knowledge 
hybrid nature data cases graph topology create partition training set 
prior knowledge graph topology exploited designing modular recursive network single modules learned separately basis partition training set operated prior topology information 
extension framework proposed manage transductions io isomorph open research problem 
case non io isomorph transductions defining encoding networks structured processing highly nontrivial quite difficult task need solved learning structural alignment input output doag determine output subgraphs generated correspondence input subgraphs 
particular case temporal sequences solutions problem learning asynchronous transductions proposed recurrent networks iohmm 
acknowledgment authors baldi tsoi fruitful discussions comments earlier version 
frasconi gori soda recurrent neural networks prior knowledge sequence processing constrained nondeterministic approach knowledge syst vol 
pp 

software complexity measure ieee trans 
software eng vol 
pp 

design analysis hierarchical software metrics acm computing surveys vol 
pp 

nilsson principles artificial intelligence 
palo alto ca tioga 
connectionist approach learning search control heuristics automated deduction systems ph dissertation dept comput 
sci tech 
univ munich 
fu syntactic pattern recognition applications 
englewood cliffs prentice hall 
pavlidis structural pattern recognition 
new york springer verlag 
gonzalez thomason syntactical pattern recognition 
reading ma addison wesley 
pattern recognition statistical structural neural approaches 
new york wiley 
frasconi general framework adaptive processing data structures bengio simard frasconi learning long term dependencies gradient descent difficult ieee trans 
neural networks vol 
pp 
mar 
hinton mapping part hierarchies connectionist networks artificial intell vol 
pp 

pollack recursive distributed representations artificial intell vol 
pp 

plate holographic reduced representations ieee trans 
neural networks vol 
pp 
may 
sperduti labeling raam connection sci vol 
pp 

encoding labeled graphs labeling raam advances neural information processing systems cowan tesauro alspector eds 
san mateo ca morgan kaufmann vol 
pp 

stability properties labeling recursive autoassociative memory ieee trans 
neural networks vol 
pp 
nov 
sperduti memory model associative access structures ieee int 
conf 
neural networks pp 

encoding syntactical trees labeling recursive autoassociative memory proc 
ecai amsterdam netherlands pp 

sperduti supervised neural networks classification structures ieee trans 
neural networks vol 

sejnowski rosenberg parallel networks learn pronounce english text complex syst vol 
pp 
feb 
elman finding structure time cognitive sci vol 
pp 

giles extraction insertion refinement symbolic rules dynamically driven recurrent neural networks connection sci vol 
pp 

rabiner tutorial hidden markov models selected applications speech recognition proc 
ieee vol 
pp 

bengio frasconi input output hmm sequence processing ieee trans 
neural networks vol 
pp 
sept 
de vries principe gamma model new neural net model temporal processing neural networks vol 
pp 

back tsoi comparison discrete time operator models nonlinear system identification advances neural information processing systems tesauro touretzky leen eds 
cambridge ma mit press vol 
pp 

pearl probabilistic reasoning intelligent systems networks plausible inference 
san mateo ca morgan kaufmann 
whittaker graphical models applied multivariate statistics 
chichester wiley 
neal asymmetric parallel boltzmann machines belief networks neural comput vol 
pp 

bishop neural networks pattern recognition 
oxford clarendon 
lauritzen graphical models statistical science series 
oxford clarendon 
buntine operations learning graphical models artificial intell 
res vol 
pp 

carrasco learning initial state second order recurrent neural network regular language inference neural comput vol 
pp 

tree automata informal survey currents theory computing aho ed 
englewood cliffs nj prentice hall pp 

jensen lauritzen bayesian updating recursive graphical models local computations comput 
statist 
quarterly vol 
pp 

frasconi gori sperduti hidden recursive models probabilistic learning structured information preprint dipartimento di sistemi informatica universit di firenze firenze italy 
saul jaakkola jordan mean field theory sigmoid belief networks artificial intell 
res vol 
pp 

lari young estimation stochastic context free grammars inside outside algorithm comput 
speech language vol 
pp 

frasconi gori soda representation finite state automata recurrent radial basis function networks machine learning vol 
pp 

giles maxwell learning invariance generalization high order neural networks appl 
opt vol 

miller giles experimental comparison effect order recurrent neural networks int 
pattern recognition artificial intell special issue applications neural networks pattern recognition guyon ed 
frasconi gori sperduti optimal learning data structures proc 
int 
joint conf 
artificial intell nagoya japan august pp 

hall molecular connectivity chi indexes kappa shape indexes structure property modeling reviews computational chemistry boyd eds 
new york vch pp 

wos overbeek lusk boyle automated reasoning applications 
englewood cliffs nj prentice hall 
letz schumann bibel setheo highperformance theorem prover automated reasoning vol 
pp 

backpropagation guiding search theorem prover int 
neural network res 
vol 
pp 

learning task dependent distributed structure representations backpropagation structure ieee int 
conf 
neural networks pp 

sperduti learning distributed representations classification terms proc 
int 
joint conf 
artificial intell pp 

sperduti extended cascade correlation syntactic structural pattern recognition advances structural syntactical pattern recognition wang rosenfeld eds vol 
lecture notes computer science 
berlin germany springer verlag pp 

fahlman recurrent cascade correlation architecture advances neural information processing systems lippmann moody touretzky eds 
san mateo ca morgan kaufmann vol 
pp 

angluin smith survey inductive inference theory methods acm comput 
survey vol 
pp 
sept 
giles constructing deterministic finite state automata recurrent neural networks acm vol 
pp 

siegelmann sontag computational power neural nets comput 
syst 
sci vol 
pp 

gold complexity automaton identification data inform 
contr vol 
pp 

kremer theory grammatical induction connectionist paradigm ph dissertation dept comput 
sci univ alberta canada 
sperduti computational power recurrent neural networks structures neural networks appear 
giles miller chen chen sun lee learning extracting finite state automata second order recurrent neural networks neural comput vol 
pp 

towell shavlik extracting refined rules knowledge neural networks machine learning vol 
pp 

mealy machines learning translators recurrent neural proc 
world congr 
neural networks san diego ca pp 

bengio recurrent neural networks missing asynchronous data advances neural information processing systems mozer touretzky perrone eds vol 

cambridge ma mit press 
paolo frasconi received sc 
degree electronic engineering ph degree computer science university florence italy 
currently assistant professor dipartimento di sistemi informatica university florence 
visiting scholar department brain cognitive science massachusetts institute technology cambridge 
visiting scientist centro studi cselt turin 
current research interests include neural networks markovian models graphical models particular emphasis problems involving learning sequential structured information 
ieee transactions neural networks vol 
september marco gori sm received degree electronic engineering universit di firenze italy ph degree universit di bologna italy 
visiting student school computer science mcgill university montreal canada 
associate professor computer science universit di firenze november joined university siena italy 
main research interests pattern recognition neural networks artificial intelligence 
dr gori general chairman second workshop neural networks speech processing held firenze organized nips workshop artificial neural networks continuous optimization local minima computational complexity summer school adapting processing sequences held september 
volume topics artificial intelligence berlin germany springer verlag collects contributions italian congress artificial intelligence 
serves program committee member workshops conferences mainly area neural networks acted guest neurocomputing journal special issue recurrent neural networks july 
associate editor ieee transactions neural networks neurocomputing neural computing survey 
italian chairman ieee neural network council member iapr siren ai ia societies 
alessandro sperduti received doctoral degrees respectively computer science university pisa italy 
spent period international computer science institute berkley ca supported postdoctoral 
returned computer science department university pisa presently assistant professor 
research interests include data sensory fusion image processing neural networks hybrid systems 
field hybrid systems focused integration symbolic connectionist systems 
dr sperduti contributed organization workshops subject served program committee conferences neural networks 
