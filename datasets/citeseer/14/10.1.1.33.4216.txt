schaal atkeson cg vijayakumar 
real time robot learning locally weighted statistical learning international conference robotics automation icra 
san francisco april 
real time robot learning locally weighted statistical learning stefan schaal usc edu www slab usc edu christopher atkeson cga cc gatech edu www cc gatech edu fac chris atkeson vijayakumar brain riken go jp www brain riken go jp computer science neuroscience univ southern california los angeles ca college computing georgia institute technology atlantic drive atlanta ga kawato dynamic brain project jst seika cho soraku gun kyoto japan laboratory information synthesis riken brain science research institute japan locally weighted learning lwl class statistical learning techniques provides useful representations training algorithms learning complex phenomena autonomous adaptive control robotic systems 
introduces lwl algorithms tested successfully real time learning complex robot tasks 
discuss major classes lwl memory lwl purely incremental lwl need remember data explicitly 
contrast traditional beliefs lwl methods high dimensional spaces provide new algorithms tested dimensional learning problems 
applicability lwl algorithms demonstrated various robot learning examples including learning devil sticking pole balancing humanoid robot arm inverse dynamics learning freedom robot 
necessity self improvement control systems apparent fields robotics factory automation autonomous vehicles impeded complexity inventing programming satisfactory control laws 
learned models complex tasks aid design appropriate control laws tasks involve decisions streams information sensors actuators data relatively plentiful 
learning viable research approach generation flexible autonomous robots perform multiple tasks hope creating autonomous humanoid robot point 
approaching learning problem alternative learning methods chosen neural network statistical machine learning literature 
current focus learning research lies increasingly sophisticated algorithms line analysis finite data sets severe constraints computational complexity algorithms 
examples algorithms include revival bayesian inference new algorithms developed framework structural risk minimization 
methods target problems classification diagnostics extensions regression problems exist 
motor learning special constraints need taken account approaching learning task 
learning problems motor learning require regression networks instance learning internal models coordinate transformations control policies evaluation functions reinforcement learning 
data motor learning usually limited finite data set robot moves new data generated included learning network 
computationally inexpensive training methods important domain line learning preferred 
significant additional problems motor learning distributions learning data may change continuously 
input distributions change due fact flexible movement system may different tasks different days creating different kinds training data 
input output relationship data conditional distribution may change learning system changes physical properties learning involves nonstationary training data reinforcement learning 
changing distributions easily lead catastrophic interference neural network paradigms useful information training new data :10.1.1.47.8934
element motor learning tasks complex motor systems high dimensional number input dimensions amplifying need efficient learning algorithms 
current trend learning research largely orthogonal problems motor learning 
advocate locally weighted learning methods lwl motor learning learning technique derived nonparametric statistics 
lwl provides approach learning models complex phenomena dealing large amounts data training quickly avoiding interference multiple tasks control complex systems 
lwl meth ods deal successfully high dimensional input data redundant irrelevant inputs keeping computational complexity algorithms linear number inputs 
lwl methods come different strategies 
memory lwl lazy learning method simply stores training data memory uses efficient lookup interpolation techniques prediction new input generated 
kind lwl useful data needs interpreted flexible ways instance forward inverse transformation 
memory lwl commitment approach data efficient 
non memory lwl essentially statistical properties memorybased lwl avoids storing data memory recursive system identification techniques 
way non memory lwl caches information training data compact representations cost flexible re evaluation data impossible lookup times new data significantly faster 
describe lwl algorithms suitable robot learning problems 
goal section provide clear pseudo code explanations algorithms 
illustrate successful application methods implementations real time robot learning involving dexterous manipulation tasks devil sticking pole balancing anthropomorphic robot arm classical problems learning high dimensional inverse dynamics models 
locally weighted learning algorithms assume data generating model regression problems standard form yf dimensional input vector noise term mean zero output dimensional 
key concept lwl methods approximate nonlinear functions means piecewise linear models similar order taylor series expansion 
locally linear models demonstrated excellent statistical compromise possible local polynomials fit data 
key problem lwl determine region validity local model trusted fit local model region 
algorithms compute region validity called receptive field linear model gaussian kernel kk kk exp xc center th linear model corresponds positive semi definite distance metric determines size shape region validity linear model 
kernel functions possible add minor differences quality function fitting 
locally weighted regression straightforward lwl algorithm locally linear models memory locally weighted regression lwr 
training lwr fast just requires adding new training data memory 
prediction needed query point weighted regression analysis performed lwr algorithm xy xxx xx compute prediction exp query point training points memory compute diagonal weight matrix build matrix vector ii ii iq py 
yy tt qn compute locally linear model prediction denotes th element regression vector computational complexity lwr proportional normally training data points receive approximately zero weight far away query point computational complexity lwr reduced significantly particularly exploiting efficient data structure kd trees keeping data memory 
lwr applied efficiently real time problems high dimensional number inputs accumulate data particular area input space 
open parameter distance metric introduced equation 
data memory increased significant amount optimized leave cross validation 
avoid open parameters usually assumed diagonal matrix diag scale parameter normalize range input dimensions variance input dimension ii crossvalidation performed onedimensional search parameter leave cross validation xx set reasonable values temporarily exclude training data compute lwr prediction reduced data choose optimal hh hh sse sse sse sse qi ii rr min locally weighted partial squares circumstances lwr algorithm needs enhanced number input dimensions grows large redundant input dimensions matrix inversion numerically unstable 
computational efficient technique statistics literature partial squares regression pls ideally suited reduce computational complexity lwr avoid numerical problems 
essence pls fit linear models hierarchy univariate regressions selected projections input space 
projections chosen correlation input output data algorithm assures subsequent projections orthogonal input space 
straightforward derive locally weighted pls algorithm shown equation 
steps may look unusual glance ones indicated equation 
steps input data regressed current projection subsequently input space reduced procedure ensures projection direction guaranteed orthogonal respect previous projection directions 
remarkable property input data locally statistically independent diagonal covariance matrix approximately locally linear find optimal linear approximation data single projection 
true chose optimal projection direction gradient data 
opens question projections chosen input data statistically independent 
typically squared error res res iteration significantly lower previous step 
simple heuristic adding projections require new projection squared error reduces certain ratio res res res res usually 
learning tasks 
lwr open parameter distance metric optimized strategy 
algorithm compute prediction xy xx xx query point training points memory compute diagonal weight matrix build matrix vector ii ii iq ii ii ii ii py exp xx xxx res sws sws res res ii ii yy recursively compute locally linear model initialize prediction initialize zx zu qq qq computational complexity assumes data zero weight fixed number projections needed achieve fit computational complexity tends linear number input dimensions 
constitutes significant saving quadratic cost lwr particularly high dimensional input spaces 
additionally correlation step select projection direction eliminates irrelevant redundant input dimensions results excellent numerical robustness 
locally weighted projection regression points concern remain lwr 
learning system receives large possibly stream input data typical online robot learning memory requirements store data computational cost evaluate algorithms large 
circumstances non memory version lwl desirable incoming data point incrementally incorporated learning system lookup speed accelerated 
approach incremental lwl algorithm suggested previous lwr starting point :10.1.1.47.8934
idea algorithm straightforward postponing computation local linear model prediction needs local models built continuously entire support area input data selected points input space see 
prediction query point formed weighted average predictions local models wy kqk weights computed weighting kernel local model equation 
incremental updates parameters linear models accomplished recursive squares techniques 
give incremental update rule new learning system called locally weighted projection regression lwpr 
note omit index necessary distinguish explicitly different linear models 
update means inputs output update local model xx zx zu training point initialize ww www res res ss nn nn nn ss sr sr res sz sz sr ss sz ss res sse sse ii es nn res equations forgetting factor determines old data regression parameters forgotten similar recursive system identification techniques 
variables ss sr sz memory terms enable achieve univariate regression step recursive squares fashion fast newton method 
steps incremental counterparts algorithm 
step computes sum squared errors determine adding projections 
predictions query point formed exactly eqn 
lwl algorithms remaining open parameter distance metric contrast algorithm determined global parameter input space possible optimize local model individually 
developed incremental optimization means gradient descent stochastic leave crossvalidation criterion :10.1.1.47.8934
derivation adapted lwpr due space constraints give new cost function needs minimized analogy :10.1.1.47.8934
gradient descent update dmm mm sws nn ki ki ij ij res upper triangular learning rules embedded incremental learning system automatically allocates new locally linear models needed initialize lwpr receptive field rf new training sample rf calculate activation update linear model activated gen create new rf def pseudo code algorithm gen threshold determines create new receptive field def initial usually diagonal distance metric :10.1.1.47.8934
initial number projections set grows criterion fulfilled 
diagonal distance metric assumption remains small computational complexity update parameters lwpr linear number input dimensions 
empirical evaluations learning devil sticking devil sticking juggling task center stick back forth 
shows sketch devil sticking robot 
robot uses top joints perform planar devil sticking details 
task robot learn continuous left right left juggling pattern 
purpose learning task modeled discrete function maps impact states hand impact states hand 
state dimensional vector pxy comprising impact position angle velocities center devil stick angular velocity respectively 
task command xy vv catch position xy hh angular trigger velocity start throw dimensional throw direction vv xy order compute appropriate lqr controllers task robot learns nonlinear mapping current state command state dimensional input dimensional output function 
task ideally suited lwr high dimensional new training data generated hz 
memorybased learning allows efficiently search space statistically new commands 
result successful devil sticking achieved trials corresponding training points memory 
remarkable learning speed humans need week hour practicing day learn juggle 
learning pole balancing implemented learning task balancing pole fingertip degree freedom anthropomorphic robot arm 
low level robot controller ran compute torque mode hz parallel processors located vme bus running real time operating system vxworks 
goal learning generate appropriate task level commands cartesian accelerations fingertip keep pole upright 
task level commands converted actuator space means extended jacobian 
input robot received data color tracking stereo vision system ms processing delays 
learning implemented line receptive field weighted regression rfwr essentially non memory counterpart lwr :10.1.1.47.8934
rfwr employs incremental learning strategies lwpr 
task rfwr acquire discrete time forward dynamics model pole compute lqr controller realize kalman predictor eliminate delays visual input 
forward model input dimensions positions lower pole angular positions corresponding velocities horizontal accelerations fingertip mapped outputs state pole 
robot received training data moved 
shows results learning 
took trials learning succeeded reliable performance longer minute 
explored learning demonstration human demonstrated balance pole seconds robot learning forward model just watching 
learning reliably accomplished single trial large variety physically different poles demonstrations arbitrary people laboratory 
inverse dynamics learning goal learning task approximate inverse dynamics model degree freedom anthropomorphic robot arm data set consisting data points collected hz actual robot performing various rhythmic discrete movement tasks corresponds minutes data collection 
data consisted input dimensions joint positions velocities accelerations 
goal learning approximate appropriate torque command shoulder robot motor response input vector 
increase difficulty learning added irrelevant dimensions inputs gaussian noise 
data points excluded training data test set 
illustration devil sticking sketch devil sticking robot flow force motor robot indicated different robot links position change due application motor motor respectively indicated small sketches number hits trial trial numb er run run ii run iii learning curves devil sticking runs 
high dimensional input space learning problem requires application lwpr 
shows learning results comparison parametric estimation inverse dynamics rigid body dynamics 
lwpr outperformed parametric model 
training points lwpr converged excellent result nmse 
employed average projections local model despite fact input dimensionality 
learning number local models increased factor initial models models 
increase due adjustment distance metric equation initialized form large kernel 
large kernel data lwpr reduced kernel size response kernels needed allocated 
locally weighted learning algorithms real time robot learning 
algorithms easy implement sound statistical techniques core converge fast accurate learning results implemented purely incremental fashion 
demonstrated latest version algorithms capable dealing high dimensional input spaces redundant irrelevant input dimensions computational complexity incremental update remained linear number inputs 
examples demonstrated lwl algorithms applied successfully complex learning problems actual robots 
best knowledge currently comparable learning framework combines required properties real time motor learning locally weighted learning 
acknowledgments possible award national science foundation kawato dynamic brain project funded japanese science technology cooperation atr human information processing research laboratories 
schaal imitation learning route humanoid robots trends cognitive sciences vol 
pp 

bishop neural networks pattern recognition 
new york oxford university press 
williams rasmussen gaussian processes regression advances neural information processing systems touretzky mozer hasselmo eds 
cambridge ma mit press pp 

vapnik estimation dependences empirical data 
berlin springer 
cortes vapnik support vector networks machine learning vol 
pp 

vapnik smola support vector method function approximation regression estimation signal processing advances neural information processing systems mozer jordan petsche eds 
cambridge ma mit press pp 

schaal atkeson constructive incremental learning local information neural computation vol :10.1.1.47.8934
pp 

atkeson schaal memory neural networks robot learning neurocomputing vol 
pp 

cleveland loader smoothing local regression principles methods bell laboratories murray hill ny 
hastie tibshirani nonparametric regression classification part nonparametric regression statistics neural networks theory pattern recognition applications 
asi proceedings subseries computer systems sciences cherkassky friedman wechsler eds springer pp 

atkeson moore schaal locally weighted learning artificial intelligence review vol 
pp 

atkeson moore schaal locally weighted learning control artificial intelligence review vol 
pp 

aha lazy learning artificial intelligence review pp 

ljung theory practice recursive identification cambridge mit press 
cleveland robust locally weighted regression smoothing scatterplots journal american statistical association vol 
pp 

hastie loader local regression automatic kernel carpentry statistical science vol 
pp 

atkeson memory approaches approximating continuous functions nonlinear modeling forecasting casdagli eubank eds 
redwood city ca addison wesley pp 

moore efficient memory learning robot control computer laboratory university cambridge october 
wold soft modeling latent variables nonlinear iterative partial squares approach perspectives probability statistics papers honour bartlett ed 
london academic press pp 

frank friedman statistical view regression tools technometrics vol 
pp 

schaal atkeson robot juggling implementation memory learning control systems magazine vol 
pp 

martin resolution kinematic redundancy proceedings symposia applied mathematics vol 
american mathematical society pp 

atkeson hollerbach model control robot manipulator 
cambridge ma mit press 
trial scratch primed model smoothed average learning curves robot pole balancing 
trials aborted successful balancing seconds 
tested long term performance learning system running pole balancing hour pole dropped 
nmse test set receptive fields training data points rigid body dynamics lwpr dexterous robot arm learning curve learning inverse dynamics model robot dimensional data set included irrelevant dimensions 
