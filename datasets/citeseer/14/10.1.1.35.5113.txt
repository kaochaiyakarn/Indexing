real valued evolutionary optimization flexible probability density estimator marcus gallagher marcus frean tom downs department computer science electrical engineering university queensland st lucia qld australia csee uq edu au 
population incremental learning pbil abstraction genetic algorithm solves optimization problems explicitly constructing probabilistic model promising regions search space 
iteration model generate population candidate solutions modified response solutions 
extension pbil real valued search spaces powerful general algorithmic framework arises enables arbitrary probability density estimation techniques evolutionary optimization 
illustrate usefulness framework propose implement evolutionary algorithm uses finite adaptive gaussian mixture model density estimator 
method offers considerable power flexibility forms density effectively modeled 
discuss general applicability framework suggest lead development better evolutionary optimization algorithms 
baluja population incremental learning pbil algorithm proposed optimization method removes genetics genetic algorithm baluja baluja caruana 
banzhaf editors proc 
genetic evolutionary computation conference gecco pp 

morgan kaufmann publishers san francisco ca 
step initialize probability vector step generate pop 
prob 
vector fx step eval 
cost fn 
sample point ff step find best sample point population min step update probability vector gamma jx goto termination condition table basic pbil algorithm 
maintaining population solutions search space pbil maintains probabilistic model promising regions search space uses model generate populations time step 
originally formulated optimizing bitstrings pbil implements evolution construction probability vector length bitstring optimized th bit candidate solution bitstring probability th bit bitstring equal 
iteration algorithm population bitstrings generated sampling distribution 
pbil updates probability model direction best member current population governed learning rate parameter pseudo code simple pbil variant table 
sophisticated versions update mean best away worst members 
variation mutate probability vector update step 
experimental evidence shown pbil significantly outperform traditional genetic algorithms variety test problems baluja 
de bonet mutual input clustering mimic algorithm improves pbil modeling pairwise conditional probabilities bits 
pbil models probability distribution bit independently mimic able offer improved performance problems structure solution bitstring major importance capturing relationships bits bitstring 
baluja considered sophisticated representations pairwise conditional probabilities tree pairwise conditional probabilities constructed optimal dependency tree 
allowing pairwise dependencies maximum spanning tree minimizes kullback leibler divergence true estimated distributions baluja davies 
extension pbil optimization functions real valued variables considered 
independent gaussian distributions model variable solution vectors 
univariate gaussian specified mean variance oe parameters 
bounded feasible region defined search space probability model initially centered region 
mean value gaussian updated pbil update rule implementation average best samples variance values initialized large value attempt ensure feasible region covered probability annealed small value geometrically decaying schedule oe oe theta fl fl propose implement different real coded version pbil 
assuming variable constrained interval low component probability vector represents probability th variable greater low 
probability vector updated pbil update rule interval variable halved appropriate direction reset algorithm continues 
sebag sebag consider gaussian distributions probability model real valued pbil 
updated pbil rule different methods updating oe discussed 
simplest choice simply fix oe constant value equivalent setting fl unity 
whilst real valued pbil algorithms binary case shown results number test problems difficulties remain applying algorithms 
particular variety methods constructing evolving probabilistic model difficult determine suitable priori problem introducing number user parameters selected appropriate values ensure results 
pbil suggests principle probabilistic sampling methods density estimation optimization problems 
section consider general framework algorithms 
formulation number insights gained fact form realvalued pbil equivalent evolutionary strategy 
section propose finite adaptive gaussian mixture model density estimator framework improve speed representational power algorithm 
section describes empirical results demonstrate advantages approach section discusses results concludes 
evolutionary optimization probabilistic modeling conceptual distinction pbil traditional evolutionary algorithms notion maintained evolved execution algorithm probabilistic model finite population solutions 
notion lasting population natural evolution traditional genetic operators crossover removed attempt provide faster simpler robust optimization algorithms 
remains principles parallel search population solutions iteration algorithm ii coupling current search results information gained parallelism guide search 
agree idea adopting approach evolutionary algorithms provides useful perspective workings 
practical viewpoint general common framework follows approach allows development new kinds evolutionary algorithms 
presents steps direction 
general probabilistic sampling framework task optimization set real valued parameters collected vector xn finding arg min ir error cost function 
assume existence probability density function gamma dx iteration generation algorithm generate candidate solutions optimization problem sampling times fx probability distribution 
sample solution individual evaluate error function giving ff probability model modified procedure phi may depend example information gained sampling error function evaluation process current previous generations 
produces probability model generation process repeats 
general algorithm may arrived abstracting steps pbil procedure way updates current probability model incorporating information obtained current population pbil variants incorporating operations mutating probability vector number best worst population members update probability vector operations simply considered part update process phi 
means considerable complexity hidden update procedure phi describes evolutionary process algorithm concisely allows flexibility meta step 
step initialize probability model step sample fx step evaluate ff step update probability model phi step increment time goto termination condition table general probabilistic population algorithm 
order framework specify particular algorithm necessary give form phi initial probability model population size real valued pbil gaussian distributions constant variance sebag stated oe phi oe gamma jx best member population time min set gaussians re centered time step position best sample previous generation 
fact equivalent simplest es schwefel oe phi oe real valued pbil algorithms discussed represented framework similar way 
previous real valued pbil algorithms simple underlying probability model univariate gaussian model regions search space promising search 
framework suggests powerful probabilistic models 
applicable pbil algorithm require methods learn model search space biased view locating extrema 
means iterative procedure updating model needed generation points generated model adjust response information gained generation best worst point 
addition existing familiar optimization techniques methods allow iterative sequential construction representation unlabeled data 
section provide example technique multivariate probability density estimation 
application adaptive mixture model estimator important possibility development better evolutionary optimization algorithms ability model sophisticated powerful probability distributions parameter search space de bonet baluja davies 
field multivariate probability density estimation see scott provides number methods aim address issue 
include kernel mixture model nearest neighbour methods 
adaptive mixture model order illustrate process applying arbitrary density estimation technique consider adaptive mixture model 
probability density represented sum number component distributions xjj number mixture components mixing coefficient th component 
coefficients allow contribution component sum varied satisfy conditions gaussians popular choice component distribution implementation discussed 
gaussian mixture models offer considerable flexibility forms distribution adequately approximate 
particular multimodal distributions effectively modeled see bishop mixture models subject ongoing development number issues remain regarding implementation adaptive mixtures algorithm undoubtedly attractive properties 
firstly uses recursive update rule meaning model assumes data points arrive sequentially update model observing new point phi procedure framework 
update equations ae gamma ae gamma gamma gamma ae gamma oe oe gamma gamma ae theta gamma gamma gamma oe secondly allowed vary execution algorithm single component adding components data observed adequately accounted existing model 
simple approach calculate mahalanobis distance new data point adding new component distance greater prespecified threshold value tc case model updated oe oe gamma gamma gamma new component assigned small mixing coefficient remaining coefficients adjusted accordingly 
empirical results problems demonstrate application adaptive mixture estimator evolutionary optimization algorithm 
standard quadratic sphere model known hump camel back function torn gamma gamma useful demonstrate convergence behaviour algorithm 
multimodal cost function local global minima able show mixture model constructs probabilistic search model able capture complex structure underlying search space 
algorithms size population set gamma es search initialized randomly region gamma 
experiment run iterations generations 
problem ran different algorithms realized framework ffl adaptive mixture model algorithm maximum number components mixture model initial variance components new mixture mahalanobis threshold adding new components 
ffl real valued pbil gaussians fixed mean value gaussians fixed variance value gaussians 
ffl real valued pbil gaussians pbil anneal fixed mean value gaussians initial variance value fl ffl gamma es es standard gaussian perturbations variable initial variance perturbations lognormal perturbations step sizes standard constants control gamma individual gamma step sizes 
details see schwefel 
results runs algorithm shown table 
typical run algorithm shown fig 

gives best results terms lowest error value reached mainly artificial chosen parameters algorithm illustrate convergence behaviour 
addition quite suited modelling gaussian mixtures 
see approaches minimum rate contrast constant variance value means initially converges slowly algorithm pbil anneal es table results problem mean std 
dev 
due steps takes small increasing variance value better suited local properties cost function slows variance large generate step sizes required continue fast convergence 
annealing variance pbil anneal result convergence ae fl chosen incorrectly result convergence frozen shown 
es provide fast convergence produces poor results high variability instance 
population size limited number iterations important considerations 
iterations adaptive mixture pbil anneal pbil const 
es illustrative run algorithm problem demonstrate typical dynamics algorithm 
example run shown components created mixture 
fig 
shows evolution mean values components means shown solid lines dashed lines 
see different components approaching different mean values asymptotically 
problem pair conjugate global minima approached components 
shows gaussian components dimension evolve form flexible multimodal density 
fig 
shows evolution standard deviations components 
see component range different values values similar case 
components seen converge variances approach zero remain larger values 
fig 
mixing coefficients shown 
component approaches global minimum topmost curve heavily weighted mixture cost 
components significant mixing coefficients correspond highest variance curves fig 

components whilst converging global minimum remain part search having corresponding variance values allow produce reasonable amount high quality sample solutions 
components effectively removed model coefficient values small 
iterations evolution mean values run 
iterations evolution variance values run 
iterations evolution mixture coefficients run 
discussion discussed general framework specification evolutionary optimization algorithms construct probabilistic model search space 
framework general include special cases large number familiar optimization techniques opening possibility techniques unsupervised learning cluster analysis probability density estimation 
motivated research pbil algorithms aims away details population operators traditional genetic evolutionary algorithms focus best information gained coupled parallel search guide search 
demonstrate applicability framework apply sophisticated method probability density estimation adaptive mixture model 
density estimator optimization context allows considerably powerful models explicitly constructed part search process 
purpose suggest proposed adaptive mixture optimization method superior existing method general sense 
typically possible find problems suitable algorithm hand vice versa 
goal research support development evolutionary algorithms potential adapt exploit structure search space flexible modeling techniques 
traded significant increase computational storage requirements methods demand 
supported university queensland postgraduate research scholarship author 
baluja baluja 

population incremental learning method integrating genetic search function optimization competitive learning 
technical report cmu cs school computer science carnegie mellon university 
baluja baluja 

empirical comparison iterative evolutionary function optimization heuristics 
technical report cmu cs carnegie mellon university 
baluja caruana baluja caruana 

removing genetics standard genetic algorithm 
technical report cmu cs carnegie mellon university 
baluja davies baluja davies 

optimal dependency trees combinatorial optimization learning structure search space 
technical report cmu cs carnegie mellon university 
bishop bishop 

neural networks pattern recognition 
oxford university press oxford 
de bonet de bonet isbell jr viola 

mimic finding optima estimating probability densities 
advances neural information processing systems volume pages 


adaptive mixtures 
journal american statistical association 


stochastic hill climbing learning vectors normal distributions 
st online workshop soft computing retrieved www nagoya ac jp wsc 
schwefel schwefel 

evolution optimum seeking 
wiley new york 
scott scott 

multivariate density estimation theory practice visualization 
wiley new york 
sebag sebag 

extending incremental learning continuous search spaces 
editor parallel problem solving nature ppsn volume lecture notes computer science pages berlin new york 
springer 
es stern 

telephone network traffic overloading diagnosis evolutionary computation techniques 

editor rd european conference artificial evolution ae volume lecture notes computer science pages berlin new york 
springer 
torn torn 

global optimization volume lecture notes computer science 
springer verlag berlin 
