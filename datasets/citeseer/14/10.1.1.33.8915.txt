cluster abstraction model unsupervised learning topic hierarchies text data appear proceedings ijcai thomas hofmann computer science division uc berkeley international cs institute berkeley ca hofmann cs berkeley edu presents novel statistical latent class model text mining interactive information access 
described learning architecture called cluster abstraction model cam purely data driven utilizes context specific word occurrence statistics 
intertwined fashion cam extracts hierarchical relations groups documents organization keywords 
annealed version expectation maximization em algorithm maximum likelihood estimation model parameters derived 
benefits cam interactive retrieval automated cluster summarization investigated experimentally 
intelligent processing text documents ultimately considered problem natural language understanding 
presents statistical approach learning language models context dependent word occurrences discusses applicability model interactive information access 
proposed technique purely data driven domain dependent background information rely predefined document categories list topics 
cluster abstraction model cam statistical latent class mixture model mclachlan basford organizes groups documents hierarchy 
compared state art techniques agglomerative clustering jardine van rijsbergen croft willett advantages additional features probabilistic model important advantages ffl sound foundation statistics probabilistic inference ffl principled evaluation generalization performance model selection ffl efficient model fitting em algorithm ffl explicit representation conditional independence relations 
additional advantages provided hierarchical nature model ffl multiple levels document clustering ffl discriminative topic descriptors document groups ffl coarse fine approach annealing 
section introduce non hierarchical probabilistic clustering model documents extended full hierarchical model 
probabilistic clustering documents emphasize clustering aspect introducing simplified non hierarchical version cam performs flat probabilistic clustering closely related distributional clustering model pereira word clustering text categorization baker mccallum fd denote documents fw denote words word stems 
refer vector sequence words dt constituting word frequencies summarized count variables indicate word occurred document denotes document length 
standard latent class approach assumed document belongs exactly cluster fc number clusters assumed fixed 
introducing class conditional word distributions wjc class prior probabilities stacked parameter vector model defined jc dt jc wjc factorial expression reflects conditional independence assumptions word occurrences bagof words model 
starting standard em approach dempster latent variable models employed 
em re estimation steps alternated ffl expectation step estimating posterior probabilities unobserved clustering variables cjw parameter estimate ffl maximization step involves maximization called expected complete data log likelihood posterior probabilities respect parameters 
em algorithm known increase observed likelihood step converges local maximum mild assumptions 
application bayes rule yields step re estimation equations distributional clustering model cjw wjc wjc step stationary equations obtained differentiating cjw wjc cjw cjw equations intuitive posteriors cjw encode probabilistic clustering documents conditionals wjc represent average word distributions documents belonging group course simplified flat clustering model defined deficits 
severe lack multi resolution structure inadequacy prototypical distributions wjc emphasize discriminative characteristic words fact typically dominated frequent word occurrences 
cure flaws main goal hierarchical extension 
document hierarchies abstraction cluster abstraction model hierarchical document clustering techniques utilize agglomerative algorithms generate cluster hierarchy dendogram product successive cluster merging cf 
willett 
cam explicit abstraction model represent hierarchical relations document groups 
achieved extending horizontal mixture model previous section vertical component captures specificity particular word context document assumed word occurrence dt associated abstraction node identified inner terminal nodes cluster hierarchy cf 

formalize sketched ideas additional latent variable vectors components dt introduced assign words exactly nodes hierarchy 
topology nodes hierarchy constraints cluster variables abstraction variables dt imposed dt notation shortcut refer nodes terminal node hierarchy 
eq 
states admissible values latent abstraction variables dt particular document latent class restricted nodes hierarchy predecessors breaks permutation symmetry abstraction nodes document clusters 
abstraction node particular place hierarchy utilized explain words documents associated terminal nodes subtree pictorial representation assigned choices abstraction nodes word occurrences dt restricted active highlighted vertical path 
may think cam mixture model horizontal mixture clusters vertical mixture abstraction levels 
horizontal component mixture vertical components path root vertical components shared different horizontal components tree topology 
generalizing non hierarchical model probability distribution words attached node inner terminal hierarchy 
application chain rule complete data model joint probability observed latent variables specified steps dt ajc ajc ja dt ja dt note additional document specific vertical mixing proportions ajc abstraction nodes cluster introduced understanding ajc case simplifying assumption mixing proportions shared documents assigned particular cluster ajc ajc solution degenerates distributional clustering model may choose ajc ffi ac propose parsimonious model fit ajc held data fraction words held document spirit model interpolation techniques jelinek mercer em algorithm distributional clustering model derive em algorithm model fitting 
step requires compute joint posterior probabilities form dt 
applying chain document cluster abstraction levels words document partitioning abstraction levels dt dt sketch cluster abstraction structure corresponding representation assigning occurrences abstraction levels terms latent class variables 
rule obtains cjw ajc dt dt ja ajc dt ja jc step re estimation equations conditional word distributions wdt dt dt dt cjw dt 
update equation class priors formula ajc cjw theta dt evaluated held data 
may worth closer look predictive word probability distribution wjd cam wjd cjw ajc assume simplicity cjw hard clustering case word probability modeled mixture occurrences different abstraction levels reflects reasonable assumption document contains certain mixture words ranging general terms ordinary language highly specific technical terms specialty words 
annealed em algorithm important problems need addressed successful application cam importantly avoid problem overfitting 
second necessary specify method determine meaningful tree topology including maximum number terminal nodes 
third may want find ways reduce sensitivity em procedure local maxima 
answer questions provided generalization called annealed em hofmann puzicha annealed em closely related technique known deterministic annealing applied clustering problems rose pereira 
thorough discussion annealed em scope theoretical background skipped focus procedural description 
key idea deterministic annealing temperature parameter ir applying annealing principle clustering variables posterior calculation generalized replacing exponent likelihood contribution linearly log probability scale general increase entropy annealed posterior probabilities 
annealed em utilized control parameter initialized high value successively lowered performance heldout data starts decrease 
annealing advantageous model fitting offers simple inexpensive regularization avoids overfitting improves average solution quality 
offers way generate tree topologies annealing leads sequence called phase transitions clusters split 
experiments lowered perplexity log averaged inverse word probability held data starts increase automatically defines number terminal nodes hierarchy 
details subject hofmann puzicha results documents experiments preprocessed word suffix stripping word stemmer 
standard word list utilized eliminate frequent words addition rarely occurring words eliminated 
exam saul jordan learning boltzmann trees 
neural computation nov vol 
verbatim introduces large family boltzmann machines trained standard gradient descent 
networks layers hidden units tree connectivity 
show implement supervised learning algorithm boltzmann machines exactly resort simulated mean field annealing 
stochastic averages yield gradients weight space computed technique decimation 
results problems bit parity detection hidden symmetries 
word stems boltzmann train standard gradient descent network layer hidden unit connect learn algorithm boltzmann simul anneal yield gradient weight space result problem detec hidden ghost writer level model base new method gener process differ effect approach set studi develop author level function propos model error method input optim gener neural obtain shown appli output level gener number set neural propos function perform method inform data obtain dynamic input level neural pattern rule number process recogni rate perform classif propos gener input neuron time data level neural optim method rule rate dynamic process pattern studi statist condition adapt limit level perceptron error gener rule calcul deriv simpl output solution level error neural perform statist activ gener number maximum pattern phase level teacher delta output replica student gain dynamic predic generated learn document collection representation terms word stems words lowest perplexity cam words occurring differentiated hierarchy level 
frequent words cam node top words learn learn control learn learn analog program feedback interact network learn algorithm educ control scheme video neural algorithm robot network student oper set dynamic oper multimedia algorithm gener model neural implement space position adapt weight problem propos propos develop arm parallel group descriptions exemplary inner nodes frequent words highest probability words respective cam node 
ple index term representation depicted 
experiments reported typical examples selected larger number performance evaluations 
datasets form core current prototype system collection papers learning including abstracts papers machine learning vol 
learn dataset papers cluster title cluster 
problem consider estimate probability word occurrence text statistical model 
shows probable words different abstraction levels occur original text 
organization helpful distinguish layers trivial unspecific word suggestions highly specific technical terms replica 
important benefits cam resolution specific extraction characteristic keywords 
visualized top levels dataset learn cluster respectively 
hierarchical organization documents satisfying topological relations clusters capture important aspects inter document similarities 
contrast multi resolution approaches distributions inner nodes hierarchy obtained coarsening procedure typically performs sort averaging respective subtree hierarchy 
abstraction mechanism fact leads specialization inner nodes 
specialization effect probabilities suitable cluster summarization 
notice low level nodes capture specific vocabulary documents associated clusters subtree 
specific terms automatically probable words component distribution higher level nodes account general terms 
stress point compared abstraction result probability distributions obtained averaging respective subtree 
summarizes exemplary comparisons showing averaging results high probabilities unspecific terms cam node descriptions highly discriminative 
node specific word distribution offer principled satisfying solution problem finding resolution specific index terms document groups opposed circulating ad hoc heuristics distinguish typical topical terms 
example run interactive coarse fine retrieval cluster collection depicted pretend interested documents clustering texture image segmentation 
real interactive scenario course display just top words describe document groups advanced shifting window approach represent actual focus large hierarchy 
addition description document groups inner node word distributions cam offers possibility attach prototypical documents nodes ones maximal probability ajd compute probable documents query types information cluster summaries level aa ab ba bb learn base new model train process experi develop inform design algorithm function result problem model environ educ design teach algorithm result perform gener network network neural propos method simul perform gener number set inform compar student subject studi instruc result develop new project engin wai problem algorithm method data model task control simul process chang network number rule gener rate unit control nonlinear scheme design robot condition distribu case function studi bound network neural pattern perform data input learner problem develop user tool gener organ process framework manag research search oper task concept new train data visual test observ suggest weight neural neuron inform network robot track error iter network neural process simul method method train propos output neural weight student group program differ result student educ cours instruc commun rule data induct method problem method approach plan agent algorithm control optim search new rule gener perform process class concept learner size target model imag adapt differ program simul graphic power support model student learner program set space logic subject visual studi task differ analog control oper implement onchip plant output studi oper type train perceptron gener error rule calcul network layer function unit error model hypertext represent test differ manag approach market product problem expert reason network neural neuron imag associ pattern rule train feedback dynamic position arm motor model data prior differ bayesian test pattern classif input classifi adapt cours project develop commun experi web network univers project program decision expert propos selec robot environ behavior action task network weight gradient deriv optim inform base studi set data grammar program consist notion rule nonlinear weight instruc learner perform studi inform support think profession concept network model approach produc time product effect rate scheme oper adapt parallel state propos function logic genet weight formula algorithm time boolean method imag recogni pattern recogn top levels cluster hierarchy learn dataset 
nodes represented probable words 
left right successors nodes row depicted row respectively 
similarly left successors nodes row rows aa ba right successors rows ab bb 
approach point genet process model applic cluster learn input vector segment object estim approach base algorithm method cluster propos model observ distrib studi cluster object problem algorithm design fuzzy data algorithm classif feature function author scale cluster state defect tissue brain patient reson pixel texture contour motion spatial center function estim equal vector network neural learn control kohonen image region segment iter color speaker speech train continu recogni example run interactive image retrieval documents texture image segmentation level look ahead cam hierarchy 
locally discriminant keywords keyword distributions nodes automatic selection prototypical documents particularly beneficial support interactive retrieval process 
due abstraction mechanism cluster summaries expected comprehensible descriptions derived simple averaging 
hierarchy offers direct way refine queries utilized actively ask user additional specifications 
cluster abstraction model novel statistical approach text mining sound foundation likelihood principle 
dual organization document cluster hierarchies keyword abstractions particularly interesting model interactive retrieval 
experiments carried small medium scale document collections emphasized important advantages 
model extracts hierarchical structures supports resolution dependent cluster summarizations application large scale databases promising 
level aa ab ba bb cluster result model data model observ distribu studi algorithm method cluster propos cluster state defect function author scale cluster object problem algorithm design data algorithm classif ion increas atom system test gener faint blue sub nonlinear simul approach point genet process model applic cluster learn input vector segment object estim approach base atom format size target redshift alpha distribu univers map imag spatial allow processor graph partition system center function estim equal vector speaker speech train continu recogni grain dynamic network matrix control case complex event time seismic region power scale spectrum partition method algorithm size learn cell format network neural learn control kohonen imag region segment iter color contrast perform wavelength file dynamic qso mass halo cdm video visual index shot object access filter glvq mountain model chip phone hmm loop atom dimer cloud dwarf lsb field background peak fractal jump tau moment constraint environ line task parallel schedul linear cost membership bayesian ellipsoid concept shell pitch phrase glass loop relax conform car lipid transition theta weak omega disc epsilon circuit file matrix neural part network pattern map scale activ brain patient reson cell layer channel grown robust plane perturb root poisson fault descrip stress spectrum mpc cdm power optim order function class similar classif descrip adapt codebook design pixel contour motion spatial top levels cluster hierarchy cluster dataset cf 
comments 
acknowledgments helpful comments jan puzicha sebastian thrun andrew mccallum tom mitchell shatkay anonymous reviewers greatly acknowledged 
thomas hofmann supported daad postdoctoral fellowship 
baker mccallum baker mccallum 
distributional clustering words text classification 
sigir 
croft croft 
clustering large files documents single link method 
journal american society information science 
dempster dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
hofmann puzicha hofmann puzicha 
statistical models occurrence data 
technical report memo ai lab cbcl 
jardine van rijsbergen jardine van rijsbergen 
hierarchical clustering information retrieval 
information storage retrieval 
jelinek mercer jelinek mercer 
interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice 
mclachlan basford mclachlan basford 
mixture models 
marcel dekker new york basel 
pereira pereira tishby lee 
distributional clustering english words 
proceedings association computational linguistics pages 
rose rose gurewitz fox 
statistical mechanics phase transitions clustering 
physical review letters 
willett willett 
trends hierarchical document clustering critical review 
information processing management 
