modular neural networks state art eric peter may comments mech gla ac uk technical report csc centre system control university glasgow glasgow uk global neural networks back propagation neural network clustering neural networks radial basis function neural network leads different advantages 
combination desirable features ot neural ways computation achieved modular neural networks mnn 
addition considerable advantage emerge mnn relevant neural representation plant behaviour 
desirable feature function approximation especially control problems lake neural models 
feature important introduce way mnn local computation models 
enable systematic mnn steps achieved 
task decomposed subtasks neural modules properly organised considering subtasks way communication inter modules integrated architecture 
achieved study main modular applications steps 
study leads main fact systematic mnn depends type task considered 
clustering networks especially local model networks seen mnn frame classification recognition problems 
euclidean distance criterion apply cluster input space leads relevant decomposition properties tasks 
irrelevant apply criteria case function approximation problems 
spatial clustering existing decomposing method ad hoc decomposition organisation architecture achieved case function approximation 
improve systematic mnn framework function approximation essential conceive method relevant task decomposition 
global neural networks model consists interconnected autonomous basic elements 
elements simple function 
complex functions obtained emergent behaviour dynamic interactions simple elements 
computation model quiet old old done mcculloch pitt real emergence domain started roughly hopfield 
self organising map model conceived kohonen important step resurgence neural networks nn applications 
real began rediscovered back propagation bp learning algorithm 
possible enable multi layer perceptron mlp forget poor learning ability perceptron demonstrated minsky papert 
mlp applied wide range domains instance pattern classification function approximation 
really gives neural architecture real credibility scientific frame considering modelling interest theoretic approximation ability 
demonstrated feed forward networks continuous differential transfer function hidden layer approximate continuous function hidden layers approximate uniformly computable function 
requirements apply theory practice consists providing mlp resources correct architecture considering input space obviously algorithm mlp learn input output mapping updating weights 
unfortunately requirements far accessible 
apply mlp bp algorithm leads certain number matters briefly 
back propagation neural network matters considering specific matters arising bpnn mlp architecture bp learning algorithm want highlight important matter kind nn black box matter 
matter relies fact difficult quiet impossible evaluate inside network computed mathematical function describing learning task approximation transfer function involved input output behaviour system model 
matter considerably reduced credibility nn 
rely system know internal representation information 
addition matter appears significant somebody wants apply nn solve particular problem 
case considering input space task nn user find adapted nn learning algorithm proper tune parameters adapted logical function characterises units network adapted network structure 
steps achieved systematically way know happened inside nn especially priori 
choices empirically achieved 
credibility nn considerably reduced black box matter 
specifically case mlp matter leads impossibility give priori resources configure properly approximate considered function 
theoretic approximation proof existence neural resource obvious matter reduces considerably mlp 
concerning bpnn usual matters highlighted nn researchers users fact leads slow learning convergence time learning important learning failures 
main reasons ones 
ffl flat area local minima 
bp learning algorithm intuitively interpreted shape errors surface 
error surface multidimensional surface represent error network weights 
bp learning consists finding minimum error surface gradient error letting marble roll valley surface 
shown authors bp error surface extensive flat areas little slope local minima marble trapped 
matters imply long learning time unsuccessful learning 
ffl interference matter 
case different tasks imply heterogeneous data vectors describing data different vectors opposite directions network reach unstable state oscillation way updating weights 
take example patterns network belongs different tasks appear task 
patterns concern localisation objects space characteristic objects 
difficult network extract existence different tasks adapt task 
problem lead slow convergence learning failure poor generalisation 
matter called spatial crosstalk 
problem mention concerning bp learning algorithm linked important plasticity 
ability interesting learn task matter nn learn task keeping ability solve previous 
case bp learning algorithm forget task learning second 
matter called temporal crosstalk 
matters highly reduce usefulness bpnn 
reason kind nn grow popularity broomhead lowe 
model characterised local computation information 
global versus clustering neural networks global nn mlp characterised fact nodes involved process pattern 
difference clustering nn radial basis function rbf nn process pattern involving part structure 
difference appears clearly compare activation function perceptron neural unit mlp activation function rbf unit 
architecture units fig 

standard rbf unit perceptron unit perceptron wn bias description perceptron rbf units weighted sum input patterns bias achieved operator perceptron 
output perceptron depend activation potential neural function oe neural function linear non linear discrete continuous 
perceptron oe oe wn rbf unit see fig behaves completely different way 
input patterns conceive pattern dimensions euclidean space 
equation see distance metric calculated roughly evaluating euclidean distance pattern centre gaussian unit potential calculated regard width oe rbf unit 
potential near zero important distance near small distance 
output neuron weighted parameter oe gamma oe gammad prototype radius pattern distance components clustering unit unit mlp performs linear non linear separation input space unit rbf nn performs clustering input space see fig 
point view say unit mlp performs linear non linear approximation function unit rbf nn performs approximation 
space clustering bpnn space decomposition example decomposition classes problem consider system see mlp compute globally pattern involving nodes 
words example hidden layer mlp output determined activation neural function oe depending weighted sum nodes output nodes hidden layer depending weighted sum nodes output nodes input layer see fig 
weights represented 
oe nh nh conventional rbf nn fig 
equation compute locally input space 
considering distance pattern centre rbf nodes gaussian probably activated 
variation output neuron input space achieved weighted parameter output layer sum operator case rbf nn global computation performed 
global computation enable nn generalise 
wn output layer input layer layer hidden clustering layer layer input layer decision un standard rbf neural network mlp hidden layer general architecture mlp rbf nn proved rbf nn universal approximator nn applied wide range domains 
bpnn matter limit 
jacobs jordan highlight complementary existing clustering nn global nn 
clustering nn requires relatively learning trials tends interpretable representation global nn said concerning bpnn converge slowly failed learning black boxes 
clustering nn tend memory intensive global nn advantage smaller storage requirement better generalisation performance 
jacobs jordan argued interesting combine desirable features way computing information obtain better neural learning system 
combination achieved modular neural network model mnn 
modular neural networks step definition speaking concept worth define concept 
definition mnn 
important omission 
want section step definition introducing desirable feature neural model considered mnn 
obviously modularity linked notion local computation 
means module independant system interacting architecture order perform function 
achieve local computation considered modular model 
point view module perform explicit interpretable relevant function mechanical physical properties system 
means system model decomposition functions 
clear take simple example consists approximating static function represented dimension 
suppose system driven functions see fig 
theta gammax gammax plot functions function approximated 
nn performs local computation way spatial clustering units region activation input space see previous section details nn 
roughly clustering function fig 
decomposition achieved series linear functions intervals segments see fig 

clustering function compare mechanical properties systems case underlined function appears clearly decomposition achieved irrelevant see fig 
words decomposition quiet meaningless mechanical properties system 
mechanical properties instance degrees freedom system 
relevant decomposition case approximate locally function sum output module obtain function see fig 

decompose system way priori knowledge problem come back 
note control framework exist lot mathematical knowledge behaviour sort plant coming industrial system identification 
priori knowledge achieve relevant decomposition 
rbf nn representation rbf rbf rbf rbf mnn representation irrelevant relevant decomposition system wonder mnn perform relevant decomposition physical mechanical properties system 
point view answer question clear 
representation neural models meaningless possible rely non linear models 
real progress arise mnn simple learning improvement 
way wish clear happened inside neural network locally give non linear approximation tool credibility needs 
reviewing different kinds mnn find advantages mnn single nn 
note advantages theoretical studies empirical results 
advantages modular neural networks advantages computational side wonder researchers nn long disregarded particular field 
instance concerning recognition tasks show performance grows modularity system 
share view muhlenbein says challenge generation neural networks learning individual weight updating composing network modules resolve competition cooperation different modules 
results different applications involving mnn lead general evidence modular neural networks implies significant learning improvement comparatively single nn especially bpnn 
constrain network topology connectivity introduce local computation network increases learning capacity nn permit apply large scale problems 
highly confirmed experience carried 
shows random pruning connections learning improves significantly network performance 
note argue complex behaviour requires bringing different kinds knowledge processing possible structure modularity 
usually mnn implementation divide conquer method 
method consists breaking task smaller complex subtasks learn task different experts nn modules reuse learning subtask solve problem 
example shown nn training greatly simplified identifying subtasks problem embedding network structure 
note method widely computer science ease 
method applied priori knowledge concerning task sufficiently precise enable split task subtasks 
separate task distinct subtasks task trained line integrate global architecture see instance 
enable acceleration learning 
instance divide conquer method applied task 
subtask identify object subtask localise 
input space corresponds retina matrix points 
object shape defined grid 
positions shapes possible 
time step simulation objects placed locations simulated retina 
single bpnn input nodes corresponding point retina matrix hidden nodes output nodes see fig 

output nodes divided subsets nodes respectively responsible identification task localisation task 
concerning architecture modular network difference architecture single bpnn relies hidden layer connectivity output layer 
hidden layer split locally connected output nodes output nodes bpnn nodes hidden layer connected output layer nodes see fig 
hidden nodes solve locally subtask hidden nodes solve subtask best results obtained 
results significantly better obtained single bpnn 
note result modular network worst obtained single bpnn 
learned subtask implies adapted nn architecture learning matters bpnn appear concerning smaller subtasks 
example subtask linearly separates need hidden layer subtask non linear implies complex neural structure subtask 
fact highlights importance priori knowledge split task structure different neural architecture 
single bpnn modular bpnn output layer output layer input layer input layer single modular bpnn architecture solve task example mnn implementation 
task consists steering tractor trailer truck centre dock backing see fig 

emulator dynamic truck takes state truck current wheel angle input produces new state output 
state truck described coordinates back trailer angles trailer relative dock cab relative trailer angle wheel relative cab 
dock located axis 
task achieved position trailer back trailer angle trailer perpendicular dock 
trailer wheel cab dock tractor trailer truck authors decompose task constituent subtasks 
subtasks realised parallel line 
truck oriented trailer nearly perpendicular dock 
control law needed achieve subtask independent truck position requires state variables trailer angle cab angle input 
perceptron needed achieve linear control subtask 
remaining task direct back trailer axis objective dock 
task achieved independently position 
large positive values trailer oriented angle ffi ffi large negative trailer oriented angle gamma ffi subtask needed achieve desired shift orientation 
perceptron driven coordinate saturation 
weight output bias weighted output hidden unit achieve desired shift orientation see fig 
network architecture 
weight nodes structure determinate priori knowledge task initialising randomly usually bpnn 
result obtained training obtained single bpnn hidden units 
authors demonstrate nn training greatly simplified eliminated completely identifying subtasks problem embedding network structure 
identifying skills reduce input space reduce search weight space 
trailer angle wheel angle cab angle modular bpnn steer tractor trailer truck centre dock examples show learning nn reduced improved introducing structural constraints model 
highlight modularity way embed nn priori knowledge known important improve significantly nn learning 
advantages involved mnn single nn 
important advantages modularity imply reduction number parameters weights 
lead better speed computation better generalisation 
assumption confirmed applications see best example arises fact ability generalise feedforward nn inversely proportional number weights involved 
way modular nn avoid interference matters affect global nn 
dividing task subtasks avoid problem temporal crosstalk see section task learned module task learned neural module applied 
interference matter called spatial crosstalk occur tasks computed single nn see section easily avoided module task 
modular nn advantage enable localisation computation system determinate role subsystem subnetwork behaviour system 
knowledge behaviour system nn lakes 
introducing modularity nn way give light internal behaviour 
way transform nn easier system analyse nn interesting tool study behaviour unknown system difficult analyse conventional mathematical models 
step long reach implication justify time spend 
marr modularity enable learning economy 
case change environment modularity permit modify local part system non adapted new shape environment system 
simply modular architecture easily modified 
addition possible reuse processing modules different tasks learning common parts tasks 
advantage modularity concerns possibility hybrid modular neural architecture 
modular architecture possible different neural functions different neural structures layer layer layers different kinds nn algorithms 
function linear non dynamic region non linear dynamic regions 
information known priori interesting modularised nn proper structure 
example task shows important different kind neural structure solve subtask 
example 
implemented multi modular architecture module characterised transfer function 
system obtained significant improvement comparatively single nn concerning speech recognition face recognition optical character recognition 
cite cooperated architecture mlp kohonen network achieve best classification 
mention architecture conceived fewer mlp cooperate successfully obtain best generalisation 
quite reasonable assumption bpnn reach different minima state learning task generalise better different point input space 
advantages highlight interest apply modular nn single nn 
problems remain solved apply mnn systematically real algorithm applying ad hoc 
modular neural network state art mnn imply execution steps ffl decomposition task subtasks ffl organisation modular architecture ffl communication modules conceive model mnn fundamental achieve steps systematically 
interesting review representative mnn purposes 
permit understand clearly problems solve order go systematic mnn 
ad hoc versus systematic decomposition environment decomposition task subtasks step apply modularity nn 
keep mind point view decomposition meaningful mechanical physical properties task system 
problem different regard kind task nn solve 
task relies clustering input space recognition tasks decomposition task quiet obvious corresponds clustering input space 
case autonomous split input space decomposition task achieved clustering techniques self organizing nn 
expect decomposition achieved spatial criterion generally relevant spatial properties tasks 
words instance generally relevant meaningful objects room spatial properties components colour red spatially colour bleu differentiation achieved shape object distance components 
spatial clustering quiet developed mnn models suitable purpose 
main ones followings 
approach mention concerns obviously clustering nn head rbf nn 
said previous section nn performs clustering input space way rbf unit 
problem nn suffers curse 
forces system exponentially increasing numbers basis function units approximate function input dimension increases 
words system memory intensive regard complexity problem 
number hidden units reduced complex function linear weighted parameter achieved called local model network lmn introduced 
system see fig 
enables clustering unit cover larger areas input space 
addition highlights considering priori knowledge cluster input space cover unit possible different kind appropriate functions 
necessary transform global learning error network calculated sum output rbf units performs standard rbf nn local learning 
change permit avoid global learning matter leads difficulties estimate efficiently units update function global error 
system implemented successfully review fewer details learning capacity estimation nn see thesis 
un general architecture local model network nn models categorised models main difference rely self clustering approach supervised 
model developed 
model named calm cate learning module 
calm module tends autonomously cluster stimuli belonging region way inhibitory connections neurons involving competition calm module 
note highlights fact model developed psychological neurological consideration mathematical usually 
model appears interesting life science point view drawbacks considering usefulness 
number parameters excessive unclear correlation architecture learning abilities difficult set optimal multi calm model 
example self clustering mnn model 
learning procedure relies phases 
find centre partition unsupervised learning achieved 
phase supervised learning done train weight fraction corresponding partition 
example considered consists kohonen algorithm cluster input space apply locally global nn 
different approach seen clustering input space embedded weight sharing networks time delay neural networks tdnn 
hidden layer unit connected input layer locally input layer units 
subsequent layer organised way regular feed forward fashion see fig 

models applied great success frame pattern recognition 
review architecture see instance 
note neocognitron probably systems mnn systems 
neocognitron models capable perform rotational translational invariant recognition line segmentation stream patterns encountered connected text 
achieved trough extreme modularisation processing layers modification basic learning rule difficult implement computationally expensive 
space input ouput layer sharing nodes example time delay neural network just seen mnn usually decomposition input space clusters different decomposition achieved architecturally time delay neural networks relies euclidean distance patterns criteria clustering 
apply criteria implies obviously euclidean distance patterns pertinent 
think case tasks 
example continuous dynamic function locally split spatial criteria 
words pertinence spatial clustering approximation function 
assume complex criteria spatio temporal criteria spatial pertinent dynamic case 
theoretical investigation need done purpose 
way considering case dynamic functions quiet obvious split task pertinent subtasks achieved simple criteria euclidean distance 
take trivial example driving car 
subtasks followings change speeds directing cars steering wheel overtaking car 
input space case heterogeneous see spatial criteria applied split efficiently 
fact case tasks input heterogeneous decomposition achieved ad hoc way priori knowledge 
case modular nn applications see instance examples previous section 
agree said decomposition task generally priori knowledge problem 
task rely clustering framework priori knowledge available concerning task way split systematically 
step apply modular nn clear find criteria method split efficiently task subtasks main matter solve enable systematic mnn 
note conceive method criteria difficult impossible 
method applied natural systems 
clear front complex tasks natural systems try adapt problem try solve way progressive complexity learning 
words behaviour consists decomposing task basic subtasks learn line solve complex task reusing learning subtasks 
behaviour corresponds self organisation described consists developing structure interacting environment 
answer problems split task subtasks involves mathematics computing formalisation investigation life science 
considering problem organisation modular architecture want mention existence done mainly jacobs jordan section 
clear model ability split non clustering task subtasks autonomously leads task decomposition problematic fashionable algorithm interesting incorporate 
neural model jacobs jordan composed different expert networks neural modules gating network see fig 

expert feedforward network experts receive input number outputs 
gating network feedforward regards information available task receive inputs experts receive fig getting net receives different inputs expert nets 
learning algo consists making different experts networks learn training patterns 
gating network mediates competition determining certain error expert example expert suitable case 
simple equation describing calculation cost function network example gamma jj probability answer expert example gamma jj scared error performs expert example 
depth explanation algorithm see 
brief gating network decides function output quality experts expert training example 
criteria apply decompose input space spatial case lmn criteria classification 
note general architecture nn quiet similar 
fact main difference algorithms fact expert networks models jacobs jordan non linear functions case lmn rbf units local function associated units linear functions 
expert layer gating network output layer xn task task input layer input patterns specifier task general architecture adaptive mixture competitive experts model behaviour jacobs jordan algorithm interpreted slightly different manner regard architecture 
considered architecture gating net expert net driven inputs 
architecture may applied available priori knowledge concerning task simply task decomposed 
example consists discriminating vowels regard second formants 
appears different experts learned concentrate pair classes 
results obtained quiet similar ones obtained single bpnn 
interpret results regards weight space different experts evolved due random initialisation weights different neural architecture neural expert 
different weight space leads different learning direction different answers networks considered patterns 
networks better different patterns 
gating network determining experts suitable training pattern weights output learning neural networks progressively expert respectively different class patterns patterns sharing scalar similarities 
advantage methode compare single bpnn possibility avoid problem spatial crosstalk arises making compute single nn patterns 
second architecture algorithm applied consists give getting net specification task different patterns belongs 
explain property architecture vision task explicit 
task seen different subtasks shown described previous section 
time step simulation objects placed locations simulated retina 
task identify object task identify location 
case expert networks driven retinal image 
task specifier indicates architecture perform task task current step 
gating network receives specification task 
different experts integrated structure 
hidden layer hidden layer 
task linearly separable authors expect expert hidden layer selected gating network compute 
result obtained obtained mnn structured ad hoc 
interestingly results show model capable allocate different experts different tasks 
expert hidden layer involved learn task implies model capable allocate expert net appropriate structure task 
interpret learning behaviour algorithm regard weight space networks evolved 
nn evolves different weight space suitable compute certain task class patterns 
gating network weights output experts regard task specification pattern learning nn expert task 
assume faster nn efficient compute patterns belonging task higher probability nn expert task 
obviously explain example linear problem allocated gating net suitable hidden layer nn 
control point view approach jacob jordan noticed 
highlight fact gain scheduling method seen piecewise method 
method consists apply different control laws different operating conditions 
operating condition view local model task 
authors give example task consists drive simulated joint robot arm variety payloads different mass desired trajectory 
payloads correspond case different operating conditions 
gating network receives input payload identity expert networks receive state robot arm desired acceleration 
result shows model jacobs jordan learns significantly faster single bpnn 
due global nature bpnn leads interference matters see section 
brief learning time imply model significantly faster imply bpnn results obtained generalisation rate learning capacity applying algorithm generally significantly better ones obtain bpnn ad hoc modular bpnn 
generalisation algorithm hierarchical em algorithm lead better results 
note addition task specification pattern determined priori algorithm considered neural model performing autonomous decomposition task 
fact capacity allocate certain pattern belonging certain task suitable nn constitutes main advantage algorithm frame mnn 
note fact equations underlying algorithm defined properties algorithm explained authors constitutes point view important advantage compare neural algorithms obscure 
ad hoc versus self structure organisation method providing useful architecture constraints 
organise modules 
instance applied genetic algorithm find appropriate modular structure calm model 
think rely genetic algorithm best way achieve configuration reason reasonable driven blend algorithm 
authors tried conceive methods structure systematically mnn 
instance case done number expert modules determinate number classes 
model lead significant better results ones obtained single nn 
regard behaviour biological neuronal structure best way achieve modular organisation enable neural network develop modular structure progressively interacting environment suggested 
interesting investigate modularisation achieved ambitious biological point view 
step achieve interactive modularisation integrate algorithm way constrain structure network learning 
known ways achieve neuron pruning incremental methods 
possible prune removing neurons decaying weights 
main useful method chose neuron prune described 
consists finding weights average data base implies smallest variation cost function 
review pruning algorithms see 
second fashion act structure neural network network learning state achieved incremental algorithms 
case structure grows learning 
main advantage method avoid algorithm bp involve matters 
review see quiet old new incremental algorithms ones 
example modularisation 
model considered improvement cascade correlation algorithm conceived fahlman lebiere 
algorithm incremental hidden units learning structure grows learning see fig 

unit trained maximise correlation output residual error seen network output 
correlation maximised learning stabilised unit frozen 
error system satisfactory unit added 
receives output units existing layer nodes hidden nodes 
hidden units added interconnected feedforward manner structure learns task 
note incremental cascade algorithms conceived see instance 
model interesting avoids bp learning algorithm perceptron trained network 
problem network grow large function task appears larger structure lower learning 
avoid matters add model process specialisation learning implemented competition pruning useful connections 
modularisation point view authors imply model independently expert 
decomposition task achieved case ad hoc 
expert trained 
tasks common relations show existence transfer knowledge relying reduction training time 
matter system applied subtasks common parts subtasks subpart system generally 
matter system remains autonomous regard decomposition task decomposition done ad hoc 
stage stage stage stage growing architecture cascade correlation model autonomous interactive modularisation purpose interesting algorithm rce algorithm performs incremental clustering learning input space decomposed spatially learning need network 
unit layer hypersphere influence input space 
unit solve problem unit added radius prototype previous unit adapted 
second layer enables decision classification see details algorithm 
clustering point view rce algorithm rbf nn interesting 
cases spatial clustering applied see previous section emerge modularity neural network learning remains unsolved problem 
want mention emergence modularity learning seen dynamic point view purely structural point view cases mentioned 
modularity seen activity correlation nodes certain period time case biological neurons 
hebbian rules way 
life science spatio temporal contiguity known important criteria structure information 
brain activity studies full description phenomena 
note tried conceive modular system updating mode combined hebbian learning 
authors report system enable specialisation cells emergence neuron assemblies related structure environment 
obtain interesting results toy tasks xor 
addition authors report sequence learning existence complex dynamic network involving answer system 
model improved 
question system improved 
biological neural network interactive emergent modular organisation system relies neural dynamism 
ambitious try conceive system 
communication modules question section concerns crucial problem interact properly modules perform global task generally achieved decision layer 
usually method consists selecting answer strongest value case winner take layer 
way avoid decision layer reconnaissance system enables send patterns features expert network patterns experts 
type processing performed gating networks model jacob jordan clustering input space achieved local model networks 
considering result obtained nn global answer modular system quiet easily achieved consider basic cases subtasks independent intersection hyperspheres instance 
lot examples tasks global answer depends answer different experts 
case task consisting control tractor trailer truck steer centre dock 
case orient wheel angle constitutes final output necessary reuse answer expert orient truck trailer nearly perpendicular dock bias weighted output hidden unit 
neocognitron time delay neural networks example reuses learning neural network lower layer perform classification complex higher layers 
systems implement reuse experts learning simplest way consists outputs different experts inputs experts 
reuse learning experts way imply satisfactory results classification frame 
bottou gallinari introduced formal framework training cooperative systems 
problematic describe modules interact learning considering nature expert networks 
introduced way identify role expert concerning output structure 
system enable cooperation architecture different kind nn instance local global neural models nn symbolic models 
communication matter leads general involved hierarchy experts known usual way reduce complexity task 
way assume expert complex task learn 
addition architecture forces system develop meaningful internal representation 
idea corresponds minsky idea consists constructing architecture interacting modules produce higher level behaviour 
note complex cognitive task broken simpler subtasks performed sequentially task depending output 
problem modular scheme expert networks understand output experts 
try answer question proposing global vocabulary enable communication modules 
architecture module represent processing knowledge explicitly output 
model conceived suitable language processing task difficult generalise tasks 
question communication inter modules remains open 
mnn leads advantages clear development general model enable broaden nn 
advantages think interesting concerns possibility enhance nn clearest box possibility understand easily behaviour nn 
important matters remain solved enable systematic mnn 
lake formalisation important matters 
define rigorously mnn certainly step reach transform mnn real model 
introduced desirable feature mnn local computation meaningful mechanical physical properties system 
far formal definition step 
practical point view important matter primordial concerns decomposition task subtasks 
matter important considering kind task nn applied 
task relies spatial clustering recognition ones systematic mnn apply 
instance local model networks suitable cases 
quiet obvious lot tasks relying non linear dynamic functions decomposed relevant manner spatial criteria 
case decomposition complex matter 
matter old real solution proposed 
natural systems apply decomposition important usefulness method essential investigate deeply topic interdisciplinary framework 
design evolution modular neural network architectures neural networks vol 
pp 

mcculloch pitts logical calculus ideas nervous activity bulletin mathematical biophysics vol 
pp 

hopfield neural networks physical systems emergent collective computational abilities proceeding national sciences vol 
pp 

kohonen self organized formation topologically correct feature maps biological cybernetics vol 
pp 

lecun de apprentissage 
phd thesis universite paris vi paris france 
mcclelland parallel distributed processing vol 

cambridge mit press 
papert perceptron 
cambridge ma mit press 
hornik new results neural network approximation neural networks vol 
pp 

cybenko approximation superposition sigmoidal function mathematics control signal systems vol 
pp 

cybenko continuous valued neural networks hidden layers sufficient tech 
rep departement computer science tufts university ma 
lapedes farber neural nets neural information processing systems anderson ed pp 
new york american institute physics 

choi choi partially trained neural networks partition unity proceeding international joint conference neural networks 
hecht nielsen neurocomputing 
addison wesley 
cave processed separate cortical visual systems 
computational investigation journal cognitive neuroscience vol 
pp 

probabilistic approach provides modular adaptive neural network architecture discrimination third international conference artificial neural networks vol 
pp 

nowlan hinton evaluation adaptive mixtures competing experts neural information processing systems lippmann ed vol 

jacobs jordan barto task decomposition competition modular connectionist architecture vision tasks cognitive science vol 
pp 

calm categorizing learning module neural networks vol 
pp 

broomhead lowe multivariable functional adaptive networks complex systems vol 
pp 

park sandberg universal approximation radial basis function networks neural computation vol 
pp 

jacobs jordan competitive modular connectionist architecture neural information processing systems lippman eds vol 

francesco functional networks 
phd thesis des sciences de universite de 
lecun generalization network design strategies tech 
rep crg tr connectionist research group university toronto 
martinez offset une methode de construction de de neurones son application la conception un automobile 
phd thesis universite paul sabatier toulouse france 
kaski choosing optimal network structure proceeding international neural conference widrow eds pp 

feldman neural representation conceptual knowledge neural connections mental computation nadel eds cambridge ma mit press 
simon sciences artificial 
cambridge ma mit press 
jenkins simplified neural network solution problem decomposition case truck backer upper ieee transaction neural networks vol 
pp 

hampshire waibel meta pi network building distributed knowledge robust pattern recognition tech 
rep cmu cs carnegie mellon university pittsburgh pa 
fogelman soulie multi modular neural network hybrid architectures review proceedings international joint conference neural networks 
bottou framework cooperation learning algorithms neural information processing systems lippmann eds vol 
morgan kaufmann 
baum haussler size net gives valid generalization neural computation vol 
pp 

modular learning neural networks 

jacobs jordan learning piecewise control strategies modular neural network architecture ieee transaction systems man cybernetics vol 
pp 


auger cooperation modularity classification trough neural network techniques international conference systems man cybernetics systems engineering service humans vol 
pp 

hoffman vidal cluster network recognition cursive script characters neural networks vol 
pp 

murray smith local model network approach nonlinear modelling 
phd thesis dept computer science strathclyde glasgow scotland uk 
johansen foss model representation adaptive control local model modeling identification control vol 
pp 

murray smith local model networks local learning fuzzy duisburg pp 

miikkulainen dyer natural language processing modular pdp networks distributed lexicon cognitive science vol 
pp 

fogelman soulie gallinari advanced neural networks form theory applications proceeding 
guyon albrecht lecun denker hubbard time delay neural network character recognizer touch terminal proceeding international neural network conference vol 
paris france pp 

lang hinton time delay neural network architecture speech recognition tech 
rep carnegie mellon university pittsburgh pa 
waibel hinton shikano lang phoneme recognition time delay neural networks tech 
rep tr advanced telecommunication research institute japan 
fukushima neocognitron self organizing neural network model mechanism pattern unaffected shift position biological cybernetics vol 
pp 

fukushima recognition segmentation connected characters selective attention neural networks vol 
pp 

action learning non symbolic systems conference cognitive science 
jacobs jordan nowlan hinton adaptive mixture local experts neural computation pp 

jordan jacobs hierarchical mixtures experts em algorithm neural computation vol 
pp 

lecun denker optimal brain damage nips vol 
pp 

jutten pruning methods review proceeding european symposium artificial neural networks verleysen ed pp 

reed pruning algorithms survey ieee transaction neural networks vol 
pp 

castillo incremental neural networks survey tech 
rep grenoble france 
lucas growing adaptive neural networks graph grammars proceeding european symposium neural network ed pp 

campbell vicente construction feed forward neural networks binary classification tasks proceeding european seminar neural network ed brussels 
torres moreno gordon coupled optimal perceptron learning proceeding european symposium artificial neural networks ed pp 

friedman self organizing modular neural networks international joint conference neural networks ijcnn seattle pp 

fahlman lebiere cascade correlation learning architecture tech 
rep 
lange voigt wolf growing artificial neural networks correlation measures decomposition local attention neurons proceedings ieee international conference neural networks vol 
pp 

chiang fu divide conquer methodology modular supervised neural network design proceedings ieee international conference neural networks vol 
orlando pp 

chen cascaded network world congress neural networks pp 

reilly cooper neural model category learning biological cybernetics vol 
pp 

hebb organization behavior 
new york wiley 
anf architectures pour le behavior control may 
korf planning search approach artificial intelligence vol 
pp 

theory hierarchical multilevel systems 
new york academic press 
jacobs initial experiments constructing domains expertise hierarchies connectionist systems connectionist models summer school pp 

minsky society mind 
new york simon schuster 
ballard modular learning neural networks sixth national conference vol 
pp 
july 

