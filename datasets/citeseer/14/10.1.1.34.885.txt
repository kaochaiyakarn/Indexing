proceedings ai adams sterling eds singapore world scientific learning continuous classes quinlan basser department computer science university sydney sydney australia empirical learning tasks concerned predicting values familiar categories 
describes new system constructs tree piecewise linear models 
case studies compared methods 

branch machine learning empirical learning concerned building revising models light large numbers exemplary cases account typical problems missing data noise 
models involve classification learning algorithms generate decision trees efficient robust relatively simple tasks require learned model predict numeric value associated case class case belongs 
instance case composition alloy associated value temperature 
researchers attempted decision tree methods value prediction dividing value range small categories systems build classification models 
attempts fail partly algorithms building decision trees implicit ordering classes 
effective learning methods available predicting real values 
cart program builds regression trees differ decision trees having values classes leaves 
mars elegant system constructs models basis functions splines 
classical statistical methods multiple linear regression address task postulating simple form model finding parameter values maximise fit training data describes new system learning models predict values 
cart builds tree models regression trees values leaves trees constructed multivariate linear models model trees analogous piecewise linear functions 
learns efficiently tackle tasks high dimensionality hundreds attributes 
ability sets cart apart mars computational requirements grow rapidly dimensionality effectively limiting applicability tasks attributes 
advantage cart model trees generally smaller regression trees proven accurate tasks investigated 
gives overview algorithm constructing model trees reports performance learning tasks 

constructing model trees suppose collection training cases 
case specified value fixed set attributes discrete numeric associated target value 
aim construct model relates target values training cases values attributes 
worth model generally measured accuracy predicts target values unseen cases 
tree models constructed divide conquer method 
set associated leaf test chosen splits subsets corresponding test outcomes process applied recursively subsets 
division produces elaborate structures pruned back instance replacing subtree leaf 
step building model tree compute standard deviation target values cases contains cases values vary slightly split outcomes test 
potential test evaluated determining subset cases associated outcome denote subset cases ith outcome potential test 
treat standard deviation sd target values cases measure error expected reduction error result test written sd gamma jt jt theta sd examining possible tests choose maximises expected error reduction 
comparison cart chooses test give greatest expected reduction variance absolute deviation 
major innovations come play initial tree grown 
detailed discussion precluded length main ideas error estimates needs estimate accuracy model unseen cases 
residual model case just absolute difference actual target value case value predicted model 
estimate error model derived set training cases determines average residual model cases 
generally underestimate error unseen cases multiplies value gamma number training cases number parameters model 
effect increase estimated error models parameters constructed small numbers cases 
linear models multivariate linear model constructed cases node model tree standard regression techniques attributes model restricted attributes referenced tests linear models subtree node 
compare accuracy linear model accuracy subtree ensures level playing field types models information 
simplification linear models linear model obtained simplified eliminating parameters minimise estimated error 
elimination parameters generally causes average residual increase reduces multiplicative factor estimated error decrease 
uses greedy search remove variables contribute little model cases removes variables leaving constant 
pruning non leaf node model tree examined starting near bottom 
selects final model node simplified linear model model subtree depending lower estimated error 
linear model chosen subtree node pruned leaf 
smoothing pregibon observes prediction accuracy tree models improved smoothing process 
value case predicted model tree value model appropriate leaf adjusted reflect predicted values nodes path root leaf 
form smoothing differs developed pregibon motivation similar 
smoothed predicted value backed leaf root follows ffl predicted value leaf value computed model leaf 
ffl case follows branch subtree number training cases pv predicted value value model predicted value backed pv theta pv theta smoothing constant default 
smoothing effect case models path predict different values models constructed training cases 

case studies evaluated learning tasks results methods available 
comparing approaches common measure performance relative error ratio variance residuals variance target values 
useful statistic correlation actual predicted values correlation coefficient indicates linear relationship actual predicted values 
authors report percentage deviation average cases ratio residual target value useful target values include 
trials performance assessed way cross validation available data divided equal sized blocks 
block turn model constructed cases remaining blocks tested cases hold block 
case tested model constructed case 
results reported obtained default values parameters minimum number cases split smoothing constant 
show effects forming models leaves smoothing results features disabled 
rm mmax rm mmax rm average model rm rm rm cycle time min mem max mem cache size min max model tree cpu performance original attributes transformed attributes correlation percentage correlation percentage deviation deviation ein dor ibp smoothing models table cpu performance data cpu performance ein dor carried experiments predict relative performance new cpus function easily determined variables cache size cycle time minimum maximum channels minimum maximum memory 
attributes directly developed composite attributes 
cases constructed linear model square root performance function transformed attributes 
kibler aha albert studied data instance approach 
ibp predicts case target value interpolating known values similar cases 
results reported original transformed attributes 
ibp gave better predictive accuracy ein dor model need resort indirect methods predicting square root target value 
run cases original attributes produced compact model tree 
results original transformed datasets shown table 
note reported accuracies ein dor performance training data model derived model presumably give worse results unseen cases 
predictive accuracies ibp shown table obtained cross validation way ibp way 
accuracy models developed comparable ibp performance degraded original data smoothing disabled markedly datasets system prevented forming models leaves 
correlation percentage relative deviation error ibp smoothing models table car price data car prices kibler aha albert evaluated ibp collection car descriptions 
case values attributes number doors engine size target value car price 
task considerably challenging previous higher dimensionality fewer training cases 
table shows results ibp data gave higher percentage deviation ibp contribution smoothing linear models apparent 
artificial dataset breiman reported regression tree results small dataset affected noise 
study variables takes values values equiprobable 
generating function target value gamma gamma random gaussian variable variance representing noise 
training cases cart constructed regression tree leaves 
similar dataset produced model tree just leaves hope 
table compares published results cart experimental results obtained way cross validation 
better performance data demonstrates clearly benefit allowing linear models just averages leaves 
activity drugs example concerns high dimensional real world task predicting correlation relative error cart smoothing models table artificial task correlation relative error smoothing models linear regression table drug activity data activity levels peptides strings amino acids 
data provided pharmaceutical consists cases records values attributes measured activity 
cross validation trial model trees constructed average leaves quite simple 
results table show contribution smoothing linear models 
comparison table shows performance standard multivariate linear model cross validation runs 

model trees regression trees learned efficiently large datasets model tree car price data constructed second data cases theta attributes required minute decstation 
model trees advantages regression trees compactness prediction accuracy attributable ability model trees exploit local linearity data 
noteworthy difference regression trees give predicted value lying outside range observed training cases model trees extrapolate 
advantage cause concern open question 
strengthened respects 
am exploring alternative heuristics guide selection tests internal nodes different smoothing algorithms 
models leaves generalised allow non linear functions attributes albeit increase computation 
pharmaceutical san francisco permission data 
research supported australian research council research agreement digital equipment 

breiman friedman olshen stone classification regression trees wadsworth belmont ca 

ein dor attributes performance central processing units relative performance prediction model communications acm 

friedman multivariate adaptive regression splines technical report laboratory computational statistics stanford university stanford ca 

kibler aha albert instance prediction real valued attributes technical report ics university california irvine ca 

pregibon private communications 

press flannery teukolsky vetterling numerical recipes cambridge university press 

quinlan programs machine learning morgan kaufmann san mateo ca 
