fl kluwer academic publishers boston 
manufactured netherlands 
scaling inductive learning massive parallelism foster john provost foster com nynex science technology avenue white plains ny john aronis aronis cs pitt edu intelligent systems laboratory university pittsburgh pittsburgh pa editor douglas fisher 
machine learning programs need scale large data sets reasons including increasing accuracy discovering infrequent special cases 
current inductive learners perform hundreds thousands training examples cases examples may necessary learn important special cases confidence 
tasks infeasible current learning programs running sequential machines 
discuss need large data sets prior efforts scale machine learning methods 
discussion motivates strategy exploits inherent parallelism learning algorithms 
describe parallel implementation inductive learning program cm connection machine show scales millions examples show uncovers special case rules sequential learning programs running smaller datasets parallel version learning program preferable sequential version example sets larger examples 
learning public health database consisting examples parallel rule learning system uncovered surprising relationship led considerable follow research 
keywords inductive learning parallelism small disjuncts 
scale 
current inductive learning programs practically large data sets examples 
catlett estimates realworld learning tasks data items require months dedicated workstation 
outlines reasons large data sets necessary summarizes past efforts scale machine learning methods 
effective way scale standard rule learner massive parallelism implementation cm connection machine 
public health domain program discovered relationships current sequential machines 
relationship led considerable follow research public health collaborators 
important reasons machine learning methods scale large data sets 
obvious reason maximize accuracy 
comprehensive date scaling machine learning catlett collection large data sets 
domain halving size training set produced statistically significant decrease accuracy 
cases degradation accuracy learning small samples stems fitting due high dimensionality concept description language due need allow program learn rules known small holte acker porter correspond special cases concept 
small disjuncts cover data items learning programs difficulty learning rules confidence 
unfortunately domains special cases account large portion concept provost 
domains high accuracy learning depends ability learn special cases 
noise complicates problem small sample impossible tell difference special case spurious data point provost weiss 
classification accuracy aside small disjuncts interest scientists business analysts precisely rules unknown previously analysts usually know common cases 
consider machine learning aid public health research 
may case general low infant mortality rate 
inductive learner trying describe class low infant mortality look linked birth infant death data produce rule japanese american low infant mortality 
learning rule high confidence problem rule represents substantial portion data rule covers small percentage data sample set examples contain instances infer rule confidence 
sample set contains japanese americans situation form small percentage births learner draw degree certainty 
sum sample large contain instances special case generalize rule confidence 
learning rules smaller subgroups japanese american population require larger samples 
example japanese americans live east coast births 
order examples generalize rule need approximately examples 
practice desirable examples generalize reduce probability rule looks chance due generation testing alternative hypotheses 
example japanese americans living east coast increasing number examples required generalize rule order magnitude pushes total number examples required 
important note principle scaling eliminate problem small disjuncts data set smaller special cases learned confidence 
clear scaling large data sets implies part faster learners developed 
course motivations fast learners 
example interactive machine learning buntine machine learner human analyst interact real time requires fast learning algorithms order practical 
automatic bias selection gordon desjardins requires fast learners systems evaluate learning multiple biases evaluation may involve multiple runs produce performance statistics cross validation experimenting biases requires large data sets avoid fitting due bias selection 
addition implications learning time scaling large data sets may require space efficient algorithms space limited platforms 
turn attention number strategies seek scale learning methods large data sets designed related goals reducing learning time space complexity 

scaling inductive learning done 
approaches take apply symbolic machine learning large problems 
straightforward albeit limited strategy scaling fast simple method 
strategy may silly consider results holte 
holte showed degenerate level decision trees called decision stumps performed terms accuracy commonly databases 
algorithm learning decision stumps fast method prohibits learning complex concept descriptions 
decision stump results suggest fast simple learning algorithm may effective tool scaling large databases 
catlett applied strategy simplifying learner representation language problem scaling showed discretization numeric attributes reduce run time decision tree learner corresponding decrease accuracy 
second strategy optimize learning program search representation possible 
optimization may involve identification constraints exploited reduce algorithm complexity efficient data structures bit vectors hash tables binary search trees 
segal etzioni highly optimized rule learner uses clever search reduction techniques efficient data structures 
learning time issue code optimization engineering practice complements methods scaling describe 
fast simple methods adequate optimization strategies necessary scale learning methods 
common method coping infeasibility learning large data sets select smaller sample initial data set 
catlett studied variety strategies sampling large data set 
despite advantages certain sampling strategies catlett concluded solution problem scaling large data sets 
sampling adequately address main reasons large data sets small samples generally reduce accuracy inhibit learning infrequent special cases 
catlett studied strategies reducing complexity associated description languages containing numeric attributes 
looking subsets examples searching split values numeric attributes run time decision tree learners significantly reduced corresponding loss accuracy 
strategies place run time learners linear number examples learning large data sets prohibitively expensive 
techniques complementary methods described learning parallel 
incremental batch learning clearwater cheng hirsch buchanan cross sampling incremental learning schlimmer fisher utgoff 
incremental batch learners process subsamples examples sequence learn large training sets 
incremental batch learning scale example sets large pure batch processing provost buchanan 
approach effective learners principle scale linearly number examples entire example set fit main memory memory management thrashing render learner useless 
approach scaling studied gaines gaines primary goal unify manual automatic knowledge acquisition 
particular gaines analyzed extent prior knowledge reduces amount data needed effective learning 
unfortunately pinpointing small set relevant domain knowledge begs question machine learning 
techniques background knowledge scale large knowledge bases 
aronis provost parallelism enable massive networks domain knowledge aid constructing new terms inductive learning 
discuss important class strategies deal large problems decomposing learning problem parallel machines process different pieces simultaneously 
approaches parallelization identified 
coarse grained approach data divided set processors processor parallel learns concept description set examples concept descriptions combined 
shaw sikora take approach genetic algorithm combine multiple concept descriptions experiment large data sets 
chan stolfo take coarse grained approach allow different learning programs run different processors 
approach takes advantage existing learning algorithms parallel infrastructure needs programmed 
unexpectedly sampling techniques may degrade classification accuracy compared learning entire data set 
provost hennessy coarse grained parallelization individual learners cooperate guaranteed rule considered acceptable distributed learner considered acceptable monolithic learner entire data set 
approach successful large data sets 
coarse grained parallel learning algorithms utilize loosely coupled computers distributed processing setting implemented successfully mimd multiple instruction multiple data parallel architecture 
second approach parallel learning rule space parallelization search rule space decomposed different processors search different portions rule space parallel 
type decomposition similar parallelizing forms heuristic search 
stated massively parallel simd single instruction multiple data machines inherently unsuitable parallel heuristic search bobrow researchers implemented heuristic search routines ida simd architectures impressive results cook lyons ferguson korf daniels 
portions search tree processors performs heuristic search 
previous dealt search mimd machines example rao kumar discuss parallel depth search kumar rao rao kumar 
mold cook holder cm connection machine rule space parallelization aq michalski hong lavrac 
aq parallelized specializing elements star overly general concept description simultaneously beam search 
approach processors cm handle problems fifteen features fewer 
maximum fifteen features imposes strict limitation utility learning program 
general type parallelization address problem large data sets 
rules distributed processors processor deal data address inability current processors deal massive data sets processor deal subsets data run problems subsampling 
load balancing issue order take full advantage parallel processing power 
load balancing interprocess communication add additional overhead 
cook holder discuss implementing rule space parallelization id quinlan conclude difficult implement provide benefit sequential id procedure 
cook holder take similar approach parallelizing perceptron method rule space parallelism scale connectionist methods rumelhart hinton williams example zhang mckenna waltz utilized massive parallelism cm parallelization backpropagation neural network 
third parallelization approach stems identification major bottleneck learning large data sets distribution computation addresses bottleneck 
specifically inductive learning programs fall generate test paradigm 
typical artificial intelligence search problems major computational cost due fact nodes generated 
previous massively parallel search concentrated distributing generation testing nodes processors 
search inductive learning differs ai searches inductive learning cost evaluating node expensive 
nodes search tree partial rules decision tree branches hypothesized matched examples 
results match guide generation subsequent hypotheses 
problem examples matching dominates computation 
approach utilizes parallel matching 
approach similar taken lathrop ariel system lathrop webster smith winston example set distributed processors massively parallel machine 
ariel run data sets larger examples reasons 
biological problem investigated consisted examples second ariel method decomposition instances maximum possible available processor cm connection machine lathrop 
stanfill waltz parallel matching approach case learning large databases 
memory learning mbl approach uses connection machine find similar instance large database 
approach inherently different parallelizing type generalization algorithm addressed 
mbl processing done new example classified approach learning concept description precedes classification concept description parallel machine necessary classification 
parallel mbl approach suitable batch classification large set examples due overhead loading data parallel machine 
furthermore mbl style learning interesting special cases apparent form explicit generalizations 

rl learning program sequential parallel variations 
rl learning program provost buchanan clearwater lee clearwater provost descendant meta dendral buchanan mitchell 
rl uses heuristic search algorithm generate series rules tests set data 
practice rl find interesting individual rules 
set rules learned rl forms disjunctive class description optimized standard techniques described example quinlan 
rl performs straightforward general specific search space rules defined conjunctions attribute value pairs features 
goal rl search find rules satisfy user defined criteria 
particular experiments rl searches rules satisfy thresholds positive threshold specifies minimum number positive examples rule cover negative threshold specifies maximum number negative examples rule may cover 
thresholds relaxes constraints coverage discovered rules mitigating effects noise data effects inadequate representation language 
rule set conditions predicted class rl evaluates statistically 
space possible rules includes possible combinations conditions size search space grows exponentially allowable number conditions rule 
rl uses beam search ensure time complexity search linear number conditions 
beam evaluation function defined user experiments reported signal noise function roughly percentage positive examples covered rule divided percentage negative examples covered 
rule tested entire set data calculate performance statistics 
introduces linear factor complexity algorithm data described solely nominal attributes log factor program searches numeric features 
data sets examples testing dominates computation 
data sets millions examples time spent checking rules data run days weeks making learning large data sets impossible practical standpoint 
simd parallel architectures cm connection machine consist front workstation issues instructions thousands processors executed simultaneously 
provides perfect match generate test inductive learning programs rl 
front generates partial rules partial rule tested data residing individual processors 
rule created broadcast cm processors simultaneously match data residing processor 
results matches sent back front guide generation subsequent rules 
sequential machine checking rule data items takes cn time simd machine processors takes time check rule 
large massively parallel machines cm favorable speedup despite fact cm individual processors relatively slow bit serial processors large 
notice processors check rule data items stored processor local memory 
communication overhead interprocessor communication 
theory collecting results individual processors returning aggregate front takes log time special hardware factor insignificant 
complexity analysis may obscure main point parallelism allows scale data sets orders magnitude larger previously possible 
checking rules takes time proportional kn large allows increase number data items making possible learn accurate concept descriptions learn small disjuncts previously practically possible learn 

experimental results 
section describes results running sequential parallel versions rl synthetic real world data sets examples 
synthetic data better control experimental parameters real world data illustrate scaling enabled massive parallelism lead useful novel discoveries 
results sequential rl relatively fast language version program run dedicated decstation main memory 
parallel rl run cm connection machine processors 

experiments synthetic data 
designed learning task concept description disjuncts various sizes 
total examples concept learned included positive examples 
table indicates example concept consisted features significant features random values 
positive examples concept example significant feature rest significant features examples significant feature second rest significant features examples significant features sixth rest significant features remainder examples concept significant features seventh significant feature 
examples complement simply significant feature 
examples concept complement random values remaining features 
table 
design synthetic concept 
positive examples example random digits examples random digits examples random digits examples random digits examples random digits examples random digits remainder examples random digits negative examples examples random digits rl allows user specify thresholds acceptability 
typically user specifies acceptable rule cover substantial portion positive examples allowing cover small number negative examples 
experiment specified rule cover positive example cover negative examples 
furthermore eliminate extraneous search comparison run times specified rl learn rules single conjunct 
characterize concept rl forced learn rule part single conjunct specifying significant features 
disjunction rules covered concept 
test designed see sequential parallel rl fact learn rules various sizes large set data compare run times 
shows time required sequential rl parallel rl learn rules characterize concept 
sequential version run data sets examples point infeasible run larger data sets 
project taken hours run sequential rl data 
parallel rl took minute learn rules data sets examples 
close examination graph reveals times parallel learning form step function 
shows parallel learning times detail 
sequential rl parallel rl seconds items 
time required learn synthetic data 
seconds items 
time required learn synthetic data detail parallel rl 
sequential times graph swamp smaller parallel times step function apparent 
cm run actual processors 
data sets examples virtual processors allocated 
processor emulated virtual processors stored multiple data items 
time new virtual processors accomodate larger data set computation time reflected increased cost emulation 
virtual processors allocated powers emulate entire hypercube 
bottom steps visible graph shown correspond example sets size 
extra example pushes run time step 
observations relevant 
basic parallel operation evaluating predicate independent data items distributed processors involve interprocessor communication 
second processor cm relatively weak bit serial processor speedup factor nearly thousands processors produces dramatic effect 
subsequent architectures combine large numbers powerful processors cm cray give impressive speedups 
parallelizing rl allowed learn rules practically impossible sequential program learn current workstations 
test workstation maximum practical sample size sequential rl examples chance nearly zero sample contain adequate representation parts concept small disjuncts 
remember special cases particularly interest scientists 
contrast parallel rl learned rules necessary cover positive examples training items 

experiments public health database 
analyzed data set comprising department health birth records linked records infant deaths 
parallel rl learn rules predict infant mortality survival 
database contained records fields including race place birth 
example problem goal form classifier predicting new infants going survive identify interesting subgroups population 
identifying subgroups unusually high unusually low infant mortality rates directs research 
long term goal formulate policies reduce nation infant mortality rate rate particular subgroups 
shows learning times required learn data set similar obtained synthetic data 
notice particular sequential program practically useless approximately number training examples synthetic data 
massively parallel system learned rule known experts field african americans high rate infant mortality vs general population 
learned small rule japanese americans sequential rl parallel rl seconds items 
time required learn health department data 
low rate infant mortality smaller rule living east coast states low rate infant mortality 
important remember dataset contained approximately records small differences see significant 
addition analysis infant mortality database parallel rl uncovered surprising relationship led considerable follow research public health collaborators 
public health researchers concerned disparity infant mortality rates african americans general population 
general population earlier care correlates reduction infant mortality rates rl discovered african americans earlier care correlated higher infant mortality rates 
statistical tests show relationship significant controlling confounding variables 
analysis explained relationship partially 
results currently written publication public health journal sharma provost aronis buchanan 


massively parallel matching succeeds attacks specific bottleneck encountered large problems matching hypotheses huge data sets computationally expensive 
opposed previous cook holder attempt parallelize entire algorithm 
generation hypotheses takes place supercomputer front workstation 
sense results matching hypotheses data guide generation subsequent hypotheses serial nature portion algorithm 
hypothesis independently checked data item done parallel 
sequential bottleneck avoided entirely cm 
loading data cm processors sequential task take minutes data points 
coupling overhead performance numbers depicted figures conclude massively parallel version preferable number examples greater 
domains approximately attributes hold domains fewer attributes point memory individual processors exhausted 
parallel matching applies generate test learning programs general style rule learners akin rl parallel matching undoubtedly enable dramatic scaling systems segal etzioni efficiency primary concern 
parallelizing generate machine learning programs slightly straightforward believe basic approach succeed 
example consider version id exploits parallel matching 
roughly speaking decision tree partial paths generated front matched data parallel 
procedure exploit recursive partitioning nature sequential id matches partial paths increasingly smaller subsets data 
speedups large rl processors extraneous matching decision tree partial path 
expect speedups dramatic large example sets 
learning methods concept description generation testing closely coupled parallel matching awkward impossible 
extreme example difficult method parallelize backpropagation learning neural network 
summary learning tasks exploratory analysis infant mortality data learning small rules important 
order learn small rules necessary large samples algorithm see cases form rule confidence 
learning sample sets containing examples infeasible standard sequential machines 
shown massive parallelism effective way scale inductive learning large data analysis problems 

public health collaborators particular ravi sharma don excitement machine learning scientific data analysis giving reason turning scaling learning programs reality 
bruce buchanan eternal confidence machine learning difference real world discovery problems 
doug fisher anonymous referees provided helpful content editorial comments jason catlett philip chan chris matheus andreas mueller gregory piatetsky shapiro provided comments reasons methods scaling large data sets 
gratefully acknowledge support pittsburgh supercomputing center center computational biology nynex science technology notes 
stolfo shaw designed dado parallel tree structured machine production system matching deal large production systems stolfo shaw stolfo 
dado principle production systems matching rule working memory independent 
principle approach parallel learning dado representation simpler dado capabilities wasted 

feature east coast appear original data set 
created constructive induction program described aronis provost 
aronis provost 

efficiently constructing relational features background knowledge inductive machine learning 
working notes aaai workshop knowledge discovery databases pp 

seattle wa aaai 
bobrow 

editorial 
artificial intelligence 
buchanan mitchell 

model directed learning production rules 
waterman hayes roth eds pattern directed inference systems 
new york ny academic press 
buntine 

theory learning classification rules 
doctoral dissertation 
school computer science university technology sydney australia 
catlett 

machine learning large databases 
doctoral dissertation 
basser department computer science university sydney australia 
catlett 

test flight 
proceedings eighth international workshop machine learning pp 

san mateo ca morgan kaufmann 
catlett 

changing continuous attributes ordered discrete attributes 
proceedings european working session learning pp 

new york ny springerverlag 
catlett 

choosing attributes efficiently 
proceedings ninth international conference machine learning pp 

san mateo ca morgan kaufmann 
chan stolfo 

meta learning multistrategy parallel learning 
proceedings second international workshop multistrategy learning pp 

fairfax va center ai george university 
chan stolfo 

parallel distributed learning meta learning 
working notes aaai workshop knowledge discovery databases pp 

seattle wa aaai 
clearwater cheng hirsch buchanan 

incremental batch learning 
proceedings sixth international workshop machine learning pp 

san mateo ca morgan kaufmann 
clearwater provost 

rl tool knowledge induction 
proceedings second international ieee conference tools artificial intelligence pp 

los alamitos ca ieee computer society press 
cook holder 

accelerated learning connection machine 
proceedings second ieee symposium parallel distributed processing pp 

los alamitos ca ieee computer society 
cook lyons 

massively parallel ida search 
international journal artificial intelligence tools 
provost 

small disjuncts action learning diagnose errors telephone network local loop 
proceedings tenth international conference machine learning pp 

san mateo ca morgan kaufmann 
gaines 

knowledge worth ton data quantitative studies trade expertise data statistically founded empirical induction 
proceedings sixth international workshop machine learning pp 

san mateo ca morgan kaufmann 
gordon desjardins 
eds 

special issue bias evaluation selection 
machine learning 
holte 

simple classification rules perform commonly datasets 
machine learning 
holte acker porter 

concept learning problem small disjuncts 
proceedings eleventh international joint conference artificial intelligence pp 

san mateo ca morgan kaufmann 
kumar rao 

parallel depth search part ii analysis 
international journal parallel programming 
lathrop webster smith winston 

ariel massively parallel symbolic learning assistant protein structure function 
winston eds ai mit expanding frontiers 
cambridge ma mit press 
lathrop 

massachusetts institute technology 
personal communication 
daniels 

simd approach parallel heuristic search 
artificial intelligence 
michalski hong lavrac 

multi purpose incremental learning system aq testing application medical domains 
proceedings fifth national conference artificial intelligence pp 

menlo park ca aaai press 
ferguson korf 

depth heuristic search simd machine 
artificial intelligence 
provost buchanan 

inductive policy pragmatics bias selection 
machine learning press 
provost buchanan clearwater lee 

machine learning service exploratory science engineering case study rl induction program 
technical report isl intelligent systems laboratory computer science department university pittsburgh pittsburgh pa provost hennessy 

distributed machine learning scaling coarsegrained parallelism 
proceedings second international conference intelligent systems molecular biology pp 

menlo park ca aaai press 
quinlan 

induction decision trees 
machine learning 
quinlan 

generating production rules decision trees 
proceedings tenth international joint conference artificial intelligence pp 

san mateo ca morgan kaufmann 
rao kumar 

parallel depth search part implementation 
international journal parallel programming 
rumelhart hinton williams 

learning internal representations error propagation 
rumelhart mcclelland eds parallel distributed processing 
cambridge ma mit press 
schlimmer fisher 

case study incremental concept induction 
proceedings fifth national conference artificial intelligence pp 

san mateo ca morgan kaufmann 
segal etzioni 

learning decision lists homogeneous rules 
proceedings twelfth national conference artificial intelligence pp 

menlo park ca aaai press 
sharma provost aronis buchanan 

unexpected relationship timing entry care race infant mortality 
preparation 
university pittsburgh pittsburgh pa shaw sikora 

distributed problem solving approach inductive learning 
technical report cmu ri tr robotics institute carnegie mellon university pittsburgh pa stanfill waltz 

memory reasoning paradigm 
proceedings workshop case reasoning pp 

san mateo ca morgan kaufmann 
stolfo 

initial performance dado prototype 
computer 
stolfo shaw 

dado tree structured machine architecture production systems 
proceedings national conference artificial intelligence pp 

menlo park ca aaai press 
utgoff 

incremental induction decision trees 
machine learning 
weiss 

learning small disjuncts 
technical report ml tr department computer science rutgers university new brunswick nj 
zhang mckenna waltz 

efficient implementation backpropagation algorithm connection machine cm 
technical report rl boston ma thinking machines 
