joins generalize text classification whirl william cohen labs research park avenue florham park nj research att com haym hirsh department computer science rutgers university new brunswick nj hirsh cs rutgers edu whirl extension relational databases perform soft joins similarity textual identifiers soft joins extend traditional operation joining tables equivalence atomic values 
evaluates whirl number inductive classification tasks data world wide web 
show whirl designed general similaritybased reasoning tasks competitive mature inductive classification systems classification tasks 
particular whirl generally achieves lower generalization error ripper nearest neighbor methods 
whirl fast times faster benchmark problems 
show whirl efficiently select large pool unlabeled items classified correctly high confidence 
consider problem exploratory analysis data obtained internet 
assuming narrowed set available information sources manageable size developed sort automatic procedure extracting data sites interest remains difficult problem 
operations support resulting data including standard relation database management system dbms operations information form data text retrieval information form text text categorization 
data collected sources support non standard dbms operation data integration monge elkan hernandez stolfo 
example data integration suppose relations place univ state contains university names states located job univ dept contains university departments hiring 
suppose interested job openings located particular state 
normally user join relations answer query 
relations extracted different unrelated web sites 
university may referred copyright fl american association artificial intelligence www aaai org 
rights reserved 
rutgers university relation rutgers state university new jersey 
solve problem traditional databases require form key normalization data cleaning relations joined 
alternative approach taken database management system dbms whirl cohen 
whirl conventional dbms extended statistical similarity metrics developed information retrieval community compare reason similarity pieces text 
metrics implement similarity joins extension regular joins tuples constructed similarity values equality 
constructed tuples user ordered list tuples containing similar pairs fields coming 
example relations described integrated whirl query select place univ place state job univ job dept place job place univ place univ indicates items similar 
result query table columns state dept rows sorted similarity 
rows rows standard equijoin relations appear table followed rows similar identical pair rutgers university rutgers state university new jersey 
whirl designed query heterogeneous sources information whirl straight forward manner traditional text retrieval relational data operations 
obviously whirl inductive classification text 
see note simple form rote learning implemented conventional dbms 
assume training data form relation train inst label associating instances inst labels label fixed set 
example instances news headlines labels subject categories set sports 
classify unlabeled object store element relation test inst sql query select test inst train label train test test inst train inst conventional dbms retrieves correct label explicitly stored training set train 
replaces equality condition test inst train inst similarity condition test inst resulting whirl query find training instances similar associate labels precisely answer corresponding similarity join table inst label score score 
tuple table associates label table instances 
training data label similar describe tuple table score depends number similarities method viewed sort nearest neighbor classification algorithm 
evaluated whirl data integration tasks cohen 
show whirl suited inductive text classification task general data manipulation system whirl support 
overview whirl assume reader familiar relational databases sql query language 
whirl extends sql new text data type similarity operator xy applies items type text 
associated similarity operator similarity function sim described shortly range larger values representing greater similarity 
focuses queries form select test inst train label test train test inst tailor explanation class queries 
query form user specified parameter whirl find set tuples hx cartesian product test train sim largest label train 
example test contains single element resulting set tuples corresponds directly nearest neighbors plus associated labels 
tuples numeric score case score hx simply sim 
step whirl select test inst train label select third columns table hx tuples 
projection step tuples equal different combined 
published descriptions prolog notation language cohen 
refer reader earlier papers descriptions full scope whirl efficient methods evaluating general whirl queries 
score new tuple hx li gamma gamma 
scores tuples hx li 
hx yn li contain similarity assessed vector space model text salton 
assume vocabulary atomic terms appear document 
text object represented vector real numbers jt component corresponds term 
denote component corresponds general idea vector space representation magnitude component related importance document represented higher weights assigned terms frequent document infrequent collection correspond proper names particularly informative terms 
tf idf weighting scheme salton log tf delta log idf 
tf number times occurs document represented idf total number records total number records contain term field 
similarity document vectors formula sim delta delta jjwjj measure large vectors share important terms small 
notice due normalizing term sim zero 
combining graph search methods indexing pruning methods information retrieval whirl implemented fairly efficiently 
details implementation full description semantics whirl cohen 
experimental results evaluate whirl tool web text classification tasks created benchmark problems listed table pages world wide web 
vary domain size generated straight forward fashion relevant pages associate text typically short name strings labels relatively small set 
table gives summary description domain number classes number terms problem number training testing examples text classification currently contains word stems morphologically inspired prefixes produced porter stemming algorithm texts appearing database 
area focused longer documents focus short text items typically appear data integration tasks 
experiments 
smaller problems fold crossvalidation cv larger problems single holdout set specified size 
case domains duplicate instances different labels removed 
whirl inductive classification label new test example example placed table similarity join performed training set table 
results table test example item tuple label occurred top similar text items training set table appears second item tuple 
exploratory analysis small number datasets plus experience yang chute related classifier see 
highest scoring label test item classification 
table empty frequent class 
compared whirl baseline learning algorithms 
quinlan feature vector decision tree learner 
binary valued features set terms whirl vectors tractability reasons terms appeared times training set features 
ripper cohen rule learning system text categorization cohen singer 
ripper set valued features cohen represent instances functionally equivalent terms binary features efficient 
feature selection performed ripper 
nn finds nearest item training set table vector space similarity measure sim gives test item item label 
yang chute distance weighted nn method called nns closely related whirl combines scores nearest neighbors differently selecting label maximizes yang chute method picks label frequent nearest neighbors 
accuracy whirl baseline method accuracy default rule labeling item frequent class training data shown table highest accuracy problem shown boldface 
information shown graphically point represents test problems 
respect accuracy whirl uniformly outperforms outperforms nn times outperforms ripper times 
whirl significantly worse learner individual problem 
value point indicates accuracy whirl problem value indicates accuracy competing method points lie line cases whirl better 
upper graph compares whirl ripper lower graph nearestneighbor methods 
mcnemar test test significant differences problems single holdout paired test folds cross validation cv 
accuracy whirl accuracy second learner relative accuracy ripper whirl whirl accuracy second learner relative accuracy nn whirl knn sum whirl knn maj whirl scatterplots learner accuracies algorithm comes closest whirl nn whirl outperforms nns times differences small 
statistically significant indicating whirl represents small statistically real improvement nn whirl selective classification error rates quite high tasks surprising nature data 
example consider problem labeling names industry operate 
fairly easy guess business watson pharmaceuticals correctly labeling requires domain knowledge 
course applications necessary label test instance find subset test data reliably labeled 
example constructing mailing list necessary determine household buy advertised product needs find relatively small set households customers 
basic problem stated follows test cases give labels set user 
call problem selective classification 
way perform selective classification learner hypothesis prediction test item pick items predicted highest confidence 
associating confidence learner prediction usu problem train test classes terms text valued field label memos cv document title category cv cdrom game name category cv common name bird order cv common scientific name bird order name industry coarse grain name industry fine grain books book title subject heading species animal name phylum url title category table description benchmark problems default ripper nn nns whirl memos cdrom books species average table experimental results accuracy ally straightforward 
whirl suggests approach selective classification 
query select test inst train label train test test inst test contain unlabeled test cases single test case 
result query table tuples hx test case label associated training instances similar implementing approach score tuple computed whirl measure confidence 
avoid returning example multiple times different labels discarded pairs hx li hx appears table higher score guarantees example labeled 
uniformly selecting nearest neighbors test example approach henceforth join method finds pairs training test examples similar projects final table intermediate result 
join method identical repeating nn style classification test query value 
advantage join method query executed test instances join method simpler faster repeated nn queries 
table compares efficiency baseline learners efficiency join method 
case report ripper confidence measure obtained proportion class training examples reached terminal node rule classifying test case 
nearest neighbor approaches confidence measure obtained score associated method combination rule 
combined time learn classify test cases 
lefthand side table gives absolute runtime method right hand side gives runtime baseline learners multiple runtime join method 
join method faster symbolic methods average speedups nearly best case average speedups nearly ripper best case 
significantly faster straight forward whirl approach nn task 
speedup obtained size intermediate table fairly large shared test instances 
experiments preliminary experiments appears give measurably worse performance 
potential disadvantage join method test cases near training cases classification larger problems example test examples receive labels 
selective classification problem 
considers top scoring classifications small apparent loss accuracy join method whirl appears slight gain 
upper graph plots error selected predictions known precision rank join method compares baseline learners perform best metric 
points averages datasets 
average join method tends outperform whirl point whirl performance begins dominate 
generally join method competitive ripper 
averages runtimes nn variants comparable whirl 
ripper performs better join method average time sec join ripper nn whirl join ripper nn whirl books species average table run time selected learning methods precision top rank average datasets whirl join whirl ripper precision whirl joins precision learner precision ripper join join whirl join experimental results selective classification conceal lot individual variation join method best average individual cases performance best 
shown lower scatter plot 
summary whirl extends relational databases reason similarity text valued fields information retrieval technology 
evaluates whirl inductive classification tasks applied natural way fact single unlabeled example classified simple whirl query closely related whirl query selectively classify pool values difference small instance join method averages errors ripper 
point compares join method baseline method point methods close average performance 
examples 
experiments show despite greater generality whirl extremely competitive state ofthe art methods inductive classification 
whirl fast special indexing methods find neighbors instance time spent building model 
whirl shown generally superior generalization performance existing methods 
cohen 

fast effective rule induction 
machine learning proceedings twelfth international conference 
morgan kaufmann 
cohen 

learning set valued features 
proceedings thirteenth national conference artificial intelligence 
aaai press 
cohen 

integration heterogeneous databases common domains queries textual similarity 
proceedings acm sigmod international conference management data 
acm press 
cohen 

web information system reasons structured collections text 
proceedings second international acm conference autonomous agents 
acm press 
cohen singer 

context sensitive learning methods text categorization 
proceedings th annual international acm conference research development information retrieval pages 
acm press 
hernandez stolfo 
merge purge problem large databases 
proceedings may 
monge elkan 
efficient domain independent algorithm detecting approximately duplicate database records 
proceedings sigmod workshop data mining knowledge discovery may 
porter 

algorithm suffix stripping 
program 
quinlan 

programs machine learning 
morgan kaufmann 
salton editor 

automatic text processing 
addison wesley 
yang chute 

example mapping method text classification retrieval 
acm transactions information systems 
