appear proceedings iea feature selection classification probabilistic wrapper approach huan liu rudy setiono department information systems computer science national university singapore kent ridge singapore email nus sg feature selection defined problem find minimum set features inductive algorithm achieve highest predictive accuracy data described original features probabilistic wrapper model proposed method exhaustive search heuristic approach 
aim model avoid local minima exhaustive search 
highest predictive accuracy criterion search smallest analysis experiments show model effectively find relevant features remove irrelevant ones context improving predictive accuracy induction algorithm 
simple straightforward providing fast solutions searching optimal 
applications model related issues discussed 

problem feature selection defined finding relevant features original attributes define data order minimize error probability reasonable selection criteria 
feature selection long focus researchers fields pattern recognition statistics machine learning see section 
methods proposed 
general classified categories filter approach ad kr feature selector independent learning algorithm serves filter sieve irrelevant redundant features wrapper approach feature selector wrapper learning algorithm relying relevant features determined 
major advantage wrapper model utilizes induction algorithm just examples 
approaches existed quite time 
see section 
self criterion selecting features context learning classification rules purpose feature selection improve performance induction algorithm 
incorporating induction algorithm process feature selection cost 
discussion 
category methods divided types exhaustive heuristic search 
difficulty feature selection stated follows special cases optimal selection done testing possible sets features chosen features applying criterion gamma delta gammam 
times 
relevant features total number times gamma delta 
prohibitive large 
practice heuristic methods way exponential computation 
heuristic methods general low order second information estimate relevance features approximately 
heuristic methods reasonably qui lw certain features high order relations example parity problem 
hand problem exponential explosion hand relevant features omitted heuristic approach taken 
goal algorithm explore high order relations features find relevant features high probability resorting exhaustive search 
feature selection problem redefined terms predictive accuracy inductive algorithm find smallest set features inductive algorithm achieve highest accuracy 

related problem feature selection long active research topic statistics pat order information contains feature second order information features feature selection classification tern recognition dk area dealt linear regression lan assumptions apply learning algorithms 
pointed common assumption monotonicity increasing number features improve performance learning algorithm past years feature selection received considerable attention machine learning knowledge discovery researchers interested improving performance algorithms cleaning data 
heuristic feature selection algorithms 
relief algorithm kr assigns relevance weight feature meant denote relevance feature target concept 
relief samples instances randomly training set updates relevance values difference selected instance nearest instances opposite classes 
kr relief assumes classes classification problems help redundant features 
features relevant concept select fraction necessary concept description 
preset algorithm mod heuristic feature selector uses theory rough sets heuristically rank features assuming binary domain 
order consider higher order information feature 
suggested lw high order information gain feature selection 
algorithms try explore combinations features fail problems parity majority functions combinations small number features help locating relevant features 
chi ls heuristic feature selector automatically discretizes continuous features removes irrelevant continuous features chi square statistics inconsistency data 
handle nominal features 
forward selection backward elimination wrapper models studied 
better guideline offered problems method 
observed results general achieves lower error rates achieves smaller numbers features 
classic exhaustive method ad monotonicity assumption valid induction algorithms machine learning 
see dataset section reproduced 
called focus authors proposed heuristic versions speed process assuming noise free binary domain 
common understanding learning algorithms built feature selection example id qui fringe ph qui 
results ad suggest rely id fringe filter irrelevant features 
conducts test individual feature proper find minimum set features 
expected shown section fail parity problems 
summary exhaustive search approach infeasible practice heuristic search approach reduce computational time significantly explore combined effects features fail hard problems parity problem remove redundant features 
right time third approach relies heuristics exhaustive search producing solutions high probability selects optimal near optimal set relevant features 

probabilistic wrapper model probabilistic approach modified version las vegas algorithms bb 
las vegas algorithms probabilistic choices help guide quickly correct solution 
kind las vegas algorithms uses randomness guide search way correct solution guaranteed unfortunate choices 
mentioned earlier heuristic search methods vulnerable datasets high order relations 
las vegas algorithms free worrying situations evening time required different situations 
time performance las vegas algorithm may better heuristic algorithms 
high probability data took long time deterministically solved faster data heuristic algorithm particularly slowed average las vegas algorithm 
algorithm generates random subsets features subset learning algorithm applied training data order obtain estimated error rate err smallest subset lowest error rate kept 
las vegas algorithm guaranteed sufficiently long time finds optimal solution 
search mini feature selection classification mum set features lvw outputs current best 
shown algorithm computation err train test computing err 
learning algorithm applied training testing data produces estimated error rate testing data 
rate reported experiments 
specified number maximum runs number runs smallest number features err current smallest error rate 
function produces set features run number features 
induction algorithm id chosen experiments 
lvw algorithm err repeat err train null err err err err output current best err err err updated times err train test analysis shows lvw give solution optimal solution sufficiently large 
random number generator selection optimal subset features considered non replacement experiments 
probability finding optimal subset gammak th experiment probability finding optimal subset experiments gamma theta gamma gamma theta theta gammak number original features 
experiments shown lvw algorithm approximated thetan larger trials lvw try 
large approximation analysis assumes optimum 
exist optima kth tossing probability finding optimum gammak 
empirical study induction algorithm lvw algorithm kind important induction algorithm fast time complexity lvw bound number runs time complexity induction algorithm 
choices chose id experiments 
program program comes quinlan book qui id results obtained running unpruned trees 
types datasets chosen experiments 
type artificial data relevant features known feature selection conducted includes corral monks tbb parity 
type real world data including credit vote labor qui ma 
choice datasets simplify comparison published 
datasets comparisons different methods described 
artificial datasets crossvalidation done 
real world datasets fold cross validation obtain estimated accuracy training data 
choice options flag indicates splitting continue purity artificial datasets default setting real world datasets 
artificial data 
corral data designed 
binary features feature irrelevant feature correlated class label time 
boolean target concept 
id chose feature root 
example datasets feature removed accurate tree result 

monk monk monk datasets taken tbb 
features 
training datasets provided feature selection 
monk monk need features describe target concepts monk requires 
training data monk contains noise 
datasets show feature selector selects relevant features relevant ones plus 

parity target concept parity bits 
dataset contains features uniformly random irrelevant 
training set contains instances randomly selected instances 
independent instances drawn form feature selection classification testing set 
heuristic feature selectors fail sort problems individual feature mean 
real world data 
vote dataset includes votes house representatives congress persons key votes identified congressional quarterly volume xl 
data set consists features training instances test instances 

credit crx dataset contains instances credit card applications 
features boolean label 
dataset divided quinlan qui training instances test instances 

labor dataset contains instances acceptable unacceptable contracts 
small dataset features training set instances testing set instances 
results shown tables 
column err means instances misclassified percentage artificial datasets lvw find relevant features 
example lvw rediscovered features relevant monk features monk features monk features parity features corral 
results give confidence lvw accuracy criterion relevant features selected presence noise monk 
real world datasets knowledge features relevant 
comparison lvw performed dimensions tree size error rate err number features att 
results id lvw summarized terms dimensions 
id figures improved lvw applied monk number features reduced tree size smaller error rate decreasing 
improvement clear cut 
datasets monk number features reduced significantly decreased error rates corral monk parity come increase tree size 
reason see discussion 
experimental results reproduced table purpose 
see details 
bf means feature selection forward fw means forward stepwise selection backward bw means backward stepwise selection relieve rl modified version relief kr significant variance relevance rankings relief 

wrapper model feature selection closely linked induction algorithm words lvw constrained limitations induction algorithm 
induction algorithm handle noisy data missing values continuous discrete values lvw 
special effort needed tailor datasets order run lvw 
achieve lowest possible error rate aim feature selector induction algorithm 
addition lvw produces intermediate solutions working better ones 
general belief fewer features simpler decision trees irrelevant features removed 
murphy pazzani mp find smallest trees typically lower predictive accuracy slightly larger trees exhaustive search simplest consistent theories necessarily lead improvement 
means accuracy criterion may lead simplest tree 
truly reflected results datasets 
size decision tree number leaves tree plus 
size measure show features contained tree 
experimental results show pursuit high accuracy lvw reduce number features improve accuracy may reduce tree size 
measure combines dimensions may help achieving high performance dimensions 
experience lvw slow running trials different patterns 
random pattern tested induction algorithm time cost ind minimum cost lvw ind 
cross validations cost increased factor related number folds cross validations 
due slowness lvw recommended applications time critical factor 
just period time slowness harm 
researchers trying speed wrapper approach lan 
lvw play role bench mark comparisons heuristic methods 
heuristic fs algorithms deterministic 
heuristic algorithms designed particular applications run fast 
lvw check feature selection classification table results tree size error rate err number features att 
id lvw 
learner id measure size err att 
lvw corral monk monk monk vote credit labor table results tree size error rate err number features att 
lvw 
learner measure size err att 
lvw corral monk monk monk vote credit labor fast solution designing heuristic algorithm 
noticed slowness caused learning algorithm 
significantly limits application probabilistic model 
current interests find criterion rely accuracy learning algorithm 
checking satisfaction new criterion faster probabilistic model applied applications 
line current research get rid wrapper approach go filter reasons addition speed issue 
trying find criterion speed probabilistic method improved significantly sacrificing results 
anonymous referees comments earlier version chew transforming graphs table 
ad almuallim dietterich 
learning boolean concepts presence irrelevant features 
artificial intelligence november 
bb brassard 
fundamentals algorithms 
prentice hall new jersey 
dk devijver kittler 
pattern recognition statistical approach 
prentice hall international 
john kohavi pfleger 
irrelevant feature subset selection problem 
machine learning proceedings eleventh international conference pages 
morgan kaufmann publisher 
kr kira rendell 
feature selection problem traditional methods new algorithm 
aaai proceedings ninth national conference artificial intelligence pages 
aaai press mit press 
lan langley 
selection relevant features machine learning 
proceedings aaai fall symposium feature selection classification table experimental results reported john bf fw forward bw backward rl relieve 
means available original 
id algorithm attributes dataset bf fw bw bf fw bw bf fw bw corral monk parity vote credit labor algorithm attributes dataset bf fw bw rl bf fw bw rl bf fw bw rl corral monk parity vote credit labor relevance 
aaai press 
ls liu setiono 
chi feature selection discretization numeric attributes 
proceedings th ieee international conference tools artificial intelligence 
lw liu wen 
concept learning feature selection 
proceedings australian new zealand conference intelligent information systems 
ma murphy aha 
uci repository machine learning databases 
ftp ics uci edu directory pub machine 
mod 
feature selection rough sets theory 
brazdil editor proceedings european conference machine learning pages 
mp murphy pazzani 
exploring decision forest empirical investigation occam razor decision tree induction 
journal art 
intel 
res march 
ph pagallo haussler 
boolean feature discovery empirical learning 
machine learning 
press teukolsky vetterling flannery 
numerical recipes cambridge university press cambridge 
qui quinlan 
induction decision trees 
machine learning 
qui quinlan 
programs machine learning 
morgan kaufmann 
tbb thrun monk problems performance comparison different learning algorithms 
technical report cmu cs carnegie mellon university december 
dubes jain 
critical evaluation intrinsic dimensionality algorithms 
gelsema kanal editors pattern recognition practice pages 
morgan kaufmann publishers 
