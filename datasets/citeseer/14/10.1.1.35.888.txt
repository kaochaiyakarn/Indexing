multi label text classi cation mixture model trained em andrew mccallum just research pittsburgh pa mccallum com important document classi cation tasks documents may associated multiple class labels 
describes bayesian classi cation approach multiple classes comprise document represented mixture model 
labeled training data indicates classes responsible generating document indicate class responsible generating word 
em ll missing value learning distribution mixture weights word distribution class mixture component 
describe bene ts model preliminary results reuters data set 
text classi cation problem assigning text document topic categories classes 
multiclass document classi cation distinguished binary document classi cation classes 
multi label classi cation document may class label 
example classes america america europe asia australia news article troops bosnia may labeled america europe classes 
describes bayesian approach multiclass multi label document classi cation 
de ne probabilistic generative model represents multi label nature document indicating words document produced mixture word distributions topic 
generative process begins selecting set classes single class labels document producing set mixture weights classes nally word document generated rst selecting class mixture weights letting class generate single word 
classi cation uses bayes rule selects class set document 
parameters model learned maximum posteriori estimation la training data 
labels indicate classes involved generating document indicate class generated individual words 
expectation maximization em ll missing value avoiding tting performing step leave document fashion 
em sets mixture weights word distributions class 
add extra class documents belong english class 
naturally captures background language common documents essence automatically extracting task speci list 
common alternatives approach 
rst build binary classi er class yang joachims nigam 
second build mapping class document pairs real valued scores scores provide ranking classes top assigned document choosing threshold schapire singer 
key features approach fold conjecture mixture model represent expressive decision boundaries binary classi er 
words binary classi er large multi faceted negative class represented single word distribution model 
approach models classes mixture individual models 
note binary classi ers tacitly assume labels assigned independently 
label provides information binary classi er fails capture 
generative model represent correlations class labels 
thresholds tuned learned 
threshold learning typically quite problematic document length attributes ect score making dicult relate scores di erent documents 
advantages formal probabilistic approach de ned generative model making available enhancements large tool chest powerful statistical parameter estimation techniques shrinkage unlabeled data mccallum nigam 
presents preliminary experimental results subset reuters data set 
nd mixture model outperforms approach collection binary classi ers reducing classi cation error labels reducing error label reducing error third 
experiments needed fully evaluate technique 
multi label classi cation mixture model generative model 
generative model contains set classes fc document associated subset classes thought binary bit vector ones indicating classes subset 
power set possible class subsets written associated individual class word distribution jc words vocabulary fw set classes document generated mixture word distributions mixture weights components associated classes forced zero 
mixture weights selected distribution mixture weights written 
labeled training data consists set documents fd vector class labels associated document written generative process document follows 
select set classes distribution 
select mixture weights classes 
start generating words document 
word select class mixture weights components written thought cj class generate single word multinomial word probability distribution jc 
probability document wjc 
generative model de ned multi label bayesian classi cation falls naturally 
test document wish select set classes probable arg max cjd 
express cjd terms generative model parameters applying bayes rule cjd dj 
generative process document de ned terms requires mixture weights 
introduce mixture weights sum total probability drop just normalization factor cjd sum cjd dj integral nite number possible real valued vectors 
finding closed form solution integral subject 
approximate integral single priori probable 
see 
single mixture weights set classes arg max approximate equation cjd dj 
assuming word independence expanding mixture model gives cjd wjc 
classi cation performed calculating posterior probability class set selecting set probable 
size class set allowed large sets evaluate exhaustively ecient accurate search necessary 
methods ecient exploration class set space restricting size nding arg max cjd restrict size classes class selected rst step 
continue process greedily adding classes added select combination 
find mixture weights produced document described step evaluate cjd sets resulting adding classes time order highest mixture weight 
note analogous automatically determining threshold second method described 
methods additional accuracy may obtained performing limited amount exploration greedy paths 
explore simulated single priori probable mixture weights class set solution early experimental evidence indicates quite di erent mixture weights di erent documents labeled class set indicating uni modal approximation improved 
interesting alternative closed form solution non parametric memory representation summing possible mixture weights sum mixture weights associated training documents labeled class set better closed form solution parametric form dirichlet poor match data 
annealing similar stochastic optimization techniques 
methods di erent biases 
uses concurrently 
parameter estimation 
determine parameters maximum posteriori estimation collection labeled training data 
class set prior distribution currently represented completely form set maximum posteriori estimation uniform prior jdj number labeled training documents labeled exactly vector smoothing parameter determining strength uniform prior 
setting results laplace smoothing 
parameters estimated directly training data aspects generative process provided labels 
speci cally labels training document indicate classes involved generating document classes responsible generating individual word 
turn expectation maximization em dempster order ll missing value 
step estimate word training document class responsible generating 
step estimates straightforwardly determine maximum posteriori maximum likelihood estimates mixture weight class class word probability distributions wjc step step cjw wjc wjc cjw jcj jw jc cjw cjw count occurs training documents particular set classes backo average mixture weights class sets training data average weighted class set overlap 
additional features 
guard tting step performed leave fashion 
estimating classes generated words particular training document uence document temporarily removed parameter estimates perform step 
parameters probabilistically weighted counts removal quite easy 
leave step deleted interpolation jelinek mercer shrinkage document classi cation mccallum addition classes add extra class documents belong 
thought english class 
due leave step class gathers words common classes essence automatically nding task speci list 
furthermore uniform word distribution added mixture component 
conjunction leave step serves automatically determine optimal amount smoothing uniform distribution sense maximizes probability training data leave fashion 
example word occurs training data leave probability zero mixtures uniform distribution mixture weight increased properly set maximize probability left document 
grain wheat corn ship trade crude root trade oil grain wheat corn gulf mln ships crude dlrs mln mln mln strike japan march agriculture export port de cit prices pct year agriculture pct shipping year year er japanese barrels crop department acres march surplus mln april farmers soviet year union exports production blah farm march production iran countries pct table probable words multinomial word distributions topic classes reuters data set 
notation refers string digits 
note model correctly placed word blah root word distribution small percentage documents classes sole word document body 
experimental results reuters distribution evaluate performance multi label mixture model 
studies populous classes joachims nigam 
text tokenized removing sgml tags gathering strings alphabetic characters removing words smart stoplist 
additionally strings digits mapped single common token 
multi label mixture model trained modapte training set 
em run convergence took iterations minutes wall clock time 
performance model compared traditional method consisting collection binary classi ers class studies yang joachims nigam binary classi er multinomial naive bayes laplace smoothing mccallum nigam results em estimation word probability distribution classes quite interesting 
expected words associated di erent class topics gather appropriate classes 
classes occur sole label document wheat tend gather words common wheat documents excluding words better explained topics 
word distributions table 
notice words overlap classes grain wheat corn 
suggests hierarchical directed acyclic graph structure classes capture redundancy aid generalization see discussion section 
comparison classi cation accuracy shown table 
multilabel mixture model outperforms traditional collection binary classi ers classes ship reducing error half wheat 
unfair advantage traditional binary classi er select vocabulary size best test set 
mixture model involves tuning 
credit class sets exact match label set full set multi label mixture model reduces error third 
class binary nb classi er multi label mixture accuracy vocab size accuracy acq corn crude earn grain interest money ship trade wheat full set table classi cation accuracy largest classes reuters 
multi label mixture model reduces error half wheat 
credit class sets exact match label set line multi label mixture model reduces error third 
preliminary results entire set reuters classes mixture model performs better binary naive bayes classi er classes moderate small number training documents 
classes extremely large training sets recall higher precision lower 
classes improved points yang analysis suggests factored class set prior may improve precision 
evaluate methods richer data sets expect leave em shine reuters classes corn wheat predicted single word 
compare support vector machines joachims boosting schapire singer relation methods expect model advantages computational eciency robustness sparse data shrinkage see section 
related various related mixture models modeling text 
nymble system bbn uses hmm uniform transition probabilities perform information extraction named entities bikel similar models text segmentation topic tracking yamron related imai mixture model multi label document classi cation 
class set conditional mixture distribution leave evaluation optimal smoothing mixing uniform distribution classi es test documents searching space class sets method heuristic threshold 
boostexter collection enhancements adaboost enable application multiclass multi label document classi cation problems schapire singer traditional boosting labels training instances re weighted 
boostexter aims predict correct labels ranking correct labels receive highest ranks 
evaluate method tuning threshold ranks 
mixture model described similar aspect clustering model described hofmann puzicha document generated mixture topic models 
supervised classi cation model unsupervised clustering 
combine model uses take advantage hierarchy topics mccallum directed acyclic graph topics 
model lends improving parameter estimates incorporating unlabeled data nigam explore complex functions priors functions 
model may additionally improved multiple mixture components topic word distribution learned completely unsupervised fashion 
bikel daniel bikel scott miller richard schwartz ralph weischedel 
nymble high performance learning name nder 
proceedings anlp pages 
dempster dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
hofmann puzicha hofmann puzicha 
statistical models cooccurrence data 
technical report ai memo ai lab mit 
imai toru imai richard schwartz francis kubala long nguyen 
improved topic discrimination broadcast news model multiple simultaneous topics 
proceedings ieee icassp volume pages munich germany april 
jelinek mercer fred jelinek robert mercer 
interpolated estimation markov source parameters sparse data 
gelsema kanal editors pattern recognition practice pages 
joachims thorsten joachims 
text categorization support vector machines learning relevant features 
machine learning ecml tenth european conference machine lea pages 
mccallum nigam andrew mccallum kamal nigam 
comparison event models naive bayes text classi cation 
aaai workshop learning text categorization 
www cs cmu edu mccallum 
mccallum andrew mccallum ronald rosenfeld tom mitchell andrew ng 
improving text cation shrinkage hierarchy classes 
international conference machine learning icml pages 
nigam kamal nigam andrew mccallum sebastian thrun tom mitchell 
text classi cation labeled unlabeled documents em 
machine learning 
appear 
schapire singer robert schapire yoram singer 
boostexter boosting system text categorization 
machine learning journal 
yamron yamron carp gillick lowe van 
hidden markov model approach text segmentation event tracking 
proceedings ieee icassp seattle washington may 
yang yiming yang 
evaluation statistical approaches text categorization 
journal information retrieval 
appear 
