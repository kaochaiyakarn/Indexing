nonlinear discriminant analysis kernel functions volker roth volker steinhage institut ur informatik iii bonn germany cs uni bonn de june linear discriminant analysis lda classical multivariate technique dimension reduction classi cation 
data vectors transformed low dimensional subspace class centroids spread possible 
subspace lda works simple prototype classi er 
resulting decision boundaries linear 
applications linear boundaries adequately separate classes possibility modeling complex boundaries desirable 
nonlinear generalization discriminant analysis implements method representing dot products pattern vectors kernel functions 
technique allows eciently compute discriminant functions arbitrary feature spaces kernel representations exist 
classical linear discriminant analysis lda statistical method attempts project data vectors belong di erent classes dimensional space way ratio group scatter sb group scatter sw maximized 
lda formally consists eigenvalue decomposition sb leading called canonical variates contain class speci information dimensional subspace 
canonical variates ordered decreasing eigenvalue size indicating rst variates contain major part information 
consequence procedure allows low dimensional representations visualization data 
interpreting lda technique dimensionality reduction seen multi class classi cation method set linear discriminant functions de ne partition projected space regions identi ed class membership 
resulting decision boundaries linear 
new observation assigned class centroid closest projected space 
concentrate classi cation properties discriminant analysis 
universit bonn technical report nr 
iai tr issn overcome limitation linear decision functions attempts incorporate nonlinearity classical algorithm 
hastie introduced called model flexible discriminant analysis lda reformulated framework linear regression estimation generalization method nonlinear regression techniques 
proposed regression techniques implement idea nonlinear mappings transform input data new space linear regression performed 
real world applications approach deal numerical problems due dimensional explosion resulting nonlinear mappings 
years approaches avoid explicit mappings kernel functions popular 
main idea construct algorithms ord dot products pattern vectors computed eciently high dimensional spaces 
examples type algorithms support vector machine kernel principal component analysis 
show possible formulate classical discriminant analysis exclusively terms dot products allows kernel methods construct nonlinear variant algorithm 
call technique kernel discriminant analysis 
contrary similar approach published algorithm real multi class classi er inherits classical linear discriminant analysis convenient property data visualization 
variant algorithm avoids solution high dimensional eigenvalue equations cause numerical problems 
organized follows section give brief review lda section standard algorithm reformulated dot products 
section give overview linear regression approach lda show section formulated exclusively terms dot products 
possible apply kernel methods section 
sections concern problem model selection followed experiments discussion 
review linear discriminant analysis assumption data centered scatter matrices sb sw de ned sb sw number patterns belong class determinant matrices simple scalar measure scatter lda chooses transformation matrix maximizes objective function jv sbv jv sw columns optimal generalized eigenvectors correspond nonzero eigenvalues sbv sw subject normalization constraint sw de ning total scatter matrix sw sb related common equation sw sb combining leads alternative eigenvalue equation identical solutions sbv sb sv sv normalization condition written terms sv th component projected pattern ik reformulating standard algorithm section give reformulation lda dot products input vectors 
necessary nonlinear version algorithm shown section 
adopted ideas proposed eigenvalue decomposition covariance matrix investigation 
derivation necessary invertible 
positive semi de nite matrix ensured applying ridge type regularization operator see section right side reads invertible vectors written vm vm symmetric nonsingular matrix eigenvectors accomplished positive eigenvalues equations hold se equation implies eigenvectors expansion terms input vectors coecients substituting leads combining arrive vm vm means solution vectors vectors sv lie span 
xn 
set equations equivalent original equation sbv sv np 
note equations solely expressed terms dot products 
input vectors 
de ning dot product matrix ij 
equations read np kl mj kr rj kj 
de ning matrix kj np kl mj vector 
write equation equation may converted standard non symmetric eigenvalue problem adding small multiple identity matrix invertible type regularization necessary optimal scoring variant discussion see section 
normalization condition translates note case classes single eigenvector multiplied arbitrary constant 
purpose model selection see section suitable choose case normalization 
virtue translates normalization condition 
ij th component projected pattern projection th eigenvector jk 
ij nonlinear version lda mapping data feature space kernel representations dot products give overview variant lda optimal scoring 
show nonlinear variant algorithm numerical advantages 
lda optimal scoring lda problem restated framework penalized optimal scoring 
considering problem classes data vectors class memberships represented categorical response variable levels 
useful code responses terms indicator matrix size th data vector belongs class point optimal scoring turn categorical variables quantitative ones assigning score classes score vector assigns real number th level vector represents vector scored training data 
vector regressed data matrix rows input vectors 
simultaneous estimation scores regression coecients constitutes optimal scoring problem minimize criterion asr kz constraint kz 
denotes positive de nite penalty matrix penalizes coecients input vectors 
particular penalization assure total scatter covariance matrix invertible necessary derivation 
equation written quadratic form combined vector asr score minimizing os partially minimized criterion min asr denotes regularized hat smoother matrix known logistic regression 
minimizing constraint kz performed procedure 
choose initial matrix satisfying constraint set 
run penalized multi response regression xb matrix regression coecients 

obtain optimal scores solve update matrix regression coecients bw matrix eigenvectors 
shown nal matrix diagonal scale matrix equivalent matrix lda vectors multiplying eigenvalue equation get sb bw bw sbv equation identical additional penalty matrix vector bw derived os problem solution penalized standard lda problem 
comply normalization condition necessary multiply diagonal scale matrix form ii th largest eigenvalue equation 
detailed derivation see 
optimal scoring dot products denoting eigenvectors accomplished eigenvalues penalized scatter matrix equation reads os ee setting follows vectors expansion terms input vectors 
particular implies vectors lie span 
xn exist coecients equation reads asr xx equation corresponding require matrix invertible introduce second penalty note positive semi de nite matrices positive de nite adding small multiple identity matrix 
contrary penalty see section clear interpretation uence making problem numerically 
spite algorithm worked experiments order score optimal vector os xx analogous partially minimized criterion min asr xx xx 
xx dot product matrix introduced section 
note equation solely written terms dot product matrix minimizing constraint kz performed procedure section 
choose satisfying set 
run penalized multi response regression xb 
solve update bw case classes normalize single projection vector length suitable model selection compare eq 
section bw 

matrix rows input vectors projected column vectors xb xx 
kz 
kz note dot product matrix needed calculate fact eigenvalue problem eq 
dimensions eq 
constitutes great advantage algorithm section 
matrix symmetric simple routines solving eigenvalue equations sucient 
nonlinear versions discriminant analysis section give nonlinear versions lda variants described sections 
main idea introducing nonlinearity algorithms perform linear discriminant analysis space observations feature space related nonlinear mapping assuming mapped data centered xn algorithms remain unchanged dot product matrix computed ij 

computation dot products feature spaces done eciently kernel functions choices exists mapping feature space acts dot product required dot product matrix obtained setting ij 
possible kernel functions polynomial kernels form 
hyperbolic kernels form tanh 
radial basis function rbf kernels form exp kx yk 
polynomial kernels correspond dot product space monomials input coordinates degree dimensionality feature space grows real world applications low degrees lead feature spaces computations lda vectors impossible mapping performed explicitly 
approaches explicit mapping theoretically impossible uses hyperbolic rbf kernels correspond mappings nite dimensional spaces 
stresses advantage reformulating algorithm kernel representations dot products 
shown assumption dropped simply writing mapping de ning matrix ij translates ij nk ij regularized discriminant analysis prevent lda tting data orts incorporate type regularization algorithm 
instance ridge type regularizations described 
idea approach stabilize estimated covariance scatter matrix adding small multiple identity matrix noticed nonsingular 
formal analogy type regularization ridge regression shown 
concept ective number parameters select regularization parameter interpretation concept structural risk minimization principle structure de ned expansion decision function rst eigenvalues parameter determines number expansion coecients model complexity 
model selection fundamental concept minimizing risk task supervised learning introduced 
central statement gives bound expected risk sum empirical risk risk training data con dence term describing complexity chosen model 
minimizing expected risk method structural risk minimization introduced structure consisting nested subsets functions increasing complexity choose xed value empirical risk set functions lowest complexity measured terms vc dimension 
linear decision functions produced discriminant analysis de ne hyperplanes feature space written 
formulation unique hyperplane corresponds canonical hyperplane additionally requiring min 

bj set canonical hyperplanes de ned pattern vectors 
xn weight vector bounded jvj vc dimension bounded 
case separating hyperplane resulting parameter set zero classi cation rule takes account distances class centroids 
estimate vc dimension select classes calculate min 

rescale weight vector hyperplane canonical hyperplane note normalized eq 
norm weight vector vj smallest sphere includes vectors computed solving quadratic optimization problem 
alternatively estimate upper bound calculating root largest diagonal element dot product matrix max ii propose strategy model selection proven useful experiments rst step choose small regularization parameter see eq 
assures nonsingular 
estimate vc dimension separating canonical hyperplanes different kernel parameters choose parameter smallest estimate 
structural risk minimization procedure proposed svm 
note contrary svm lda guarantee smallest possible norm weight vector xed kernel 
despite theoretical shortcoming discovered experiments algorithm produced decision boundaries vc dimension svm 
second step increase regularization parameter choose optimal value minimizing misclassi cation rates test set cross validation procedure 
experiments tested algorithm mpi chair database consists regular spaced views form upper viewing hemisphere di erent classes chairs training set random views class testing generalization ability 
available images pixels 
experiments additional edge detection patterns view 
classi cation results di erent classi ers reported literature oriented lter approach described svm fully connected perceptron hidden layer neurons 
results table column shows performance algorithm proposed 
svm mlp table test error rates mpi chair database test views training set regular spaced views 
column order support vector machine multi layer perceptron oriented filter kernel discriminant analysis 
obtained best results rbf kernels form exp kx yk 
results variants identical algorithm implementing optical scoring version discriminant analysis signi cantly faster 
parameter selected minimizing estimated vc dimension rst classes database 
estimates plotted parameter comparison trained svm data 
see algorithms reach minimum estimated vc dimension 
may explain similar performance classi ers 
experiment classes algorithm showed signi cantly faster svm due simpler optimization procedure due possibility handling multi class problems directly constructing class classi cation rules necessary standard svm database available ftp ftp mpg de pub chair dataset estimated vc dimension kernel parameter svm estimated vc dimensions plotted kernel parameter svm 
algorithm 
noticed exists improved svm algorithm solves multi class problem directly see 
discussion similar nonlinear versions classical linear discriminant analysis 
main idea map input vectors high nite dimensional feature space apply linear algorithms enlarged space 
restating lda way dot products input vectors needed possible kernel representations dot products 
overcomes numerical problems high dimensional feature spaces 
studied classi cation performance classi er mpi chair database widely benchmarking literature 
show experimentally leads separating hyperplanes minimum possible estimate vc dimension constructed svm 
furthermore principle structural risk minimization minimizing estimated vc dimension showed useful strategy controlling prediction error 
outperformed best svm results nd literature 
classical lda algorithm inherits convenient property data visualization allows low dimensional views data vectors 
intuitive interpretation possible helpful practical applications 
studying performance classi cation problems theoretical comparison optimization criteria svm algorithm subject 
supported deutsche forschungsgemeinschaft dfg 
heavily pro tted discussions armin cremers lothar hermes 
blanz sch olkopf burges vapnik vetter 
comparison view object recognition algorithms realistic models von der malsburg von seelen eds arti cial neural networks pp 
berlin springer lecture notes computer science vol boser guyon vapnik 
training algorithm optimal margin classi ers fifth annual workshop computational learning theory pittsburgh acm pp 

application bias discriminant analysis commun 
statist theor 
meth pp 
duda hart 
pattern classi cation scene analysis wiley sons hastie buja 
penalized discriminant analysis annals statistics pp 
hastie buja 
flexible discriminant analysis optimal scoring jasa pp 
mika weston sch olkopf uller 
fisher discriminant analysis kernels proceedings august available svm gmd de publications html sch olkopf 
support vector learning 
phd thesis 
published oldenbourg verlag munich 
isbn 
sch olkopf smola uller 
nonlinear component analysis kernel eigenvalue problem neural computation vol 
issue pp 
mit press vapnik 
statistical learning theory wiley sons 
