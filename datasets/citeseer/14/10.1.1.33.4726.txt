september microsoft research st george house street cambridge cb nh united kingdom tel fax research microsoft com probabilistic principal component analysis michael tipping microsoft com christopher bishop microsoft com principal component analysis pca ubiquitous technique data analysis processing probability model 
demonstrate principal axes set observed data vectors may determined maximum likelihood estimation parameters latent variable model closely related factor analysis 
consider properties associated likelihood function giving em algorithm estimating principal subspace iteratively discuss illustrative examples advantages conveyed probabilistic approach pca 
keywords principal component analysis probability model density estimation maximum likelihood em algorithm gaussian mixtures 
copy supplied personal research 
published probabilistic principal component analysis journal royal statistical society series part pp 

probabilistic principal component analysis principal component analysis pca jolliffe established technique dimensionality reduction chapter subject may numerous texts multivariate analysis 
examples applications include data compression image processing visualisation exploratory data analysis pattern recognition time series prediction 
common derivation pca terms standardised linear projection maximises variance projected space hotelling 
set observed dimensional data vectors ft ng principal axes qg orthonormal axes retained variance projection maximal 
shown vectors dominant eigenvectors largest associated eigenvalues sample covariance matrix data sample mean sw principal components observed vector vector 
variables covariance matrix diagonal elements complementary property pca closely related original discussions pearson orthogonal linear projections principal component projection minimises squared reconstruction error kt optimal linear reconstruction wx notable feature definitions pca remarked texts absence associated probabilistic model observed data 
objective address limitation demonstrating pca may derived density estimation framework 
obtain probabilistic formulation pca gaussian latent variable model closely related statistical factor analysis 
model outlined section discuss existing precedence approach literature 
framework propose detailed section principal axes emerge maximum likelihood parameter estimates may computed usual eigen decomposition sample covariance matrix subsequently incorporated model 
alternatively latent variable formulation leads naturally iterative computationally efficient expectation maximisation em algorithm effecting pca 
probabilistic formulation intuitively appealing definition likelihood measure enables comparison probabilistic techniques facilitating statistical testing permitting application bayesian methods 
motivation probabilistic pca conveys additional practical advantages probability model offers potential extend scope conventional pca 
example illustrate section multiple pca models may usefully combined probabilistic mixture pca projections may obtained data values missing 
application dimensionality reduction probabilistic pca utilised general gaussian density model 
benefit doing maximum likelihood estimates parameters associated covariance matrix efficiently computed data principal components 
potential applications include classification novelty detection give example section 
conclude discussion section mathematical details concerning key results left appendices 
probabilistic principal component analysis latent variable models factor analysis pca factor analysis latent variable model seeks relate dimensional observation vector corresponding dimensional vector latent unobserved variables common model factor analysis relationship linear wx matrix relates sets variables parameter vector permits model non zero mean 
motivation latent variables offer parsimonious explanation dependencies observations 
conventionally latent variables defined independent gaussian unit variance 
additionally specifying error noise model likewise gaussian equation induces corresponding gaussian distribution observations ww 
model parameters may determined maximum likelihood closed form analytic solution values obtained iterative procedure 
motivation key assumption factor analysis model constraining error covariance diagonal matrix elements usually estimated data observed variables conditionally independent values latent variables latent variables intended explain correlations observation variables represents variability unique particular factor analysis fundamentally differs standard pca effectively treats covariance variance identically 
links factor analysis pca distinction variance covariance standard factor analysis model subspace defined maximum likelihood estimates columns generally correspond principal subspace observed data 
certain links methods previously established connections centre special case isotropic error model residual variances constrained equal 
approach adopted early young whittle factor analysis model young whittle addition residual variance presumed known model likelihood function 
case maximum likelihood equivalent squares criterion principal component solution emerges straightforward manner 
methodology employed young whittle differed conventionally adopted factors considered parameters estimated random variables 
stochastic treatment recovers similar result smallest eigenvalues sample covariance equal case simple show observation covariance model ww exact assuming correct choice may determined analytically eigen decomposition resort iteration anderson pp 

restrictive rarely justified practice assume known model second order statistics data exact 
presence additive observation noise exact covariance model generally undesirable 
particularly true practical application pca require exact characterisation covariance structure minor subspace information effectively discarded dimensionality reduction process 
probabilistic principal component analysis remainder focus case interest consider nature maximum likelihood estimators realistic case proposed model covariance equal sample counterpart estimated data enters likelihood function 
case investigated related pca early factor analysis literature lawley anderson rubin appear widely known 
authors show stationary points likelihood function occur matrix columns scaled eigenvectors sample covariance matrix average variance discarded dimensions give details shortly 
derivations fall short showing principal eigenvectors represent global maximum likelihood 
section re establish link pca factor analysis extending earlier derivation show appendix maximum likelihood estimators wml ml isotropic error model correspond principal component analysis 
give full characterisation properties likelihood function choose term probabilistic pca ppca 
addition give iterative em algorithm estimating parameters interest potential computational benefits 
motivate underline definition probability model advantageously exploited practice offer examples practical application ppca section 
probabilistic pca probability model isotropic gaussian noise model conjunction equation implies conditional probability distribution space tjx wx marginal distribution latent variables gaussian conventionally defined marginal distribution observed data readily obtained integrating latent variables likewise gaussian observation covariance model specified ww corresponding loglikelihood ln ln jcj tr maximum likelihood estimator mean data case sample covariance matrix observations ft estimates may obtained iterative maximisation example em algorithm appendix algorithm standard factor analysis rubin thayer 
contrast factor analysis may obtained explicitly see shortly 
conditional distribution latent variables observed may calculated bayes rule gaussian xjt defined note size probabilistic principal component analysis properties maximum likelihood estimators appendix shown ww likelihood maximised wml column vectors matrix principal eigenvectors corresponding eigenvalues diagonal matrix arbitrary orthogonal rotation matrix 
combinations eigenvectors non principal ones correspond saddle points likelihood function 
latent variable model defined equation effects mapping latent space principal subspace observed data 
may shown wml maximum likelihood estimator ml clear interpretation variance lost projection averaged lost dimensions 
practice find model estimate ml wml simplicity effectively ignore choose 
alternatively employ em algorithm detailed appendix convergence considered arbitrary 
factor analysis revisited estimators result application simple constraint standard factor analysis model note important distinction resulting isotropic noise covariance ppca covariant rotation original data axes standard pca factor analysis covariant component wise rescaling 
point contrast factor analysis factors factor model necessarily single factor model 
probabilistic pca see principal axes may incrementally 
dimensionality reduction general motivation pca transform data reduced dimensionality representation minor algebraic manipulation wml may obtain standard projection principal axes desired 
natural probabilistic perspective consider dimensionality reduction process terms distribution latent variables conditioned observation 
distribution may conveniently summarised mean hx jt wml note corresponding conditional covariance ml independent 
seen 
wml wml represents orthogonal projection latent space standard pca recovered 
density model singular undefined 
practice determined latent projection skewed origin result gaussian marginal distribution reconstruction wml hx jt orthogonal projection optimal squared reconstruction error sense 
optimal reconstruction observed data conditional latent mean may obtained case wml wml wml jt 
probabilistic principal component analysis examples give examples probabilistic pca exploited practice 
consider visualisation data sets missing values extend single projection model mixture case giving example covariance parameterisation implicit ppca offers effective mechanism restricting number degrees freedom gaussian model 
missing data probabilistic pca offers natural approach estimation principal axes cases data vectors nd exhibit missing random values 
drawing standard methodology maximising likelihood gaussian model presence missing values little rubin em algorithm ppca appendix may derive iterative algorithm maximum likelihood estimation principal axes latent variables fx missing observations ft complete data 
left plot shows projection examples dimensional data utilised ripley illustrate standard pca 
interest plot evidence sub groupings example 
simulated missing data randomly removing value dataset probability 
right shown equivalent ppca projection obtained em algorithm conditional means averaged conditional distribution missing observed values 
salient features projection remain clear despite fact data vectors suffered missing value 
projections data pca full dataset left ppca missing values right 
mixtures probabilistic pca models pca defines single linear projection relatively simplistic technique significant interest obtaining complex projection methods combining multiple pca models notably image compression dony haykin visualisation bishop tipping 
complex model readily implemented mixture probabilistic pca models 
means simple illustration shows pca projections virus data obtained component mixture model optimised em algorithm derived combining standard methods titterington smith makov probabilistic principal component analysis algorithm appendix theory projection data point appear plot corresponding sets principal axes associated component mixture practice examples need shown plot corresponding component model negligible conditional probability having generated 
effectively implements simultaneous automated clustering visualisation data powerful simply sub setting data eye performing individual principal component analyses 
multiple plots offer potential reveal complex structure single pca projection 
projections data obtained component ppca mixture model 
note locations projection planes superimposed single principal component projection plot left aid interpretation data structure 
controlling degrees freedom alternative perspective ppca applied simply covariance model data covariance defined terms auxiliary parameters particularly relevant larger values data dimensionality moderately sized data sets usually inappropriate fit full covariance model implies estimation free parameters 
cases constraints placed covariance matrix example diagonal parameters proportional identity matrix parameter 
covariance model probabilistic pca comprises dq free parameters permits control model complexity choice 
stress considering predictive power model explanatory sense interpret traditional factor analysis 
illustrate table shows estimated prediction error case negative log likelihood example various gaussian models fitted data 
dimensionality data large compared number examples complex models easily fit 
ppca density model latent space dimension gives lowest error 
practically problems apply ppca modelling class conditional densities select value values optimised classification accuracy 
explicitly eigen decomposition sample covariance matrix search appropriate complexity model performed explicitly relatively cheaply 
probabilistic principal component analysis covariance model isotropic diagonal ppca full equivalent number parameters prediction error table complexity bootstrap estimate prediction error various gaussian models data 
note isotropic full covariance models equivalent special cases ppca respectively 
discussion reiterated extended earlier lawley anderson rubin shown principal component analysis may viewed maximumlikelihood procedure probability density model observed data 
probability model gaussian model covariance determined simply application equations requiring computing eigenvectors eigenvalues sample covariance matrix 
addition explicit formulation em algorithm finding principal axes iteratively maximising likelihood function approach may efficient larger values data dimensionality discussed appendix examples section demonstrated utility probabilistic formalism performed pca dataset missing values generalised single model mixture case demonstrated capacity constrain number free parameters gaussian density model 
exploited possibilities practice obtain powerful algorithms data visualisation efficient methods image compression 
note factor analysis generally applied elucidate explanation data probabilistic pca closely related factor analysis formulation examples reflect motivation application general explanatory 
considered ppca mechanism probabilistic dimension reduction predictive density model 
authors referees valuable comments suggestions 
michael tipping originally supported aston university epsrc contract gr neural networks visualisation high dimensional data authors isaac newton institute cambridge hospitality preparation 
probabilistic principal component analysis anderson 

asymptotic theory principal component analysis 
annals mathematical statistics 
anderson rubin 
statistical inference factor analysis 
neyman ed proceedings third berkeley symposium mathematical statistics probability volume cal berkeley pp 



latent variable models factor analysis 
london charles griffin 

statistical factor analysis related methods 
new york wiley 
bishop tipping 
hierarchical latent variable model data visualization 
ieee transactions pattern analysis machine intelligence 
dony haykin 
optimally adaptive transform coding 
ieee transactions image processing 
hotelling 

analysis complex statistical variables principal components 
journal educational psychology 
jolliffe 

principal component analysis 
new york springer verlag 
marriott 
multivariate analysis part classification covariance structures repeated measurements 
london edward arnold 
lawley 

modified method estimation factor analysis large sample results 
uppsala symposium psychological factor analysis number monograph series pp 

uppsala almqvist wiksell 
little rubin 
statistical analysis missing data 
chichester john wiley 
pearson 

lines planes closest fit systems points space 
london edinburgh dublin philosophical magazine journal science sixth series 
ripley 

pattern recognition neural networks 
cambridge cambridge university press 
rubin thayer 
em algorithms ml factor analysis 
psychometrika 
titterington smith makov 
statistical analysis finite mixture distributions 
new york wiley 
whittle 

principal components square methods factor analysis 

young 

maximum likelihood estimation factor analysis 
psychometrika 
probabilistic principal component analysis maximum likelihood pca stationary points log likelihood gradient log likelihood respect may obtained standard matrix differentiation results see marriott sc stationary points sc assuming exists see requires rank assumption implies loss practicality 
possible classes solutions 
trivially seen minimum log likelihood 
second case covariance model exact smallest eigenvalues identical discussed section 
identifiable ww known solution square matrix columns eigenvectors corresponding diagonal matrix eigenvalues arbitrary orthogonal rotation matrix 
interesting solutions represent third case sc find express parameter matrix terms singular value decomposition matrix orthonormal column vectors diag diagonal matrix singular values orthogonal matrix 
substituting decomposition gives manipulation sul equation implies su vector column eigenvector corresponding eigenvalue arbitrary 
potential solutions may written matrix columns eigenvectors arbitrary orthogonal matrix diagonal matrix elements corresponding eigenvalue case may seen equivalent 
global maximum likelihood matrix may contain eigenvectors identify maximise likelihood expression substituted log likelihood function give ln ln ln probabilistic principal component analysis number non zero eigenvalues corresponding eigenvectors retained discarded 
maximising respect gives ln ln ln note implies rank stated earlier 
wish find maximum respect choice eigenvectors eigenvalues retain discard 
exploiting constancy sum eigenvalues condition maximisation likelihood expressed equivalently minimisation quantity ln ln depends discarded values non negative jensen inequality 
interestingly minimisation leads requirement discarded adjacent spectrum ordered eigenvalues equation requires deduce smallest eigenvalue discarded 
sufficient show minimised smallest eigenvalues likelihood maximised largest eigenvalues noted maximised respect fewest terms sums occurs zero 
furthermore minimised may seen equivalent case 
nature stationary points stationary points represented minor eigenvector solutions stable maxima local maximisation em algorithm example guaranteed converge global maximum comprising principal eigenvectors 
may show minor eigenvector solutions fact saddle points likelihood surface 
consider stationary point gradient equation may contain arbitrary eigenvectors defined contains corresponding eigenvalue 
clarity rotation ignored easily incorporated analysis 
consider small perturbation column vector form arbitrarily small positive constant discarded eigenvector 
represent stable solution dot product likelihood gradient perturbation negative 
dot product may straightforwardly computed ignoring terms value corresponding eigenvalue corresponding perturbation positive definite sign gradient determined 
term negative case maximum stable 
saddle point 
stationary point probabilistic principal component analysis stable average eigenvalues eigenvalues eigenvalues identical 
case considered section 
considering possible perturbations possible column vectors seen stable maximum occurs comprises principal eigenvectors 
equality eigenvalues equality principal eigenvalues affect analysis 
consideration instance minor discarded eigenvalue equal identical smallest principal retained eigenvalue 
practice particularly case sampled covariance matrices exact case 
consider example extracting components data covariance matrix possessing eigenvalues 
case second principal axis uniquely defined minor subspace 
spherical noise distribution defined addition explaining residual variance optimally explain second principal component 
zero effectively comprises single vector 
combination single vector noise distribution represents maximum likelihood 
em algorithm probabilistic pca em approach maximising likelihood ppca consider latent variables fx missing data complete data comprise observations latent variables 
corresponding complete data log likelihood ln fp ppca definitions section exp kt wx exp kx step take expectation respect distributions jt hl ln tr hx hx tr omitted terms independent model parameters hx hx hx 
note statistics computed current fixed values parameters follow earlier 
probabilistic principal component analysis step hl maximised respect giving new parameter estimates hx hx nd kt hx tr hx maximise likelihood sufficient statistics conditional distributions calculated revised estimates parameters obtained 
equations iterated sequence algorithm judged converged 
may gain considerable insight operation em algorithm substituting hx hx 
manipulation leads step step combined re written sw sw tr swm note instance equation old value parameter matrix second instance new value calculated equation 
equations indicate data enters em formulation covariance matrix expected 
algebraically convenient express em algorithm terms note care exercised implementation 
considerable computational savings obtained explicitly evaluating need done initialisation 
computation requires nd operations inspection indicates complexity 
reflected fact require terms form sw tr 
computing sw efficient equivalent finding explicitly 
trade cost initially computing directly computing sw cheaply iteration clearly depend number iterations needed obtain accuracy solution required ratio final point note convergence columns wml span principal subspace need orthogonal wml wml diagonal common factor analysis iterative pca algorithms exists element rotational ambiguity 
required true principal axes may determined noting equation represents eigenvector decomposition wml wml transposed rotation matrix simply matrix columns eigenvectors matrix wml wml 
