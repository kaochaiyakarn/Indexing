informing loads enabling software observe react memory behavior mark horowitz margaret martonosi todd mowry michael smith technical report 
csl tr numbered stan cs july research supported arpa contract dabt 
addition margaret martonosi supported part national science foundation career award ccr 
todd mowry supported research natural sciences engineering research council canada 
michael smith supported national science foundation young investigator 
ccr 
informing loads enabling software observe react memory behavior mark horowitz margaret martonosi todd mowry michael smith computer systems department department electrical division laboratory electrical engineering computer engineering applied sciences stanford university princeton university university toronto harvard university technical report csl tr numbered stan cs july computer systems laboratory departments electrical engineering computer science stanford university stanford california memory latency important bottleneck system performance adequately solved hardware 
promising software techniques shown address problem successfully specific situations 
generality software approaches limited current architectures provide fine grained low overhead mechanism observe memory behavior directly 
fill need propose new set memory operations called informing memory operations particular describe design functionality informing load instruction 
instruction serves primitive allows software observe cache misses act information inexpensively processor typically idle current software context 
informing loads enable new solutions important software problems 
demonstrate examples show usefulness collection fine grained memory profiles high precision low overhead ii automatic improvement memory system performance compiler techniques take advantage cache information 
find apparent benefit informing load instruction quite high hardware cost functionality quite modest 
fact bulk required hardware support today high performance processors 
key words phrases memory latency performance monitoring prefetching processor architecture cache notification 
copyright mark horowitz margaret martonosi todd mowry michael smith gap processor memory speeds continues widen memory latency dominant bottleneck application execution time 
current uniprocessor machines main memory takes order processor cycles dek shared memory multiprocessors latencies remote memory times larger koh 
latencies expected increase 
cope memory latency computer systems today rely cache hierarchy reduce effective memory access time 
caches important step addressing problem purely hardware mechanisms stream buffers jou complete solutions 
addition hardware mechanisms number software techniques proposed avoiding tolerating memory latency 
example performance monitoring tools attempt measure memory bottlenecks lie give indications programmers fix 
automatic compiler transformations attempt generate code improved locality better latency tolerance 
operating systems attempt adjust page coloring migration strategies response memory referencing behavior 
techniques successful cases handicapped fact software directly observe behavior memory system 
basic limitation monitoring tools forced measure memory overhead indirectly coarser granularity desirable gh rely simulated data memory system behavior mar lw compiler optimizations driven static guesses dynamic observations caching behavior mow page migration mapping optimizations operating systems turning specialized hardware support bus hardware monitors cache lookaside buffers 
fundamental problem load instructions defined memory hierarchies flat memory latency prime concern 
model uniform high speed memory 
memory system important bottleneck time re examine semantics memory instructions 
propose new set memory operations informing memory operations serve primitives allow software observe cache misses directly act knowledge inexpensively current software context 
informing load instruction example gives software ability react differently unexpected case load missing primary cache execute code processor normally stall 
note informing loads represent new load instructions added functionality intended supplant existing types load instructions 
particular informing loads replace need prefetch instructions inserted compiler believes subsequent load operation cache 
concentrate loads define informing memory operations informing stores indicate store misses cache informing prefetches software detect unexpected undesirable case prefetch hitting cache 
potentially different types informing memory operations ways implement functionality hardware share number common characteristics 
section presents characteristics context informing load instruction contrasts approach existing approaches observing reacting memory behavior 
section focuses observability misses feasible inexpensive collect wide variety memory profiling information 
information useful pro 
software care particular load hits misses free regular informing loads 
grammer compiler operating system automatically improve performance demonstrate section 
section shows informing load primitive update statistics take active measures improve memory performance prefetching data item expected having demonstrated utility mechanism section describes number implementations informing loads shows added implementation cost modest 
section summarizes findings describes continuing efforts area 
architectural characteristics informing loads computing industry largely agrees critical importance growing processor memory performance gap reached consensus type degree hardware support appropriate monitoring memory behavior 
number disjoint specialized solutions proposed different parts problem 
example chip hardware counters implemented offer minimal support compilers performance monitors dec chip hardware cache lookaside buffer proposed support operating systems activities page coloring 
approaches general efficient support needs compiler operating system 
furthermore existing approaches contains solutions heavyweight access monitoring information greatly disrupts behavior monitored program coarse grained monitoring information summary actual memory system behavior 
characteristics inappropriate software techniques requiring fly high precision observation reaction memory system behavior finegrained lightweight mechanism needed 
reasons fact spans number disciplines computer architectures motivated define memory system monitoring mechanism characteristics general independent particular hardware organization fine grained enables low level observation memory system selective notification invoked triggering action occurs low overhead introduces little perturbation monitored program invoked instantaneous notification supports fast response triggering action primitive provides notification triggering action 
proposal meet goals informing load instruction non blocking load instruction capable squashing inhibiting execution instruction immediately follows informing load sequential program 
single issue risc machine instruction delay slot informing load instruction 
sense informing load akin squashing branches sparc architecture pau operations hp pa risc architecture hp 
informing load operation delay slot instruction load hits cache 
informing loads non blocking loads software invoke alternate action processing informing load misses cache 
words instruction informing load executed informing load misses cache 
functionality allows single instruction speculative prefetch related load may inlined informing load 
implement complex instruction sequences filling informing load slot jump code monitoring routine starting point alternatively scheduled code 
functionality informing loads exhibit lower overhead existing approaches monitoring memory system 
major difficulty observing reacting memory behavior memory interspersed code fine granularity 
instrumentation techniques attempt trap read privileged timers memory clearly fail execution time overhead cache perturbation techniques prohibitive 
machines offered high resolution timers programmers 
timers support loop level monitoring code gh high overhead time record individual memory latencies 
attempt address problem inclusion various forms hardware cache counters 
example alpha includes performance counter configured cause interrupt cache misses occurred dec 
overhead cache perturbation handling full interrupt techniques intrusive larger count 
processor architects willing define counter part architecture read user level overhead reading counter values load doing comparison prohibitive monitoring load 
quantify section low overhead notification allows fine grained memory performance monitoring 
finer granularity opens new possible uses memory performance information adaptive prefetching discussed section 
low overhead informing load instruction coupled instantaneous notification currently executing thread 
latency hiding techniques force light weight context switch cache 
course notification cache informing load implement lightweight context switch functionality 
addition light weight context switches informing loads support wide variety application level responses cache example speculatively execute additional code cache misses dynamically control degree parallelism running code increase reduce number parallel application threads execute alternative instruction schedule statically optimized case cache broader sense code executed response informing load access referencing application context simply hiding machine latency executing independent unrelated thread 
individual users get benefits informing loads sequential code 
informing load instruction primitive supports variety proposed latency tolerance avoidance techniques unified way 
preliminary list include program performance monitoring ii software controlled prefetching iii multithreading iv speculative execution operating system page coloring migration vi dynamic instruction rescheduling 
sections examine evaluate implementations techniques informing loads 
monitoring program performance number performance tools proposed monitor program caching behavior bm gh lw mar 
major stumbling blocks building tools gathering appropriately detailed memory statistics low runtime overheads minimal perturbations monitored program 
example gathers memory statistics loop nests comparing basic block execution times program runs estimates execution times ideal memory behavior 
way able techniques program counter sampling gather program memory statistics 
main drawback approaches statistics loop basic block granularity coarse grained useful understanding program memory bottlenecks 
example blocked matrix multiply codes access matrices main loop nest 
matrices blocked matrix susceptible poor memory performance due conflict misses 
statistics report problem entire loop pinpointing bottleneck particular data structure point 
finer grained memory statistics tools rely dedicated hardware monitor memory 
burkhart bm implemented tools data collected special hardware bus monitors 
approaches increasingly difficult due levels integration modern processors 
level second level caches chip cache performance monitoring warrants integrated processor support 
techniques monitor memory behavior trapping particular accesses simulating trapping values sampled hardware counters dec 
trap techniques incur overheads cycles trapped event just get monitoring code 
addition cache perturbation due trap handling significant 
contrast informing loads trap costs cycles depending implementation 
low overhead greatly increases flexibility monitoring style 
furthermore philosophical efficiency clear justification requiring applications operating system services monitor performance 
drawbacks hardware trap monitoring tools mar cprof lw direct execution simulation 
approaches require dedicated hardware unfortunately streamlined implementations simple simulators impose slowdown factors application execution time 
overheads may acceptable alternatives gathering needed data unavoidable tradeoff accuracy memory system simulated tool runtime overhead 
final significant drawback multiprocessors overheads simulation approaches scale linearly number simulated processors due fine grained synchronization necessary parallel simulation 
informing loads primitive capturing program memory performance information avoid high simulation overheads scaling behavior monitoring parallel programs 
integrated processor design avoiding black box problem faced dedicated hardware bus monitors today highly integrated processors 
low overhead allows development fine grained monitoring tools 
memory performance monitor informing loads informing loads general primitive imagine implement different tools 
tools range extremely inexpensive techniques program counts sampling extremely detailed techniques including high level program semantics correlating misses surrounding loop iterations data addresses 
demonstrate utility informing loads performance monitoring implemented fairly simple low overhead memory performance tool gives precise counts cache misses different points code 
program loads accessing memory stack pointer informing load 
informing load delay slot place procedure call jump link monitor 
omit stack frame generally hit cache clearly monitor 
ing code 
load turns cache hit procedure call squashed application execution continues 
load turns cache procedure call short sequence monitoring code executed 
action taken monitoring call depends degree sampling 
sampling informing load results hash table lookup subsequent increment appropriate counter 
application program counter value generates informing load separate count 
statistics kept static point code 
informing loads monitoring code execute directly application context tool takes advantage fact return address register monitoring procedure uniquely identifies suffered value index hash table 
efficiency access return address register monitoring code written assembly language 
sampling nth results actions misses simply decrement sampling counter 
show results equal sampling 
memory monitoring approaches sampling restricted choice sampling frequency reduce execution time overhead improve accuracy 
trap techniques monitor infrequently order reduce overhead cache perturbation 
limits ability keep fine grained statistics 
hand sampling techniques overcome cold start effect sample large chunks memory simulated sample 
informing loads low overhead low perturbation flexibility choose sampling frequency broader range values 
results run tool collection spec applications dix smi 
collected static data described 
sections information execution time overhead informing load tool 
quantify data cache perturbation induced running monitoring code interspersed application code 
results sections presenting simulation results collected simulator pixie smi 
model single issue processor split direct mapped primary instruction data caches kb kb direct mapped unified secondary cache 
primary satisfied secondary cache takes cycles primary going way memory takes total cycles 
floating point stalls modelled 
execution time overhead measure assumes essentially worst case behavior simple single issue pipeline described section 
superscalar machines typically free instruction fetch execute positions available overheads decrease 
initial tool overhead shows normalized execution times memory tool monitor applications 
bars normalized execution time code 
columns give tool overheads different levels monitoring sampling 
execution time overheads implementation range grep espresso 
wide range overheads stems variance application rate cache rate degree monitoring code perturbs application caching behavior 
sampling implementation overheads drop range grep compress 
comparison produces detailed statistics spite reports overheads range subset applications gh 
uses simulation approach collect detailed memory statistics execution time overheads typically range 
performance overheads informing load tool competitive high level tools offering detailed statistics superior simulation tools offering similar levels detail statistics 
importantly generate statistics true executions program cache simulations 
statistics reflect impact operating system multiprogramming program cache behavior 
data cache perturbations monitoring real program execution tool potentially perturb program behavior 
data cache effects fold impact 
monitoring potentially perturb application data cache behavior measured memory statistics closely correspond memory behavior application 
second monitoring substantially increases data cache rate may slow application execution appear form tool runtime overhead 
indicate impact data cache perturbation compares application data cache behavior monitored code 
application leftmost bar gives data load rate monitoring 
bars show application data load rate monitoring occurring different sampling frequencies 
data simulate combined cache effects monitoring application code 
keeps data oriented statistics approximately execution time spent cache simulation lightweight context switching overheads drop significantly data oriented statistics omitted 
version tool described uses sampling reduce overheads 
simulating tenth total stream large samples overheads 
simulation approach overcome cold cache effect part trace sampling aggressive sampling feasible informing loads total misses leads intolerable inaccuracy 

normalized execution times monitored applications different levels sampling 
sampling bar represents monitoring informing load cases keep statistics informing load respectively 
normalized execution time 
sampling xlisp compress espresso alvinn tomcatv grep sampling sampling keep separate statistics misses incurred behalf application 
isolates effect monitoring application caching behavior 
frequent performance monitoring data indicate performance monitoring impact data cache behavior 
case data load rates elevated 
moderate sampling cache perturbation drops dramatically 
applications compress perturbation negligible 
particular case perturbation remains frequent conflicts important program variable location save away application register performing sampling count check 
avoid sorts perturbation implemented version tool uncached loads stores memory performed monitoring code 
monitoring cached directly perturb application data cache behavior 
version uncached loads allows virtually perfect reproduction application original memory behavior 
application memory behavior perturbations due secondary effects 
example time dilation due monitoring may mean application undergoes context switches 
version uncached execution overhead higher tool cached ranging grep compress 
relative cached tool overhead increased factors 
despite overhead increase uncached version useful applications cached monitoring code causes significant perturbation 
users opt tool notice discrepancy memory behavior predicted cached version coarser grained loop level lower perturbation tool implemented informing loads 
studies direct mapped caches second levels memory hierarchy way set associativity decrease cache conflicts application monitoring code expand set applications amenable faster cached monitoring 
parallel applications monitoring uncached loads easier account monitoring time synchronization points tools correct monitoring time ensure dynamic task assignments identical monitored code 

effect monitoring application cache rates 
application cache rate data loads xlisp compress espresso alvinn tomcatv grep sampling sampling sampling tool implementations described case studies highlight strengths informing loads 
providing low overhead means observing reacting cache misses informing loads give crucial support fine grained memory performance monitoring tools 
initial cached tool monitors individual memory code low overheads tolerable data cache perturbations 
version uncached higher overhead eliminates data cache perturbation 
improving memory performance automatically improve application performance informing loads requires merge methods described previous section software techniques improving memory performance 
techniques include compiler optimization blocking mc wl gl software controlled prefetching mow por operating system optimizations page coloring kh page migration le cf bfs 
informing loads success automatic techniques depend heavily compiler predict caching behavior ahead time 
unfortunately predicting dynamic caching behavior static information quite difficult appears tractable scientific codes regular predictable access patterns 
regular codes complications unknown loop bounds set associative caches difficult model caching behavior accurately 
expect incorporation dynamic information decision making process techniques important step overcoming current limitations 
dynamic information fed back optimization process ways runs program recompile second run behavior run ii run program code able monitor adapt dynamic information fly 
advantage approach run program benefit dynamic information potential disadvantage runtime overhead processing reacting dynamic information 
subsections software controlled prefetching example demonstrate approaches implemented informing loads 
dynamic memory information compile time idea dynamic information compile time new 
compilers historically control flow feedback known branch profiling perform aggressive instruction scheduling branches fis smi 
informing loads practical collect accurate rates entire applications demonstrated earlier section similar feedback methodology enhance aggressive memory optimizations software controlled prefetching 
previous studies demonstrated codes regular access patterns compiler inserted prefetching effectively hide memory latency improving execution time twofold uniprocessor multiprocessor systems mow 
key step compiler algorithm locality analysis predict dynamic suffer cache misses prefetched 
locality analysis helpful reducing prefetch overhead scope limited affine array accuracy limited nature model 
enhance predictions locality analysis regular access patterns prediction irregular accesses hit cache exploit memory feedback information 
precise rates may sound perfect information subtle issue handle intermediate rates 
ideally prefetch unfortunately information relating individual misses occur lost course summarizing single rate 
simplest approach prefetch time depending contribution total misses exceeds certain threshold 
sophisticated approaches involve reasoning misses occurred 
regular access patterns combination locality analysis control flow feedback may helpful 
example consider code 
assume element bytes cache line contains bytes primary cache size kbytes memory feedback tells load suffered rate 
locality analysis expect spatial locality inner loop possibly temporal locality outer loop depend large relative cache size 
control flow feedback indicates loops average trip counts iterations respectively expect iterations fourth iteration explaining rate 
isolate misses peeling iteration unrolling factor 
locality analysis control flow feedback may shed light misses occur recognize regular access patterns address irregular access patterns scope locality analysis 
example rate may correspond different patterns 
possibility combination temporal spatial locality described 
possibility locations cache loop nest entered misses occurred sporadically iterations due occasional conflicts foo 
improve information content memory profile correlate misses dynamic context occur 
array codes useful dynamic context distinguish loop iteration remaining iterations capture temporal locality loop iteration modulo cache line size capture spatial locality 
example combination temporal spatial locality described earlier notice misses occurred iteration fourth iteration sporadic pattern due conflicts notice misses scattered iterations 
irregular access patterns dereference pointer dynamic context consist paths control flow graph arrive point 
example discover 
note rate misleading metric relatively low rate cause majority misses application frequently executed 

examples may suffer misses occasionally 
foo foo dereferencing results rate misses correspond directly part conditional statement taken time schedule prefetch path minimizing instruction overhead 
properties informing loads flexibility low overhead complete access current software context feasible collect profile misses correlated dynamic contexts 
having discussed range possible implementations experimental results demonstrate performance benefits exploiting dynamic memory information compile time 
experiments focus regular array codes 
informing loads collect rates load similar monitoring code described section misses correlated occur 
augment compiler algorithm rates follows 
performing locality analysis predicted observed rates compared 
disagree certain margin locality analysis model adjusted factors uncertainty control flow feedback account find explanation rate consistent intrinsic data reuse details see mow 
allows compiler reason intermediate rates schedule prefetches dynamic instances expected simulated array scientific codes earlier prefetching study architectural assumptions 
cases improved significantly memory feedback ocean uniprocessor version splash application swg 
shows performance ocean broken categories time spent executing instructions including instruction overhead prefetching stall time due data misses stall time due memory overheads caused prefetching primarily contention primary cache tags prefetch fills 
performance ocean speeds memory feedback static information 
reason static analysis fails case critical loop nest split separate files outer loop file inner loop inside procedure call file 
shows simplified version scenario 
version suif compiler perform interprocedural analysis separate files prefetching algorithm recognize group locality due outer loop issues prefetches 
feedback information available compiler immediately recognizes group locality avoiding unnecessary prefetches 
interestingly eliminating normalized execution time prefetch memory overhead ocean memory access stalls instructions 
performance ocean memory feedback compile time 
prefetching prefetching static analysis prefetching feedback 
jm int col im col fact col col col col col prefetches reduces memory stall time case eliminating register spilling spilled conflicting data 
ocean illustrates codes regular access patterns normally expect static analysis perform benefit dynamic information compile time 
experience compiling array codes indicates reasoning intermediate rates challenging part memory feedback greater gains achieved informing loads fully exploited correlate misses dynamic contexts 
expect irregular codes show greater benefit memory feedback currently viable means predicting misses static information codes 
dynamic memory information run time memory feedback gives compiler information reason shortcomings 
feedback process bit cumbersome requires program compiled twice 
second may difficult impossible capture representative dynamic profile particularly behavior depends critically data set fits cache data set size determined run time 
generating code fixed memory optimization strategy possibility generate code adapts dynamically run time 
example informing loads indicate misses occurring expected code adapt issuing prefetches 
similarly code reduce number prefetches detects hitting cache 
tailoring code possible contingency theoretically result exponential code growth news practice appear small number different cases specialize 
intuitively key distinction data set fits cache typically results just distinct prefetching strategies 
compiler uncertain potentially generate cases choose appropriate execute run time 
potential drawback adapting run time additional overhead processing reacting dynamic information key concern frequently code need adapt strategy 
array scientific codes regular repetitive nature computation allows detect trends infrequent checks counts demonstrate section 
contrast unpredictable nature irregular codes means may want adapt strategy may sound daunting properties informing loads allow react fine granularity minimal overhead simply schedule code actively improves performance informing load delay slots describe section 
adapting regular codes counts regular access patterns array codes easy detect dynamic trends patterns recur pass loop 
reasonable way adapt prefetching strategy loop instrument initial set iterations behavior guide approach handling remaining iterations 
illustrates implemented little run time overhead 
assuming elements fit cache line elements potentially fit cache loop iterations sufficient hide memory latency shows code prefetch elements cache entering loop prefetches result unnecessary overhead 
hide cache latency paying unnecessary prefetches modify prefetching strategy shown 
informing prefetches analogous informing loads test initial elements cache 
prefetching continue prefetching usual 
note informing prefetches informing loads case able hide latency iterations testing presence data 
hardware counters directly manipulated transferring general purpose register suffice simple example 
informing prefetches advantageous loops multiple array maintain separate counts providing greater flexibility adaptiveness 
experimental results bcopy library routine copy block data location 
bcopy simple routine interesting reasons 
library routine compiler assumptions input parameters analyze call sites locality 
second bcopy frequently executed operating system improving performance may significantly improve system performance 
rewrote bcopy hand adaptive prefetching similar code 
simple workload drive bcopy mainly interested testing ends spectrum 
workload consisted loop repeatedly called bcopy distinct arrays block size arguments 
identical block copies performed subsequent iterations potentially temporal locality benefit blocks fit kbyte cache 
shows performance bcopy various block sizes loop iteration counts 
see case bcopy called times byte blocks original code prefetching suffers significant amount latency 
misses occur time routine called data remains cache subsequent calls 
see middle bar code 
separate hardware counters maintained load misses undesirable prefetch misses desirable prefetch hits 

example dynamic counts adapt prefetching array codes 
original code cache 
sum sum code static prefetching prolog prefetch steady state prefetch sum sum sum sum epilog sum sum code adaptive prefetching pf count issue prefetches informing prefetch pf count misses prefetches hit far 
pf count prefetching sum sum continue prefetching prefetch sum sum sum sum sum sum statically prefetches blocks time performs worse original case due large instruction overhead unnecessary prefetches 
adaptive code shown righthand bar offers best performance prefetches data appropriate 
cases show larger block sizes fit cache 
cases stopping check counts pure overhead best strategy prefetch time see overhead negligible impact performance 
enjoy benefits adaptiveness paying additional run time costs 
adapting irregular codes active measures irregular codes containing pointers linked lists pose greater challenge adaptive memory optimizations unclear detect sustained trends patterns trends exist 
contrast coarse grained approach array codes check react dynamic behavior pass loop fine grained approach may appropriate irregular codes want adapt strategy frequently load 
example load may wish launch prefetches avoid misses 
multithreaded architecture may wish switch threads spawn new threads response overhead fine grained reactions normally prohibitive hide overhead scheduling instructions actively improve performance normalized execution time prefetch memory overhead memory access stalls instructions 
results adaptive version bcopy prefetching statically prefetch time adapt prefetching dynamically 
means byte block copied destination times 
performance renormalized case 
informing load delay slots 
majority turn hit cache code execute maximum speed adaptive code invoked 
experiment active approach modified compiler developed torch smi fill informing load delay slots prefetch load address possible 
shows resulting performance compress spec benchmark case prefetches inserted just ahead preceding loads allowing misses pipelined 
note experiments assume informing load hits slot instruction waste execution cycles 
aggressive base model section typical modern processors instruction fetching decoupled execution 
see adaptively issuing prefetches response individual load misses offers twice performance improvement statically issuing prefetches time results significantly instruction overhead 
section shown number ways informing loads improve performance software controlled prefetching 
low overhead selective notification informing loads provided building block collect needed information minimal performance overhead 
expect capabilities benefit automatic memory optimizations cache blocking page coloring 
hardware implementation previous sections shown informing loads useful primitive measuring reducing memory overhead programs 
section describes informing load implemented current processors 
mechanisms needed informing loads processors support lockup free caches squashing branches 
demonstrate fact section describes implement informing load instruction number different machine pipelines 
describing simple single issue risc pipeline simplification mips pipeline hp 
reviewing basic pipeline machine organization discuss changes needed support informing loads 
look implementations informing load functionality simple machine move description implementations complex 
model instruction cache penalties accurately 
appear part instructions category happen negligible case 
normalized execution time prefetch memory overhead memory access stalls instructions 
performance compress active prefetching prefetching statically prefetch time adaptively issue prefetches informing load delay slots 
machines 
spectrum possible implementations case representative hardware software costs informing load instruction 
base machine model base machine stage pipeline instruction fetch instruction decode register fetch rd alu execution ex data memory access register write back wb 
instructions complete wb phase pipeline squashed turned null operation asserting suitable signal wb 
squashing instructions pipeline necessary support precise exceptions hp 
normal load operation pipeline calculates effective address load ex accesses cache address load data returns cpu stage write data register file wb 
illustrates pipeline important instruction timings 
load misses cache pipeline generally stalls load data brought cache returned processor 
options stall machine 
simplest option stall stalls machine detected holds pipeline data returned 
stall model little conservative instructions load don load data processor execute processing cache stall model allows machine continue operate missing data referenced 
option stall common potential performance benefits 
principle cost alternative stall model cache able handle serviced 
requires lockup free cache cache handle requests outstanding 
simplest case cache handle sophisticated designs cache handle multiple outstanding misses kro lau 
machine informing loads execute instructions cache machine stall stall changing stall model necessary hardest part implementing informing load stall circuitry lockup free cache design complex 
modern machines implement stall model focus changes required 
changes relate instruction informing load delay slot squashed 
squashing slot instruction informing load differs normal squashing instructions squashing branches sparc architecture operations hp pa risc signal comes late earliest 
simple machine condition arrives easily cancel effects instruction informing load delay slot 
pipeline changes branches alu operations occur ex slot instruction informing load 
rd ex wb data returns cache hit signalled exceptions taken branch condition determined effective address available 
pipeline simple machine showing various instructions signals occur 
hardware cancel effects instruction due exception normal load operation cancelling slot instruction difficult 
similarly exceptions signaled stage instruction instruction delay slot informing load cause extraneous exception 
machines direct mapped cache signal valid data means settle wb 
machines start executing wb cycle load indicated provide way re execute cycle cache occurs 
facility needed handle situation shown instruction ex phase result load 
instruction received wrong value re executed correct data returned 
mechanism implement informing load 
common case cache hit squash load delay slot instruction default informing load misses re execute cycle allow slot instruction complete overhead approach lost cycle occurs 
machines extra cycle necessary decision informing load slot store informing load slot change state soon model fact stores take memory cycles tag probe store data memory state changed wb 
simple pipeline branch delay cycles 
machines including branch point sooner occurs rd machines branch control transfer instruction cti troublesome changes state early pipeline particular hit determined informing load see 
want update program counter outcome branch informing load delay slot informing load misses dilemma 
solution similar proposed late signals 
cti encountered slot ignored assumed squashed bit set indicating instruction slot cti 
informing load slot instruction cti instructions pipeline informing load squashed machine starts fetching cti instruction treats normally 
bulk hardware needed support approach keeping addresses instructions pipeline squashing instructions currently needed execution support exception handling 
additional hardware state bit indicates slot instruction branch 

ignoring subtle important effects performance small amount extra capacitive loading squash signal due extra squash condition 
architectural mechanisms slots handling exceptions informing load requires care ensure machine knows execute slot instruction 
informing loads easy interrupt occurs load completes set return pc correct location slot slot depending status load 
rd ex wb rd ex rd ex detect wb wb wb ex load load slot dependent instr 
pipeline diagram showing machine late signal 
cost simple solution machine loses cycles branch instruction re executed 
overhead occurs informing load misses infrequent case small overhead tolerated disrupted pipeline 
reduce delay inclusion additional register pc unit machine 
register store calculated target cti executed 
occurs restarting cti instruction machine load register pc restart target branch 
situation order issue superscalar machine similar scalar processor 
fact ability deal late information allows slot instruction issued informing load cycle load 
informing load delay slot instruction assumed squashed re executed informing load misses 
surprisingly situation order issue machines ways simpler 
machines dependency analysis scheduling hardware issue informing load slot instruction held cache hit resolution known 
point instruction issued squashed 
instruction issued execution unit known need back pipeline occurs 
complexity case forming dependency informing load slot instruction load different standard case 
hard principle adding order machine trivial 
addition informing load instruction architecture supports squashing branches hit caches difficult 
complexity arises situations hit information available early desired 
appropriate approach assume informing load hit optimize pipeline actions common case 
performance overhead section showed hardware cost support informing load instruction quite small 
unfortunately simple single issue pipelines simple implementation performance cost associated 
simple scalar machines cost set need fetch slot instruction informing load 
machines fetch execute instruction cycle usually squashed slot instruction increases instruction count application uses informing loads 
cost adding instruction quite instruction informing load cases load delay slot filled nop instruction dependent instruction causes load interlock 
actual overhead ranged high extra instructions informing load program compress relatively nops load low extra instructions informing load program grep mips compiler usefully fill load delay slots 
overhead slightly smaller rd ex wb hit branch rd ex wb load slot branch 
diagram showing problem machines early commit cti instructions 
gets simply counting fill rate load delay slots adding instruction load may cause load slot instruction replace nop branch delay slot 
collection spec benchmarks unix utilities dix smi overhead averaged instructions informing load 
superscalar machine informing load overhead dramatically lower average data dependences typically limit average execution rate instructions value peak rate machine 
means exists free fetch execute positions informing load slot instructions 
aggressive superscalar compiler built torch smi simple dual issue machine limited parallel resources 
compared cycle counts original code code replaced loads informing loads 
cycle counts assumed caches hit informing load slot instructions pure overhead 
average cost lower instructions informing load compress instructions informing load grep 
increasing issue width superscalar machine providing wide variety parallel functional units basically reduces overhead zero 
current superscalar machines informing loads provide truly low overhead method monitoring memory system performance 
proposals provide software feedback memory system performance directed solving particular problem 
providing specific support particular application informing loads provide programmers general mechanism extracting detailed information memory system particular notification load operation hit cache 
overhead general mechanism small especially superscalar machines informing loads program finegrained fashion collect information react situation requires fast extremely lightweight response 
primitive easily added architecture informing memory operations allow programmer decide information needed information 
demonstrate utility informing load instruction described improve functionality important software techniques performance monitoring software controlled prefetching 
shown detailed memory profiles captured overhead increase execution time un instrumented code methods 
demonstrated informing loads exploited variety ways compiler automatically improve performance offered software controlled prefetching possible static information 
clearly done areas 
example performance monitoring informing loads categorize cache misses different data types develop extremely efficient tools parallel memory performance monitoring 
expect automatic memory optimizations cache blocking page coloring benefit informing loads 
latency hiding techniques include potentially complex functionality designed informing memory operations primitive 
goal provide memory system information back user simple form possible giving application flexibility choose information collect actions take 
preliminary results indicate successful achieving goals 
informing loads provide software information memory system performance low cost 
identified evaluated ways software take advantage informing loads 
feel general availability informing loads real hardware spur innovative uses 
acknowledgments chi luk university toronto help modifying compiler providing simulation results 
research supported part arpa contract dabt 
addition margaret martonosi supported part national science foundation career award ccr 
michael smith supported national science foundation young investigator 
ccr 
todd mowry supported research natural sciences engineering research council canada 
abu kuck lawrie 
automatic program transformations virtual memory computers 
proc 
national computer conference pages june 
bfs william bolosky robert fitzgerald michael scott 
simple effective techniques numa memory management 
proceedings th acm symposium operating system principles pages 
bershad lee romer chen 
avoiding conflict misses dynamically large direct mapped caches 
proceedings sixth international conference architectural support programming languages operating systems pages october 
bm burkhart roland millen 
performance measurement tools multiprocessor environment 
ieee transactions computers may 
chandra devine verghese gupta rosenblum 
scheduling page migration multiprocessor compute servers 
proceedings sixth international conference architectural support programming languages operating systems pages october 
cf alan cox robert fowler 
implementation coherent memory abstraction numa multiprocessor experiences platinum 
proceedings th acm symposium operating system principles pages 
chen mahlke chang hwu 
data access microarchitectures superscalar processors compiler assisted data prefetching 
proceedings 
richard george watson ray ng 
special report memory 
ieee spectrum october 
dec dec risc microprocessor preliminary data sheet 
technical report 
dek todd daniel hugh james robin stewart 
design dec axp systems high performance workstations 
digital technical journal 
dix 
new cpu benchmark suites spec 
proc 
compcon spring 
dan mhz dual issue cmos microprocessor 
international solid state circuits conference digest technical papers pages feb 
fis josh fisher 
trace scheduling technique global microcode compaction 
ieee transactions computer july 
gh aaron goldberg john hennessy 
integrated system performance debugging shared memory multiprocessor applications 
ieee transactions parallel distributed systems pages january 
gallivan jalby meier sameh 
impact hierarchical memory systems linear algebra algorithm design 
technical report university 
gl golub van loan 
matrix computations 
johns hopkins university press 

fills powerpc product line 
microprocessor report oct 
hp john hennessy david patterson 
computer architecture quantitative approach 
morgan kaufmann 
hp hp 
pa risc architecture instruction set manual 
hewlett packard 
jou norm jouppi 
improving direct mapped cache performance addition small cache prefetch buffers 
proc 
th annual int 
symposium computer architecture pages may 
kh kessler mark hill 
page placement algorithms large real index caches 
acm tocs 
koh jeff dave mark heinrich stanford flash multiprocessor 
proc 
st annual int 
symposium computer architecture pages april 
kro david 
lockup free instruction fetch prefetch cache organization 
proceedings th symposium computer architecture pages 
lau james laudon 
architectural implementation tradeoffs multiple context processors 
phd thesis stanford university 
le richard jr carla ellis 
experimental comparison memory management policies numa multiprocessors 
acm transactions computer systems november 
james laudon anoop gupta mark horowitz 
interleaving multithreading technique targeting multiprocessors workstations 
sixth int 
conference architectural support programming languages operating systems asplos pages october 
lam rothberg wolf 
cache performance optimizations blocked algorithms 
proceedings fourth international conference architectural support programming languages operating systems pages april 
lw alvin lebeck david wood 
cache profiling spec benchmarks case study 
ieee computer october 
mar margaret martonosi 
analyzing tuning memory performance sequential parallel programs 
phd thesis stanford university december 
mc coffman 
organization matrices matrix operations paged multiprogramming environment 
cacm 
mowry lam gupta 
design evaluation compiler algorithm prefetching 
proceedings fifth international conference architectural support programming languages operating systems volume pages october 
mow mowry 
tolerating latency software controlled data prefetching 
phd thesis stanford university march 
pau richard paul 
sparc architecture assembly language programming prentice hall 
por porterfield 
software methods improvement cache performance supercomputer applications 
phd thesis department computer science rice university may 
reinhardt hill larus wisconsin wind tunnel virtual prototyping parallel computers 
proc 
acm sigmetrics conference measurement modeling computer systems 
pages may 
swg singh 
weber gupta 
splash stanford parallel applications shared memory 
technical report csl tr stanford university april 
smi michael smith 
tracing pixie 
technical report csl tr stanford university november 
smi smith 
support speculative execution high performance processors 
phd thesis stanford university november 
tjiang wolf lam integrating scalar optimizations parallelization 
springerverlag pp 
august 
wl wolf lam 
data locality optimizing algorithm 
proceedings sigplan conference programming language design implementation pages june 
