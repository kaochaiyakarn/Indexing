bundling heterogeneous classi ers advisor perceptrons stephen lee division statistics university idaho moscow id steve laplace edu john elder iv elder research place charlottesville va elder com october suppose multiple advisors trained di erent specialty consult decision 
decide categorically call say hedging allowed require advisors 
group faced sim ilar questions recorded advice subsequent outcome case 
trust advisor working need examine nite set advisor weight vectors discover optimal historically 
new robust way multiple trained classi ers ad perceptrons elder demonstrated datasets heterogeneous classi ers believe combine fruitfully homogeneous ones 
compared common alternative combination techniques shown improve generalization accuracy 
keywords classi er combination ensemble methods bagging boosting bumping stacked generalization decision tree neural network projection pursuit regression learn ing vector quantization logistic regression decade modeling algorithms witnessed great creativity elds researchers extended successfully applied powerful nonlinear techniques decision trees neural networks adaptive splines polynomial networks forecasting classi cation diagnosis challenges ripley cheng titterington cherkassky friedman wechsler elder pregibon ripley 
number distinct techniques employ inductively create classi cation model smaller rst appears methods slight deviations 
instance case reasoning form nearest neighbor modeling polynomial neural networks technique 
reducing apparent diversity require communicating side specialty discipline 
plethora algorithms natural question ask works best 
study described michie spiegelhalter taylor reviewed elder 
researchers ensemble models accurate new data best single model jacobs wolpert perrone cooper hashem schmeiser yih 
process bundling models thought approximation theoretical bayesian approach tting possible models combining prior weights 
bundling requires stages generating su ciently diverse models training data combining outputs 
researchers generated multiple classi ers family xu jordan romero tumer ghosh skalak het classi ers elder 
generating homogeneous classi ers diversity come training models di erent guiding parameters varied initial neural network weights battiti hansen salamon di erent model structure di erent stopping pruning criteria iterative changes case weights boosting fre und schapire bootstrapping training data noise raviv intrator noise bagging breiman 
believe greatest ful diversity come employing diverse individually accurate methods neural network machine learning statistics communities 
combination common approaches averaging weighted averaging voting 
method bundling bumping tibshirani knight doesn combine ignoring best bootstrapped alternative models 
formally introduce demonstrate utility new method combination advisor perceptrons aps 
type weighted voting model possible weight sets exhaustively enumerated straightforward manner 
invented elder rst applied combining heterogeneous techniques lee 
strengthens extends preliminary results 
advisor perceptrons formalizing analogy suppose outputs classi ers yc want combine form integrated output aps rst discretize yc binary input unbiased perceptron weighted sum threshold weights fw 
consistent assuming classi er misleading constrain weights non negative worst ignore classi er advice 
discrete inputs outputs nitely vectors form identical functions 
example binary ap inputs weight set behaves 
weight space partitioned regions possible aps nite number represent region single point say closest origin 
aps enumerated examined training data nd best combination 
enumerate aps dimensions solve linear program minimizing sum weights wj subject constraints 
simplicity may add restriction weights nonincreasing 
generates ap templates may permuted form possible combinations 
new templates inputs zero weights may generated lower dimensional new template permutations 
sets new templates fg 
permute padded enumerate weight space 
example weight sets input perceptron templates generate aps 
perceptron corresponds majority voting ignore advice inputs follow third 
interestingly 
working early chip layout problem happened similar aps showed weight sets involving fewer inputs aps unique employ integer values 
functions inputs fractional optimum weights example multiple solutions 
new classes weight sets start inputs con rmed providing may read example curse dimensionality low dimensional behavior extrapolate cleanly higher dimensions 
aps remain nite number grow rapidly dimension shown table reasonable subset explore functions possible consider 
binary logic table number distinct perceptrons perceptrons new templates integrate di erent classi ers leading advisor perceptrons clearly exhaustive enumeration reliable searching space voting weights faster 
classi cation algorithms brie outline algorithms employed implemented plus ver sion chambers hastie ripley 
logistic regression decision trees projection pursuit regression standard plus functions 
neural net learning vector quantization classi ers come brian ripley nnet classif libraries statlib archive 
xji represent th attribute pattern xi true class yi 
yi jxi denote probability xi 
yi jxi denote estimated probability xi 
logistic regression lr model px logit yi logit log parameters estimated maximum likelihood myers 
yi jxi pattern xi classi ed class class 
decision tree tree tree algorithms recursively partitions feature space locally constant regions usually hypercubes parallel feature axes 
fully grown tree rst tted training data pruned back subtree lesser number terminal nodes avoid overtraining pruning stopping introduced cart breiman help avoid getting trapped type local minima 
modi ed plus algorithm behave bit cart fold cross validation cv performance training data determine cut point 
projection pursuit regression ppr statistical procedure proposed friedman stuetzle scores low dimensional projections high dimensional data scatterplot smoother estimate output remaining previous smoothes removed 
model form logit yi kx px sets parameters projection directions unknown smooth activation functions projection weights estimated sequential manner back tting neural network nn supervised feedforward single hidden layer neural network yi jxi xed monotonic di erentiable functions 
networks wh general shown authors theoretically capable approximating px continuous function input variables su ciently large numbers hidden units projection pursuit regression nn training algorithms estimate weights simultaneously randomized gradient search procedure relatively slow 
learning vector quantization lvq method originated self organizing map traditionally labeled neural network method di erent supervised feedforward multi layer perceptron 
basic idea replace example cases representative set codebook vectors 
classi er points assigned classes randomly lvq algorithm adjusts location feedback manner converge stable state 
classi cation rule new case class nearest codebook vector moving implicitly adjusts class boundaries 
lr datasets experiments described large numbers uninformative variables inputs 
methods decisions required degrees freedom model complexity employ 
cv tests performed trees decide neural networks suggest number hidden units projection pursuit regression models indicate best number smoothes complexity considered roughly equivalent cv tests indicative fool proof larger numbers values experiments 
number determine number lvq codebook vectors cross validation cumbersome method 
output vectors classi ers serve inputs ap integration 
lvq provides binary output directly produce real valued outputs discretized binary values 
rst loss reduce precision probability information estimates manner forcing advisor methods binary decision vote add robustness situations 
case alternative methods combination tracked 
data sets data sets employed uci repository generated described 
brief overview hypothyroid full data set qualitative quantitative input variables missing values 
just considered quantitative variables denoted tt removed cases remaining missing values 
leaves cases class negative class positive 
waveform simulated data set consists cases types waves denoted having probability described breiman 

subroutine generating data uci repository 
binary grouped waves groups form class leaving waves group class 
credit data german bank tracking applicants credit 
numeric variables ignore qualitative variables 
total cases customers class bad customers class 
diabetes data gathered pima indians national institute diabetes kidney diseases 
consists cases input variables 
input variables medical measurements pregnancy information patient 
response cases tested positive diabetes class cases tested negative class 
investment data generated rule base adapted eld shown table addresses issues involved investment advising 
data generating system consists input variables shown bold table binary response variable invest stocks represents advice put investing stocks represents advice invest stocks 
generated cases approximately equal class 
table investment rule base saving adequate income adequate invest stocks dependent saving adequate saving adequate assets high saving adequate dependent income adequate income adequate debt low income adequate saving dependents dependent saving adequate income dependents dependent income adequate assets income assets high annual debt income debt low note exact correspondence rule base decision tree model induced sample data 
diagonal conditions di cult axes parallel tree capture structure rst thought 
normal dataset generated dimensional normal distributions skewed respect overlapping 
mean vectors covariance matrices equal identity diagonal zero diagonal correlations rst correlations second 
half cases distribution class training evaluation 
quadratic discriminant analysis assumptions structure exactly match problem parameters estimated got error training data evaluation data 
experiments nite datasets rst split randomly training remaining evaluation 
key user de ned model parameter settings number terminal nodes pruned tree number projections ppr number hidden units nn recorded fold cross validation training data set 
data training sub model tested remaining results accumulated runs case turn sample case 
tested values ranging winner shown rst rows table 
largest nn ppr parameter counts methods nn ppr lvq described 
second phase nd optimal advisor perceptron ap sample predictions training data round cross validation cv 
cv produces sub models method lead vector size original dataset sample estimates 
su cient lead single ap may tie performance cv process repeated best ap stabilized 
third phase full training set di erent classi ers user de ned model parameter settings phase 
lastly evaluation set employed test system individual trained classi ers ap alternatives voting averaging ap weights voting 
resulting misclassi cation rates methods combining techniques shown table 
discussion single best technique tests neural network winning competitors times 
algorithm best nearly dataset 
hard tell problem description training performance algorithm leader 
similarly various combining methods take turns best 
ap beats simple averaging probably common combination approach times best single method nn proportion 
get clearer picture relative merits individual methods combin ing techniques data set scale range observed misclassi cation rates just smaller lowest misclassi cation rates just larger highest mis classi cation rates unit interval 
method technique relative error rate close data sets 
plot relative error rates versus data sets individual methods averaging voting aps ap weights voting 
clear side side plots aps achieve best performance ap weights voting second place 
aps stable performance individual method reliable problem obvious solution structure 
preliminary results suggest advisor perceptrons candidate combination technique explore considering useful strategy bundling models 
battiti 

democracy neural nets voting schemes classi cation neural networks vol 
pp 

breiman 

bagging predictors machine learning vol 
pp 

breiman friedman olshen stone 

classi cation regression trees 
monterey california wadsworth brooks 
chambers hastie 

statistical models paci grove california wadsworth brooks 
cheng titterington 

neural networks review statistical perspective statistical science vol 
pp 

cherkassky friedman wechsler 

statistics neural network theory pattern recognition applications 
springer 
elder 

optimal discrete perceptrons graded learning international systems man cybernetics conf chicago illinois october 
elder 

heuristic search model structure bene ts restraining greed chapter learning data arti cial intelligence statistics lecture notes statistics eds 
fisher 
lenz springer verlag new york 
elder 

review machine learning neural statistical classi cation eds 
michie spiegelhalter taylor ellis horwood american statistical association 
elder pregibon 

statistical perspective knowledge discovery databases ch 
advances knowledge discovery data mining eds fayyad piatetsky shapiro smith uthurusamy 
aaai mit press 
freund schapire 

experiments new boosting algorithm machine learning proceedings th international conference july 
friedman stuetzle 

projection pursuit regression journal amer statistical association vol 
pp 

hansen salamon 

neural networks ensembles ieee transactions pattern analysis machine intelligence vol 
pp 

hashem schmeiser yih 

optimal linear combinations neural networks overview proceedings ieee international conference neural networks vol 
pp 



applied logistic regression 
new york john wiley sons 
jacobs nowlan hinton 

adaptive mixtures local experts neural computation vol 
pp 

kohonen 

self organizing maps 
springer verlag heidelberg 
lee 

combining neural statistical classi ers perceptron working notes thirteenth national conference onarti cial intelligence workshop integrating multiple learned models improving scaling machine learning algorithms august 
eld 

arti cial intelligence design expert systems 
benjamin cummings 
michie spiegelhalter taylor 

machine learning neural statistical classi cation 
new york ellis horwood 


enumeration threshold functions variables ieee computers september 
myers 

classical modern regression applications 
boston pws kent 
perrone cooper 

learning learned super learning multi neural networks systems proceedings world congress neural networks vol iii pp 

quinlan 

program machine learning 
san mateo morgan kaufmann 
raviv intrator 

bootstrapping noise ective regularization technique technical report tel aviv university 
ripley 

statistical aspects neural networks networks chaos statis tical probabilistic aspects eds nielsen jensen kendall 
pp 

london chapman hall 
ripley 

neural networks related methods classi cation discus sion journal royal statistical society vol 
pp 

ripley 

pattern recognition neural networks 
cambridge university press 
romero 

comparison symbolic connectionist ap proaches local experts integration proceedings ieee technical application confer ence 
pp 
portland 
skalak 

prototype selection composite nearest neighbor classi ers 
ph dissertation dept computer science univ amherst 
tibshirani knight 

model searching inference bootstrap bumping univ toronto dept statistics tech 
rpt nov tumer ghosh 

theoretical foundations linear order statistics combiners neural pattern classi ers unpublished manuscript university texas austin 
ripley 

modern applied statistics plus 
new york springer 
wolpert 

stacked generalization neural networks vol pp 

xu jordan 

em learning generalized finite mixture model combining multiple classi ers proceedings world congress neural networks port land vol iv pp 

table misclassi cation rates evaluation training data data sets hypothyroid waveform credit diabetes investment normal evaluation data nodes proj 
hidden units lr tree ppr nn lvq avg vote ap avg ap vote ap training data lr tree ppr nn lvq avg vote ap avg ap vote neural network logistic regression linear vector quantization projection pursuit regression decision tree diabetes gaussian hypothyroid german credit waveform investment advisor perceptron ap weighted average diabetes gaussian hypothyroid german credit waveform investment vote average model performance data sets individual models bundles 
