bayesian grammar induction language modeling stanley chen tr january center research computing technology harvard university cambridge massachusetts bayesian grammar induction language modeling stanley chen aiken computation laboratory division applied sciences harvard university cambridge ma sfc das harvard edu describe corpus induction algorithm probabilistic context free grammars 
algorithm employs greedy heuristic search bayesian framework post pass inside outside algorithm 
compare performance algorithm gram models inside outside algorithm language modeling tasks 
domains algorithm outperforms techniques marking time grammar language model surpassed gram modeling task moderate size 
applications speech recognition handwriting recognition spelling correction performance limited quality language model utilized bahl baker kernighan srihari 
static language modeling performance remained basically unchanged advent gram language models years ago shannon 
gram language models capture dependencies word window currently largest practical natural language dependencies natural language occur window 
addition gram models extremely large making difficult implement efficiently memory constrained applications 
appealing alternative grammar language models 
language models expressed probabilistic grammar tend compact gram language models ability model long distance dependencies lari young resnik schabes 
date little success constructing grammar language models competitive gram models problems magnitude 
describe corpus induction algorithm probabilistic context free grammars outperforms gram models inside outside algorithm baker medium sized domains 
result marks time grammar language model surpassed gram modeling task moderate size 
algorithm employs greedy heuristic search bayesian framework post pass inside outside algorithm 
grammar induction search grammar induction framed search problem framed exception past research angluin smith 
search space taken class grammars example search space probabilistic contextfree grammars 
objective function taken measure dependent training data generally wants find grammar sense accurately models training data 
language modeling including gram models inside outside algorithm falls maximum likelihood paradigm takes objective function likelihood training data grammar 
optimal grammar objective function generates strings training data strings 
grammars poor language models overfit training data model language large 
gram models inside outside algorithm issue bounding size form grammars considered optimal grammar expressed 
wish limit size grammars considered 
basic problem maximumlikelihood objective function encompass compelling intuition occam razor simpler smaller grammars preferable complex larger grammars 
factor objective function favors smaller grammars large prevent objective function preferring grammars overfit training data 
solomonoff presents bayesian grammar induction framework includes factor motivated manner 
goal grammar induction taken finding grammar largest posteriori probability training data finding grammar arg max denote training data observations 
unclear estimate directly apply bayes rule get arg max arg max frame search search objective function likelihood training data multiplied prior probability grammar 
satisfy goal favoring smaller grammars choosing prior assigns higher probabilities grammars 
particular solomonoff proposes universal priori probability solomonoff closely related minimum description length principle proposed rissanen 
case grammatical language modeling corresponds gammal length description grammar bits 
universal priori probability elegant properties salient dominates enumerable probability distributions multiplicatively 
search algorithm described take grammar induction search grammar optimizes objective function 
thorough discussion universal priori probability li vit anyi 
framework restrict particular grammar formalism consider probabilistic context free grammars 
assume simple greedy search strategy 
maintain single hypothesis grammar initialized small trivial grammar 
try find modification hypothesis grammar addition grammar rule results grammar higher score objective function 
find superior grammar new hypothesis grammar 
repeat process longer find modification improves current hypothesis grammar 
initial grammar choose grammar generate string assure grammar cover training data 
initial grammar listed table 
sentential symbol expands sequence expands nonterminal symbol grammar 
initially set nonterminal symbols consists different nonterminal symbol expanding terminal symbol 
notice sentence sequence independently generated nonterminal symbols 
maintain property search process symbol add grammar add rule assures sentential symbol expand symbol adding symbol affect probabilities grammar assigns strings 
term move set describe set modifications consider current hypothesis grammar hopefully produce superior grammar 
move set includes moves move create rule form bc move create rule form bjc context free grammar possible express equivalent grammar rules forms 
mentioned new symbol create rule evaluating objective function consider task calculating objective function grammar calculating gammal inexpensive calculating requires parsing entire training data 
afford parse training sx gamma ffl ffl gamma fs xg set nonterminal symbols set terminal symbols probabilities rule parentheses 
table initial hypothesis grammar data grammar considered practical data sets millions words afford parse data 
achieve goal employ approximations 
notice need calculate absolute value objective function need able distinguish move applied current hypothesis grammar produces grammar higher score objective function need able calculate difference objective function resulting move 
done efficiently quickly approximate probability training data changes move applied 
possible approximate probability training data probability single probable parse viterbi parse training data 
furthermore recalculating viterbi parse training data scratch move applied heuristics predict move change viterbi parse 
example consider case training data consists sentences talks slowly mary talks display viterbi parse data initial hypothesis grammar algorithm 
consider move adding rule talks slowly initial grammar concomitant rule 
reasonable heuristic predicting viterbi parse change replace adjacent expand talks slowly respectively single expands displayed 
actual heuristic moves form bc analogous heuristics move move set 
predicting differences viterbi parse resulting move quickly estimate change probability training data 
notice predicted viterbi parse stray great deal actual viterbi parse errors accumulate move move applied 
minimize effects process training data incrementally 
brief delay parsing sentence find optimal grammar previous sentences training data 
yield accurate viterbi parses simply parse corpus initial hypothesis grammar parse sentence 
parameter training section describe parameters grammar probability associated grammar rule set 
ideally evaluating objective function grammar optimal parameter settings training data full score grammar achieve 
searching optimal parameter values extremely expensive computationally 
grossly approximate optimal values deterministically setting parameters viterbi parse training data parsed far 
rely post pass described refine parameter values 
referring rules table parameter ffl set arbitrary small constant 
values parameters set smoothed frequency reduction viterbi parse data seen far 
remaining symbols set expand uniformly possible expansions 
phi phi phi phi phi phi phi mary mary talks talks slowly slowly phi phi phi phi phi phi bob bob talks talks slowly slowly initial viterbi parse phi phi phi phi mary mary phi phi phi talks talks slowly slowly phi phi phi bob bob phi phi phi talks talks slowly slowly predicted viterbi parse constraining moves consider move creating rule form bc 
corresponds different specific rules created current number symbols grammar 
computationally expensive consider rules point search heuristics constrain moves 
left hand side rule create new symbol 
heuristic selects optimal choice vast majority time constraint moves described earlier section yield arbitrary context free languages 
partially address add move move create rule form iteration move construct grammars generate arbitrary regular languages 
implemented moves enable construction arbitrary context free grammars belongs 
constrain symbols consider right hand side new rule call triggers 
trigger phenomenon viterbi parse sentence indicative particular move lead better grammar 
example fact symbols talks slowly occur indicative profitable create rule talks slowly developed set triggers move move set consider specific move triggered sentence currently parsed incremental processing 
post pass conspicuous shortcoming search framework grammars search space fairly 
firstly recall grammars model sentence sequence independently generated symbols language large dependence adjacent constituents 
furthermore free parameters search parameters symbols fixed expand uniformly 
choices necessary search tractable 
confused term triggers dynamic language modeling 
address issue inside outside algorithm post pass 
methodology derived described lari young 
create new nonterminal symbols fx xng create rules form ng ng old gamma fs xg old denotes set nonterminal symbols acquired initial grammar induction phase taken new sentential symbol 
new rules replace rules listed table 
parameters rules initialized randomly 
grammar starting point run inside outside algorithm training data convergence 
words naive rule attach symbols parsing data rules depend inside outside algorithm train randomly initialized rules intelligently 
post pass allows express dependencies adjacent symbols 
addition allows train parameters fixed initial grammar induction phase 
previous mentioned employs bayesian grammar induction framework described solomonoff 
solomonoff specify concrete search algorithm suggestions nature 
similar research includes cook 
stolcke omohundro 
employs heuristic search bayesian framework 
different prior probability grammars algorithms efficient applied small data sets 
grammar induction algorithms successful language modeling include algorithm lari young lari young pereira schabes special case expectation maximization algorithm dempster glass 
uses heuristic search procedure similar different search criteria 
knowledge algorithm surpassed performance gram models language modeling task substantial scale 
results evaluate algorithm compare performance algorithm gram models inside outside algorithm 
gram models tried domain 
smoothing particular gram model took linear combination lower order gram models 
particular follow standard practice jelinek mercer bahl brown take smoothed gram probability linear combination gram frequency training data smoothed gamma gram probability jw gamma delta delta delta gamma ww gamma jw gamma delta delta delta gamma denotes count word sequence training data 
smoothing parameters trained forwardbackward algorithm baum held data 
parameters tied similar prevent data sparsity 
inside outside algorithm follow methodology described lari young 
create probabilistic contextfree grammar consisting chomsky normal form rules nonterminal symbols fx xn terminal symbols rules ng ng denotes set terminal symbols domain 
parameters initialized randomly 
starting point inside outside algorithm run convergence 
smoothing combine expansion distribution symbol uniform distribution take smoothed parameter ff ff gamma pu ff pu ff denotes unsmoothed parameter 
value number different ways symbol expands lari young methodology 
parameter trained inside outside algorithm held data 
smoothing performed inside outside post pass algorithm 
domain tried 
computational demands algorithm currently impractical apply large vocabulary large training set problems 
results algorithm medium sized domains 
case sentences training sentences held smoothing 
test sentences measure performance entropy test data 
domains created training test data artificially ideal grammar hand benchmark results 
particular probabilistic grammar generate data 
domain created grammar hand grammar small english probabilistic context free grammar consisting roughly nonterminal symbols terminal symbols rules 
second domain derived grammar manually parsed text 
words parsed wall street journal data penn treebank extracted frequently occurring symbols frequently occurring rules expanding symbols 
symbol occurs right hand side rule frequent symbols create rule expands symbol unique terminal symbol 
removing unreachable rules yields grammar roughly nonterminals terminals rules 
parameters set reflect frequency corresponding rule parsed corpus 
third domain took english text reduced size vocabulary mapping word part speech tag 
tagged wall street journal text penn treebank tag set size 
tables summarize results 
ideal grammar denotes grammar generate training test data 
algorithm list best performance achieved tried best column states value realized performance 
achieve moderate significant improvement performance gram models inside outside algorithm domains part speech domain outperformed gram models vastly outperform inside outside algorithm 
table display sample number parameters execution time decstation associated algorithm 
choose yield approximately equivalent performance algorithm 
pass row refers main grammar induction phase algorithm post pass row refers post pass 
notice algorithm produces significantly compact model gram model running significantly faster inside outside algorithm inside outside post pass 
part discrepancy due fact require smaller number new nonterminal symbols achieve equivalent performance post pass converges quickly number nonterminal symbols 
discussion research represents step forward quest developing grammar language models natural language 
induce models substantially compact outperform gram language models medium sized domains 
algorithm runs essentially time space linear size training data larger domains reach 
feel largest contribution lie actual algorithm specified indication potential induction framework described solomonoff 
implemented subset moves developed inspection results gives reason believe additional moves may significantly improve performance algorithm 
solomonoff induction framework restricted probabilistic context free grammars 
completing implementation move set plan explore modeling contextsensitive phenomena 
demonstrates solomonoff elegant framework deserves consideration 
indebted stuart shieber suggestions guidance invaluable comments earlier drafts 
best entropy 
relative bits word gram ideal grammar gamma algorithm gamma gram model inside outside table english artificial grammar best entropy 
relative bits word gram ideal grammar gamma algorithm gamma gram model inside outside table wall street journal artificial grammar best entropy 
relative bits word gram gram model algorithm inside outside table english sentence part speech sequences wsj entropy 
time artif 
bits word params sec gram io pass post pass table parameters training time material supported national science foundation number iri stuart shieber 
angluin smith 

inductive inference theory methods 
acm computing surveys 
bahl baker cohen jelinek lewis mercer 

recognition continuously read natural corpus 
proceedings ieee international conference acoustics speech signal processing pages tulsa oklahoma april 
bahl frederick jelinek robert mercer 

maximum likelihood approach continuous speech recognition 
ieee transactions pattern analysis machine intelligence pami march 
baker 

dragon system overview 
ieee transactions acoustics speech signal processing february 
baker 

trainable grammars speech recognition 
proceedings spring conference acoustical society america pages boston ma june 
baum 

inequality application statistical estimation probabilistic functions markov processes model ecology 
bulletin american mathematicians society 
peter brown vincent dellapietra peter desouza jennifer lai robert mercer 

class gram models natural language 
computational linguistics december 
craig cook rosenfeld alan aronson 

grammatical inference hill climbing 
information sciences 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
frederick jelinek robert mercer 

interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice amsterdam netherlands northholland may kernighan church gale 

spelling correction program noisy channel model 
proceedings thirteenth international conference computational linguistics pages 
lari young 

estimation stochastic context free grammars inside outside algorithm 
computer speech language 
lari young 

applications stochastic context free grammars inside outside algorithm 
computer speech language 
ming li paul vit anyi 

kolmogorov complexity applications 
springer verlag 
michael james glass 

empirical acquisition word phrase classes atis domain 
third european conference speech communication technology berlin germany september 
fernando pereira yves schabes 

reestimation partially bracket corpora 
proceedings th annual meeting acl pages newark delaware 
resnik 

probabilistic tree adjoining grammar framework statistical natural language processing 
proceedings th international conference computational linguistics 
rissanen 

modeling shortest data description 
automatica 
schabes 

stochastic lexicalized grammars 
proceedings th international conference computational linguistics 
shannon 

prediction entropy printed english 
bell systems technical journal january 
solomonoff 

preliminary report general theory inductive inference 
technical report cambridge ma november 
solomonoff 

formal theory inductive inference 
information control march june 
rohini srihari charlotte 

combining statistical syntactic methods recognizing handwritten sentences 
aaai symposium probabilistic approaches natural language pages 
andreas stolcke stephen omohundro 

best model merging hidden markov model induction 
technical report tr international computer science institute berkeley ca 
