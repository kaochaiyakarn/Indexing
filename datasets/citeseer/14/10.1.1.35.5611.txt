treadmarks shared memory computing networks workstations alan cox dwarkadas pete keleher lu ramakrishnan yu willy zwaenepoel department computer science rice university treadmarks supports parallel computing networks workstations providing application shared memory abstraction 
shared memory facilitates transition sequential parallel programs 
identifying possible sources parallelism code data structures retained change synchronization needs added achieve correct shared memory parallel program 
additional transformations may necessary optimize performance done incremental fashion 
discuss techniques treadmarks provide efficient shared memory experience large applications mixed integer programming genetic linkage analysis 
high speed networks rapidly improving microprocessor performance networks workstations increasingly appealing vehicle parallel computing 
relying solely commodity hardware software networks workstations offer parallel processing relatively low cost 
network workstations multiprocessor may realized processor bank number processors dedicated purpose providing computing cycles 
alternatively may consist dynamically varying set machines idle cycles perform long running computations 
case hardware cost essentially zero organizations extensive workstation networks place 
terms performance improvements processor speed network bandwidth latency allow networked workstations deliver performance approaching exceeding supercomputer performance increasing class applications 
means position loosely coupled multiprocessors render obsolete tightly coupled designs 
particular lower latencies higher bandwidths tightly coupled designs allow efficient execution applications stringent synchronization communication requirements 
argue advances networking technology research supported part national science foundation ccr ccr cda cda texas advanced technology program tech sym 
processor performance greatly expand class applications executed efficiently network workstations 
discuss experience parallel computing networks workstations treadmarks distributed shared memory dsm system 
dsm allows processes assume globally shared virtual memory execute nodes physically share memory :10.1.1.14.3607
illustrates dsm system consisting networked workstations memory connected network 
dsm software provides abstraction globally shared memory processor access data item programmer having worry data obtain value 
contrast native programming model networks workstations message passing programmer decide processor needs communicate communicate data send 
programs complex data structures sophisticated parallelization strategies daunting task 
dsm system programmer focus algorithmic development managing partitioned data sets communicating values 
addition ease programming dsm provides programming environment hardware sharedmemory multiprocessors allowing programs written dsm ported easily sharedmemory multiprocessor 
porting program hardware shared memory multiprocessor dsm system may require modifications program higher latencies dsm system put greater value locality memory access 
programming interfaces dsm systems may differ variety respects 
focus memory structure memory consistency model 
unstructured memory appears linear array bytes structured memory processes access memory terms objects tuples 
memory model refers updates shared memory reflected processes system 
intuitive model shared memory read return value written 
unfortunately notion value written defined distributed system 
precise notion sequential consistency memory appears processes executing single multiprogrammed processor 
sequential consistency notion value written precisely defined 
simplicity model may exact high price terms performance research done relaxed memory models 
relaxed memory model necessarily return read value written 
proc mem proc mem proc mem network shared memory proc distributed shared memory processor sees shared address space denoted dashed outline collection distributed address spaces 
terms implementation techniques primary distinguishing features dsm system uses virtual memory page protection hardware detect accesses shared memory 
naive virtual memory protection hardware may lead poor performance discrepancies page size machine granularity sharing application 
system discussed treadmarks provides shared memory linear array bytes 
memory model relaxed memory model release consistency 
implementation uses virtual memory hardware detect accesses uses multiple writer protocol alleviate problems resulting mismatch page size application granularity sharing 
treadmarks runs user level unix workstations 
kernel modifications special privileges required standard unix interfaces compilers linkers 
result system fairly portable 
particular ported number platforms including ibm rs ibm sp ibm sp dec alpha dec decstation hp sgi sun sparc 
describes application programming interface provided treadmarks section 
discuss performance problems observed dsm systems conventional sequential consistency section techniques address problems treadmarks sections 
briefly describe implementation treadmarks section basic operation costs experimental environment section 
demonstrate treadmarks efficiency discussing experience large applications mixed integer programming genetic linkage analysis section 
discuss related section offer directions section 
shared memory programming application programming interface treadmarks api simple powerful see language interface 
provides facilities process creation destruction synchronization shared memory allocation 
shared memory allocation done tmk malloc 
memory allocated tmk malloc shared 
memory allocated statically call malloc private process 
focus primitives synchronization 
synchronization way programmer express ordering constraints shared memory accesses different processes 
simple form synchronization occurs critical sections 
critical section guarantees process time executes inside critical section 
construct useful multiple processes updating data structure concurrent access allowed 
formally shared memory accesses conflicting issued different processors memory location write 
parallel program data race synchronization conflicting accesses 
example accesses read write synchronization read write may read write execute different outcomes execution 
data races bugs program final outcome execution timing dependent 
course guaranteed program data races produces results programmer intended 
data races avoided introducing synchronization 
treadmarks provides synchronization primitives barriers exclusive locks 
process waits barrier calling tmk barrier 
barriers global calling process stalled processes system arrived barrier 
tmk lock acquire call acquires maximum number parallel processes supported treadmarks define actual number parallel processes particular execution extern unsigned process id integer range 
extern unsigned number lock synchronization objects provided treadmarks define number barrier synchronization objects provided treadmarks define initialize treadmarks start remote processes void int argc char argv terminate calling process 
processes unaffected 
void int status block calling process process arrives barrier 
void unsigned id block calling process acquires specified lock 
void unsigned id release specified lock 
void unsigned id allocate specified number bytes shared memory char unsigned size free shared memory allocated 
void char ptr treadmarks interface lock calling process tmk lock release releases 
process acquire lock process holding 
particular choice synchronization primitives way fundamental design treadmarks primitives may added 
demonstrate treadmarks primitives simple applications 
simple illustrations figures illustrate treadmarks api jacobi iteration solving traveling salesman problem tsp 
jacobi illustrates barriers tsp provides example locks 
aware overly simplistic nature example codes 
included demonstration purposes 
larger applications discussed section 
jacobi method solving partial differential equations 
example iterates twodimensional array 
iteration matrix element updated average nearest neighbors left right 
jacobi uses scratch array store new values computed iteration avoid overwriting old value element neighbor 
parallel version processors assigned roughly equal size bands rows 
rows boundary band shared neighboring processes 
treadmarks version uses arrays grid scratch array 
grid allocated shared memory scratch private process 
grid allocated initialized process 
synchronization jacobi done means barriers 
tmk barrier guarantees initialization process visible processes start computing 
tmk barrier sure processor overwrites value grid processors read value computed previous iteration 
terminology introduced section avoids data race reads grid array nested loop writes second nested loop 
tmk barrier prevents processor starting iteration grid values computed current iteration installed 
words avoids data race writes grid second nested loop reads grid nested loop iteration 
traveling salesman problem tsp finds shortest path starts designated city passes city map exactly returns original city 
simple branch bound algorithm 
program maintains length shortest tour far shortest length 
partial tours expanded city time 
current length partial tour plus lower bound remaining portion path longer current shortest tour partial tour explored lead shorter tour current minimum length tour 
lower bound computed fast conservative approximation length minimum spanning tree connecting nodes tour nodes current tour 
sequential tsp program keeps queue partial tours promising head 
promise determined sum length current tour lower bound length connect remaining cities 
program keeps adding partial tours queue path longer threshold number cities head queue 
removes path queue tries permutations remaining cities 
compares shortest tour including path current shortest tour updates current shortest tour necessary 
program goes back tour queue tries remove long promising tour queue 
shows pseudo code parallel treadmarks tsp program 
shared data structures queue minimum length allocated process 
exclusive access define define float grid shared array float scratch private array main grid sizeof float initialize grid length length length number iterations scratch grid grid grid grid grid scratch pseudo code treadmarks jacobi program shared data structures achieved surrounding accesses lock acquire lock release 
processes wait tmk barrier sure shared data structures properly initialized computation starts 
process acquires queue lock find promising partial tour long expanded sequentially 
tour process releases queue lock 
expanding current partial tour acquires lock minimum length updates minimum length necessary releases lock 
starts iteration loop acquiring queue lock finding promising tour queue empty 
implementation challenges dsm systems migrate replicate data provide abstraction shared memory 
dsm systems choose replicate data approach gives best performance wide range application parameters interest 
replicated data provision memory consistency heart dsm system dsm software control replication manner provides abstraction single shared memory 
consistency model defines programmer expect memory system behave 
dsm system ivy implemented sequential consistency :10.1.1.14.3607
memory model processes observe shared memory executing multiprogrammed uniprocessor single memory 
words total order memory accesses total order compatible program order memory accesses individual process 
ivy implementation sequential consistency virtual memory hardware maintain memory consistency 
local physical memories processor form cache global virtual address space see 
page local memory processor page fault occurs 
dsm software brings date copy page remote location local memory restarts process 
example shows activity occurring result page fault processor results copy necessary page retrieved local memory processor 
ivy furthermore distinguishes read faults write faults 
read faults page replicated read access replicas 
write faults invalidate message sent processors copies page 
processor receiving message invalidates copy page sends message writer 
result writer copy page sole copy 
simplicity intuitive appeal sequential consistency generally viewed natural consistency model 
implementation cause large amount communication occur 
communication expensive workstation network 
sending message may involve traps operating system kernel interrupts context switches execution possibly layers networking software 
number messages amount data exchanged kept low 
illustrate communication problems ivy examples section 
consider example updates length current shortest tour tsp 
ivy shared memory update causes invalidations sent processors cache page containing variable 
variable accessed critical section protected corresponding lock suffices send invalidations processor acquiring lock time lock acquisition 
second problem relates potential false sharing 
false sharing occurs unrelated data objects located page written concurrently separate processors 
consistency units large virtual memory pages false sharing queue int int main queue sizeof sizeof int initialize heap true queue empty keep adding queue long promising tour appears head path delete tour head length recursively try cities path find shortest tour length length length pseudo code treadmarks tsp program page fault local physical memories global virtual memory dsm software proc proc proc proc operation ivy dsm system potentially serious problem 
demonstrates possible page layout grid array jacobi 
processors update portion grid array writing concurrently page 
assume initially processor holds sole writable copy page 
processor writes page sends invalidate message processor sends page invalidates copy 
writes page sequence events occur interchanged 
process writes page held process page travel network 
repeated back forth page referred ping pong effect 
order address problems experimented novel implementations relaxed memory consistency models designed protocols combat false sharing problem 
approaches discussed 
lazy release consistency release consistency model intuition underlying release consistency follows 
parallel programs data races lead nondeterministic results 
sufficient synchronization prevent data races 
specifically synchronization conflicting accesses shared memory 
synchronization need reflect shared memory updates process process synchronize second process access data synchronization vm page grid array false sharing example jacobi operation executed 
illustrate principle jacobi tsp examples section 
writes shared memory jacobi occur barrier passed newly computed values copied scratch array grid array see 
phase computation terminates barrier passed 
barrier prevent processes starting iteration new values installed grid array 
barrier needs correctness avoid data races regardless memory model 
presence allows delay notifying process updates grid process barrier lowered 
tsp tour queue primary shared data structure 
processors fetch tasks queue creating new tasks process 
newly created tasks inserted queue 
updates task queue structure require series shared memory writes size task head queue atomic access task queue data structure required order correct program execution 
processor permitted access task queue data structure time 
guarantee achieved putting lock acquire lock release operations 
order access tour queue process needs acquire lock 
suffices inform process acquires lock changes tour queue done time lock acquired 
examples illustrate general principle underlying release consistency 
synchronization introduced shared memory parallel program prevent processes looking certain memory locations synchronization operation completes 
follows necessary inform process modifications shared memory locations synchronization operation completes 
program data races appear program executes sequentially consistent memory intuitive memory model programmers expect 
true condition synchronization done treadmarks supplied primitives 
treadmarks tell shared memory consistent 
turn slightly formal discussion 
addition ordinary memory accesses release consistency takes account synchronization operations 
distinguishes acquire synchronization primitives signal series accesses shared memory release synchronization primitives signal series accesses shared memory 
lock acquire departure barrier treated acquire lock release arrival barrier treated release 
synchronization operations mapped acquires releases 
partial order happened denoted hb defined releases acquires shared memory accesses way ffl ordinary shared memory accesses releases acquires processor occurs program order hb ffl release processor corresponding acquire processor hb lock acquire corresponds release acquires releases lock 
barrier acquire corresponds release acquire departure release arrival instance barrier 
ffl hb hb hb release consistency requires processor may continue past acquire shared accesses precede acquire hb reflected acquiring processor 
release consistency implementations definition release consistency specifies latest possible time shared memory update visible particular processor 
allows implementation release consistency considerable latitude deciding exactly shared memory update gets propagated 
treadmarks uses lazy release consistency algorithm implement release consistency 
roughly speaking lazy release consistency enforces consistency time acquire contrast earlier implementation release consistency munin referred eager release consistency enforced consistency time release 
shows intuitive argument lazy release consistency 
assume replicated processors 
eager release consistency message needs sent processors release informing change processor acquires lock access lazy release consistency processor informed change addition reduction message traffic resulting having inform processes cache copy lazy release consistency allows notification modification piggybacked lock message going releasing acquiring process 
addition treadmarks uses invalidate protocol 
time acquire modified pages invalidated 
access page cause access turn cause date copy page installed 
alternative update protocol acquire message contains new values modified pages 
detailed discussion protocols treadmarks scope 
refer reader keleher thesis detail 
compare performance various implementations release consistency sequential consistency section 
acq rel acq rel acq acq rel acq rel acq lazy vs eager release consistency lazy implementation performs consistency actions acquire eager implementation performs release 
multiple writer protocols hardware cache dsm systems single writer protocols 
protocols allow multiple readers access page simultaneously writer required sole access page performing modifications 
single writer protocols easy implement copies page identical page faults satisfied retrieving copy page processor currently valid copy 
unfortunately simplicity comes expense message traffic 
page written copies invalidated 
invalidations cause subsequent access misses processors pages invalidated accessing page data 
false sharing cause single writer protocols perform worse interference unrelated accesses 
dsm systems typically suffer false sharing hardware systems track data accesses granularity virtual memory pages cache lines 
name implies multiple writer protocols allow multiple processes time writable copy page 
explain multiple writer protocol works referring back example showing possible memory layout jacobi program see 
assume process initially identical valid copy page 
treadmarks uses virtual memory hardware detect accesses modifications shared memory pages see 
shared page initially write protected 
write occurs treadmarks creates copy page twin saves part treadmarks data structures page user address space writes page occur software intervention 
arrives barrier modified copy user address space unmodified twin 
doing word word comparison user copy twin create diff encoding modifications page 
diff created twin discarded 
sequence events happens important note entire sequence events local processors require message exchanges case single writer protocol 
part barrier mechanism informed modified page vice versa invalidate copy page 
access page part iteration take access fault 
treadmarks software knows modified page sends message requesting diff applies diff page arrives 
sequence events happens replaced vice versa 
exception time processor accesses page copy page updated exclusively applying diffs new complete copy page needed 
primary benefit diffs implement multiple writer protocols reducing effects false sharing 
addition significantly reduce bandwidth requirements diffs typically smaller page 
reader may wonder happens processes modify overlapping portions page 
note corresponds data race means processes write location intervening synchronization 
certainly error program 
sequentially consistent memory outcome timing dependent 
true treadmarks 
possible modify treadmarks checks occurrences 
diff arrives page locally modified treadmarks check overlap modifications currently done 
write create twin twin release diff encode changes replicated write protect writable diff treadmarks system treadmarks implemented entirely user level library top unix 
modifications unix kernel necessary modern unix implementations provide communication memory management functions required implement treadmarks user level 
programs written fortran compiled linked treadmarks library standard compiler language 
result system relatively portable 
currently runs sparc decstation dec alpha ibm rs ibm sp ibm sp hp sgi platforms ethernet atm networks 
section briefly describe communication memory management treadmarks implemented 
detailed discussion implementation refer reader keleher ph thesis 
treadmarks implements communication berkeley sockets interface 
depending underlying networking hardware example ethernet atm treadmarks uses udp ip aal message transport protocol 
default treadmarks uses udp ip machines connected atm lan 
aal connection oriented best efforts delivery protocol specified atm standard 
udp ip aal guarantee reliable delivery treadmarks uses light weight operation specific user level protocols insure message arrival 
message sent treadmarks request message response message 
request messages sent treadmarks result explicit call treadmarks library routine page fault 
machine sent request message blocks request message expected response message arrives 
response arrives certain timeout original request retransmitted 
minimize latency handling incoming requests treadmarks uses signal handler 
message arrival socket receive request messages generates signal 
aal connection oriented protocol socket corresponding machines 
determine socket holds incoming request handler performs select system call 
handler receives request message performs specified operation sends response message returns interrupted process 
implement consistency protocol treadmarks uses mprotect system call control access shared pages 
attempt perform restricted access shared page generates sigsegv signal 
sigsegv signal handler examines local data structures determine page state examines exception stack determine read write 
local page invalid handler executes procedure obtains essential updates shared memory minimal set remote machines 
read page protection set read 
write handler allocates page pool free pages performs bcopy create twin 
action taken response fault resulting write page read mode 
handler upgrades access rights original page returns 
basic operation costs experimental environment consists decstation running ultrix 
machine fore atm interface connected fore atm switch 
connection interface boards switch operates mbps switch aggregate throughput gbps 
interface board programmed transmit receive fifos requires messages assembled disassembled atm cells software 
interrupts raised message nearly full receive fifo 
noted performance numbers describe processor executions atm lan low level adaptation layer protocol aal 
minimum round trip time send receive smallest possible message seconds 
sending minimal message takes seconds receiving takes seconds remaining seconds divided wire time interrupt processing resuming processor blocked receive 
signal handler receive message processors round trip time increases seconds 
minimum time remotely acquire free lock seconds manager processor hold lock seconds 
minimum time perform processor barrier seconds 
remote access obtain byte page processor takes seconds 
time twin microseconds page size kilobytes 
time diff somewhat data dependent 
page unchanged takes microseconds 
entire page changed takes microseconds 
worst case occurs word page changed 
case making diff takes microseconds 
applications number applications implemented treadmarks performance benchmarks reported earlier 
describe experience large applications implemented treadmarks 
applications mixed integer programming genetic linkage analysis parallelized starting existing efficient sequential code authors sequential code help authors 
difficult quantify effort involved amount modification arrive efficient parallel code proved relatively minor demonstrated rest section 
mixed integer programming mixed integer programming mip version linear programming lp 
lp objective function optimized region described set linear inequalities 
mip variables constrained take integer values just values 
shows precise mathematical formulation shows simple dimensional instance 
treadmarks code solve mip problem uses branch cut approach 
mip problem relaxed corresponding lp problem 
solution lp problem general produce non integer values variables constrained integers 
step pick variables branch new lp problems added constraint bx added constraint dx see 
time algorithm generates tree branches 
soon solution minimize subject 
mixed integer programming mip problem dimensional instance mip branch bound solution mip problem solution establishes bound solution 
nodes branch tree solution lp problem generates result inferior bound need explored 
order expedite process algorithm uses technique called essentially depthfirst search tree find integer solution establish bound quickly possible 
final algorithmic improvement interest cutting planes 
additional constraints added lp problem tighten description integer problem 
code solve problems library 
library includes representative examples airline crew scheduling network flow plant location fleet scheduling shows speedups obtained problems sequential running times seconds 
problems speedup near linear 
problem exhibits super linear speedup parallel code happens hit solution early execution pruning branch bound tree 
problem little speedup solution shortly pre processing step parallelized 
addition problems library code solve previously unsolved multicommodity flow problem 
problem took roughly cpu days processor ibm sp exhibited near linear speedup 
genetic linkage analysis genetic linkage analysis statistical technique uses family pedigree information map human genes locate disease genes human genome 
advances biology genetics enormous amount genetic material available making computation bottleneck discovery disease genes 
classical theory inheritance child chromosomes receive strand parent chromosomes 
reality inheritance complicated due recombination 
recombination occurs child chromosome strand contains piece strands parent chromosome see 
goal linkage analysis derive probabilities recombination occurred gene looking genes known locations 
probabilities approximate location gene chromosome computed 
parallelized version widely genetic linkage analysis program part linkage package 
takes input family tree called pedigree augmented genetic information members family 
computes maximum likelihood estimate recombination probability 
top level consists loop optimizes 
iteration optimization loop program traverses entire pedigree nuclear family time computing likelihood current genetic information known family members 
member nuclear family algorithm updates large array conditional probabilities representing probability individual certain genetic characteristics conditioned part family tree traversed 
algorithm parallelized splitting iteration space nuclear family available processors manner balances load 
load balancing essential relies knowledge genetic information represented array elements 
alternative approach splitting tree traversal failed produce speedups computation occurs small part tree typically nodes closest root representing deceased individuals little genetic information known 
presents speedups obtained various data sets 
data sets originate actual disease gene location studies 
data sets long running time speedups achieved 
smallest data sets speedup communication computation processors speedup results library line represents different data set 
numbers bottom indicate sequential execution seconds corresponding data set 
data sets sequential running times larger seconds 
mendel recombination dna recombination ratio larger 
general speedup highly dependent communication ratio particular number messages second 
data set smallest speedup exchanged approximately messages second data set best speedup number messages second went approximately 
overhead time spent executing application code dominated idle time unix overhead 
idle time results load imbalance waiting messages arrive network 
unix overhead time spent executing unix library code system calls 
unix overhead related network communication 
small portion overhead spent executing code treadmarks library 
conclude largest single overhead contribution stems network communication related events validating focus reducing number messages amount data exchanged 
space overhead consists memory twins diffs treadmarks data structures 
current version system megabytes memory statically allocated diffs megabyte data structures 
garbage collection procedure invoked limits exceeded 
space twins dynamically allocated 
representative example large run data set sequential running time seconds maximum memory usage twins point execution approximately megabyte 
related goal section provide extensive survey parallel programming research illustrate alternative approaches 
example system alternative approaches 
discuss alternative programming models turn different implementations shared memory programming models 
alternative programming models message passing pvm 
currently message passing prevailing programming paradigm distributed memory systems 
parallel virtual machine pvm popular software message passing package 
allows heterogeneous network computers appear single concurrent computational engine 
treadmarks currently restricted homogeneous set nodes 
programming pvm easier portable programming native message passing paradigm underlying machine application programmer needs write code exchange messages explicitly 
goal treadmarks remove burden programmer 
programs complex data structures sophisticated parallelization strategies believe major advantage 
believe genetic linkage program provides compelling example argument 
built treadmarks pvm implementation 
iteration process updates subset large sparse array probabilities 
implement message passing requires additional code remember locations updated marshal modified values memory message vice versa 
treadmarks shared memory mechanism transparently moves values processors needed 
downside message passing implementations efficient shared memory implementations 
returning example pvm updates sent single message treadmarks program takes page faults equal number message exchanges accomplish goal 
detailed comparison programmability performance treadmarks pvm refer reader lu thesis includes different applications 
processors speedup speedup results line represents different data set 
numbers bottom indicate sequential execution time seconds corresponding data set 
implicit parallelism hpf 
treadmarks pvm explicitly parallel programming methods programmer divide computation different threads synchronization message passing control interactions concurrent threads 
implicit parallelism hpf user writes single threaded program parallelized compiler 
particular hpf contains data distribution primitives may compiler drive parallelization process 
approach suitable data parallel programs jacobi 
memory accesses programs regular determined completely compile time 
applications compiler typically produce code runs efficiently application coded dsm compiler predict accesses dsm system react 
explored extensions paradigm irregular computations involving sparse arrays 
programs exhibiting dynamic parallelism tsp mip easily expressed hpf framework 
alternative distributed shared memory implementations hardware shared memory implementations dash 
alternative approach shared memory implement hardware snooping bus protocol small number processors directory protocol larger number processors 
share approach programming model implementation avoids expensive cache controller hardware 
hand hardware implementation efficiently support applications finer grain parallelism 
limited experience comparing performance hardware software shared memory 
particular compared performance applications including slightly older version processor sgi hardware shared memory multiprocessor treadmarks running processor atm network decstation 
interesting aspect comparison systems processor running clock speed primary cache compiler 
provision shared memory different hardware bus snoopy protocol sgi software release consistent protocol treadmarks 
identical programs sgi treadmarks 
data sets long running times communication computation ratio small differences minimal 
shorter runs differences pronounced 
sequentially consistent software distributed shared memory ivy 
sequential consistency messages sent roughly speaking write shared memory page valid copies outstanding 
contrast release consistency messages sent synchronization operation 
net effect somewhat application dependent release consistent dsms general send fewer messages sequentially consistent dsms perform better 
particular carter ph thesis contains comparison application programs run eager release consistency munin sequential consistency 
compared sequentially consistent dsm munin achieves performance improvements ranging percent depending application 
lazy vs eager release consistency munin 
lazy release consistency causes fewer messages sent 
time lock release munin sends messages processors cache data modified releasing processor 
contrast lazy release consistency consistency messages travel releaser new 
lazy release consistency somewhat complicated eager release consistency 
release munin forget modifications releasing processor prior release 
case lazy release consistency third processor may acquire lock need see modifications 
practice experience indicates networks workstations cost sending messages high gains achieved reducing number messages outweighs cost complex implementation 
particular keleher compared performance applications lazy eager release consistency fast fourier transform lazy implementation performed better 
shown invalidate protocol works better update protocol large amount data resulting update protocol 
entry consistency midway 
entry consistency relaxed memory model 
release consistency consistency actions taken conjunction synchronization operations 
release consistency entry consistency requires shared data object associated synchronization object 
synchronization object acquired modified data associated synchronization object consistent 
association release consistency shared data consistent 
result entry consistency generally requires data traffic lazy release consistency 
entry consistency implementation midway uses update protocol treadmarks uses invalidate protocol 
programmability performance differences approaches understood 
structured dsm systems linda 
providing programmer shared memory space organized linear array bytes structured dsm systems offer shared space objects tuples accessed properly synchronized methods 
advantages programming perspective approach allows compiler infer certain optimizations reduce amount communication 
instance tsp example object oriented system treat queue partial tours queue object enqueue dequeue operations 
similarly linda operations implemented means primitives tuple space 
operations typically implemented efficiently paging queue data structure happens dsm 
downside objects natural sequential program right grain parallelization requiring changes arrive efficient parallel program 
sparse updates larger arrays instance little connection objects program updates 
experience demonstrates suitable implementation techniques distributed shared memory provide efficient platform parallel computing networks workstations 
large applications ported treadmarks distributed shared memory system little difficulty performance 
intend experiment additional real applications including seismic modeling code 
developing various tools ease programming burden improve performance 
particular investigating compiler support prefetching performance monitoring tools eliminate unnecessary synchronization 
adve hill 
unified formalization shared memory models 
ieee transactions parallel distributed systems june 
ahuja gelernter 
linda friends 
ieee computer august 
bershad 
midway distributed shared memory system 
proceedings compcon conference pages february 
carter 
munin efficient distributed shared memory multi protocol release consistency 
phd thesis rice university october 
appeared rice technical report rice comp tr 
keleher 
distributed shared memory lazy release consistency 
phd thesis rice university december 
appeared rice technical report rice comp tr 
koelbel schreiber steele jr 
high performance fortran handbook 
mit press 
lamport 
multiprocessor computer correctly executes multiprocess programs 
ieee transactions computers september 
lenoski laudon gharachorloo gupta hennessy 
directory cache coherence protocol dash multiprocessor 
proceedings th annual international symposium computer architecture pages may 
li hudak :10.1.1.14.3607
memory coherence shared virtual memory systems 
acm transactions computer systems november 
lu 
message passing versus distributed shared memory networks workstations 
master thesis rice university april 
appeared rice technical report rice 
stumm zhou 
algorithms implementing distributed shared memory 
ieee computer may 
sunderam 
pvm framework parallel distributed computing 
concurrency practice experience december 
theses referred obtained anonymous ftp cs rice edu directory public treadmarks papers 
information obtaining treadmarks system send mail treadmarks ece rice edu 

