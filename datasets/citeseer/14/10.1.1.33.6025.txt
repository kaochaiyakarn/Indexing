model selection support vector machines olivier chapelle vladimir vapnik research labs red bank nj lip paris france research att com new functionals parameter model selection support vector machines introduced concepts span support vectors rescaling feature space 
shown functionals predict best choice parameters model relative quality performance value parameter 
support vector machines svms implement idea map input vectors high dimensional feature space maximal margin hyperplane constructed 
shown training data separable error rate svms characterized radius smallest sphere containing training data margin distance hyperplane closest training vector feature space 
functional estimates vc dimension hyperplanes separating data margin perform mapping calculate svm technique uses positive definite kernel specifies inner product feature space 
example kernel radial basis function rbf jjx jj kernel free parameter generally kernels require parameters set 
treating noisy data svms parameter penalizing training errors needs set 
problem choosing values parameters minimize expectation test error called model selection problem 
shown parameter kernel minimizes functional provides choice model minimum functional coincides minimum test error :10.1.1.117.3731
shapes curves different 
article introduce refined functionals specify best choice parameters parameter kernel parameter penalizing training error produce curves better reflect actual error rate 
organized follows 
section describes basics svms section introduces new functional concept span support vectors section considers idea rescaling data feature space section discusses experiments model selection functionals 
support vector learning introduce standard notation svms complete description see 
set training examples belong class labeled 
decision function svm sgn coefficients obtained maximizing functional constraints constant controls tradeoff complexity decision function number training examples misclassified 
svm linear maximal margin classifiers high dimensional feature space data mapped non linear function 

points called support vectors 
distinguish call respectively support vectors second category 
prediction span support vectors results introduced section leave cross validation estimate 
procedure usually estimate probability test error learning algorithm 
leave procedure leave procedure consists removing training data element constructing decision rule basis remaining training data testing removed element 
fashion tests elements training data different decision rules 
denote number errors leave procedure 
known leave procedure gives unbiased estimate probability test error expectation test error machine trained examples equal expectation 
provide analysis number errors leave procedure 
purpose introduce new concept called span support vectors 
span support vectors results section depend feature space consider loss generality linear svms 
suppose solution optimization problem 
fixed support vector define set constrained linear combinations support vectors category fi note 
define quantity call span support vector minimum distance set see min inf inf support vectors 
set semi opened dashed line 
shown set empty dsv dsv diameter smallest sphere containing support vectors 
intuitively smaller leave procedure error vector formally theorem holds theorem leave procedure support vector corresponding recognized incorrectly inequality holds max theorem implies separable case number errors leave procedure bounded follows max max 
improvement compared functional dsv depending geometry support vectors value span diameter dsv support vectors equal zero 
go assumption set support vectors change leave procedure leads theorem theorem sets support vectors second categories remain leave procedure support vector equality holds decision function svm trained respectively training set point removed 
proof theorem follows theorem 
assumption set support vectors change leave procedure obviously satisfied cases 
proportion points violate assumption usually small compared number support vectors 
case theorem provides approximation result leave procedure pointed experiments see section 
noticed larger important decision function support vector surprising removing point causes change decision function proportional lagrange multiplier kind result theorem derived svms threshold inequality derived :10.1.1.117.3731:10.1.1.117.3731
span takes account geometry support vectors order get precise notion important point 
previous theorem enables compute number errors leave oneout procedure corollary assumption theorem test error prediction leave procedure note points support vectors correctly classified leave procedure 
defines number errors leave procedure entire training set 
assumption theorem box constraints definition removed 
consider hyperplanes passing origin constraint removed 
assumptions computation span unconstrained minimization quadratic form done analytically 
support vectors category leads closed form sv pp matrix dot products support vectors category 
similar result obtained 
section span rule model selection separable nonseparable cases 
rescaling mentioned functional bounds vc dimension linear margin classifier 
bound tight data fills surface sphere enclosing training data data lie flat ellipsoid bound poor radius sphere takes account components largest deviations 
idea rescaling data feature space radius sphere stays constant margin increases apply bound rescaled data hyperplane 
consider linear svms mapping high dimensional space 
rescaling achieved computing covariance matrix data rescaling eigenvalues 
suppose data centered normalized eigenvectors covariance matrix data 
compute smallest enclosing box containing data centered origin edges parallels 
box approximation smallest enclosing ellipsoid 
length edge direction max jx 
rescaling consists diagonal transformation dx 
consider dw 
decision function changed transformation 

data fill box side length 
functional replace rescaled data box estimated radius enclosing ball norm classical norm 
theoretical works needs done justify change norm 
non linear case note map data high dimensional feature space lie linear subspace spanned data 
number training data large subspace dimension 
purpose tools kernel pca matrix normalized eigenvectors gram matrix ij eigenvalues dot product 
replaced ik 
ik achieve diagonal transformation functional max ik ik experiments check new methods performed series experiments 
concerns choice width rbf kernel linearly separable database postal database 
dataset consists handwritten digit size test set examples 
split training set subsets training examples 
task consists separating digit 
error bars figures standard deviations trials 
experiment try choose optimal value noisy database breast cancer database dataset split randomly times training set containing examples test set containing examples 
section describes experiments model selection span rule separable case non separable section shows vc bounds model selection separable case rescaling 
model selection span rule section prediction test error derived span rule model selection 
shows test error prediction span different values width rbf kernel postal database 
plots functions different values breast cancer database 
see method predicts correct value minimum 
prediction accurate curves identical 
available horn gmd de data breast cancer log sigma error test error span prediction log test error span prediction choice postal database choice breast cancer database test error prediction span rule 
computation span rule involves computing span support vector 
note interested inequality exact value span minimizing find point minimization point correctly classified leave procedure 
turned experiments time required compute span prohibitive training time 
noteworthy extension application span concept 
denote hyperparameter kernel derivative computable possible compute analytically derivative upper bound number errors leave procedure see theorem 
provides powerful technique model selection 
initial approach choose value width rbf kernel minimum span rule 
case possible try different values 
hyperparameters example component possible exhaustive search possible values hyperparameters 
previous enables find optimal value classical gradient descent approach 
preliminary results show approach previously mentioned kernel improve test error 
vc dimension rescaling section perform model selection postal database functional rescaled version 
shows values classical bound different values 
bound predicts correct value minimum reflect actual test error 
easily understandable large values data input space tend mapped flat ellipsoid feature space fact taken account 
shows performing rescaling data manage tighter bound curve reflects actual test error 
log sigma vc dim vc dimension log sigma vc dimension rescaling rescaling rescaling bound vc dimension different values postal database 
shape curve rescaling similar test error 
introduced new techniques model selection svms 
span rescaling data feature space 
demonstrated techniques predict optimal values parameters model evaluate relative performances different values parameters 
functionals lead new learning techniques establish generalization ability due margin 
acknowledgments authors jason weston patrick haffner discussions comments 
burges :10.1.1.117.3731
tutorial support vector machines pattern recognition 
data mining knowledge discovery 
jaakkola haussler 
probabilistic kernel regression models 
proceedings conference ai statistics 
opper winther 
gaussian process classification svm mean field results leave estimator 
advances large margin classifiers 
mit press 
appear 
scholkopf shawe taylor smola williamson 
kernel dependent support vector error bounds 
ninth international conference artificial neural networks pp 
scholkopf smola 
muller 
kernel principal component analysis 
artificial neural networks icann pages berlin 
springer lecture notes computer science vol 

vapnik 
statistical learning theory 
wiley new york 
vapnik chapelle 
bounds error expectation svm 
neural computation 
submitted 
