regularization methods inferential sensing nuclear power plants wesley hines andrei ibrahim robert uhrig nuclear engineering department university tennessee knoxville tennessee inferential sensing information related plant parameter infer actual value 
common method inferential sensing uses mathematical model infer parameter value correlated sensor values 
collinearity predictor variables leads ill posed problem causes inconsistent results data models linear regression neural networks 
presents linear non linear inferential sensing methods including linear regression neural networks 
methods modified original form solve ill posed problems produce consistent results 
compare techniques data florida power crystal river nuclear power plant predict drift feedwater flow sensor 
report entitled feedwater flow measurement nuclear power generation stations commissioned electric power research institute meter single frequent cause water reactors 
presents viable solutions problem 
safe economical operation nuclear power plants requires knowledge state plant 
knowledge obtained measuring critical plant parameters sensors instrument chains 
correct operation sensor systems validated assure safe efficient operation nuclear power plants 
traditional approaches sensor validation nuclear power plants involve redundant sensors coupled periodic instrument calibration 
periodic sensor calibration techniques require process shut instrument taken service instrument loaded calibrated 
method lead equipment damage incorrect calibrations due adjustments non service conditions increased radiation exposure maintenance personnel possibly increased downtime 
sensors calibration result instruments unnecessarily maintained 
correct adjustment vital maintaining proper plant operation alternative condition technique desirable 
implementing condition calibration methods instruments calibrated determined calibration 
line real time sensor calibration monitoring allow nuclear utilities reduce maintenance efforts necessary assure instruments calibration increase reliability components 
utility line monitoring working group estimates industry wide cost savings years depending values applied indirect benefits 
inferential sensing prediction plant variable correlated plant variables 
line calibration monitoring systems produce inferred value compare sensor value determine sensor status 
system monitor sensors drift failures making periodic instrument calibrations unnecessary 
methods inferential sensing including types regression neural networks fuzzy logic statistical techniques 
techniques produce believable results adversely affected collinearity predictor variables 
investigates collinearity adversely affects inferential sensing techniques making results inconsistent presents regularization potential solution 
sensor fault detection subset fault detection isolation includes detection faults components 
surveys fdi technologies published including basseville patton willsky 
techniques studies sensor fault detection nuclear power plants including expert systems uhrig model techniques hardy state estimation techniques black gross singer artificial neural networks hines uhrig fuzzy logic hines hybrid combinations techniques 
techniques divided basic categories physical model techniques data driven models 
define physical model techniques mathematical models developed principals data driven techniques constructed data collected process 
investigate linear non linear data driven methodologies inferential sensing 
linear regression lr 
ridge regression rr 
truncated singular value decomposition 
partial squares pls 
neural networks nn methodologies compared basis prediction performance including ability model non linearities ease design ability handle collinear predictor variables producing consistent results 
example comparisons inferential measurement nuclear power plant feedwater flow 
data set extremely ill conditioned illustrate necessity application regularization techniques 
inferential sensing problems extreme easier solve regularization techniques 
feedwater flow measurement united states nuclear power plant operating limit tied thermal power production 
steam generators feedwater flow rate input calculation thermal power accurately known 
majority water reactors utilize meters measure feedwater flow ruggedness precision 
meters susceptible due products feedwater 
increases measured pressure drop meters results estimation flow rate 
consequently reactors thermal power overestimated nuclear news 
stay regulatory limits reactor operators forced plants 
report entitled feedwater flow measurement nuclear power generation stations commissioned meter single frequent cause 
amount report varied insignificant full power 
average full power 
estimated unit cost utility approximately day cost electricity 
despite susceptibility meter common flow measurement instrument nuclear reactors 
overcome loss generating capacity utilities developed coefficient correction factor offset degradation measurements accuracy 
drawback method flow measurement corrected fact resulting reactor operating thermal power limits 
best solution problem inferential sensing system accurately predict feedwater flow 
presents methodologies perform function 
prediction problems caused collinear data traditionally line prediction instrument performance redundant sensors 
redundant sensors highly correlated measurements predictor variables causes potential problem data prediction models 
problem occurs due collinearity predictor variables 
variables collinear data vectors representing lie line subspace dimension 
generally variables collinear vectors represent lie subspace dimension vectors linear combination 
practice exact collinearity rarely occurs due process measurement noise 
broader notion collinearity needed deal problem affects statistical estimation 
loosely variables collinear lie line angle small 
event variables constant equivalent saying high degree correlation 
show degrading effects collinearity prediction consider redundant sensors operating noisy environment infer value third redundant sensor common linear regression model equation 
redundant sensors outputs respective noise term perfectly collinear 
singular value decomposition svd jolliffe determine variables lie subspace dimension 
method performs linear transformation data new coordinate system maximum amount variation data principal axis remaining variation second axis 
mathematical simulation redundant sensor case resulted singular values 
square singular values proportional amount variance data 
case variation original signals represented single signal 
words variables lie single dimensional subspace 
shows high degree collinearity problem condition number 
solving prediction model linear regression yields coefficients 
huge weights result solution larger variance predictor variables 
results noisy inconsistent predictions 
regularization technique section solution generated 
optimal solution problem average redundant predictors 
regularization solution slightly biases small norm resulting weights slightly smaller optimal solution 
cost regularization benefit regularization stable inferred value smaller variance predictors 
methodology development data inferential sensing system consists collecting training testing data preprocessing data remove outliers scaling data allow statistical signal evaluation techniques 
data collected preprocessed inferential model developed tested 
final system shown vector predictor signals xs preprocessed scaled signal inferred signal 
data pre processing model fig 
inferential sensing system inferential models inferential sensing including linear techniques linear regression principal component regression ridge regression partial squares non linear techniques non linear regression non linear partial squares artificial neural networks fuzzy inference systems 
primary categories methods deal problem collinearity 
category transforms predictors new orthogonal space removing collinearity second category called direct regularization methods deals making ill conditioned problem conditioned problem 
matrix shows techniques inferential sensing method 
investigate methodologies section 
data preprocessing method predictor transformations direct regularization linear regression partial squares neural network prior statistical evaluation data number preprocessing techniques applied raw data ensure consistent results 
common preprocessing techniques filtering scaling 
outliers known fact squares models sensitive outliers 
just outlier significantly distort estimation computed squares method 
fact noise spike occurring sample time uncorrelated signals result correlation coefficient 
reduce measurement noise suggest median filter known outlier rejection fast digital implementation properties 
data scaling data scaling controversial issues addressed regularization ill posed problems 
scaling change singular value spectrum data matrix change precision regularized solution 
types data scaling commonly known scaling provides signals zero mean unit variance second term range scaling linearly scales data range zero 
examples range scaling mean center data 
regularization inferential measurement feedwater flow sensor drift inference actual feedwater flow rate 
actual flow predicted relationship correlated plants parameters 
problem parameters predictors highly correlated feedwater flow correlated 
degree correlation quite high data matrix ill conditioned problem drift detection ill posed sense hadamard 
hadamard defined posed problem problem satisfies conditions 
solution problem exists 

solution unique 

solution stable smooth small perturbations data small perturbations data produce small perturbations solution 
conditions met problem termed ill posed special considerations taken ensure reliable solution 
understand essence ill posed problems inferential sensing consider linear squares model objective find linear combination predictor variables accurately models response variable 
min xw data matrix containing samples predictor variables related feedwater flow rate vector measured values feedwater flow solution regression coefficients 
valuable tool analysis ill posed problems singular value decomposition svd golub 
svd data matrix written called left right eigenvectors singular values matrix terms svd solution squares problem written wls assume matrix full rank equation gives insight essence 
division small singular values results amplification high frequency oscillations right singular vectors data matrix deal ill conditioned problems methods developed essentially damp filter high frequency oscillations 
methods called regularization methods regularize smooth potentially unstable squares solutions 
simple regularization method truncated svd method 
method truncates sum equation value avoiding small singular values denominator equation 
heuristics method regularization follows 
singular values distinct gap spectrum 
location gap singular values curve natural choice truncation parameter 
left right singular vectors ui vi tend sign changes elements index increases decreases hansen 
heuristic guaranteed hold totally positive matrices hansen 
matrix called totally positive minors order positive 
matrices arise practical applications usually satisfy heuristic worth checking property prior application regularization method heuristic 
kind regularization especially appropriate ill conditioned problems having large gap say orders magnitude pair singular values 
kinds problems called problems determined numerical rank 
real world problems determined numerical rank 
singular value spectrum distinct gap problem ill determined numerical rank choice truncation regularization parameter evident case 
stressed hansen success depends satisfaction discrete picard conditions dpc hansen existence particular gap singular value spectrum data matrix deal ill conditioned problems having ill determined numerical rank method regularization due tikhonov 
method minimization problem replaced augmented functional xw min lw regularization parameter controls trade smoothness solution fitness data 
conditioned matrix example discrete approximation derivative operator 
main assumption tikhonov regularization solution smooth non oscillating 
case identity matrix tikhonov functional said standard form known statistical literature ridge regression 
case write regularized solution ui vi ui vi left right singular vectors data matrix singular values matrix filter factors 
role filter factors suppress contribution minor components solution providing stable non oscillating solution 
tikhonov regularization filter factors large close small tend zero providing necessary filtering minor components 
heuristic tikhonov regularization formulated singular value spectrum decays zero particular gap singular values 
heuristic tikhonov regularization smoothness stability regularized solution depends satisfaction conditions heuristic 
noted practical situation singular value spectrum decay zero levels index due unavoidable errors measurements instrumentation noise 
noise level right left parts xw crucial factor satisfaction discrete picard condition existence regularized solution reasonable approximation desired true solution 
theoretical considerations mind tackle problem drift detection feed water flow instrumentation 
problem belongs called inferential virtual measurements inverse problem aim recover infer unknown parameters physical system measurable noisy data 
feedwater flow data variables correlated feed water flow selected predictor variables inferential models 
variables include pump speeds pressures temperatures levels flow rates npp secondary 
listing included appendix 
inferential models constructed predictor variables estimate feed water flow rate 
training region models chosen plant start days fuel cycle 
feedwater flow removed acid cleaned calibrated fuel cycles assumed working correctly cycle 
training data consists samples predictor variables recorded minute intervals 
response variable feed water flow rate recorded sample times 
training data sets median filtered scaled 
resulting training data linear correlation coefficients showing data linearly correlated feedwater flow 
principal components analysis pca jolliffe shows principal component contains variation training data 
result indicates data extremely collinear predictors lie axis indicates prediction problem ill conditioned 
inferential prediction problem exhibit instability problems caused collinearity 
results solutions inferential sensing problems highly collinear ill conditioned inconsistent unstable 
results may dependent selected training data construct inferential model architecture inferential model initialization inferential model 
raises concerns stability feedwater flow drift estimation reliability inferential measurements 
evaluate consistency inferential drift estimation system bootstrap technique efron 
bootstrap technique statistical method evaluating stability prediction models 
training data set sample size bootstrap technique samples values predictor response variables random replacement providing bootstrap sample size original values duplicated missing 
bootstrap sample set construct inferential model estimate feedwater flow 
method repeated large number times bootstrap procedure provides set solution models consistency evaluated forming probability density function drift estimates specific time 
specifically bootstrap samples generated original data develop inferential models predict feedwater flow sensor drift value months fuel cycle 
method determine inferential model influenced small changes instrumentation noise 
linear regression models linear regression model feedwater flow prediction probability density function drift estimates shown 
pdf drift value mean klb hour large variance standard deviation klb hour 
results show inconsistency drift estimation ordinary squares model 
instability drift estimation due ill conditioned nature problem consequence due high sensitivity ordinary squares solution small perturbations data 
stabilize drift prediction regularization method alleviate ill conditioning problem 
feedwater flow feedwater flow estimation linear regression sample number minute intervals fig 
pdf feedwater drift ols probability density function bootstrap estimation drift value check point drift value check point klb hr standard form tikhonov regularization ridge regression regularize problem 
prior applying form regularization regularization parameter chosen resolve subtle compromise smoothness regularized solution solution bias 
biasing small regression coefficients price paid smoothness stability regularized solution 
want get smoothness possible significantly biasing solution 
methods optimal payment proposed 
principle discrepancy requires knowledge right hand side error equation xw estimation available method yields regularization solution 
highly regarded methods regularization parameter selection assume knowledge error level extraction information data 
generalized cross validation method golub assumption arbitrary element yi right hand side removed corresponding regularized solution predict observation hansen 
common method determining regularization parameter curve method hansen 
curve plot residual norm versus solution norm 
residual norm comprised error reduced model bias due regularization 
solution norm measure size regression coefficients 
regularization parameter increased regression coefficients reduced making solution smoother bias added solution 
best solution occurs little bias solution smooth 
optimal regularization parameter locating corner curve 
curve method reliable simple implemented choose regularization parameter 
singular values ill posed problem drift detection table 
noted significant singular value singular values smaller reside instrumentation noise 
curve plotted log log plot singular values span orders magnitude 
analysis curve shows best corresponding corner curve value chosen optimal regularization parameter 
value slightly smaller significant singular value tends damp information contained eigenvectors corresponding insignificant singular values 
truncated singular value decomposition method completely removes information contained significant components regularize problem section 
table 
singular values solution norm curve 
corner residual norm fig 

curve tikhonov regularization 
optimal regularization parameter regularize solution bootstrap procedure test solution stability 
results application bootstrap technique regularization shown 
seen variance drift estimation reduced times standard deviation klb hour 
stability regularized drift clearly seen unimodal nature pdf function 
mean value regularized drift klb hour 
value corresponds drift months operation coincides previous feedwater flow rate drift estimation studies 
probability density function bootstrap estimation drift value check point drift value check point klb hr fig 

pdf tikhonov regularization 
second method regularization linear solution truncated singular value decomposition 
method linearly transforms data orthogonal space singular value decomposition principal components analysis 
transformed variables called principal components 
principal components evaluated determine kept predictor variables 
components orthogonal collinearity longer problem 
case principal component valuable information 
regressed response variable calculate single weight 
transformation matrix weight predict feedwater flow 
results bootstrap procedure shown produce mean drift klb hour standard deviation klb hour 
shows model properly stabilizes solution 
partial squares model probability density function bootstrap estimation drift value check point drift value check point klb hr fig 

pdf truncated singular value decomposition 
partial squares pls factor technique perform multilinear regression 
principal components analysis pca methods pls architecture see linearly transforms input space orthogonal space removing collinearity inputs 
pca transforms inputs principal component explains variance data pls transforms inputs latent vector accounts majority covariance inputs response variable 
method takes consideration response variable performing transformation pca considers predictor variables 
latent vectors iteratively transformed regressed response variable linear weight matrix 
inputs fig 

pls inferential model architecture output optimal number latent variables perform prediction calculated calculating reduced eigenvalues kramer 
reduced eigenvalue remaining orders magnitude smaller 
magnitude reduced eigenvalues equal amount covariance explain corresponding latent vector 
case reduced eigenvalue significant latent vector model 
results bootstrap procedure shown produce mean drift klb hour standard deviation klb hour 
shows pls method properly stabilized solution 
probability density function bootstrap estimation drift value check point drift value check point klb hr fig 

pdf partial squares non linear partial squares model qin performs non linear mapping latent vectors response variable neural networks 
technique useful non linear relationships exist predictor variables response variable 
methods linear covariances perform non linear relationships 
example relationship predictors response variable highly linear methods needed 
neural network models neural networks popular devices non linear regression estimation pattern recognition 
non linear non parametric method extremely flexible models inferential sensing 
disadvantages natural extension advantages 
challenging problems correct application neural networks regularization 
nonlinear regularization difficult problem linear counterpart general solution due nonlinear error propagation existence multiple local minima multiple solutions error surface 
linear regression models minima 
multiple local minima training procedure may find solutions meets error criteria 
solutions different generalization properties evaluated 
generalization ability model respond correctly inputs training 
solutions provided neural networks depend number factors including weight initialization network architecture stopping criteria training algorithm 
insure consistent inferential measurements neural network solution invariant conditions 
able get consistency different conditions need estimate reliability inference 
number methods proposed guarantee stability consistency neural network solutions 
popular methods evaluated training weight decay levenberg marquardt training algorithm cross validation training small weight initialization bayesian regularization 
different neural network methods tested data set determine dependence initial conditions network architecture 
neural network study standard multilayer perceptron mlp single nonlinear hidden layer linear output layer 
mlp regularization trained mean squared error ordinary gradient decent neural network baseline comparison 
network architecture hidden neurons hyperbolic tangent activation function 
training performed times different initial weights limited number epochs equal 
mean value estimated drift standard deviation klb hour klb hour correspondingly 
changes neural network architecture error goal improve training results 
probability density function pdf drift estimation check point network shown 
obvious pdf extremely broad ranging negative positive values drift estimation 
clearly unstable performance benchmark neural network unacceptable task inferential sensing 
investigate neural network regularization methods 
fig 

drift dependence random start mlp trained gradient decent 
levenberg marquardt training levenberg marquardt lm algorithm inherent regularization properties discussed 
check regularization properties lm algorithm trained number neural networks times starting different initial conditions different numbers hidden neurons different numbers training points 
results training tables 
mean predicted drift denotes mean value predicted drift calculated runs different initial weights architecture number training points 
seen tables mean value drift estimation depends slightly neural network architecture largely number training points 
number training points increased network constrained output moves stable correct estimate 
shows importance complete training set 
large standard deviation small training sets shows neural network inference unstable different random starts 
noted variance drift inference substantially reduced compared ordinary gradient decent solution 
lm algorithm able reduce training error orders magnitude ordinary gradient decent 
table 
lm method number training patterns number hidden neurons mean predicted drift standard deviation predicted drift table 
lm method number training patterns number hidden neurons mean predicted drift standard deviation predicted drift table 
lm method number training patterns number hidden neurons mean predicted drift standard deviation predicted drift training weight decay instability neural network inference attributed redundant flexibility neural networks function approximators collinearity training data 
known hertz krogh palmer development neural networks generalization capabilities requires type complexity control imposed neural net 
control implemented controlling magnitude weights biases neural net 
shown david mackay complexity function implemented neural network depends magnitude weights larger weights complex function neural network approximate 
proven neural networks sufficiently large numbers hidden neurons approximate arbitrary complex function degree accuracy hornick 
complexity controls neural network approximate noise artifacts training data general entire data set 
results predictive model poor generalization capabilities 
constraining neural network complexity subtle compromise fitting data keeping model simple possible resolved 
accordance occam razor states simple model preferred complex provided consistent data 
easiest way restrict complexity neural net add penalty term square error function 
penalty term usually sum squares network weights biases analogous ridge regression 
penalized functional total performance mean squares error term penalty term regularization parameter controls trade rationale type regularization mapping smooth non oscillating second term formula penalizes 
parameter chosen prior application method regularization 
choice difficult problem linear models difficult models nonlinear due multiple error minima 
parameter defined amount noise data usually known priori 
value parameter roughly approximated analysis eigenvalues hessian matrix neural network bishop 
results neural network training weight decay shown tables different numbers hidden neurons different regularization parameters 
number hidden neurons network reinitialized times check dependence random starts 
dependence summarized standard deviation predicted drift value 
table regularization parameter appropriate giving preference penalization data fitting result network large standard deviation prediction substantially different mean values different architectures 
increasing regularization parameter helps reduce standard deviation mean value predicted drift consistent results robust model linear regression lm trained neural net 
table 
training weight decay function lm regularization parameter number hidden neurons mean predicted drift std predicted drift table 
training weight decay function lm regularization parameter number hidden neurons mean predicted drift std predicted drift table training weight decay function lm regularization parameter number hidden neurons mean predicted drift std predicted drift cross validation training popular method neural network regularization termed early stopping cross validation 
regularization method largely ad hoc dividing training data sets training set validation set 
basic idea training neural network begins learn noise spurious structures training data 
training neural network learns structure data point neural net begins learn pseudo structure noise providing rough mapping 
validation set provide independent test set verifying trained network generalize previously unseen data 
results regularization method shown table 
test number training patterns chosen number validation patterns 
table seen predicted drift depends number hidden neurons random initialization 
standard deviation relatively small large networks hidden neurons unacceptably big small networks 
obvious limitation type regularization final solution depends initial start path system evolved final state 
addition requires division data sets decreasing amount data available training case scarce data serious limitation 
obvious advantage kind complexity control simplicity 
table 
training cross validation lm number hidden neurons mean predicted drift std predicted drift regularization small weight initialization lesser known type regularization neural networks regularization initialization 
method initial weights neural network set small values forcing neural network search smaller area error surface minimizing dependence random starts 
kind regularization ad hoc setting initial weights small values partly specifies solution 
results application kind regularization shown tables 
tables show reach consistent results small standard deviation specific training set mean values different different training sets changing data changes error surface 
inconsistency respect training set method unreliable inferential sensing highly collinear data 
table 
hidden nonlinear neurons random starts training patterns weights biases initialization mean predicted drift standard deviation predicted drift table 
hidden nonlinear neurons random starts training patterns weights biases initialization mean predicted drift standard deviation predicted drift bayesian regularization advanced method neural network regularization bayesian regularization david mackay 
bayesian point view neural network training different traditional views 
traditional methods variations maximum likelihood principle states variety possible models pick probable respect observed data 
maximum likelihood principle considers model parameters unknown fixed values tries estimate parameters available data providing set parameters claimed generated observed data 
conventional neural network training single set weights inference 
contrast maximum likelihood principle bayesian approach considers model parameters random variables having priori distribution 
having priori distribution bayesian inference uses application bayes theorem modify prior distribution produce posterior distribution depends prior information current data 
key success bayesian training correct choice prior distributions 
considered fatal flaw bayesian inference subjective nature choice 
dealing ill posed problems prior information data solution 
having chosen prior distribution weights bayesian training gives rise posterior weight distributions turn gives rise distribution output values inference new data 
mean output distribution considered inferred value 
results application bayesian regularization problem drift detection shown table 
number training points method 
seen table bayesian regularization method generates neural network depend random weight initialization 
shown standard deviation drift prediction due different random starts zero 
inference method drastically depends initial number hidden neurons network architecture provides inconsistent estimations drift mean value 
table 
bayesian regularization number hidden neurons predicted drift std neural network methods summary neural networks powerful flexible tool non parametric modeling inference regularization consistency highly collinear data challenging due inherent local minima 
neural networks provide inconsistent results non interpretable non repeatable 
results show stability respect initial conditions attained bayesian regularization 
hand bayesian training provided drastically different drift estimations different network architectures 
serious limitations bayesian approach neural network training computational burden limits application small amounts data small networks 
approach line system probably practical 
simpler network regularization methods cross validation weight decay provide reasonable trade stability computational time stabilize solutions 
levenberg marquardt algorithm proved stable technique stable linear methods 
stability algorithm explained built regularization properties help dampen high frequency noise weights vicinity solution 
regularization initialization new technique validity evaluated rigorously theoretical practical aspects results show reduce dependence initial conditions effective computational point view require additional computational efforts 
data driven inferential methods provide unstable estimation predictor variables collinear 
instability caused ill conditioning data matrix manifests non smooth squares solutions 
solution sensitive noise data 
regularization methods combat instability provides consistent drift predictions 
data study extremely collinear relationship predictors response variable feedwater flow linear 
data set chosen worst case example results usual data sets significantly better 
linear non linear techniques applied predict feedwater flow drift months fuel cycle 
results techniques summarized table 
table shows techniques sort regularization results evidenced large standard deviations predictions 
shows linear regularization techniques give consistent results 
problem linear nonlinear techniques demonstrate potential problems neural networks solve ill posed problems 
non linear drift predictions somewhat dependent neural network model architecture evidenced different mean drifts 
adverse feature ill posed problems dealt knowing solution priori 
solution feedwater drift detection problem known due availability redundant ultrasonic measurement device plants 
inferential sensing problems concerned measuring level drift ascertaining sensor drifted efficient maintenance planned 
example study difficult problem inferential sensing problems 
table 
results summary method type model mean drift standard deviation linear regression linear ridge regression linear linear pls non linear nn gradient descent non linear nn lm training non linear nn weight decay non linear nn cross validation non linear nn small weight initialization non linear nn bayesian regularization non linear techniques perform linear techniques substantial non linearities mapping non linear techniques ability perform mapping linear techniques limited giving best linear fit 
neural network method regularization results totally unusable 
lm training algorithm performed large amounts training data 
weight decay method unstable 
cross validation method may performed equal lm algorithm additional training data available lm training algorithm 
small weight initialization limited solution search space consistently solution solution dependent training set equal actual drift computed linear methods 
lastly bayesian regularization consistent dependent architecture 
non linear techniques disadvantages come additional flexibility 
neural network techniques solve ill conditioned problems done caution knowledge potential problems 
importance thorough validation testing techniques 
problems properly documented literature giving neural network users false sense security 
proper regularization techniques linear techniques accurately consistently predict feedwater flow drift accurately 
non linear techniques proper architecture selection training set selection regularization validation testing incorporated network design 
lastly noted example worst case example ill conditioned prediction inferential problems principal factors involved constructing reliable prediction 
basseville 

detecting changes signals systems survey automatica 
bishop christopher 
neural networks pattern recognition oxford university press 
black uhrig hines system modeling instrument calibration verification non linear state estimation technique published proceedings maintenance reliability conference knoxville tn may 
cherkassky muller learning data john wiley sons 
desai ray fault detection isolation methodology proc 
th conf 
decision control 
feedwater flow measurement nuclear power generation stations tr electric power research institute 
efron 
bootstrap resampling plans 
philadelphia penn society industrial applied mathematics 
ake regularization tools training large scale neural networks technical report department computing science university sweden 

applications theory matrices interscience publishers new york 
kowalski partial squares regression tutorial acta pp 

survey model failure detection isolation complex plants ieee control systems magazine pp 


fault detection nuclear power plants applying sequential probability ratio test mar residual time series proceedings ai frontiers innovative computing nuclear industry jackson wyoming 
golub heath wahba generalized cross validation method choosing ridge parameter technometrics pp 

golub van loan matrix computations third edition johns hopkins university press baltimore md berg model fault detection diagnosis process plant operation proceedings seventh power plant dynamics control testing symposium knoxville tennessee 
gross singer herzog van application model fault detection system nuclear plant signals proc 
th intl 
conf 
applications power systems seoul korea 
gull bayesian inductive inference maximum entropy maximum entropy bayesian methods science engineering vol erickson smith eds pp kluwer dordrecht 
hadamard 
lectures cauchy problem linear partial differential equations yale university press new haven 
hansen 
regularization truncated bit pp 

hansen 
discrete picard condition discrete ill posed problems bit pp 

hansen 
regularization tools matlab package analysis solution discrete problems numer 
algorithms vol pp 

hansen 
test matrices regularization methods siam sci 
comput 
vol 
pp 

hansen 
rank deficient discrete ill posed problems 
numerical aspects linear inversion siam philadelphia 
hansen 
analysis discrete ill posed problems means curve siam review vol 
pp 

hardy miller hajek model approach malfunction isolation interacting systems proceedings th power plant control testing symposium knoxville tennessee 
hertz krogh palmer theory neural computation addison wesley 
hines signal validation adaptive neural fuzzy inference system nuclear technology august pp 

hines uhrig autoassociative neural networks signal validation journal intelligent robotic systems kluwer academic press 
hines uhrig black xu evaluation instrument calibration monitoring artificial neural networks published proceedings american nuclear society winter meeting albuquerque nm november 
ridge regression biased estimation nonorthogonal problems technometrics pp 
integrated signal validation system nuclear power plants nuclear technology vol 
pp 

fuzzy logic power plant signal validation proceedings th power plant control testing symposium knoxville tennessee may 
hornick stinchcombe white multilayer feedforward networks universal approximators neural networks pp 
pls regression methods journal chemometrics vol 
pp 

uhrig methodology performing virtual measurements nuclear reactor system transactions american nuclear society international conference years controlled nuclear chain reaction past chicago illinois 

process fault detection modeling estimation methods survey automatica vol 

jolliffe 
principal components analysis springer verlag new york 
belle monitoring feedwater flow rate component thermal performance water reactors means artificial neural networks nuclear technology vol 


detection sensor failures nuclear plants analytical redundancy transactions american nuclear society 
kramer 
techniques quantitative analysis marcel dekker 
mackay david 
bayesian interpolation neural computation vol 
pp 
mackay david 
practical bayesian framework backpropagation networks neural computation vol 
pp mackay david 
bayesian methods neural networks theory applications course notes neural network summer school 
university cambridge programme industry 

methods solving incorrectly posed problems springer verlag new york 
nuclear news flow rate causes unneeded nucl 
news feb 
patton chen review parity space approaches fault diagnosis ifac imacs conference baden baden germany 
qin nonlinear pls modeling neural networks computers chem 
engng vol 
pp 
uhrig development expert system signal validation topical report prepared department energy university tennessee knoxville doe ne 
singer gross herzog king model nuclear power plant monitoring fault detection theoretical foundations proc 
th intl 
conf 
intelligent systems applications power systems seoul korea 
tikhonov 
solution incorrectly formulated problems regularization method doklady akad 
nauk ussr pp 


expert systems power plant applications overview proceedings th power plant control testing symposium knoxville tennessee may pp 
uhrig 
artificial neural networks potential applications nuclear power plants proceedings conference structural mechanics reactor technology germany 
uhrig hines nelson integration artificial intelligence systems monitoring diagnostic system nuclear power plants special meeting instrumentation control research center norway march 
application neural networks sensor validation plant monitoring nuclear technology vol 

willsky 
survey design methods failure detection dynamic systems automatica vol 

flow rate causes unneeded nucl 
news feb 
appendix var 
num 
description range units speed rpm high level percent feedwater pump speed rpm linear power ch ni percent heater inlet cond temp heater outlet cond temp 
inlet cond temp heater inlet fw temp discharge temp temp heater outlet fw temp steam gen inlet fw temp heater outlet fw temp steam gen level op percent steam gen level full inches steam gen level start inches steam gen inlet fw temp steam gen level start inches steam gen inlet fw temp steam gen inlet fw temp cold press 
cold press 
cold press 
extr lp turb pressure 
