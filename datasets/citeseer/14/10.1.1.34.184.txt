mining surprising patterns temporal description length soumen chakrabarti sunita sarawagi byron dom ibm almaden research center harry road san jose ca sunita almaden ibm com propose new notion surprising temporal patterns market basket data algorithms find patterns 
distinct finding frequent patterns addressed common mining literature 
argue analyst familiar prevalent patterns data greatest incremental benefit changes relationship item frequencies time 
simple measure surprise extent departure model estimated standard multivariate time series analysis 
unfortunately estimation involves models smoothing windows parameters optimal choices vary dramatically application 
contrast propose precise characterization surprise number bits basket sequence encoded carefully chosen coding scheme 
scheme inexpensive encode sequences itemsets steady known correlation items 
conversely sequence large code length hints possibly surprising correlation 
notion surprise desirable property score set items offset surprising user may know marginal distribution proper subset 
parameters support confidence smoothing windows need estimated specified user 
experimented real life market basket data 
algorithm successfully rejected large number frequent sets items bore obvious steady complementary relations cereal milk 
algorithm itemsets showed statistically strong fluctuations correlation time 
items obviously complementary roles 
data warehousing technology enabled corporations store huge amounts data data mining major motivating application 
large data sources suitable mining growing number size literally passing moment 
data source collected years decades prevalent patterns broad regularities known domain experts permission copy fee part material granted provided copies distributed direct commercial advantage vldb copyright notice title publication date appear notice copying permission large data base endowment 
copy republish requires fee special permission endowment 
proceedings th vldb conference new york usa surprising patterns novel unexpected non trivial explain 
may patterns types statistically significant 
broad consensus success data mining depend critically ability go obvious patterns find novel useful patterns :10.1.1.33.4036
results mining large lack novelty making overwhelming analyst sieve 
domain expert familiar application domain satisfied merely prevalent patterns presumably exploiting extent possible competition knows patterns 
payoff data mining lies surprising second order phenomena 
ill defined vague notion domain knowledge gets way separating novel patterns prevalent ones 
principle propose various defined notions domain knowledge 
analyst mental multivariate distribution attributes system reports certain class patterns reduce distance mental distribution true distribution quickest rate 
course impossible implement real system 
contributions proposes explores notion analysis variation inter item correlations time approximate role domain knowledge search interesting patterns 
concentrate problem boolean market basket data 
set items declared interesting necessarily absolute support exceeds user defined threshold relationship items changes time 
furthermore support itemset changes time considered interesting changes totally explained changes support smaller subsets items 
develop notion interest number bits needed encode itemset sequence specific coding scheme design 
scheme takes relatively bits encode sequences itemsets steady correlation items known 
conversely sequence large code length relative baseline unconstrained coding scheme hints possibly surprising correlation 
surprise value itemset related difference ratio constrained unconstrained code lengths 
subroutine computation analysis produces formal page information theoretic sense best segmentation time interesting itemsets relationship items changing 
segmentation technique independent interest 
potentially leads improved accuracy data understanding 
data collected long intervals generated processes drifting parameters 
single model time may lead poor models predictive accuracy 
scenario large volume data available mining hurts helps 
analysis builds models optimal segments time avoiding problem 
applications include segmentation data supervised learning time discover interesting changes decision boundaries time passes 
comparison approaches prefer somewhat involved analysis simple statistical tests 
local estimate support needs time window small window leads poor statistical confidence estimated parameters large window may skip surprising segment average larger uninteresting 
window size smoothing function test statistic surprise threshold critical choices best highly experienced art 
similar statements hold approaches frequency transformations 
wish avoid need tuning base assumed property baskets independently drawn possibly drifting distribution 
distinction statistical approaches discussed detail 
compared standard mining algorithms core operations pattern generation tuple counting analyses involves expensive computations 
part describe techniques greatly reduce computation cost complexity algorithms remains 
believe stand justified view urgency mining tools ignore mundane rules discover novel ones :10.1.1.33.4036
time saved computation may spent analysts discarding rules inspection 
organization review information theory basics describe model sequences market baskets 
components analysis find surprising patterns 
give overview fit 
techniques improving performance method 
report experience real life market basket data sets 
related alternative approaches reviewed concluding remarks 
modeling sequences market baskets information theory basics underlying premise data described bits simple predictable interesting surprising 
information theory gives general way construct model data regarding compression problem 
imagine sender wishing send data receiver bits possible 
suitable model transmitted consuming bits 
typically chosen class models associated parameter space agreed advance 
example may transmit mean variance normal distribution reason believe data follows normal distribution 
second data encoded suitably sent denote xjm bits information 
knows data compression may greatly enhanced xjm may smaller 
specifically suppose pr xjm probability specific data value model shannon classical information theorem states data encoded xjm gamma log pr xjm bits 
note motivated pick coding scheme minimizes xjm 
initial choice parameters values usually maximize pr xjm 
classical minimum description length mdl principle argues generally lead models capture exactly regularities data avoid fitting random deviations 
example suppose bit strings identical length asked identify complex information 
reasonable approach compress universal lempel ziv lz compression scheme select larger compressed strings 
lz yields asymptotically optimal compression produce model explanation data 
data finite sequence floating point numbers drawn normal distribution lz poor choice 
data model basket sequences case raw data sequence market baskets basket set items ordered time focus fixed itemset items 
basket contains possible subsets items 
reasoning items regard basket outcome tossing sided coin better called die presence absence th item encoded th bit toss outcome written bit number gamma 
consider case sequence tosses baskets generated coin 
model associates face coin probability 
items model terms denotes probability items absent denotes probability absent second 
notation generalizes items obvious way 
capture drift process underlying baskets assume random process generates data follows set coins various face page probabilities unknown observers 
picks coin arbitrarily tosses arbitrary number trials moving coin 
coin defines segment time itemset distribution stationary 
observe sequence outcomes 
simplicity efficiency pick stationary models segment ones parameters gradually drift time 
spirit simple decision boundaries classification regression trees 
complicated segment models larger complexity 
summarize model represent itemset sequence set segments segment generated sided coin 
model cost parts parameter cost segment includes cost encoding coin biases segmentation cost includes cost encoding number segments boundary segment 
data cost estimated applying shannon theorem segment summing log probabilities segments assumed independent 
segmentation basket sequences model introduced basis defining interest measure itemset 
develop definition steps 
discuss find best segmentation itemset model described 
call unconstrained segmentation problem 
unconstrained model provide best compression ignores factors 
modeling itemset user knowledge gamma itemsets exploited 
second individual parameters model change segment segment relationship parameters may remain constant time 
second stage incorporate factors constrained segmentation problem 
explore means comparing complexity diverse itemsets segmentation 
difference ratio constrained segmentation large regard itemset surprising 
unconstrained segmentation discuss simpler case segmenting single item discuss generalizations higher dimensions 
single item store item basket thought outcome single sided coin toss item purchased 
sequence tosses find segmentation 
simple question may glance 
extreme assume coins bias model data fits perfectly probability generating sequence 
extreme posit coin model probability generating sequence may low sequence generated coins diverse biases 
try estimating biases windows trials merge split 
danger picking windows 
small windows give bias estimates sufficient statistical confidence large windows may skip interesting small segment bias remarkably different 
turns notion correct segmentation sequence defined terms mdl defined earlier method finding segmentation 
claim segmentation coin parameters minimizes xjm computed time 
proof 
construct graph nodes directed edges trials heads tails observed values collectively called 
edge assigned cost represents model sum parameter segmentation cost data cost encoding tosses excluded included 
calculating cost involves steps estimating model parameters need find model parameters 
values parameters optimize data fit maximum likelihood ml estimates calculated data gamma 
finding data encoding cost parameters calculate data encoding cost segment shannon theorem xjm gamma log gamma log parameter segment 
finding parameter encoding cost need transmit knowing 
note maximum likelihood estimate take values need send log bits parameters real numbers 
finding segmentation cost just encoding boundaries segment number costing log bits segments 
coins just need log gamma gamma gamma delta bits close log find shortest path node note jej jv log jv time 
edge shortest path segment optimal segmentation 
capability find exact optimum important baseline computation takes avoid problems parameters approaching zero laplace rule ignore detail description 
page linear time 
explore means greatly reduce computation cost practice producing segmentation near optimal quality 
larger itemsets itemsets model sequence generated sided coin 
apply shortest path procedure itemset case find best segmentation 
difference detail computing edge weights corresponding segment 
consider case items 
suppose set baskets items items item second second 
induce parameters rs 
data encoding cost direct extension item case viz gamma rs log rs way estimate model parameters rs encode costs changes item case models choose 
model corresponds case items independent case parameters delta delta needed specify coin rest calculated delta delta delta gamma parameter cost case log log second model corresponds items dependent case need parameters specify sided coin 
model cost logarithm number ways trials divided outcomes log gamma gamma delta log models get maximum likelihood estimates required parameters data parameters evaluate data cost 
segmentation cost item case 
evaluate total cost edge model types take smaller cost 
number possible models grows number items 
itemsets depending marginals related possibilities 
simplest case way itemsets independent case parameters needed 
general case way marginals correlated case parameters needed 
items cases form independent independent dependent cases form dependent dependent independent 
parameters suffice cases 
similar enumerations generated larger itemsets 
general recipe calculating edge weights itemset follows possible model 
estimate model parameters corresponding counts data 
example independent model itemset case estimate delta delta delta delta 
estimate dimensional parameters model parameter calculated 
example independent model dimensions estimate delta delta delta gamma general parameters may closed form solutions estimating lower dimensional probabilities 
discuss general procedure 

find data encoding cost dimensional parameters 
straightforward generalization itemset case gamma rs log rs 
find parameter encoding cost independent parameters model 
model parameters log estimate cases refinements possible discussed 
find segmentation cost 

find total cost sum data parameter segmentation cost 
select model smallest total cost assign cost edge 
number models searched increase exponentially number dimensions 
greatly reduce number simple heuristic 
gamma dimensional itemset particular model best dimensional itemset start model consider generalizations model 
example itemset dependent model better independent model itemset consider model independent 
estimating dimensional probabilities marginals fewer dimensions general possible get closed form formulas estimation 
consider instance itemset case 
cases discussed earlier yield closed form solutions instance item pairs say dependent pairs independent calculate expected value delta delta delta problem case way atomic probabilities correlated 
case explicit formula computing expected support observed marginals 
simple iterative procedures converge maximum likelihood ml estimate iteration cases direct formulas exist iterative process yield iteration answer closed form formulas exist page 
describe classical algorithm called bartlett method finding probability itemset marginal probability subsets 
bartlett iterative procedure simplicity discuss process dimensions 
input consists twelve way marginals pairs items delta delta delta delta item pair pairs 
process converges values way probabilities restrictive maximal likelihood estimates dimensional probabilities preserving page specified values way marginal probabilities 
delta 
process starts assigning starting value way probabilities 
step iteration scales way probabilities come closer observed marginals 
repeats process suitable error threshold reached 
initialize ijk error iterations high twelve way marginals update way probabilities fit marginal better delta update delta delta delta gamma claim iteration guaranteed converge measure surprise single segmentation section propose answers question detect sequence baskets interesting 
intuitively want answer properties ffl itemset surprising simply larger support pre specified quantity support significantly different expected marginal supports proper subsets 
similar concerns raised brin 
ffl order produce ranking surprise measure measure reflect uniform way different itemsets diverse absolute support complexity variation correlation time 
satisfy requirement need method calculating expected support itemset support subsets 
estimate expected support single items 
lacking prior knowledge data assume items equally prior knowledge easily integrated 
expected support item data ratio average transaction length number items 
itemsets observed marginal supports delta delta individual items best maximum likelihood estimate itemset delta delta derived assumption itemsets independent 
actual support ae itemset interesting 
natural choice measure surprise delta delta delta delta delta delta larger itemsets method discussed calculate expected support lower dimensional marginals 
instance way marginals dependent iterative procedure discussed earlier calculate expected support 
notion generalizes previous estimating expected value brin simplifying assumption way itemsets interesting way marginals dependent 
single segmentation recall scheme compressing data model chosen certain model class 
unconstrained case sequence coins 
basket sequences perform unconstrained segmentation coin sequences complex 
direct formal way comparing complexity coin sequences appears elusive 
example large number unconstrained segments itemset may surprising segmentation caused items items independent 
assume analyst assumes constant proved encode data model restricted model class containing possible sequences coins coins value say value computed data time 
call single segmentation 
roughly speaking itemsets large code length model class depart analyst prior expectation interesting 
approximate solution general consider itemset 
take entire data find independent dependent model fits data better procedure 
call global model independent model global value costing just bit estimate value delta delta setting shortest path instance assign coding cost edge 
segmentation parameter costs computed 
compute data cost estimate coin face probabilities observed record counts records time range 
calculate observed marginals delta delta delta delta give parameters coin segment delta delta delta gamma resulting coin inconsistent face probability zero declare edge having infinite cost 
easy see feasible path shortest path problem viz path remains unaffected approximation 
discuss elaborate exact procedure 
coin estimate probability observed data segment pr hj log 
add data cost model cost evaluated various cases described take minimum edge cost 
page procedure remains essentially unchanged larger itemsets 
itemset find time best model complete model involving dimensional probabilities parameters expected value delta delta delta computed bartlett iteration gamma dimensional marginals global calculated delta delta delta delta delta delta segment compute observed gamma marginals delta delta delta records interval 
invoke iterative algorithm observed marginals obtain delta delta delta compute discussed item case 
coin inconsistent mark edge infeasible looked kind approximation 
complete model apply model segments compute edge cost unconstrained case 
exact solution itemsets important special case itemsets need approximate exact characterization code length 
envision system precise analysis case approximations noted interesting patterns patterns explained way marginals 
potential difficulty discarding inconsistent coins may exist consistent coin satisfying global constraint exactly fit observed marginals reasonably high data probability 
compute directly observed data cast constrained optimization problem 
want assign consistent values variables obey constraints maximize data probability minimize data cost 
observed numbers heads types solve constrained non linear optimization problem unknowns quantities numerically known max log subject ae rs solved variety iterative techniques 
simple steepest ascent algorithm 
turn question optimal way encoding parameters 
produce particular coding scheme argue better 
scheme omit argument lack space 
recall transmitted segments negligible cost 
segments fixed value 
segment section skipped reading 
send parameters 
noted earlier interpreted real numbers model assume discrete number configurations send parameters integers 
pick integers oe delta oe delta send 
intent receiver compute model delta oe delta delta oe delta oe delta oe delta know rs ensure oe rs consistent specifically hold delta delta delta delta replacing oe simplifying get oe delta oe delta oe delta oe delta oe delta oe delta apart standard constraints oe number bits required encode oe delta oe delta log number ways chosen subject constraints 
boundaries smooth continuous analysis approximate number choices volume feasibility region constraints 
cases 
case easy verify constraints oe delta oe delta sufficient yield consistent sided coin 
constraint kicks 
simplifying notation need find area region bounded xy evaluates gamma gamma dx gamma ln gamma briefly discuss computational problem carrying exact analysis larger itemsets 
constraint equation volume evaluation equation written closed form general 
numerical monte carlo integration may expensive 
piecewise constant segmentation files diverse length reasonable compress compare absolute sizes compressed files pick largest complex 
compression ratio better indicator complexity large ratio original final size implies complexity 
similarly code length various itemsets constant segmentation directly comparable baseline code length needed itemset 
option unconstrained code length 
may smaller larger constrained code length 
larger item situation example unconstrained segmentation induced sudden changes marginal probability value segments joint probability tracks page changes 
unconstrained segmentation assign new parameter coin segments 
contrast single segmentation paying parameters segment paying 
conversely unconstrained codelength smaller constrained model poor fit data 
positive differences interesting negative differences zero significant notch scale 
option baseline piecewise constant segmentation 
relaxation single model class 
piecewise constant model specifies outer segmentation 
outer segment specifies single value 
outer segment specifies inner segmentation assigned coin 
inner coins assigned outer segment value associated outer segment 
necessary parameters inner coins specified single case 
finding piecewise constant segmentation simple previous building blocks 
set shortest path problem 
assign cost edge run constant segmentation algorithm segment data 
avoid invocations constant procedure practice heuristically restrict potential segmentation points unconstrained segmentation 
efficiency simplicity unconstrained baseline experiments 
algorithm summary previous section described building blocks technique 
put show picture system 
proceed small large itemsets usual 
surprise measure conditioned marginals variation time design pruning strategies different simple fixed support filtering commonly bottom itemset search algorithms minimum support 
outline algorithm takes form 
item 
select items possibly segment 
pruning criterion discussed 
find unconstrained segments single items 
find code length global coin start finish 
trivial constrained segmentation problem single items 

order items difference ratio relative difference code lengths 
display items user 
items 
marginals proper subsets compute maximum support itemset time 
call support envelope 

envelope estimate itemset possibly segment 
eliminate itemset 
property monotonic respect itemset containment itemset segment superset 
pruning helps 

compute segmentations itemset piecewise constant segmentation single segmentation 

difference ratio relative difference code lengths order itemsets surprise value 
computational issues discuss important performance issues section control itemset expansion pruning compute near optimal segmentations near linear time 
pruning criteria prune itemset support sequence time close zero possible get segment sequence sequence 
superior absolute aggregated support pruning differentiate sequences aggregated support sequences ones concentrated contiguous segments ones spread uniformly entire time base 
clearly interesting 
economic reasons interested itemsets support value include additional filtering step 
property absolute support pruning monotonic meaning itemset pruned supersets pruned 
apply apriori technique pruning itemsets subsets pruned candidate generation phase 
addition prune looking estimated upper envelope support sequence itemset 
upper envelope calculated candidate generation minimum supports immediate subsets point time 
upper envelope meets pruning criteria drop itemset 
claim sequence length pruned fraction sequence estimate coin bias large satisfy log log nf gamma gamma log gamma gamma log gamma pf proof 
sketch consider sequence best case segments identical segment time zero 
sequence find coding costs assuming entire sequence single segment assuming separate segment 
higher data encoding cost extra segment page pays higher parameter segmentation cost see 
costs expressed terms refer parameters sequence 
inequality find condition 
fast shortest path approximations second important performance issue concerns shortest path computation 
key subroutine algorithms 
data mining applications millions transactions clearly unacceptable quadratic complexity time 
section study arrest quadratic growth get segmentation 
immediate way cut running time pre aggregate data 
developing algorithms dealing market basket reading random variable data sets pre aggregated daily weekly levels 
data clearly sense segment finer day week 
fact input data aggregated sequence optimality preserved run lengths zeroes ones 
care may needed aggregate coarser levels 
larger chunks cut running time may gloss narrow interesting segments 
simple fixed size chunking pre aggregation followed single shortest path computation achieve goal 
heuristic suppose transactions nodes graph 
fix constant ffl decided break graph gammaffl chunks having ffl nodes 
solve gammaffl instances shortest path chunks ffl time total time ffl 
nodes shortest path chunk called chosen 
heuristically hope chunks completely contained global optimal segment edges local shortest path chunk 
case chunk size large 
construct version original graph 
nodes edges go chosen node 
heuristic assumption holds graph gammaffl chosen nodes final shortest path run takes gamma ffl time 
total time ffl gamma ffl smallest value ffl assuming course choice yields small number chosen nodes 
report shortest path sparse graph 
analysis 
quality approximate segmentation compared optimal 
claim approximate shortest path corresponding code length twice optimal 
proof 
skipped 
consider chunk having nodes global optimal shortest path edges chunk local shortest path skips avoiding illustration approximate shortest paths 

note necessarily leftmost rightmost nodes belong nodes 
note parameter data encoding costs 
consider total cost path cost path preferred local shortest path 
compare optimal optimal 
edges global segmentation cost log parameter data costs optimal maximum likelihood model data costs respectively 
observe ml estimates 
similarly compare cost optimal cost approximate 
approximate cost larger 
optimal large factor 
consider optimal path may interact chunk may touch node case 
enters node leaves node 
nodes chosen adjustment construct path passing chosen nodes times optimal cost 
practice ffl small optimal path avoids chunks cost increase quite negligible experiments 
roughly proportional number hops optimal path 
exploiting marginal segmentations source information exploited faster segmentation segmentation marginals 
advantage marginals fixed width chunking statistically swings joint distribution explained marginals 
note heuristic analysis depend initial chunks derived 
experiments experience developed prototype studied real life market basket data years 
dataset consisted transactions spanning years page epsilon time epsilon effect ffl fast shortest path heuristic 
line shows running time shows quality result 
optimal uses bits heuristic uses bits quantity plotted gamma 

time stamps transaction recorded granularity day 
sequence length 
total number items average length transaction 
dataset consisted transactions spanning period years 
recording granularity day yielding total time points item sequence 
total items average length transaction 
describe performance system evaluate quality output system anecdotal evidence quantitative measures 
performance apparent complexity data analyses may evoke questions practicality 
method works reasonable time 
main potential complexity shortest path computation 
significantly mitigated fast shortest path heuristic 
shows fast approximation cut time shortest path computation 
specifically study effect ffl 
consider plot running time ffl 
evident broad range ffl time taken approximate algorithm smaller optimal algorithm order magnitude 
monotonic small valley near 
small ffl chunks chosen nodes chunk endpoints 
final phase 
larger ffl larger fewer chunks phase chosen nodes second phase 
consider plot error ffl 
plot ratio approximate bits optimal bits minus 
big message error tiny typically 
error shows non monotonicity 
small ffl mdl extremely unwilling posit coin chunk ok tiny chunks straddle optimal segment boundaries 
chunk size increases mdl maintains stand time errors accumulate 
eventually mdl gives increases chosen nodes point final phase picks paths 
summarizing fast heuristic enables analysis execute reasonable time losing accuracy 
quality results section compare quality output algorithm simpler previously known alternatives 
mdl approach discussed 
stat standard statistical approach data aggregated level granularity week default experiments 
regular segment measure value find spread ratio standard deviation mean segments 
order itemsets measure 
experimented measures interest standard deviation spread chi squared values spread standard deviation values 
report comparisons spread measure showed best results 
mdl stat shortcoming method users need specify fixed window size segment sequence 
mdl method find best segmentation automatically 
test hybrid methods mdl approach get best segmentation solving unconstrained segmentation problem 
order itemsets spread segments statistical approach 
correlation approach ignore time simply calculate value entire sequence aggregated single point 
kinds evaluation 
anecdotal evidence top sets output method interesting alternatives explain case 
undertake quantitative evaluation 
consider top items interesting method find positions itemset orderings alternative methods 
call rank order evaluation 
evaluate sharpness selectivity different approaches 
consider fraction total itemsets high values interest measure 
intuitively method small number itemsets high values interest measure better high interest values spread large number itemsets making harder define sharp cut 
call selectivity evaluation 
comparisons consider itemsets size experiments large datasets itemset surprising marginals item subsets 
page stat mdl stat mdl correlation sequences ranked top methods 
axis shows time 
top row shows marginals delta delta averaged weekly window 
middle row shows support items basket lowest row shows values 
anecdotes show various sequences pairs itemsets 
itemset column ranked high stat method mdl stat methods interesting mdl 
second item pair ranked high mdl ranked low approaches appear top ranked list final itemset third column ranked high correlation interesting mdl 
quick look shows fluctuates fair amount cases 
normal 
want separate statistically significant fluctuations random noise 
closer look sequences shows differences 
consider itemset near top stat mdl stat list uninteresting mdl column 
spread values high sequence high peaks caused due small support marginals 
mdl approach robust small marginals having small code lengths 
items turned complementary shirt shorts 
consider itemset picked interesting mdl ranked low methods second column 
ignore short range noise notice increases independent steady range 
change happens gradually result deviation measures find interesting 
items men women shorts 
complementary roles obvious reason dependence items increase time pattern statistically significant 
justified regarding surprising pattern 
itemset ranked high correlation mdl observe fluctuates large constant mean fluctuation small relative compared mdl 
item pair turned pillow cases people routinely buy 
rank order far user concerned exact measure complexity itemset sequences important ordering imposes itemsets 
typical application envision user shown ranked lists surprising itemsets measure single double larger itemsets explained smaller itemsets shown earlier list 
accordingly section compare rankings computed various suggested approaches 
comparing rankings graphs plot ranks assigned mdl axis rank assigned stat mdl stat methods 
reduce clutter random collection itemset pairs plotted 
note strong correlation mdl stat method 
correlation coefficient itemset pairs practically close zero 
better segmentation mdl stat method correlation improves shown second correlation strong te method substitute mdl 
instance zooming grid near origin find top itemsets page mdl list occur top mdl stat list 
stat rank mdl stat rank correlation correlation scatter plots comparing ranks itemsets different methods 
figures axis rank assigned mdl 
sensitivity window sizes stat method 
potential problem stat approach coming window size compute 
show sensitivity result parameter show correlation stat method different values periodicity mdl method 
increase window size week weeks correlation mdl increases drop slightly increase window size 
best correlation achieved mdl find best segmentation indicated extreme point marked opt 
opt window size weeks correlation mdl change correlation mdl approach difference window sizes 
selectivity give measure selectivity different methods filtering interesting itemsets rest 
related sharpness count number itemsets interest threshold falls 
compare measure different methods 
axis shows interest measure method axis shows count number itemsets interest value 
note sharpest gradient obtained mdl method 
total itemsets interest measure greater zero 
number significantly higher methods 
related review related spanning statistics machine learning data mining discuss compares existing approaches 
stat mdl stat mdl corr comparing sharpness selectivity methods 
axis shows interest value method 
axis shows count number itemsets interest value 
statistics 
principle problem permit standard statistical approach involving steps 
decide model time series usually deep domain knowledge process 

choose suitable smoothing window estimate model parameter windows process assumed stationary 
devise confidence test 
various simplifying assumptions normal approximations may stage 

judge window shows shift estimate parameters apply confidence test reporting deviations threshold approach requires user critical choices significant tuning domain may entailed 
approximation may valid application may behave poorly setting 
example saw section different window sizes give different interest rankings 
issues echoed textbooks subject page difficult formulate smoothing give mathematical statistical solution 
practitioner proceed basis general experience intuition smoothing leads estimated trend descriptive analytic explanatory 
explicit probabilistic model method treated fully rigorously terms mathematical statistics 
attempt address issue 
system needs tuning closer needs mining systems deal diverse data 
machine learning 
segmentation problem sense dimensional unsupervised clustering scenario 
similar segmentation problems addressed dom context image mentation rissanen context identifying stretches production short lived items factory ron freund blum chalasani context learning set distributions 
proposed algorithms worse quadratic deal identifying segments drift relationship variables potentially ignoring drifts explained drifts marginals 
data mining 
issue efficiently updating mining results incrementally relatively studied data mining literature 
papers addressed issue discovering interesting patterns time market basket data 
algorithm discovering cyclic associations rules provided user specifies period segment size interest 
lent discuss plot support versus time frequent itemsets queried find interesting trends time 
methodology partition data fixed number segments find support segments provide query interface resulting timeseries discussed querying shapes time series user interface front system 
proposed explored new approach extracting temporally surprising patterns just prevalent patterns market basket databases 
attempt substitute user domain knowledge effectively eliminate patterns known 
minimum description length principle appropriate encoding scheme model class achieve 
domain expertise model selection parameter tuning needed user 
experiments market basket data showed method effective eliminating prevalent obvious itemsets milk cereal extracting itemsets obvious complementary relationship showing statistically strong variation dependence time 
seasonal variations opens avenues exploration 
experiments top ranking itemsets seasonal marginals major reason high ranks 
interesting handle predictable seasonal variation 
current coding scheme extended recognize simplicity regular reuse coin parameters transmitting index known coin past parameters scratch 
incremental mining attention time provides natural framework doing incremental mining 
fold new transactions global estimates support maintain incremental shortest paths integrate new segment data existing segmentation 

rakesh agrawal nguyen ramakrishnan srikant helpful discussions martin van den berg comments manuscript 
andrew goldberg shortest path library starting point system 
agrawal imielinski swami 
database mining performance perspective 
ieee transactions knowledge data engineering december 
agrawal mannila srikant toivonen verkamo 
fast discovery association rules 
fayyad piatetsky shapiro smyth uthurusamy editors advances knowledge discovery data mining chapter pages 
aaai mit press 
agrawal psaila 
active data mining 
proc 
st int conference knowledge discovery databases data mining montreal canada august 
agrawal psaila wimmers zait 
querying shapes histories 
proc 
st int conference large databases zurich switzerland september 
anderson 
statistical analysis time series 
john wiley sons 
bishop fienberg holland 
discrete multivariate analysis theory practice 
mit press 
blum chalasani 
learning switching concepts 
proc 
fifth annual workshop learning theory 
everitt 
analysis contingency tables 
monographs statistics applied probability 
chapman hall second edition 
cheung han ng wong 
maintenance discovered association rules large databases incremental updating techniques 
proc 
int conference data engineering new orleans usa february 
cover thomas 
elements information theory 
john wiley sons 
dom 
mdl estimation small sample sizes including application problem segmenting binary strings bernoulli models 
international conference computer vision pattern recognition cvpr 
ieee computer society june 
longer version ibm research report rj 
fayyad piatetsky shapiro smyth 
data mining knowledge discover overview 
fayyad piatetsky shapiro smyth editors advances knowledge discovery data mining 
aaai mit press 
freund ron 
learning model sequences generated switching distributions 
proceedings eighth annual acm conference computational learning theory colt 
gill murray wright 
practical optimization 
academic press 
klemettinen mannila ronkainen toivonen verkamo 
finding interesting rules large sets discovered association rules 
third international conference information knowledge management pages 

laplace 
philosophical essays probabilities 
springerverlag new york 
translated dale th french edition 
lent agrawal srikant 
discovering trends text databases 
proc 
rd int conference knowledge discovery databases data mining newport beach california august 
ramaswamy silberschatz 
cyclic association rules 
proc 
int conference data engineering 
rissanen 
stochastic complexity statistical inquiry 
world scientific computer science 
rissanen 
failure time prediction 
technical report rj ibm research division almaden research center harry road san jose ca 
silberschatz tuzhilin 
patterns interesting knowledge discovery systems 
ieee transactions knowledge data engineering 
special issue data mining 
silverstein motwani brin 
market baskets generalizing association rules correlations 
sigmod 

data mining fool gold 
dec 
utgoff berkman clouse 
decision tree induction efficient tree restructuring 
machine learning journal oct 
page 
