pvfs parallel file system linux clusters philip walter iii parallel architecture research laboratory clemson university clemson sc usa parl clemson edu robert ross rajeev thakur mathematics computer science division argonne national laboratory argonne il usa mcs anl gov linux clusters matured platforms lowcost high performance parallel computing software packages provide key services emerged especially areas message passing networking 
area devoid support parallel file systems critical highperformance clusters 
developed parallel file system linux clusters called parallel virtual file system pvfs 
pvfs intended high performance parallel file system download tool pursuing research parallel parallel file systems linux clusters 
describe design implementation pvfs performance results chiba city cluster argonne 
provide performance results workload concurrent reads writes various numbers compute nodes nodes request sizes 
performance results mpi io pvfs concurrent read write workload benchmark 
compare performance myrinet network versus fast ethernet network related communication pvfs 
obtained read write bandwidths high mbytes sec myrinet mbytes sec fast ethernet 
supported part mathematical information computational sciences division subprogram office advanced scientific computing research department energy contract eng part national aeronautics space administration research nag 
cluster computing emerged mainstream method parallel computing application domains linux leading pack popular operating system clusters 
researchers continue push limits capabilities clusters new hardware software developed meet cluster computing needs 
particular hardware software message passing matured great deal early days linux cluster computing cases cluster networks rival networks commercial parallel machines 
advances broadened range problems effectively solved clusters 
area commercial parallel machines maintained great advantage parallel file systems 
production quality highperformance parallel file system available linux clusters file system linux clusters large intensive parallel applications 
developed parallel file system linux clusters called parallel virtual file system pvfs potentially fill void 
pvfs number sites argonne national laboratory nasa goddard space flight center oak ridge national laboratory 
researchers pvfs studies 
main objectives developing pvfs 
needed basic software platform pursuing research parallel parallel file systems context linux clusters 
purpose needed stable full featured parallel file system 
second objective meet need paral lel file system linux clusters 
designed pvfs goals mind provide high bandwidth concurrent read write operations multiple processes threads common file 
support multiple apis native pvfs api unix posix api apis mpi io 
common unix shell commands ls cp rm pvfs files 
applications developed unix api able access pvfs files recompiling 
robust scalable 
easy install 
addition firmly committed distributing software open source 
describe designed implemented pvfs meet goals 
performance results pvfs chiba city cluster argonne national laboratory 
performance workload comprising concurrent reads writes native pvfs calls 
results workload functions native pvfs functions 
consider difficult access pattern benchmark 
compare performance myrinet network versus fast ethernet network related communication 
rest organized follows 
section discuss related area parallel file systems 
section describe design implementation pvfs 
performance results discussed section 
section outline plans 
related related parallel distributed file systems divided roughly groups commercial parallel file systems distributed file systems research parallel file systems 
group comprises commercial parallel file systems pfs intel paragon gpfs ibm sp hfs hp exemplar xfs sgi origin 
file systems provide high performance functionality desired intensive applications available specific platforms vendor implemented 
sgi released xfs linux 
sgi developing version xfs clusters called knowledge available linux clusters 
second group comprises distributed file systems nfs afs coda intermezzo xfs gfs 
file systems designed provide distributed access files multiple client machines consistency semantics caching behavior designed accordingly access 
types workloads resulting large parallel scientific applications usually mesh file systems designed distributed access particularly distributed file systems designed high bandwidth concurrent writes parallel applications typically require 
number research projects exist areas parallel parallel file systems ppfs galley 
focuses viewing viewpoint transactions ppfs research focuses adaptive caching prefetching galley looks disk access optimization alternative file organizations 
file systems may freely available research prototypes intended everyday 
pvfs design implementation parallel file system primary goal pvfs provide high speed access file data parallel applications 
addition pvfs provides consistent name space enables user controlled striping data disks different nodes allows existing binaries operate pvfs files need recompiling 
file systems pvfs designed client server system multiple servers called daemons 
daemons typically run separate nodes cluster called nodes disks attached 
pvfs file striped disks nodes 
application processes interact pvfs client library 
pvfs manager daemon handles metadata operations permission checking file creation open close remove operations 
manager participate read write operations client library daemons handle file intervention manager 
clients daemons manager need run different machines 
running different machines may result higher performance 
pvfs primarily user level implementation kernel modifications modules necessary install operate file system 
created linux kernel module simple file manipulation convenient 
issue touched section 
pvfs currently uses tcp internal communication 
result dependent particular message passing library 
pvfs manager metadata single manager daemon responsible storage access metadata pvfs file system 
metadata context file system refers information describing characteristics file permissions owner group important physical distribution file data 
case parallel file system distribution information include file locations disk disk locations cluster 
traditional file system metadata file data stored raw blocks single device parallel file systems distribute data physical devices 
pvfs simplicity chose store file data metadata files existing local file systems directly raw devices 
pvfs files striped set nodes order facilitate parallel access 
specifics file distribution described metadata parameters base node number number nodes stripe size 
parameters ordering nodes file system allow file distribution completely specified 
example metadata fields file pvfs foo table 
field specifies data spread nodes base specifies base node node specifies stripe size unit file divided nodes kbytes 
user set parameters file created pvfs default set values 
table metadata example file pvfs foo 
inode 
base application processes communicate directly pvfs manager tcp performing operations opening creating closing removing files 
application opens file manager returns application locations nodes file data stored 
information allows applications communicate directly nodes file data accessed 
words manager contacted read write operations 
issue development pvfs directory hierarchy pvfs files application processes 
implement directory access functions simply nfs export metadata directory nodes applications run 
provided global name space nodes applications change directories access files name space 
method drawbacks 
forced system administrators mount nfs file system nodes cluster problem large clusters limitations nfs scaling 
second default caching nfs caused problems certain metadata operations 
drawbacks forced reexamine implementation strategy eliminate dependence nfs metadata storage 
done latest version pvfs result nfs longer requirement 
removed dependence nfs trapping system calls related directory access 
mapping routine determines pvfs directory accessed operations redirected pvfs manager 
trapping mechanism extensively pvfs client library described section 
daemons data storage time file system installed user specifies nodes cluster serve nodes 
nodes need distinct compute nodes 
bytes local local local file striping example ordered set pvfs daemons runs nodes 
daemons responsible local disk node storing file data pvfs files 
shows example file pvfs foo distributed pvfs metadata table 
note nodes example file striped nodes starting node metadata file specifies striping 
daemon stores portion pvfs file file local file system node 
name file inode number manager assigned pvfs file example 
mentioned application processes clients open pvfs file pvfs manager informs locations daemons 
clients establish connections daemons directly 
client wishes access file data client library sends descriptor file region accessed daemons holding data region 
daemons determine portions requested region locally perform necessary data transfers 
shows example regions case regularly strided logical partition mapped data available single node 
logical partitions discussed section intersection regions defines call stream 
stream data transferred logical file order network connection 
retaining ordering implicit request allowing underlying stream protocol handle packetization additional overhead incurred control messages application layer 
logical partitioning application physical stripe daemon resulting stream intersection stripe partition stream example application programming interfaces pvfs multiple application programming interfaces apis native api unix posix api mpi io 
apis communication daemons manager handled transparently api implementation 
native api pvfs functions analogous unix posix functions contiguous reads writes 
native api includes partitioned file interface supports simple strided accesses file 
partitioning allows noncontiguous file regions accessed single function call 
concept similar logical file partitioning vesta file views mpi io 
user specify file partition pvfs special ioctl call 
parameters offset gsize stride specify partition shown 
offset parameter defines far file partition begins relative byte file gsize parameter defines size simple strided regions data accessed stride parameter defines distance start consecutive regions 
offset gsize stride partitioning parameters implemented mpi io interface top pvfs implementation mpi io 
designed ported easily new file systems implementing small set functions new file system 
feature enabled mpi io implemented top pvfs short time 
contiguous read write functions pvfs mpi io implementation partitioned file interface pvfs supports subset noncontiguous access patterns possible mpi io 
noncontiguous mpi io accesses implemented top contiguous read write functions optimization called data sieving 
optimization large contiguous requests extracts necessary data 
currently investigating pvfs partitioning interface general support noncontiguous accesses 
pvfs supports regular unix functions read write common unix shell commands ls cp rm 
note file locks implemented 
furthermore existing binaries unix api access pvfs files recompiling 
section describes implemented features 
trapping unix calls system calls low level methods applications interacting kernel example disk network 
calls typically calling wrapper functions implemented standard library handle details passing parameters kernel 
straightforward way trap system calls provide separate library users relink code 
approach example condor system help provide checkpointing applications 
method requires relinking application needs new library 
compiling applications common practice dynamic linking order reduce size executable shared libraries common functions 
side effect type linking executables take advantage new libraries supporting functions recompilation relinking 
method linking pvfs client library trap system calls passed kernel 
provide library system call wrappers loaded standard library linux environment variable ld preload 
result existing binaries access pvfs files recompiling 
shows organization system call mechanism library loaded 
applications call functions library libc turn call system calls wrapper functions implemented libc 
calls pass appropriate values kernel performs desired operations 
shows organization system call mechanism time pvfs client library place 
case libc wrappers replaced pvfs wrappers determine type file operation performed 
file pvfs file pvfs library handle function 
parameters passed actual kernel call 
method trapping unix calls limitations 
call exec destroy state save user space new process able file descriptors referred open pvfs files exec called 
second porting feature new architectures operating systems nontrivial 
appropriate system library calls identified included library 
process repeated apis system libraries change 
example gnu library glibc api constantly changing result libc syscall wrappers library pvfs library kernel pvfs library loaded standard operation application kernel library application pvfs syscall wrappers trapping system calls constantly change code 
linux kernel vfs module trapping technique described provide necessary functionality existing applications pvfs files shortcomings method effort required keep changes library encouraged seek alternative solution 
linux kernel provides necessary hooks adding new file system support loadable modules recompiling kernel 
accordingly implemented module allows pvfs file systems mounted manner similar nfs 
mounted pvfs file system traversed accessed existing binaries just file system 
note performance experiments reported pvfs library kernel module 
performance results performance results pvfs chiba city cluster argonne national laboratory 
cluster configured follows time experiments 
nodes mhz pentium iii processors mbytes ram gbyte quantum atlas iv scsi disk mbits sec intel pro fast ethernet network card operating full duplex mode bit myrinet card revision 
nodes running linux pre 
mpi implementations mpich fast ethernet mpich gm myrinet 
kernel compiled single processor processor machine unused experiments 
nodes nodes available time experiments 
nodes compute nodes nodes pvfs 
quantum atlas iv gbyte disk advertised sustained transfer rate mbytes sec 
performance disk measured bonnie file system benchmark showed write bandwidth mbytes sec read bandwidth mbytes sec accessing mbyte file sequential manner 
write performance measured bonnie slightly higher advertised sustained rates test accessed file sequentially allowing file system caching read ahead write better organize disk accesses 
pvfs currently uses tcp communication measured performance tcp networks cluster 
purpose ttcp test version 
tried buffer sizes kbytes kbytes kbytes ttcp reported bandwidth mbytes sec fast ethernet mbytes sec myrinet 
measure pvfs performance performed experiments grouped categories concurrent reads writes native pvfs calls concurrent reads writes mpi io benchmark 
varied number nodes compute nodes size measured performance fast ethernet myrinet 
default size kbytes experiments 
concurrent read write performance test program parallel mpi program processes perform operations native pvfs interface open new pvfs file common processes concurrently write data blocks disjoint regions file close file reopen simultaneously read data blocks back file close file 
application tasks synchronize operation 
recorded time read write operations node calculating bandwidth maximum time taken processes 
tests compute node wrote read single contiguous region size mbytes number nodes 
example case application processes accessed nodes application task wrote mbytes resulting total file size mbytes 
test repeated times lowest highest values discarded 
average remaining tests value reported 
shows read write performance fast ethernet 
reads bandwidth increased rate approximately mbytes sec compute node mbytes sec nodes mbytes sec nodes mbytes sec nodes 
cases performance remained level approximately compute nodes performance began tail erratic 
nodes performance increased mbytes sec compute nodes began drop 
nodes performance increased quickly attained approximately peak read performance nodes dropped similar manner 
indicates reached limit scalability fast ethernet 
performance similar writes fast ethernet 
bandwidth increased rate approximately mbytes sec compute node node cases reaching peaks mbytes sec mbytes sec mbytes sec respectively utilizing available tcp bandwidth 
cases began tail approximately compute nodes 
similarly nodes performance increased peak mbytes sec leveling nodes obtained better performance 
slower rate increase bandwidth indicates exceeded maximum number sockets efficient service requests client side 
observed significant performance improvements running pvfs code tcp myrinet fast ethernet 
shows results 
read bandwidth increased mbytes sec compute process leveled approximately mbytes sec nodes mbytes sec nodes mbytes sec nodes mbytes sec nodes 
nodes bandwidth reached mbytes sec compute nodes maximum tested size 
writing bandwidth increased rate approximately mbytes sec higher rate measured ttcp 
know exact cause small implementation difference resulted pvfs utilizing slightly higher fraction true myrinet bandwidth ttcp 
performance levelled mbytes sec nodes mbytes sec nodes mbytes sec nodes mbytes sec nodes mbytes sec nodes 
contrast fast ethernet results performance myrinet maintained consistency number compute nodes increased number nodes case nodes compute nodes largest number tested efficiently serviced 
mpi io performance modified test program mpi io calls native pvfs calls 
number nodes fixed number compute nodes varied 
shows performance mpi io native pvfs versions program 
performance versions comparable mpi io added small overhead top native pvfs 
believe overhead reduced careful tuning 
benchmark benchmark nasa ames research center simulates required time stepping flow solver periodically writes solution matrix 
solution matrix distributed processes distribution process responsible disjoint subblocks points cells grid 
solution matrix stored process dimensional arrays number compute nodes read performance ion mb ion mb ion mb ion mb ion mb aggregate bandwidth mbytes sec number compute nodes write performance ion mb ion mb ion mb ion mb ion mb pvfs performance fast ethernet number compute nodes read performance ion mb ion mb ion mb ion mb ion mb aggregate bandwidth mbytes sec number compute nodes write performance ion mb ion mb ion mb ion mb ion mb pvfs performance myrinet number cells process 
arrays dimensional dimension elements distributed 
data stored file order corresponding column major ordering global solution matrix 
access pattern noncontiguous memory file difficult handle efficiently unix posix interface 
full mpi io version benchmark uses mpi derived datatypes describe memory file uses single collective function perform entire implementation mpi io optimizes request merging accesses different processes making large wellformed requests file system 
benchmark obtained nasa ames performs writes 
order measure read bandwidth access pattern modified benchmark perform reads 
ran class problem size uses element array total size mbytes 
number nodes fixed tests run compute nodes benchmark requires number compute nodes perfect square 
table summarizes results 
fast ethernet maximum performance reached compute nodes 
compute nodes smaller granularity access resulted lower performance 
configuration attained peak concurrent read performance peak concurrent write performance mea aggregate bandwidth mbytes sec number compute nodes native read read native write write versus native pvfs performance myrinet nodes table performance mbytes sec nodes class problem size 
compute fast ethernet myrinet nodes read write read write section 
time spent computation communication required merge accesses different processes collective implementation 
merging performance significantly lower numerous small reads writes application 
myrinet maximum performance reached compute nodes 
see benefit high speed network smaller requests resulting compute nodes able attain higher performance 
performance obtained peak performance peak concurrent write performance measured section 
pvfs brings high performance parallel file systems linux clusters testing tuning needed production ready available 
inclusion pvfs support mpi io implementation easy applications written portably mpi io api take advantage available disk subsystems lying dormant linux clusters 
pvfs serves tool enables pursue research various aspects parallel parallel file systems clusters 
outline plans 
limitation pvfs uses tcp communication 
result fast gigabit networks communication performance limited tcp networks usually unsatisfactory 
redesigning pvfs tcp faster communication mechanisms gm st available 
plan design small communication abstraction captures pvfs communication needs implement pvfs top abstraction implement abstraction separately tcp gm 
similar approach known device interface successfully mpich 
performance results particularly cases fast ethernet performance drops suggest tuning needed 
plan instrument pvfs code obtain detailed performance measurements 
data plan investigate performance improved tuning parameters pvfs tcp priori dynamically run time 
plan design general file partitioning interface handle noncontiguous accesses supported mpi io improve client server interface better fit expectations kernel interfaces design new internal description format flexible existing partitioning scheme investigate adding redundancy support develop better scheduling algorithms daemons order better utilize networking resources 
availability source code compiled binaries documentation mailing list information pvfs available pvfs web site www parl clemson edu pvfs 
information source code mpi io implementation available www mcs anl gov 
thomas anderson michael dahlin neefe david patterson drew roselli randolph wang 
serverless network file systems 
proceedings fifteenth acm symposium operating systems principles pages 
acm press december 
rajesh steven don mark davis 
experimental evaluation hewlett packard exemplar file system 
acm sigmetrics performance evaluation review december 
peter 
coda distributed file system 
linux journal june 
peter michael callahan phil schwan 
intermezzo filesystem 
proceedings reilly perl conference august 
tim bray 
bonnie file system benchmark 
www com bonnie 
bruno 
implementing beam warming method hypercube 
proceedings third conference hypercube concurrent computers applications january 
chiba city argonne scalable cluster 
www mcs anl gov chiba 
coda file system 
www coda cs cmu edu 
peter corbett dror feitelson 
vesta parallel file system 
acm transactions computer systems august 
peter corbett dror feitelson jean pierre george sandra johnson anthony hsu julian marc snir robert brian joseph thomas morgan anthony 
parallel file systems ibm sp computers 
ibm systems journal january 
intel scalable systems division 
paragon system user guide 
order number may 
william gropp ewing lusk nathan anthony skjellum 
high performance portable implementation mpi message passing interface standard 
parallel computing september 
william gropp ewing lusk rajeev thakur 
mpi advanced features messagepassing interface 
mit press cambridge ma 
jay huber christopher daniel reed andrew chien david blumenthal 
ppfs high performance portable parallel file system 
proceedings th acm international conference supercomputing pages barcelona july 
acm press 
ieee ansi std 

portable operating system interface posix part system application program interface api language edition 
intermezzo 
www inter org 
michael litzkow todd tannenbaum jim miron livny 
checkpoint migration unix processes condor distributed processing system 
technical report computer sciences technical report university april 
message passing interface forum 
mpi extensions message passing interface july 
www mpi forum org docs docs html 
steven moyer sunderam 
scalable parallel system distributed computing environments 
proceedings scalable high performance computing conference pages 
myrinet software documentation 
www com scs 
nas application benchmark 
parallel nas nasa gov 
nils david kotz 
galley parallel file system 
parallel computing june 
kenneth andrew barry jonathan erickson nygaard christopher steven david matthew keefe 
bit shared disk file system linux 
proceedings seventh nasa goddard conference mass storage systems 
ieee computer society press march 
high performance portable mpi io implementation 
www mcs anl gov 
scheduled transfer application programming interface mappings st api 
www org html 
daniel reed ryan fox mario medina james nancy tran wang 
framework adaptive storage input output computational grids 
proceedings third workshop runtime systems parallel programming 
hal stern 
managing nfs nis 
reilly associates 
gil 
mpi io parallel file system cluster workstations 
proceedings ieee computer society international workshop cluster computing pages 
ieee computer society press december 
test tcp 
ftp ftp arl mil pub ttcp 
rajeev thakur william gropp ewing lusk 
device interface implementing portable parallel interfaces 
proceedings th symposium frontiers massively parallel computation pages 
ieee computer society press october 
rajeev thakur william gropp ewing lusk 
data sieving collective 
proceedings seventh symposium frontiers massively parallel computation pages 
ieee computer society press february 
rajeev thakur william gropp ewing lusk 
implementing mpi io portably high performance 
proceedings th workshop parallel distributed systems pages 
acm press may 
parallel virtual file system 
www parl clemson edu pvfs 
vi architecture 
www org 
xfs generation bit filesystem guaranteed rate www sgi com technology 
html 
