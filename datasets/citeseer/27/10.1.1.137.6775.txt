look study human robot engagement candace sidner cory kidd christopher lee neal lesh mitsubishi electric research labs mit media lab cambridge ma sidner lee lesh merl com media mit edu reports study human subjects robot designed mimic human conversational gaze behavior collaborative conversation 
robot human subject performed demonstration invention created laboratory demonstration lasted minutes 
briefly discuss robot architecture focus study effects robot operating different conditions 
offer study importance engagement 
video clips subject interactions robot conference 
categories subject descriptors information systems user interfaces 
keywords human robot interaction engagement intelligent user interfaces collaborative conversation 

creation dimensional collaborative partners raises important challenges behavior computational entities 
reports results creating robot engagement capabilities 
engagement mean process participants establish maintain perceived connection 
process includes initial contact negotiating collaboration checking part interaction evaluating stay involved deciding connection 
robot developed interacts single user collaboration involves spoken language understanding generation beat gestures arm head gestures track user turn look objects interest interaction 
robot initiates interactions users performs typical conversation 
capabilities increase means robot engage user interaction 
capabilities possible robot face toface conversation person 
conversations presumably require just talking 
robot face 
vision capabilities assess activities human conversational partner 
permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
iui january madeira portugal 
copyright acm 
effective capabilities requires careful study evaluation multiple users interacting robots 
explore impact robot looks conversation particular regard objects interest conversation 
reports user study subjects interacted robot task collaboratively performing demonstration invention created laboratory 
describes robot created provides example interaction user 
video clips available users interacting robot presentation conference 
main body discusses user study 

creating engaging robot robotic agent stationary robot created mitsubishi electric research labs merl 
uses control movement robot head mouth wings 
robot takes appearance penguin called mel 
mel open close beak nod turn head flap wings 
speaker provides audio output 
cameras near mel provide vision capabilities microphones provide speech recognition far distance microphone sound location microphones focal plane vision cameras 
shows mel associated hardware 

mel robotic penguin architecture collaborative interactions uses different systems algorithms largely developed merl 
architecture illustrated 
conversational collaborative capabilities robot provided collagen tm middleware collaborative agents commercially available speech recognition software ibm 
face detection algorithm sound location algorithm speech detection algorithm object recognition algorithm fuse sensory data passing results collagen tm system 
agent control decisions proceed interaction rules engagement proceed middle ends interaction state dialogue provided collagen tm system 
agent actions speakers robot speech speech synthesis robot utterances conversation model collagen user utterances recognition probabilities speech recognition microphones architecture human robot interaction conversation state gesture gaze stance commands engagement info faces sounds environment state conversation state engagement rules mel drawn analysis human human interactions videotapes pair people demonstrating observing equipment merl 
rules determine robot gesture user turn turn gesture say 
particular mel turn look objects relevant conversation 
times looks user speaks 
expects user look points equipment case user expected view equipment 
failure cause mel choose response guide user attention 
researchers robotics exploring aspects gesture example current unique modeling human robot interaction degree involves numerous aspects engagement collaborative conversation set 
robotics researchers interested collaboration dialogue extensive theoretical research collaboration conversation accomplished mel 
focused emotive interactions contrast 
conversational agents researchers notably explored agents produce gestures conversation 
tried incorporate recognition production gestures focused full range behaviors accomplish maintenance engagement conversation 
robot developed carnegie mellon university serves museum guide navigates sound analysis agent control passed speech synthesizer robot control algorithms produce gestures 
operations occur real time 
details architecture current implementation 
robot motors robot motions arm body motions head gaze control speaker position speech detection robot control sensor fusion visual analysis face locations gaze tracking body object location cameras avoiding humans interacts users talking head minimal engagement abilities 
mel stationary robot hands manipulate objects typical robot task navigating space picking objects delivering impossible 
challenge choosing useful task people 
focus group hosting activities robots activities agent environment provides services particularly information entertainment services concluded challenging unique robot give demo similar demo videotapes human human interactions 
invention merl called lent human robot interaction human physical actions directions explanations coming robot 
specially designed cup senses fluid level capacitance communicates level special table inductance 
mel programmed demonstrate table human visitor 
typical interaction shown robot gestures head table noted text 
different conversations possible including demo human participant refuses shown demo 
hi mel robotic penguin 
hi 
name 
sam 
sam show demo 
ok 
really interesting 
come 
try 
okay 
really nice know kind progress people making dining 
paul dietz created new product called 
way sam know paul 

paul member merl 
demo creations 
okay 
stands instrumented 
sam right robot turns head table cup robot turns head user 
near table readout 
word re fill readout means empty 
ok right ll see cup 
right 
pour water glass pitcher fill 
sure glass table 
okay 
notice bar readout robot turns head readout display table shows glass full robot turns back user 
right 
okay pour water back pitcher 
okay 
waiting reading table glass 

see registers needing re fill 

explain works 
sure 
copper glass robot turns head readout display transmits readout display inductance surface table 
readout displays information coming table 
robot turns head back user cool 
know technology restaurants 

glass tells restaurant customer needs refill 
restaurants drinks mean profit gadget easier restaurants sell drinks money 
okay 
sam 
go see paul dietz 
long 
bye 

typical interaction mel interaction mel uses vision system find conversational partner set people room zeroing person responds hello face detection algorithms detect faces sound location algorithms 
mel tracks conversational partner face adjusts gaze partner partner moves 
mel eyes head see gaze merely communicates focus attention partner 
mel look human partner times demo turns look table contents speaks 
mel prompts partner fails look table notice objects 
demo explanation conclude mel wishes partner goodbye waves drops head chest indicate longer available 
note interactions mel greatly affected uncertainty sensory information 
mel interactions designed speaker english training 
speech recognition errors brief exchanges 
addition early discovered opportunity say users say unpredictable set responses mel 
designed demo interaction mel robot controlled conversation robot directs conversation elicits limited types responses users 
design reduced unpredictability user exchanges eliminate entirely 
user study users asked questions offered explanations part refusals statements demonstration 
likewise interpretation vision input relies uncertain information mel looses faces users 
able regain occasionally user moves camera detect face 
cases mel finds user look looks place saw user 

user study began study intended goal determine effective mel mimicking human conversational behavior 
wanted know mel gestures appropriate ones ones cause users behave intended feel natural interacting robot 
learned evaluation went intended goal 
results provide information appropriateness robot gestures improve gestures 
data sources videotapes subjects mel provided great deal material subject proceeded conversation 
sense observations devised categories conversational behaviors subjects measures 
measures revealed happens subjects talk robot just talking head compared active head body 
study circumstances subjects tested different conditions 
mover condition fully functional robot conducted demonstration table 
second talker condition robot gave demonstration terms verbal utterances constrained talk moving beak synchrony words spoke 
initially subject vision system head remained looking direction subject 
constraint meant cases robot look subject demo 
entire interaction videotaped see 
study design subject interacted robot conditions 
protocol study participant randomly preassigned conditions 
subjects participated mover condition talker condition 
video camera turned subject arrived 
subject introduced robot mel told stated purpose interaction see demo mel 
subjects told asked series questions completion interaction 
robot turned subject instructed approach mel 
interaction began experimenter left room 
demo subjects short questionnaire contains scales described results section 
lastly reviewed videotape experimenter discuss thoughts interaction 

subject interacting mel results come different sources questionnaires meant elicit subjects response interaction perceived behavioral assessments taken observations video data 

results questionnaires subjects provided post interaction questionnaires 
questionnaires devoted different factors concerning robot general liking mel devised experiment items measure gives participants impressions robot interactions 
knowledge confidence knowledge demo devised experiment items concerns task differences 
difference subjects expected difference telling conditions interaction 
confidence knowledge demo finer grained measure task differences 
engagement interaction adapted items lombard notion engagement different measure natural interactive experience person interacting robot 
reliability robot adapted items directly related outcome interaction perceived reliability robot indicator participants depend robot information ongoing basis 
higher rating reliability means robot perceived positively interactions 
effectiveness movements devised experiment items measure determine quality gestures looking 
multivariate analysis condition gender condition crossed gender interaction effects provided results category summarized table factors difference effects evident subjects understood demo confident response 
knowledge right wrong encoding answers questions 
general subjects got answers correct average talkers 
confidence scored point likert scale 
conditions rated highly average talkers 
subjects liked mel disliked 
point likert scale average 
average mover condition talker condition higher 
subject difficulty interaction removed mover average comes 
differences conditions significant 
table 
summary questionnaire results liking mel effects knowledge demo effects confidence knowledge demo effects engagement interaction effect female gender female average male average borderline significance reliability mel effect talker condition mover average talker average high significance appropriateness movements effect mover condition mover average talker average significance factors effects subjects provide insight interaction mel 
consider effects gender engagement 
sense engagement concerns captured experience 
questions factor included engaging interaction 
relaxing exciting experience 
completely senses engaged 
experience caused real feelings emotions involved interaction lost track time 
results certainly interesting conclude male female users may interact different ways fully functional robots 
result mirrors differences gender engagement credibility 
concerning appropriateness movements mover subjects perceived robot moving appropriately 
contrast talker subjects felt mel move appropriately 
talker subjects indicate thought moved 
effect confirms sense talking head doing robot doing interaction people objects 
mover subjects responses indicated thought interaction mel just interacting real person mel looked appropriate times mel confuse moved head wings 
striking subjects talker condition robot reliable 
subjects responded statements depend mel correctly time mel reliable task mel way trust mel need 
possible drawn reliability response appropriateness robot behaviors correct consistently produced devices robots moving parts seen complicated break reliable 
clearly remains done users perfectly comfortable robot 
behavioral observations section behavior subjects observed videos taken interactions robot reviewed 
videos showed number ways improve robot changing individual gestures improving recovery speech recognition errors recovery loss subject face 
wanted know differences subjects conversational behavior robot acting conditions 
unaware studies looked human robot conversational behavior detail preliminary results reported 
decide behaviors consider 
choose consider length interaction time amount shared looking looking looking objects measure coordinated participants amount looking robot subject turn measure attention robot amount looking robot attentional measure 
wanted understand effects utterances robot turned demo table 
utterances robot turned table coded subjects turned terms words utterance robot movements 
summarize results measures table 
explain measure results detail 
total interaction time conditions varied significant amount row table 
difference coincides subjective sense talkers interested robot interested doing demo 
nature subject pools respect shared looking coded 
shared looking occurred subject robot looked called mutual gaze looked object table contents 
shared looking indication coordinated participants interaction 
shared looking participants share interest engagement interaction 
shared looking relevant simply mutual gaze participants collaboration objects discussed pay attention partner coordination content conversation 
study robot looked table turned head table directions left beak serving defined pointer 
robot seeing eyes head turns table provided clear information looking table devices nearby room computer monitors laptops 
general view table considered means telling exactly objects subject robot viewing 
table 
summary behavioral test results measure mover talker test result significance interaction time shared looking mutual gaze talk directed mel look backs table look table look seconds seconds single factor anova single factor anova single factor anova single factor anova looks median looks median single factor anova tests tests significant difference significant difference significant difference significant difference highly significant difference weak significance tailed significance tailed measured percentage entire interaction participants engaged shared looking 
mover subjects engaged shared looking robot significantly talker subjects row table 
understand effect necessary look determined mutual gaze 
reasoned shared looking differences indicate happening result robot able look subject table components effect unclear 
mover talker subjects slightly different rates mutual gaze statistically significant measured percentage total interaction time row table 
clearly mutual gaze account differences shared looking 
differences shared looking robot subject looking table 
talking version robot looks table fully functional robot difference shared looking 
additional analyses offer insight engaging robots 
discovered mover talker subjects offer talk directly mel take turn interaction similar rates 
measure considers averages subjects percentage total interaction time subject row table 
result greatly surprised 
expect group involved robot 
talking head moving compelling conversation partner 
features interaction far indicate subjects condition affected gestural abilities robot 
consider additional aspects interaction considered 
significant difference behavior number times subjects looked back robot looking table 
subjects spend proportion time looking table talkers fact interrupt table looks look back mel indication engaged mel compared demonstration objects 
subjects turned bodies demo table began interacting primary focus body stance table 
mover subjects looked back robot far talker subjects average number looks interaction subjects row table 
subject behavior considered utterances direct commands robot generally changed looking 
deictic right occur beginnings robot turns right cup near table readout copper glass transmits readout display inductance surface table 
mover robot typically turned head table talker robot 
instance table look right mover subjects turned heads eye gaze phrase cup 
subjects change just robot turned head table 
remaining subjects looking robot subjects turned subjects turn table subject subject screen 
contrast talker subjects subjects turned head gaze cup 
remaining subjects looking table robot spoke subjects looked robot utterances subjects subject camera 
second declarative utterance table look copper glass mover subjects turned phrases glass transmits subjects glass 
cases changes looking followed just robot change looking 
remaining mover subjects looking table utterance start subjects looked phrase glass robot turned subject looked copper robot turned earlier conversation subject 
subjects hear utterance taken different path interaction 
comparison talker subjects turned utterance distribution rest time subjects looked room looked robot looking table 
subjects conditions performed actions expressed imperative utterances 
wider turned copper glass transmits subjects turned utterances turn 
remaining talker subjects looking utterance began subject distracted outside intervention counted subjects took different path interaction 
results utterances sparse provide strong evidence 
indicate subjects pay attention robot turns head attention table 
robot move subjects turn attention factors appear include robot spoken utterance interest demo table 
talking robot engages people just head talking movement occur 
engagement compelled speech conversation powerful devices engaging people interactions 
looking gestures provide additional power 
cause people pay attention robot may cause people adjust looking robot looking 

results study suggest interactional differences robot uses body head gesture look user objects 
gesturing talking robots capture user attention users respond changes head direction gaze changing gaze head direction 
users engage mutual gaze robots direct gaze turns conversation follow commands asked perform tasks 
robots just talking heads influential conversational partners 
users mutually gaze talk directly take turn conversation follow commands 
users appear sensitive appropriateness gestures aware just talking head expect conversational participant 
robot body indicate attention human objects relevance interaction 
coming years robot partners interactions commonplace engagement interaction including capturing head gestures arm gestures gaze conversation management ways people expect continuing challenge intelligent user interfaces 

research careful reading conversation reveal robot turns conversation long 
human conversation contains smaller chunks punctuated backchannels conversation participant 
backchannels spoken gestural come form nods 
fact subjects mel especially positive response turns 
behavior suggests utterance chunks backchannels produce typical conversation style 
recognize nods users mel camera head position tracking algorithm recognizing head nods 
experimenting effects recognition nods production nods mel conversation 
current gestural rules primitive 
experimented proceed user looks away take turn mel change behavior user looks away long time long take turn conversation 
clearly behavior faulty 
secondly human human observations know people track times 
look away see going time share tasks 
natural looking complex mel currently undertakes 
third mel points currently beak 
mel degrees freedom wing point wings 
produce natural gestures head wing pointing humans people look bring arms hands point close timing 
mobile robot engage users conversations indicate focus attention 
plan mel attend users greet finding faces offering greetings approaching 
addition mobile mel able turn face objects interest conversation 
change allow understand role body stance indicator focus attention 

chuck rich assistance collagen mel anonymous reviewers ideas improvements 

beardsley detection 
mitsubishi electric research labs tr cambridge ma february 
breazeal affective interaction humans robots 
proceedings european conference artificial life ecal 
prague czech republic 
bruce nourbakhsh simmons 
role expressiveness attention human robot interaction 
proceedings ieee international conference robotics automation washington dc may 
burgard fox lakemeyer schulz steiner thrun interactive museum tour guide robot 
proceedings aaai pp 
aaai press menlo park ca 
cassell sullivan prevost churchill embodied conversational agents 
mit press cambridge ma 
fong thorpe baur collaboration dialogue human robot interaction 
th international symposium robotics research lorne victoria australia november 
johnson rickel lester animated pedagogical agents face face interaction interactive learning environments 
international journal artificial intelligence education 
ishiguro imai ono mase constructive approach developing interactive humanoid robots 
proceedings iros ieee press ny 
kidd sociable robots role presence task human robot interaction 
thesis mit media laboratory june 
lombard heart concept presence 
journal computer mediated communication 
university ca school 
lombard crane davis gil measuring presence literature approach development standardized pencil instrument 
presence third international workshop presence delft netherlands 
darrell adaptive viewbased appearance models 
proceedings ieee conference computer vision pattern recognition vol 
june 
cassell model face face grounding 
proceedings st acl meeting sapporo japan pp 

reeves wise robots versus screen agents effects social emotional responses 
proceedings chi acm press 
rich sidner collagen collaboration manager software interface agents user modeling user adapted interaction vol 
pp 

rich sidner lesh collagen applying collaborative discourse theory human computer interaction 
ai magazine special issue intelligent user interfaces aaai press menlo park ca vol 

sidner human robot interaction engagement humans robots hosting activities 
proceedings ieee international conference multimodal interfaces pp 

sidner lee architecture engagement collaborative conversations robot humans 
merl technical report tr june 
sidner lee lesh engagement looking behaviors robots collaborating people 
proceedings th workshop semantic pragmatics dialogue eds university saarland pp 

viola jones rapid object detection boosted cascade simple features 
proceedings ieee conference computer vision pattern recognition hawaii pp 

