implementing transparent shared memory clusters virtual machines matthew chapman heiser university new south wales sydney australia national ict australia sydney australia cse unsw edu au shared memory systems smp ccnuma topologies simplify programming administration 
hand clusters individual workstations commonly due cost scalability considerations 
developed virtual machine solution dubbed seeks provide numa environment commodity cluster single operating system instance transparent shared memory 
design preliminary evaluation 
workloads require processing power feasible single processor 
shared memory multiprocessors smp numa systems tend easier administer program networks workstations 
shared memory systems single system image single operating system instance presenting single interface namespace 
hand clusters individual workstations tend cost effective solution easier scale reconfigure 
various techniques proposed provide simplicity shared memory programming networks workstations 
depend simulating shared memory software virtual memory paging known distributed shared memory dsm 
middleware layer dsm libraries available treadmarks 
libraries require software explicitly written utilise provide facets single system image transparent thread migration 
projects attempted retrofit distribution existing operating systems mosix clustering software linux 
linux designed distribution mind mosix provide thread migration system calls need routed back original node 
projects attempted build distributed operating systems ground amoeba 
order gain wide ac published proceedings usenix general track anaheim ca usa april 
operating system virtual machine monitor node node node node fast interconnect cluster operating systems need provide compatibility large body existing unix applications easy task 
alternative approach utilising techniques 
useful hiding hardware complexities operating system 
privileged virtual machine monitor interposes operating system hardware presenting virtual hardware may different real hardware 
example disco simulates multiple virtual smp systems numa system 
uses essentially opposite simulating single virtual numa machine multiple workstations dsm techniques provide shared memory 
previous achieve true single system image legacy operating system significant modifications operating system 
focus linux guest operating system supports numa hardware source code available 
means optimisations improve locality improvements necessary 
chose target itanium architecture virtual machine 
numerous ia virtual machine monitors exist number techniques patents 
itanium positioned intel industry standard architecture particularly high systems 
itanium virtual machine monitor presents research opportunities independent distribution aspects 
implementation overview startup order achieve best possible performance type vmm executes lowest system software level support operating system 
started directly initialises devices installs set exception handlers 
nodes cluster selected bootstrap node providing guest kernel part configuration 
bootstrap node starts relocates kernel virtual machine address space branches start address interaction virtual machine exceptions 
nodes wait startup node provides start address branch guest kernel code data fetched lazily dsm 
privileged instruction emulation order ensure virtual machine bypassed guest operating system demoted unprivileged privilege level 
privileged instructions fault virtual machine monitor 
vmm read current instruction memory decode emulate effects respect virtual machine 
example instruction instruction pointer mov psr simulated psr register copied register 
instruction pointer incremented 
itanium architecture perfectly way number sensitive instructions fault require vmm intervention 
substituted faulting instructions 
currently done statically compilation time possible runtime necessary replacement instructions chosen fit original instruction slots 
cover instruction simply replaced break 
replaced moves registers model specific registers normally operating system instructions conveniently take register operands 
distributed shared memory virtual machine simulated physical address space referred machine address space 
level dsm operates 
machine page associated protection bits metadata maintained dsm system 
guest os establishes virtual mapping effective protection bits virtual mapping calculated logical requested protection bits dsm protection bits 
keeps track published proceedings usenix general track anaheim ca usa april 
virtual mappings machine page protection bits updated dsm system virtual mappings updated 
initial dsm algorithm simple sequentially consistent multiple reader single writer algorithm ivy systems 
machine pages virtual machine divided nodes node manages subset pages 
node faults page manager node contacted instance 
manager node forwards owner owner owner returns data directly requesting node 
sent data necessary receiving node performs invalidations 
version numbers avoid re sending unchanged page data 
evaluation test environment consists single processor mhz itanium workstations link gigabit ethernet cards connected back back crossover cable form processor cluster 
similar dual processor smp itanium workstation comparison 
obviously intended system scale nodes software stable benchmarking larger cluster 
guest kernel linux kernel compiled hp simulator platform 
modifications kernel tiny change enable smp hp simulator usually uniprocessor static instruction replacement described section 
splash benchmarks known set benchmarks shared memory machines 
existing implementation designed standard pthreads threading library 
results splash applications ocean water barnes 
case measured performance different topologies single processor workstation workstation measure virtual machine overhead single processor workstations dual processor smp workstation 
processor cycle counter obtain timings want place trust accuracy gettimeofday virtual machine 
ocean ocean simulates large scale ocean movements solving partial equations 
grid representing ocean partitioned processors 
iteration computation performed element grid requires values neighbours causing communication partition boundaries 
simulation time smp proc proc splash ocean application proc proc simulation grid size proc proc results ocean application results shown 
consider single processor results demonstrate virtual machine performance independent dsm 
smallest grid size virtual machine performance fact benchmark runs marginally faster virtual machine 
due fact parts memory management done virtual machine monitor involving guest operating system mechanisms implemented long format advantageous workloads compared implemented linux 
grid size working set size increases number tlb misses page faults involve guest kernel increases 
significantly expensive virtual machine ultimately outweigh memory management improvements 
largest grid size virtual machine imposes overhead 
hand distribution efficiency increases problem size 
granularity communication increase linearly side grid 
sparse access patterns compared granularity dsm simply see greater utilisation pages transferred overhead remains roughly constant meaning relative overhead 
grid overhead significant compared actual amount done clearly worthwhile 
passed breakeven point node performs better single processor 
largest problem size benchmark largely computation bound works 
relative single processor workstation speedup compared smp 
water water example application performs dsm environment published proceedings usenix general track anaheim ca usa april 
simulation time smp proc proc splash water application proc proc proc proc number molecules proc proc results water application performs 
water evaluates forces potentials occur time system water molecules 
processor needs data subset calculations stores results locally 
timestep processors accumulate results shared copy 
alternating read sharing update phases 
results shown 
virtual machine overhead minimal working set sizes smaller ocean mb largest problem size compared mb 
distribution overhead scales number molecules size shared data expected small 
largest problem size speedup compared smp 
barnes hand barnes example application known perform dsm environments 
barnes simulates gravitational interaction system bodies dimensions barnes hut hierarchical body method 
data represented octree leaves containing information body internal nodes representing space cells 
stages timestep calculating forces updating particle positions octree 
results shown 
force calculation phase distributes fairly certainly larger problem sizes 
tree update phase pattern reads writes finegrained unpredictable results significant false sharing 
false sharing particularly problematic currently uses sequentially consistent multiple reader single writer dsm means pages simultaneously writable multiple nodes 
benchmark perform simulation time simulation time smp proc proc smp proc proc splash barnes application proc proc number particles proc proc number particles proc proc splash barnes application force calculation proc proc results barnes application 
results show scientific applications splash suite performance surprisingly dominated application dsm costs kernel paging overheads 
applications behave conventional dsm systems water perform best 
typically applications computation intensive share pages reading writing 
significant advantages middleware dsm systems providing true single system image simple migration path smp applications 
utilises networks commodity workstations cost effective reconfigurable specialised ccnuma hardware 
believe classes applications provide useful alternative systems 
improvements need perform benchmarks larger clusters prove scalability 
published proceedings usenix general track anaheim ca usa april 
supported linkage australian research council arc hp org project hardware hp intel 
national ict australia funded australian government department communications information technology arts arc backing australia ability ict research centre excellence programs 
li 
shared virtual memory loosely coupled multiprocessors 
phd thesis yale univ dept computer science 
rr 
keleher dwarkadas cox zwaenepoel 
treadmarks distributed shared memory standard workstations operating systems 
proc 
winter usenix conference 
barak la 
scalable cluster computing mosix linux 
proceedings th annual linux expo 
mullender van rossum tanenbaum van renesse van staveren 
amoeba distributed operating system 
ieee computer 
heiser russell liedtke 
single address space operating system 
softw pract 
exp jul 
bugnion devine rosenblum 
disco running commodity operating systems scalable multiprocessors 
proc 
th sosp 
intel itanium architecture software developer manual oct 
developer intel com design itanium family 
gray chapman mosberger tang heiser 
itanium system implementor tale 
proc 
usenix techn 
conf anaheim ca usa apr 
christian 
optimised itanium processor family 
proc 
rd virtual machine research technology symp 
li hudak 
memory coherence shared virtual memory systems 
trans 
comp 
syst 
woo singh gupta 
splash programs characterization methodological considerations 
proc 
nd isca 
chapman heiser 
itanium page tables tlb 
technical report unsw cse tr school comp 
sci 
engin university nsw sydney australia may 
iftode 
home shared virtual memory 
phd thesis princeton university dept computer science 

