appears proceedings th annual intl 
symposium computer architecture isca pp 
june 
hardware mechanism dynamic extraction program hot spots matthew andrew trick erik nystrom ronald barnes wen mei hwu coordinated science lab west main street mc urbana il nystrom hwu crhc uiuc edu presents new mechanism collecting deploying runtime optimized code 
code collecting component resides instruction retirement stage lays hot execution paths improve instruction fetch rate enable code optimization 
code deployment component uses extension branch target buffer migrate execution new code modifying original code 
significant delay added total execution program due components 
code collection scheme enables safe runtime optimization paths span function boundaries 
technique provides better platform runtime optimization trace caches traces longer persist main memory context switches 
additionally traces susceptible transient behavior restricted frequently executed code 
empirical results show average mechanism achieve better instruction fetch rates kb hardware trace cache requiring kb hardware producing long persistent traces suited optimization 

development order execution automatic dynamic speculation led dramatic improvements performance modern microprocessors 
techniques steps allowing microprocessor determine execute code optimally 
point time optimization decisions limited scope typically onthe fly persistent record 
presents framework dynamic optimization persistent code transformations demonstrates dynamic optimization targeting high instruction issue throughput 
optimizations rely accurate profile information profitably transform code 
compilers support profile information software vendors reluctant add profiling step development cycles 
difficult determine profile representative way program presence profiling behavior certain programs may change 
reasons automatic transparent mechanism profiling code current usage advantageous 
automatic system improve performance ways static compiler 
program behavior changes time code current behavior advantage temporal relationships typical static compiler optimizes average behavior entire execution 
automatic system lead targeted optimizations 
hardware profiler called hot spot detector 
runtime determines frequently executed branch instructions collecting profile behavior 
snapshot profile taken hot spot detector determines execution primarily confined collected branches 
set branches considered hot spot 
process performed quickly runtime unique opportunity exists optimize currently executing code leaving sufficient time gain benefit executing optimized code 
general characteristics hot spots considered section hot spot detector mechanism discussed section 
hardware serves basis code collection mechanism 
hot spot detector developed hardware runtime optimization performs code straightening loop unrolling partial function inlining branch promotion improve instruction fetch performance 
propose enable wide fetch collecting instructions placing memory executed order enabling traditional instruction cache fetch multiple blocks cycle 
schemes fetching multiple basic blocks cycle requires extra hardware critical path microprocessor uses modest kb hardware plus control logic located instruction retirement stage processor pipeline 

related runtime optimization applications promises deliver higher performance currently available static techniques 
number software dynamic reoptimization systems dynamo daisy fx emerged optimize running applications storing optimized sequences portion memory extended execution 
systems may suffer significant software overhead related code profiling optimization 
mechanism similarly uses memorybased code storage technique utilizes hardware structure rapid profiling analysis optimization 
earlier systems require software executive monitor control reoptimization process system hardware manages process 
mechanisms algorithms utilize runtime optimization techniques attack problem traditionally managed special purpose hardware compiler techniques 
order achieve higher levels instruction level parallelism fetch unit required supply multiple basic blocks cycle execution units 
early designs include sequential instruction caches collapsing buffer designed fetch contiguous blocks cache boundaries non sequential intra cache line blocks respectively single cycle 
complexity shift logic collapsing buffer may cause undesirable increase fetch latency 
solutions wide fetch problem involve reordering code blocks executed order storing special cache called trace cache 
cache organization shown perform instructions normally separated taken branches fetched single cycle 
compilers improve fetch performance reordering program blocks sequentially expected frequent paths 
compilers profile information organize functions blocks functions performing function inlining appropriate 
software trace cache technique constructs traces compile time similar traces constructed runtime trace cache 
static techniques system optimizes library dll boundaries 
goal allow processor transform frequently executed code dynamically better fits current usage profile better matches available resources microarchitecture 
venues dynamic optimization explored transformation traces trace cache 
optimizations limited relatively short length traces fact traces persistent 

hotspot characteristics prior shown majority dynamic execution takes place inside program hot spots 
hot spots dynamic number taken branches heavily outweigh number fall branches case inner loops internal controlflow 
hot spots contain loops complex control flow 
number call return instructions average dynamic control flow ex bbs rec 
call ret call call indirect call ret call bbs distinct calls ret ret hot call ret hot path cold path hot branch cold branch basic block flow graph selected functions dominant hot spot li 
benchmarks indicates hot spots complex control flow 
control transfers highly predictable frequency presents barrier wide fetch optimization 
benchmarks fair number unconditional jumps representing average dynamic control flow instructions 
instructions perform control decisions obvious candidates optimization 
indirect jumps indirect calls large portion branches frequent hazards long trace formation benchmarks 
inlining potential target may eliminate effects fetch allow wide fetch indirect jump call 
dynamic control flow instructions fall sequential instruction addresses 
consequently traditional fetch architectures break fetches taken branches limited basic block fetch 
effort better understand composition hot spots important hot spot li benchmark closely examined 
hot spot represents dynamic execution training input 
portion hot spot depicted control flow graph black boxes arrows represent branches paths collected part hot spot 
entire hot spot consists branches spanning approximately static instructions 
hot spot simply tight loop control proceeds number functions minimal inner loops 
function indirect call wide variety functions qualify inclusion hot spot 
control enters portion hot spot function point proceeds calls exits point majority branches biased indicating primary path functions 
likewise loops marked back edges iterate times invocation hot spot 
execution hot spot largely consistent static branches dynamic branches highly consistent dynamic branch direction greater direction 
number layout optimization opportunities exist example code 
specifically code straightening optimization eliminate taken conditional branches remove cold blocks trace 
furthermore function inlining may performed remove fetch optimization barriers caused calls returns 

architecture section describes hardware driven mechanism capable automatically extracting frequently executed regions code remapping code closely matches dynamic behavior program 
additional system requirements imposed mechanism include small kb table associated registers control logic pages reserved virtual address space process 
remapping hardware involved sensitive latency may lag actual instruction retirement little effect processor critical path 
understand functionality system useful consider high level view system operation 
system identify profile suitable hot spots 
referred profile mode 
hot spot detection scan mode entered code executes normal system searches suitable branch trace 
suitable entry branch system toggle fill mode pending mode code executed remapped reserved area virtual memory called memory code cache 
fill mode requires processor write remapped code memory reduces execution speed place 
fill mode infrequent execution shown table making overhead insignificant 
remapping process complete mechanism returns scan mode look additional traces hot spot 
time period allowed remapping code detected hot spot expires system returns profiling state begins searching new hot spots 
memory code cache dynamic optimizing systems optimized code saved location execution 
systems utilize specific hardware caches contain optimized code automatically substitute original code 
systems including region memory commonly called code cache contain translated code 
system standard paging mechanisms manage set virtual pages reserved code cache 
virtual memory contain optimized code standard paging instruction caching mechanisms allow translations persist context switches 
code cache pages allocated operating system process initialization time marked read executable code 
course remapping hardware allowed write reserved region memory 
limitation simple system code cache grow initial size 
sufficiently large code cache allocated process initialization 
initial experiments assume large code cache remove remapped code cache 
code cache replacement policy useful production system outside scope 
hot spot detection step process remapping hot spots detection frequently executed blocks 
employing hardware scheme possible eliminate overhead profiling hot spot detection completely transparent system 
heart hardware profiling mechanism structure called branch behavior buffer bbb shown 
program executes bbb collects profile data individual branches eventually provide sufficient information allow reconstruction hot paths 
bbb indexed branch address contains fields enable branch profiling tag branch address branch execution count branch taken count branch candidate flag 
bbb described extended basic design facilitate process trace formation 
additional fields hot spot detection explained subsequent sections 
collect hot spot information bbb monitors retired branch instructions allocating entry new branch long unused entry available 
entry acquired branch execution counter incremented branch subsequently retired branch taken taken counter incremented 
execution counter reaches maximum value counters locked preserve profile bias 
periodically refresh timer invalidates bbb entries execution counter exceed preset candidate branch threshold 
effectively implements frequently policy 
possible conflicts prevent important branch entering bbb 
consequently proposed remapping algorithm designed easily tolerate occasional missing branch profile 
attached bbb hot spot detection counter monitors percentage executed branches fall current set candidate branches 
hot spot detection counter increments execution branch decrements execution candidate branch controlled values execution remains intensely executed set branches sufficient length time counter eventually reaches zero 
counter hits zero signals hot spot detection time current bbb state frozen long trace generation unit construct traces hot spot profile mode 
avoid hot spots optimized branches memory code cache placed bbb cause detection counter move direction 
code cache instructions may identified simply instruction address 
trace generation overview hot spot identified processor enters scan mode 
mode program continues execute normal manner trace generation unit refresh timer reset timer branch address branch behavior buffer branch tag exec cntr taken cntr cand flag taken valid bit remapped taken addr ft valid bit remapped ft addr call id touched bit hot spot detection counter saturating adder zero hot spot detected branch behavior buffer new fields shaded gray 
fetch decode execute order retire icache branch predictor btb tgu bbb memory instruction supply portion processor remapping hardware 
tgu examines retired instructions 
guided profile information collected bbb trace generation unit detects instructions follow hot path stores instructions code cache 
depicts interaction tgu components 
tgu writes instructions code cache performs important functions 
creates connected regions code embody detected hot spot defines entry points regions 
way program control enters hot region selected entry point control remain inside region code significant length time 
second tgu automatically performs code frequently executed paths hot spot 
tgu writes instructions code cache sequential blocks traces 
trace associated entry point point control enter trace original code 
mechanism activate trace entry points discussed section 
hot spot collection traces trace set created code comprises hot spot 
individual trace may contain internal branches branches traces set transfers control directly code different trace set 
remapped hot spot self contained region code independently optimized deployed 
code deployment transfer execution optimized code handled branch target buffer 
code self checks criterion system original code altered way 
new trace constructed code cache entry point trace written array located page code cache 
preset interval timer signals remapping current hot spot 
time routine initiated install array entry points btb 
entry point btb target entry point branch updated address entry point target code cache 
entry point bit set btb lock entry place btb flush 
context switch routine invoked entry points btb process basis 
new hardware required needed update btb entries ignore branch address calculations selectively branches entry point bit set 
fill pending mode described section desirable execution remain original code jumping code cache 
prevents new traces containing copies code cache instructions ensures exit branches return original code 
operating modes processor ignore entry point bit 
additional number branch mispredictions incurred due locked ignored entry branches small little time spent executing fill pending modes 
trace generation algorithm illustrates decision logic tgu 
traces remapped memory code cache tgu transitions modes operation scan mode search trace entry point 
initial mode hot spot detection 
fill mode construct trace writing retired instruction code cache 
pending mode pause trace construction new path executed 
assist fly code remapping additional fields added bbb shown 
bbb taken target bbb fall target fields hold offsets code cache code corresponding original branch direction laid 
valid bits target indicate path remapped 
field supports remapping tagging target fields particular calling context 
prevents code different contexts linked problem discussed section 
touched bit added support rollback operation described section 
addition bbb tgu uses set registers listed table 
rules cause state transition listed condition triggers rule 
section rules associated operation trace listed followed list actions taken conditions satisfied 
new entry point scan mode rule taken conditional branch unconditional jump retired corresponding candidate bbb entry direction remapped 
profile mode new entry point fill mode scan mode rule condition new entry point jmp candidate taken cold branch trace candidate dir remapped trace candidate merge path branch cnt pending max path allowed mode ret ret addr mismatch continue candidate recursive call rule condition merge candidate dir remapped exec dir remapped continue addr matches pending target exec dir remapped cold branch jmp candidate trace generation modes 
offset available memory location code cache 
updated time 
original address branch instruction entry point current trace 
code cache target 
original address instruction follows current trace 
pending mode determine continue filling trace 
number non candidate branches executed filling current trace 
integer value unused call id call id estimates calling context 
stack representing current call chain fill mode 
entry contains call id return address 
top entry current function entries fixed number parents 
table tgu registers code remapping 
align cache line boundary set bbb taken target set branch pc set reset contains entries transition scan mode fill mode trace fill mode rule conditional branch retired candidate bbb entry directions remapped 
rule conditional branch retired candidate bbb entry exceeds maximum path threshold 
rule return instruction retired pc match return address top stack 
usually result executing return instruction having inlined corresponding call instruction 
rule conditional branch retired candidate bbb entry bbb top entry 
indicates recursion 
emit conditional branch taken direction matches direction followed retired branch 
retired branch fall direction executed branch sense inverted 
direction remapped code cache offset branch bbb entry branch target 
pc original code 
emit unconditional jump opposite direction 
direction remapped jump transfer control point code cache return control original code 
install current entry point 
written array entry points code cache insertion btb 
transition fill mode scan mode merge fill mode rule conditional branch retired candidate bbb entry current branch direction remapped direction remapped 
may possible continue growing trace execution follows path 
emit conditional branch bbb current target branch target 
bbb current target refers taken target field retired branch taken fall target field 
address original code instruction opposite retired branch direction transition fill mode pending mode continue pending mode rule conditional branch retired target address matches current direction remapped 
set bbb current target transition pending mode fill mode cold branch pending mode rule conditional branch retired candidate bbb entry exist 
emit unconditional jump 
install current entry point 
transition pending mode scan mode trace example optimizations section steps remapping process code example 
shows original code layout lists execution sequence seen entering scan mode 
basic remapping generates trace shown 
application remapping optimizations patching branch replication results trace shown 
patching reduces premature execution order remapping mode 
rule set block rm 
bbb taken target updated offset block rm 
tgu enters fill mode block filled trace 
entrance rm rm rm opt opt opt step opt retired ft block tk block seen fall branch written code cache rm 
time branch remapped rm xn branch emitted falls target xn 
example rm falls ft rm branch sense rm inverted 
bbb taken target invalid point taken target rm point back tk block original code 
bbb fall target updated point block rm 
entrance rm rm rm rm rm opt opt block filled rm 
causes rm ft set block rm 
rm ft equivalent tk rm opposite branch sense said inverted 
rm tk points block original code 
bbb taken target updated block rm block filled rm ft rm rm rm rm rm opt opt opt step opt addition actions address remapped branch written unused bbb target field 
allows remapped branch modified target redirected 
case bbb taken target set address branch rm bbb fall target set address rm 
distinguish case targets branch remapped bbb taken target bbb fall target remain marked invalid 
bbb trace formation exec taken ftv fta block rm block rm block rm block rm block rm block rm touch trace generation example 
trace exits branch replication performs aggressive code straightening unrolling loops process 
steps refer basic algorithm 
steps opt opt refer algorithm optimizations 
operations performed unoptimized algorithm performed optimized version 
depicts final contents bbb remapping 
aid explanation notation refers static branch xn refers th instance dynamically 
tk taken target static branch ft fall target 
xn ft indicates xn falls ft xn tk means xn branches target tk case indirect jumps 
block basic block instructions terminated branch rm xn copy code cache caused retirement xn 
step opt retired tk block shown candidate branch seen scan step opt retired tk block time seen taken rm ft directed taken path block rm 
previously step ft seen bbb fall target valid 
rm inverted rm tk directed bbb fall target block rm 
step opt previously step opt ft seen rm tk directed original code seen 
tk provides taken path 
simple optimization called patching allows rm tk updated branch target code cache 
patching patching rm tk cause trace exit code cache prematurely system forced execute original code new entry point encountered 
patching performed step address rm placed currently invalid target field bbb entry case bbb taken target 
step opt rm tk patched target rm ft shown dotted arc rm block rm 
patching performed fill mode trace encounters branch remapped opposite direction 
step retired tk block tk seen entry branch tk matches rule merged back block rm 
rm written code cache rm rm 
ft seen tgu switches fill pending mode wait ft cold branch 
code cache offset replicated branch instruction trace 
bbb index replicated branch 
target fields entry trace rolled back 
index call id stack corresponds calling context replicated branch 
step opt retired tk block merging rm block rm loop unable take advantage rest cache line 
improve instruction issue bandwidth desirable eliminate taken branches possible reducing instruction cache performance 
tgu second optimization called branch replication avoid taken jump rm block rm 
branch replication branch replication general optimization dual effect unrolling small loops tail duplicating blocks remapped multiple traces 
branch replication trace filled past particular branch direction 
subsequent copies branch trace set inverted respect copy target addresses point fall address copy 
branch replication hand allows traces continue past branch multiple times linking back copy branch section contains precise treatment branch replication 
applying optimization example merging rm tk block rm rm inverted rm ft points block rm 
bbb taken target updated contains valid target 
ft seen rm tk point back block fall address original code 
step retired ft block tgu pending block cold branch tgu stays pending mode 
step opt retired ft block ft tk seen 
rule trace case rm tk set jump bbb fall target block rm 
close trace unconditional jump rm ft go taken target bbb entry block rm 
branch replication higher priority rule tgu continue filling 
bbb taken target point rm tk block rm block filled rm ft step retired ft block tgu pending block cold tgu stays pending mode 
step opt retired ft block time ft seen 
rm address stored bbb fall target step opt patching done rm done rm step 
rm tk patched block rm 
block filled rm ft code cache 
bbb taken target valid rm tk points block rm 
table tgu registers branch replication 
step retired ft block tgu pending block target 
tgu re enters fill mode fills block rm ft step opt retired ft block ft encountered time 
rm tk set point bbb taken target block rm block filled rm ft step opt filling continues enters cold code triggers rule 
branch replication details complexity branch replication section presents complete picture implementation details 
support branch replication tgu requires additional registers listed table 
branch replication may performed time conditions defined rules arise 
normally conditions result trace merge operation 
branch replication adds additional stipulation conditions 
remapped target points previous trace bbb current target remapped target code cache offset sequential cache banks branch replication performed trace merge operation 
recall bbb current target refers bbb taken target branch taken bbb fall target 
likewise bbb target refers target opposite bbb current target 
bbb current target points previous trace branch replication tail duplicates block current trace actions performed valid bbb target set bbb target 
updates address patching 
emit conditional branch taken target follows direction opposite currently executing branch 
direction remapped bbb target branch target 
original target address 
set bbb current target 
updates bbb target offset current trace 
bbb current target trace cache banks away branch replication effectively loop conditional branch 
heuristic assumes bank cache line applied types cache configurations 
tgu performs actions set set current bbb index set current top valid bbb target set bbb target 
previous case emit conditional branch taken target follows direction opposite currently executing branch 
reset touched bits bbb 
registers set loop unrolling event rollback performed 
trace ends trace conditions rules branch replication continue duplicate blocks finds loop crosses cache banks time trace rolled back previous replicated branch 
specifically rollback takes place contains valid address branch replication condition fails due distant branch target trace target address precedes current code cache offset cache bank 
rollback code cache offset reset replicated branch trace merge operation deferred branch replication performed 
rollback triggers actions set set top invert branch overwriting target current target advance 
invalidate bbb target fields touched bit set 
targets remapped perform trace operation 
target remapped perform merge set target tgu combines patching branch replication optimizations care taken ensure correct operation 
particular touched bit bbb entry set entry modified indicate fields point filled block 
touched bits reset branch replication 
rollback touched fields invalidated clearing valid bits values 
backtracking occasionally fill mode actual execution follow frequently executed path hot spot 
avoid creation suboptimal traces optimization called backtracking discard current trace execution follows cold path 
tgu checks execution cold path comparing bbb branch bias currently executing direction preset cold threshold 
taken branches execution counter bbb entry shifted right bits subtracted taken counter 
result positive branch taken frequency exceeds cold threshold 
perform check fall branches taken counter subtracted execution counter 
returning example execution fallen branch iterating tgu discarded trace 
tgu performs actions backtracking reset 
invalidate bbb entries touched bit set 
transition fill mode scan mode patching branch replication performed trace longer safe backtrack 
single bit flag raised operations suppress backtracking remainder trace 
useful limit backtracking amount time spent fill mode minimal 
done simply small counter counts number consecutive backtracks suppresses backtracking reaching threshold 
branch promotion high instruction issue rates limited number branches predicted single cycle 
method overcoming limitation mark instruction static prediction technique called branch promotion 
trace cache implementations branch bias table track long term behavior branch promoting consistent instructions traces require dynamic prediction 
static prediction wrong processor suffers branch misprediction penalty cause promoted instruction revert back dynamically predicted form 
similarly wide issue instruction cache mechanisms suffer limit branches cycle 
hot spot detection mechanism suited promotion decisions remapping profile branches exists bbb 
mispredictions expensive chose promote instructions execute direction bbb 
example rm promoted branch taken executed counters value 
occasionally branches change behavior course program execution causing mispredictions may negate benefit promotion 
require means misbehaving instructions 
propose small buffer empirically chosen contain entries implementation called branch demotion buffer tracks typically small number promoted branches exhibit mispredictions marks misbehaving instructions instruction cache force dynamic prediction 
buffer organized fully associative cache containing saturating counters 
demotion re promotion performed instructions counter reaches set threshold values 
automatic inlining example call return instructions account significant portion control transfers 
benefit gained inlining calls 
consider example shown 
caller consists single block loop serial calls callee 
remapping process begins normal placing block code cache 
point direct call seen inlining call begins 
process begins assigning callee available call id example pushing hardware execution order remapping ret jmp original code layout ret block block block block rm rm rm rm ret rm rm rm rm rm jmp remapped code layout call id stack call id stack trace example inlining 
call id stack call id stack call stack expected return address 
call inline instruction inserted place original call 
executed instruction push return address original call site block process stack allow execution fall instruction trace 
pushing original return address fail safe guarantees control return correct function execution takes early exit trace 
hides existence code cache programs read return address value directly 
blocks ret filled code cache normal 
branch remapped bbb field replaced current call id 
return instruction reached tgu pops top entry call id stack compares expected return address address retired instruction 
addresses match rule tgu terminates trace emitting return instruction 
mismatch commonly occurs trace corresponding inlined call call id stack empty 
mismatch occur pending mode program returns current function enters different call site continuing fill trace 
example call id popped call id stack expected return address matches block 
tgu continues fill instructions past return emitting normal return instruction uses return inline instruction allows control fall subsequent instructions trace 
return inline normally falls instruction compare return address stack original address trace instruction 
check necessary event program directly modifies return address 
comparison fails return inline behaves exactly normal return instruction 
return inline instruction block remapped inlining begins 
recall rm tk points original code 
call id fields branch incorrectly patched point current code cache offset 
avoid problem potentially distinct calling context acquires unique call id bbb lookup remapping bbb match call id top stack target fields invalidated 
consequently function inlined multiple times targets associated previous inlined copies ignored overwritten offsets copy 
example second call remapped call id pushed stack 
bbb bbb fall target invalidated bbb taken target set block rm 
block filled call site function encountered 
inlined call id pushed stack 
block rm filled trace 
bbb lookup performed bbb value matches call id stack entries current stack pointer 
condition rule satisfied tgu ends trace emitting conditional jump block unconditional jump block 
new instructions new instructions remapped code 
instructions designed support runtime optimization hardware visible programmer 
instructions may emulated traditional instructions may need implemented microarchitecture maximally efficient 
call inline return addr original code normal call program counter set sequential instruction return address set operand value pc 
process stack branch prediction return address stack ras properly maintained case normal return executed 
return inline expected return addr execution speculatively continues instructions immediately return inline 
operand expected return address compared return address stack 
values match misprediction occurs normal return executed 
call indirect inline indirect addr inlined addr return addr original code actual indirect target calculated normally compared inlined address operand 
mismatch actual target inlined target normal call calculated target 
call inline performed 
cases original return address provided operand pushed stack 
jmp indirect inline indirect addr inlined addr actual indirect target calculated normally compared inlined address operand 
mismatch actual target inlined target normal jump calculated target 
control passes instruction immediately inlined jump 
instruction particularly useful optimizing dll boundaries 
benchmark go ksim gcc compress li ijpeg perl vortex adobe photo adobe photo num 
actions traced insts 
stone training input clt training input training input test training input count enlarged train lsp training input queens vigo ppm training input pl training input vortex training input open mb doc file search close load page doc word count select entire doc change font undo close vb script generates si diffusion data graphs load detailed tiff image increase contrast save exported detailed tiff image encapsulated postscript load page ps file view zoom perform text extraction table benchmarks remapping experiments 
parameter setting num bbb entries bbb associativity way exec taken cntr size bits candidate branch thresh refresh timer interval branches clear timer interval branches hot spot detect cntr size bits hot spot detect cntr hot spot detect cntr dec table hardware parameter settings 

experimental evaluation trace driven simulations performed number benchmarks order explore hot spot characteristics establish effectiveness proposed hot spot remapping scheme 
specint common windowsnt applications simulated provide broad spectrum typical programs 
benchmarks input sets summarized table 
applications specint benchmark suite compiled source code microsoft vc compiler optimize speed inline suitable settings 
windowsnt applications executing variety tasks simulated 
applications general distribution versions compiled respective independent software vendors 
order extract complete execution traces applications user code including statically dynamically linked libraries employed special hardware capable capturing dynamic instruction traces amd platform 
traced instructions isa variable length instructions simulation 
ensure examination executed user instructions sampling trace acquisition simulation 
hot spot detection hardware trace generation unit simulated instruction instruction basis entire execution 
new instructions listed section remapped extra operand sizes taken account 
hot spot detector configured parameters listed table 
parameters similar determined empirically detect consistent concise hot spots 
compared previous able re model icache trace cache remap size way block size way block code traditional kb remap kb trace cache kb kb kb tc remap kb kb kb trace cache kb kb kb tc remap kb kb kb table fetch mechanism models 
duce bbb table size entries incurring excessive number conflicts 
collect slightly tighter hot spots reducing clear timer interval increasing hot spot detector counter size 
additional fields bbb occupies approximately kb hardware 
instruction fetch mechanism simulations sequential block instruction fetch unit performed featuring kb way set associative byte line split block cycle penalty icache 
icache consists kb way set associative byte line split block cycle penalty cache 
fetch units coupled trace caches featuring kb kb way set associative byte lines 
table summarizes various configurations 
trace caches allowed form traces remapped code 
simulated icache model split block configuration line divided banks 
request falls second bank bank subsequent cache line returned 
instruction buffer capable delivering sixteen instructions cycle decoders issue instructions past taken branch 
branches may issued cycle instructions fill buffer fall third branch verified predicted path 
icache assumes information identify instruction boundaries branches 
bit history gshare branch predictor modeled pattern history table consisting entries bit counters capable predictions cycle 
addition conditional branch predictor entry return address stack entry indirect address predictor 
model ideal btb isolate effect storing entry points btb 
entry point replacement policy deferred 
model trace cache indexed trace starting address allows partial matches ability fetch trace prediction mismatch 
trace cache models kb kb coupled icache branch predictor icache 
fetch request units accessed parallel trace cache hit takes precedence icache hit caches icache accessed 
trace cache block modeled design 
cache line bytes wide slots instructions branches 
target addresses stored line provide fetch address case partial matching 
traces limit instructions branches reached indirect branch instruction encountered 
traces built basic block granularity fraction taken control flow go ksim gcc li ijpeg perl vortex word word excel pd pd returns indirect calls calls indirect 
taken branches reduction taken control flow instructions remapped code compared original code 
half line wasted case partial blocks may filled 
trace cache utilizes branch bias table entries approximately bytes facilitate branch promotion traces 
including additional target addresses tag stored cache line combined size kb trace cache entry approximately kb 
call inline return inline instructions require branch prediction consume branch resources allowed cycle 
call inlines unconditional control flow return inlines return inlined caller emitted code cache respective call inline trace 
predicted statically functions modify return address result branch misprediction returning 
call indirect inline jmp indirect inline terminate trace trace cache counterparts 
fetched icache instructions predicted conditional branches 
fall prediction causes issue inlined target taken prediction relies indirect predictor obtain new fetch address 
performance remapped code summarizes reduction taken instructions due remapping optimization 
pair bars benchmark normalized taken control transfers original code 
bars remapped applications include taken control transfers code cache original code 
average reduction seen benchmarks signifying effectiveness code straightening techniques 
notice call return inlining particularly effective removing taken control transfers vortex sizeable amounts benchmarks 
code straightening techniques conditional branches yield average reduction taken control transfers pd pd 
results show dramatic reduction number taken branches benchmarks 
table summarizes average static length useful lifetime traces formed remapping system 
measure effective trace length time trace entered number static instructions executed exiting trace length static inst 
executed entry trace age millions inst 
executed creation table aggregate trace characteristics 
benchmark remap scan fill code entry code pending mode size kb points go ksim gcc compress li ijpeg perl vortex word word excel pd pd average table benchmark remapping results 
trace counted 
distribution shows static instructions executed indicating traces may expose optimization opportunities trace cache 
measure useful lifetime traces time trace entered number instructions executed installation trace calculated 
age executed trace usually greater instructions entire runtime benchmarks provide opportunity optimization 
table presents results remapping optimization system 
large percentage dynamic execution occurs remapped code 
typically percent execution spent looking traces form hot spot scan pending modes small percentage spent remapping code fill mode 
remapping process requires cycles remapped instruction total overhead 
evaluate effectiveness layout optimizations benchmark simulated different fetch unit configurations 
shows performance various fetch mechanisms 
optimizations targeted fetch unit fetched instructions cycle metric selected appropriate gauge effectiveness 
bar graph represents baseline native applications 
aggressive multiple block fetch unit operating original code 
second bar depicts remapped code averages improvement base case 
improvement achieved comparably sized trace cache 
adding trace cache addition remapping hardware yields benefit base case 
larger trace cache approximately times larger size remapping hardware improved base remapping hardware included 
despite large reduction taken control transfers shown necessarily scale fetch ipc ic kb ic kb remap ic kb tc kb ic kb tc kb remap tc ic kb tc kb go ksim gcc compress li ijpeg perl vortex word word excel pd pd average ic kb tc kb remap tc fetched ipc various fetch mechanisms 
accordingly 
primarily branch mispredictions cause dramatic number stall cycles lessens effect improving throughput useful cycles 
advantage system trace cache ability inline returns indirect branches 
benefit inlining returns evident trace constructed example hot spot li benchmark shown 
tgu forms single trace begins prior call continues hot branches inlining calls returns tgu terminates trace maximum number allowed path branches exceeded 
trace instructions long represents approximately program execution achieves average executing trace 
addition traces may include loops shown 
conveying loop structure potentially allows better optimization performed simpler traces 

innovations microprocessor design processor control execute code optimally 
system advances state art allowing processor detect frequently executed code perform code straightening partial function inlining loop unrolling optimizations deploy code immediate transparent user application 
detection extraction frequently executed code done retirement stage processor timing critical paths 
preliminary results show optimizations applied system achieve significant fetch performance improvement little extra hardware cost 
addition remapped code consists important persistent traces mechanism creates opportunities aggressive optimizations 

acknowledgments authors members impact research group especially joe john valuable insight assistance developing 
prof sanjay patel insight operation trace cache anonymous referees constructive comments 
research supported advanced micro devices hewlett packard intel microsoft 

bala duesterwald banerjia 
transparent dynamic optimization design implementation dynamo 
technical report hpl hewlett packard laboratories cambridge june 
conte menezes mills patel 
optimization instruction fetch mechanisms high issue rates 
proc 
nd annual int symp 
computer architecture pages june 
ebcioglu altman 
daisy dynamic compilation architectural compatibility 
proc 
th int symp 
computer architecture pages june 
friendly patel patt 
putting fill unit dynamic optimizations trace cache microprocessors 
proc 
th annual ieee acm int symp 
microarchitecture pages december 

digital fx combining emulation binary translation 
digital technical journal august 
hwu mahlke chen chang warter hank holm 
superblock effective technique vliw superscalar compilation 
journal supercomputing january 
trick george gyllenhaal hwu 
hardware driven profiling scheme identifying program hot spots support runtime optimization 
proc 
int symp 
computer architecture pages may 
patel friendly patt 
evaluation design options trace cache fetch mechanism 
ieee transactions computers special issue cache memory related problems february 
pettis hansen 
profile guided code positioning 
proc 
acm sigplan conf 
programming language design implementation pages june 
pey navarro torrellas 
software trace cache 
proc 
int conf 
supercomputing june 
rotenberg bennett smith 
trace cache low latency approach high bandwidth instruction fetching 
proc 
th int symp 
microarchitecture pages december 
