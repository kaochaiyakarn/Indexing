neural decoding movements linear nonlinear trajectory models byron yu john cunningham krishna shenoy dept electrical engineering neurosciences program stanford university stanford ca usa gatsby computational neuroscience unit ucl london uk shenoy stanford edu gatsby ucl ac uk 
date neural decoding time evolving physical state example path foraging rat arm movements largely carried linear trajectory models primarily due computational efficiency 
possibility better capturing statistics movements nonlinear trajectory models yielding accurate decoded trajectories 
nonlinear decoding usually carries higher computational cost important consideration real time settings 
techniques nonlinear decoding employing modal gaussian approximations propagation gaussian quadrature 
compare decoding accuracy versus computation time tradeoffs high dimensional simulated neural spike counts 
key words nonlinear dynamical models nonlinear state estimation neural decoding neural expectation propagation gaussian quadrature consider problem decoding time evolving physical state neural spike trains 
examples include decoding path foraging rat hippocampal neurons decoding arm trajectory motor cortical neurons 
advances area enabled development neural devices seek allow disabled patients regain motor function limbs computer cursors controlled neural activity 
decoders including population vectors linear filters linearly map observed neural activity estimate physical state 
direct linear mappings effective recursive bayesian decoders shown provide accurate trajectory estimates 
addition recursive bayesian decoders provide confidence regions trajectory estimates allow nonlinear relationships neural decoding movements neural activity physical state variables 
recursive bayesian decoders specification probabilistic model comprising trajectory model describes physical state variables change time step observation model describes observed neural activity relates time evolving physical state 
function trajectory model build decoder prior knowledge form trajectories 
case decoding arm movements trajectory model may reflect hard physical constraints limb example elbow bend backward soft control constraints imposed neural mechanisms example arm move smoothly jerky motion physical surroundings person objectives environment 
degree trajectory model captures statistics actual movements directly affects accuracy trajectories decoded neural data 
commonly trajectory models assume linear dynamics perturbed gaussian noise refer collectively linear gaussian models 
family linear gaussian models includes random walk model constant time varying forcing term forcing term time varying state transition matrix higher order markov dependencies 
linear gaussian models successfully applied decoding path foraging rat arm trajectories ellipse tracing pursuit tracking center reach tasks 
linear gaussian models widely primarily due computational efficiency important consideration real time decoding applications 
particular types movements family linear gaussian models may restrictive unable capture salient properties observed movements 
proposed general approach constructing trajectory models exhibit complex dynamical behaviors decoder implemented running time parallel implementation simpler trajectory models 
particular demonstrated probabilistic mixture linear gaussian trajectory models accurate limited regime movement capture salient properties goal directed reaches multiple targets 
mixture model yielded accurate decoded trajectories single linear gaussian model viewed discrete approximation single unified trajectory model nonlinear dynamics 
alternate approach decode single unified nonlinear trajectory model discretization 
decoding problem difficult nonlinear transformations parametric distributions typically longer easily parametrized 
state estimation nonlinear dynamical systems field active research substantial progress years including application numerical quadrature techniques dynamical systems development expectation propagation ep application dynamical systems improvement com neural decoding movements putational efficiency monte carlo techniques 
techniques rigorously tested compared context neural decoding typically involves observations high dimensional vectors non negative integers 
particular tradeoff decoding accuracy computational cost different neural decoding algorithms studied detail 
knowing accuracy computational cost tradeoff important real time applications may need select accurate algorithm computational budget computationally intensive algorithm minimal acceptable decoding accuracy 
takes step direction comparing particular deterministic gaussian approximations 
section introduce nonlinear dynamical model neural spike counts decoding problem 
sections detail deterministic gaussian approximations focus report global laplace gaussian quadrature ep gq ep laplace propagation lp 
section compare decoding accuracy versus computational cost techniques 
nonlinear dynamical model neural decoding report consider nonlinear dynamical models neural spike counts form xt xt xt xt poisson xt xt vector containing physical state variables time 

corresponding observed spike count neuron 
taken time bin width covariance matrix 
functions general nonlinear 
initial state gaussian distributed 
notational compactness spike counts simultaneously recorded neurons assembled vector yt ith element note observations typically equations referred trajectory observation models respectively 
task neural decoding involves finding timepoint physical states xt neural activity observed time 
words seek compute filtered state posterior xt previously showed estimate filtered state posterior linear function 
consider compute xt nonlinear 
extended kalman filter ekf commonly technique nonlinear state estimation 
unfortunately directly applied current problem observation noise additive gaussian 
possible alternatives unscented kalman filter ukf quadrature kalman filter employ quadrature neural decoding movements techniques approximate gaussian integrals analytically intractable 
ukf shown outperform ekf ukf requires making gaussian approximations observation space 
property ukf undesirable standpoint current problem observed spike counts typically due relatively short distinctly non gaussian 
result ukf yielded substantially lower decoding accuracy techniques sections gaussian approximations state space 
tested number quadrature points required grows geometrically quickly impractical moderate values longer consider ukf remainder 
decoding techniques described sections naturally yield smoothed state posterior xt filtered state posterior xt 
focus smoothed state posterior 
filtered state posterior time easily obtained smoothing observations timepoints 
global laplace idea estimate joint state posterior entire sequence global state posterior gaussian matched location curvature mode laplace method 
mode defined argmax argmax log log log xt xt log xt 
known distributions gradients computed exactly local mode applying gradient optimization technique 
global state posterior approximated 
expectation propagation briefly summarize application ep dynamical models 
details cited 
primary distributions interest marginal xt pairwise joint neural decoding movements xt xt state posteriors 
distributions expressed terms forward backward messages follows xt xt xt xt xt xt xt xt yt xt xt xt xt xt xt messages typically approximated exponential family density unnormalized gaussian 
approximate messages iteratively updated matching expected sufficient statistics marginal posterior pairwise joint posterior 
updates usually performed sequentially multiple forward backward passes 
forward pass updated remain fixed xt xt xt xt yt xt xt dxt xt xt dxt xt xt xt xt dxt xt xt exponential family distribution expected sufficient statistics matched xt xt xt xt assumed gaussian 
backward pass proceeds similarly updated remain fixed 
decoded trajectory obtained combining messages shown completing forwardbackward passes 
section investigate accuracy computational cost tradeoff different numbers forward backward iterations 
expected sufficient statistics moments xt xt typically computed analytically nonlinear dynamical model approximated gaussian quadrature 
ep decoder referred gq ep 
applying ideas laplace propagation lp closely related decoder developed uses modal gaussian approximation xt xt matching moments 
technique uses message passing scheme gq ep referred lp 
practice possible encounter invalid message updates 
example variance xt numerator larger denominator due approximation error choice update rule assign xt negative variance 
way problem simply skip message update hope update longer invalid approximating distributions assumed gaussian equivalent matching moments 
neural decoding movements forward backward iteration 
alternative set xt guarantees valid update xt 
referred update implications decoding accuracy computation time considered section 
results evaluated decoding accuracy versus computational cost techniques described sections 
performance comparisons model erf log ix di parameters rp ci rp di error function erf acts element element argument 
chosen dynamics fully connected recurrent network due nonlinear nature claims suitability particular decoding applications rat paths arm trajectories 
recurrent networks directly model neural activity important emphasize vector physical state variables decoded vector neural activity 
generated state trajectories time points corresponding spike counts model model parameters randomly chosen range provided biologically realistic spike counts typically spike bin 
time constant set 
understand algorithms scale different numbers physical state variables observed neurons considered pairings 
pairing repeated procedure times 
global laplace decoder modal trajectory re conjugate gradients quadratic cubic line searches wolfe powell stopping criteria minimize carl rasmussen available www tuebingen mpg de bs people carl code minimize 
stabilize gq ep modal gaussian proposal distribution custom precision quadrature rule non negative quadrature weights described 
gq ep lp minimize find mode xt xt fig 
illustrates decoding accuracy versus computation time techniques 
decoding accuracy measured evaluating marginal state posteriors xt actual trajectory 
higher log probability accurate decoder 
panel corresponds different number state variables observed neurons 
gq ep dotted line lp solid line varied number forward backward iterations circles decoders 
panels global laplace required computation time yielded state neural decoding movements log probability log probability computation time sec computation time sec computation time sec fig 

decoding accuracy versus computation time global laplace line gq ep dotted line lp solid line 

circles bars represent mean sem 
variability computation time represented plots negligible 
computation times obtained ghz amd athlon processor gb ram running matlab 
note scale vertical axes panel error bars small seen 
estimates accurate accurate techniques 
key result report 
implemented basic particle smoother number particles chosen computation time order shown fig 
results shown 
particle smoother yielded substantially lower decoding accuracy global laplace gq ep lp deterministic techniques compared developed monte carlo techniques described section 
fig 
shows techniques computation times scale number state variables neurons particular required computational time typically scales sub linearly increases far sublinearly increases increases accuracies techniques similar note different panels different vertical scales advantage performing multiple forward backward iterations gq ep lp 
decoding accuracy required computation time typically increase number iterations 
cases gq ep fig 
possible accuracy decrease slightly going iterations presumably due sided updates 
theory gq ep require greater computation time lp needs perform modal gaussian approximation proposal distribution gaussian quadrature 
practice possible lp neural decoding movements slower needs sided updates cf 
fig 
sided updates usual update fails 
furthermore lp required greater computation time fig 
fig 
due need sided updates despite having times fewer neurons 
previously shown local optimum solution global laplace fixed point lp 
modal gaussian approximation matches local curvature second order shown estimated covariances global laplace lp equal 
empirically statements true sided updates required lp 
due connections global laplace lp accuracy lp forward backward iterations similar global laplace panels fig 

lp may computational savings compared global laplace certain applications global laplace substantially faster particular graph structure described 
deterministic techniques nonlinear state estimation global laplace gq ep lp compared decoding accuracy versus computation cost context neural decoding involving high dimensional observations non negative integers 
extended directions 
deterministic techniques compared developed monte carlo techniques yielded increased accuracy reduced computational cost compared basic particle filter smoother applications neural decoding 
examples include gaussian particle filter sigma point particle filter embedded hidden markov model 
second compared decoders particular non linear trajectory model 
non linear trajectory models model describing primate arm movements tested see decoders similar accuracy computational cost tradeoffs shown 
acknowledgments 
supported nih fellowship nsf graduate research fellowship gatsby charitable foundation michael flynn stanford graduate fellowship christopher reeve foundation burroughs fund career award biomedical sciences stanford center integrated systems nsf center systems engineering caltech office naval research sloan foundation whitaker foundation 

brown frank tang quirk wilson statistical paradigm neural spike train decoding applied position prediction neural decoding movements ensemble firing patterns rat hippocampal place cells 
neurosci 
zhang ginzburg mcnaughton sejnowski interpreting neuronal population activity reconstruction unified framework application hippocampal place cells 
neurophysiol 
beck laubach chapin kim biggs srinivasan real time prediction hand trajectory ensembles cortical neurons primates 
nature 
schwartz taylor extraction algorithms cortical control arm 
curr opin 
fellows donoghue robustness decoding algorithms 
biol cybern 
rojas kass recursive bayesian decoding motor cortical signals particle filtering 
neurophysiol 
wu black mumford gao bienenstock donoghue modeling decoding motor cortical activity switching kalman filter 
ieee trans biomed eng 
yu ryu meng shenoy mixture trajectory models neural decoding goal directed movements 
neurophysiol 
chapin markowitz real time control robot arm simultaneously recorded neurons motor cortex 
nat neurosci 
fellows donoghue instant neural control movement signal 

taylor schwartz direct cortical control devices 
science 
doherty santucci dimitrov patil learning control brain machine interface reaching grasping primates 
biology 
corneil andersen cognitive control signals neural 
science 
ryu yu shenoy high performance brain computer interface 
nature 
hochberg caplan chen penn donoghue neuronal ensemble control devices human 
nature 
wu gao bienenstock donoghue black bayesian population decoding motor cortical activity kalman filter 
neural comput 
meng optimal estimation feed forward controlled linear systems 
proc ieee icassp 

srinivasan eden willsky brown state space analysis reconstruction goal directed movements neural signals 
neural comput 
srinivasan brown state space framework movement control dynamic goals brain driven interfaces 
ieee trans biomed eng neural decoding movements 
shoham fellows donoghue normann statistical encoding model primary motor cortical interface 
ieee trans biomed eng 
wan van der merwe unscented kalman filter 
haykin ed kalman filtering neural networks 
wiley publishing 
julier uhlmann unscented filtering nonlinear estimation 
proceedings ieee 
haykin elliott discrete time nonlinear filtering algorithms gauss hermite quadrature 
proceedings ieee 
minka expectation propagation approximate bayesian inference 
proceedings th conference uncertainty artificial intelligence uai 

heskes expectation propagation approximate inference dynamic bayesian networks 
darwiche friedman eds proceedings uai 

heskes improved unscented kalman smoothing stock volatility estimation 
barros principe larsen douglas eds proceedings ieee workshop machine learning signal processing 

heskes novel approximations inference nonlinear dynamical systems expectation propagation 
neurocomputing 
yu shenoy expectation propagation inference non linear dynamical models poisson observations 
proc ieee nonlinear statistical signal processing workshop 

doucet de freitas gordon eds sequential monte carlo methods practice 
springer verlag 
van der merwe wan sigma point kalman filters probabilistic inference dynamic state space models 
proceedings workshop advances machine learning 

gaussian particle filtering 
ieee transactions signal processing 
mackay information theory inference learning algorithms 
cambridge university press 
smola vishwanathan eskin laplace propagation 
thrun saul sch lkopf eds advances neural information processing systems 
mit press cambridge ma 
minka lafferty expectation propagation generative aspect model 
proceedings th conference uncertainty artificial intelligence uai 

doucet godsill andrieu sequential monte carlo sampling methods bayesian filtering 
statistics computing 
neal beal roweis inferring state sequences non linear systems embedded hidden markov models 
thrun saul sch lkopf eds advances neural information processing systems 
mit press cambridge ma 
chan moran computational model primate arm hand position joint angles joint torques muscle forces 
neural eng 
