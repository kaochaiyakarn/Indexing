proceedings tenth national conference onarti cial intelligence 
san jose aaai press 
analysis bayesian classi ers pat langley wayne iba kevin thompson iba ptolemy arc nasa gov ai research branch nasa ames research center mo ett field ca usa case analysis bayesian classi er simple induction algorithm fares remarkably learning tasks 
analysis assumes monotone conjunctive target concept independent noise free boolean attributes 
calculate probability algorithm induce arbitrary pair concept descriptions compute probability correct classi cation instance space 
analysis takes account number training instances number attributes distribution attributes level class noise 
explore behavioral implications analysis presenting predicted learning curves arti cial domains give experimental results domains check reasoning 
probabilistic approaches induction goal research machine learning discover principles relate algorithms domain characteristics behavior 
researchers carried systematic experimentation natural arti cial domains search empirical regularities kibler langley 
focused theoretical analyses paradigm probably approximately correct learning haussler 
experimental studies informal analyses learning task formal analyses address worst case bear little relation empirical results 
third approach proposed cohen howe involves formulation average case models speci algorithms testing experimentation 
pazzani study conjunctive learning provides excellent example technique hirschberg pazzani inducing cnf concepts 
assuming information target concept num recom technologies sterling software 
ber attributes class attribute frequencies obtain predictions behavior induction algorithms experiments check analyses 
research focus algorithms typically experimental practical sides machine learning case analyses extended methods 
growing interest probabilistic approaches inductive learning 
example fisher described cobweb incremental algorithm conceptual clustering draws heavily bayesian ideas literature reports number systems build allen langley iba gennari thompson langley 
cheeseman 
outlined auto class nonincremental system uses bayesian methods cluster instances groups researchers focused induction bayesian inference networks cooper 
bayesian learning algorithms complex easily amenable analysis share common ancestor simpler tractable 
supervised algorithm simply bayesian classi er comes originally pattern recognition duda hart 
method stores probabilistic summary class summary contains conditional probability ofeach attribute value class probability base rate class 
data structure approximates representational power perceptron describes single decision boundary instance space 
algorithm encounters new instance updates probabilities stored speci ed class 
order training instances occurrence classi cation errors ect process 
test instance classi er uses evaluation function describe detail rank alter related approach involves deriving optimal learning algorithm certain assumptions implementing approximation algorithm opper haussler 
domain bayes ind freq 
soybean chess splice promoters table percentage accuracies induction algorithms classi cation domains accuracy predicting frequent class 
native classes probabilistic summaries assigns instance highest scoring class 
evaluation function summary descriptions bayesian classi ers assume attributes statistically independent 
unrealistic natural domains researchers concluded algorithm behave poorly comparison induction methods 
studies examined extent violation assumption leads performance degradation probabilistic approach quite robust respect noise irrelevant attributes 
earlier studies clark niblett evidence practicality algorithm 
table presents additional experimental evidence utility classi ers 
study compare method ind emulation algorithm buntine caruana algorithm simply predicts modal class 
domains uci database collection murphy aha include small soybean dataset chess games involving king rook king pawn confrontation cases lymphography diseases biological datasets 
domain randomly split data set training instances test instances repeating process obtain separate pairs training test sets 
table shows mean accuracy con dence test sets domain 
domains bayesian classi er accurate reimplementation 
argue bayesian classi er superior sophisticated method results show behaves variety domains 
bayesian classi er promising induction algorithm deserves closer inspection careful analysis give insights behavior 
simplify matters limiting analysis induction conjunctive concepts 
furthermore assume classes attribute boolean attributes independent 
divide study parts 
rst determine probability analysis bayesian classifiers gorithm learn particular pair concept descriptions 
derive accuracy arbitrary pair descriptions instances 
taken expressions give accuracy learned concepts 
nd number factors uence behavior algorithm including number training instances number relevant irrelevant attributes amount class attribute noise class attribute frequencies 
examine implications analysis predicting behavior speci domains check reasoning experiments domains 
probability induced concepts consider concept de ned monotone conjunction relevant features features negated 
assume irrelevant features ar ar aj probability feature aj occurring instance 
concept descriptions learned classi er fully determined training instances observed 
compute probability concept description consider di erent possible combinations training instances 
consider probability algorithm observed exactly positive instances 
probability observing positive instance observed fraction positive instances expression represents probability observed exactly negative instances 
assume concept monotone con junctive attributes independent aj simply product probabilities relevant attributes 
agiven number positive instances produce alternative descriptions positive class depending instances observed 
envision concept description cell dimensional matrix dimension ranging count dimension representing number positive instances attribute aj 
envision similar matrix negative instances having dimensionality dimension ranging count dimension representing number negative instances occurred 
shows positive cell matrix 
designated cell holds probability algorithm seen instances instance instances 
matrices index cell concept description length cell probability algorithm produced positive cell matrix attributes 
values axes represent numbers positive instances 
cell indexed vector positive matrix positive instances cell analogous probability cell negative matrix 
weighted product terms gives probability learning algorithm generate particular pair concept descriptions cell kp cell words multiplies probability seeing positive instances probabilities encountering cell positive matrix cell negative matrix 
determine probability cell matrix 
positive matrix straightforward attributes remain independent instance conjunctive concept 
cell yj uj probability cell positive matrix yj represents observed fraction instances attribute aj 
furthermore probability observe aj exactly uj instances uj uj analysis bayesian classifiers uj uj absence noise forall relevant attributes jjc irrelevant attributes 
calculation di cult cells negative matrix 
simply take product probabilities index cell conjunctive concept attributes statistically independent 
compute probability observed negative instances composed particular combination instances 
jjc negative instance multinomial distribution compute probability exactly instances instance instance andd instance expression 


jc jc wjc dw gives probability particular combination negative instances combination compute concept description cell indices result 
course instances may produce concept description simply sums probabilities combinations get total probability cell 
need operational jjc probability ofi negative instance 
absence noise simply ij 
extend framework handle class noise modifying de nitions basic terms 
common de nition class noise involves corruption class names replacing actual class opposite certain probability 
probability class corrupted values noted iba langley 
irrelevant attribute aj probability una ected class noise remains equal aj attribute independent class 
situation relevant attributes complicated 
de nition corrupted conditional probability relevant attribute aj possibly corrupted class aj noisy class probability 
rewrite numerator specify situations corruption class name occur giving jjc zp know relevant attribute aj conjunctive concepts aj involves terms existed corruption class name 
similar reasoning compute probability ofany particular instance negative 
rewrite ij zp case special conditions somewhat di erent 
negative instance jjc second term numerator zero 
contrast positive instance jjc rst term disappears 
taken conditions generate probabilities cells negative matrix added noise class name 
replacing jjc jc jjc jc expressions earlier section compute probability bayesian classi er induce particular pair concept descriptions cells matrices 
information necessary calculation number training instances number relevant irrelevant attributes distributions level class noise 
analysis holds monotone conjunctive concepts domains independent attributes ideas carry restricted classes domains 
accuracy induced concepts calculate accuracy training instances sum expected accuracy possible instance weighted instance probability 
formally expected accuracy ix compute expected accuracy ij instance ij wemust determine pair cells positive negative matrices instance classi cation 
test instance classi ed computing score class description selecting class highest score choosing randomly case ties 
de ne accuracy ij pair concept descriptions tobe cor rectly predicts ij class incorrectly predicts class tie occurs 
previous notation number observed instances number observed positive instances uj number positive instances attribute aj occurs vj number negative instances aj occurs 
instance ij compute score positive class description score uj uj analysis bayesian classifiers aj analogous equation negative class substituting toavoid multiplying attribute observed training instances test instance follow clark niblett suggestion replacing small value 
compute expected accuracy instance sum possible values pairs concept descriptions product probability particular pair concept descriptions positive instances pair accuracy nx ux vx accuracy second third summations occur possible vectors index positive matrix negative matrix complete calculations need expression isthe product probabilities features ini implications learning behavior equations previous sections give formal description bayesian classi er behavior implications obvious 
section examine ects various domain characteristics algorithm classi cation accuracy 
number possible concept descriptions grows exponentially number training instances number attributes predictions limited small number 
addition theoretical predictions report learning curves summarize runs randomly generated training sets 
curve reports average classi cation accuracy runs single test set randomly generated instances containing noise 
case bound mean accuracy con dence intervals show degree predicted learning curves observed ones 
experimental results provide important check reasoning revealed number problems development analysis 
shows ects concept complexity rate learning bayesian classi er noise 
case hold number irrelevant attributes constant hold probability occurrence constant number training instances number relevant attributes determine complexity target concept 
normalize ects base rate probability concept constant means relevant attributes isp varied di erent conditions 
typical learning curves initial accuracies low gradually improve increasing numbers training instances 
ect concept complexity agrees intuitions introducing alternative approach hold constant relevant attributes causing initial accuracies upward little ect learning curves 
probability correct classification relevant number training instances analysis bayesian classifiers probability correct classification class noise class noise class noise number training instances predictive accuracy bayesian classi er conjunctive concept assuming presence irrelevant attribute function training instances number relevant attributes amount class noise 
lines represent theoretical learning curves error bars indicate experimental results 
additional features target concept slows learning rate ect asymptotic accuracy conjunctive concepts test cases 
rate learning appears degrade gracefully increasing complexity 
predicted observed learning curves close agreement lends con dence average case analysis 
theory experiment show similar ects vary attributes learning rate slows introduce misleading features algorithm gradually converges perfect accuracy 
presents similar results interaction class noise number training instances 
hold number relevant attributes constant number constant examine separate levels class noise 
analysis assume test instances free noise normalizes accuracies eases comparison 
expect increasing noise level decreases rate learning 
probabilistic nature bayesian classi er leads graceful degradation asymptotic accuracy una ected 
nd close theoretical behavior experimental learning curves 
analysis incorporate attribute noise experiments factor produce similar results 
case equivalent levels lead somewhat slower learning rates expect attribute noise corrupt multiple values class noise ects 
compare behavior bayesian classi er pazzani 
issue interest number training instances required achieve criterion level accuracy 
quantitative comparison nature scope respective analyses experiments show algorithm ected irrelevant tributes bayesian classi er sensitive number relevant irrelevant attributes 
bayesian classi er robust respect noise algorithm 
discussion analysis bayesian classi er 
treatment requires concept monotone conjunctive instances free attribute noise attributes boolean independent 
information number relevant irrelevant attributes frequencies level class noise equations compute expected classi cation accuracy number training instances 
explore implications analysis plotted predicted behavior algorithm function number training instances number relevant attributes amount noise nding graceful degradation increased 
check analysis run algorithm arti cial domains characteristics 
obtain close ts predicted behavior correcting errors reasoning empirical studies revealed 
additional experiments compare behavior bayesian classi er reimplementation widely algorithm induces decision trees 
general probabilistic method performs comparably despite greater sophistication 
results suggest simple methods deserve increased attention studies theoretical experimental 
plan extend analysis ways 
particular current equations handle class noise angluin laird shown attribute noise problematic learning algorithms 
developed tentative equations case attribute noise expressions complex class noise possible corruption combination attributes instance appear 
need relax constraint target concepts monotone conjunctive 
direction extend involves running additional experiments 
assumptions current analysis empirically study extent violated assumptions alter observed behavior algorithm 
addition analyze attribute frequencies domains commonly experiments determine analytic model ability predict behavior domains frequencies input 
approach extend usefulness average case model articial domains wehave tested date 
encouraged results obtained 
demonstrated simple bayesian classi er compares favorably sophisticated induction algorithm important characterized average case behavior restricted class domains 
analysis con rms intuitions robustness bayesian algorithm face noise concept complexity provides fertile ground research approach induction 
stephanie sage kimball collins andy philips discussions helped clarify ideas 
allen langley 

integrating memory search planning 
proceedings workshop innovative approaches planning scheduling control pp 

san diego morgan kaufmann 
angluin laird 

learning noisy examples 
machine learning 
buntine caruana 

ind recursive partitioning technical report fia 
mo ett field ca nasa ames research center arti cial intelligence research branch 
cheeseman kelly self stutz taylor freeman 

autoclass bayesian classi cation system 
proceedings fifth international conference machine learning pp 

ann arbor mi morgan kaufmann 
clark niblett 

cn induction algorithm 
machine learning 
cohen howe 

evaluation guides ai research 
ai magazine 
analysis bayesian classifiers cooper herskovits 

bayesian method constructing bayesian belief networks databases 
proceedings seventh conference uncertainty arti cial intelligence pp 

los angeles morgan kaufmann 
duda hart 

pattern classi cation scene analysis 
new york john wiley sons 
fisher 

knowledge acquisition incremental conceptual clustering 
machine learning 
haussler 

probably approximately correct learning 
proceedings eighth national conference onarti cial intelligence pp 

boston aaai press 
iba gennari 

learning recognize movements 
fisher pazzani langley eds concept formation knowledge experience unsupervised learning 
san mateo morgan kaufmann 
iba langley 

induction level decision trees 
proceedings ninth international conference machine learning 
aberdeen morgan kaufmann 
hirschberg pazzani 

analysis cnf learning algorithm technical report 
irvine university california department information computer science 
kibler langley 

machine learning experimental science 
proceedings third european working session learning pp 

glasgow pittman 
murphy aha 

uci repository machine learning databases machine readable data repository 
irvine university california department information computer science 
opper haussler 


calculation learning optimal classi cation algorithm learning perceptron noise 
proceedings fourth annual workshop computational learning theory pp 

santa cruz morgan kaufmann 
pazzani 

average case analysis conjunctive learning algorithms 
proceedings seventh international conference learning pp 

austin tx morgan kaufmann 
thompson langley 

concept formation structured domains 
fisher pazzani langley eds concept formation knowledge experience learning 
san mateo morgan kaufmann 
