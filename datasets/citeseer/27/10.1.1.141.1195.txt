proceedings fifteenth annual conference uncertainty artificial intelligence uai pages stockholm sweden august general algorithm approximate inference application hybrid bayes nets daphne koller uri lerner dragomir computer science dept stanford university koller uri cs stanford edu clique tree algorithm standard method doing inference bayesian networks 
works manipulating clique potentials distributions variables clique 
approach works networks limited need maintain exact representation clique potentials 
presents new unified approach combines approximate inference clique tree algorithm circumventing limitation 
known approximate inference algorithms viewed instances approach 
algorithm essentially clique tree propagation approximate inference estimate densities clique 
settings computation approximate clique potential done easily statistical importance sampling 
iterations gradually improve quality estimation 
bayesian networks bns allow represent complex probabilistic models compactly naturally 
range inference algorithms exact approximate developed task probabilistic reasoning bns computing probability events evidence 
bayesian network models designed admitted inference existing algorithms 
years growing interest extending range probabilistic reasoning applications 
domains new challenges bn technology larger complex involve wider range probability models 
commonly algorithm bn inference clique tree algorithm 
basic principle algorithm divide set variables overlapping subsets called cliques 
inference decomposed operations variables single clique 
results computation clique transmitted turn subsequent computation 
complexity algorithm determined size cliques discrete domains roughly exponential number variables largest clique 
clique tree algorithm main advantages 
exact inference algorithm returns correct answer joint distribution represented bn 
second take advantage structural properties domain conditional independences represented bn structure 
independences allow construction small cliques reducing complexity computation 
situations clique tree algorithm breaks 
main difficulty intermediate results cliques get complex represented manipulated effectively 
intermediate result function space values variables discrete bns functions usually represented tables entry assignment appropriate domain 
representation exponential number variables expression 
large bns qmr dt cliques large allow exact representation factors 
problem severe want represent continuous hybrid domains ones involving discrete continuous variables 
case intermediate results cliques typically complex admit closed form representation 
completely different approach taken monte carlo inference algorithms likelihood weighting gibbs sampling extend easily hybrid bns 
idea estimate joint distribution set possibly weighted samples 
unfortunately convergence algorithms quite slow highdimensional spaces samples explore small part space 
general approach combining exact approximate inference cases achieves best worlds 
clique tree algorithm approach builds clique tree propagates messages clique 
computing messages correspond exact intermediate results algorithm uses approximate inference techniques compute manipulate messages 
general scheme instantiations 
vary primary dimensions class functions represent intermediate results approximation algorithm compute new intermediate results previous ones 
example instantiation choose represent intermediate results mixtures gaussians 
compute new intermediate result product factors random sampling generate samples new distribution statistical density estimation techniques em estimate new result 
see scheme raises interesting issues 
example best approximate message clique sends 
restrict complexity message approximation tries fit entire function may optimal 
may better provide better approximation regions space turn important 
course early stages computation know parts space turn important 
approximation appears may turn highly suboptimal 
observation leads iterative approximation intermediate result may estimated times 
believe approach defines interesting useful class approximate inference algorithms 
known approximate inference algorithms viewed special cases scheme 
unified framework allows relate algorithms combine improve performance 
interestingly general framework define new algorithm inference hybrid bayesian networks 
consider particular instantiation general algorithm combines importance sampling statistical density order compute approximate messages clique potentials 
promising empirical results showing approach deals effectively difficult task inference networks 
preliminaries section review inference algorithms form basis discussion 
assume bn variables variable takes values main may discrete continuous 
usual bn directed acyclic graph nodes variables 
set random variables represent joint domain variables denote parents node node associated conditional probability distribution cpd specifies con ditional distribution value defines probability dis tribution density function denote function discrete domains cpd implemented table simply lists appropriate probability instantiation clique tree algorithm algorithm commonly perform exact inference discrete bayes nets 
basic data structure algorithm called clique tree join tree 
clique tree undirected tree nodes called cliques clusters 
clique defined contain subset random variables bn 
cpd assigned clique contain denote set edges clique tree 
edge associated sepset set random variables intersection cliques endpoints edge 
denote sepset denote ways executing inference type data structure 
focus shafer shenoy algorithm suitable purposes avoids division operations 
initially assigned clique assign cpd clique contains denote set cpds assigned clique executes local computations variables clique correspond multiplying messages cpds marginalizing result variables clique summing rest 
results computations sent messages neighboring cliques turn computation 
clique ready send message neighbor received messages neighbors 
algorithm terminates clique sent messages neighbors 
simplicity model algorithm incorporating evidence multiplying cpds clique indicator function evidence relevant clique 
shorthand algorithm exact clique tree propagation repeat choose received incoming msgs compute compute messages sent compute normalize call functions intermediate factors 
final result computation clique potential clique clique potential represents posterior distribution variables conditioned evidence 
high level algorithm extends trivially case continuous variables simply replace summation line integration 
addition exact inference bayesian networks support range approximate inference algorithms 
commonly approach monte carlo sampling 
sampling general purpose technique inference probabilistic models applicability goes far bayesian networks 
basic idea approximate distribution generating independent samples 
estimate value quantity relative original distribution computing value relative samples 
probability distribution space discrete expectation function defined 
continuous case simply replace summation integral 
assume generate set random sam ples compact approximation distribution approximate main problem distribution sample directly 
example bayesian network easily generate samples prior distribution 
far trivial generate samples posterior distribution 
importance sampling provides approach dealing difficulty 
view samples assume desired distribution sampling probability function may normalized 
sample probability distribution sample 
requirement approximate normalized version sampling precisely generate samples sign weight normalize weights samples sum view set weighted samples approximation easily shown general inference algorithm approach simple idea papers see section 
requiring algorithm produce exact result computing fac tors lines alg 
allow produce approximation admits compact representation 
course general idea implemented ways 
choose variety representations factors 
clearly choice representation strong effect quality approximation 
need determine implement various operations factors specified algorithm 
example lines approximate inference algorithm need take set approximate exact messages multiply cpds evidence function generate approximation outgoing message issue complicates representation approximation intermediate factors messages guaranteed 
example circumstances message may simply cpd 
parents distributions cpd joint distribution variables mentions 
density estimation circumscribed general purpose function approximation choose restrict attention densities 
assume ranges continuous variables bounded 
normalize factor density ascribing uniform distributions variables distribution defined factor 
note multiplication constant factor influence correctness algorithm clique potentials normalized 
utilize variety density estimators representing factors 
available approximation techniques course depend choice representation 
example choose single gaussian representation factors compute optimal gaussian approximation relatively easily simply estimating second moments distribution represented factor 
general approach approximation combines importance sampling statistical density estimation 
consider task computing final clique potential 
want generate samples distribution apply density estimation algorithm learn associated density 
certain rare cases sample directly resulting density 
cases importance sampling provides solution described previous section 
assume chosen representation factors 
best approximate obvious solution try find approximation minimizes metric reasonable choice kl distance correct message normalized produce density approximation unfortunately intuition flawed 
need approximate message se 
really want create message give best result approximate potential goal lead different requirements 
intuitively phenomenon clear 
approximation relative kl distance accurate regions density high value 
message multiplied additional factors different function regions low weight original message high weight 
approximation may poor 
general approximation message low kl distance imply result multiplying message factors approximation 
leading provide rough analysis phenomenon 
stress analysis intended exact bound give insight design algorithm 
computation true potential described line algorithm 
easy verify propor tional product factors messages cpds evidence term 
define rest products factors rest sim ply 
minimizing strive reduce rest normalizing constants 
rest note rest best approximation true potential achieve clique possibly approximate incoming messages 
want minimize rest rest equal penalty get inaccuracies proportional marginal density new potential sepset variables accurate areas big necessarily areas big 
similar derivation appears 
problem finding approximation distribution accuracy optimal relative distribution nontrivial 
density estimation algorithms applied naively task learning construct approximation accurate regions high value 
example generated samples samples refined estimate density regions high mass explored different heuristics task follow roughly scheme generate density estimate modify fit approach worked best experiments quite simple 
generate samples case generating samples right regions 
weight samples making estimates desired distribution 
apply density estimation resulting weighted samples 
solution estimated approximate potential computing computing express purpose compute ways resolve problem 
current best approximation potential approach plausible misguided 
sending message information available approximation known flawed guide message approximation 
second solution observation interested marginal approximation marginal mentioned fact need send message indicates informed sense guide approximation message course intuition partially valid may ways informed general note choice function guide approximation heuristic influence correctness message 
function derive sampling distribution weights samples guarantee actual function approximated observation immediately leads final major improvement basic algorithm iterative improve ment messages 
send message rely current approximation may get message clique leads accurate approximation sends message new improved approxi mation basis approximating resulting approximate message certainly reliable 
reliable outgoing messages reliable 
algorithm iterative improvement algorithm 
continues improve current best estimate clique potential produce accurate message 
accurate messages turn compute accurate approximate potentials destination cliques 
ready general algorithm 
point leave issues open different implementations 
show existing algorithms instances general scheme new instantiation detail 
phase virtually algorithm approximate computation place exact 
omit details 
second phase algorithm iterates cliques trying improve approximation potentials messages algorithm iteration phase approximate propagation repeat choose clique approximate set generate estimate reweight convergence intuitively algorithm repeatedly selects clique improvement criterion 
estimates revised clique potential uses basis re estimating messages 
described message estimated match current potential 
reweighted represent correct function send clearly instantiations general schema 
example algorithm select clique potential update variety ways 
simplest cheapest approach execute repeated upward downward passes mirroring computation standard clique tree inference 
mentioned earlier approximation phase done ways 
approach importance sampling possibly followed density estimation 
particular instantiation algorithm approximations done technique 
initial calibration phase approximation prior distribution evidence inserted sampling distribution 
obtain approximation prior distribution sampling original bn simple top sampling algorithm 
ability effectively due precisely fact sampling prior 
iteration phase current posterior sampling distribution discussed 
algorithm hybrid bns section describe detail instantiation general approach described designed perform effective inference general hybrid bns 
algorithms exact inference hybrid bns applicable narrow families bns multivariate gaussians sense conditional gaussians 
algorithm general sense deal virtually function cpd long compute value function point sample value node conditional density defined particular assignment parents 
flexibility allows algorithm deal significantly interesting models 
particular algorithm capable handling arbitrary dependencies including ones discrete node continuous parents 
initial implementation incorporates basic flexible cpds allow represent fairly wide range models 
discrete child discrete parents implemented standard conditional probability table model 
continuous node discrete continuous parents implemented standard conditional linear gaussian model 
model child mean linear function continuous parents covariance fixed 
separate linear gaussian model child value discrete parents 
allow uniform distributions 
cpds fairly standard defined new class cpds modeling dependence discrete node discrete continuous parents 
cpd interesting useful generalization standard softmax density 
conditional linear gaussian cpd softmax cpd separate component instantiation discrete parents 
suffices describe model case discrete node continuous parents 
intuitively softmax cpd defines set regions parameter choice 
regions defined set linear functions continuous variables 
region characterized part space particular linear function higher 
region associated distribution values discrete child distribution variable region 
actual cpd continuous version region idea allowing smooth transitions distributions neighboring regions space 
precisely discrete variable continuous parents assume possible values regions defined vectors parameters vector vector weights specifying linear function associated region 
vector probability distribution associated region 
cpd defined words distribution weighted average region distributions weight region depends exponentially high value defining linear function relative rest 
power choose number regions large wish key rich expressive power generalized softmax cpd 
demonstrates expressivity 
example cpd binary variable regions 
show cpd represent simple classifier 
sensor values low medium high 
probability values depends value continuous parent note easily accomodate variety noise models sensor reliable borderline situations making transitions regions moderate inherently noisy having probabilities different values regions farther away 
important decision general algorithm decision actual representation potentials 
case needed representation enables express hybrid density functions 
furthermore looking representation samples generated easily turn easily estimated set samples 
chose focus density trees 
structure density tree resembles classification decision tree representing conditional distribution class variable set features density tree simply represents unconditional density function set random variables density tree types nodes interior nodes leaves 
interior node defines set mutually exclusive exhaustive events branch corresponds events 
definition allows arbitrary events example may interior node branches corresponding definition generalizes 
edge corresponding outcome labeled conditional probabil ity events path root node define probability path product probabilities path probability conjunction events path 
leaf tree corresponds probability distribution consistent events path leading inconsistent path events probability zero 
find probability instantiation traverse tree root leaf unique path consistent traversal compute path probability 
multiply path probability probability assigned leaf 
implementation uses events possible values discrete variables possible value get branch 
restriction simplifies learning algorithm 
means value variable appearing path determined leaf leaf simply joint distribution remaining variables 
chosen fairly simply representation leaves 
discrete variables assumed independent simply keep marginal distribution 
dependency manifested splits tree 
continuous variables independent discrete variables 
multivariate distribution discrete variables represented mixture gaussians 
basic operations supported density trees computing probability instantiation marginalization introducing evidence tree instantiation random variables tree sampling 
operations fairly easy implement 
marginalization instantiation done time linear size tree sampling finding probability instantiation done time linear depth tree 
note possible representations joint density bn necessarily linear time guarantees 
remains describe process density tree gaussian mixture leaves constructed weighted samples 
build tree start recursively root 
node determine discrete variable want split 
discussed relative entropy empirical distribution split appropriate error function 
uniform distributions leaf leading simple splitting criterion 
case uniform leaf distributions clearly appropriate representation leaf product multinomials mixture gaussians allows accurate density learning 
associated splitting rule relative entropy error function expensive compute 
simple heuristic rule splitting variable partitions samples equally possible branches 
simple heuristic density trees yield results 
order avoid overfitting splitting node contains minimal number samples 
note splitting criterion splitting rule samples weight 
property particularly helpful tailoring estimate message accurate areas clique potential large 
samples generated clique potential samples regions clique potential high density leads refined partition regions message density low 
leaves reached marginals discrete variables estimated simple bayesian estimation algorithm dirichlet prior regularization 
density continuous variables estimated mixture gaussians diagonal covariance matrix 
execute estimation variant standard em algorithm gaussian mixtures 
unfortunately em behaved gaussian mixtures discrete settings 
problem likelihood data arbitrarily large allow variance estimated gaussians go zero 
techniques preventing situation 
experimented 
simple commonly technique assume low medium high outdoor temperature indoor temperature thermostat reading season thermostat ok outdoor temperature indoor temperature thermostat reading thermostat ok expressive power generalized softmax cpd 
thermostat network 
gaussians mixture covariance matrix 
second turned better version em minimizes regularized error function standard negative log likelihood error function analogous regularized error function neural networks 
specifically represent training instances instantiation variables 
simplicity ignore weights instances 
assume trying estimate distribution mixture gaussians 
em algorithm setting uses current set parameters means variances gaussians weights mixture components estimate gaussian data instance uses results minimize negative log likelihood function leading stan dard update rules means covariances see 
try minimize regularized error function penalizes algorithm letting variances get small 
letting denote variance th mixture component define error function regularization efficient determines extent penalty 
derivative relative setting zero obtain setting minimizes function mean gaussian th dimension 
part expression identical standard em update rule second part causes covariance larger warranted data 
note extent increase depends weight samples assigned firmly mixture component samples associated firmly gaussian effect second term variance 
precisely behavior 
see section regularization coefficient leads significantly better results 
instructive compare algorithm simple pre discretization approach bns 
main problem high dimensional cliques finely discretized variables large fit memory making exact inference impractical 
ignore memory constraints domains impossible 
may want adapt parameters new data obtained requiring re discretization time 
furthermore cases function expensive evaluate small number times 
example function comes visual tracking domain cpd defining probability image state system 
computing function involves geometric projection rendering quite expensive 
experimental results tested algorithm networks 
continuous version real life bat network 
experiments time slices bat network leading variables continuous 
wanted test network continuous parents discrete node version bat 
constructed small hybrid bn shown models simple thermostat 
thermostat reading variable classifier node described 
evaluate algorithm needed find approximation ground truth 
discretized continuous variables bns discrete values 
computed kl distance result exact inference discrete network discretization result approximate inference hybrid network 
refer measure kl error 
note obtaining accurate discrete estimator done general networks discussed 
experiments ensure clique tree contained continuous variables 
investigating influence regular lambda lambda lambda lambda lambda lambda quality density estimation function 
kl error function ization coefficient em 
consider pure density estimation problem temporarily ignoring algorithm 
shows result fitting gaussians set samples created runs shown top right side different values runs different values example overfits data capture correctly area samples concentrated plausible result 
examining kl error running algorithm gives similar results 
case averaged runs thermostat network querying outside temperature time thermostat readings times 
graph shows kl errors iteration iteration defined pass propagation clique tree stream stream 
small get gaussians small variance 
result get estimation density areas true density larger 
leads big kl error shown graph 
hand big get estimation uniform distribution 
gives smaller kl error useful estimation 
give results 
give important parameter influences running time algorithm number samples drawn clique 
table shows time iteration sun ultra bat network function number samples average kl error iterations 
cliques join tree 
samples iteration sec avg kl error results hardly surprising 
running time algorithm grows linearly number samples error decreases increase number samples 
natural improvement algorithm start relatively small number samples order get initial estimate potentials increase number samples iterations 
sampling algorithms algorithm sensitive likelihood evidence 
claim iterative nature algorithm solves problem partially 
tested algorithm timeslice bat network 
time slice bat contains continuous variables xdot velocity car axis left right movement xdot sensed sensed value xdot 
probability sensor functional reading xdot plus gaussian noise 
probability sensor broken reading uniform independent xdot 
tested scenarios 
instantiated xdot time xdot sensed time queried time 
case instantiated variables relatively close values situation 
second different values leading scenario fact exact discrete inference probability sensor broken case went 
expected algorithm performed better case 
demonstrated 
see density estimation time easy case 
expected algorithm converges quickly correct answer 
see convergence second difficult case 
time quality estimation convergence slower 
sense second scenario encouraging 
algorithm slowly improve quality estimation 
fact density go zero far peak indicates iterations algorithm correctly identified relatively high probability sensor broken 
part demonstrates point kl error cases 
interesting compare performance algorithm likelihood weighting 
fair comparison gave algorithms cpu time compared kl error 
algorithm takes advantage structure network reduce dimensionality samples drawn expected algorithm scale better lot evidence introduced network 
test hypothesis ran experiments evidence node evidence nodes note evidence 
cases queried node xdot time 
results averaged runs 
see lw provides rough initial estimate short time algorithm requires time perform initial clique estimation iteration 
startup cost algorithm outperforms lw just evidence node 
difference dramatic presence evidence 
performance lw degrades algorithm unaffected extra evidence 
kl error exact answer iteration iteration iteration likelihood weighting samples clique samples clique cpu time exact answer iteration iteration iteration easy scenario difficult scenario effect iterations easy scenario 
difficult scenario 
kl error 
kl error likelihood samples clique samples clique cpu time comparison lw evidence node evidence nodes existing instances approach known approximate inference algorithms turn special cases general scheme 
viewing perspective understand relationship inference algorithms 
importantly point ideas possible improvements general approach 
briefly sketch crucial properties mapping 
conditional gaussian cg networks algorithm representation clique potentials table entry combination discrete variables clique 
entry table specifies single multivariate gaussian continuous variables clique 
approximate computation messages done exact algorithm gets best gaussian approximation factor 
lauritzen shows algorithm preserves correct second moments propagation algorithm exact respect 
iterations improve quality result 
hybrid propagation hugs family algorithms quite closely related uses sampling substitute exact operations clique 
restricted messages vectors weighted samples type density estimation smoothing 
importantly algorithm samples clique variable restricted take sampled values 
sampled values poor due example uninformed sampling message propagation resulting value space variable potentially badly skew remainder computation 
algorithms contain iterative phase phase may help address limitation 
dynamic discretization algorithm works hybrid bns instance general approach 
uses piecewise constant representation domain continuous variables clique 
representation structured hierarchically regions split finer pieces tree 
instances algorithm iterative phase 
iterations limited complete upward downward passes 
algorithm flexible ability concentrate computational efforts specific areas clique tree approximation inaccurate 
likelihood weighting roughly speaking algorithm samples bn weighting sample likelihood evidence relative sample 
viewed special case algorithm clique tree single clique containing network variables 
representation densities set samples single iteration performed 
introduce iterations algorithm turns bootstrap sampling algorithm set samples basis sampling 
add density estimation step algorithm turns smoothed bootstrap algorithm 
survival fittest sof extensions algorithm performs monitoring dynamic bayesian network dbn 
maintains belief state distribution current state evidence far 
sof maintains belief state time set weighted samples 
get time samples sof samples time samples proportionately weight stochastically propagates time slice weights likelihood time evidence 
terms general approach algorithm performs single forward pass basic clique tree unrolled dbn containing clique pair consecutive time slices 
message generated simple importance sampling 
extension viewed augmenting importance sampling clique density estimation phase 
general approximate inference algorithm bayesian networks 
algorithm best viewed general schema instantiated different ways representation scheme different ways manipulate 
known inference algorithms turn special case general view 
approach combines best features exact approximate inference approximate inference algorithms deal complex domains including hybrid networks clique tree algorithm exploits locality structure bayes net reduce dimensionality probability densities involved computation 
described particular instantiation algorithm uses monte carlo importance sampling combined density estimation order estimate necessary functions 
particular instantiation general sense required factor representation ability sample factors 
allows provide general purpose approximate inference algorithm arbitrary hybrid bns 
empirical results showing algorithm gives results nontrivial networks 
believe algorithm ability scale significantly larger problems visual tracking complex environments existing inference algorithms infeasible 
acknowledgments 
ron parr xavier boyen simon tong useful discussions 
research supported aro muri program integrated approach intelligent systems number daah darpa contract subcontract iet generosity powell foundation sloan foundation 
bishop 
neural networks pattern recognition 
oxford university press 
cover thomas 
elements information theory 
wiley 
dawid kj lauritzen 
hybrid propagation junction trees 
advances intelligent computing volume 
springer verlag 
dempster laird rubin 
maximumlikelihood incomplete data em algorithm 
royal statistical society 
forbes huang kanazawa russell 
bayesian automated taxi 
proc 
ijcai 
hern ndez moral 
mixing exact importance sampling propagation algorithms dependence graphs 
international journal intelligent systems 
huang koller malik rao russell weber 
automatic symbolic traffic scene analysis belief networks 
proc 
aaai 
isard blake 
contour tracking stochastic propagation conditional density 
proc 
eccv volume 
jordan 
computing upper lower bound likelihoods intractable networks 
proceedings annual conference uncertainty artificial intelligence uai pages 
jensen lauritzen olesen 
bayesian updating recursive graphical models local computations 
computational statistical quarterly 
kanazawa koller russell 
stochastic simulation algorithms dynamic probabilistic networks 
proc 
uai pages 
kj 
hugs combining exact inference gibbs sampling junction trees 
proc 
uai 
koller 
learning approximation stochastic processes 
proc 
icml 
koller 
nonuniform dynamic discretization hybrid networks 
proc 
uai pages 
lauritzen 
propagation probabilities means variances mixed graphical association models 
journal american statistical association 
lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
roy 
stat 
soc 
radford neal 
probabilistic inference markov chain monte carlo methods 
technical report crg tr university toronto 
shachter 
gaussian influence diagrams 
management science 
shachter peot 
simulation approaches general probabilistic inference belief networks 
proc 
uai 
shenoy shafer 
axioms probability belief function propagation 
proc 
uai volume pages 
