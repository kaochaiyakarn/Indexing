journal arti cial intelligence research submitted published parameter learning logic programs symbolic statistical modeling sato kameya dept computer science graduate school information science engineering tokyo institute technology ku tokyo japan sato mi cs titech ac jp mi cs titech ac jp propose logical mathematical framework statistical parameter learning parameterized logic programs de nite clause programs containing probabilistic facts parameterized distribution 
extends traditional herbrand model semantics logic programming distribution semantics possible world semantics probability distribution unconditionally applicable arbitrary logic programs including ones hmms pcfgs bayesian networks 
propose new em algorithm graphical em algorithm class parameterized logic programs representing sequential decision processes decision exclusive independent 
runs new data structure called support graphs describing logical relationship observations explanations learns parameters computing inside outside probability generalized logic programs 
complexity analysis shows combined oldt search explanations observations graphical em algorithm despite generality time complexity existing em algorithms baum welch algorithm hmms inside outside algorithm pcfgs singly connected bayesian networks independently research eld 
learning experiments pcfgs corpora moderate size indicate graphical em algorithm signi cantly outperform inside outside algorithm 

parameter learning common various elds neural networks reinforcement learning statistics 
tune systems best performance classi ers statistical models 
numerical systems described mathematical formulas symbolic systems typically programs amenable kind parameter learning 
little literature parameter learning programs 
attempt incorporate parameter learning computer programs 
reason twofold 
theoretically wish add ability learning computer programs authors believe necessary step building intelligent systems 
practically broadens class probability distributions traditionally numerical ones available modeling complex phenomena gene inheritance consumer behavior natural language processing 
ai access foundation morgan kaufmann publishers 
rights reserved 
sato kameya type learning consider statistical parameter learning applied logic programs 
assume facts unit clauses program probabilistically true parameterized distribution 
clauses non unit de nite clauses true encode laws pair blood type genes blood type ab 
call logic programs type parameterized logic program statistical modeling ground atoms provable program represent observations blood type ab parameters program inferred performing ml maximum likelihood estimation observed atoms 
probabilistic rst order framework sketched termed statistical abduction sato kameya amalgamation statistical inference abduction probabilistic facts play role abducibles primitive hypotheses 
statistical abduction powerful subsumes diverse symbolic statistical frameworks hmms hidden markov models rabiner pcfgs probabilistic context free grammars manning schutze discrete bayesian networks pearl castillo gutierrez gives freedom arbitrarily complex logic programs modeling 
semantic basis statistical abduction distribution semantics introduced sato 
de nes parameterized distribution probability measure set possible truth assignments ground atoms enables derive new em algorithm ml estimation called graphical em algorithm kameya sato 
parameter learning statistical abduction done phases search em learning 
parameterized logic program observations rst phase searches explanations observations 
redundancy rst phase eliminated partial explanations oldt search tamaki sato warren sagonas warren ramakrishnan rao sagonas swift warren shen yuan zhou 
returns support graph compact representation discovered explanations 
second phase run graphical em algorithm support graph 
logic programs mean de nite clause programs 
de nite clause program set de nite clauses 
de nite clause clause form ln ln atoms 
called head ln body 
variables universally quanti ed 
reads ln hold holds 
case clause called unit clause 
general clause body may contain negated atoms 
program including general clauses called general program lloyd doets 

familiarity readability somewhat loosely distribution synonym probability measure 

logic programming adjective ground means variables contained 

abduction means inference best explanation set observations 
logically search explanation kb atom representing observation kb knowledge base conjunction atoms chosen abducibles class formulas allowed primitive hypotheses kakas kowalski toni flach kakas 
consistent kb 

existing symbolic statistical modeling frameworks restrictions limitations various types compared arbitrary logic programs see section details 
example bayesian networks allow recursion 
hmms pcfgs stochastic grammars allow recursion lack variables data structures 
recursive logic programs allowed ngo haddawy framework assume domains nite function symbols prohibited 

em algorithm stands class iterative algorithms ml estimation incomplete data mclachlan krishnan 
parameter learning logic programs symbolic statistical modeling learn parameters distribution associated program 
redundancy second phase removed inside outside probability logic programs computed support graph 
graphical em algorithm accomplished combined oldt search explanations time complexity specialized ones baum welch algorithm hmms rabiner inside outside algorithm pcfgs baker despite generality 
surprising conducted learning experiments pcfgs real corpora outperformed inside outside algorithm orders magnitudes terms time iteration update parameters 
experimental results enhance prospect symbolic statistical modeling parameterized logic programs complex systems stochastic grammars modeling di cult simply lack appropriate modeling tool sheer complexities 
contributions distribution semantics parameterized logic programs uni es existing frameworks graphical em algorithm combined tabulated search general em algorithm runs support graphs cient prospect suggested learning experiments modeling learning complex symbolic statistical phenomena 
rest organized follows 
preliminaries section probability space parameterized logic programs constructed section mathematical basis subsequent sections 
propose new em algorithm graphical em algorithm parameterized logic programs section 
complexity analysis graphical em algorithm section hmms pcfgs pseudo sc bns 
section contains experimental results parameter learning pcfgs graphical em algorithm real corpora demonstrate ciency graphical em algorithm 
state related section followed section 
reader assumed familiar basics logic programming lloyd doets probability theory chow bayesian networks pearl castillo stochastic grammars rabiner manning schutze 

preliminaries subject intersects logic programming em learning quite di erent nature separate preliminaries 
logic programming oldt logic programming program db set de nite clauses execution search sld refutation goal top interpreter recursively selects 
pseudo probabilistic context sensitive grammars context sensitive extension pcfgs proposed charniak carroll 
sc bn shorthand singly connected bayesian network pearl 

deal general logic programs 
sato kameya goal unfolds tamaki sato subgoals nondeterministically chosen clause 
computed result sld refutation solution substitution variable binding db usually refutation search space refutations described sld tree may nite depending program goal lloyd doets 
applications require solutions 
natural language processing instance parser able nd possible parse trees sentence syntactically correct 
similarly statistical abduction need examine explanations determine 
solutions obtained searching entire sld tree choice search strategy 
prolog standard logic programming language backtracking search solutions conjunction xed search order goals textually left right clauses textually top bottom due ease simplicity implementation 
problem backtracking forgets previous choice point quite prove goal resulting exponential search time 
answer avoid problem store computed results reuse necessary 
oldt instance scheme tamaki sato warren sagonas ramakrishnan shen 
reuse proved subgoals oldt search drastically reduces search time solutions especially refutations top goal include common 
take example logic program coding hmm 
string exist exponentially transition paths output oldt search applied program takes time linear length nd exponential time prolog backtracking search 
oldt statistical abduction 
viewpoint statistical abduction reuse proved subgoals equivalently structure sharing sub refutations top goal brings structure sharing explanations addition reduction search time mentioned producing highly compact representation explanations em learning parameterized distributions multinomial distribution normal distribution provide convenient modeling devices statistics 
suppose random sample xt size random variable drawn distribution parameterized unknown observed 
value determined ml estimation mle maximum likelihood estimate maximizer likelihood xi 
things get di cult data incomplete 
think probabilistic relationship non observable cause observable ect diseases symptoms medicine assume uniquely determine cause incomplete sense carry information completely determine parameterized joint distribution task perform ml estimation condition 
solution ambiguously mean answer substitution proved atom gives 
sato kameya 
distribution semantics section introduce parameterized logic programs de ne declarative semantics 
basic idea follows 
start set probabilistic facts atoms set non unit de nite clauses 
sampling determines set true atoms herbrand model determines truth value atom db atom considered random variable true false 
follows formalize process construct underlying probability space denotation db 
basic distribution pf db de nite clause program rst order language countably variables function symbols predicate symbols set unit clauses facts set non unit clauses rules 
sequel stated consider simplicity db set ground instances clauses db assume consist countably nite ground clauses nite case similarly treated 
construct probability space db steps 
introduce probability space herbrand interpretations truth assignments ground atoms extend probability space herbrand interpretations ground atoms model semantics lloyd doets 
xed enumeration atoms nite vector hx herbrand interpretation way ai true resp 
false xi resp 
xi 
isomorphism set possible herbrand interpretations coincides cartesian product def construct probability measure pf sample space collection nite joint distributions xn xi xn xn xn xn xn xn equation called compatibility condition 
proved chow compatibility condition exists probability space pf pf probability measure minimal algebra containing open sets pf xn xn 
herbrand interpretation interprets function symbol uniquely function ground terms assigns truth values ground atoms 
interpretation function symbols common herbrand interpretations correspondence truth assignments ground atoms distinguish 

regard topological space product topology equipped discrete topology 
parameter learning logic programs symbolic statistical modeling call pf basic distribution 
choice free long compatibility condition met 
want interpretations equiprobable set xn hx 
resulting pf uniform distribution just unit interval 
hand stipulate interpretation hc possible put xi ci xn pf places probability masson gives probability rest 
de ne parameterized logic program de nite clause program db set unit clauses set non unit clauses clause head uni able unit clause parameterized basic distribution pf associated parameterized pf obtained collection parameterized joint distributions satisfying compatibility condition 
generally complex exible pf cost tractability 
choice parameterized sato simple nite distributions ny pbs bs pbs represents probabilistic binary switch bernoulli trial exclusive atoms way true trial 
parameter specifying probability switch 
resulting pf probability measure nite product independent binary outcomes 
look simple expressive bayesian networks markov chains hmms sato sato kameya 
extending pf pdb subsection extend pf probability measure pdb possible worlds set possible truth assignments ground atoms 
naming pf despite probability measure partly re ects observation behaves nite joint distribution pf nite random vector ha xn marginal distributions 
reason intuitiveness 
considerations apply pdb de ned subsection 

clauses necessarily ground 
sato kameya herbrand model lloyd doets 
proceeding need couple notations 
atom de ne take herbrand interpretation atoms true false 
set atoms true imagine de nite clause program db herbrand model db lloyd doets 
db characterized xed point mapping tdb db def bk db fb set ground atoms 
equivalently inductively de ned tdb db account fact db function employ functional notation mdb denote mdb 
turning back enumeration ground atoms form db similarly cartesian product denumerably identify set possible herbrand interpretations ground atoms possible worlds 
extend pf probability measure pdb db follows 
introduce series nite joint distributions db xn db xn def mdb def pf note set pf measurable de nition db compatibility condition xn db xn db xn satisfy exists probability measure pdb db extension pf pdb xn pf xn 
de nes mutually herbrand interpretation ground atom true herbrand model program herbrand interpretation ground instance clause program true 

note enumeration enumerates ground atoms 
parameter learning logic programs symbolic statistical modeling nite atoms binary vector hx xi 
de ne denotation program db pf pdb 
denotational semantics parameterized logic programs de ned called distribution semantics 
remarked regard pdb kind nite joint distribution pdb 
mathematical properties pdb listed appendix semantics proved extension standard model semantics logic programming possible world semantics probability measure 
programs distributions distribution semantics views parameterized logic programs expressing distributions 
traditionally distributions expressed mathematical formulas programs discrete distributions gives far freedom exibility mathematical formulas construction distributions recursion arbitrary composition 
particular program contain nitely random variables probabilistic atoms recursion describe stochastic processes potentially involve nitely random variables markov chains derivations pcfgs manning schutze 
programs enable procedurally express complicated constraints distributions sum occurrences alphabets output string hmm multiple 
feature procedural expression arbitrarily complex discrete distributions quite helpful symbolic statistical modeling 
providing mathematically sound semantics parameterized logic programs thing implementing distribution semantics tractable way 
section investigate conditions parameterized logic programs probability computation tractable making usable means large scale symbolic statistical modeling 

graphical em algorithm preceding section parameterized logic program db rst order language parameterized basic distribution pf herbrand interpretations ground atoms speci es parameterized distribution pdb herbrand interpretations section develop step step cient em algorithm parameter learning parameterized logic programs interpreting pdb distribution observable non observable events 
new em algorithm termed graphical em algorithm 
applicable arbitrary logic programs satisfying certain conditions described provided basic distribution direct product multi ary random switches slight complication binary ones introduced section 
section assume db consists usual de nite clauses containing universally quanti ed variables 
de nitions changes relating assumption 
nite derivation occur pcfgs 
take simple pcfg fp ssg start symbol terminal symbol 
pcfg rewritten probability ss probability probability occurrence nite derivation calculated max non zero chi geman 
sato kameya listed 
predicate introduce de nition def yn tn wn vector new variables length equal arity ti wi enumeration clauses db vector variables occurring ti wi 
de ne comp asfollows 
head eq comp def fb ground instance clause head appearing rg def fi appears clause head rg def ff function ff di erent function ft term properly containing xg def eq eq clark equational theory clark deductively simulates uni cation 
likewise comp rst order theory deductively simulates sld refutation help eq replacing clause head atom clause body lloyd doets 
introduce de nitions frequently 
atom 
explanation db conjunction set comprised conjuncts holds proper subset satis es 
set explanations called support set designated db 
motivating example review distribution semantics concrete example 
consider program db modeling blood type determined blood type genes probabilistically inherited parents 
rst clauses rb state blood type determined genotype pair blood type genes instance btype says blood type genotype ha ai ha oi ho ai 
propositional rules 
succeeding clauses state general rules terms logical variables 
fth clause says regardless values having genotype hx yi caused events gene father inheriting gene father gene mother inheriting gene mother 
gene msw gene clause connecting rules rb probabilistic facts fb 
tells gene inherited parent choice represented msw gene 

de nition support set di ers sato kameya sato 

implicitly emphasize procedural reading logic programs prolog conventions employed sterling shapiro 
stands implied respectively 
strings capital letter universally quanti ed variables quoted ones constants 
underscore anonymous variable 

msw abbreviation multi ary random switch msw expresses probabilistic choice nite alternatives 
framework statistical abduction msw atoms abducibles explanations constructed conjunction 
parameter learning logic programs symbolic statistical modeling btype 
btype 
btype 
btype ab 
gene father gene mother 
gene msw gene 
fb gene father msw gene father msw gene father msw gene mother msw gene mother msw gene mother abo blood type program db genetic knowledge choice chance fa og expressed specifying joint distribution follows 
msw gene msw gene msw gene def andt father mother 
probability inheriting gene parent 
statistical independence choice gene father mother expressed putting msw gene father msw gene father msw gene father msw gene mother msw gene mother msw gene mother setting atoms representing observation obs dbb btype btype btype ab observe say btype infer possible explanation minimal conjunction abducibles msw gene rb btype 
obtained applying special sld refutation procedure goal preserves msw atoms resolved refutation 
explanations 
msw gene father msw gene mother msw gene father msw gene mother msw gene father msw gene mother btype dbb btype support set btype 
probability explanation respectively computed proposition appendix follows btype btype sato kameya fact mutually exclusive choice gene exclusive 
parameters determined ml estimation performed random sample btype btype ab btype follows 
oi btype btype oi btype ab argmax oi program contains function symbol recursion semantics allows 
see example containing program hmm rabiner juang 
simplifying conditions dbb simple probability computation easy 
generally case 
primary interest learning especially cient parameter learning parameterized logic programs concentrate identifying property program probability computation easy dbb cient parameter learning possible 
answer question precisely formulate modeling process 
suppose exist symbolic statistical phenomena gene inheritance hope construct probabilistic computational model 
rst specify target predicate ground atom represents observation phenomena 
explain empirical distribution write parameterized logic program db having basic distribution pf parameter reproduce observable patterns 
observing random sample st ground atoms adjust ml estimation maximizing likelihood qt pdb st pdb approximates closely empirically observed distribution possible 
rst sight formulation looks right reality 
suppose events observed 
pdb pdb 
likelihood simply distribution semantics andp di erent realizations random variable 
quick remedy note case blood type program dbb obs dbb btype btype btype ab observable atoms true observation atom true false 
words atoms collectively behave single random variable having distribution values obs dbb 
keeping mind introduce condition 
obs db head set ground atoms represent observable events 
call observable atoms 
uniqueness condition pdb obs db obs db pdb 
parameter learning logic programs symbolic statistical modeling uniqueness condition enables introduce new random variable yo representing observation 
fix enumeration observable atoms obs db de ne yo yo 
db gk gk obs db random sample size thenl qt pdb qt pdb yo kt quali es likelihood function yo 
second condition concerns reduction probability computation addition 
take blood type 
computation btype decomposed summation explanations support set exclusive 
introduce exclusiveness condition obs db support set db pdb db 
exclusiveness condition proposition appendix pdb db pf modeling point view means single event single observation nite explanations db db allowed true observation 
introduce db set explanations relevant db def db db obs db uniqueness condition exclusiveness condition enumeration explanations db 
follows proposition pdb si sj pdb db obs db db obs db pdb pdb able introduce uniqueness condition exclusiveness condition random variable xe representing explanation de third condition concerns termination 
xe 
sk db 
pdb guarantees measure gk obs db satisfying gk 
case put yo 

values set measure zero ect part discussion follows 
applies de nition xe 
sato kameya finite support condition obs db db nite 
nite summation pm pf si 
condition pdb computed support set db fs smg help exclusiveness condition prevents nite summation hardly computable 
fourth condition simpli es probability computation multiplication 
recall explanation obs db conjunction am abducibles fa amg 
order reduce computation pf pf am multiplication pf pf am assume distribution condition set msw ground atoms parameterized distribution msw speci ed 
atom msw intended simulate multi ary random switch name outcome trial generalization primitive probabilistic events coin tossing dice rolling 

consists probabilistic atoms msw 
arguments ground terms called switch name trial id value switch respectively 
assume nite set vi ground terms called value set associated andv vi holds 

write vi fv 
ground atoms msw msw msw vm exclusively true takes value trial 
vi associated 
probability msw true vi 

ground terms vi vi random variable msw independent msw words introduce family parameterized nite distributions msw msw vm xm def vm xm vm xk 
xk de ne nite product def condition compute msw probability explanation asthe product parameters 
suppose msw ij msw ij di erent conjuncts explanation msw msw 
holds independent construction 
independent msw construction 
result whichever condition may hold msw computed parameters 
parameter learning logic programs symbolic statistical modeling modeling principle point introduced conditions uniqueness condition exclusiveness condition nite support condition distribution condition simplify probability computation 
easy satisfy 
adopt 
assume parameterized distribution introduced previous subsection 
unfortunately rest satis ed automatically 
modeling experiences mildly di cult satisfy uniqueness condition exclusiveness condition long obey modeling principle 
modeling principle db msw describes sequential decision process modulo auxiliary computations uniquely produces observable atom obs db decision expressed msw atom 
translated programming level says take care writing program sample uniquely exist goal obs db successful refutation db wecan con rm principle blood type program dbb fb rb 
describes process gene inheritance arbitrary sample gene father msw gene mother exists unique goal btype case successful sld refutation rb 
idea principle decision process produces result observable atom di erent decision processes di er msw entailing mutually exclusive observable atoms 
uniqueness condition exclusiveness condition automatically satis ed 
satisfying nite support condition di cult virtually equivalent writing program db solution search obs db terminates 
apparently general solution problem far speci models hmms pcfgs bayesian networks concerned met 
programs models satisfy nite support condition conditions 
conditions revisited subsection discuss relax simplifying conditions introduced subsection purpose exible modeling 
rst examine uniqueness condition considering crucial role adaptation em algorithm semantics 
uniqueness condition guarantees exists mapping explanations observations em algorithm applicable dempster 
possible relax uniqueness condition justifying application em algorithm 
assume mar missing random condition introduced rubin statistical condition complete data explanation incomplete data observation customarily assumed implicitly explicitly statistics see appendix 
assuming mar condition apply em 
decisions process nite subset msw 
sato kameya algorithm non exclusive observations uniqueness condition seemingly destroyed 
see mar condition action simple example 
imagine walk road front occasionally observe state road dry lawn wet 
assume lawn sprinkler running probabilistically 
program db rl rl rl describes sequential process outputs observation observed road lawn road lawn 
rl observed road lawn msw rain wet wet msw sprinkler dry wet dry dry 
rl msw rain msw rain msw sprinkler msw sprinkler basic distribution rl db rl speci ed subsection omit 
msw rain program determines rains msw sprinkler determines sprinkler works ne 
sampled values nog fon uniquely exists observation observed road lawn mapping hx yi 
words apply em algorithm observations observed road lawn 
happen observe exclusively state road lawn 
logically means observe observed road lawn observed road lawn 
apparently uniqueness condition met observed road wet lawn observed road lawn wet compatible true rains 
despite non exclusiveness observations apply em algorithm mar condition case translates observe lawn road randomly regardless state 
brie check conditions 
basically relaxed cost increased computation 
exclusiveness condition instance need additional process transforming support set db goal set exclusive explanations 
instance explanations msw transform msw msw 
clearly transformation exponential number msw atoms ciency concern leads assuming exclusiveness condition 
nite support condition practice equivalent condition sld tree nite 
relaxing condition induce nite computation 

msw transformed disjunction exclusive msw atoms va msw 
parameter learning logic programs symbolic statistical modeling relaxing distribution condition accepting probability distributions serve expand horizon applicability parameterized logic programs 
msw particular parameterized joint distributions vk boltzmann distributions switches msw vk values switches correlated 
distributions facilitate writing parameterized logic programs complicated decision processes decisions independent interdependent 
obviously hand increase learning time added exibility distributions deserves increased learning time seen 
naive approach em learning subsection derive concrete em algorithm parameterized logic programs db assuming satisfy uniqueness condition exclusiveness condition nite support condition 
start introduce yo random variable representing observations xed enumeration observable atoms obs db 
introduce random variable xe representing explanations xed enumeration explanations db 
understanding xe non observable yo observable joint distribution pdb xe yo denotes relevant parameters 
immediate section derive concrete em algorithm function de ned def pdb input random sample observable atoms output mle sake readability substitute observable atom obs db yo write db db yo 
likewise substitute explanation db write pdb pdb xe yo 
follows uniqueness condition pdb db msw db need notation 
explanation de ne count msw def jf msw done preparations 
suppose observations gt gt obs db 
put def fi msw db gt def msw db gt set switch names appear explanation gt denotes parameters associated switches 
nite due nite support condition 
sato kameya various probabilities function computed proposition appendix assumptions follows 
pdb gt pdb msw def vi tx db def db gt vi tx pdb gt pdb gt ln db gt db gt gt vi msw msw lnp vi jensen inequality obtain 
note pdb gt db gt count msw sld refutation gt 
speaking likelihood function qt pdb gt shown subsection footnote implies 
reach procedure learn naive db nds mle parameters 
array variable stores current procedure learn naive db initialize appropriate values small positive number ptt ln pdb gt compute log likelihood 
repeat foreach vi tx foreach vi pdb gt db gt msw vi update parameters 
ptt ln pdb gt compute log likelihood 
terminate converged 
em algorithm simple correctly calculates mle calculation pdb gt line may su er combinatorial explosion explanations 
db gt grows exponentially complexity model 
instance db gt hmm states exponential length input output string 
suppressing explosion realize cient computation polynomial order possible suitable conditions avoiding multiple computations subgoal see 
parameter learning logic programs symbolic statistical modeling inside probability outside probability logic programs subsection generalize notion inside probability outside probability baker lari young logic programs 
major computations learn naive db terms line pdb gt db gt msw 
computational redundancy naive computation terms 
example 
suppose propositional program dbp fp rp fp fa mg rp observable atom 
assume independent fa bg fc dg pair wise exclusive 
support set calculated dbp fa light may compute pfp pfp pfp pfp computation requires multiplications pfp pfp pfp additions 
hand possible compute ciently factoring common computations 
ground atom 
de ne inside probability def pdb applying theorem appendix comp rp unconditionally holds semantics independent exclusiveness assumption fp equations inside probability derived 
obtained solving multiplications additions required 
quite straightforward generalize proceeding look program fmg fg mg observable atom msw atom 
semantics compute clearly wrong ignores fact clause bodies mutually exclusive atoms clause body independent 
similarly equation totally incorrect 

note fact msw 
sato kameya add temporarily subsection top exclusiveness condition nite support condition equations mathematically correct 
rst assumption clause bodies mutually exclusive clauses pdb second assumption body atoms independent bk rule pdb bk pdb pdb bk holds 
please note clause subsection special meaning 
intended mean goal tabled explanation obtained oldt search explained subsection 
words additional conditions imposed source program result oldt search 
clauses auxiliary computations need satisfy 
suppose clauses occur db bl bl il bh ih atom 
theorem appendix assumptions ensure ily bl suggests gt considered function equations inside probabilities hierarchically organized way gt belongs top layer appearing left hand side refers belong lower layers 
refer condition acyclic support condition 
acyclic support condition equations form unique solution computation pdb inside probabilities allows take advantage reusing intermediate results stored contributing faster computation pdb gt 
tackle intricate problem computation db gt msw 
sum equals msw db gt concentrate computation gt def db gt msw 
note explanation contains 
gt expressed gt gt gt gt depend 
generalizing obser gt vation arbitrary ground atoms introduce outside probability ground atom gt gt def gt 
logical relationship corresponds table atoms 
parameter learning logic programs symbolic statistical modeling assuming conditions inside probability 
view problem computing gt reduced computing gt recursively computable follows 
suppose occurs ground program db bk wk bk wk ik gt function bk assumption chain rule derivatives leads gt gt gt bk wk ik gt gt gt gt gt bk wk inside probabilities computed outside probabilities recursively computed top downward program layers 
chosen atoms compute desired sum calculated requires multiplications addition compared multiplications addition naive computation 
gains obtained computing inside outside probability may small case problem size grows enormous compensate additional restrictions imposed result oldt search 
oldt search compute inside outside probability recursively need programming level tabulation mechanism structure sharing partial explanations 
independence assumption body atoms wh ih anda independent 
wh wh wh sato kameya subgoals 
henceforth deal programs db set table db predicates declared advance 
ground atom containing table predicate called table atom 
purpose table atoms store support sets eliminate need recomputation doing construct hierarchically organized explanations table atoms msw atoms 
db parameterized logic program satis es nite support condition uniqueness condition 
gt random sample observable atoms obs db 
additional assumptions 
assumptions exists nite set kt table atoms associated conjunctions kt mk comp gt kt kt kt mk kt set subset msw acyclic support condition 
put gt call respectively def db set table atoms gt se explanation set explanations denoted edb db function table atoms 
explanations mutually exclusive kt pdb exclusiveness condition 
es kt mk conjunction independent atoms independent condition 
assumptions aimed cient probability computation 
acyclic support condition dynamic programming possible exclusiveness condition reduces pdb pdb independent condition reduces pdb pdb pdb 
point concerning ciency 
note computation dynamic programming proceeds partial order db imposed acyclic support condition access table atoms simpli ed linearly ordered 
topologically sort db respecting said partial order call linearized db satisfying assumptions acyclic support condition exclusiveness condition independent condition hierarchical system explanations gt 
db gt assuming edb implicitly 
hierarchical system explanations gt successfully built 
pre abbreviation tabled 

independence mentioned concerns positive propositions 
head db say independent pdb pdb pdb 
precedes top execution db invokes directly indirectly 

holds precedes parameter learning logic programs symbolic statistical modeling source program equations inside probability outside probability automatically derived solved time proportional size equations 
plays central role approach cient em learning 
way obtain explanations oldt search tamaki sato warren complete refutation method logic programs 
oldt search goal called rst time set entry solution table store answer substitutions 
call instance occurs solving try retrieve answer substitution stored solution table unifying remaining answer substitutions prepare lookup table hold pointer 
self look details oldt search sample program fh rh depicts hmm 
hmm states fs 
state transition probabilistically chooses destination fs fh state hmm values init 
values 
values tr 
hmm cs generate string chars cs msw init si set initial state si hmm si cs 
enter loop clock 
hmm cs loop msw output state msw tr transit 
put clock ahead 
hmm cs 
continue loop recursion 
hmm 
finish loop clock 
state hmm program db 
temporary marks part program 

hmm de nes probability distribution strings set alphabets works stochastic string generator rabiner juang output string sample de ned distribution 
sato kameya alphabet fa bg emit 
note specify fact set associated distribution compactly introduce new notation values vm 
declares contains msw atoms form msw fv distribution subsection 
example values tr introduces msw tr atoms program ground term fs ground term distribution tr msw tr msw tr tr 
program runs prolog program 
non ground top goal hmm functions stochastic string generator returning list alphabets variable follows 
top goal calls clause selects initial state executing subgoal msw init si returns si initial state probabilistically chosen fs 
second clause called ground ground probabilistic choice output alphabet asking msw determines state asking msw tr 
transition 
simplicity length output strings xed 
way execution termed sampling execution corresponds random sampling top goal ground hmm works acceptor returning success failure 
explanations hmm sought keep msw atoms resolved refutation conjunction explanation repeat process backtracking refutation 
need explanations backtracking abandoned sharing partial explanations explanations purpose explanations impossible 
oldt search top hmm cs ans tab hmm cs ans 
tab hmm cs hmm cs hmm cs 
tab hmm cs hmm cs hmm cs 
msw init msw init 
msw init msw init 
hmm cs msw init si tab hmm si cs 
hmm cs msw msw tr tab hmm cs 
hmm 
translated program db 
msw called ground ground logical variable behaves random variable 
instantiated term probability selected value set vi declared values atom 
hand ground term called procedural semantics msw equal msw parameter learning logic programs symbolic statistical modeling explanation search 
case hmm program example build hierarchical system explanations hmm oldt search rst declare hmm hmm table predicate 
explanation conjunction hmm atoms hmm atoms msw atoms 
translate program logic program analogously translation de nite clause grammars dcgs prolog sterling shapiro 
forms list predicate purpose accumulating msw atoms table atoms conjuncts explanation 
translation applied db yields program 
translated program clause corresponds top goal hmm input string explanation table atom hmm returned ans 
auxiliary clauses add callee list table atom form hmm hmm respectively time step state 
general table predicate original program table predicate translated program auxiliary predicate tab inserted signal oldt interpreter check solution table check exist explanations likewise clauses pair corresponding insert msw init callee list 
clauses respectively correspond 
hmm hmm msw init hmm msw init hmm hmm hmm msw msw tr hmm msw msw tr hmm hmm hmm msw msw tr hmm msw msw tr hmm hmm hmm hmm hmm hmm hmm hmm hmm hmm hmm msw msw tr hmm msw msw tr hmm msw msw tr hmm msw msw tr hmm msw msw tr hmm msw msw tr hmm msw msw tr hmm msw msw tr hmm hmm hmm solution table 
general means predicate arity hmm share predicate name hmm di erent predicates 
sato kameya top hmm ans translation apply oldt search ing added list uence oldt procedure ii associate solution table atom solution table list explanations 
resulting solution table shown 
rst row reads call hmm occurred entered solution table solution hmm variable binding generated explanations msw init hmm msw init hmm 
remaining task topological sorting table atoms stored solution table respecting acyclic support condition 
done depth rst search trace explanations top goal example 
obtain hierarchical system explanations hmm 
support graphs looking back need compute inside outside probability hierarchical system explanations essentially boolean combination primitive events msw atoms compound events table atoms intuitively representable graph 
reason help visualizing learning algorithm introduce new data structure termed support graphs new em algorithm subsection described solely hierarchical system explanations 
illustrated support graph gt graphical representation hierarchical system explanations db kt gt gt 
consists totally ordered disconnected subgraphs labeled comprises corresponding table atom db kt 
subgraph labeled special nodes start node node graphs corresponding explanation db 
explanation graph linear graph node labeled table atom switch msw se called table node switch node respectively 
support graph hmm obtained solution table 
table node labeled refers subgraph labeled data sharing achieved distinct table nodes referring subgraph 
graphical em algorithm describe cient em learning algorithm termed graphical em algorithm introduced kameya sato runs support graphs 
suppose random sample gt observable atoms 
suppose support graphs gt hierarchical systems explanations satisfying acyclic support condition exclusiveness condition independent condition successfully constructed parameterized logic program db satisfying uniqueness condition nite support condition 
graphical em algorithm re nes learn naive db introducing subroutines get inside probs db compute inside probabilities get expectations db compute outside probabilities 
called main routine learn gem db 
learning prepare arrays support graph gt inside probability 
pdb see parameter learning logic programs symbolic statistical modeling start msw msw msw explanation graph msw msw start msw msw hmm start msw init hmm msw init hmm msw msw tr hmm hmm start msw msw tr hmm msw msw tr hmm hmm start msw msw tr hmm support graph general form gt hmm hmm program db double circled node refers table node 
outside probability gt 
gt see explanation probability db pdb sato kameya procedure learn gem db select initial parameters get inside probs db ptt ln gt repeat get expectations db foreach vi tt gt foreach vi vi get inside probs db 
ptt ln gt procedure get inside probs db gt kt downto foreach se edb fa jsj msw foreach 
procedure get expectations db foreach vi gt kt kt foreach se edb fa je sj msw foreach 
graphical em algorithm 
count msw db gt msw call procedure learn gem db 
main routine learn gem db initially computes inside probabilities line enters loop get expectations db called rst compute expected count parameters updated line 
inside probabilities renewed updated parameters entering loop line 
parameter learning logic programs symbolic statistical modeling subroutine get inside probs db computes inside probability pdb stores table atom bottom layer topmost layer gt line hierarchical system explanations gt see subsection 
takes explanation db line decomposes conjuncts multiplies inside probabilities known line computed line 
subroutine get expectations db computes outside probabilities recursive de nitions subsection stores outside probability gt table atom 
rst sets outside probability top goal gt line computes rest outside probabilities line going layers explanation gt described subsection 
line adds gt expected count msw asa contribution msw 
line increments outside probability gt equation 
notice computed fors shown subsection learn naive db mle procedure theorem holds 
theorem db parameterized logic program gt random sample observable atoms 
suppose conditions uniqueness nite support subsection acyclic support exclusiveness independence subsection met 
learn gem db nds mle locally maximizes likelihood qt pdb gt 
proof sketch 
main routine learn gem db learn naive db computation pt show db gt msw msw db gt 
kt msw es db gt see line get expectations db gt msw msw gt msw see equation msw db gt fact contains msw msw msw holds gt se msw contribution msw se gt msw msw gt 
formal proof kameya 
proved common parameters learn naive db coincides gem db 
parameters updated values 
starting initial values parameters converge values 
sato kameya conditions applicability graphical em algorithm may look hard satisfy 
fortunately modeling principle section stands due care modeling lead program meets 
see section programs standard symbolic statistical frameworks bayesian networks hmms pcfgs satisfy conditions 

complexity section analyze time complexity graphical em algorithm applied various symbolic statistical frameworks including hmms pcfgs pseudo bayesian networks 
results show graphical em algorithm competitive specialized em algorithms developed independently research eld 
basic property em algorithm iterative algorithm unable predict converges measure time complexity time taken iteration 
estimate time iteration repeat loop learn gem db gt 
observe iteration support graph gt scanned twice get inside probs db get expectations db 
scan addition performed explanations multiplication possibly division performed msw atoms table atoms 
time spent gt iteration graphical em algorithm linear size support graph number nodes support graph gt 
put db def db db num maxsize def max je def max es db je sj recall db set table atoms gt db set explanations appearing right hand side subsection 
num maximum number explanations support graph gt maxsize maximum size explanation gt respectively 
obvious 
proposition time complexity graphical em algorithm iteration linear total size support graphs num notation coincides space complexity graphical em algorithm runs support graphs 
general result compare graphical em algorithm em algorithms remember input graphical em algorithm support graphs observed atom actual total learning time oldt time number iterations num parameter learning logic programs symbolic statistical modeling oldt time denotes time construct support graphs time oldt search time topological sorting table atoms part order wise represent oldt time time oldt search 
observe total size support graphs exceed time oldt search order wise 
evaluate oldt time speci class models hmms need know time table operations 
observe oldt search special sense table atoms ground called resolution solved goals 
accordingly solution table check entry solution table called add new searched explanation list discovered explanations entry 
time complexity operations equal table access depends program implementation solution table 
rst suppose programs carefully written way arguments table atoms table access integers 
programs subsequent complexity analysis db subsection dbg db subsection dbg subsection satisfy satisfy condition replacing non integer terms appropriate integers 
suppose solution table implemented array table access done time 
follows detailed analysis time complexity graphical em algorithm applied hmms pcfgs pseudo bayesian networks assuming time access solution table 
way space complexity just total size solution tables support graphs 
hmms standard em algorithm hmms baum welch algorithm rabiner rabiner juang 
example hmm shown subsection 
observations wt output string length computes lt time iteration forward probability qj backward probability ot mot ot state time step string wt ot ot ot set states number states 
factor comes fact state possible destinations 
think oldt search top goal gt 
searches msw atoms table atoms create solution table doing auxiliary computations 
time complexity number msw atoms table atoms support graph coincides time need topologically sort table atoms solution table depth rst search gt 

sagonas 
ramakrishnan 
discuss implementation oldt 

arrays available may able balanced trees giving log access time number data solution table may able hashing giving average time access certain condition cormen leiserson rivest 

treat state emission hmms emit symbol depending state 
type arc emission hmms emitted symbol depends transition arc treated similarly 
sato kameya compute forward backward probability destination state 
computing parameters updated 
total computation time iteration baum welch algorithm estimated lt rabiner juang manning schutze 
compare result graphical em algorithm hmm program appropriate modi cations length string state set declarations fh output alphabets 
string ol hmm om om ol reads hmm state time output om om ol reaches nal state 
declaring hmm hmm table predicate translation see apply oldt search goal top hmm ol ans translated program obtain explanations hmm ol 
complexity argument translated program talk terms sake simplicity 
search search strategy multi stage depth rst strategy tamaki sato 
assume solution table accessible time 
length list third argument hmm decreases recursion nitely choices state transition output alphabet search terminates leaving nitely explanations solution table satisfy acyclic support condition respectively 
sampling execution hmm sequential decision process decisions msw atoms exclusive independent generate unique string means satis es exclusiveness condition independence condition uniqueness condition respectively 
graphical em algorithm applicable set hierarchical systems explanations hmm wt produced oldt search observations wt output string 
put wt ot ot ot follows fhmm ot qg fhmm ot ot hmm ot msw om msw tr hmm ot top goal hmm ot ot nl calling patterns hmm call causes calls hmm implying occur nl calls hmm 
call computed due tabling mechanism num 
maxsize 
applying proposition reach proposition suppose strings length suppose table operation oldt search done time 
oldt time lt graphical em algorithm takes lt time iteration number states 
lt time complexity baum welch algorithm 
graphical em algorithm runs ciently baum welch algorithm 

possible translated program section identify goal pattern hmm rst arguments constants integers 

baum welch algorithm graphical em algorithm input support graphs generated update parameters value initial values 
parameter learning logic programs symbolic statistical modeling way viterbi algorithm rabiner rabiner juang provides hmms cient way nding transition path input output string 
similar algorithm parameterized logic programs determines explanation goal derived 
runs time linear size support graph case hmms complexity viterbi algorithm sato kameya 
pcfgs compare graphical em algorithm inside outside algorithm baker lari young 
inside outside algorithm known em algorithm pcfgs manning schutze 
takes grammar chomsky normal form 
nonterminals production rule grammar takes form nonterminals named numbers isa starting symbol form terminal 
iteration computes inside probability outside probability partial parse tree sentence update parameters production rules 
time complexity measured time iteration described number nonterminals number terminals sentence 
fort observed sentences lari young 
compare graphical em algorithm inside outside algorithm start propositional program dbg fg rg representing largest grammar containing possible rules nonterminals nonterminal starting symbol sentence 
fg number rg msw 
msw wd 
pcfg program dbg dbg arti cial parsing program sole purpose measure size oldt tree created oldt interpreter parses sentence wl 

pcfg probabilistic context free grammar backbone cfg probabilities parameters assigned production rule 
nonterminal having production rules fa ng probability pi assigned pn pi 
probability sentence sum probabilities leftmost derivation product probabilities rules derivation 

precise oldt structure case tree dbg contains constants datalog program occurs need creating new root node 
sato kameya note td td appears td oldt tree query input sentence wl embedded program separately msw second clauses rg treatment ect complexity argument 
reads th nonterminal spans position position substring wd wd rst clauses msw supposed textually ordered lexicographic order tuples hi 
parser top goal set 
asks parser parse sentence wl syntactic category sentence 
exhaustive search query oldt search 
multistage depth rst search strategy time access solution table assumed 
time complexity oldt search measured number nodes oldt tree 

illustrates msw atoms omitted 
seen tree similar subtrees put see note 
due depth rst strategy recursive structure contains subtree 
nodes leftmost atom underlined solution nodes solve leftmost atoms rst time entire refutation process 
underlined atoms computed subtrees left 
check solution table entries 
prolog variable constant denoting sentence length 

table predicate 

inductively proved contains computed 
parameter learning logic programs symbolic statistical modeling computed time 
single child node 
enumerate clauses ground execution generates number nodes 
contained nodes newly created 
number nodes estimated 
consequently number total time oldt search computed pl support graph 
pn 
result hd size consider non propositional parsing program db ground instances constitute propositional program dbg 
db probabilistic variant dcg program pereira warren declared table predicate 
semantically dbg speci es probability distribution atoms form fq list 
si sj sk si length 
msw 
msw 
probabilistic dcg program db top goal parse sentence wl wl 
invokes wl measuring length input sentence calling length 
general works identically arguments added 
supplies unique trial id tobe body latest trial id current computation list holding substring 
added arguments ect shape 
focus subtree range 
number nodes 
number nodes td negligible 

negligible 

program simple possible assume integer represented ground term sn def 
assume ground goal returns integer time proportional jd 

omit obvious program length sn computes length sn list jlj time 
sato kameya search tree extra computation caused length iso insertion nl respectively oldt time remains size support graph 
apply graphical em algorithm correctly need con rm conditions applicability 
apparent oldt refutation form wl dbg terminates leaves support graph satisfying nite support condition acyclic support condition 
exclusiveness condition independent condition hold refutation process faithfully simulates leftmost stochastic derivation wl choice production rule msw si sc sj sk exclusive independent trial ids di erent di erent choices 
remains uniqueness condition 
con rm consider program dbg modi cation dbg rst goal length body rst clause rst goal second clause rg moved position bodies respectively 
dbg dbg logically equivalent semantically equivalent viewpoint distribution semantics 
think sampling execution oldt interpreter top goal dbg variable multi stage depth rst search strategy 
easy see rst execution fails second oldt refutation terminates sentence wl returned third conversely set msw atoms resolved refutation uniquely determines output sentence wl 
sampling execution guaranteed terminate sampling pf pf uniquely generates sentence observable atom uniqueness condition satis ed dbg dbg 
sampling execution guaranteed terminate 
words grammar generate nite sentences 
giving general answer di cult known parameter values pcfg obtained learning nite sentences stochastic derivation pcfg terminates probability chi geman 
summary assuming appropriate parameter values say parameterized logic program db largest pcfg nonterminal symbols satis es applicability conditions oldt time sentence length size support graph 
proposition conclude proposition db logic program representing pcfg nonterminals form db gt representing sentences length suppose table operation oldt search done time 
oldt search iteration learn gem respectively done time 

called times nl times 
trial ids refutation record rule step derivation wl 
called 
dbg represent integers ground terms keep program short 
integers ground terms rst arguments check goal previously called check done time 
parameter learning logic programs symbolic statistical modeling time complexity inside outside algorithm iteration lari young algorithm cient inside outside algorithm 
pseudo pcfgs improved making choices context sensitive attempts pseudo pseudo probabilistic context sensitive grammars rule chosen probabilistically depending nonterminal expanded parent nonterminal charniak carroll 
pseudo easily programmed 
add extra argument representing parent node predicate replace msw msw 
leftmost derivation sentence pseudo sequential decision process described modi ed program graphical em algorithm applied support graphs generated modi ed program observed sentences correctly performs ml estimation parameters pseudo 
pseudo thought pcfg rules form parent nonterminal arguments previous subsection carried minor changes 
details omitted proposition db logic program pseudo nonterminals shown gt observed atoms representing sampled sentences length suppose table operation oldt search done time 
oldt search iteration learn gem completed time 
bayesian networks relationship cause ect probabilistic diseases symptoms mathematically captured conditional probability ect cause know inverse probability candidate cause evidence calculated bayes theorem 
bayesian networks representational computational framework best type probabilistic inference pearl castillo 
bayesian network graphical representation joint distribution xn xn nitely random variables xn 
graph dag directed acyclic graph ones node random variable 
graph conditional probability table cpt representing xi xi ui associated node xi represents xi parent nodes ui values 
xi parent topmost node graph table just marginal distribution xi xi 
joint distribution de ned product ts 
deal discrete cases 
sato kameya singly connected multiply connected bayesian networks conditional distributions xn xn graph de nes ny xi xi ui pg pg pg pg pg pg pg values corresponding random variables respectively 
mentioned basic tasks bayesian networks compute marginal probabilities 
example marginal distribution pg computed 
pg pg pg pg pg pg pg pg pg pg pg pg clearly cient 
observe graph way factorize computations requiring exponentially operations 
problem computing marginal probabilities np hard general factorization assured graph singly connected loop viewed undirected graph 
case computation possible jv time set vertices graph pearl 
graph called multiply connected need exponential time compute marginal probabilities 
sequel show 
discrete bayesian network de ning distribution pg xn parameterized logic program dbg predicate bn bn xn pg xn 

acyclicity graph losing generality may assume xi ancestor node xj 

notational simplicity shall omit random variables confusion arises 
parameter learning logic programs symbolic statistical modeling arbitrary factorizations order compute marginal distribution exists tabled program accomplishes computation speci ed way 
graph singly connected evidence exists tabled program dbg oldt time bn jv time complexity iteration graphical em algorithm jv aswell 
bayesian network de ning joint distribution pg xn xi xi ui xi val xi ui val conditional probabilities associated val xi set xi possible values val denotes set possible values parent nodes random vector respectively 
construct parameterized logic program de nes distribution pg xn 
program dbg fg rg shown 
fg msw par ui xi ui val xi val xi rg bn xn msw par xi 
bayesian network program dbg fg comprised msw atoms form msw par ui xi probability exactly conditional probability pg xi xi ui 
xi empty list 
rg singleton containing clause body conjunction msw atoms corresponds product conditional probabilities 
note intentionally identify random variables xn logical variables xn convenience 
proposition dbg denotes distributions 
proof hx xn realization random vector hx xn holds construction bn xn ny ny case program msw msw par ui xi pg xi xi ui pg xn bn msw par msw par msw par msw par msw par msw par 

prolog constants place integers 
sato kameya left right sampling execution gives sample realization random vector marginal distribution computed bn xn adding new clause dbg 
example compute pg add bn bn dbg result db compute pdb bn equal pg db bn bn bn pg regrettably computation corresponds factorization 
cient probability computation factorization possible carrying summations proper order 
sketch example carry speci ed summations speci ed order introducing new clauses 
suppose joint distribution suchthat respectively computed atoms 
suppose hope compute sum rst eliminate corresponding elimination introduce new predicates compute compute follows 


note clause body contains existentially quanti ed local variables clause head contains variables shared atoms 
view correspondence easy con rm program realizes required computation 
easy see generalizing example prove exists parameterized logic program carries summations order arbitrary bayesian network particular able simulate variable elimination zhang poole ambrosio approach 
cient computation marginal distributions possible known class bayesian networks singly connected bayesian networks exists cient algorithm compute marginal distributions message passing pearl castillo 
show graph singly connected construct cient tabled bayesian network program dbg assigning table predicate node 
avoid complications explain construction procedure informally concentrate case interested variable 
singly parameter learning logic programs symbolic statistical modeling connected graph 
pick probability pg 
construct tree root node letting nodes dangling shows transformed tree select node root node 
transformed graph transforming tree examine 
add node graph corresponding clause dbg purpose visit nodes connected calls suppose started root node evidence generated clause 
proceed inner node calls 
original graph parent child nodes fv 
topmost node tree general situation node add clause 
called parent node ground rst generate possible values calling val call call visit nodes connected 
treated 
visiting nodes connecting parent nodes nodes connected visited value random variable determined sampling msw atom jointly indexed values sato kameya 
visit children 
topmost node original graph add clause 
tbn msw par call 
call val call val call msw par call call 
call msw par 
dbg nal program containing clauses 
apparently dbg constructed time linear number nodes network 
note successive unfolding tamaki sato atoms form call clause bodies starts yields program db similar contains msw atoms call 
dbg db de ne distribution proved proposition pg db bn pdb tbn holds details omitted 
way assume construction starts topmost node evidence necessary 
suppose change start inner node case replace clause call msw par just 
time replace head clause tbn add goal call body 
changed program db straightforward prove db tbn pg holds 
true construction tabled program db shown crude lot room optimization su ces show parameterized logic program singly connected bayesian network runs jv time set nodes 
estimate time complexity oldt search dbg declare tbn predicate form call table predicate verify conditions applicability graphical em algorithm details omitted 
time complexity oldt search goal tbn dbg notice calls occur pre order scan parents node children tree calls call occur val times 
invokes calls rest nodes parents children graph caller node di rent set variable instantiations second call call refers solutions stored solution table time 
number added computation steps 
distribution semantics model semantics unfold fold transformation tamaki sato preserves herbrand model transformed program unfold fold transformation applied parameterized logic programs preserves denotation transformed program 

dbg transformed oldt interpreter collect msw atoms case hmm program 
parameter learning logic programs symbolic statistical modeling oldt search bounded constant val val val val case 
result oldt time proportional number nodes original graph size support graph provided number edges connecting node values random variable bounded 
proposition singly connected bayesian network de ning distribution pg set nodes dbg tabled program derived 
suppose number edges connecting node values random variable bounded constant 
suppose table access done time 
oldt time computing pg observed variable means dbg jv time iteration required graphical em algorithm 
observations time complexity jv jt 
jv time complexity required compute distribution singly connected bayesian network standard algorithm pearl castillo em algorithm 
conclude graphical em algorithm cient em algorithm singly connected bayesian networks 
wemust quickly add graphical em algorithm applicable arbitrary bayesian networks proposition says explosion support graph avoided appropriate programming case singly connected bayesian networks 
summarize graphical em algorithm single generic em algorithm proved time complexity specialized em algorithms baum welch algorithm hmms inside outside algorithm pcfgs singly connected bayesian networks developed independently research eld 
table summarizes time complexity em learning oldt search graphical em algorithm case observation 
rst column sc bns represents singly connected bayesian networks 
second column shows program 
hmm subsection dbg pcfg program subsection dbg transformed bayesian network program subsection respectively 
oldt time third column time oldt search complete search explanations 
gem fourth column time iteration taken graphical em algorithm update parameters 
respectively number states hmm number nonterminals pcfg length input string network 
column standard specialized em algorithm model 

marginal distribution pg variable required construct similar tabled program computes marginal probabilities jv time adding extra arguments convey evidence embedding program 

check conditions dbg 
uniqueness condition obvious sampling uniquely generates sampled value random variable 
nite support condition satis ed nite number random variables values 
acyclic support condition immediate acyclicity bayesian networks 
exclusiveness condition independent condition easy verify 
sato kameya model program oldt time gem specialized em hmms baum welch pcfgs dbg inside outside sc bns dbg jv jv castillo user model table time complexity em learning oldt search graphical em algorithm modeling language prism developing symbolic statistical modeling prism url mi cs titech ac jp prism implementation distribution semantics sato sato kameya sato 
language modeling complex symbolic statistical phenomena discourse interpretation natural language processing gene inheritance interacting social rules 
programming language looks extension prolog new built predicates including msw predicate special predicates manipulating msw atoms parameters 
prism program comprised parts directives modeling utilities 
directive part contains declarations values telling system msw atoms execution 
modeling part set non unit de nite clauses de ne distribution denotation program msw atoms 
part utility part arbitary prolog program refers predicates de ned modeling part 
utility part learn built predicate carry em learning observed atoms 
prism provides modes execution 
sampling execution random sampling drawn distribution de ned modeling part 
second computes probability atom 
third returns support set goal 
execution modes available built predicates 
report implementation graphical em algorithm ed oldt search mechanism way completed 
currently prolog naive db section available em learning realized partially sharing explanations learn naive db 
putting computational aside problem expressing learning hmms pcfgs pseudo bayesian networks models current version 
learning experiments section parser substitute oldt interpreter independently implemented graphical em algorithm 

learning experiments complexity analysis graphical em algorithm popular symbolic probabilistic models previous section look actual behavior graphical em algorithm real data section 
conducted learning experiments pcfgs parameter learning logic programs symbolic statistical modeling corpora contrasting characters compared performance graphical em algorithm inside outside algorithm terms time iteration time updating parameters 
results indicate graphical em algorithm outperform inside outside algorithm orders 
reported sato kameya abe shirai 
proceeding review inside outside algorithm completeness 
inside outside algorithm inside outside algorithm proposed baker generalization baum welch algorithm pcfgs 
algorithm designed estimate parameters cfg grammar chomsky normal form containing rules expressed numbers nonterminals starting symbol 
suppose input sentence wl 
iteration rst computes bottom manner inside probabilities ws wt computes outside probabilities ws iwt wl top manner 
computing probabilities parameters updated process iterates predetermined criterion likelihood input sentence achieved 
baker give analysis inside outside algorithm lari young showed takes time iteration la erty proved em algorithm 
true inside outside algorithm recognized standard em training pcfgs notoriously slow 
literature explicitly stating time required inside outside algorithm carroll rooth carroll riezler rooth 
reported example trained pcfg rules corpus german subordinate clauses average ambiguity trees clause machines mhz sun ultrasparc mhz sun ultrasparc ii took hours complete iteration 
inside outside algorithm slow 
learning experiments corpora report parameter learning existing pcfgs corpora moderate size compare graphical em algorithm inside outside algorithm terms time iteration 
mentioned support graphs input em algorithm generated parser parser 
measurements mhz sun ultrasparc ii gb memory solaris threshold increase log likelihood input sentences set stopping criterion em algorithms 
experiments atr corpus edr corpus converted pos part speech tagged corpus 
similar size contrasting characters sentence length ambiguity grammars 
rst experiment employed atr corpus japanese english corpus japanese part developed atr matsuo morita 
contains short 
parser tomita generalized lr parser developed tanaka tokunaga laboratory tokyo institute technology tanaka 
sato kameya conversational sentences minimum length average length maximum length respectively 
skeleton pcfg employed context free grammar comprising rules nonterminals terminals manually developed atr corpus tanaka yields parses sentence 
inside outside algorithm accepts cfg chomsky normal form converted chomsky normal form atr atr contains rules nonterminals terminals 
divided corpus subgroups similar length containing randomly chosen sentences 
preparations compare length graphical em algorithm applied atr inside outside algorithm applied atr terms time iteration running convergence 
sec sec sec gem original gem chomsky nf gem original gem chomsky nf time iteration vs gem atr curves show learning results axis length input sentence axis average time taken em algorithm iteration update parameters contained support graphs generated chosen sentences parameters grammar change 
left graph inside outside algorithm plots cubic curve labeled 
omitted curve drawn graphical em algorithm drew axis 
middle graph magni es left graph 
curve labeled gem original plotted graphical em algorithm applied original grammar labeled gem chomsky nf atr average sentence length measured whichever grammar employed graphical em algorithm runs hundreds times faster times faster case times faster case atr inside outside algorithm iteration 
right graph shows linear dependency updating time graphical em algorithm sentence length 
di erence anticipated learning speed speed gap inside outside algorithm graphical em algorithm unexpectedly large 
conceivable reason atr corpus contains short sentences atr parameter learning logic programs symbolic statistical modeling ambiguous parse trees sparse generated support graphs small ects favorably graphical em algorithm 
conducted experiment corpus contains longer sentences ambiguous grammar generates dense parse trees 
edr japanese corpus japan edr containing japanese news article sentences 
process re annotation part randomly sampled sentences available labeled corpus 
compared atr corpus sentences longer average length sentences minimum length maximum length cfg grammar rules converted chomsky normal form grammar edr containing rules developed ambiguous keep coverage rate having parses sentence length parses sentence length 
sec sec sec gem original gem original time iteration vs gem edr shows obtained curves experiments edr corpus graphical em algorithm applied vs inside outside algorithm applied edr condition atr corpus plotting average time iteration process sentences designated length plotted time inside outside algorithm average iterations graphical em algorithm average iterations 
clear middle graph time graphical em algorithm runs orders magnitude faster inside outside algorithm 
average sentence length takes second takes seconds giving speed ratio 
sentence length takes seconds takes seconds giving speed ratio 
speed ratio widens compared atr corpus 
explained mixed ects time complexity inside outside algorithm moderate increase total size support graphs notice right graph shows total size support graphs grows sentence length time iteration graphical em algorithm linear total size support graphs 
sato kameya implemented inside outside algorithm faithfully baker lari young room improvement 
kita gave re ned inside outside algorithm kita 
implementation mark johnson inside outside algorithm loadable www cog brown edu 
implementations may lead di erent 
conducted learning experiments entire atr corpus implementations measured updating time iteration sato 
turned implementations run twice fast naive implementation take seconds iteration graphical em algorithm takes second iteration orders magnitude faster 
regrettably similar comparison entire edr corpus available moment abandoned due memory ow parsing construction support graphs 
learning experiments far compared time iteration ignore extra time search parsing required graphical em algorithm 
question naturally arises comparison terms total learning time 
assuming iterations learning atr corpus estimated considering parsing time graphical em algorithm combined parser runs orders magnitude faster implementations kita johnson inside outside algorithm sato 
course estimation directly apply graphical em algorithm combined oldt search oldt interpreter take time parser time needed depends oldt search 
conversely may able take rough indication far approach graphical em algorithm combined oldt search support graphs go domain em learning pcfgs 
performance gap previous subsection compared performance graphical em algorithm inside outside algorithm pcfgs corpora implementations inside outside algorithm 
experiments graphical em algorithm considerably outperformed inside outside algorithm despite fact time complexity 
look causes performance gap 
simply put inside outside algorithm slow primarily lacks parsing 
backbone cfg grammar explicitly take advantage constraints imposed grammar 
see help review inside probability nonterminal spans th word th word calculated inside outside algorithm grammar 
bc grammar bc bc probability associated production rule bc 
note xed triplet usual term bc non zero 
answer question right implementation oldt search completed 
parameter learning logic programs symbolic statistical modeling relatively small number determined successful parses rest combinations give term 
inside outside algorithm attempts compute term iteration possible combinations repeated possible resulting lot redundancy 
thesame kind redundancy occurs computation outside probability inside outside algorithm 
graphical em algorithm free redundancy runs parse trees parse forest represented support graph 
added hand superiority learning speed graphical em algorithm realized cost space complexity inside outside algorithm merely requires nl space array store probabilities graphical em algorithm needs space store support graph number nonterminals sentence length 
trade understandable notices graphical em algorithm applied considered partial evaluation inside outside algorithm grammar appropriate data structure output 
parsing preprocess em learning pcfgs unique graphical em algorithm fujisaki jelinek cocke black stolcke 
approaches contain redundancies compared graphical em algorithm 
instance stolcke uses earley chart compute inside outside probability parses implicitly reconstructed iteration dynamically combining completed items 

related discussion related crossroads logic programming probability theory considering enormous body done elds incompleteness unavoidable reviewing related 
having said look various attempts integrate probability computational logic logic programming 
reviewing immediately notice types usage probability 
type constraint approach emphasizes role probability constraints necessarily seek unique probability distribution logical formulas 
type distribution approach explicitly de nes unique distribution model theoretical means proof theoretical means compute various probabilities propositions 
atypical constraint approach seen early probabilistic logic nilsson 
central problem probabilistic entailment problem compute upper lower bound probability target sentence away compatible knowledge base containing logical sentences necessarily logic programs annotated probability 
probabilities constraints 
emphasize di erence inside outside algorithm graphical em algorithm solely computational ciency converge parameter values starting initial values 
linguistic evaluations estimated parameters graphical em algorithm reported sato 


omit literature leaning strongly logic 
logic concerning uncertainty kyburg 
sato kameya possible range 
linear programming technique solve problem inevitably delimits applicability approach nite domains 
lukasiewicz investigated computational complexity probabilistic entailment problem slightly di erent setting 
knowledge base comprises statements form representing 
showed inferring tight np hard general proposed tractable class knowledge base called conditional constraint trees 
uential nilsson haddawy introduced deductive system probabilistic logic remedies drawbacks nilsson approach computational intractability lack proof system 
system deduces probability range proposition rules probabilistic inferences unconditional conditional probabilities 
instance rules infers propositional variables designates probability range 
turning logic programming probabilistic logic programming formalized ng subrahmanian subrahmanian constraint approach 
program set annotated clauses form fn atom fi basic formula conjunction disjunction atoms sub interval indicating probability range 
query fn answered extension sld refutation 
formalization assumed language contains nite number constant predicate symbols function symbol allowed 
similar framework proposed lakshmanan sadri syntactic restrictions nitely constant predicate symbols function symbols di erent uncertainty setting 
annotated clauses form bn bi atoms con dence level represents belief interval doubt interval expert clause 
seen de ning unique probability distribution secondary concern constraint approach 
sharp contrast bayesian networks discipline rests ability networks de ne unique probability distribution pearl castillo 
researchers bayesian networks seeking away mixing bayesian networks logical representation increase inherently propositional expressive power 
breese logic programs automatically build bayesian network query 
breese approach program union de nite clause program set conditional dependencies form qn atoms 
query network constructed dynamically connects query relevant atoms program turn de nes local distribution connected atoms 
logical variables appear atoms function symbol allowed 
ngo haddawy extended breese approach incorporating mechanism re ecting context 
clause form ai called atoms probabilistic atoms lj context atoms disjoint atoms computed general logic program satisfying certain parameter learning logic programs symbolic statistical modeling tions 
query set evidence context atoms relevant ground atoms identi ed resolving context atoms away sldnf resolution local bayesian network built calculate probability query 
proved soundness completeness query evaluation procedure condition programs acyclic domains nite 
de ning local distribution query poole de ned global distribution probabilistic horn abduction 
program consists de nite clauses disjoint declarations form disjoint hn pn speci es probability distribution hypotheses abducibles fh 
assigned probabilities ground atoms help theory logic programming furthermore proved bayesian networks representable framework 
previous approaches language contains function symbols acyclicity condition imposed programs semantics de nable severe restriction 
probabilities de ned quanti ed formulas 
bacchus 
rst order probabilistic language clauses annotated probabilities 
language allows statistically quanti ed term kx denote ratio individuals nite domain satisfying satisfying 
assuming world interpretation language equally de ne probability sentence knowledge worlds kb base kb limit limn worlds number kb possible worlds containing individuals satisfying parameters judging approximations 
limit necessarily exist domain nite showed method cope di culties arising direct inference default reasoning 
linguistic vein muggleton formulated slps stochastic logic programs procedurally extension pcfgs probabilistic logic programs 
clause range restricted annotated probability probability goal product ps appearing refutation modi cation subgoal invoke clauses pi ci refutation step probability th clause normalized pk pi 
cussens enriched slps introducing special class log linear models sld refutations goal 
example considers possible sld refutations general goal de nes probability refutation exp 
number associated clause ci feature number occurrences ci normalizing constant 
probability assigned sum probabilities refutation 

condition says ground atom assigned unique integer bn holds ground instance clause form bn 
condition program includes write recursive clauses 

syntactic property appearing head appear body clause 
unit clause ground 
sato kameya limitations potential problems approaches described far similar limitations potential problems 
descriptive power con ned nite domains common limitation due linear programming technique nilsson due syntactic restrictions allowing nitely constant function predicate symbols ng subrahmanian lakshmanan sadri 
bayesian networks limitation nite number random variables representable 
various semantic syntactic restrictions logic programs 
instance acyclicity condition imposed poole ngo haddawy prevents unconditional clauses local variables range imposed muggleton cussens excludes programs usual membership prolog program 
type problem possibility assigning con icting probabilities logically equivalent formulas 
slps andp necessarily coincide may di erent refutations muggleton cussens 
consequently slps trouble naively interpret probability true 
assigning probabilities arbitrary quanti ed formulas scope approaches slps 
big problem common approach probabilities numbers come 
generally speaking binary random variables model determine probabilities completely specify joint distribution ful lling requirement reliable numbers quickly impossible grows 
situation worse unobservable variables model possible causes disease 
apparently parameter learning observed data natural solution problem parameter learning logic programs studied 
distribution semantics proposed sato attempt solve problems line global distribution approach 
de nes distribution probability measure possible interpretations ground atoms arbitrary logic program rst order language assigns consistent probabilities closed formulas 
distribution semantics enabled derive em algorithm parameter learning logic programs rst time 
naive algorithm dealing large problems di cult exponentially explanations observation hmms 
believe ciency problem solved large extent graphical em algorithm 
em learning em learning central issues separately mention related em learning symbolic frameworks 
koller pfe er approach knowledge model construction em learning estimate parameters labeling clauses 
express probabilistic dependencies events de nite clauses annotated probabilities similarly ngo haddawy approach locally build bayesian network relevant context evidence 
recursive probability models proposed pfe er koller extension bayesian networks allow nitely random variables 
organized attributes classes probability measure attribute values introduced 
parameter learning logic programs symbolic statistical modeling query 
parameters learned applying constructed network specialized em algorithm bayesian networks castillo 
dealing pcfg statically constructed bayesian network proposed pynadath wellman possible combine em algorithm method estimate parameters pcfg 
unfortunately constructed network singly connected time complexity probability computation potentially exponential length input sentence 
closely related em learning parameter learning log linear models 
riezler proposed im algorithm approach probabilistic constraint programming 
im algorithm general parameter estimation algorithm incomplete data log linear models probability function takes form exp pn parameters estimated th feature observed object normalizing constant 
feature function log linear model highly exible includes distribution special case 
price pay computational cost requires summation exponentially terms 
avoid cost exact computation approximate computation monte carlo method possible 
whichever may choose learning time increases compared em algorithm 
fam failure adjusted maximization algorithm proposed cussens em algorithm applicable pure normalized slps may fail 
deals special class log linear models cient im algorithm 
statistical framework fam di erent distribution semantics comparison graphical em algorithm di cult 
slightly tangential em learning koller 
developed functional modeling language de ning probability distribution symbolic structures showed computed results leads cient probability computation singly connected bayesian networks pcfgs 
corresponds computation inside probability inside outside algorithm computation outside probability untouched 
directions parameterized logic programs expected useful modeling tool complex phenomena 
various types modeling stochastic grammars bayesian networks modeling gene inheritance tribe white rules bi lateral cross cousin marriage interact rules genetic inheritance sato 
model quite interdisciplinary exibility combining msw atoms means de nite clauses greatly facilitated modeling process 
satisfying conditions section uniqueness condition roughly cause yields ect nite support condition nite number explanations observation acyclic support condition explanations cyclic sato kameya exclusiveness condition explanations mutually exclusive independence condition events explanation independent applicability graphical em algorithm daunting modeling experiences far tell modeling principle section ectively guides successful modeling 
return obtain declarative model described compactly high level language parameters ciently learnable graphical em algorithm shown preceding section 
directions relax applicability conditions especially uniqueness condition prohibits generative model failure generating multiple observable events 
pointed section mar condition appendix adapted semantics replace uniqueness condition validates graphical em algorithm complete data uniquely determine observed data just case partially bracketed corpora pereira schabes feel need research topic 
investigating role acyclicity condition theoretically interesting acyclicity related learning logic programs reddy tadepalli 
scratched surface individual research elds hmms pcfgs bayesian networks 
remains done clarifying experiences research eld re ected framework parameterized logic programs 
example need clarify relationship symbolic approaches bayesian networks spi li ambrosio approach 
unclear compiled approach junction tree algorithm bayesian networks incorporated approach 
aside exact methods approximate methods probability computation specialized parameterized logic programs developed 
direction improving learning ability introducing priors ml estimation cope data sparseness 
basic distributions probabilistic switches correlated worth trying near 
important take advantage logical nature approach handle uncertainty 
example shown sato learn parameters negative examples grass wet treatment negative examples parameterized logic programs infancy 
concerning developing complex statistical models programs distributions scheme stochastic natural language processing exploits semantic information promising 
instance uni cation grammars abney may target pcfgs feature structures logically describable ambiguity feature values expressible probability distribution 
building mathematical basis logic programs continuous random variables challenging research topic 
parameter learning logic programs symbolic statistical modeling 
logical mathematical framework statistical parameter learning parameterized logic programs de nite clause programs containing probabilistic facts parameterized probability distribution 
extends traditional herbrand model semantics logic programming distribution semantics possible world semantics probability distribution possible worlds herbrand interpretations unconditionally applicable arbitrary logic programs including ones hmms pcfgs bayesian networks 
new em algorithm graphical em algorithm section learns statistical parameters observations class parameterized logic programs representing sequential decision process decision exclusive independent 
works support graphs new data structure specifying logical relationship observed goal explanations estimates parameters computing inside outside probability generalized logic programs 
complexity analysis section showed oldt search complete tabled refutation method logic programs employed support graph construction table access done time graphical em algorithm despite generality time complexity existing em algorithms baum welch algorithm hmms inside outside algorithm pcfgs singly connected bayesian networks developed independently research eld 
addition pseudo probabilistic context sensitive grammars nonterminals showed graphical em algorithm runs time sentence length compare actual performance graphical em algorithm inside outside algorithm conducted learning experiments pcfgs section real corpora contrasting characters 
atr corpus containing short sentences grammar ambiguous parses sentence edr corpus containing long sentences grammar ambiguous average sentence length 
cases graphical em algorithm outperformed inside outside algorithm orders magnitude terms time iteration suggests ectiveness approach em learning graphical em algorithm 
semantics limited nite domains nitely random variables applicable logic programs arbitrary complexity graphical em algorithm expected give general cient method parameter learning models complex symbolic statistical phenomena governed rules probabilities 
acknowledgments authors wish anonymous referees comments suggestions 
special go takashi mori shigeru abe stimulating discussions learning experiments tanaka tokunaga laboratory kindly allowing parser linguistic data 
sato kameya appendix properties db appendix list properties pdb de ned parameterized logic program db countable rst order language pdb assigns consistent probabilities closed formula pdb def pdb db guaranteeing continuity sense limn pdb tn pdb limn pdb tn pdb enumeration ground terms proposition proposition relates pdb herbrand model 
prove need terminology 
factor closed formula prenex disjunctive normal form qi existential quanti cation universal quanti cation matrix 
length quanti cations called rank factor 
de ne set formulas factors conjunctions disjunctions 
associate formula multi set ranks factor quanti cation fng factor rank stands union multi sets 
instance 
multi set ordering proof proposition usual induction complexity formulas 
lemma formula ground atoms pdb pf mdb 
proof prove lemma conjunction atoms form dxn xi 
pdb dxn pdb db dxn db dn xn pf mdb dxn proposition closed formula pdb pf mdb 

de nitions pf mdb db pdb see section 

consistent mean probabilities assigned logical formulas respect laws probability andp 
parameter learning logic programs symbolic statistical modeling proof recall closed formula equivalent prenex disjunctive normal form belongs 
prove proposition formulas induction multi set ordering fr 
quanti cation 
proposition correct lemma 
suppose 
write indicates single occurrence factor assume similarly treated 
assume bound variables renamed avoid name clash 
xq xg light validity xa xa contains free pdb pdb pdb xg lim pdb tk lim pdb tk lim pf mdb tk induction hypothesis pf mdb xg pf mdb prove theorem de nition introduced section 
distribution semantics considers program db set nitely ground de nite clauses set facts probability measure pf set rules clause head appears put head def fb appears clause head wi enumeration clauses de ne form rules db def mdb herbrand model obvious 
lemma head mdb 
theorem 
states general level sides de nition yn tn wn ofp coincide random variables instantiated ground term 
theorem form rules head 
pdb pdb pdb 

expression means may occur speci ed positions indicates single occurrence positive boolean formula holds 

de nition di erent usual lloyd doets talking ground level 
true disjuncts true 
sato kameya proof pdb pdb db pdb lim pdb db db wig lim pdb db lim pf mdb lemma wig lim pf mdb pf mdb pf lemma follows pdb wig wig pdb pdb pdb prove proposition useful probability computation 
db bethe support set atom introduced section set explanations 
sequel ground atom 
write db fs db de ne set def 
db db proposition head pdb pdb pdb db 
proof pdb proof exactly parallels theorem replaced fact true herbrand model form mdb 
pdb pdb pdb db pdb db show distribution semantics probabilistic extension traditional herbrand model semantics logic programming proving theorem 
says probability mass distributed exclusively possible herbrand models 
de ne set herbrand models generated xing varying subset program db 
set fe formulas denotes nite disjunction parameter learning logic programs symbolic statistical modeling def 
db mdb note merely subset db conclude pdb priori theorem theorem states pdb distribution semantics distributes probability mass exclusively possible herbrand models 
prove theorem need preparations 
recalling atoms outside head chance proved db weintroduce def 
db ground atom head herbrand interpretation db jf restriction atoms lemma db herbrand interpretation 
mdb db head 
proof part immediate property herbrand model 
part suppose satis es right hand side 
show mdb jf 
mdb jf coincide atoms head prove give truth values atoms head 
take head write db suppose wehave sj jf sj mdb jf sj implies mdb jf sj follows mdb jf arbitrary conclude mdb jf agree truth values assigned atoms head 
theorem pdb 
proof lemma 
db mdb head pdb proposition 
prove pdb enumeration atoms belonging head provable db false herbrand model mdb 
pdb lim pdb db lim pf mdb pf countable conjunction measurable sets probability measure probability measure follows pdb head 
sato kameya appendix mar missing random condition original formulation em algorithm dempster 
assumed exists mapping complete data incomplete observed data case parsing parse tree input sentence uniquely determines uniqueness condition ensures existence mapping explanations observations 
face situation mapping complete data incomplete data wish apply em algorithm 
dilemma solved missing data mechanism complete data incomplete 
missing data mechanism distribution parameterized observed data described 
says incomplete correspondence fhx yi naturally 
rubin derived conditions data missing random data observed random collectively called mar missing random condition showed assume missing data mechanism observations satis es mar condition may estimate parameters distribution simply applying em algorithm observed data 
adapt mar condition parameterized logic programs follows 
generative model satisfying uniqueness condition outputs goals parse trees 
extend model additionally inserting missing data mechanism observation assume satis es mar condition 
extended model correspondence explanations observations generates non exclusive observations thatp causes pg pdb 
mar condition allowed apply em algorithm observations 
put di erently uniqueness condition seemingly destroyed em algorithm applicable just assuming missing data mechanism satisfying mar condition 
abney 

stochastic attribute value grammars 
computational linguistics 


learning acyclic rst order horn sentences entailment 
proceedings eighth international workshop algorithmic learning theory 
ohmsha springer verlag 
bacchus grove halpern koller 

statistical knowledge bases degrees belief 
arti cial intelligence 
baker 

trainable grammars speech recognition 
proceedings spring conference acoustical society america pp 

parameter learning logic programs symbolic statistical modeling carroll riezler rooth 

inside outside estimation lexicalized pcfg german 
proceedings th annual meeting association computational linguistics acl pp 

breese 

construction belief decision networks 
computational intelligence 
carroll rooth 

valence induction head lexicalized pcfg 
proceedings rd conference empirical methods natural language processing emnlp 
castillo gutierrez 

network models 
springer verlag 
expert systems probabilistic charniak carroll 

context sensitive statistics improved grammatical language models 
proceedings th national conference arti cial intelligence aaai pp 

chi geman 

estimation probabilistic context free grammars 
computational linguistics 
chow 

probability theory rd ed 
springer 
clark 

negation failure 
gallaire minker 
eds logic databases pp 

plenum press 
cormen leiserson rivest 

algorithms 
mit press 
cussens 

loglinear models rst order probabilistic reasoning 
proceedings th conference uncertainty arti cial intelligence uai pp 

cussens 

parameter estimation stochastic logic programs 
machine learning 
ambrosio 

inference bayesian networks 
ai magazine summer 
subrahmanian 

hybrid probabilistic programs 
proceedings th international conference programming iclp pp 

dempster laird rubin 

maximum likelihood incomplete data em algorithm 
royal statistical society 
doets 

logic logic programming 
mit press 
flach kakas 
eds 

abduction induction essays relation integration 
kluwer academic publishers 
haddawy 

anytime deduction probabilistic logic 
journal arti cial intelligence 
fujisaki jelinek cocke black 

probabilistic parsing method sentence disambiguation 
proceedings st international workshop parsing technologies pp 

japan edr 

edr electronic dictionary technical guide nd edition 
technical report japan electronic dictionary research institute sato kameya kakas kowalski toni 

abductive logic programming 
journal logic computation 
kameya 

learning representation symbolic statistical knowledge japanese 
ph 
dissertation tokyo institute technology 
kameya sato 

cient em learning parameterized logic programs 
proceedings st conference computational logic cl vol 
lecture notes arti cial intelligence pp 

springer 
kita 

probabilistic language models japanese 
tokyo kai 
koller mcallester pfe er 

ective bayesian inference stochastic programs 
proceedings th national conference arti cial intelligence aaai pp 

koller pfe er 

learning probabilities noisy rst order rules 
proceedings th international joint conference arti cial intelligence ijcai pp 

kyburg 

uncertainty logics 
gabbay hogger robinson 
eds handbook logics arti cial intelligence logic programming pp 

oxford science publications 
la erty 

derivation inside outside algorithm em algorithm 
technical report ibm watson research center 
lakshmanan sadri 

probabilistic deductive databases 
proceedings international symposium logic programming ilps pp 

lari young 

estimation stochastic context free grammars inside outside algorithm 
computer speech language 
li ambrosio 

cient inference bayes networks combinatorial optimization problem 
international journal approximate reasoning 
lloyd 

foundations logic programming 
springer verlag 
lukasiewicz 

probabilistic deduction conditional constraints basic events 
journal arti cial intelligence research 
manning schutze 

foundations statistical natural language processing 
mit press 
mclachlan krishnan 

em algorithm extensions 
wiley interscience 
muggleton 

stochastic logic programs 
de raedt 
ed advances inductive logic programming pp 

ios press 
ng subrahmanian 

probabilistic logic programming 
information computation 
ngo haddawy 

answering queries context sensitive probabilistic knowledge bases 
theoretical computer science 
nilsson 

probabilistic logic 
arti cial intelligence 
parameter learning logic programs symbolic statistical modeling pearl 

probabilistic reasoning intelligent systems 
morgan kaufmann 
pereira schabes 

inside outside reestimation partially bracketed corpora 
proceedings th annual meeting association computational linguistics acl pp 

pereira warren 

de nite clause grammars language analysis survey formalism comparison augmented transition networks 
arti cial intelligence 
pfe er koller 

semantics inference recursive probability models 
proceedings seventh national conference arti cial intelligence aaai pp 

poole 

probabilistic horn abduction bayesian networks 
arti cial intelligence 
pynadath wellman 

generalized queries probabilistic context free grammars 
ieee transaction pattern analysis machine intelligence 
rabiner 

tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
rabiner juang 

foundations speech recognition 
prentice hall 
ramakrishnan rao sagonas swift warren 

cient tabling mechanisms logic programs 
proceedings th international conference logic programming iclp pp 

mit press 
reddy tadepalli 

learning rst order acyclic horn programs entailment 
proceedings th international conference machine learning proceedings th international conference inductive logic programming 
morgan kaufmann 
riezler 

probabilistic constraint logic programming 
ph thesis universitat tubingen 
rubin 

inference missing data 
biometrika 
sagonas warren 

xsb cient deductive database engine 
proceedings acm sigmod international conference management data pp 

sato 

statistical learning method logic programs distribution semantics 
proceedings th international conference programming iclp pp 

sato 

modeling scienti theories prism programs 
proceedings ecai workshop machine discovery pp 

sato 

minimum likelihood estimation negative examples statistical abduction 
proceedings ijcai workshop abductive reasoning pp 

sato kameya 

prism language symbolic statistical modeling 
proceedings th international joint conference arti cial intelligence ijcai pp 

sato kameya sato kameya 

viterbi algorithm em learning statistical abduction 
proceedings uai workshop fusion domain knowledge data decision support 
sato kameya abe shirai 

fast em learning family pcfgs 
titech technical report dept cs tr tokyo institute technology 
shen yuan zhou 

linear tabulated resolution prolog control strategy 
theory practice programming 
sterling shapiro 

art prolog 
mit press 
stolcke 

cient probabilistic context free parsing algorithm computes pre probabilities 
computational linguistics 
tamaki sato 

unfold fold transformation logic programs 
proceedings nd international conference programming iclp lecture notes computer science pp 

springer 
tamaki sato 

old resolution tabulation 
proceedings rd international conference programming iclp vol 
lecture notes computer science pp 

springer 
tanaka 

japanese grammar speech recognition considering method 
proceedings meeting sig slp spoken language processing slp pp 

information processing society japan 
japanese 
matsuo morita 

atr integrated speech language database 
technical report tr atr interpreting telecommunications research laboratories 
japanese 
warren 

memoing logic programs 
communications acm 


probabilistic languages review open questions 
computing surveys 
white 

anatomy kinship 
prentice hall 
zhang poole 

exploiting causal independence bayesian network inference 
journal arti cial intelligence research 

