institute physics publishing network computation neural systems network comput 
neural syst 
pii neural coding decoding communication channels quantization 
alexander dimitrov john miller center computational biology montana state university mt usa mail alex cns montana edu jpm cns montana edu received july final form march published august online stacks iop org network novel analytical approach studying neural encoding 
step model neural sensory system communication channel 
method typical sequence context show coding scheme bijective relation equivalence classes stimulus response pairs 
analysis allows quantitative determination type information encoded neural activity patterns time identification code information represented 
due high dimensionality sets involved relation extremely difficult quantify 
circumvent problem limited data set available efficiently technique information theory quantization 
quantize neural responses reproduction set small finite size 
possible quantizations choose preserves informativeness original stimulus response relation possible information distortion function 
method allows study coarse highly informative approximations coding scheme model refine automatically data available 
steps understanding neural basis animal behaviour characterizing code nervous system represents information 
computations underlying animal behavioural decisions carried context code 
determination neural coding schemes extremely important goal due interest nature code constraints knowledge places development theories biophysical mechanisms underlying neural computation 
iop publishing printed uk dimitrov deciphering neural code sensory system reduced interconnected tasks 
task determine correspondence neural activity sensory signals 
task reduced interrelated problems determining specific stimulus parameters features encoded neural ensemble activity determining nature neural symbols information encoded 
ancillary task define quantitative measures significance sensory information associated neural symbols correlated 
considerable progress approaching tasks independent problems 
approaches taken include stimulus reconstruction impoverished stimulus sets characterize stimulus response properties 
provided direct estimates information theoretic measures correlations stimulus response completely bypassing problem identifying stimulus 
independent treatment interconnected tasks introduces multiple assumptions prevent complete solution 
approaches start assumption relevant codewords single spike order stimulus reconstruction method mean spike rate defined interval proceed calculating expected stimulus features correlated codewords 
approaches assumption relevant stimulus features moving bars gratings investigation parts visual cortex proceed identify pattern spikes follow presentation features 
developed analytical approach takes tasks consideration simultaneously 
specifically aim approach allow quantitative determination type information encoded neural activity patterns time identify code information represented 
specific goals 
formulate model neural sensory system communication channel section 
context show coding scheme consists classes stimulus response pairs form structure akin dictionary class consists stimulus set response set synonymous 
classes independent intersecting members 
second goal design method discovering dictionary structure section 
quantize neural responses reproduction set small finite size 
possible quantizations choose preserves informativeness original stimulus response relation possible information distortion function 
start observation neural code satisfy conflicting demands 
hand organism recognize natural object identical repeated exposures 
sense signal processing operations organism need deterministic behavioural level 
hand neural coding scheme deal projections sensory environment smaller stimulus space uncertainties introduced external internal noise sources 
neural signal processing necessity stochastic finer scale 
light functional issues confront early stages biological sensory system similar issues encountered communication engineers transmitting messages noisy media 
mind model input output relationship biological sensory system communication channel 
approach suggested knowledge properties information theory assigns object completely appreciated neural research literature 
principal method identification jointly typical sequences appendix stimulus response sets 
joint typicality rigorously defined concept extensively information theory described detail section 
technique neural coding decoding elucidate structure neural stimulus response relation 
coding model stochastic demonstrate deterministic relation emerges naturally level clusters stimulus response pairs 
investigate model consider possibility temporal patterns spikes small ensemble cells basic elements information transmission system 
due high dimensionality sets involved relation extremely difficult quantify 
circumvent problem limited data set available efficiently quantize neural responses reproduction set small finite size section 
quantization standard technique information theory appendix 
quantizing reproduction space size sufficiently small assure data size requirements diminished 
possible quantizations choose preserves informativeness original stimulus response relation possible information distortion function section 
relationship stimulus reproduction approximation coding scheme described 
method allows study coarse highly informative models coding scheme automatically refine data available 
ultimately simple description stimulus response relation recovered section 

neural information processing 
neural systems communication channels communication channels characterize relation random variables input output 
neural systems output space usually set activities group neurons 
input space sensory stimuli environment set activities group neurons 
recover correspondence stimuli responses call coding scheme 
problem inputs outputs coding scheme map correspondence input space output space 
decoding scheme inverse map 
general maps probabilistic 
provide precise definition describes situation considered deterministic bijective 
early stages neural sensory processing encode information sensory stimuli representation common nervous system 
consider encoding process probabilistic framework signal produced source probability 
example may sensory stimulus animal environment activity set neurons 
encoder stochastic map stochastic signal 
model operations neuronal layer 
output signal produced probability 
temporal pattern activity set cells 
description naturally cast formalism information theory 
consider neuron group neurons communication channel apply theory directly insights operation neural sensory system including detailed picture correspondence sensory stimuli neural representations 
basic assumption sensory stimuli represented patterns spikes neurons 
number neurons comprising information channel temporal extent patterns temporal precision spikes parameters determined data 
formulate hypotheses particular coding schemes fixing values parameters 
initial assumption distinct patterns fixed length precision may important 
ideas information transmission presence dimitrov noise allow group patterns larger classes consider coding equivalence classes 
scheme general encompasses existing hypotheses nature neural coding special cases 
equivalence classes uncovered analysed regard formal rules physiological mechanisms describe succinctly 
certain cases may turn commonly considered coding schemes 
sensory stimuli neural representation quite complex 
information theory suggests ways dealing complexity extracting essential parts signal maintaining information content 
achieve method typical sequences 
follows informal discussion properties relation coding model sensory system 
terms preceded nearly close describe events occur high probability 
sections discuss standard topics information theory 
see appendix formal definitions properties complete treatment 

typical sequences basic concepts information theory entropy mutual information random sources appendix 
analysis communication systems specific meaning 
consider random source sequences length form nth extension described nh distinct messages 
typical sequences typical sequences events comprise typical set nearly equiprobable 
typical set probability near unity number elements nearly nh theorem appendix 
summarize saying events equally surprising 
enables divide set sequences sets typical set entropy sequence close entropy set set contains rest sequences 
property proved typical sequences true high probability determine average behaviour large sample sequences 
functional property entropy give approximate size relevant set events illustrate consider bernoulli source 
example particular sequence length 
typical sequences sequences length approximately np zeros nq ones 
notice despite simplicity process model neural activity presence spike marked absence zero 
spikes sequence usually independent considered zeroth order approximation 
typical sequences applicable continuous random variables 
drawn normally distributed random variable mean zero variance 
sequence variables xn obviously drawn dimensional normal distribution 
typical sequences points contained ball radius 
property extended general normal distributions full covariance matrix case typical set lies ellipsoid determined principal directions eigenvalues covariance matrix 
neural coding decoding encoder early sensory areas visual auditory 
jointly typical sequences output channel 
neural sensory systems information channels 
decoder higher neural areas behaviour analysing information channels deal sets random sequences input output 
case necessary consider combined behaviour pair 
approach jointly typical sequences appendix 
pair sources nh jointly typical sequences pairs typical product space 
typical sequences elements nearly equiprobable set probability close unity appendix 
correspondence input output sequences necessarily due noise mismatch stimulus response sizes 
pairs typical typical jointly typical 
typical sequence nh sequences jointly typical vice versa 
probability randomly chosen pair jointly typical ni 
fixed consider ni come jointly typical pair 
suggests ni distinguishable messages codewords communicated 
knowledge places important bounds performance coding scheme 
structure coding scheme framework seen intuitively argument 
typical input sequence nh possible jointly typical sequences approximately equally 
total number possible typical sequences nh order insure sequences produce output sequence need divide output set chunks size nh corresponding different input sequences 
total number disjoint sets procedure ni transmit ni distinguishable sequences length 
coding decoding jointly typical sequences jointly typical pairs represent related stimulus response signals 
ni distinguishable codewords nh signals signals represent dimitrov 
structure jointly typical set 
nh typical stimulus sequences nh typical response sequences nh jointly typical sequences 
suggests ni distinguishable equivalence classes ci stimulus response pairs 
class nh input sequences nh output sequences 
number stimulus response pairs class ci codeword 
signals representing codewords combined equivalence classes call codeword classes 
codeword classes represent distinguishable messages transmitted communication system 
class stimulus invokes corresponding jointly typical response high probability ni 
define codebook system map codebook stochastic individual elements represented association probabilities 
considered codeword classes map bijective 
probability close unity elements codeword class associated elements codeword class 
shall decode output inputs belong codeword class 
similarly shall consider representation input outputs codeword class 
stimuli equivalence class considered indistinguishable responses class 
stimulus response additional structure resides metric space natural distance classes represented succinctly constraints structure exemplar minimum reconstruction error distance 
example coding jointly typical sequences 
shall binary source sequences finite extensions log log 
communication system consider example binary symmetric channel bsc 
bsc model communication system noise 
input output binary digits zero 
channel described transition probabilities pr correct pr error flipped bit 
bsc nice representation terms typical sequences 
considered random binary source probability measure generates sequences ones zeros 
sequences summed modulo xor ed original source neural coding decoding 
bsc 
dimensional binary space particular coding scheme allows correction flipped bit 
example codewords arrows mark decoding process 
pair opposing vertices provides functionality 
produce output 
theorems typicality appendix channel typically produce sequences nq ones errors np zeros correct transmission 
nh possible errors transmitted input sequence nh 
natural error measure bsc hamming distance returns number bits sequences differ 
illustrates example correct error block 
distinct sets arrows denote binary balls unit radius code correct error 
opposing vertices codewords transmit bit information 
equivalence classes elements 
element decoded centre binary ball hamming distance zero original codeword 
consider uniform input distribution 
sequences typical 
nh 
bsc error probability produce nh typical sequences nq flips 
possible sequence received nh possible neighbours order transmit small probability error need pick codewords sequences nh bits apart hamming distance 
nh sequences 

comments continuous case picture essentially discrete finite sets relations 
readers may raise issue scheme input output processes continuous space time 
argue cases handled current scheme 
notice continuous coding scheme contain uncertainties due channel noise measurement error receptor level 
scheme approximated arbitrary level detail discrete coding scheme process quantization appendix comes set uncertainties quantization noise 
schemes indistinguishable practice functionalities lie error bars 
distinguishable experiment resolve issue 
insist considering continuous stimuli responses arguments point benefit discrete coding schemes 
case object dimitrov feature recognition noted continuity stimuli usually due symmetries signal contribute interfere recognition features 
solution suggested pre process stimulus remove continuous symmetries possible continue recognition algorithm 
argument comes rate distortion theory 
shown optimal reproduction space continuous source continuous special cases source gaussian 
case compressing transmitting continuous source done best discrete representatives 
argue analytical approach covers conceivable case 
intention illustrate coding discrete sets wider context may initially perceived 

neural coding jointly typical sequences picture coding decoding jointly typical sequences gives framework think problem neural coding decoding 
description idealization prove properties optimal communication coding schemes basic structure valid coding scheme 
bijective bijective coding scheme needed robust decoding animals confusing distinct stimuli 
real system noise compromises amount information represented 
achieve robustness capabilities system devoted combating noise 
noise reduction achieved combining signals confused equivalence classes 
coding scheme bijective equivalence classes consistent representation sensory modality 
idealized case coding jointly typical sequences codeword classes neural system need similar size 
model property self consistent 
stimuli belonging certain codeword class invoke response codeword class high probability rarely produce response outside codeword class class 
case stimulus considered incorrectly represented 
noted idealized picture coding schemes optimal sense error rate asymptotically approaches zero 
achieved splitting input output clusters size appropriate channel input output connecting random 
applied neural case means neural coding scheme similar sensory modalities unique species individuals 
neural coding schemes currently seen special cases general coding scheme describe 
example rate coding schemes sequences short interval number spikes considered equivalence class 
codeword classes labelled number spikes 
stimuli precede sequence codeword class considered jointly typical decoded represented class 
guarantee classes nonoverlapping decoding error small 
similarly population vector coding schemes output sequences assigned classes value linear functional number spikes neuron population mean 
decoding done labelled neural coding decoding class identity interpreted direction planned hand movement 
spike latency code example continuous coding scheme 
fits formalism process quantization section 
classes determined mean latency jitter variance spike timing 
classes may partially overlapping 
stimulus feature decoded rate code case latency class spike falls 
model neural subsystem task recover codebook 
basic ways approach 
codebook deterministic equivalence classes approximate deterministic map 
equivalence class identified fixed members 
errors caused disregarding certain regions event space 
general approach represent conditional probability class membership case explicitly model probability misclassification lose simplicity offered jointly typical sequences 
shall approaches warranted 

finding codebook communication systems usual information theory design coding scheme structure communication channel 
application differs analysing neural coding scheme implemented animal nervous system 
goal uncover structure described far section observations stimulus response properties neural system investigation 

quantizing response mentioned information quantities depend underlying probability function structure event space 
allows estimate cases traditional statistical measures variance correlations simply exist 
drawback model necessary probabilities large numbers data estimate probabilities 
problem compounded factors 
research suggests nervous systems represent sensory stimuli relatively long temporal windows tens ms diverse preparations coordinated activity multiple neurons 
research geared finding better unbiased estimates goal recover complete coding scheme sensory system 
pointed number data points needed nonparametric analysis neural responses correlated long time periods length multiple neurons grows exponentially conceivable systems required data recording time may exceed expected lifespan system 
resolve issue choose sacrifice detail description coding scheme order obtain robust estimates coarser description 
achieved quantization appendix neural representation coarser representation smaller event space yn 
yn referred reproduction controlling size reproduction ensure data requirements describe relation diminished 
exponentially growing number needed data proportional size reproduction chosen researcher small 
results section valid general case sources continuous ergodic random variables 
formulation general case requires special attention details 
clarity presentation shall assume random variables finite possibly large discrete 
dimitrov quantizers maps probability space 
deterministic functions stochastic conditional probability appendix 
size reproduction space smaller size quantized space 
shall consider general case stochastic quantizer yn probability response belonging class yn 
deterministic quantizer yn special case takes values zero 
cases stimulus response reproduction form markov chain yn 
quality quantization characterized distortion function appendix 
shall look minimum distortion quantization information distortion function discuss relationship codebook estimation problem 
quantization idea implicitly neuroscience time 
example rate coding scheme effectively uses deterministic quantizer assign neural response classes number spikes pattern 
metric space approach uses explicit cost distortion function different sequences identical difference cost function certain threshold 
cost function identification threshold induce deterministic quantization input space smaller output space 
decided state problem explicitly language information theory powerful methods developed context putting ideas unified framework 

distortion measure mutual information engineering applications distortion function usually chosen fairly arbitrary fashion introduces structures original space preserved quantization 
avoid arbitrariness expect neural system reflecting pertinent structures sensory stimuli preserve reproduction 
choice distortion function determined informativeness quantization 
mutual information tells different states average distinguished observing quantize yn reproduction elements estimate yn mutual information reproduction yn 
information preservation criterion require choose quantizer preserves mutual information possible choose quantizer yn minimizes difference di yn yn note di 
functional di measure average distortion quality quantization 
interpreted information distortion measure appendix appendix symbol di term depends quantization yn reformulate problem maximization effective functional deff yn 
closely related method cost function :10.1.1.39.9882
reasons useful consider full functional 
may choose quantize stimulus case quantizer xn quantize stimulus response case quantizers 
versions parts information distortion relevant 
second reason average distortion rewritten expectation pointwise distortion function interesting form 
definition mutual information markov relation yn spaces express di appendix expectation di ep yn yn neural coding decoding yn kl yn kullback leibler kl directed divergence input stimulus conditioned response relative stimulus conditioned reproduction yn 
intuitively measures similar stimulus partition induced quantization partition induced sensory system 
expression appealing theoretical standpoint due various properties kl divergence 
example allows describe cases solution quantization problem exact representation coding scheme 
properties kl reproduction zero distortion achieved yn exactly case coding scheme described section 

implementations quantization deterministic stochastic output space appendix allows control exponential growth required data 
approach estimate quantity known lower bound actual mutual information 
obtain biased estimate control precision estimated 
theorems quantization theory appendix insure estimates quantized information quantities bounded original quantities refinement quantization lower estimates 
environment beneficial fix coarseness quantization size reproduction look quantization minimizes information distortion measure di yn described previously 

maximum entropy nonlinear information distortion constraint 
problem optimal quantization formulated class linear distortion functions maximum entropy problem :10.1.1.33.3047
analysis directly case distortion function depends nonlinearly quantizer 
maximum entropy formulation 
reasoning quantizers satisfy set constraints maximum entropy quantizer implicitly introduce restrictions problem 
framework minimum distortion problem posed maximum quantization entropy problem distortion constraint max yn yn constrained di yn yn yn ordinary constrained optimization problem solved numerically standard optimization tools 
cost function yn concave yn probability constraints yn yn linear yn 
constraint di concave yn theorem appendix problem concave maximization 
problem formulation relies knowing di depends mutual information easily avoid need effective distortion deff yn 
case optimization problem max yn yn constrained deff yn io yn yn dimitrov solution optimization problem depends single parameter io interpreted informativeness quantization 
io distortion constraint satisfied obtain unconstrained maximum entropy solution yn pairs yn 
io distortion constraint active uniform quantizer longer solution optimization problem 
convexity deff optimal solution lie boundary constraint carry yn io bits information 
formulation nice intuitive interpretation find maximum entropy quantizer carries io bits information 
continue pushing io informative solutions lower distortion reach point problem solutions 
max point best lower bound element reproduction 
solution continuous respect io values near max lower bounds 
choosing optimal quantizers near max achieve minimum distortion quantization 

maximum cost linear probability constraint 
standard approach constrained optimization problems lagrange multipliers 
system solved unconstrained optimization yn di yn max yn yn yn solution depends parameters constraints di yn yn yn function free parameter discard reformulate optimization problem finding maximum cost function max yn yn max yn di yn yn constrained yn 
yn equation shall continue discussion effective distortion deff 
case max yn yn max yn deff yn yn constrained yn 
yn formulation identical transferring nonlinear constraint cost function analyse problem 
consider behaviour cost function limiting cases :10.1.1.33.3047:10.1.1.33.3047
yn optimal solution unconstrained maximum entropy solution yn corresponds case io section 
limit deff solution optimization problem approaches maximum deff minimal di solution 
identical case io max 
order avoid divergence cost function rescale neural coding decoding extrema bounded 
intermediate values produce intermediate solutions connect limiting cases series bifurcations 
parameter meaning annealing parameter procedure general class distortion function named deterministic annealing drawing analogy physical annealing process :10.1.1.33.3047:10.1.1.33.3047

implicit solution optimal quantizer 
analysis problem uses simplicity linear constraint 
extrema setting derivatives respect quantizer yn zero 
subsequent steps shall explicitly assumption spaces finite discrete 
results continuous random variables easily adapted analogous methods calculus variations 
latin indices denote members original spaces greek indices elements reproduction yn 
mind appendix continue solve lagrange multiplier problem yj deff yk ln yk deff ln yk deff yk 
yk ln yk deff yk yk deff constraint requires yk deff yk deff yk 
substitute equation obtain implicit expression optimal yk yk deff yk 
deff yk note deff function quantizer yn 
deff di implicit solution terms information distortion di yn yn di 
practice expression iterated fixed value obtain solution optimization problem starting particular initial state 
small dimitrov bifurcation described occurs obvious initial condition uniform solution yn solution value initial condition subsequent value solutions continuous respect :10.1.1.33.3047

approximations codebook optimal information distortion procedure help resolve neural decoding problem outlined section 
limit distortion information loss identity transformation preserves structure output usually unavailable due lack data 
quantize neural response fixing size reproduction bounds estimate deff log bits 
ideal case max deff max yn log general lower 
hand bound yn 
log increases constant independent bounds intersect nc point adding elements yn improve distortion measure 
yn increases nc levels identify correct nc examining behaviour expected distortion equivalently deff yn function sufficient data 
take elements yn labels equivalence classes wanted find 
quantizer yn gives probability response belonging equivalence class yn 
rose conjectured optimal quantizer low distortions high deterministic effectively deterministic case duplicate classes :10.1.1.33.3047
case responses associated class yn yn yn 
optimal quantizer induces coding scheme yn yn yn 
informative approximation original relation 
induces quantization xn associating xn stimulus set xn yn correspond output class yn 
resulting relation yn xn bijective recover complete reproduction model described section 
data support complete recovery reduced data requirements algorithm earlier 
criterion case estimate deff change error bounds obtained analytically statistical re estimation methods bootstrap jack knife 
quantized mutual information log recover classes original classes combined 
quantizer may deterministic due lack data resolve uncertainties 
recover somewhat impoverished picture actual input output relationship refined automatically data available increasing repeating optimization procedure 

results shall discuss application method described far synthetic test cases 
applying physiological data sensory system involves additional difficulties associated estimates di complex input stimuli dealt 

random clusters analysis probability distribution shown 
model assume represents range possible stimulus properties represents range possible spike train patterns 
constructed clusters pairs stimulus response neural coding decoding 
joint probability relation random variables elements 
optimal quantizers yn different numbers classes 
panels represent conditional probability yn pattern horizontal axis belonging class yn vertical axis 
white represents zero black represents intermediate values represented levels grey 
behaviour mutual information increasing seen log linear plot 
dashed curve upper bound yn 
space 
cluster corresponds range responses elicited range stimuli 
model chosen resemble model coding decoding jointly typical sequences section 
mutual information sequences bits comparable mutual information conveyed single neurons stimulus parameters unrelated biological sensory systems 
analysis assume original relation known joint probability explicitly 
results application approach shown panels 
grey scale map representations quantizer depicts zero white black intermediate values levels grey 
class reproduction forced algorithm recovers incomplete representation 
representation improved class refinement 
refinement separates classes correctly recovers mutual information 
refinements fail split classes effectively identical note classes evenly populated class membership close uniform half 
quantized mutual information increases number classes approximately log recovers original mutual information point levels 
details course optimization procedure lead optimal quantizer panel 
behaviour deff function annealing parameter seen top panel 
snapshots optimal quantizers different values bottom row panels 
observe bifurcations optimal solution corresponding transitions effective distortion 
abrupt transitions similar ones described linear distortion function :10.1.1.33.3047
observe transitions appear smooth deff solution optimal quantizer undergoes bifurcation 
random permutation rows columns joint probability channel structure 
quantization identical case applying inverse permutation fully recovers permuted classes quantization contravariant respect action permutation group 
bits dimitrov bits 
hamming code 
behaviour deff top optimal quantizer yn bottom function annealing parameter 
exist noise correcting codes transform blocks symbols input larger blocks binary symbols way original block recovered noise perturbs codewords 
example going simple noise correcting code hamming code 
operates binary blocks size expands block binary symbols adding parity bits 
blocks considered vectors boolean vector space 
input output hamming code described linear operator spaces addition modulo 
code detect correct error flipped bit block 
properties known 
going system testbed evaluate performance different implementations algorithms described earlier 
case data sampled joint probability perform estimates 
variables related manner generated sequence points uniform source example applied 
independent copies result perturbed noise flipped random bit sample 
perturbed outputs xo yo 
relation shown 
bits mutual information xo yo 
permutation rows columns orders equivalent sequences relation easier comprehend 
variables permuted versions xo neural coding decoding 
joint probability xo yo original variables 

joint probability permuting rows columns optimal quantizations different number classes 
behaviour mutual information increasing seen log linear plot 
dashed curve 
yo relation 
mentioned section permutation variables leaves information distortion invariant 
results xo yo equivalent 
results seen 
reproduction classes permuted follow roughly relation 
section algorithm recovers incomplete representations refinement previous 
refinements correct number clusters fail improve distortion classes effectively copies 
quantized mutual information increases number classes approximately log recovers original mutual information point levels 
bits dimitrov 
linear encoding 
joint probability linear relation random variables optimal quantization different number classes 
behaviour mutual information increasing seen 
dashed curve 
applied algorithm case previous cases clearly defined clusters 
model tries simulate process physical measurement represents physical system set possible results measurement 
example model linear relation gaussian measurement noise kx drawn normal distribution zero mean variance particular relation contains bits mutual information 
results seen 
algorithm recovers series representations quantizer refinement previous 
reproduction classes permuted roughly follow original linear relation 
natural stopping point quantized mutual information yn approaches original mutual information gradually 
contrast previous cases rate change yn abruptly decreased course bifurcations differs previous cases 
reproduction classes permuted roughly follow original linear relation 
obvious abrupt transitions uncertainty quantizer resolved smoothly 

discussion goals 
goal formulate precise model early stage sensory system communication channel describe properties model 
general communication channel fully described conditional probability response stimulus 
ideas information theory optimal information transmission presence noise method jointly typical sequences demonstrated existence equivalence classes stimulus response pairs called codeword classes 
coding scheme system described deterministic map bits neural coding decoding bits 
behaviour deff top optimal quantizer yn bottom function annealing parameter linear encoding case 
restricted codeword classes 
number codeword classes related mutual information stimulus response 
second goal provide method recovering structure model observations 
characterizing relation individual elements stimulus response spaces shown require large numbers data points increasing exponentially length spike sequences number neurons considered 
choose recover impoverished description coding scheme quantizing responses reproduction set elements 
assess quality reproduction define information distortion di yn measures information lost quantization process 
fixed reproduction size pose optimization problem finding quantization smallest distortion preserves information original relation refining reproduction increasing shown decrease distortion 
demonstrate empirically set synthetic problems original relation contains disjoint clusters sufficiently fine optimal quantization recovers completely 
quantization coarse clusters combined way large fraction original information preserved 
realistic cases physiological recordings usually data support sufficiently fine quantization 
cases forced accept coarse quantization recover structure particular neural coding scheme 
criterion adopted stopping refinement process estimate information distortion change error bounds may obtained analytically statistical re estimation procedures 
coding schemes currently seen special cases method 
rate code described deterministic quantization set integers 
quantizer assigns spike patterns number spikes dimitrov equivalence class 
spike latency code seen quantization classes determined latency jitter spike timing 
stimulus feature decoded rate code case latency class spike falls 
metric space approach uses explicit cost distortion function determine different sequences identical equivalent cost function difference certain threshold 
cost function identification threshold induce deterministic quantization input space smaller output space 
current approaches studying neural coding rely formulating hypothesis coding scheme neural system may observations estimate parameters hypothesis 
complexity hypothesis determines amount data needed reliable estimates necessary parameters 
method report offers means data driven hypothesis formulation 
refinement reproduction due lack data effectively formulate hypothesis informative coding scheme supported available amount data 
observations available hypothesis refined automatically include better approximation 
applications information theory problems neural coding concentrate estimating information theoretic quantities regard actual stimulus response relationship produced 
strong point approach require detailed understanding coding process weak point perspective 
mentioned model coding scheme candidate hypotheses investigated 
example basic elements pair events combination rate latency codes 
approach described characterize coding schemes underlying physical processes produce 
codeword classes emerge analysis described simple mechanism ability analyse describe quite succinctly coding schemes considered far 
developed information distortion method purely practical necessity need describe neural coding scheme large input output spaces 
earlier research estimate just bits mutual information neuron distinct sensory systems 
view model coding scheme suggests just codeword classes need identified regardless size response space 
natural devise method clustered neural representation large sets preserving mutual information 
quite excited find developed approach suggested similar method information bottleneck completely reasons authors attempt extract meaningful relevant information pair interacting systems :10.1.1.39.9882:10.1.1.39.9882
different motivations obvious 
explicitly concentrate case finite albeit large spaces method applicable computer recorded data numerical analysis 
information bottleneck method uses variational approach continuous random variables better suited analysis :10.1.1.39.9882
assess applicability actual stimulus response datasets examples performance :10.1.1.39.9882
application cited suffers unfortunate choices 
applies method problem albeit real understood distinguish limitations algorithm constraints problem 
second chooses forgo simplicity offered small reproduction space probabilistic clustering uses ad hoc deterministic clustering method find approximation solution problem assessing properties neural coding decoding method difficult :10.1.1.33.3047
hope developments approaches eventually converge unified method data analysis 
information distortion di extremely suited uncovering structure information channels 
appendix addresses properties object research needed elucidate 
concave second derivatives continuous boundaries domain amenable various analytical techniques help clarify structure optimal solution bifurcations different values distortion 
scope 
interesting note neural coding mind developing information distortion method ensuing analysis way limited nervous systems 
constraints pair signals analyse general represent pair interacting physical systems 
case finding minimal information distortion reproduction allows recover certain aspects interaction physical systems may improve considerably subsequent analysis performed 
possible analyse parts structure single physical system ifx system known properties signal generator controlled researcher perturb cases point exciting possibility obtaining automated approach succinct descriptions arbitrary physical systems minimal information distortion quantizers 
acknowledgments research supported part nih mh mh jpm 
authors indebted university tom kay kirkpatrick montana state university anonymous reviewers numerous discussions suggestions helped shape 
appendix information theory goal section summarize results information theory pertinent body 
statements standard results precision interested reader trace foundation discussion coding quantization earlier 
extended treatment topics 
formal treatment subject approached greater care mathematical precision 
appendix 
source space probability measure random variables information source mathematical model physical system produces succession symbols manner unknown treated random 
set containing possible output symbols called alphabet source 
field subsets treatment events sets sequences symbols achieved assigning probability measure pr 
triplet called random variable context alphabet measure clear 
cases alphabets define probability measure product space context induced probability measures individual spaces called marginal measures marginals 
important measure dimitrov emerges setup conditional probability defined random variables called independent measurable function defined values measurable space mapping property defined usually measurable function linear vector space rn perform integration functional space measurable functions 
probability theory called expectation measurable function defined epg sum replaced integral continuous alphabets 
appendix 
information theoretic quantities basic concepts information theory entropy mutual information 
notion mutual information introduced integral measure degree dependence pair variables 
concept entropy understood self information random variable 
special cases general integral quantity called relative entropy kl divergence integral measure difference probability distributions 
mutual information defined discrete continuous alphabets entropy continuous alphabets problematic quantity undefined measures interest 
fortunately identities bounds mutual information valid uses integral measure probability kl divergence relative entropy probability measures event space kl different probability measures kl quantifies difference probability measures sample space extensively probabilistic decision theory 
note quantities kl depends probability measures elements space non numeric 
expectations defined irrespective structure event space 
usual definitions quantities base logarithm log function implicitly assume base 
rare occasions shall natural logarithm denoted symbol ln 
appendix 
mutual information 
order measure statistical independence random variables useful introduce notion mutual information 
kl distance joint probability product alphabet product marginal probabilities 
equal zero independent kl ep log 
mutual information symmetric respect arguments 
neural coding decoding appendix 
entropy 
measure self information probability distribution entropy ep log 
communication theory measures average information symbol carries sampled 
joint entropy pair random variables defined ep log 
define conditional entropy random variable expected value entropies conditional distributions ep ep log 
appendix 
information identities 
various identities connecting 
short list frequently 
xn xi xi xn xn appendix 
typical sequences xi xi 
theorem appendix asymptotic equipartition property aep 
xn independent identically distributed random variables probability measure log xn probability 
theorem simple consequence weak law large numbers 
proof theorems consult 
extension arbitrary ergodic finite valued processes known shannon mcmillan breiman theorem 
definitions theorems section form general correct ergodic sources 
aep allows define structure set events definition 
typical set respect set sequences xn property xn 
typical set properties 
dimitrov theorem appendix properties typical sequences 
xn log xn 
ii pr sufficiently large 
iii sufficiently large 
number elements set appendix 
jointly typical sequences analysing information channels deal sets random sequences input output 
case necessary consider combined behaviour pair 
definition 
set jointly typical sequences xn yn respect distribution set xn log log log xn yn xi yi 
theorem appendix properties jointly typical sequences 
pair sequences length hold 
pr 
ii sufficiently large 
iii pair random variables joint probability xn yn xn yn independent marginal distributions xn sufficiently large pr properties jointly typical sequences prove principal theorems information theory channel coding theorem 
lack space 
just state important result 
complete proof techniques 
definition 
discrete channel system consisting input source output source transition probability observing output symbol sent 
channel memoryless transition probability conditionally independent previous channel inputs outputs 
definition 
channel capacity discrete memoryless channel max 
theorem appendix channel coding theorem 
channel capacity rate exists sequence codes rate maximum probability error 
conversely sequence codes neural coding decoding appendix 
distortion theory appendix 
data processing inequality 
data processing inequality proofs statements shall take time give proper definitions proofs 
definition 
random variables form markov chain denoted conditional distribution depends conditionally independent case joint probability written 
particular deterministic function note 
theorem appendix data processing inequality 

proof 
chain rule item expand mutual information different ways 

independent implies 
hand 
particular 
inequality physical measurement deterministic usually decrease information carried random variable 
appendix 
quantization 
random variable related random variable yn process quantization 
yn referred reproduction quantizers deterministic functions stochastic conditional probability 
size reproduction space smaller size quantized space 
deterministic quantizer just quantizer referred hard clustering simple measurable function xf reproduction space xf finitely elements xi quantizer induces partition qi qi ii qi qf iii qi information quantities reproduction xf xf ep xf log xf ep qf log qf xf yg ep xf yg log xf yg xf yg ep qf qg log qf qg :10.1.1.33.3047
qf qg quantizer refines partition qh induced refines partition qf induced qh refines qf qh qf qh subset dimitrov qf note case xh xf form certain markov chain original space 
stochastic quantizer soft clustering mapping probability measure space xq :10.1.1.33.3047
mapping conditional probability xq interpreted probability belonging reproduction class xq 
source space induces probability measure reproduction space xq xq 
stochastic quantizer considered communication channel 
deterministic quantizer discussed special case stochastic quantizer xq ifx xq zero 
information quantities xq xq ep xq log xq xq yg ep xq yg log xq yg 
xq yg deterministic case nice inverse image quantizer map define refinements usual way 
choose different property quantizer refinement quantizer refines ifx xh xf xf upstream xh markov chain 
properties quantizers deterministic stochastic useful discussing information transmission 
markov relation theorem appendix follows xh xf xh xf 
discrete continuous quantizer xf estimates quantized spaces lower bounds actual information quantities 
continuous xf diverges refinements 
hand obtained upper bound refinements 
constraints achieved deterministic quantizer :10.1.1.33.3047
statements suggest quantizations provide lower bound estimates controlled precision size pattern set fixed size quantized space potentially lower size original pattern space 
allows obtain precise estimates quantities question 
appendix 
distortion theory 
quality quantization topic distortion theory 
characterized distortion function xq distortion function xq measures represented xq 
general arbitrary 
expected distortion xq eq xq xq measures quality quantization quantizer xq 
definition 
rate distortion function source reproduction xq distortion xq defined min xq xq xq 
neural coding decoding minimization stochastic quantizers xp satisfy distortion constraint 
solution standard minimization problem convex function xq convex set xq xq xq xq xq xp xp xp 
analytically solved lagrange multipliers minimize xq xq xq xq find constraints 
appendix properties information distortion function appendix 
definition average information distortion di random variable reproduction yn defined random variable related yn form markov chain 
case define information distortion yn di yn yn 
part depends quantizer yn yn concentrate properties deff yn 
appendix 
properties di bounded function quantizer 
di yn implies di 
yn di expected distortion integral characteristic relation sets yn 
write form includes explicitly expectation pointwise distortion function 
yn yn yn yn wehave di yn log yn yn log yn yn yn log log yn yn yn yn log yn yn kl yn 
yn uses bayes property log common parts cancels 
step uses markov property yn yn 
shows information distortion di ep yn kl yn expectation kl divergence respect yn 
pointwise distortion functions usually investigated information theory depends quantizer yn thorough yn 
xq dimitrov process investigating information distortion function may evaluate values yn conditional probabilities 
requirement want impose non negative yn 
reason need define certain relations automatically true yn conditional probability 
shall probability function 
mind 
define yn yn yn yn yn yn yn 
automatically true yn conditional probability 
yn conditional probability probabilities non negative 
yn yn yn conditional probability 
case yn yn bayes property definitions 
definitions natural extensions information quantities arbitrary non negative yn yn yn yn log yn yn yn yn log yn 
yn lemma appendix convexity di 
information distortion function di concave function yn 
proof 
consider di yn 
term constant respect yn consider second term 
definition see yn function yn 
fixed yn convex function yn shown yn probability easily extensible yn 
yn yn yn yn 
need show convex function yn 
yn yn 
consider yn yn yn yn yn yn yn yn neural coding decoding yn yn yn yn yn yn yn yn yn yn follows consequence convexity 
finishes proof yn convex function yn 
di yn di concave function quantizer yn 
appendix 
derivatives di function quantizer yn 
effective distortion deff yn derivatives respect quantizer di opposite sign shall consider derivatives deff 
section shall explicitly assumption spaces finite discrete 
fact yn conditional probability 
final results ready programming computer finite dimensional analysis 
results continuous random variables easily adapted analogous methods calculus variations 
latin indices denote members original spaces greek indices elements reproduction yn 
differentiating shall natural logarithm ln definitions rescale results base logarithm log 
terms quantizer deff xi ln xi xi xi yj xi yj yj yj 
beneficial calculate derivatives attempting deff 
fact yj yk jk 
derivatives xi xi yk yk yk 
yk deff deff yk xi ln yk xi xi dimitrov xi xi ln xi ln xi ln yk xi yk xi yk ln xi xi yk yk xi xi xi yk xi yk ln xi xi xi yk yk xi deff xi yk ln xi 
xi second derivatives deff yl yk yl xi yk ln xi xi xi yk ln xi ln yl xi yl yl xi yk xi deff xi yk xi yl yl yk xi yk yl 
cases assume relevant quantities absolutely continuous respect divisions performed 
practice means optimization restricted away zero gradients diverge 
deterministic quantizer hard clustering required optimization brought close boundary yn yn thresholded obtain deterministic map 
posing optimization problem section encounter functional yn di analyse need derivatives yn yn yk yj yj ln yj yk ln yj yj jk yk ln yk 
second derivatives yn yl yk yl yk ln yk yk yk kl 
obtain results information quantities measured bits derivatives divided ln 
neural coding decoding adrian basis sensation action sense organs new york norton atick information theory provide ecological theory sensory processing 
network comput 
neural syst 
barlow possible principles underlying transformation sensory messages sensory communications ed cambridge ma mit press berry firing events fundamental symbols neural code retinal ganglion cells 
computational neuroscience trends research ed bower amsterdam elsevier brenner strong bialek de van synergy neural code neural comput 
cover thomas elements information theory wiley series communication new york wiley de van strong bialek reproducibility variability neural spike trains science dimitrov aspects cortical information processing phd thesis university chicago dimitrov miller natural time scales neural encoding neurocomputing dimitrov miller spike pattern coding schemes cricket cercal sensory system computational neuroscience trends research ed bower published dimitrov miller non uniform quantization neural spike sequences information distortion measure computational neuroscience trends research ed bower amsterdam elsevier georgopoulos schwartz kettner neuronal population coding movement direction science gersho gray vector quantization signal compression dordrecht kluwer gray entropy information theory berlin springer hubel wiesel receptive fields binocular interaction functional architecture cat visual cortex physiol 
lond 
jaynes rationale maximum entropy methods proc 
ieee johnson information theoretic analysis neural code comput 
neurosci 
hertz richmond decoding cortical neuronal signals network models information estimation spatial tuning comput 
neurosci 
koch segev ed methods neuronal modeling cambridge ma mit press kullback information theory statistics new york wiley berry neural code retina neuron oram wiener richmond stochastic nature precisely timed spike patterns visual system neural responses neurophysiol 
schultz treves rolls correlations encoding information nervous system proc 
soc 
sirovich global dimensionality face space proc 
th int 
conf 
automatic face gesture recognition grenoble los alamitos ca ieee computer society press pp reid temporal coding visual information thalamus neurosci 
rieke de van bialek spikes exploring neural code cambridge ma mit press rose mapping approach rate distortion computation analysis ieee trans 
inform 
theory rose annealing clustering compression classification regression related optimization problems proc :10.1.1.33.3047
ieee abbott vector reconstruction firing rates comput 
neurosci 
schultz temporal correlations neural spike train entropy phys 
rev lett 
shannon mathematical theory communication bell syst 
tech 
slonim tishby agglomerative information bottleneck advances neural information processing systems vol ed solla leen ller cambridge ma mit press pp strong de van bialek entropy information neural spike trains phys 
rev lett 
theunissen miller temporal encoding nervous systems rigorous definition comput 
neurosci 
dimitrov theunissen miller information theoretic analysis dynamical encoding primary sensory interneurons cricket cercal system neurophysiol 
theunissen miller representation sensory information cricket cercal sensory system 
ii 
information theoretic calculation system accuracy optimal tuning curve widths primary interneurons neurophysiol 
tishby pereira bialek information bottleneck method lanl preprint arxiv org abs physics treves upward bias measures information derived limited data samples neural comput :10.1.1.39.9882
victor brain uses time represent process visual information brain res 
victor metric space analysis spike trains theory algorithms application network comput 
neural syst 
reading spikes real time processing neural systems phd thesis university california berkeley 
