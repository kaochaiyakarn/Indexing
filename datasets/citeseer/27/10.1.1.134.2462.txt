journal arti cial intelligence research submitted published reinforcement learning survey leslie pack kaelbling cs brown edu michael littman cs brown edu computer science department box brown university providence ri usa andrew moore cs cmu edu smith hall carnegie mellon university forbes avenue pittsburgh pa usa surveys eld reinforcement learning computer science perspective 
written accessible researchers familiar machine learning 
historical basis eld broad selection current summarized 
reinforcement learning problem faced agent learns behavior trial error interactions dynamic environment 
described resemblance psychology di ers considerably details word reinforcement 
discusses central issues reinforcement learning including trading exploration exploitation establishing foundations eld markov decision theory learning delayed reinforcement constructing empirical models accelerate learning making generalization hierarchy coping hidden state 
concludes survey implemented systems assessment practical utility current methods reinforcement learning 

reinforcement learning dates back early days cybernetics statistics psychology neuroscience computer science 
attracted rapidly increasing interest machine learning arti cial intelligence communities 
promise way programming agents reward punishment needing specify task achieved 
formidable computational obstacles ful lling promise 
surveys historical basis reinforcement learning current computer science perspective 
give high level overview eld taste speci approaches 
course impossible mention important eld taken exhaustive account 
reinforcement learning problem faced agent learn behavior trial error interactions dynamic environment 
described strong family resemblance psychology di ers considerably details word reinforcement 
appropriately thought class problems set techniques 
main strategies solving reinforcement learning problems 
rst search space behaviors order nd performs environment 
approach taken genetic algorithms genetic programming ai access foundation morgan kaufmann publishers 
rights reserved 
kaelbling littman moore standard reinforcement learning model 
novel search techniques schmidhuber 
second statistical techniques dynamic programming methods estimate utility actions states world 
devoted entirely second set techniques take advantage special structure reinforcement learning problems available optimization problems general 
clear set approaches best circumstances 
rest section devoted establishing notation describing basic reinforcement learning model 
section explains trade exploration exploitation presents solutions basic case reinforcement learning problems maximize immediate reward 
section considers general problem rewards delayed time actions crucial gaining 
section considers classic model free algorithms reinforcement learning delayed reward adaptive heuristic critic td learning 
section demonstrates continuum algorithms sensitive amount computation agent perform actual steps action environment 
generalization cornerstone mainstream machine learning research potential considerably aiding reinforcement learning described section 
section considers problems arise agent complete perceptual access state environment 
section catalogs reinforcement learning successful applications 
section concludes speculations important open problems reinforcement learning 
reinforcement learning model standard reinforcement learning model agent connected environment perception action depicted 
agent receives input indication current state environment agent chooses action generate output 
action changes state environment value state transition communicated agent scalar reinforcement signal behavior choose actions tend increase long run sum values reinforcement signal 
learn time systematic trial error guided algorithms subject sections 
formally model consists discrete set environment states discrete set agent actions reinforcement learning survey set scalar reinforcement signals typically real numbers 
gure includes input function determines agent views environment state assume identity function agent perceives exact state environment consider partial observability section 
intuitive way understand relation agent environment example dialogue 
environment state 
possible actions 
agent ll take action 
environment received reinforcement units 
state 
possible actions 
agent ll take action 
environment received reinforcement units 
state 
possible actions 
agent ll take action 
environment received reinforcement units 
state 
possible actions 
agent job nd policy mapping states actions maximizes long run measure reinforcement 
expect general environment non deterministic action state di erent occasions may result di erent states di erent reinforcement values 
happens example state applying action produces di ering reinforcements di ering states occasions 
assume environment stationary probabilities making state transitions receiving speci reinforcement signals change time 
reinforcement learning di ers widely studied problem supervised learning ways 
important di erence presentation input output pairs 
choosing action agent told immediate reward subsequent state told action best long term interests 
necessary agent gather useful experience possible system states actions transitions rewards actively act optimally 
di erence supervised learning line performance important evaluation system concurrent learning 

assumption may disappointing operation non stationary environments motivations building learning systems 
fact algorithms described sections ective varying non stationary environments little theoretical analysis area 
kaelbling littman moore aspects reinforcement learning closely related search planning issues arti cial intelligence 
ai search algorithms generate satisfactory trajectory graph states 
planning operates similar manner typically construct complexity graph states represented compositions logical expressions atomic symbols 
ai algorithms general reinforcement learning methods require prede ned model state transitions exceptions assume determinism 
hand reinforcement learning kind discrete cases theory developed assumes entire state space enumerated stored memory assumption conventional search algorithms tied 
models optimal behavior start thinking algorithms learning behave optimally wehave decide model optimality 
particular specify agent take account decisions behave 
models subject majority ofwork area 
nite horizon model easiest think moment time agent optimize expected reward steps hx need worry happen 
subsequent expressions represents scalar reward received steps 
model ways 
rst agent non stationary policy changes time 
rst step take termed step optimal action 
de ned best action available steps remaining gain reinforcement 
step take step optimal action nally takes step optimal action terminates 
second agent receding horizon control takes step optimal action 
agent acts policy value limits far ahead looks choosing actions 
nite horizon model appropriate 
cases may know precise length agent life advance 
nite horizon discounted model takes long run reward agent account rewards received geometrically discounted discount factor rt interpret ways 
seen interest rate probability living step mathematical trick bound nite sum 
model conceptually similar receding horizon control discounted model mathematically tractable nite horizon model 
dominant reason wide attention model received 
reinforcement learning survey optimality criterion average reward model agent supposed take actions optimize long run average reward lim hx policy referred gain optimal policy seen limiting case nite horizon discounted model discount factor approaches bertsekas 
problem criterion way distinguish policies gains large amount initial phases 
reward gained initial pre agent life overshadowed long run average performance 
possible generalize model takes account long run average amount initial reward gained 
generalized bias optimal model policy preferred maximizes long run average ties broken initial extra reward 
contrasts models optimality providing environment changing model optimality changes optimal policy 
example circles represent states environment arrows state transitions 
single action choice state start state upper left marked incoming arrow 
rewards zero marked 
nite horizon model actions yield rewards rst action chosen nite horizon discounted model choices yield second action chosen average reward model third action chosen leads average reward 
change second action optimal nite horizon model rst nite horizon discounted model average reward model prefer best long term average 
choice optimality model parameters matters important carefully application 
nite horizon model appropriate agent lifetime known important aspect model length remaining lifetime decreases agent policy may change 
system hard deadline appropriately modeled way 
relative usefulness nite horizon discounted bias optimal models debate 
bias optimality advantage requiring discount parameter algorithms nding bias optimal policies understood nding optimal nite horizon discounted policies 
measuring learning performance criteria previous section assess policies learned algorithm 
able evaluate quality learning 
incompatible measures 
eventual convergence optimal 
algorithms come provable guarantee asymptotic convergence optimal behavior watkins dayan 
reassuring useless practical terms 
agent quickly reaches plateau kaelbling littman moore finite horizon infinite horizon average reward comparing models optimality 
unlabeled arrows produce reward zero 
optimality may applications preferable agent guarantee eventual optimality sluggish early learning rate 
speed convergence optimality 
optimality usually asymptotic result convergence speed ill de ned measure 
practical speed convergence optimality 
measure begs de nition near optimality su cient 
related measure level performance time similarly requires de ne time 
noted wehave di erence conventional supervised learning 
expected predictive accuracy statistical ciency prime concerns 
example known pac framework valiant learning period mistakes count performance period 
framework provides bounds necessary length learning period order probabilistic guarantee subsequent performance 
usually inappropriate view agent long existence complex environment 
spite mismatch embedded reinforcement learning train test perspective provides pac analysis learning described section sheds light connection views 
measures related speed learning additional weakness 
algorithm merely tries achieve optimality fast possible may incur unnecessarily large penalties learning period 
aggressive strategy longer achieve optimality gaining greater total reinforcement learning preferable 
regret 
appropriate measure expected decrease reward gained due executing learning algorithm behaving optimally 
measure known regret berry 
penalizes mistakes occur run 
unfortunately results concerning regret algorithms quite hard obtain 
reinforcement learning survey reinforcement learning adaptive control adaptive control graham concerned algorithms improving sequence decisions experience 
adaptive control mature discipline concerns dynamic systems states actions vectors system dynamics smooth linear locally linearizable desired trajectory 
common formulation cost functions adaptive control quadratic penalties deviation desired state action vectors 
importantly dynamic model system known advance estimated data structure dynamic model xed leaving model estimation parameter estimation problem 
assumptions permit deep elegant powerful mathematical analysis turn lead robust practical widely deployed adaptive control algorithms 

exploitation versus exploration single state case major di erence reinforcement learning supervised learning reinforcement learner explicitly explore environment 
order highlight problems exploration treat simple case section 
fundamental issues approaches described cases transfer complex instances reinforcement learning discussed 
simplest possible reinforcement learning problem known armed bandit problem subject great deal study statistics applied mathematics literature berry 
agent room collection gambling machines called armed bandit colloquial english 
permitted xed number pulls pulled turn 
machines require deposit play cost wasting pull playing suboptimal machine 
arm pulled machine pays underlying probability parameter payo independent events unknown 
agent strategy 
problem illustrates fundamental tradeo exploitation exploration 
agent believe particular arm fairly high payo probability choose arm time choose information worse 
answers questions depend long agent expected play game longer game lasts worse consequences prematurely converging sub optimal arm agent explore 
wide variety solutions problem 
consider representative selection deeper discussion number important theoretical results see book 
term action indicate agent choice arm pull 
eases transition delayed reinforcement models section 
important note bandit problems de nition reinforcement learning environment single state self transitions 
section discusses solutions basic state bandit problem formal correctness results 
extended problems real valued rewards apply directly general multi state delayed reinforcement case 
kaelbling littman moore section presents techniques formally justi ed wide practice applied similar lack guarantee general case 
formally justi ed techniques fairly developed formal theory exploration simple problems 
instructive methods provides scale complex problems 
dynamic programming approach agent going acting total steps basic bayesian reasoning solve optimal strategy berry 
requires assumed prior joint distribution parameters natural pi independently uniformly distributed 
compute mapping belief states summaries agent experiences run actions 
belief state represented tabulation action choices payo fn nk denotes state play pulled ni times wi payo write nk wk expected payo remaining total pulls available remaining pulls optimally 
ni remaining pulls nk wk 
basis recursive de nition 
value belief states pulls remaining compute value belief state pulls remaining payo agent takes action nk wk maxi acts optimally remaining pulls max iv posterior subjective probability action paying prior probability 
uniform priors result beta distribution 
expense lling table values way attainable belief states linear number belief states times actions exponential horizon 
gittins allocation indices gittins gives allocation index method nding optimal choice action step armed bandit problems gittins 
technique applies discounted expected reward criterion 
action consider number times chosen versus number times paid certain discount factors published tables index values look index value 
represents comparative measure combined value expected payo action history payo value information get choosing 
gittins shown choosing action largest index value guarantees optimal balance exploration exploitation 
reinforcement learning survey automaton states 
top row shows state transitions previous action resulted reward bottom row shows transitions reward 
states left half gure action right action taken 
guarantee optimal exploration simplicity technique table index values approach holds great deal promise complex applications 
method proved useful application robotic manipulation immediate reward ungar 
unfortunately able nd analog index values delayed reinforcement problems 
learning automata branch theory adaptive control devoted learning automata surveyed narendra originally described explicitly nite state automata 
automaton shown provides example solves armed bandit arbitrarily near optimally approaches nity 
inconvenient describe algorithms nite state automata move describe internal state agent probability distribution actions chosen 
probabilities di erent actions adjusted previous successes failures 
example stands set algorithms independently developed mathematical psychology literature bower linear reward inaction algorithm 
agent probability action action succeeds action fails remains unchanged 
algorithm converges probability containing single rest choosing particular action probability 
unfortunately converge correct action probability converges wrong arbitrarily small making small narendra 
literature regret algorithm 
ad hoc techniques kaelbling littman moore reinforcement learning practice simple ad hoc strategies popular 
rarely best choice models optimality wehave may viewed reasonable computationally tractable heuristics 
thrun surveyed avariety techniques 
greedy strategies rst strategy comes mind choose action highest estimated payo aw early unlucky sampling indicate best action reward reward obtained suboptimal action 
suboptimal action picked leaving true optimal action starved data superiority discovered 
agent explore ameliorate outcome 
useful heuristic optimism face actions selected greedily strongly optimistic prior beliefs put payo strong negative evidence needed eliminate action consideration 
measurable danger starving optimal unlucky action risk arbitrarily small 
techniques reinforcement learning algorithms including interval exploration method kaelbling described shortly exploration bonus dyna sutton curiosity driven exploration schmidhuber exploration mechanism prioritized sweeping moore atkeson :10.1.1.134.8196
randomized strategies simple exploration strategy take action best estimated expected reward default probability choose action random 
versions strategy start large value encourage initial exploration slowly decreased 
objection simple strategy experiments non greedy action try promising alternative clearly hopeless alternative 
slightly sophisticated strategy boltzmann exploration 
case expected reward action er action probabilistically distribution er er temperature parameter decreased time decrease exploration 
method works best action separated su ers somewhat values actions close 
may converge unnecessarily slowly temperature schedule manually tuned great care 
interval techniques exploration cient second order information certainty estimated values actions 
kaelbling interval estimation algorithm stores statistics actiona number successes number trials 
action chosen computing upper bound reinforcement learning survey con dence interval success probability action highest upper bound 
smaller values parameter encourage greater exploration 
payo boolean normal approximation binomial distribution construct con dence interval binomial small 
payo distributions handled associated statistics nonparametric methods 
method works empirical trials 
related certain class statistical techniques known experiment design methods box draper comparing multiple treatments example drugs determine treatment best small set experiments possible 
general problems multiple states reinforcement immediate solutions replicated state 
generalization required solutions integrated generalization methods see section straightforward simple ad hoc methods understood maintain theoretical guarantees 
techniques focus converging regime exploratory actions taken rarely appropriate environment stationary 
environment non stationary exploration continue take place order notice changes world 
ad hoc techniques modi ed deal plausible manner keep temperature parameters going decay statistics interval estimation theoretically guaranteed methods applied 

delayed reward general case reinforcement learning problem agent actions determine immediate reward probabilistically state environment 
environments thought bandit problems agent take account state immediate reward decides 
model long run optimality agent determines exactly value account 
agent tobe able learn delayed reinforcement may take long sequence actions receiving insigni cant reinforcement nally arrive state high reinforcement 
agent able learn actions desirable reward take place arbitrarily far 
markov decision processes problems delayed modeled markov decision processes mdps 
mdp consists set states set actions reward function kaelbling littman moore state transition function 
probability distribution set maps states probabilities 
probability making transition state state action state transition function probabilistically speci es state environment function current state agent action 
reward function speci es expected instantaneous reward function current state action 
model markov state transitions independent ofany previous environment states agent actions 
mdp models bellman bertsekas howard puterman 
general mdps mayhave nite uncountable state action spaces discuss methods solving nite state nite action problems 
section discuss methods solving problems continuous input output spaces 
finding policy model consider algorithms learning behave environments explore techniques determining optimal policy correct model 
dynamic programming techniques serve foundation inspiration learning algorithms follow 
restrict attention mainly nding optimal policies nite horizon discounted model algorithms analogs average case models 
rely result nite horizon discounted model exists optimal deterministic stationary policy bellman 
speak optimal value state expected nite discounted sum reward agent gain starts state executes optimal policy 
complete decision policy written optimal value function unique de ned solution simultaneous equations max assert value state expected instantaneous reward plus expected discounted value state best available action 
optimal value function specify optimal policy value iteration arg max rt way nd optimal policy nd optimal value function 
determined simple iterative algorithm called value iteration shown converge correct values bellman bertsekas 
reinforcement learning survey initialize arbitrarily loop policy loop loop max loop loop obvious value iteration algorithm 
important result bounds performance current greedy policy function bellman residual function williams baird 
says maximum di erence successive value functions value greedy policy policy obtained choosing state action maximizes estimated discounted reward current estimate value function di ers value function optimal policy state 
provides ective stopping criterion algorithm 
puterman discusses stopping criterion span semi norm may result earlier termination 
important result greedy policy guaranteed optimal nite number steps value function may converged bertsekas 
practice greedy policy optimal long value function converged 
value iteration exible 
assignments need done strict order shown occur asynchronously parallel provided value state gets updated nitely nite run 
issues treated extensively bertsekas proves convergence results 
updates equation known full backups information possible successor states 
shown updates form max long pairing updated nitely sampled distribution sampled mean bounded variance learning rate decreased slowly 
type sample backup singh critical operation model free methods discussed section 
computational complexity value iteration algorithm full backups iteration quadratic number states linear number actions 
commonly transition probabilities sparse 
average constant number states non zero probability cost iteration linear number states linear number actions 
number iterations required reach optimal value function polynomial number states magnitude largest reward discount factor held constant 
worst case number iterations grows polynomially convergence rate slows considerably discount factor approaches littman dean kaelbling :10.1.1.108.2266
policy iteration kaelbling littman moore policy iteration algorithm manipulates policy directly nding indirectly optimal value function 
operates follows choose arbitrary policy loop compute value function policy solve linear equations improve policy state arg maxa value function policy just expected nite discounted reward gained state executing policy 
determined solving set linear equations 
know value state current policy consider value improved changing rst action taken 
change policy take new action situation 
step guaranteed strictly improve performance policy 
improvements possible policy guaranteed optimal 
jaj jsj distinct policies sequence policies improves step algorithm terminates exponential number iterations puterman 
important open question iterations policy iteration takes worst case 
known running time pseudopolynomial xed discount factor polynomial bound total size mdp littman :10.1.1.108.2266
enhancement value iteration policy iteration practice value iteration faster iteration policy iteration takes fewer iterations 
arguments put forth ect approach large problems 
puterman modi ed policy iteration algorithm puterman shin provides method trading iteration time iteration improvement smoother way 
basic idea expensive part policy iteration solving exact value nding exact value perform steps modi ed value iteration step policy held xed successive iterations 
shown produce approximation converges linearly practice result substantial speedups 
standard numerical analysis techniques speed convergence dynamic programming accelerate value policy iteration 
multigrid methods quickly seed initial approximation high resolution value function initially performing value iteration coarser resolution 
state aggregation works collapsing groups states single meta state solving abstracted problem bertsekas casta non 
computational complexity reinforcement learning survey value iteration works producing successive approximations optimal value function 
iteration performed steps faster sparsity transition function 
number iterations required grow exponentially discount factor condon discount factor approaches decisions results happen farther farther 
practice policy iteration converges fewer iterations value iteration iteration costs jsj prohibitive 
tight worst case bound available policy iteration littman :10.1.1.108.2266
modi ed policy iteration puterman shin seeks trade cheap ective iterations preferred rust 
linear programming schrijver extremely general problem mdps solved general purpose linear programming packages ho man karp 
advantage approach commercial quality linear programming packages available time space requirements quite high 
theoretic perspective linear programming known algorithm solve mdps polynomial time theoretically cient algorithms shown cient practice 

learning optimal policy model free methods previous section reviewed methods obtaining optimal policy mdp assuming model 
model consists knowledge state transition probability function reinforcement function 
reinforcement learning primarily concerned obtain optimal policy model known advance 
agent interact environment directly obtain information means appropriate algorithm processed produce optimal policy 
point ways proceed 
model free learn controller learning model 
model learn model derive controller 
approach better 
matter debate reinforcement learning community 
anumber algorithms proposed sides 
question appears elds adaptive control dichotomy direct indirect adaptive control 
section examines model free learning section examines model methods 
biggest problem facing reinforcement learning credit assignment 
dowe know action just taken ects 
strategy wait reward actions taken result punish result bad 
ongoing tasks di cult know require great deal memory 
insights value iteration adjust estimated value state kaelbling littman moore ahc rl architecture adaptive heuristic critic 
immediate reward estimated value state 
class algorithms known temporal di erence methods sutton :10.1.1.132.7760
consider di erent temporal di erence learning strategies discounted nite horizon model 
adaptive heuristic critic td adaptive heuristic critic algorithm adaptive version policy iteration barto sutton anderson value function computation longer implemented solving set linear equations computed algorithm called td 
block diagram approach isgiven 
consists components critic labeled ahc reinforcement learning component labeled rl 
reinforcement learning component instance armed bandit algorithms modi ed deal multiple states non stationary rewards 
acting maximize instantaneous reward acting maximize heuristic value computed critic 
critic uses real external reinforcement signal learn map states expected discounted values policy executed currently instantiated rl component 
see analogy modi ed policy iteration imagine components working alternation 
policy implemented rl xed critic learns value function policy 
critic rl component learn new policy maximizes new value function 
implementations components operate simultaneously 
alternating implementation guaranteed converge optimal policy appropriate conditions 
williams baird explored convergence properties class ahc related algorithms call incremental variants policy iteration williams baird 
remains explain critic learn value policy 
de ne hs experience tuple summarizing single transition environment 
agent state transition choice action instantaneous reward receives resulting state 
value policy learned sutton td algorithm sutton uses update rule state visited estimated value updated closer instantaneous reward received estimated value occurring state :10.1.1.132.7760
analogous sample backup rule value iteration di erence sample drawn real world simulating known model 
key idea sample value reinforcement learning survey correct incorporates real learning rate adjusted properly slowly decreased policy held xed td guaranteed converge optimal value function 
td rule really instance general class algorithms called td 
td looks step ahead adjusting value estimates eventually arrive correct answer take quite 
general td rule similar td rule applied state eligibility just immediately previous state version eligibility trace de ned tx sk sk ifs sk eligibility degree visited past reinforcement received update states visited eligibility 
equivalent 
roughly equivalent updating states number times visited run 
note update eligibility online follows current state computationally expensive execute general td converges considerably faster large dayan dayan sejnowski 
making updates cient changing de nition td consistent certainty equivalent method singh sutton discussed section :10.1.1.51.4764:10.1.1.32.9278
learning components ahc accomplished uni ed manner watkins learning algorithm watkins watkins dayan 
learning typically easier implement 
order understand learning additional notation 
expected discounted reinforcement action state continuing choosing actions optimally 
note value assuming best action taken initially max 
written recursively max note max arg max optimal policy 
function action explicit estimate values online method essentially td de ne policy kaelbling littman moore action chosen just maximum value current state 
learning rule max hs experience tuple described earlier 
action executed state nite number times nite run decayed appropriately values converge probability watkins tsitsiklis jaakkola jordan singh 
learning extended update states occurred step previously peng williams :10.1.1.56.7356
values nearly converged optimal values appropriate agent act greedily situation action highest value 
learning di cult exploitation versus exploration trade 
formally justi ed approaches problem general case standard practice adopt ad hoc methods discussed section 
ahc architectures di cult learning practical level 
hard get relative learning rates right ahc components converge 
addition learning exploration insensitive values converge optimal values independent agent behaves data collected long state action pairs tried 
means exploration exploitation issue addressed learning details exploration strategy ect convergence learning algorithm 
reasons learning popular ective model free algorithm learning delayed reinforcement 
address issues involved generalizing large state action spaces 
addition may converge quite slowly policy 
model free learning average reward described learning applied discounted nite horizon mdps 
applied undiscounted problems long optimal policy guaranteed reach reward free absorbing state state periodically reset 
schwartz examined problem adapting learning average reward framework 
learning algorithm exhibit convergence problems mdps researchers average reward criterion closer true problem wish solve discounted criterion prefer learning learning mahadevan 
mind researchers studied problem learning optimal policies 
mahadevan surveyed model average reward algorithms reinforcement learning perspective di culties existing algorithms 
particular showed existing reinforcement learning algorithms average reward dynamic programming algorithms produce bias optimal policies 
jaakkola jordan singh described average reward learning algorithm guaranteed convergence properties 
uses monte carlo component estimate expected reward state agent moves environment 
reinforcement learning survey addition bertsekas presents learning algorithm average case reward new textbook 
provides needed theoretical foundation area reinforcement learning important problems remain unsolved 

computing optimal policies learning models previous section showed possible learn optimal policy knowing models orr learning models en route 
methods guaranteed nd optimal policies eventually little computation time experience extremely ine cient data gather require great deal experience achieve performance 
section assuming don know models advance examine algorithms operate learning models 
algorithms especially important applications computation considered cheap real world experience costly 
certainty equivalent methods conceptually straightforward method rst learn functions exploring environment keeping statistics results action compute optimal policy methods section 
method known certainty kumar varaiya 
serious objections method arbitrary division learning phase acting phase 
gather data environment initially 
random exploration dangerous environments immensely ine cient method gathering data requiring exponentially data whitehead system interleaves experience gathering policy building tightly koenig simmons 
see example 
possibility environment problematic 
breaking agent life pure learning pure acting phase considerable risk optimal controller early life detection suboptimal controller environment changes 
idea certainty equivalence model learned continually agent lifetime step current model compute optimal policy value function 
method ective available data ignores question exploration extremely computationally demanding fairly small state spaces 
fortunately number model algorithms practical 
dyna sutton dyna architecture exploits middle ground yielding strategies ective model free learning computationally cient kaelbling littman moore 
goal environment due whitehead random exploration take take steps reach goal intelligent exploration strategy assume untried action leads directly goal require steps 
certainty equivalence approach 
simultaneously uses experience build model uses experience adjust policy uses model adjust policy 
dyna operates loop interaction environment 
experience tuple hs ri behaves follows update model incrementing statistics transition action receiving reward action state updated models update policy state newly updated model rule max value iteration update values 
perform additional updates choose state action pairs random update rule sk ak max choose action perform state values modi ed exploration strategy 
dyna algorithm requires times computation learning instance typically vastly naive model method 
reasonable value determined relative speeds computation action 
shows grid world cell agent actions transitions deterministically adjacent cell block occurs 
see table dyna requires order magnitude fewer steps experience learning arrive optimal policy 
dyna requires times computational ort 
reinforcement learning survey state grid world 
formulated shortest path problem yields result reward goal reward zero discount factor 
steps backups convergence convergence learning dyna prioritized sweeping table performance algorithms described text 
methods exploration heuristic optimism face uncertainty state previously visited assumed default goal state 
learning optimal learning rate parameter deterministic maze 
dyna prioritized sweeping permitted take backups transition 
prioritized sweeping priority queue emptied backups 
kaelbling littman moore prioritized sweeping queue dyna dyna great improvement previous methods su ers relatively undirected 
particularly unhelpful goal just reached agent dead continues update random state action pairs concentrating interesting parts state space 
problems addressed prioritized sweeping moore atkeson queue dyna peng williams independently developed similar techniques :10.1.1.134.8196
describe prioritized sweeping detail 
algorithm similar dyna updates longer chosen random values associated states value iteration state action pairs learning 
appropriate choices store additional information model 
state remembers predecessors states non zero transition probability action 
addition state priority initially set zero 
updating random state action pairs prioritized sweeping updates states highest priority 
high priority state follows remember current value state old 
update state value max set state priority back 
compute value change jv old modify priorities predecessors value state changed amount immediate predecessors informed event 
state exists action priority promoted priority exceeded value 
global behavior algorithm real world transition surprising agent happens goal state instance lots computation directed propagate new information back predecessor states 
realworld transition boring actual result similar predicted result computation continues deserving part space 
running prioritized sweeping problem see large improvement dyna 
optimal policy reached half number steps experience third computation dyna required times fewer steps twice computational ort learning 
model methods reinforcement learning survey methods proposed solving mdps model context modelbased methods 
rtdp real time dynamic programming barto bradtke singh model method uses learning concentrate computational ort areas state space agent occupy 
speci problems agent trying achieve particular goal state reward 
account start state nd short path start goal necessarily visiting rest state space 
planning system dean kaelbling kirman nicholson kirman exploits similar intuition :10.1.1.48.6957
starts making approximate version mdp smaller original 
approximate mdp contains set states called envelope includes agent current state goal state 
states envelope summarized single state 
planning process alternation nding optimal policy approximate mdp adding useful states envelope 
action may take place parallel planning case irrelevant states pruned envelope 

generalization previous discussion tacitly assumed possible enumerate state action spaces store tables values 
small environments means impractical memory requirements 
ine cient experience 
large smooth state space generally expect similar states similar values similar optimal actions 
surely compact representation table 
problems continuous large discrete state spaces large continuous action spaces 
problem learning large spaces addressed generalization techniques allow compact storage learned information transfer knowledge similar states actions 
large literature generalization techniques inductive concept learning applied reinforcement learning 
techniques need tailored speci details problem 
sections explore application standard function approximation techniques adaptive resolution models hierarchical methods problem reinforcement learning 
reinforcement learning architectures algorithms discussed included storage variety mappings including policies value functions functions rewards deterministic transitions transition probabilities 
mappings transitions immediate rewards learned straightforward supervised learning handled wide variety function approximation techniques supervised learning support noisy training examples 
popular techniques include various methods rumelhart mcclelland fuzzy logic lee 
cmac albus local memory methods moore atkeson schaal generalizations nearest neighbor methods 
mappings especially policy kaelbling littman moore mapping typically need specialized algorithms training sets input output pairs available 
generalization input reinforcement learning agent current state plays central role selection actions 
viewing agent state free black box description current state input 
depending agent architecture output action selection evaluation current state select action 
problem deciding di erent aspects input ect value output called structural credit assignment problem 
section examines approaches generating actions evaluations function description agent current state 
rst group techniques covered specialized case reward delayed second group generally applicable 
immediate reward agent actions uence state transitions resulting problem choosing actions maximize immediate reward function agent current state 
problems bear resemblance bandit problems discussed section agent condition action selection current state 
reason class problems described associative reinforcement learning 
algorithms section address problem learning immediate boolean reinforcement state vector valued action boolean vector 
algorithms context delayed reinforcement instance rl component ahc architecture described section 
generalized real valued reward reward comparison methods sutton 
crbp complementary algorithm ackley littman crbp consists feed forward network mapping encoding state encoding action 
action determined probabilistically activation output units output unit activation bit action vector value probability 
neural network supervised training procedure adapt network follows 
result generating action network trained input output pair hs ai 
result network trained input output pair hs ai 
idea training rule action fails generate reward crbp try generate action di erent current choice 
algorithm oscillate action complement happen 
step training network change action slightly output probabilities tend move action selection random increases search 
hope random distribution generate action works better action reinforced 
arc associative reinforcement comparison arc algorithm sutton instance ahc architecture case boolean actions consisting feed reinforcement learning survey forward networks 
learns value situations learns policy 
simple linear networks hidden units 
simplest case entire system learns optimize immediate reward 
consider behavior network learns policy mapping vector describing 
output unit activation action generated normal noise 
adjustment output unit simplest case rst factor reward received action second encodes action taken 
actions encoded magnitude reward action sign action action 
described network tend seek actions positive reward 
extend approach maximize reward compare reward baseline changes adjustment output second network 
second network trained standard supervised mode estimate function input state variations approach variety applications anderson barto lin sutton 
reinforce algorithms williams studied problem choosing actions maximize reward 
identi ed broad class update rules perform gradient descent expected reward showed rules backpropagation 
class called reinforce algorithms includes linear reward inaction section special case 
generic reinforce update parameter ij written wij ij bij ln gj wij ij non negative factor current reinforcement ij reinforcement baseline probability density function randomly generate actions unit activations 
ij ij take di erent values ij ij constant system expected update exactly direction expected reward gradient 
update half space gradient necessarily direction steepest increase 
williams points choice baseline ij profound ect convergence speed algorithm 
logic methods strategy generalization reinforcement learning reduce learning problem associative problem learning boolean functions 
boolean function vector boolean inputs single boolean output 
inspiration mainstream machine learning kaelbling developed algorithms learning boolean functions reinforcement uses bias dnf drive kaelbling littman moore generalization process kaelbling searches space syntactic descriptions functions simple generate test method kaelbling 
restriction single boolean output techniques di cult apply 
benign learning situations possible extend approach collection learners independently learn individual bits complex output 
general approach su ers problem unreliable reinforcement single learner generates inappropriate output bit learners receive 
cascade method kaelbling allows collection learners trained collectively generate appropriate joint outputs considerably reliable require additional computational ort 
delayed reward method allow reinforcement learning techniques applied large state spaces modeled value iteration learning 
function approximator represent value function mapping state description value 
experimented approach boyan moore local memory methods conjunction value iteration lin backpropagation networks learning watkins cmac learning tesauro backpropagation learning value function backgammon described section zhang dietterich backpropagation td learn strategies job shop scheduling 
positive examples general unfortunate interactions function approximation learning rules 
discrete environments guarantee operation updates value function bellman equations reduce error current value function optimal value function 
guarantee longer holds generalization 
issues discussed boyan moore give simple examples value function errors growing arbitrarily large generalization value iteration 
solution applicable certain classes problems discourages divergence permitting updates estimated values shown near optimal battery monte carlo experiments 
thrun schwartz function approximation value functions dangerous errors value functions due generalization compounded max operator de nition value function 
results gordon tsitsiklis van roy show appropriate choice function approximator guarantee convergence necessarily optimal values 
baird residual gradient technique baird provides guaranteed convergence locally optimal solutions 
counter examples misplaced 
boyan moore report counter examples problem speci hand tuning despite unreliability algorithms provably converge discrete domains 
sutton shows modi ed versions boyan moore examples converge successfully 
open question general principles ideally supported theory help understand value function approximation succeed 
sutton com reinforcement learning survey experiments boyan moore counter examples changes aspects experiments 
small changes task speci cations 

di erent kind function approximator cmac albus weak generalization 

di erent learning algorithm sarsa rummery niranjan value iteration 

di erent training regime 
boyan moore sampled states uniformly state space sutton method sampled empirical trajectories 
intuitive reasons believe fourth factor particularly important careful research 
adaptive resolution models cases partition environment regions states considered purposes learning generating actions 
detailed prior knowledge environment di cult know granularity placement partitions appropriate 
problem overcome methods adaptive resolution course learning partition constructed appropriate environment 
decision trees environments characterized set boolean variables possible learn compact decision trees representing values 
learning algorithm chapman kaelbling works follows 
starts assuming partitioning necessary tries learn values entire environment state 
parallel process gathers statistics individual input bits asks question bit state description values states signi cantly di erent values states 
bit split decision tree 
process repeated leaves 
method able learn small representations function presence overwhelming number irrelevant noisy state attributes 
outperformed learning backpropagation simple environment mccallum conjunction techniques dealing partial observability learn behaviors complex driving simulator 
acquire partitions attributes signi cant needed solve parity problems 
variable resolution dynamic programming algorithm moore enables conventional dynamic programming performed real valued multivariate state spaces straightforward discretization fall prey curse dimensionality 
kd tree similar decision tree partition state space coarse regions 
coarse regions re ned detailed regions parts state space predicted important 
notion importance obtained running mental trajectories state space 
algorithm proved problems full high resolution arrays impractical 
disadvantage requiring guess initially valid trajectory state space 
start goal kaelbling littman moore dimensional maze problem 
point robot nd path start goal crossing barrier lines 
path taken entire rst trial 
begins intense exploration nd route entirely enclosed start region 
having eventually reached su ciently high resolution discovers gap proceeds greedily goal temporarily blocked goal barrier region 
second trial 
algorithm moore algorithm moore solution problem learning achieve goal con gurations deterministic high dimensional continuous spaces learning adaptive resolution model 
divides environment cells cell actions available consist aiming neighboring cells aiming accomplished local controller provided part problem statement 
graph cell transitions solved shortest paths online incremental manner minimax criterion detect group cells coarse prevent movement obstacles avoid limit cycles 
cells split higher resolution 
eventually environment divided just choose appropriate actions achieving goal unnecessary distinctions 
important feature reducing memory computational requirements structures exploration state space multi resolution manner 
failure agent initially try di erent rectify failure resort small local changes qualitatively di erent strategies exhausted 
dimensional continuous maze 
shows performance robot algorithm rst trial 
shows second trial started slightly di erent position 
fast algorithm learning policies spaces dimensions minute 
restriction current implementation deterministic environments limits applicability 
mccallum suggests related tree structured methods 
generalization actions reinforcement learning survey networks described section generalize state descriptions inputs 
produce outputs discrete factored representation seen generalizing actions 
cases actions described combinatorially important generalize actions avoid keeping separate statistics huge number actions chosen 
continuous action spaces need generalization pronounced 
estimating values neural network possible distinct network action network distinct output action 
action space continuous approach possible 
alternative strategy single network state action input value output 
training network conceptually di cult network nd optimal action challenge 
method local gradient ascent search action order nd high value baird klopf 
gullapalli developed neural reinforcement learning unit continuous action spaces 
unit generates actions normal distribution adjusts mean variance previous experience 
chosen actions performing variance high resulting exploration range choices 
action performs mean moved direction variance decreased resulting tendency generate action values near successful 
method successfully employed learn control robot arm continuous degrees freedom 
hierarchical methods strategy dealing large state spaces treat hierarchy learning problems 
cases hierarchical solutions introduce slight sub optimality performance potentially gain deal ciency execution time learning time space 
hierarchical learners commonly structured gated behaviors shown 
collection behaviors map environment states low level actions gating function decides state environment behavior actions switched executed 
maes brooks aversion architecture individual behaviors xed priori gating function learned reinforcement 
mahadevan connell dual approach xed gating function supplied reinforcement functions individual behaviors learned 
lin dorigo colombetti approach rst training behaviors training gating function 
hierarchical learning methods cast framework 
feudal learning feudal learning dayan hinton watkins involves hierarchy modules 
simplest case high level master low level slave 
master receives reinforcement external environment 
actions consist commands kaelbling littman moore structure gated behaviors 
give low level learner 
master generates particular command slave reward slave actions satisfy command result external reinforcement 
master learns mapping states commands 
slave learns mapping commands states external actions 
set commands associated reinforcement functions established advance learning 
really instance general gated behaviors approach slave execute behaviors depending command 
reinforcement functions individual behaviors commands learning takes place simultaneously high low levels 
compositional learning singh compositional learning ql consists hierarchy temporal sequencing subgoals 
elemental tasks behaviors achieve recognizable condition 
high level goal system achieve set conditions sequential order 
achievement conditions provides reinforcement elemental tasks trained rst achieve individual subgoals 
gating function learns switch elemental tasks order achieve appropriate high level sequential goal 
method tham prager learn control simulated multi link robot arm 
hierarchical distance goal especially consider reinforcement learning modules part larger agent architectures important consider problems goals dynamically input learner 
kaelbling hdg algorithm uses hierarchical approach solving problems goals achievement agent get particular state quickly possible agent dynamically 
hdg algorithm works analogy navigation harbor 
environment partitioned priori addresses case learning partition set regions centers known landmarks 
agent reinforcement learning survey hall office printer hall example partially observable environment 
currently region goal uses low level actions move goal 
high level information determine landmark shortest path agent closest landmark goal closest landmark 
agent uses low level information aim landmark 
errors action cause deviations path problem best aiming point recomputed step 

partially observable environments real world environments possible agent tohave perfect complete perception state environment 
unfortunately complete observability necessary learning methods mdps 
section consider case agent observations state environment observations may noisy provide incomplete information 
case robot instance observe corridor open room junction observations error prone 
problem referred problem incomplete perception perceptual aliasing hidden state 
section consider extensions basic mdp framework solving partially observable problems 
resulting formal model called partially observable markov decision process pomdp 
state free deterministic policies naive strategy dealing partial observability ignore 
treat observations states environment behave 
shows simple environment agent attempting get printer ce 
moves ce chance agent places look hall require di erent actions getting printer 
consider states agent possibly behave optimally 

resulting problem markovian learning guaranteed converge 
small breaches markov requirement handled learning possible construct simple environments cause learning oscillate chrisman kaelbling littman moore littman 
possible model approach act policy gather statistics transitions observations solve optimal policy observations 
unfortunately environment isnot markovian transition probabilities depend policy executed new policy induce new set transition probabilities 
approach may yield plausible results cases guarantees 
reasonable ask optimal policy mapping observations actions case np hard littman nd mapping best mapping poor performance :10.1.1.135.717
case agent trying get printer instance deterministic state free policy takes nite number steps reach goal average 
state free stochastic policies improvement gained considering stochastic policies mappings observations probability distributions actions 
randomness agent actions get stuck hall forever 
jaakkola singh jordan developed algorithm nding locally optimal stochastic policies nding globally optimal policy np hard 
example turns optimal stochastic policy agent state looks hall go east probability west probability 
policy solving simple case quadratic program 
fact simple example produce irrational numbers gives indication di cult problem solve exactly 
policies internal state behave truly ectively wide range environments memory previous actions observations disambiguate current state 
variety approaches learning policies internal state 
recurrent learning intuitively simple approach recurrent neural network learn values 
network trained backpropagation time suitable technique learns retain history features predict value 
approach anumber researchers mcgraw blank lin mitchell schmidhuber :10.1.1.45.8935:10.1.1.32.3932
ectively simple problems su er convergence local optima complex problems 
classi er systems classi er systems holland goldberg explicitly developed solve problems delayed reward including requiring short term memory 
internal mechanism typically pass reward back chains decisions called bucket brigade algorithm bears close resemblance learning 
spite early successes original design appear handle partially observed environments robustly 
approach reexamined insights literature success 
dorigo comparative study learning classi er systems dorigo bersini 
cli ross start wilson zeroth reinforcement learning survey se structure pomdp agent 
level classi er system wilson add bit memory registers 
nd system learn short term memory registers ectively approach scale complex environments 
dorigo colombetti applied classi er systems moderately complex problem learning robot behavior immediate reinforcement dorigo dorigo colombetti 
finite history window approach way restore markov property allow decisions history observations actions 
lin mitchell xed width nite history window learn pole balancing task 
mccallum describes utile su memory learns variable width window serves simultaneously model environment nite memory policy 
system excellent results complex driving simulation domain mccallum :10.1.1.54.132
ring neural network approach uses variable history window adding history necessary disambiguate situations 
pomdp approach strategy consists hidden markov model hmm techniques learn model environment including hidden state model construct perfect memory controller cassandra kaelbling littman lovejoy monahan :10.1.1.53.7233:10.1.1.135.717
chrisman showed forward backward algorithm learning hmms adapted learning pomdps 
mccallum gave heuristic rules attempt learn smallest possible model environment 
resulting model integrate information agent observations order decisions 
illustrates basic structure perfect memory controller 
component left state estimator computes agent belief state function old belief state action current observation context belief state probability distribution states environment indicating likelihood agent past experience environment states 
state estimator constructed straightforwardly estimated world model bayes rule 
left problem nding policy mapping belief states action 
problem formulated mdp di cult solve techniques described earlier input space continuous 
chrisman approach take account uncertainty yields policy small amount computation 
standard approach operations research literature solve kaelbling littman moore optimal policy close approximation thereof representation convex function belief space 
method computationally intractable may serve inspiration methods approximations cassandra littman cassandra kaelbling :10.1.1.53.7233

reinforcement learning applications reason reinforcement learning popular serves theoretical tool studying principles agents learning act 
unsurprising anumber researchers practical computational tool constructing autonomous systems improve experience 
applications ranged robotics industrial manufacturing combinatorial search problems computer game playing 
practical applications provide test cacy usefulness learning algorithms 
inspiration deciding components reinforcement learning framework practical importance 
example researcher real robotic task provide data point questions important optimal exploration 
break learning period exploration phases exploitation phases 
useful model long term reward finite horizon 
discounted 
nite horizon 
computation available agent decisions 
prior knowledge build system algorithms capable knowledge 
examine set practical applications reinforcement learning bearing questions mind 
game playing game playing dominated arti cial intelligence world problem domain eld born 
player games established reinforcement learning framework optimality criterion games maximizing reward face xed environment maximizing reward optimal adversary minimax 
reinforcement learning algorithms adapted general class games littman researchers reinforcement learning environments :10.1.1.135.717
application spectacularly far ahead time samuel checkers playing system samuel 
learned value function represented linear function approximator employed training scheme similar updates value iteration temporal di erences learning 
tesauro applied temporal di erence algorithm backgammon 
backgammon approximately states making table reinforcement learning impossible 
tesauro backpropagation layer reinforcement learning survey training games hidden units results basic poor td lost points games td lost points games td lost point games table td gammon performance games top human professional players 
backgammon tournament involves playing series games points player reaches set target 
td gammon won tournaments came su ciently close considered best players world 
neural network function approximator value function board position probability victory current player versions learning algorithm 
rst call basic td gammon little prede ned knowledge game representation board position virtually raw encoding su ciently powerful permit neural network distinguish conceptually di erent positions 
second td gammon provided raw state information supplemented anumber handcrafted features backgammon board positions 
providing hand crafted features manner example inductive biases human knowledge task supplied learning algorithm 
training learning algorithms required months computer time achieved constant self play 
exploration strategy system greedily chose move largest expected probability victory 
naive exploration strategy proved entirely adequate environment surprising considerable reinforcement learning literature produced numerous counter examples show greedy exploration lead poor learning performance 
backgammon important properties 
firstly policy followed game guaranteed nite time meaning useful reward information obtained fairly frequently 
secondly state transitions su ciently stochastic independent policy states occasionally visited wrong initial value function little danger starving visiting critical part state space important information obtained 
results table td gammon impressive 
competed top level international human play 
basic td gammon played professional standard 
schaal atkeson devil sticking robot 
tapered stick hit alternately hand sticks 
task keep devil stick falling hits possible 
robot motors indicated torque vectors 
experiments games cases produced interesting learning behavior success close td gammon repeated 
games studied include go schraudolph dayan sejnowski chess thrun 
open question success td gammon repeated domains 
robotics control years robotics control applications reinforcement learning 
concentrate examples interesting ongoing robotics investigations underway 

schaal atkeson constructed armed robot shown learns juggle device known devil stick 
complex non linear control task involving dimensional state space msecs control decision 
initial attempts robot learns keep juggling hundreds hits 
typical human learning task requires order magnitude practice achieve pro ciency mere tens hits 
juggling robot learned world model experience generalized unvisited states function approximation scheme known locally weighted regression cleveland moore atkeson 
trial form dynamic programming speci linear control policies locally linear transitions improve policy 
form dynamic programming known linear quadratic regulator design sage white 
reinforcement learning survey 
mahadevan connell discuss task mobile robot pushes large boxes extended periods time 
box pushing known di cult robotics problem characterized immense uncertainty results actions 
learning conjunction novel clustering techniques designed enable higher dimensional input tabular approach permitted 
robot learned perform competitively performance human programmed solution 
aspect mentioned section pre programmed breakdown monolithic task description set lower level tasks learned 

mataric describes robotics experiment viewpoint theoretical reinforcement learning high dimensional state space containing dozens degrees freedom 
mobile robots traveled enclosure collecting small disks transporting destination region 
enhancements basic learning algorithm 
firstly pre programmed signals called progress estimators break monolithic task subtasks 
achieved robust manner robots forced estimators freedom pro inductive bias provided 
secondly control decentralized 
robot learned policy independently explicit communication 
thirdly state space quantized small number discrete states values small number pre programmed boolean features underlying sensors 
performance learned policies simple hand crafted controller job 

learning elevator dispatching task crites barto :10.1.1.17.5519
problem implemented simulation stage involved elevators servicing 
objective minimize average squared wait time passengers discounted time 
problem posed discrete markov system states simpli ed version problem 
crites barto neural networks function approximation provided excellent comparison study learning approach popular sophisticated elevator dispatching algorithms 
squared wait time controller approximately best alternative algorithm empty system heuristic receding horizon controller half squared wait time controller frequently real elevator systems 

nal example concerns application reinforcement learning authors task food processing industry 
problem involves lling containers variable numbers non identical products 
product characteristics vary time sensed 
depending task various constraints placed container lling procedure 
examples mean weight produced manufacturer declared weight kaelbling littman moore number containers declared weight 
containers may produced weight tasks controlled machinery operates various setpoints 
conventional practice setpoints chosen human operators choice easy dependent current product characteristics current task constraints 
dependency di cult model highly non linear 
task posed nite horizon markov decision task state system function product characteristics amount time remaining production shift mean wastage percent declared shift far 
system discretized discrete states local weighted regression learn generalize transition model 
prioritized sweeping maintain optimal value function new piece transition information obtained 
simulated experiments savings considerable typically wastage reduced factor 
system deployed successfully factories united states 
interesting aspects practical reinforcement learning come light examples 
striking cases real system proved necessary supplement fundamental algorithm extra pre programmed knowledge 
supplying extra knowledge comes price human ort insight required system subsequently autonomous 
clear tasks knowledge free approach achieved worthwhile performance nite lifetime robots 
forms pre programmed knowledge take 
included assumption linearity juggling robot policy manual breaking task subtasks mobile robot examples box clustering technique values assumed locally consistent values 
disk collecting robots additionally manually discretized state space 
packaging example far fewer dimensions required correspondingly weaker assumptions assumption local piecewise continuity transition model enabled massive reductions amount learning data required 
exploration strategies interesting 
juggler careful statistical analysis judge pro experiment 
mobile robot applications able learn greedy exploration exploiting deliberate exploration 
packaging task optimism face uncertainty 
strategies mirrors theoretically optimal computationally intractable exploration proved adequate 
considering computational regimes experiments 
di erent indicates di ering computational demands various reinforcement learning algorithms array di ering applications 
juggler needed fast decisions low latency hit long periods seconds trial consolidate experiences collected previous trial perform aggressive computation necessary produce new reactive controller trial 
box pushing robot meant reinforcement learning survey operate autonomously hours decisions uniform length control cycle 
cycle su ciently long quite substantial computations simple qlearning backups 
disk collecting robots particularly interesting 
robot short life minutes due battery constraints meaning substantial number impractical signi cant combinatorial search signi cant fraction robot learning lifetime 
packaging task easy constraints 
decision needed minutes 
provided opportunities fully computing optimal value function state system control cycle addition performing massive cross validation optimization transition model learned 
great deal currently progress practical implementations reinforcement learning 
insights task constraints produce important ect shaping kind algorithms developed 

variety reinforcement learning techniques ectively variety small problems 
techniques scale larger problems 
researchers done bad job inventing learning techniques di cult solve arbitrary problems general case 
order solve highly complex problems wemust give rasa learning techniques incorporate bias give leverage learning process 
necessary bias come variety forms including shaping technique shaping training animals bower teacher presents simple problems solve rst gradually exposes learner complex problems 
shaping supervised learning systems train hierarchical reinforcement learning systems bottom lin alleviate problems delayed reinforcement decreasing delay problem understood dorigo colombetti dorigo 
local reinforcement signals possible agents reinforcement signals local 
applications possible compute gradient rewarding agent steps gradient just achieving nal goal speed learning signi cantly mataric 
imitation agent learn watching agent perform task lin 
real robots requires perceptual abilities available 
strategy supply appropriate motor commands robot joystick steering wheel pomerleau 
problem decomposition decomposing huge learning problem collection smaller ones providing useful reinforcement signals subproblems powerful technique biasing learning 
interesting examples robotic reinforcement learning employ technique extent connell mahadevan 
re thing keeps agents know learning hard time nding interesting parts space wander kaelbling littman moore random getting near goal killed immediately 
problems ameliorated programming set re cause agent act initially way reasonable mataric singh barto grupen connolly 
re eventually overridden detailed accurate learned knowledge keep agent alive pointed right direction trying learn 
explores re robot learning safer cient 
appropriate biases supplied programmers teachers complex problems eventually solvable 
done interesting questions remaining learning techniques especially regarding methods approximating decomposing incorporating bias problems 
marco dorigo anonymous reviewers comments helped improve 
colleagues reinforcement learning community done explained 
leslie pack kaelbling supported part nsf iri iri 
michael littman supported part bellcore 
andrew moore supported part nsf research initiation award 
ackley littman 

generalization scaling reinforcement learning 
touretzky 
ed advances neural information processing systems pp 
san mateo ca 
morgan kaufmann 
albus 

new approach manipulator control cerebellar model articulation controller cmac 
journal dynamic systems measurement control 
albus 

brains behavior robotics 
byte books subsidiary mcgraw hill new hampshire 
anderson 

learning problem solving multilayer connectionist systems 
ph thesis university massachusetts amherst ma 


hierarchical learning stochastic domains 
master thesis brown university providence rhode island 
baird 

residual algorithms reinforcement learning function approximation 
prieditis russell 
eds proceedings twelfth international conference machine learning pp 
san francisco ca 
morgan kaufmann 
baird klopf 

reinforcement learning high dimensional continuous actions 
tech 
rep wl tr wright patterson air force base ohio wright laboratory 
reinforcement learning survey barto bradtke singh 

learning act real time dynamic programming 
arti cial intelligence 
barto sutton anderson 

neuronlike adaptive elements solve di cult learning control problems 
ieee transactions systems man cybernetics smc 
bellman 

dynamic programming 
princeton university press princeton nj 


arti cial neural networks approximate reasoning intelligent control space 
american control conference pp 

berry 

bandit problems sequential allocation experiments 
chapman hall london uk 
bertsekas 

dynamic programming deterministic stochastic models 
prentice hall englewood cli nj 
bertsekas 
dynamic programming optimal control 
athena scienti belmont massachusetts 
volumes 
bertsekas casta non 
adaptive aggregation nite horizon dynamic programming 
ieee transactions automatic control 
bertsekas tsitsiklis 

parallel distributed computation numerical methods 
prentice hall englewood cli nj 
box draper 

empirical model building response surfaces 
wiley 
boyan moore 

generalization reinforcement learning safely approximating value function 
tesauro touretzky leen 
eds advances neural information processing systems cambridge ma 
mit press 
graham 

control theory including optimal control 
ellis horwood 
cassandra kaelbling littman 

acting optimally partially observable stochastic domains 
proceedings twelfth national conference arti cial intelligence seattle wa 
chapman kaelbling 

input generalization delayed reinforcement learning algorithm performance comparisons 
proceedings international joint conference onarti cial intelligence sydney australia 
chrisman 

reinforcement learning perceptual aliasing perceptual distinctions approach 
proceedings tenth national conference onarti cial intelligence pp 
san jose ca 
aaai press 
kaelbling littman moore chrisman littman 

hidden state short term memory 
presentation reinforcement learning workshop machine learning conference 


fast cient reinforcement learning truncated temporal di erences 
prieditis russell 
eds proceedings twelfth international conference machine learning pp 
san francisco ca 
morgan kaufmann 
cleveland 

locally weighted regression approach regression analysis local tting 
journal american statistical association 
cli ross 

adding temporary memory 
adaptive behavior 
condon 

complexity stochastic games 
information computation 
connell mahadevan 

rapid task learning real robots 
robot learning 
kluwer academic publishers 
crites barto 

improving elevator performance reinforcement learning 
touretzky mozer hasselmo 
eds neural information processing systems 
dayan 

convergence td general machine learning 
dayan hinton 

feudal reinforcement learning 
hanson cowan giles 
eds advances neural information processing systems san mateo ca 
morgan kaufmann 
dayan sejnowski 

td converges probability machine learning 
dean kaelbling kirman nicholson 

planning deadlines stochastic domains 
proceedings eleventh national conference onarti cial intelligence washington dc 


probabilistic production inventory problem 
management science 


finite state markovian decision processes 
academic press new york 
dorigo bersini 

comparison learning classi er systems 
animals animats proceedings third international conference simulation adaptive behavior brighton uk 
dorigo colombetti 

robot shaping developing autonomous agents learning 
arti cial intelligence 
reinforcement learning survey dorigo 

learning control real robot distributed classi er systems 
machine learning 


cient reinforcement learning 
proceedings seventh annual acm conference computational learning theory pp 

association computing machinery 
gittins 

multi armed bandit allocation indices 
wiley interscience series systems optimization 
wiley chichester ny 
goldberg 

genetic algorithms search optimization machine learning 
addison wesley ma 
gordon 

stable function approximation dynamic programming 
prieditis russell 
eds proceedings twelfth international conference machine learning pp 
san francisco ca 
morgan kaufmann 
gullapalli 

stochastic reinforcement learning algorithm learning real valued functions 
neural networks 
gullapalli 

reinforcement learning application control 
ph thesis university massachusetts amherst ma 
bower 

theories learning fourth edition 
prentice hall englewood cli nj 
ho man karp 

nonterminating stochastic games 
management science 
holland 

adaptation natural arti cial systems 
university michigan press ann arbor mi 
howard 

dynamic programming markov processes 
mit press cambridge ma 
jaakkola jordan singh 

convergence stochastic iterative dynamic programming algorithms 
neural computation 
jaakkola singh jordan 

monte carlo reinforcement learning non markovian decision problems 
tesauro touretzky leen 
eds advances neural information processing systems cambridge ma 
mit press 
kaelbling 

hierarchical learning stochastic domains preliminary results 
proceedings tenth international conference machine learning amherst ma 
morgan kaufmann 
kaelbling 

learning embedded systems 
mit press cambridge ma 
kaelbling 

associative reinforcement learning generate test algorithm 
machine learning 
kaelbling littman moore kaelbling 

associative reinforcement learning functions dnf 
machine learning 
kirman 

predicting real time planner performance domain characterization 
ph thesis department computer science brown university 
koenig simmons 

complexity analysis real time reinforcement learning 
proceedings eleventh national cial intelligence pp 
menlo park california 
aaai press mit press 
kumar varaiya 

stochastic systems estimation identi cation adaptive control 
prentice hall englewood cli new jersey 
lee 

self learning rule controller employing approximate reasoning neural net concepts 
international journal intelligent systems 
lin 

programming robots reinforcement learning teaching 
proceedings ninth national conference onarti cial intelligence 
lin 

hierachical learning robot skills reinforcement 
proceedings international conference networks 
lin 

reinforcement learning robots neural networks 
ph thesis carnegie mellon university pittsburgh pa lin mitchell 

memory approaches reinforcement learning non markovian domains 
tech 
rep cmu cs carnegie mellon university school computer science 
littman 

markov games framework multi agent reinforcement learning 
proceedings eleventh international conference machine learning pp 
san francisco ca 
morgan kaufmann 
littman 

memoryless policies theoretical limitations practical results 
cli husbands meyer wilson 
eds animals animats proceedings third international conference simulation adaptive behavior cambridge ma 
mit press 
littman cassandra kaelbling 

learning policies partially observable environments scaling 
prieditis russell 
eds proceedings twelfth international conference machine learning pp 
san francisco ca 
morgan kaufmann 
littman dean kaelbling 

complexity solving markov decision problems 
proceedings eleventh annual conference uncertainty arti cial intelligence uai montreal quebec canada 
lovejoy 
algorithmic methods partially observable markov decision processes 
annals operations research 
reinforcement learning survey maes brooks 

learning coordinate behaviors 
proceedings eighth national conference onarti cial intelligence pp 

morgan kaufmann 
mahadevan 

discount discount reinforcement learning case study comparing learning learning 
proceedings eleventh international conference machine learning pp 
san francisco ca 
morgan kaufmann 
mahadevan 

average reward reinforcement learning foundations algorithms empirical results 
machine learning 
mahadevan connell 

automatic programming behavior robots reinforcement learning 
proceedings ninth national conference arti cial intelligence anaheim ca 
mahadevan connell 

scaling reinforcement learning robotics exploiting subsumption architecture 
proceedings eighth international workshop machine learning pp 

mataric 

reward functions accelerated learning 
cohen hirsh 
eds proceedings eleventh international conference machine learning 
morgan kaufmann 
mccallum 

reinforcement learning selective perception hidden state 
ph thesis department computer science university 
mccallum 

overcoming incomplete perception utile distinction memory 
proceedings tenth international conference machine learning pp 
amherst massachusetts 
morgan kaufmann 
mccallum 

instance utile distinctions reinforcement learning hidden state 
proceedings twelfth international conference machine learning pp 
san francisco ca 
morgan kaufmann 
mcgraw blank 

emergent control planning autonomous vehicle 

ed proceedings fifteenth annual meeting cognitive science society pp 

erlbaum associates hillsdale nj 


rapid safe incremental learning navigation strategies 
ieee transactions systems man cybernetics 
monahan 

survey partially observable markov decision processes theory models algorithms 
management science 
moore 

variable resolution dynamic programming ciently learning action maps multivariate real valued spaces 
proc 
eighth international machine learning workshop 
kaelbling littman moore moore 

parti game algorithm variable resolution reinforcement learning multidimensional state spaces 
cowan tesauro alspector 
eds advances neural information processing systems pp 
san mateo ca 
morgan kaufmann 
moore atkeson 

investigation memory function approximators learning control 
tech 
rep mit arti cal intelligence laboratory cambridge ma 
moore atkeson 

prioritized sweeping reinforcement learning data real time 
machine learning 
moore atkeson schaal 

memory learning control 
tech 
rep cmu ri tr cmu robotics institute 
narendra 

learning automata 
prentice hall englewood cli nj 
narendra 

learning automata survey 
ieee transactions systems man cybernetics 
peng williams 

cient learning planning dyna framework 
adaptive behavior 
peng williams 

incremental multi step learning 
proceedings eleventh international conference machine learning pp 
san francisco ca 
morgan kaufmann 
pomerleau 

neural network perception mobile robot guidance 
kluwer academic publishing 
puterman 

markov decision processes discrete stochastic dynamic programming 
john wiley sons new york ny 
puterman shin 

modi ed policy iteration algorithms discounted markov decision processes 
management science 
ring 

continual learning reinforcement environments 
ph thesis university austin austin texas 


mathematical computational techniques multilevel adaptive methods 
society industrial applied mathematics philadelphia pennsylvania 
rumelhart mcclelland 
eds 

parallel distributed processing explorations cognition 
volume foundations 
mit press cambridge ma 
rummery niranjan 

line learning connectionist systems 
tech 
rep cued infeng tr cambridge university 
reinforcement learning survey rust 

numerical dynamic programming economics 
handbook computational economics 
elsevier north holland 
sage white 

optimum systems control 
prentice hall 
ungar 

active exploration learning real valued spaces multi armed bandit allocation indices 
prieditis russell 
eds proceedings twelfth international conference machine learning pp 
san francisco ca 
morgan kaufmann 
samuel 

studies machine learning game checkers 
ibm journal research development 
feigenbaum feldman editors computers thought mcgraw hill new york 
schaal atkeson 

robot juggling implementation memory learning 
control systems magazine 
schmidhuber 

general method multi agent learning incremental unrestricted environments 
yao 
ed evolutionary computation theory applications 
scienti publ 
singapore 
schmidhuber 

curious model building control systems 
proc 
international joint conference networks singapore vol 
pp 

ieee 
schmidhuber 

reinforcement learning markovian non markovian environments 
lippman moody touretzky 
eds advances neural information processing systems pp 
san mateo ca 
morgan kaufmann 
schraudolph dayan sejnowski temporal di erence learning position evaluation game go 
cowan tesauro alspector 
eds advances neural information processing systems pp 
san mateo ca 
morgan kaufmann 
schrijver 

theory linear integer programming 
wiley interscience new york ny 
schwartz 

reinforcement learning method maximizing undiscounted rewards 
proceedings tenth international conference machine learning pp 
amherst massachusetts 
morgan kaufmann 
singh barto grupen connolly 

robust reinforcement learning motion planning 
cowan tesauro alspector 
eds advances neural information processing systems pp 
san mateo ca 
morgan kaufmann 
singh sutton 

reinforcement learning replacing eligibility traces 
machine learning 
kaelbling littman moore singh 

reinforcement learning hierarchy models 
proceedings tenth national conference onarti cial intelligence pp 
san jose ca 
aaai press 
singh 

transfer learning composing solutions elemental sequential tasks 
machine learning 
singh 

learning solve markovian decision processes 
ph thesis department computer science university massachusetts 
technical report 


stochastic optimal control 
john wiley sons 
sutton 

generalization reinforcement learning successful examples sparse coarse coding 
touretzky mozer hasselmo 
eds neural information processing systems 
sutton 

temporal credit assignment reinforcement learning 
ph thesis university massachusetts amherst ma 
sutton 

learning predict method temporal di erences 
machine learning 
sutton 

integrated architectures learning planning reacting approximating dynamic programming 
proceedings seventh international conference machine learning austin tx 
morgan kaufmann 
sutton 

planning incremental dynamic programming 
proceedings eighth international workshop machine learning pp 

morgan kaufmann 
tesauro 

practical issues temporal di erence learning 
machine learning 
tesauro 

td gammon self teaching backgammon program achieves play 
neural computation 
tesauro 

temporal di erence learning td gammon 
communications acm 
tham prager 

modular learning architecture manipulator task decomposition 
proceedings eleventh international conference machine learning san francisco ca 
morgan kaufmann 
thrun 

learning play game chess 
tesauro touretzky leen 
eds advances neural information processing systems cambridge ma 
mit press 
reinforcement learning survey thrun schwartz 

issues function approximation reinforcement learning 
mozer smolensky touretzky elman weigend 
eds proceedings connectionist models summer school hillsdale nj 
lawrence erlbaum 
thrun 

role exploration learning control 
white 
eds handbook intelligent control neural fuzzy adaptive approaches 
van nostrand reinhold new york ny 
tsitsiklis 

asynchronous stochastic approximation learning 
machine learning 
tsitsiklis van roy 

feature methods large scale dynamic programming 
machine learning 
valiant 

theory learnable 
communications acm 
watkins 

learning delayed rewards 
ph thesis king college cambridge uk 
watkins dayan 

learning 
machine learning 
whitehead 

complexity cooperation learning 
proceedings eighth international workshop machine learning evanston il 
morgan kaufmann 
williams 

class gradient estimating algorithms reinforcement learning neural networks 
proceedings ieee international conference neural networks san diego ca 
williams 

simple statistical gradient algorithms connectionist reinforcement learning 
machine learning 
williams baird iii 

analysis incremental variants policy iteration steps understanding actor critic learning systems 
tech 
rep nu ccs northeastern university college computer science boston ma 
williams baird iii 

tight performance bounds greedy policies imperfect value functions 
tech 
rep nu ccs northeastern university college computer science boston ma 
wilson 

classi er tness accuracy 
evolutionary computation 
zhang dietterich 

reinforcement learning approach job shop scheduling 
proceedings international joint conference onarti cial 

