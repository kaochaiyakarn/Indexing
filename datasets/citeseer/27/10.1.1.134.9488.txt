extracting relevant structures information gal chechik naftali tishby cs huji ac computer science engineering interdisciplinary center neural hebrew university jerusalem israel draft please distribute problem extracting relevant aspects data face structures inherent modeling complex data 
extracting structure random variable relevant principally addressed information bottleneck method :10.1.1.39.9882
auxiliary variables contain formation required due structures irrelevant task 
cases fact easier specify task hand 
identifying relevant structures considerably improved mini information irrelevant variable 
give general formulation problem derive formal aswell algorithmic solution 
operation demonstrated synthetic example real world problems context text tion face images 
original information bottleneck problem related rate distortion theory distortion measure relevant information extracting relevant features removing irrelevant ones related rate distortion side information 
fundamental goal machine learning find regular structures construct predictive comprehensible models 
general goal unfortunately ill defined data sets contain alternative conflicting lying structures 
example documents may classified subject writing style spoken words labeled meaning identity speaker pro classified structure function valid alternatives 
alternative structures relevant implicit problem formulation 
problem identifying relevant structures commonly addressed tasks providing relevant label data selecting features discriminative respect label 
information theoretic generalization super approach proposed information bottleneck method ib :10.1.1.39.9882
approach relevance introduced random variable label supervised learning goal compress source variable information auxiliary relevance variable 
framework proven powerful numerous applications clustering objects respect verbs documents respect terms genes respect tissues stimuli respect spike patterns :10.1.1.42.7488:10.1.1.28.7256
important condition approach auxiliary variable cor responds task 
situations pure variable available 
variable may fact contain alternative conflicting structures 
show general common problem alleviated providing negative information information unimportant irrelevant aspects data interfere desired structure learning 
illustration consider simple nonlinear regression problem 
variables related functional form known noise distribution depends sample pairs goal extracting relevant dependence noise may contain information interfere extracting 
knowing joint distribution course improve 
real life example analysis gene expression data 
generated dna chips technology considered empirical joint distribution gene expression levels different tissues tissues taken biological conditions pathologies 
search expressed genes testify existence pathology may obscured genetic correlations exist conditions 
sample irrelevant expression data taken instance healthy population enable clustering analysis focus pathological ignore spurious structures 
examples numerous instantiations common problem inorder better extract relevant structures information irrelevant components data incorporated 
naturally various solutions suggested problem different contexts spectral subtraction weighted regression analysis 
current presents general unified information theoretic framework problems extending original information bottleneck variational problem deal discriminative tasks nature observing analogy rate distortion side information 
information theoretic formulation formalize problem extracting relevant structures consider gamma occurrence distributions known 
goal uncover structures exist gamma 
distribution may contain conflicting underlying structures may exist gamma 
variables stand example set terms set documents structure seek additional set documents gamma set genes sets tissues different biological conditions 
examples gamma conditionally independent assumption factorizes gamma jx gamma jx 
relationship variables expressed venn diagram area circle corresponds entropy variable see discussion type diagrams intersection circles corresponds mutual information 
mutual information random familiar symmetric functional joint distribution log venn diagram illustrating relations entropy mutual formation variables gamma area circle corresponds entropy variable intersection circles corresponds mutual information 
gamma independent mutual information vanishes known overlap included circle graphical model representation side information 
variables gamma seek compact stochastic representation preserves information removes information gamma graph gamma conditionally independent identify relevant structures joint distribution aim extract representation variable minimal loss mutual information relevant variable time maximal loss information irrelevance variable gamma goal information bottleneck side information therefor find stochastic map new variable tjx way mutual information minimizes mutual information gamma general achieve goal perfectly asymptotically finite case leads sub optimal compression example depicted blue region 
constrains cast single variational functional gamma fi theta gamma fli gamma lambda lagrange parameter fi determines tradeoff compression infor mation extraction parameter fl determines tradeoff preservation relevant variable loss information irrelevant gamma applications communication value fl may determined bythe relative cost transmitting information gamma means 
information bottleneck variational problem introduced special case current variational problem fl side irrelevant information isavailable :10.1.1.39.9882
case distributions tjx jt determined 
solution characterization complete lagrangian constrained optimization problem tjx gamma fi theta gamma fli gamma lambda gamma tjx normalization lagrange multipliers 
minimization formed respect stochastic mapping tjx account jz gamma 
proposition extrema obey self consistent equations tjx gamma fi dkl jx jjp jt gamma gamma jx jjp gamma jt tjx jt jx tjx gamma jt gamma jx tjx exp gamma fi dkl jx jjp jt gamma gamma jx jjp gamma jt normalization factor dkl log kullback leibler diver gence proof markovian relation yjx yjx write tjx tjx yjx tjx obtain second term eq ffi ffip tjx ffi ffip tjx jx tjx log jt jx log jt jx jx gamma dkl theta jx jjp jt lambda dkl theta jx jjp lambda similar differentiation terms yield ffi ffip tjx log tjx gamma fi gamma dkl theta jx jjp jt lambda theta gamma jx jjp gamma jt lambda delta gamma gamma fi dkl jx jjp gamma gamma jx jjp gamma holds terms independent equating derivative zero yields equa tion proposition 
interestingly performing minimization tjx jt gamma jt leads solution self consistent equations 
formal solutions variational problem exponential form generalization solution original ib problem 
original ib fi goes infinity lagrangian reduces gamma fli gamma hard clustering solution tjx binary cluster 
intuition operation obtained rewriting second term eq 
gamma fli gamma gamma gamma log jt gamma fl gamma gamma log gamma jt gamma log jt gamma jt fl gamma fl je gamma 
fl fixed level operates extract compact representation maximizes mean log likelihood ratio log jt gamma jt je gamma discriminability distribution jt gamma jt 
setup extended case multiple variables multi information preserved fy variables multi removed fy gamma gamma gamma discussed 
yields tjx gamma fl dkl jx jjp jt fl gamma dkl gamma jx jjp gamma jt solved self consistent conditions similarly eq 

relation rate distortion theory side information problem formulated related theory rate distortion side informa tion 
rate distortion theory source variable variable decoded side channel 
achievable code rate distortion level bounded rate known rate distortion function 
optimal encoding stochastic map tjx representation quantization average distortion 
optimal code 
rate improved utilizing side information form variable known ends channel 
case improved rate achieved avoiding sending information extracted rate distortion function side information lower lower bound gamma optimal quantization case distortion constraint see ch 
details 
information bottleneck framework average distortion replaced mutual information rate distortion function turned convex curve characterizes complexity relation variables see :10.1.1.39.9882:10.1.1.122.8863
similarly avoids differentiating instances informative contain information gamma variable gamma analogous side information variable just informative original ib 
problems helps mathematical formulation important emphasize different problems motivation scope 
side information specific communication problem arbitrary distortion function problem general statistical non parametric depends solely choice variables gamma different pattern recognition discriminative learning problems cast theoretic framework far original setting side information 
algorithms set self consistent equations eq 
solved iterating equations distributions similar algorithm ib similar convergence proofs :10.1.1.39.9882
care taken convergence guaranteed fl simply problem guaranteed feasible solutions fl 
case ib various heuristics applied deterministic annealing increasing parameter fi obtain finer clusters greedy clustering sequential means algorithm sib :10.1.1.122.8863
provides compromise top annealing agglomerative greedy approaches excellent performance 
algorithm adopted 
applications describe applications method simple synthetic example real world problem hierarchical text categorization features extraction face images 
synthetic example demonstrate ability approach uncover weak interesting hidden struc tures data designed occurrences matrix contains competing sub structures see 
demonstration purposes matrix created observed left weaker structure right 
compressing clusters preserving information ib fl yields clus tering half clustered 
strong structure left 
demonstration operation 
joint distribution con tains distinct conflicting structure 
clustering clusters bottleneck method separates upper lower values structure 
joint distribution gamma contains single structure sim ilar nature stronger structure 
clustering clusters successfully extract weaker structure 
created second occurrence matrix identifying relevant struc ture half yield similar distributions gamma jx 
applying sequential successfully ignores strong irrelevant structure retrieves weak structure 
importantly done unsupervised manner strong irrelevant structure 
example designed demonstration purposes irrelevant structures manifested gamma 
example shows approach real data structures covert 
hierarchical text categorization text categorization fundamental task information retrieval 
typically large set texts groups homogeneous subjects 
slonim colleagues showed ib method achieves categorization predicts manually pre defined categories great accuracy largely outperforms competing methods 
clearly unsupervised task difficult texts similar subjects alternative categories extracted correct 
chosen clusters accuracy illustration newsgroups hierarchical data 
accuracy vs word clusters ib dashed line 
solid line 
problem alleviated side information form additional docu ments categories 
specifically useful hierarchical document categorization known categories refined grouping documents sub categories 
applied problem operating terms documents cooccurrence matrix top level groups focusing relevant struc tures 
identify clusters terms cluster group documents subgroups targeted learning structures unsupervised manner chosen labelled dataset documents order able measure results agree manual classification 
labels algorithms learn ing serve quantify performance 
newsgroups database collected preprocessed described :10.1.1.22.6286
database consists equal sized groups documents hierarchically organized groups content 
aimed cluster documents belong newsgroups super group computer documents similar subjects comp sys ibm pc hardware comp sys mac hardware 
side information documents science sci crypt sci electronics sci med sci space 
demonstrate power double clustering separate documents groups 
goal clustering phase identify clusters terms extract relevant structures data 
goal second simply provide quantitative measure goodness features extracted phase 
therefor performed procedure fre quent words documents clustered clusters tive clustering 
word clusters sorted single cluster discriminative score dkl jt jjp gamma gamma jt jjp gamma clusters highest chosen 
word clusters clustering documents 
performance process evaluated measuring overlap resulting clusters withthe original groups 
plots document clustering accuracy fl compared ib method fl 
suc improves mean clustering accuracy percent percents 
image id pixel id samples face images women face different light sources matrix joint distribution 
column matrix corre sponds different image bank left columns correspond images left source light 
face images demonstrate applicability diverse types real world data problem extracting features face images 
ar database faces focused images contained strong light source left 
illumination conditions imply strong statistical structures set images 
examples images brought 
matrix face images column vector different picture database 
effects light source apparent columns left half matrix corresponding images left light source appear similar 
rows corresponding pixels upper part matrix similar rows lower part 
suggests clustering distribution images expected reflect effects light source 
applied problem additional set men faces side gamma 
choice side data harder method features numerous structures common men women faces images illuminated objects appropriate task 
depicts results obtained clustering pixels clusters varying levels parameter fl 
panel clusters appear different colors colors code differential information gamma fli gamma cluster see color baron right 
side data ignored fl clustering pixels extracts structure reflects il conditions 
positive fl values effects light direction salient features enhanced eyes surrounding mouth 
clusters obtained various values fl 
panel depicted different colors colors code level differential information gamma fli gamma cluster 
similar results obtained different numberof clusters information theoretic approach extracting relevant structures may contain multiple conflicting structures 
approach stems theory rate distortion side information information bottleneck method data irrelevant structures 
amir noam slonim eli helpful discussions 
supported israeli ministry science fellowships foundation 
baker mccallum 
distributional clustering words text classification 
research development information retrieval sigir 
acm press new york 
cover thomas 
elements information theory 
plenum press new york 
csiszar korner 
information theory coding theorems discrete memory systems academic press new york 
nd edition 
dumais chen 
hierarchical classification web content 
ingwersen belkin 
leong editors research development information retrieval acm sigir pages 
acm press new york 
hoffman 
probabilistic latent semantic indexing 
research development retrieval sigir pages 
acm press new york 
lang :10.1.1.22.6286
learning filter netnews 
proc 
th int conf 
machine learning 
martinez 
ar face database 
technical report cvc technical report 
friedman slonim tishby 
multivariate information bottleneck 
proceedings uncertainty ai 
pereira tishby lee 
distributional clustering english words 
meeting association computational linguistics pages 
slonim tishby van bialek 
analyzing neural codes information bottleneck method 
submitted 
sinkkonen kaski 
clustering conditional distribution auxiliary space 
neural computation 
slonim friedman tishby 
unsupervised document classification sequential information maximization 
proceedings th conference development information retrieval sigir 
appear 
slonim tishby :10.1.1.122.8863
agglomerative information bottleneck 
slonim tishby :10.1.1.39.9882
document clustering word clusters information bottleneck method 
ingwersen belkin 
leong editors re search development information retrieval sigir pages 
acm press new york 
tishby pereira bialek :10.1.1.39.9882
information bottleneck method 
proc th allerton conference communication computation 

probabilistic framework hierarchic organization classification document collections 
journal intelligent 
special issue automated text categorization 
wyner ziv 
rate distortion function source coding side information decoder 
ieee trans 
theory 
