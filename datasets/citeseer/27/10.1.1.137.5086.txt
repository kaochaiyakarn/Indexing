data prefetch mechanisms steven ibm server group david lilja university minnesota expanding gap microprocessor dram performance necessitated increasingly aggressive techniques designed reduce hide latency main memory access 
large cache hierarchies proven effective reducing latency frequently data uncommon programs spend half run times stalled memory requests 
data prefetching proposed technique hiding access latency data referencing patterns defeat caching strategies 
waiting cache initiate memory fetch data prefetching anticipates misses issues fetch memory system advance actual memory 
effective prefetching implemented way prefetches timely useful introduce little overhead 
secondary effects cache pollution increased memory bandwidth requirements taken consideration 
despite obstacles prefetching potential significantly improve program execution time overlapping computation memory accesses 
prefetching strategies diverse single strategy proposed provides optimal performance 
survey examines alternative approaches discusses design tradeoffs involved implementing data prefetch strategy 
categories subject descriptors memory structures design styles cache memories hardware memory structures general terms design performance additional key words phrases memory latency prefetching supported part national science foundation mip cda cda minnesota supercomputing institute university minnesota ibm shared university research project 
steve partially supported ibm graduate research fellowship preparation 
authors addresses system architecture performance design ibm server group highway north rochester mn email ibm com lilja dept electrical computer engineering university minnesota union st se minneapolis mn email lilja ece umn edu 
permission digital hard copy part personal classroom granted fee provided copies distributed profit commercial advantage copyright notice title publication date appear notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee 
acm acm computing surveys vol 
june contents 

background 
software data prefetching 
hardware data prefetching sequential prefetching prefetching arbitrary strides 
integrating hardware software prefetching 
prefetching multiprocessors 

metric microprocessor performance increased dramatic rate past decade 
trend sustained continued architectural innovations advances microprocessor fabrication technology 
contrast main memory dynamic ram dram performance increased rate shown 
chief latency reducing techniques cache memory hierarchies smith 
static ram sram memories caches managed keep pace processor memory request rates continue expensive main store technology 
large cache hierarchies proven effective reducing average memory access penalty programs show high degree locality addressing patterns uncommon scientific data intensive programs spend half run times stalled memory requests mowry 
large dense matrix operations form basis applications typically exhibit little data reuse may defeat caching strategies 
poor cache utilization applications partially result demand memory fetch policy caches 
policy fetches data cache main memory processor requested word absent cache 
situation illustrated data prefetch mechanisms computation including memory satisfied cache hierarchy represented upper time line main memory access time represented lower time line 
data blocks associated memory cache hierarchy fetched main memory 
assuming simple order execution unit processor stalled waits corresponding cache block fetched 
data returns main memory cached forwarded processor computation may proceed 
note fetch policy result cache access cache block previously accessed data stored cache 
cache misses known cold start compulsory misses 
referenced data part large array operation data replaced room new array elements streamed cache 
data block needed processor bring main memory incurring full main memory access latency 
called capacity cache misses avoided augment demand fetch policy cache data prefetch operation 
waiting cache perform memory fetch data prefetching anticipates misses issues fetch memory system advance actual memory 
prefetch proceeds parallel processor computation allowing memory system time transfer desired data main memory cache 
ideally prefetch complete just time processor access needed data cache stalling processor 
increasingly common mechanism initiating data prefetch explicit fetch instruction issued processor 
minimum specifies acm computing surveys vol 
june lilja performance year 
system dram performance 
system performance measured specfp dram performance row access times 
values normalized equivalents source internet ftp ftp cs toronto edu pub 
computation memory delay computation memory delay computation memory delay time address data word brought cache space 
fetch system dram computation memory access cache hit cache prefetch 
execution diagram assuming prefetching perfect prefetching degraded prefetching 
acm computing surveys vol 
june struction executed address simply passed memory system prefetching prefetching forcing processor wait response 
cache responds fetch manner similar ordinary load instruction exception referenced word forwarded processor cached 
shows prefetching improve execution time demand fetch case 
latency main memory accesses hidden overlapping computation memory accesses resulting reduction run time 
represents ideal case prefetched data arrives just requested processor 
optimistic situation depicted 
prefetches issued late avoid processor stalls data fetched early realize benefit 
note data arrives early hide memory latency held processor cache period time processor 
time prefetched data exposed cache replacement policy may evicted cache 
occurs prefetch said useless performance benefit derived fetching block early 
prematurely prefetched block may displace data cache currently processor resulting known cache pollution 
note effect distinguished normal cache replacement misses 
prefetch causes data prefetch mechanisms prefetch overhead 
software prefetching overhead 
cache occurred prefetching defined cache pollution 
prefetched block displaces cache block referenced prefetched block ordinary replacement resulting cache occurred prefetching 
subtle side effect prefetching occurs memory system 
note memory requests occur time units program startup requests compressed period time units 
removing processor stall cycles prefetching effectively increases frequency memory requests issued processor 
memory systems designed match higher bandwidth avoid saturated benefits prefetching burger 
particularly true multiprocessors bus utilization typically higher single processor systems 
interesting note software prefetching achieve reduction run time despite adding instructions execution stream 
memory effects ignored computational components run time shown 
seen prefetch instructions increase amount done processor 
hardware prefetching techniques proposed require explicit fetch instructions 
techniques employ acm computing surveys vol 
june lilja special hardware monitors processor attempt infer prefetching opportunities 
hardware prefetching incurs instruction overhead generates unnecessary prefetches software prefetching 
unnecessary prefetches common hardware schemes speculate memory accesses benefit compiletime information 
speculation incorrect cache blocks needed brought cache 
unnecessary prefetches affect correct program behavior result cache pollution consume memory bandwidth 
effective data prefetching implemented way prefetches timely useful introduce little overhead 
secondary effects memory system taken consideration designing system employs prefetch strategy 
despite obstacles data prefetching potential significantly improve program execution time overlapping computation memory accesses 
prefetching strategies diverse single strategy provides optimal performance proposed 
sections alternative approaches prefetching examined comparing relative strengths weaknesses 

background prefetching form existed mid early studies cache design wang recognized benefits fetching multiple words main memory cache 
effect block memory transfers prefetch words surrounding current hope advantage spatial locality memory 
hardware prefetching separate cache blocks implemented ibm amdahl smith 
software techniques 
smith acm computing surveys vol 
june alluded idea survey cache memories smith time usefulness 
porterfield proposed idea cache load instruction approximately time motorola introduced touch load instruction intel proposed dummy read operations fu 
today nearly microprocessor isas contain explicit instructions designed bring data processor cache hierarchy 
noted prefetching restricted fetching data main memory processor cache 
generally applicable technique moving memory objects memory hierarchy needed processor 
prefetching mechanisms instructions file systems commonly prevent processor stalls example see young shekita patterson gibson 
brevity techniques apply data objects residing memory considered 
nonblocking load instructions share similarities data prefetching 
prefetches instructions issued advance data actual take advantage parallelism processor memory subsystem 
loading data cache specified word placed directly processor register 
nonblocking loads example binding prefetch named value prefetched variable bound named location processor register case time prefetch issued 
nonblocking loads discussed forms binding prefetches examined 
data prefetching received considerable attention literature potential means boosting performance multiprocessor systems 
interest stems desire reduce particularly high memory latencies systems 
memory delays tend high multiprocessors due added contention shared resources shared bus memory modules symmetric multiprocessor 
memory delays pronounced distributed memory multiprocessors memory requests may need satisfied interconnection network 
masking significant memory latencies prefetching effective means speeding multiprocessor applications 
due emphasis prefetching multiprocessor systems prefetching mechanisms discussed studied exclusively context 
mechanisms may effective single processor systems multiprocessor prefetching treated separate topic prefetch mechanism inherent systems 

software data prefetching contemporary microprocessors support form fetch instruction implement prefetching bernstein 
fetch simple load processor register hardwired zero 
slightly sophisticated implementations provide hints memory system prefetched block 
information may useful multiprocessors data prefetched different sharing states example 
particular implementations vary fetch instructions share common characteristics 
fetches nonblocking memory operations require lockup free cache allows prefetches bypass outstanding memory operations cache 
prefetches typically implemented way fetch instructions cause exceptions 
exceptions suppressed prefetches insure remain data prefetch mechanisms optional optimization feature affect program correctness initiate large potentially unnecessary overhead page faults memory exceptions 
hardware required implement software prefetching modest compared prefetching strategies 
complexity approach lies judicious placement fetch instructions target application 
task choosing program place fetch instruction relative matching load store instruction known prefetch scheduling 
practice possible precisely predict schedule prefetch data arrives cache moment requested processor case 
execution time prefetch matching memory may vary memory latencies 
uncertainties predictable compile time require careful consideration scheduling prefetch instructions program 
fetch instructions may added programmer compiler optimization pass 
optimizations occur frequently program tedious implement hand prefetch scheduling done effectively programmer 
studies indicate adding just prefetch directives program substantially improve performance mowry gupta 
programming effort kept minimum program contains prefetching opportunities compiler support may required 
hand coded automated compiler prefetching loops responsible large array calculations 
loops provide excellent prefetching opportunities common scientific codes exhibit poor cache utilization predictable array referencing patterns 
establishing patterns compile time fetch instructions acm computing surveys vol 
june lilja placed inside loop bodies data loop iteration prefetched current iteration 
example loop prefetching may consider code segment shown 
loop calculates inner product vectors manner similar innermost loop matrix multiplication calculation 
assume cache block code segment cause cache fourth iteration 
attempt avoid cache misses adding prefetch directives shown 
note source code representation assembly code generated compiler 
simple approach prefetching suffers problems 
need prefetch iteration loop fetch brings words cache block cache 
extra prefetch operations illegal unnecessary degrade performance 
assuming cache block aligned prefetching done fourth iteration 
solution problem surround fetch directives condition tests modulo true 
overhead explicit prefetch predicate offset benefits prefetching avoided 
better solution unroll loop factor equal number words prefetched cache block 
shown unrolling loop involves replicating loop body times increasing loop stride note fetch directives replicated index value calculate prefetch address changed code segment removes cache misses unnecessary prefetches improvements possible 
note cache misses occur itera acm computing surveys vol 
june ip ip fetch fetch ip ip fetch fetch ip ip ip ip ip ip ip ip fetch ip fetch fetch fetch fetch ip ip ip ip ip ip ip ip ip ip 
inner product calculation prefetching simple prefetching prefetching loop unrolling software pipelining 
tion loop prefetches issued initial iteration 
unnecessary prefetches occur iteration unrolled loop fetch commands attempt access data past loop index boundary 
problems remedied software pipelining techniques shown 
fetch ip fetch fetch fetch fetch ip ip ip ip ip ip ip ip ip ip extracted select code segments loop body placed side original loop 
fetch statements prepended main loop prefetch data iteration main loop including ip 
segment code referred loop prolog 
added main loop execute final inner product computations initiating unnecessary prefetch instructions 
code said cover loop preceded matching prefetch 
final refinement may necessary prefetches effective 
examples written implicit assumption prefetching iteration ahead data actual sufficient hide latency main memory accesses 
may case 
early studies callahan assumption levy recognized sufficiently general solution 
loops contain small computational bodies may necessary initiate prefetches data prefetch mechanisms 
final inner product loop transformation 
prolog prefetching main loop prefetching computation epilog computation average memory latency measured processor cycles estimated cycle time shortest possible execution path loop iteration including prefetch overhead 
choosing shortest execution path loop iteration ceiling operator calculation designed err conservative side increase likelihood prefetched data cached requested processor 
returning main loop assume average latency processor cycles loop iteration time cycles final inner product loop transformation shown 
loop transformations outlined fairly mechanical refinements applied recursively nested loops 
sophisticated compiler algorithms approach developed varying degrees success automatically add fetch instructions optimization pass compiler mowry 
bernstein measured run times scientific benchmarks prefetching powerpc system 
prefetching typically improved acm computing surveys vol 
june lilja run times benchmark ran faster ran slightly slower due prefetch instruction overhead 
specfp benchmark programs ran faster pa system prefetching enabled 
remaining specfp programs showed improvement run time program slowed 
compiler able reliably predict memory access patterns prefetching normally restricted loops containing array accesses indices linear functions loop indices 
loops relatively common scientific codes far general applications 
attempts establishing similar software prefetching strategies applications hampered irregular referencing patterns chen lipasti luk mowry 
complex control structures typical general applications limited window reliably predict particular datum accessed 
cache block accessed chance successive cache blocks requested data structures graphs linked lists 
comparatively high temporal locality general applications result high cache utilization diminishing benefit prefetching 
restricted conformed looping structures explicit fetch instructions performance penalty considered software prefetching 
fetch instructions add processor overhead require extra execution cycles fetch source addresses calculated stored processor 
ideally prefetch address retained need recalculated matching load store acm computing surveys vol 
june instruction 
allocating retaining register space prefetch addresses compiler register space allocate active variables 
addition fetch instructions said increase register pressure turn may result additional spill code manage variables spilled memory due insufficient register space 
problem exacerbated prefetch distance greater implies maintaining address registers hold multiple prefetch addresses storing addresses memory required number registers available 
comparing transformed loop original loop seen software prefetching results significant code expansion turn may degrade instruction cache performance 
software prefetching done statically unable detect prefetched block prematurely evicted needs 

hardware data prefetching hardware prefetching schemes proposed add prefetching capabilities system need programmer compiler intervention 
changes existing executables necessary instruction overhead completely eliminated 
hardware prefetching take advantage run time information potentially prefetching effective 
sequential prefetching prefetching schemes designed fetch data main memory processor cache units cache blocks 
noted multiple word cache blocks form data prefetching 
grouping consecutive memory words single units caches exploit principle spatial locality implicitly prefetch data referenced near 
degree large cache blocks effective prefetching data limited ensuing cache pollution effects 
cache block size increases amount potentially useful data displaced cache room new block 
shared memory multiprocessors private caches large cache blocks may cause false sharing lilja occurs processors wish access different words cache block accesses store 
accesses logically applied separate words cache hardware unable distinction operates cache blocks 
accesses treated operations applied single object cache coherence traffic generated ensure changes block store operation seen processors caching block 
case false sharing traffic unnecessary processor executing store word written 
increasing cache block size increases likelihood processors sharing data block false sharing arise 
sequential prefetching take advantage spatial locality introducing problems associated large cache blocks 
simplest sequential prefetching schemes variations block lookahead obl approach initiates prefetch block block accessed 
differs simply doubling block size prefetched blocks treated separately regard cache replacement coherence policies 
example large block may contain word frequently referenced words 
assuming lru replacement policy entire block retained data prefetch mechanisms portion block data 
large block replaced smaller blocks evicted room active data 
similarly smaller cache blocks reduces probability false sharing occur 
obl implementations differ depending type access block initiates prefetch 
smith summarizes approaches prefetch tagged prefetch algorithms discussed 
prefetch algorithm simply initiates prefetch block access block results cache cached memory access initiated 
tagged prefetch algorithm associates tag bit memory block 
bit detect block demand fetched prefetched block referenced time 
cases sequential block fetched 
smith tagged prefetching reduces cache ratios unified instruction data cache set simulations 
prefetch half effective tagged prefetching reducing ratios 
reason prefetch effective illustrated behavior algorithm accessing contiguous blocks shown 
seen strictly sequential access pattern result cache cache block prefetch algorithm access pattern results cache employing tagged prefetch algorithm 
hp pa chan serves example contemporary microprocessor uses obl prefetch hardware 
pa implements tagged prefetch scheme directed undirected mode 
undirected mode sequential line prefetched 
directed mode prefetch direction forward backward acm computing surveys vol 
june lilja demand fetched prefetched distance determined pre post increment amount encoded load store instructions 
contents address register auto incremented cache block associated new address prefetched 
compared base case prefetching pa achieved run time improvements range specfp benchmark programs 
performance application dependent programs ran faster prefetching enabled 
note shortcoming obl schemes prefetch may initiated far advance actual avoid processor memory demand fetched prefetched demand fetched prefetched demand fetched prefetched demand fetched demand fetched demand fetched prefetched prefetched prefetched prefetched prefetched prefetched demand fetched prefetched prefetched demand fetched prefetched prefetched prefetched demand fetched prefetched prefetched prefetched prefetched 
forms sequential prefetching prefetch tagged prefetch sequential prefetching 
acm computing surveys vol 
june stall 
sequential access stream resulting tight loop example may allow sufficient lead time block request block 
solve problem possible increase number blocks prefetched demand fetch known degree prefetching 
prefetching subsequent blocks aids memory system staying ahead rapid processor requests sequential data blocks 
prefetched block accessed time cache check blocks cache missing blocks fetched memory 
prefetch efficiency time note scheme identical tagged obl prefetching 
increasing degree prefetching reduces rates sections code show high degree spatial locality additional traffic cache pollution generated sequential prefetching program phases show little spatial locality 
overhead tends sequential prefetching unfeasible values larger 
dahlgren proposed adaptive sequential prefetching policy allows value vary program execution way matched degree spatial locality exhibited program particular point time 
prefetch efficiency metric periodically calculated cache indication current spatial locality characteristics program 
prefetch efficiency defined ratio useful prefetches total prefetches useful prefetch occurs prefetched block results cache hit 
value initialized incremented prefetch efficiency exceeds predetermined upper threshold decremented efficiency drops lower threshold shown 
note reduced zero prefetching data prefetch mechanisms 
sequential adaptive prefetching 
upper threshold lower threshold effectively disabled 
point prefetch hardware begins monitor cache block occurs block cached restarts prefetching respective ratio numbers exceeds lower threshold prefetch efficiency 
simulations shared memory multiprocessor adaptive prefetching achieve appreciable reductions cache ratios tagged prefetching 
simulated run time comparisons show slight differences schemes 
lower ratio adaptive sequential prefetching partially nullified associated overhead increased memory traffic contention 
jouppi proposed approach prefetched blocks brought fifo stream buffer brought cache 
buffer entry referenced brought cache remaining blocks moved queue new block prefetched tail position 
note prefetched data placed directly cache scheme avoids cache pollution 
occurs cache desired block head stream buffer buffer flushed 
prefetched blocks accessed order brought buffer acm computing surveys vol 
june lilja stream buffers provide performance benefit 
palacharla kessler studied stream buffers cost effective alternative large secondary caches 
primary cache occurs stream buffers allocated service new stream 
stream buffers allocated lru order newly allocated buffer immediately fetches blocks missed block buffer 
palacharla kessler stream buffers provided adequate performance simulation study 
parameters stream buffer hit rates percentage primary cache misses satisfied stream buffers typically fell 
memory bandwidth requirements increase sharply result large number unnecessary prefetches generated stream buffers 
help mitigate effect small history buffer record primary cache misses 
history buffer indicates misses occurred block block stream allocated blocks prefetched buffer 
selective stream allocation policy bandwidth requirements reduced expense slightly reduced stream buffer hit rates 
stream buffers described palacharla kessler provide economical alternative large secondary caches eventually incorporated cray multiprocessor 
general sequential prefetching techniques require changes existing executables implemented relatively simple hardware 
compared software prefetching sequential hardware prefetching performs poorly nonsequential memory access patterns encountered 
scalar array accesses large strides result unnecessary acm computing surveys vol 
june prefetches types access patterns exhibit spatial locality sequential prefetching 
enable prefetching strided irregular data access patterns elaborate hardware prefetching techniques proposed 
prefetching arbitrary strides techniques proposed employ special logic monitor processor address referencing pattern detect constant stride array originating looping structures baer chen fu 
accomplished comparing successive addresses load store instructions 
chen baer technique aggressive proposed far 
illustrate design assume memory instruction addresses successive loop iterations 
prefetching initiated assumed stride series array accesses 
prefetch address predicted value observed address 
prefetching continues way note approach requires previous address memory instruction stored detected stride 
recording histories memory instruction program clearly impossible 
separate cache called prediction table rpt holds information memory instructions 
organization rpt 
table entries contain address memory instruction previous address accessed instruction stride value entries pc effective address instruction tag previous address stride state established stride state field records entry current state 
state diagram rpt entries 
rpt indexed cpu program counter pc 
memory instruction executed time entry rpt state set initial signifying prefetching initiated instruction 
executed rpt entry evicted stride value calculated subtracting previous address stored rpt current effective address 
illustrate functionality rpt consider matrix multiply code associated rpt entries 
example load instructions arrays considered assumed arrays addresses respectively 
simplicity cache blocks assumed 
data prefetch mechanisms prefetch address 
state transition graph prediction table entries 
iteration innermost loop state rpt instruction addresses represented pseudocode mnemonics 
rpt contain entries instructions stride fields initialized zero entry placed initial state 
result cache second iteration strides computed shown 
entries array placed transient state newly computed strides match previous stride 
state indicates instruction referencing pattern may transition tentative prefetch issued block address effective address stride cached 
rpt entry array placed steady state previous current strides match 
entry acm computing surveys vol 
june lilja initial steady transient prediction stride zero prefetching issued instruction 
third iteration entries array move steady state tentative strides computed previous iteration confirmed 
prefetches issued second iteration result cache hits provided prefetch distance sufficient 
discussion seen rpt improves sequential policies correctly handling strided array 
described rpt limits prefetch distance loop iteration 
remedy shortcoming distance field may added rpt specifies prefetch distance explicitly 
prefetch addresses calculated effective address stride distance addition distance field requires method establishing value rpt entry 
calculate appropriate value chen baer decouple maintenance rpt prefetch engine 
rpt entries maintained direction pc described prefetches initiated separately pseudo program counter called lookahead program counter la pc allowed precede pc 
difference pc acm computing surveys vol 
june initial start state 
prefetching 
transient stride transition 
tentative prefetch 
steady constant stride 
prefetch stride 
prediction stride indeterminate 
prefetching 
correct stride prediction incorrect stride prediction incorrect prediction stride update 
rpt execution matrix multiply 
la pc prefetch distance 
implementation issues arise addition lookahead program counter interested reader referred baer chen 
chen baer compared rpt prefetching mowry software prefetching scheme mowry method showed consistently better performance simulated shared memory multiprocessor 
performance depends individual program characteristics benchmark programs study 
software prefetching effective certain irregular access patterns indirect calculate prefetch address 
rpt may able establish access pattern instruction uses indirect address instruction may generate effective addresses separated constant stride 
rpt efficient loop 
prefetches issued rpt access pattern established 
means prefetches issued array data iterations 
chen baer note may take iterations rpt achieve prefetch distance completely masks memory latency la pc 
float rpt prefetch past array bounds incorrect prediction necessary subsequent prefetching 
loop steady state rpt able dynamically adjust prefetch distance achieve better overlap memory latency software scheme array access patterns 
software prefetching incurs instruction overhead resulting prefetch address calculation fetch instruction execution spill code 
dahlgren compared tagged rpt prefetching context distributed shared data prefetch mechanisms tag previous address stride state ld initial ld initial ld initial tag previous address stride state ld transient ld transient ld steady tag previous address stride state ld steady ld steady ld steady 
access field list element prompt prefetching subsequent list element 
memory multiprocessor 
examining simulated run time behavior benchmark programs concluded rpt prefetching showed limited performance benefits tagged prefetching tends perform better common memory access patterns 
dahlgren showed array strides block size covered tagged prefetch policy 
addition scalar show limited amount spatial locality captured tagged prefetch policy rpt mechanism 
memory bandwidth acm computing surveys vol 
june lilja field field field field limited conjectured conservative rpt prefetching mechanism may preferable tends produce fewer useless prefetches 
software prefetching majority hardware prefetching mechanisms focus regular array referencing patterns 
notable exceptions 
example harrison mehrotra propose extensions rpt mechanism allow prefetching data objects connected pointers 
approach adds fields rpt enable detection indirect strides arising structures linked lists sparse matrices 
hardware mechanism designed prefetch data structures connected pointers described roth 
rpt derivatives scheme uses hardware table log executed load instructions 
table detect dependencies load instructions establishing patterns single instructions 
specifically table records data values loaded executed load instructions detects instances values base addresses subsequent loads 
address dependencies common linked list processing pointer list element base address fields subsequent list element shown 
hardware table establishes dependencies prefetches triggered execution load instructions produce base addresses 
example address known field field field field 
block prefetching vector matrix product calculation 
acm computing surveys vol 
june field field prefetched 
prefetch initiate prefetching 
aggressive prefetching generally useful relatively long processing time required element sufficient hide latency prefetches 
alexander kedem proposed prefetch mechanism observation cache address tends followed belongs relatively predictable subset addresses 
take advantage property hardware table associate current cache address set successor cache addresses 
cache occurs address missed block index table find block addresses cache previous observations 
prefetches issued blocks 
prefetching data processor cache data blocks prefetched sram buffers integrated dram memory chips 
cache correctly predicted prefetch mechanism corresponding data read directly sram buffers avoiding relatively time consuming dram access 
data brought processor cache hierarchy scheme prefetches large amounts data order exploit high chip bandwidth exists integrated dram arrays sram buffers 
alexander kedem prefetch units range cache blocks gave best performance benchmark suite scientific engineering applications 
prefetching units cache yields average prediction accuracy 
similar approach joseph grunwald studied markov predictor drive hardware data prefetch mechanism 
mechanism attempts predict previous pattern misses begun repeat dynamically recording sequences cache hardware table similar alexander kedem 
current cache address table prefetches subsequent misses issued prefetch request queue 
prevent cache pollution wasted memory bandwidth prefetch requests may displaced queue requests belong sequences higher probability occurring near 
probability determined strength markov chain associated 
triggering data prefetches memory instructions researchers chang lui proposed triggering branch instructions stored branch target buffer btb smith 
branch instruction executed entry btb inspected predict branch target address data consumed predicted branch taken 
advantage approach memory operations associated single branch target reducing amount tag space required support prefetching 
entries btb control prefetching similar contents rpt entry fields previous data address stride state entry 
fields updated fashion similar rpt branch prediction correct corresponding memory instructions executed 
studies branch predicted prefetching indicate prefetch entries btb entry required yield close data prefetch mechanisms maximum benefit approach 
simulations processor prefetching prefetch entries btb entry rates improved specint benchmarks 

integrating hardware software prefetching software prefetching relies exclusively compile time analysis schedule fetch instructions user program 
contrast hardware techniques discussed far infer prefetching opportunities run time compiler instruction set support 
noting approaches advantages researchers proposed mechanisms combine elements software hardware prefetching 
describe variation tagged hardware prefetching degree prefetching particular stream calculated compile time passed prefetch hardware 
implement scheme prefetching degree pd field associated cache entry 
special fetch instruction provided prefetches specified block cache sets tag bit value pd field cache entry holding prefetched block 
blocks sequential stream prefetched instruction 
tagged block demand fetched value pd field added block address calculate prefetch address 
pd field newly prefetched block set tag bit set 
insures appropriate value propagated stream 
prefetching nonsequential patterns handled ordinary fetch instructions 
zhang torrellas suggest integrated technique enables prefetching irregular data structures 
accomplished tagging acm computing surveys vol 
june lilja memory locations way element data object initiates prefetch elements referenced object objects pointed referenced object 
array elements data structures connected pointers prefetched 
approach relies compiler initialize tags memory actual prefetching handled hardware memory system 
programmable prefetch engine proposed chen extension prediction table described section 
chen prefetch engine differs rpt tag address stride information supplied program dynamically established hardware 
entries inserted engine program entering looping structures benefit prefetching 
programmed prefetch engine functions rpt prefetches initiated processor program counter matches tag fields prefetch engine 
lilja propose prefetch engine external processor 
engine simple processor executes program prefetch data cpu 
shared second level cache relationship established engine prefetches new data blocks cache previously prefetched data accessed processor 
processor partially directs actions prefetch engine writing control information memory mapped registers prefetch engine support logic 
integrated techniques designed take advantage compiletime program information introducing instruction overhead pure software prefetching 
speculation pure hardware prefetching eliminated resulting fewer unnecessary prefetches 
acm computing surveys vol 
june commercial systems support model prefetching simulation studies evaluate techniques indicate performance enhanced pure software hardware prefetch mechanisms 

prefetching multiprocessors addition prefetch mechanisms multiprocessor specific prefetching techniques proposed 
prefetching systems differs uniprocessors reasons 
multiprocessor applications typically written different programming paradigms uniprocessors 
paradigms provide additional array referencing information enables accurate prefetch mechanisms 
second multiprocessor systems frequently contain additional memory hierarchies provide different sources destinations prefetching 
performance implications data prefetching take added significance multiprocessors systems tend higher memory latencies sensitive memory interconnects 
fu patel examined data prefetching improve performance vectorized multiprocessor applications 
study assumes vector operations explicitly specified programmer supported instruction set 
vectorized programs describe computations terms series vector matrix operations compiler analysis stride detection hardware required establish memory access patterns 
stride information encoded vector available processor caches associated prefetch hardware 
prefetching policies studied 
variation prefetch policy consecutive blocks cache fetched processor cache 
implementation prefetch differs earlier prefetches issued scalars vector stride equal cache block size 
second prefetch policy referred vector prefetching similar policy exception prefetches vector large strides issued 
vector block misses cache blocks stride stride stride fetched 
fu patel prefetch policies improve performance prefetch alliant fx simulator 
speedups pronounced smaller cache blocks assumed small block sizes limit amount spatial locality cache capture prefetching caches offset disadvantage simply prefetching blocks 
contrast studies fu patel sequential prefetching policies effective values 
apparent conflict earlier studies sequential prefetching degrade performance 
discrepancy may explained noting vector instructions exploited prefetching scheme fu patel 
case prefetch prefetching suppressed large stride specified instruction 
avoids useless prefetches degraded performance original policy 
vector prefetching issue prefetches large stride referencing patterns precise mechanism sequential schemes able take advantage stride information provided program 
comparing schemes applications large strides benefit vector prefetching expected 
programs scalar unit stride dominate prefetch policy tends perform slightly better 
programs lower ra data prefetch mechanisms resulting vector prefetching policy offset corresponding increase bus traffic 
examined prefetching distributed memory multiprocessor global local memory connected multistage interconnection network 
data prefetched global local memory large asynchronous block transfers achieve higher network bandwidth possible word atime transfers 
large amounts data prefetched data placed local memory processor cache avoid excessive cache pollution 
form software controlled caching assumed responsible translating global array addresses local addresses data placed local memory 
software prefetching single processor systems loop transformations performed compiler insert prefetch operations user code 
inserting fetch instructions individual words loop body entire blocks memory prefetched loop entered 
shows block prefetching may vector matrix product calculation 
iterations original loop partitioned nproc processors multiprocessor system processor iterates note array prefetched row time 
possible pull prefetch entire array fetched local memory entering outermost loop assumed large prefetch entire array occupy local memory available 
block fetches add processor overhead original computation manner similar software prefetching scheme described earlier 
prefetch operations require size stride information significantly acm computing surveys vol 
june lilja nproc fetch fetch overhead incurred word oriented scheme fewer prefetch operations needed 
assuming equal problem sizes ignoring prefetches loop generates block prefetches compared prefetches result applying word oriented prefetching scheme 
single bulk data transfer efficient dividing transfer smaller messages approach tends increase network congestion messages transferred 
combined increased request rate prefetching induces network contention lead significantly higher average memory latencies 
set numerical benchmark programs notes prefetching increases average memory latency factor prefetch case 
implication prefetching local memory cache array prefetched 
general scheme requires data read prefetch coherence mechanism provided allows writes processor seen processors 
data 
block prefetching vector matrix product calculation 
acm computing surveys vol 
june transfers restricted control dependencies loop bodies 
array predicated conditional statement prefetching initiated array 
done reasons 
conditional may test true subset array initiating prefetch entire array result unnecessary transfer potentially large amount data 
second conditional may guard referencing nonexistent data initiating prefetch data result unpredictable behavior 
honoring data control dependencies limits amount data prefetched 
average loop memory benchmark programs prefetched due constraints 
increased average memory latencies suppression prefetches limited speedup due prefetching benchmark programs 
mowry gupta studied effectiveness software prefetching dash dsm multiprocessor architecture 
study alternative designs considered 
design places prefetched data remote access cache rac lies interconnection network processor cache hierarchy node system 
second design alternative simply prefetched data remote memory directly primary processor cache 
cases unit transfer cache block 
separate prefetch cache rac motivated desire reduce contention primary data cache 
separating prefetched data demand fetched data prefetch cache avoids polluting processor cache provides cache space 
approach avoids processor stalls result waiting prefetched data placed cache 
case remote access cache remote memory operations benefit prefetching rac placed system bus access times approximately equal main memory 
simulation runs scientific benchmarks prefetching directly primary cache offered benefit average speedup compared average rac 
despite significantly increasing cache contention reducing cache space prefetching primary cache resulted higher cache hit rates proved dominant performance factor 
software prefetching single processor systems benefit prefetching application specific 
speedups array programs achieved speedups non prefetch case third regular program showed speedup 

prefetching schemes diverse 
help categorize particular approach useful answer basic questions concerning prefetching mechanism prefetches initiated prefetched data placed data prefetch mechanisms prefetched 
prefetches initiated explicit fetch operation program logic monitors processor referencing pattern infer prefetching opportunities combination approaches 
initiated prefetches issued timely manner 
prefetch issued early chance prefetched data displace useful data higher levels memory hierarchy displaced 
prefetch issued late may arrive actual memory introduce processor stall cycles 
prefetching mechanisms differ precision 
software prefetching issues fetches data hardware schemes tend prefetch data speculative manner 
decision place prefetched data memory hierarchy fundamental design decision 
clearly data moved higher level memory hierarchy provide performance benefit 
majority schemes place prefetched data type cache memory 
schemes place prefetched data dedicated buffers protect data premature cache evictions prevent cache pollution 
prefetched data placed named locations processor registers memory prefetch said binding additional constraints imposed data 
multiprocessor systems acm computing surveys vol 
june lilja introduce additional levels taken consideration memory hierarchy 
data prefetched units single words cache blocks contiguous blocks memory program data objects 
amount data fetched determined organization underlying cache memory system 
cache blocks may appropriate size uniprocessors smps larger memory blocks may amortize cost initiating data transfer interconnection network large distributed memory multiprocessor 
questions independent 
example prefetch destination small processor cache data prefetched way minimizes possibility polluting cache 
means precise prefetches need scheduled shortly actual prefetch unit kept small 
prefetch destination large timing size constraints relaxed 
prefetch mechanism specified natural compare schemes 
unfortunately comparative evaluation various proposed prefetching techniques hindered widely varying architectural assumptions testing procedures 
general observations 
majority prefetching schemes studies concentrate numerical array applications 
programs tend generate memory access patterns comparatively predictable yield high cache utilization benefit prefetching general applications 
result automatic techniques effective general programs received comparatively little attention 
context array ref acm computing surveys vol 
june erencing patterns prefetch mechanisms provide varying degrees coverage depending flexibility mechanism 
unit small stride array referencing patterns common easily detected prefetching schemes capture type access pattern 
sequential prefetching techniques concentrate exclusively patterns 
frequent large stride array referencing patterns result poor cache utilization 
design rpt motivated desire capture constant stride referencing patterns including large strides 
array referencing patterns constant stride frequently change strides typically covered pure hardware techniques 
software prefetching provide coverage arbitrary referencing pattern pattern detected compile time 
integrated techniques discussed far designed augment existing hardware techniques software support coverage limited underlying hardware mechanisms 
flexibility tends lend accuracy prefetch scheme 
software integrated techniques avoid unnecessary prefetches characteristic pure hardware mechanisms constrained prefetch streams produce 
unnecessary prefetches displace active data cache data processor 
addition causing cache pollution unnecessary prefetches needlessly expend memory bandwidth may limited due added pressure prefetching naturally places memory system 
prefetch mechanisms introduce degree hardware overhead 
techniques rely cache hardware supports prefetch operation 
sequential schemes introduce additional cache logic exception stream buffers require hardware external cache 
techniques require logic added processor 
pure software prefetching requires inclusion fetch instruction processors instruction set rpt derivatives add comparatively large amount logic overhead processor 
software prefetching minimal hardware requirements technique introduces significant amount instruction overhead user program 
integrated schemes attempt strike balance pure hardware software schemes reducing instruction overhead offering better prefetch coverage pure hardware techniques 
memory systems designed match added demands prefetching imposes 
despite reduction execution time prefetch mechanisms tend increase average memory latency removing processor stall cycles 
effectively increases memory request rate processor turn introduce congestion memory system 
problem particularly multiprocessor systems buses interconnect networks shared processors 
despite application system constraints data prefetching demonstrated ability reduce program execution time simulation studies real systems 
efforts improve extend known techniques diverse architectures applications active promising area research 
need new prefetching techniques continue motivated increasing memory access penalties arising widening gap microprocessor memory performance complex memory hierarchies 
acknowledgments authors anonymous reviewers useful suggestions 
data prefetch mechanisms alexander kedem 
distributed prefetch buffer cache design high performance memory systems 
proceedings nd ieee symposium high performance computer architecture 
ieee press piscataway nj 
wang 
performance evaluation computing systems memory hierarchies 
ieee trans 
comput 

baer 
chen 

effective chip preloading scheme reduce data access penalty 
proceedings conference supercomputing albuquerque nm nov martin chair 
acm press new york ny 
bernstein cohen freund 
compiler techniques data prefetching powerpc 
proceedings ifip wg working conference parallel architectures compilation techniques pact cyprus june bic hm 
chairs 
ifip working group algol manchester uk 
burger goodman 
limited bandwidth affect processor design 
ieee micro 
callahan kennedy porterfield 
software prefetching 
sigarch comput 
arch 
news apr 

modeling cache pollution 
proceedings second iasted conference modeling simulation 

chan 
design hp pa cpu 
hewlett packard 
chang liu 
data cache prefetching 
proceedings second workshop shared memory multiprocessing systems 
chen 

effective programmable prefetch engine chip caches 
proceedings th annual international symposium microarchitecture ann arbor mi nov dec mudge lu chairs 
ieee computer society press los alamitos ca 
chen 
baer 
performance study software hardware data prefetching schemes 
proceedings st international symposium computer architecture chicago il apr 

chen 
bear 
effective hardware data prefetching high performance processors 
ieee trans 
comput 
may 
chen mahlke chang hwu 

data access microarchitectures superscalar processors compiler assisted data prefetching 
proceedings th annual international acm computing surveys vol 
june lilja symposium microarchitecture micro albuquerque nm nov chair 
acm press new york ny 
dahlgren 
effectiveness hardware stride sequential prefetching shared memory multiprocessors 
proceedings st ieee symposium high performance computer architecture raleigh nc jan 
ieee press piscataway nj 
dahlgren dubois 
fixed adaptive sequential prefetching shared memory multiprocessors 
proceedings international conference parallel processing st charles il 

fu 
performance microarchitecture processor 
proceedings ieee international conference computer design cambridge ma 

fu patel 
data prefetching multiprocessor vector cache memories 
proceedings th international symposium computer architecture toronto ont canada 

fu patel janssens 
stride directed prefetching scalar processors 
proceedings th annual international symposium microarchitecture micro portland dec 
hwu chair 
ieee computer society press los alamitos ca 

integrated hardware software scheme shared memory multiprocessors 
proceedings international conference parallel processing st charles il 


compiler directed data prefetching multiprocessors memory hierarchies 
proceedings acm international conference supercomputing ics amsterdam netherlands june sameh van der vorst chairs 
acm press new york ny 
harrison mehrotra 
data prefetch mechanism accelerating general computation 

university illinois urbana champaign champaign il 
joseph grunwald 
prefetching markov predictors 
proceedings th international symposium computer architecture isca denver june mudge chairs 
acm press new york ny 
jouppi 
improving direct mapped cache performance addition small cache prefetch buffers 
proceedings th international symposium computer architecture isca seattle wa may 
ieee press piscataway nj 
acm computing surveys vol 
june levy 
architecture software controlled data prefetching 
sigarch comput 
arch 
news may 

lockup free instruction fetch prefetch cache organization 
proceedings th international symposium computer architecture minneapolis mn june 
acm press new york ny 
lilja 
cache coherence large scale shared memory multiprocessors issues comparisons 
acm comput 
surv 
sept 
lipasti schmidt 
software prefetching pointer call intensive environments 
proceedings th annual international symposium microarchitecture ann arbor mi nov dec mudge lu chairs 
ieee computer society press los alamitos ca 
lui 
branch directed stride data cache prefetching 
proceedings international conference computer design iccd austin tx 
ieee computer society press los alamitos ca 
luk 
mowry 
prefetching recursive data structures 
acm sigops oper 
syst 
rev 
mowry gupta 
tolerating latency software controlled prefetching shared memory multiprocessors 
parallel distrib 
comput 
june 
mowry lam gupta 
design evaluation compiler algorithm prefetching 
proceedings th international conference architectural support programming languages operating systems asplos boston ma oct eggers chair 
acm press new york ny 
kessler scott 
cray architecture overview 
cray supercomputers falls mn 
palacharla kessler 
evaluating stream buffers secondary cache replacement 
proceedings st international symposium computer architecture chicago il apr 
patterson gibson 
exposing concurrency informed prefetching 
proceedings third international conference parallel distributed information systems austin tx 

porterfield 
software methods improvement cache performance supercomputer applications 
ph thesis 
ph dissertation 
rice university houston tx 

performance impact block sizes fetch strategies 
proceedings th international symposium computer architecture seattle wa 

roth sohi 
prefetching linked data structures 
proceedings th international conference architectural support programming languages operating systems asplos viii san jose ca oct agarwal chairs 
acm press new york ny 
hsu 

data prefetching hp pa 
proceedings th international symposium computer architecture isca denver june mudge chairs 
acm press new york ny 

prefetch unit vector operations scalar computers 
proceedings th international symposium computer architecture gold coast qld australia 

smith 
sequential program prefetching memory hierarchies 
ieee computer 
smith 
cache memories 
acm comput 
surv 
sept 
smith 
study branch prediction techniques 
proceedings th interna received january revised march accepted april data prefetch mechanisms tional symposium computer architecture minneapolis mn june 
acm press new york ny 
lilja 
compiler assisted data prefetch controller 
proceedings international conference computer design iccd austin tx 
hsu lilja 
caches data prefetching techniques 
ieee computer 

mips superscalar microprocessor 
ieee micro apr 
young shekita 
intelligent cache prefetch mechanism 
proceedings international conference computer design iccd cambridge ma 
ieee computer society press los alamitos ca 
zhang torrellas 
speeding irregular applications shared memory multiprocessors 
proceedings nd annual international symposium computer architecture isca santa margherita ligure italy june patterson chair 
acm press new york ny 
acm computing surveys vol 
june 
