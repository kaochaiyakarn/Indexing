level caching efficient query processing large web search engines long cis department polytechnic university brooklyn ny cis poly edu large web search engines answer thousands queries second interactive response times 
due sizes data sets involved range multiple terabytes single query may require processing hundreds megabytes index data 
keep immense workload large search engines employ clusters hundreds thousands machines number techniques caching index compression index query pruning improve scalability 
particular level caching techniques cache results repeated identical queries frontend index data frequently query terms cached node lower level 
propose evaluate level caching scheme adds intermediate level caching additional performance gains 
intermediate level attempts exploit frequently occurring pairs terms caching intersections projections corresponding inverted lists 
propose study offline online algorithms resulting weighted caching problem turns surprisingly rich structure 
experimental evaluation large web crawl real search engine query log shows significant performance gains best schemes isolation combination caching levels 
observe careful selection cache admission eviction policies crucial best performance 
categories subject descriptors information systems miscellaneous information systems information storage retrieval software engineering metrics general terms algorithms performance experimentation keywords web search inverted index caching 

due rapid growth web pages current size pages users increasingly depend web search engines locating relevant information 
main challenges search engines provide ranking function identify useful results relevant pages lot research focused improve ranking clever term supported nsf career award nsf ccr new york state center advanced technology telecommunications polytechnic university 
copyright held international world wide web conference committee iw 
distribution papers limited classroom personal 
www may chiba japan 
acm 
torsten suel cis department polytechnic university brooklyn ny suel poly edu scoring link analysis evaluation user traces 
ranking function engineered query throughput critical issue 
large search engines need answer thousands queries second collections pages 
construction optimized index structures user query requires significant amount data processing average 
deal workload search engines typically implemented large clusters hundreds thousands servers techniques index compression caching result query pruning increase throughput decrease cost 
better understand performance issue need look basic structure current search engines 
engines information retrieval tools inverted index index structure allows efficient retrieval documents containing particular word term 
inverted index consists inverted lists inverted list iw contains ids documents collection contain particular word sorted document id measure plus additional information number occurrences document exact positions occurrences context title anchor text 
query containing search terms apple orange pear typical search engine returns documents score highest respect terms 
engine traverses inverted list query term uses information embedded inverted lists number occurrences terms document positions context compute score document containing search terms 
addition scores link analysis user feedback added total score document cases affect structure computation contributions precomputed offline pagerank 
clearly inverted list smaller document collection scanning inverted lists search terms preferable scanning entire collection 
lengths inverted lists grow linearly size collection terabyte collections billions pages lists commonly search terms range tens hundreds megabytes 
query evaluation expensive large numbers machines needed support query loads hundreds thousands queries second typical major engines 
motivates search new techniques increase number queries second sustained set machines addition index compression query pruning caching techniques widely studied deployed 
caching search engines studied levels 
level caching result caching takes place frontend deals case identical queries issued repeatedly different users 
keeping cache results returned engine filter repeated queries workload increase throughput 
result caching studied :10.1.1.122.203
gives measurable benefit low cost result simply stored complete html page kb benefit limited degree repetition input stream 
lower level list caching keep inverted lists corresponding frequently search terms main memory resulting additional benefits engines disk index structures 
benefits level caching approach actual search engine studied 
propose evaluate level architecture additional intermediate level caching 
level called intersection caching projection caching depending implementation caches inverted list data pairs terms commonly occur queries search terms 
basic idea simple relies fact major search engines default return documents contain search terms 
contrast lot ir community document containing terms participates ranking discuss issue 
search engines need score documents occur intersection inverted lists 
unfortunately cases efficient way find intersection involves complete scan lists dominates cost query processing 
caching pairwise intersections lists typically smaller lists hope significantly reduce cost subsequent queries 
note basic idea caching intersections proposed context search scenario objectives different discussed 
idea caching intersections simple resulting weighted caching problem turns quite challenging 
main technical part discuss evaluate online offline caching algorithms 
restricted classes problem np complete show practical approaches perform better basic landlord algorithm weighted caching typical query traces 
perform evaluation performance caching levels 
caching gives significant boost query throughput level contributes measurably 
section gives technical background section discusses related 
level caching approach described discussed detail section 
section studies resulting intersection caching problem presents basic approaches 
section refines approaches performs detailed experimental evaluation caching levels 
section provides concluding remarks 

search engine query processing section provide background query execution search engines 
assume document collection 
dn web pages crawled available disk 

wm different words occur collection 
typically text string appears separating symbols spaces commas treated valid word term indexing purposes current engines 
indexes inverted index collection consists set inverted lists iw iw 
iw list iw contains posting occurrence word posting contains id document word occurs byte approximate position document possibly information context title large bold font anchor text word occurs 
postings inverted list sorted document ids enables compression list 
boolean queries implemented unions intersections lists phrase searches new york answered looking positions words 
refer details :10.1.1.51.7802
queries query 
td set terms words 
simplicity ignore search options phrase searches queries restricted certain domains point 
caching problems long sequence queries 
ql qi 

term ranking common way perform ranking ir systems comparing words terms contained document query 
precisely documents modeled unordered bags words ranking function assigns score document respect current query frequency query word page collection length document context occurrence higher score term title bold face 
formally ranking function function query 
td assigns document score 
system returns documents highest score 
popular class ranking functions cosine measure example ti ti ln ft ln fd fd ft frequency term document entire collection respectively :10.1.1.51.7802
ranking functions proposed techniques limited particular class 
vs ranking function studied ir community including cosine measure require document contain query terms order returned results 
document containing query terms multiple times title may score higher document containing terms 
search engines enforce semantics queries consider documents containing query terms 
done various reasons involving user expectations collection size preponderance short queries queries documents containing query terms 
approach fundamentally depends semantics default essentially major engines google altavista inktomi lycos 
query execution inverted index query executed computing scores documents intersection inverted lists query terms 
efficiently done document time approach simultaneously scan inverted lists usually sorted document id compute scores document encountered lists 
shown approach efficient time approach process inverted lists 
scores computed en materializing intersection lists top scores maintained heap structure 
case semantics cost perform ing arithmetic operations computing scores dominated cost traversing lists find documents intersection intersection usually smaller complete lists 
search engines number additional factors standard cosine type ranking functions context term occurs title url bold face term distance documents terms occur close far apart text link analysis user feedback 
factors easily included computing scores outlined 
commonly way integrate factors precompute global importance score document done pagerank importance scores different topic groups simply add scores term scores query execution :10.1.1.14.4723:10.1.1.109.4049
approach depend ranking function long total cost dominated inverted list traversal 
search engine architecture major search engines large clusters servers connected high speed lans query typically executed parallel number machines 
particular current engines usually employ local index organization machine assigned subset documents builds inverted index subset 
user queries received frontend machine called query integrator broadcasts query participating machines 
machine returns local top results query integrator determine top documents :10.1.1.122.203
subset collection replicated indexed nodes multiple independent query integrators 
note alternative partitioning approaches global index organization various hybrids commonly large engines may advantages certain scenarios see discussion :10.1.1.12.6196:10.1.1.111.6826
query processing optimizations simple mechanisms load balancing concurrency machine local index organization results highly efficient parallel processing 
problem optimizing throughput reduces single node case maximize number queries second processed locally machine reasonable response time 
commonly technique compress inverted list various coding techniques reducing disk index structures increasing cpu :10.1.1.51.7802
tradeoff fairly simple fast techniques tend outperform schemes geared optimal compression 
experiments compression depend 
optimizations attempt determine top results complete scan intersection union inverted lists lists contributions score terminating traversal early removing low scoring postings index altogether 
significant amount ir database communities issue various scenarios see :10.1.1.14.4723
various schemes apparently current engines details closely guarded 
note techniques designed certain types ranking functions easily support term distance documents 
experiments full traversal list intersections approach adapted pruned schemes scope 
third common optimization caching schemes discussed detail section 

discussion related background indexing query execution ir search engines see 
basics search engine architecture refer :10.1.1.122.203
focus previous caching issues directly relevant 
result caching indicated result caching filters repetitions query stream caching complete results previous queries limited amount time 
studied probably major engines :10.1.1.122.203
result caching easily implemented query integrator proposes caching results internet closer user 
connects result caching problem efficiently returning additional result pages query efficiently done computing storing just results query 
result caching works completely identical queries limited benefits query stream 
easy implement give significant benefits simple caching policies 
side effect result caching average number search terms increases queries executed single term term queries cached 
list caching lower level inside machine frequently accessed inverted lists cached main memory save done transparently file system database system berkeley db store index typical ir web search workloads better results may achievable specialized caching policies 
course list caching applies disk resident index structures engines attempt keep index main memory optimum performance 
level caching evaluate level caching architecture result list caching search engine show level contributes significantly benefit 
list caching simple lru approach 
note possible techniques similar major engines type information usually kept highly confidential aware 
caching search basic idea caching results intersections level caching approach proposed context peer peer search 
note approach quite different 
main goal avoid repeated transmissions inverted list data peer peer system global index organization interested improving query throughput node decreasing disk traffic cpu load 
main emphasis distributed data structures keeping track intersections cached system case problem easily solved standard local data structure 
emphasis intersection caching level cluster architecture different algorithms cost trade offs peer peer environment performance large query load real engine 
pointed similarity views join indexes database systems 
set intersections note scenarios course efficient ways intersect lists scan particular lists different lengths 
inverted indexes true ratio list lengths quite large order thousands tens thousands depending decoding cost account 
adaptive set intersection techniques disk resident structures limited benefits possible main memory peer peer environments 
optimizations phrases caching intersections related problem building optimized index structures phrase queries new york 
particular intersections evaluate phrase queries hand profitable pairs lists intersection caching turn common phrases 
note exhaustive index structures word phrases small constant factor overhead standard index occurrence word directly followed word 
caching intersections terms hand impossible appropriate caching policies needed 
weighted caching caching problems benefit caching object proportional size caching avoid disk network traffic 
weighted caching problems deal case object size benefit may completely independent size 
weighted caching problems studied propose analyze simple algorithm called landlord basically assigns leases objects size benefit evicts object earliest expiring lease 
perform competitive analysis landlord algorithm experimental results web caching scenario unrelated search 
case dealing weighted caching problem size cached object size intersection projection benefit difference size sizes complete lists 
cost inserting object cache requires employ appropriate cache admission policies 

level caching approach describe discuss proposed level caching architecture detail 
architecture motivated simple observations available search engine logs 
particular result caching works single term term queries perform queries terms exactly repeated 
analysis large query logs indicates queries terms contain pair terms previously appeared 
term query processed scanning inverted list ia term cached list intersection ib ic 
lists ia ib ic approximately length intersection ib ic smaller lists save factor pair having occurred previously 
pairs previously occurred scanning intersections save cost query 
level caching architecture result caching query integrator list caching main memory node intersection caching disk 
discuss exact format treatment cached intersections 
combining result intersection list caching get level caching architecture shown summarized follows result caching query integrator maintains cache results queries memory disk 
cache size eviction policy typically critical large numbers results cached cheaply 
query log queries results cached essentially entire log 
queries covered result caching broadcast query processing nodes 
intersection caching node certain amount extra space say disk space index reserved caching intersections 
intersections reside disk basically treated part inverted index separate inverted index 
query check pairwise intersections cached process query 
addition processing create en additional intersections pairs terms query add cache 
show done efficiently 
list caching lowest level limited amount main memory typically mb nodes gb memory cache frequently accessed inverted lists intersections 
intersection caching complements result caching focuses queries terms orthogonal list caching 
intersection caching relevant disk memory index structures performance ramifications somewhat different see 
intersection vs projection caching discuss precise format cached intersections 
recall inverted list sequence postings sorted document id posting containing document id additional information occurrence term document 
order intersection original list query execution data preserved postings document ids appear lists 
posting intersection list consist document id information occurrences words document 
implementation decided follow slightly different approach call projection caching 
creating intersection lists ia ib create projections ia ib ia contains postings ia document id appears ib ib vice versa 
advantages approach projected inverted lists exactly format inverted lists changes query processor required 
creation projections complete lists simple 
ia ib treated independently list intersection caching mechanisms evicted separately desirable cases 
additional minor optimizations possible query execution query executed ia ib ic pairs 
disadvantage projections projections slightly larger single intersection document id stored twice 
decided projections query processor advantages outweigh slight space penalty 
note results stated terms intersection projection caching performance schemes comparable 
caching overheads common assumption caching object result cost apart limited administrative overhead data structures space caching 
context creation projections caching piggybacked query execution involves inverted lists retrieved disk query processor anyway 
reality costs associated creating projections inserting cache need taken account order get performance 
addition small overhead making caching query execution decisions significant costs write cost projection cache disk newly inserted projection written disk 
encoding cost writing projection encoded index compression scheme inverted index 
projection creation projections created en execution query additional disk accesses certain cpu overhead due necessary changes query processor 
report cost experiments terms number blocks written show kept fairly low level compared savings read costs 
second cost typically fairly small provided fast compression scheme index 
case variable byte compression scheme evaluated achieves compression low cost 
subtle issue cpu overhead creating projections 
show online schemes costs kept low level adopting suitable cache admission policy prevents creation projections evicted cache 
part experimental evaluation report results terms logical disk block accesses including disk reads query processing disk writes adding projections cache ignoring caching lists main memory 
gives rough view relative performance various schemes 
subsection discuss detail cpu savings overhead due projection creation subsection evaluates effect adding list caching 
note optimal choice caching policies depends relative speeds disk cpu choice compression scheme index primarily disk memory unable evaluate cases limited space 
show significant performance gains possible primarily memory index structures overhead schemes low 

basic policies intersection caching section study cache maintenance policies intersection caching 
define problem discuss complexity issues greedy algorithm offline version problem describe landlord algorithm weighted caching 
problem definition complexity recall sequence queries 
ql qi 

query 
td executed generate cache projections subject maximum cache size size 
query executed scanning currently cache 
cost executing query equal sum lengths lists scanned goal minimize total query execution cost 
note results section stated terms intersections projections 
offline version problem assume sequence queries known ahead time set projections cache selected created start execution 
online version queries time projections created cached execution queries described evicted point time 
simplicity charge cost creating projections definition experiments consider issue 
offline version difficult see problem np complete reduction subset sum caching problems allow arbitrary object sizes 
observation really capture full complexity problem 
strengthen result follows 
theorem 
offline problem np complete case projections size queries limited terms 
proof sketch reduction vertex cover 
graph integer construct instance caching problem follows 
term tu node addition special term edge create query tu tv 
assume projections terms size achieved making lists disjoint small set doc ument ids appear lists select cache size fits exactly projections 
assume list significantly larger itu say itu 
exists vertex cover size iff exists selection cached projections allows query trace executed total cost 
note projections size queries limited terms problem solved polynomial time 
hand offline problem terms remains np complete allow creation eviction projections query execution charge cost creation projections 
discuss online problem 
simple greedy algorithm reality prior knowledge query sequence offline problem practical interest cache space assignments analysis issued queries 
reason describe simple greedy algorithm offline problem step adds projection cache maximizes ratio additional savings query processing projection size 
implemented follows 
query qi sequence projection qi create entry 
note fields size projection benefit full list 

combine entries identical single additional field called total benefit contains sum benefits combined entries sequence query numbers combined entries attached new entry 

load entries heap allows extraction element maximum ratio total benefit projection size 

repeatedly extract element add cache 
fit discard element choose heap empty 
extraction entry decrease total benefit projections appear common query 
size benefit projection efficiently estimated simple sampling techniques scan inverted lists really required 
ignoring estimation costs algorithm runs time lg worst case sum squares query sizes 
practice moderate constant times number queries queries short 
landlord algorithm consider online problem 
projection caching problem instance weighted caching problem objects arbitrary sizes caching object results savings independent linear sizes 
problems studied class algorithms called landlord proposed analyzed competitive analysis 
note problem comes additional twist reminiscent view selection problems databases ia ia executing query little benefit 
projection frequently occurring pair may benefit may better projections available cache queries applicable 
landlord works follows 
object inserted cache assigned deadline ratio benefit size 

object needs evicted room new evict element smallest deadline dmin dmin deadlines elements currently cache 

element cache deadline reset appropriate value discussed 
note step dmin entries algorithm best implemented summing values dmin deducted far sum properly account 
algorithm highly efficient 
step deadlines reset original value ratio benefit size algorithm seen generalization lru weighted caching problems 
algorithm shown competitive optimum solution analysis carry problem due twist 
experiment variations landlord approach perform better workload 

experimental evaluation experimental setup give baseline results basic versions greedy landlord algorithms 
subsection evaluate modified policies improved performance subsection discusses cpu overhead projection creation 
subsection presents evaluation levels caching 
data sets experimental setup experiments subset pages selected random crawl web pages crawled web crawler october 
subset size corresponds scenario pages evenly distributed node search engine typical setup lab 
case projection caching occurs individual node machine cluster 
uncompressed size pages gb duplicate elimination indexing obtained inverted index structure size gb 
current commercial engines index pages partitioned replicated thousands machines 
believe setup pages node realistic scenario 
queries taken large log queries issued excite search engine pst december 
experiments removed queries stopwords words appear data collection 
number remaining queries total different words average number words query 
assume result cache infinite size query log general expect result cache size eviction policy critical terms system design resources 
created different experimental setups evaluate query processing costs 
setup measured disk access costs various policies terms total number kb disk block accesses 
provides reasonable estimate cost actual query processor disks fast cpu 
setup preloaded inverted lists main memory measure cpu costs methods important systems bandwidth 
experiments run dell gx machine ghz gb memory gb disks running linux 
query characteristics look distribution ratios total costs queries various numbers terms issuing queries query processor caching completely turned 
see result caching nearly half total block access cost spent queries terms queries represent queries 
percentage total queries total cost words query distribution frequencies total costs queries different numbers terms 
look distribution changes filter repeated queries result caching 
shows number queries result caching different numbers terms query 
see number queries words reduced significantly fewer queries terms filtered 
result caching higher percentage total block accesses spent long queries average cost remaining query increases blocks 
ran preliminary measurements projection caching infinite cache size estimate maximum potential benefit adding feature 
results omitted due space constraints showed small number queries completely covered projections meaning term projection complete list queries projections 
particularly case longer queries expect projection caching combination result caching 
queries queries result caching queries result caching words query number queries result caching 
results greedy algorithm results basic versions greedy landlord algorithms 
experiments sure warm levels caches running query log measuring performance queries number cases 
costs stated average number blocks scanned query filtered result caching list caching improve performance 
baseline projection caching block accesses query 
performance caching levels evaluated subsection 
experiment greedy algorithm previous section window queries training window directly preceding queries measured evaluation window 
queries analyzed greedy algorithm allocate space cache projections encountered projections allowed cache 
different ways approach analyzing queries training window preload projection cache projection selected greedy algorithm 
done say day night large bulk operation order improve performance peak hours 
second approach create selected projections encounter corresponding pair evaluation window 
see performance approaches similar range cache sizes 
online method benefits projection caching second time pair encountered evaluation window projection blocks query online creation precomputed cache size gb block accesses query greedy projection caching various cache sizes 
zero cache size means projection caching 
shown curves case precomputed projections created online 
cache size blocks query table cost online projection creation kb block writes query various amounts cache space gb 
created time 
method benefits occurrence uses half cache space precomputed projections evaluation window 
note cache size plotted maximum cache size assumed greedy algorithm amount cache filled projections lower size assumed greedy algorithm online case 
observe gb cache size index size get significant improvement blocks query 
online case include overhead due creation intersections shown table number blocks written query fairly small 
case ignore cost preloading projections 
results indicate probably gained precomputing projections online approach preferable 
fact difficult improve results online case sliding window approach avoids starting empty cache start new window 
show online policies tuning landlord approach perform better omit discussion optimizations greedy approach 
performance basic landlord show results basic landlord algorithm reset deadlines original values cached projection 
observe improved performance terms number blocks scanned query compared greedy algorithm partly due warm cache start measurement period 
amount block writes landlord shown second graph top read costs quite high large number projections created quickly evicted cache 
take overhead account basic landlord approach provide benefit cache sizes compared projection caching 
subsection refinements basic landlord approach dramatically reduce overheads approach improving block read costs 
main idea need appropriate cache admission policy blocks query blocks scanned blocks scanned written cache size gb block reads lower graph block reads plus writes upper graph executed query projection caching different amounts cache space basic landlord approach 
addition cache eviction policy prevent unprofitable projections generated place 
note different caching scenarios web caching may preferable just add encountered objects cache rely eviction policy weed useless entries 
optimized landlord policies consider engineer basic landlord policy improve performance 
evaluate main ideas setting generous deadline projections cache renewed limiting number projections generated inserting possible projection encounter 
due space constraints give sample representative experimental results 
landlord parameters basic landlord approach projection deadline reset original value 
order give boost projections proved useful versus newly encountered pairs terms decide give longer deadline projections reset 
gets longer lease just moves 
particular renewed projection gets original deadline plus fraction remaining deadline 
addition experimented keeping different fraction remaining deadline second subsequent 
experimented number values performance values achieve similar results 
cache admission policies experimented techniques limiting number projections created inserted cache 
simple approach insert projection pair terms previously encountered say hours 
refined rule take account cost projection creation amount benefit results projection full lists 
experimenting arrived policy 
choose window previous queries maintain statistics encountered queries 
projection ia inserted cache corresponding pair terms occurred ia ia ia times queries parameter 
works quite 
particular means insert pair terms previously occurred queries 
decreasing result fewer insertions cache projection ia smaller list ia inserted occurred repeatedly 
choice done follows 
select insertion overhead willing tolerate say blocks projections written query average 
start initial value say periodically evaluate average number blocks written query increase decrease adjust discrepancy 
converges quickly results number block writes query close target comparison projection caching leftmost bar gb cache size bars gb cache size rightmost bars various refinements landlord 
cache size show read cost solid portion bar write cost outlined policies left right basic landlord insertion second encounter pair alphas cache admission policy described 
case choose resulting negligible fact invisible chart write cost 
results optimizations shown indicate fine tuning policies extremely important problem 
best approach rule performs slightly block reads query processing extremely conservative inserting projections minimizes write costs cache 
subsection show results small cpu overhead piggybacked projection creation query processing 
hybrid method experimented hybrid landlord greedy method shows promising results perform better completely engineered bottlenecks 
hybrid sliding window approach periodically queries greedy algorithm analyze certain window queries say queries 
projection chosen greedy algorithm marked protected cached evicted unprotected run greedy algorithm 
simultaneously run landlord optimizations utilize cache space claimed greedy algorithm 
experimented various ratios cache size greedy method versus total cache details omitted due space constraints 
cpu overhead creating projections address cpu overhead creating projections query processor 
need understand query processor generates intersections inverted lists normal query execution 
done document time manner simultaneously scanning lists 
precisely scan element shortest list search forward matching element second shortest list 
element search forward list return shortest list 
queries keywords quite common forward searches longest list skip blocks time appropriate indexing scheme 
rare skips long improve disk performance modern hard disks inverted list laid sequentially disk optimized scanning 
skipping blocks result savings list decoding avoid decoding entire list assuming blocked coding scheme 
generating projection say longest shortest list decode blocks resulting higher cpu cost normal query processing 
additional cpu cost related size created projection policies decrease number block writes created projections tend terms cpu performance 
percentage cpu time projection creation tuples encoded decoded cpu time disk access target overhead query cost query optimized landlord relative query processor projection caching 
show number tuples encoded decoded cpu time number disk blocks accessed 
plot cpu cost assumption projection generation free small values target overhead close total cpu cost indicating projection generation done en essentially free query execution 
show cost query processing optimized projection caching versus query processor projection caching measured executing queries memory resident inverted lists 
see policies low conservative generating projections perform terms cpu cost cpu cost closely correlates total number tuples encoded decoded 
projection caching performs additional decoding tuples creation projections saves decoding projections queries 
observe decrease cpu cost implying increase query throughput cpu limited system benefit disk bound systems higher 
experimented block sizes blocked compression scheme observed similar relative behavior bytes kb 
absolute terms smaller block sizes result lower cpu cost query processing decode fewer postings relative benefit projection caching 
evaluation multi level caching evaluate query processing performance levels caching 
suggested lru list caching 
inverted lists corresponding projections treated list caching mechanism just inverted list turns perform best 
projection caching optimized version landlord previous subsection negligible overhead generating projections online 
note report average number block reads queries including filtered result caching 
allows comparison levels caching 
blocks query blocks query cache rc rc pc rc lc rc pc lc rc lc rc pc lc list cache mb list cache gb list cache number block reads query schemes left right caching result caching result plus projection caching result plus list caching levels caching mb list cache result plus list caching levels caching gb list cache 
caching blocks read query reduced blocks just result caching surviving query having cost blocks shown 
brought blocks query adding projection caching 
result caching list caching mb list cache get performance blocks reduced blocks adding projection caching 
note cache mb total index size gb example mainly disk bound setup 
hand list cache hold total index shown rightmost bars disk access decreases low level means cpu primary bottleneck 
case projection caching increases disk accesses reduces cpu significantly shown previous subsection desirable outcome case 
essentially disk bottleneck better fetch small projection disk larger inverted list memory 

concluding remarks proposed new level caching architecture web search engines improve query throughput 
architecture introduces new intermediate caching level search engines query semantics including essentially current major engines exploit redundancies query stream captured result list caching level architectures 
experimental evaluation large query log excite search engine showed significant improvements performance due extra level caching 
actual performance highly dependent selection caching policies system bottlenecks particular architecture 
open questions arise 
particular interesting perform formal study offline online intersection caching problems defined 
example study approximation results greedy heuristic competitive ratios landlord approach scenario look case include cost generating projections corresponding weighted caching problem 
interesting theoretical question concerns performance caching schemes certain classes input sequences sequences follow zipf distributions term frequencies 
appears simple lru scheme previously best possible policy list caching 
fact seen interesting improvements adaptations landlord algorithm parameters list caching 
note approach related megiddo modha caching policies outperform lru variety applications 
currently studying list caching policies detail 
practical side expect additional tuning caching policies availability larger traces show additional gains plan fully integrate intersection caching existing high performance query processor 
open question concerns relationship intersection caching specialized index structures common phrases 
interesting evaluate combinations caching pruning techniques 
believe integrating projection caching pruning techniques difficult reasons discussed projection treated just inverted list index 
second observed choice policies parameters overhead generating projections tiny small aggressive pruning brings baseline cost 
note search engines appear distance query terms page important factor ranking 
knowledge published apply pruning types ranking functions simple combination scores different terms 

anh moffat 
vector space ranking effective early termination 
proc 
th annual sigir conf 
research development information retrieval pages sept 
anh moffat 
compressed inverted files reduced decoding overheads 
proc 
st annual sigir conf 
research development information retrieval pages 
arasu cho garcia molina raghavan 
searching web 
acm transactions internet technologies june 
baeza yates ribeiro neto ziviani 
distributed query processing partitioned inverted files 
proc 
th string processing information retrieval symposium spire sept 
baeza yates ribeiro neto 
modern information retrieval 
wesley 
williams zobel 
efficient phrase querying auxiliary index 
proc 
th annual int 
acm sigir conf 
research development information retrieval pages 
bhattacharjee chawathe gopalakrishnan keleher 
efficient peer peer searches result caching 
proc 
nd int 
workshop peer peer systems 
brewer 
lessons giant scale services 
ieee internet computing pages august 
brin page :10.1.1.109.4049
anatomy large scale hypertextual web search engine 
proc 
seventh world wide web conference 
broder 
resemblance containment documents 
compression complexity sequences pages 
ieee computer society 
cao irani 
cost aware www proxy caching algorithms 
usenix symp 
internet technologies systems usits 
chaudhuri gravano 
optimizing queries multimedia repositories 
data engineering bulletin 
demaine lopez ortiz munro 
adaptive set intersections unions differences 
proc 
th annual acm siam symp 
discrete algorithms pages 
fagin 
combining fuzzy information multiple systems 
proc 
acm symp 
principles database systems 
fagin carmel cohen maarek 
static index pruning information retrieval systems 
proc 
th annual sigir conf 
research development information retrieval pages sept 
fagin lotem naor 
optimal aggregation algorithms middleware 
proc 
acm symp 
principles database systems 
garey johnson 
computers intractability guide theory np completeness 
wh freeman 
haveliwala 
topic sensitive pagerank 
proc 
th int 
world wide web conference may 
jonsson franklin srivastava 
interaction query evaluation buffer management information retrieval 
proc 
acm sigmod int 
conf 
management data pages june 
kaszkiel zobel sacks davis 
efficient passage ranking document databases 
acm transactions information systems tois oct 
lempel moran :10.1.1.122.203
optimizing result prefetching web search engines segmented indices 
proc 
th int 
conf 
large data bases aug 
lempel moran 
predictive caching prefetching query results search engines 
proc 
th int 
world wide web conference 
li loo hellerstein kaashoek karger morris 
feasibility peer peer web indexing 
proc 
nd int 
workshop peer peer systems 
long suel 
optimized query execution large search engines global page ordering 
proc 
th int 
conf 
large data bases september 
markatos 
caching search engine query results 
th international web caching content delivery workshop may 
megiddo modha 
outperforming lru adaptive replacement cache 
ieee computer pages april 
melnik raghavan yang garcia molina 
building distributed full text index web 
proc 
th int 
world wide web conference may 
persin zobel sacks davis 
filtered document retrieval frequency sorted indexes 
journal american society information science may 
richardson domingos 
intelligent surfer probabilistic combination link content information pagerank 
advances neural information processing systems 

multi tier architecture web search engines 
latin american web congress pages 

search engines web dynamics 
computer networks 
de moura ziviani fonseca ribeiro neto 
rank preserving level caching scalable search engines 
proc 
th annual sigir conf 
research development information retrieval pages sept 
williams zobel 
compression inverted indexes fast query evaluation 
proc 
th annual sigir conf 
research development information retrieval pages 
shkapenyuk suel 
design implementation high performance distributed web crawler 
proc 
int 
conf 
data engineering 
suel mathur wu zhang long shanmugasundaram 
peer peer architecture scalable web search information retrieval 
international workshop web databases webdb june 
tomasic garcia molina 
performance inverted indices distributed text document retrieval systems 
proc 
nd int 
conf 
parallel distributed information systems pdis 
witten moffat bell :10.1.1.51.7802
managing gigabytes compressing indexing documents images 
morgan kaufmann second edition 
xie hallaron 
locality search engine queries implications caching 
ieee infocom pages 
young 
line file caching 
proc 
th annual acm siam symp 
discrete algorithms pages 
