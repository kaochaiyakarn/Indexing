ieee transactions pattern analysis machine intelligence vol 
september simultaneous feature selection clustering mixture models martin law student member ieee rio figueiredo senior member ieee anil jain fellow ieee clustering common unsupervised learning technique discover group structure set data 
exist algorithms clustering important issue feature selection attributes data clustering algorithms rarely touched 
feature selection clustering difficult supervised learning class labels data obvious criteria guide search 
important problem clustering determination number clusters clearly impacts influenced feature selection issue 
propose concept feature saliency introduce expectation maximization em algorithm estimate context mixture clustering 
due minimum message length model selection criterion saliency irrelevant features driven zero corresponds performing feature selection 
criterion algorithm extended simultaneously estimate feature saliencies number clusters 
index terms feature selection clustering unsupervised learning mixture models minimum message length em algorithm 
goal clustering discover natural grouping set patterns points objects knowledge class labels 
clustering cluster analysis prevalent discipline involves analysis multivariate data 
course impractical exhaustively list numerous uses clustering techniques 
image segmentation important problem computer vision formulated clustering problem :10.1.1.160.2324
documents clustered generate topical hierarchies information access retrieval 
clustering perform market segmentation biology study genome data 
clustering algorithms proposed different application scenarios 
divided roughly categories hierarchical clustering creates tree branches merging different levels partitional clustering divides data different flat clusters 
input clustering algorithms proximity matrix containing similarities dissimilarities pairs points pattern matrix item described vector attributes called features 
shall focus partitional clustering pattern matrix input 
principle information pattern better clustering algorithm expected perform 
suggest features possible represent patterns 
case practice 
features just noise law jain department computer science engineering michigan state university engineering building east lansing michigan 
mail jain cse msu edu 
figueiredo instituto de es instituto superior cnico torre av 
pais lisboa portugal 
mail mtf lx pt 
manuscript received may accepted feb 
recommended acceptance frey 
information obtaining reprints article please send mail tpami computer org ieeecs log number tpami 
contributing degrading clustering process 
task selecting best feature subset known feature selection variable selection subset selection 
feature selection important reasons fundamental arguably noisy features degrade performance learning algorithms see example fig 

supervised learning known feature selection improve performance classifiers learned limited amounts data leads economical storage computation classifiers cases may lead interpretable models 
feature selection particularly important data sets large numbers features classification problems molecular biology may involve thousands features web page represented thousands different key terms :10.1.1.141.6372
appearance image classification methods may pixel feature easily involving thousands features 
feature selection widely studied context supervised learning see ultimate goal select features achieve highest accuracy unseen data :10.1.1.30.525
feature selection received comparatively little attention unsupervised learning clustering 
important reason clear assess relevance subset features resorting class labels 
problem challenging number clusters unknown optimal number clusters optimal feature subset interrelated illustrated fig :10.1.1.28.2970
taken :10.1.1.135.3582
note methods variance principal components analysis need select features clustering features large variance independent intrinsic grouping data see example fig 

feature selection algorithms involve combinatorial search space feature subsets :10.1.1.30.525
usually heuristic methods adopted size space ieee published ieee computer society law simultaneous feature selection clustering mixture models fig 

uniformly distributed irrelevant feature difficult gaussian mixture learning algorithm recover underlying clusters :10.1.1.28.2970:10.1.1.28.2970
feature clusters easily identified 
curves horizontal vertical axes indicate marginal distribution respectively 
fig 

feature explaining data variance feature spurious identification clusters data set 
previous attempts solve feature selection problem unsupervised learning 
details approach section 
experimental results reported section followed comments proposed algorithm section 
conclude section outline directions 
fig 

number clusters interrelated feature subset 
optimal feature subsets identifying clusters data set fx fx fx respectively 
hand optimal number clusters feature subsets fx fx fx respectively 
exponential number features 
case generally loses guarantee optimality selected feature subset 
propose solution feature selection problem unsupervised learning casting estimation problem avoiding combinatorial search 
selecting subset features estimate set real valued quantities feature call feature saliencies 
estimation carried em algorithm derived task 
presence model selection type problem necessary avoid situation saliencies take maximum possible value 
achieved adopting minimum message length mml penalty done select number clusters :10.1.1.28.2970:10.1.1.28.2970
mml criterion encourages saliencies irrelevant features go zero allowing prune feature set 
integrate process feature saliency estimation algorithm proposed obtaining method able simultaneously perform feature selection determine number clusters :10.1.1.28.2970:10.1.1.28.2970:10.1.1.28.2970
algorithm respect gaussian mixture clustering extend types model clustering 
algorithm appears 
remainder organized follows section review approaches feature selection related literature feature selection pertains supervised learning classification regression 
feature selection algorithms broadly divided categories filters wrappers :10.1.1.30.525
filter approaches evaluate relevance feature subset data set regardless subsequent learning algorithm 
relief enhancement representatives class basic idea assign feature weights consistency feature value nearest neighbors data point 
informationtheoretic methods evaluate features mutual information relevant feature class labels high 
nonparametric methods compute mutual information involving continuous features 
feature regarded irrelevant conditionally independent class labels features 
concept markov blanket formalize notion 
hand wrapper approaches invoke learning algorithm evaluate quality feature subset :10.1.1.30.525
specifically learning algorithm nearest neighbor classifier decision tree naive bayes method run feature subset feature subset assessed estimate classification accuracy 
wrappers usually computationally demanding superior accuracy compared filters ignore properties learning task hand :10.1.1.28.2970:10.1.1.30.525
approaches filters wrappers usually involve combinatorial searches space possible feature subsets task different types heuristics sequential forward backward searches floating search beam search bidirectional search genetic search suggested :10.1.1.30.3522:10.1.1.30.525
possible construct set weak boosting sense classifiers feature apply boosting effectively performs feature ieee transactions pattern analysis machine intelligence vol 
september selection 
proposed approach feature selection rough set theory 
approaches mentioned concerned feature selection presence class labels 
comparatively done feature selection unsupervised learning 
course method conceived supervised learning class labels unsupervised learning case methods measure feature similarity detect redundant features mutual information maximum information compression index 
normalized log likelihood cluster separability evaluate quality clusters obtained different feature subsets :10.1.1.135.3582
different feature subsets numbers clusters multinomial model clustering evaluated marginal likelihood likelihood 
algorithm described uses automatic relevance determination priors select features clusters :10.1.1.28.2970
clustering tendency feature assessed entropy index 
genetic algorithm feature selection means clustering 
feature selection symbolic data addressed assuming irrelevant features uncorrelated relevant features 
describes notion category utility feature selection conceptual clustering task 
clique algorithm popular data mining community finds shaped clusters subset attributes large database :10.1.1.131.5152:10.1.1.131.5152
wrapper approach adopted select features clustering explored earlier 
methods referred perform hard feature selection feature selected 
algorithms assign weights different features indicate significance 
weights assigned different groups features means clustering score related fisher discriminant 
feature weighting means clustering considered goal find best description clusters identified 
method described classified learning feature weights conditional gaussian networks 
em algorithm bayesian shrinking proposed unsupervised learning 
em algorithm feature saliency section propose em algorithm performing mixture model clustering feature selection 
mixture clustering data point modeled having generated set probabilistic models 
clustering done learning parameters models associated probabilities 
pattern assigned mixture component generated 
derivations refer gaussian mixtures generalized types mixtures 
mixture densities finite mixture density components defined xk yj set parameters jth component components assumed form gaussian kg denote full parameter set 
goal mixture estimation infer set data points fy assumed samples distribution density 
yi dimensional feature vector yi sequel indices run data points mixture components features respectively 
known maximum likelihood ml estimate ml arg yj maximum posteriori map estimate prior map arg yj log analytically 
usual choice em algorithm finds local maxima criteria 
algorithm set fz missing latent labels zi zi zik zij zip meaning sample 
brevity notation write zi zi 
complete data log likelihood log likelihood observed log zj xn zij log jp ij em algorithm produces sequence estimates alternating steps step compute zjy expected value missing data current parameter estimate plug log zj yielding socalled function log wj 
elements binary pr zij yi wi bj pk bk notice priori probability zij belongs cluster wij corresponding posteriori probability observing step update parameter estimates arg max fq log case map estimation log ml case 
feature saliency section define concept feature saliency derive em algorithm estimate value 
assume features conditionally independent hidden component label law simultaneous feature selection clustering mixture models fig 

graphical model probability model case features different indicator variables 
corresponds existence arc yl corresponds absence 


yj xk yj xk ylj jl jl pdf lth feature jth component 
assumption enables utilize power em algorithm 
particular case gaussian mixtures conditional independence assumption equivalent adopting diagonal covariance matrices common choice high dimensional data na bayes classifiers latent class models emission densities continuous hidden markov models 
different definitions feature proposed supervised learning adopt suggested suitable unsupervised learning lth feature irrelevant distribution independent class labels follows common density denoted ylj 
set binary parameters feature relevant 
mixture density rewritten yj jg lg xk ylj jl ylj related model feature selection supervised learning considered 
intuitively determines edges exist hidden label individual features yl graphical model illustrated fig 
case 
notion feature saliency summarized steps treat ls missing variables define feature saliency probability lth feature relevant 
definition sense difficult know sure certain feature irrelevant unsupervised learning 
resulting model likelihood function written see proof appendix form reflects prior knowledge distribution features 
principle distribution gaussian student mixture 
shall limit gaussian leads reasonable results practice 
equation generative interpretation 
standard finite mixture select component label sampling multinomial distribution parameters 
feature flip biased coin probability getting head get head mixture component jl generate lth feature common component 
graphical model representation shown fig 
case 
em algorithm treating hidden class labels hidden variables derive see details appendix em algorithm parameter estimation step compute quantities aij jl bij cij wij zi cij cij uij zi jjy aij wij cij vij zi jjy wij uij step reestimate parameters expressions yj xk lp ylj jl ylj ff jg lg lgg set parameters model 
intuitive way see obtained notice ylj jl ylj written ylj jl ylj binary 
fig 

graphical model showing mixture density 
variables hidden observed 
ieee transactions pattern analysis machine intelligence vol 
september bj wij ij wij wij mean jl uij fig 

unsupervised feature saliency algorithm 
var jl mean var bl uij uij uij vij ij vij uij vij mean jl vij mean ij vij uij equations variable uij measures important ith pattern jth component lth feature :10.1.1.28.2970
natural estimates mean variance jl weighted sums weight uij similar relationship exists vij term ij uij interpreted equals explaining estimate proportional ij uij model selection standard em mixtures exhibits weaknesses affect em algorithm introduced requires knowledge initialization essential reaching local optimum 
overcome difficulties adopt approach mml criterion :10.1.1.28.2970:10.1.1.28.2970:10.1.1.28.2970
mml criterion model see details appendix consists minimizing respect cost function discarding order term log yj log log log number parameters jl respectively 
univariate gaussians arbitrary mean variance 
parameter estimation viewpoint equivalent maximum posteriori map estimate arg max log yj rk log rd log log dirichlet type improper priors js ls yk yd rd rk priors conjugate respect complete data likelihood em algorithm undergoes minor modification step replaced rd bj max pi wij max pi wij rd max bl max kr kr max addition log likelihood terms simple interpretations 
term log standard mdl type parameter code length corresponding values values 
lth feature jth component effective number data points estimating jl parameters jl corresponding code length log 
similarly lth feature common component number law simultaneous feature selection clustering mixture models fig 

solid ellipses represent gaussian mixture components dotted ellipse represents common density 
number parenthesis axis label feature saliency reaches common component longer applicable feature :10.1.1.28.2970
common component degenerates line feature saliency feature common density degenerates point :10.1.1.28.2970
data set initialization snapshot pruned local minimum best local minimum 
effective data points estimation 
term log feature 
key property pruning behavior forcing go zero go zero 
pruning behavior indirect benefit protecting singular covariance matrices weight component usually small pruned iterations 
concerns message length may invalid boundary values circumvented arguments goes zero lth feature longer salient kl removed goes dropped :10.1.1.28.2970:10.1.1.28.2970:10.1.1.28.2970
model selection algorithm determines number components initialized large value alleviating need initialization shown :10.1.1.28.2970:10.1.1.28.2970
component wise version em adopted :10.1.1.28.2970:10.1.1.28.2970
algorithm summarized fig 

postprocessing feature saliency feature saliencies generated algorithm fig 
attempt find best way model data different component densities 
alternatively consider feature saliencies best discriminate different components 
appropriate ultimate goal discover separated clusters 
components separated pattern generated component 
quantitative measure separability clusters xn log zi ti arg maxj zi jjy 
intuitively sum logarithms posterior probabilities data assuming data point generated component maximum posterior probability implicit assumption mixture clustering 
maximized varying keeping parameters fixed 
mml criterion optimized em algorithm 
defining gil xk jl ylj lp jl easy show log wij gil log wij xn log wij xn il ieee transactions pattern analysis machine intelligence vol 
september fig 

feature saliencies gaussian data set fig 
trunk data set 
mean values plus minus standard deviation runs shown 
recall features gaussian data set noisy features 
features saliencies gaussian features saliencies trunk 
table real world data sets data set data points features classes 
feature constant value image discarded 
gradient hessian calculated accordingly ignore dependence ti constrained nonlinear optimization software find optimal values 
matlab optimization toolbox experiments 
obtaining set optimized fix estimate remaining parameters em algorithm 
experimental results synthetic data synthetic data set consists data points mixture equiprobable gaussians mi fig 

noisy features sampled density appended data yielding set dimensional patterns 
ran proposed algorithm times initialized common component initialized cover entire set data feature saliency values initialized 
stopping threshold typical run algorithm shown fig 

runs mixture components correctly identified 
saliencies features standard deviations error bars shown fig 

conclude case algorithm successfully locates true clusters correctly assigns feature saliencies 
second experiment consider trunk data dimensional gaussians ffiffi ffiffiffi 
data obtained sampling points gaussians 
note features arranged descending order relevance 
stopping threshold set initial values 
runs performed components detected 
feature saliencies shown fig 

lower rank number important feature 
see general trend feature number increases saliency decreases accordance true characteristics data 
real data tested algorithm data sets different characteristics table 
wine recognition data set wine contains results chemical analysis wines grown different 
goal predict type wine chemical composition data points features classes 
wisconsin diagnostic breast cancer data set obtain binary diagnosis benign malignant features extracted cell nuclei image data points 
image segmentation data set image contains data points features classes pattern consists features extracted region taken types outdoor images sky foliage cement window path grass 
texture data set texture consists dimensional gabor filter features collage brodatz textures 
data set zer zernike moments extracted images handwriting numerals images digit law simultaneous feature selection clustering mixture models table result proposed algorithm random runs error corresponds mean error rates testing set clustering results compared ground truth labels :10.1.1.28.2970
bc denotes number gaussian components estimated 
note postprocessing change number gaussian components 
numbers parenthesis standard deviation corresponding quantities 
totaling patterns 
data sets wine image zernike uci machine learning repository www ics uci edu mlearn html 
normalization zero mean unit variance performed texture data set contribution different features roughly equal priori 
data sets collected supervised classification class labels involved experiment evaluation clustering results 
data set randomly divided halves training testing 
algorithm fig 
run training set 
feature saliency values post processed described section 
evaluate results interpreting components clusters compare ground truth labels 
data point test set assigned component generated pattern classified class represented component 
compute error rates test data 
comparison run mixture gaussian algorithm features number classes data set lower bound number components :10.1.1.28.2970:10.1.1.28.2970
gives fair ground comparing gaussian mixtures feature saliency 
order ensure data respect number features algorithm covariance matrices mixture components restricted diagonal different different components :10.1.1.28.2970:10.1.1.28.2970:10.1.1.28.2970
entire procedure repeated times results shown table 
show feature saliency values different features different runs gray level image maps fig 

table see proposed algorithm reduces error rates compared features data sets :10.1.1.28.2970
improvement significant image data set may due increased number components estimated 
high error rate zernike due fact digit images inherently difficult cluster example written manner similar difficult unsupervised learning algorithm distinguish 
postprocessing increase contrast feature saliencies image maps fig 
show deteriorating accuracy 
easier perform hard feature selection feature saliencies application 
discussion complexity major computational load proposed algorithm step step 
step iteration computes quantities 
quantity computed constant time time complexity step 
similarly step takes time 
total amount computation depends number iterations required convergence 
sight amount computation demanding 
close examination reveals iteration step step standard em algorithm takes time 
value standard em usually smaller proposed algorithm starts larger number components 
number iterations required algorithm general larger increase number parameters 
true proposed algorithm takes time standard em algorithm parameter setting 
proposed algorithm determine number clusters feature subsets 
want achieve goal standard em algorithm wrapper approach need rerun em multiple times different number components different feature subsets 
computational demand heavier proposed algorithm heuristic search guide selection feature subsets 
strength proposed algorithm initialization large number gaussian components algorithm sensitive local minimum problem standard em algorithm 
reduce complexity adopting optimization techniques applicable standard em gaussian mixture sampling data compressing data efficient data structures :10.1.1.19.3377
postprocessing step section computation quantity gradient hessian takes time 
number iterations difficult predict depends optimization routine 
put upper bound number iterations trade speed optimality results 
ieee transactions pattern analysis machine intelligence vol 
september fig 

image maps feature saliency different data sets postprocessing procedure 
feature saliency shown pixel gray level 
vertical horizontal axes correspond feature number trial number respectively 
wine proposed algorithm wine postprocessing proposed algorithm postprocessing image proposed image postprocessing texture proposed algorithm texture postprocessing zernike proposed algorithm zernike postprocessing 
relation shrinkage estimate interpretation regularize distribution feature different components common distribution 
analogous shrinkage estimator covariance matrices class conditional densities weighted sum estimate class specific covariance matrix global covariance matrix estimate 
pdf lth feature weighted sum component specific pdf common density 
important difference weight estimated data mml principle set heuristically commonly done 
shrinkage estimators law simultaneous feature selection clustering mixture models empirical success combat data scarcity regularization viewpoint alternative explanation usefulness proposed algorithm 
limitation proposed algorithm limitation proposed algorithm feature independence assumption conditioned component 
empirically violation independence assumption usually affect accuracy classifier supervised learning quality clusters unsupervised learning negative influence feature selection problem 
specifically feature redundant distribution independent component label feature modeled feature independence assumption 
result features kept 
explains general feature saliencies somewhat high 
postprocessing section cope problem considers posterior distribution discard features help identifying clusters directly 
extension semisupervised learning may knowledge class labels different gaussian components 
happen say adopt procedure combine different gaussian components form cluster semisupervised learning scenario small amount labeled data help identify gaussian component belongs class 
additional information suggest combination gaussian components form single class cluster allowing identification non gaussian clusters 
postprocessing step take advantage information 
suppose know classes posterior probability pattern yi belongs cth class denoted ric computed ric pk zi 
example know components class set zero 
postprocessing modified accordingly redefine ti ti arg maxc ric class label yi view extra information replace log zi log ri ti gradient hessian computed easily noting wij wij log ric log wij wij ric cj wij xk gil ric gil optimize modified carry postprocessing 
note maximizing posterior probability sum logarithm maximum posterior probability considered postprocessing section regarded sample estimate unorthodox type entropy see posterior distribution 
regarded limit renyi entropy tends infinity log xk entropy parameter estimation maximum entropy framework corresponding procedure closely related minimax inference :10.1.1.28.2970
functions posterior probabilities shannon entropy posterior distribution 
preliminary studies show different types entropy affect results significantly 
em algorithm estimate importance different features best number components gaussian mixture clustering 
proposed algorithm avoid running em times different numbers components different feature subsets achieve better performance available features clustering 
usefulness algorithm demonstrated synthetic benchmark real data sets 
avenues 
space complexity proposed algorithm slow algorithm significantly data set size nd large intermediate variables held memory :10.1.1.28.2970
extend algorithm cope challenging problem 
may attempt model dependency different features explicitly 
merging proposed algorithm basically wrapper filter techniques lead hybrid algorithm applicable data sets enormous numbers features 
replace mixture gaussians mixture multinomial distribution making proposed algorithm applicable categorical data 
may extend current algorithm handle different salient features different components 
principles mml variational bayes adopted perform model selection 
appendix mixture model recall conditional density yj xk ylj jl ylj treat set missing variables define set parameters estimated feature saliencies 
assume ls mutually independent independent hidden component label pattern yj xk xk ylj jl ylj lp ylj jl ylj ieee transactions pattern analysis machine intelligence vol 
september marginal density xk xk xk yd lp ylj jl ylj lp ylj jl ylj ylj jl ylj 
note features independent component label appendix deriving em algorithm complete data log likelihood model zi define quantities lp jl wij zi uij zi vij zi calculated current parameter estimate note uij vij wij pn pk wij expected complete data log likelihood log zi log log log log zi log log jl log log log wij log part vij log log jl zi uij log jl part log part uij log vij part parts equation maximized separately 
recall densities univariate gaussian characterized means variances 
result maximizing expected complete data log likelihood leads step 
observe yi lp ylj jl yl jl yl yl jl yl lp ylj jl lp ylj jl follows ylj aij cij zi jjy aij appendix wij cij applying minimum message length minimum message length mml criterion see details arg min log log yj log ji log set parameter model dimension log yj expected fisher information matrix negative expected value hessian log likelihood ji determinant :10.1.1.28.2970:10.1.1.28.2970
information matrix model difficult obtain analytically 
approximate information matrix complete data log likelihood ic :10.1.1.28.2970:10.1.1.28.2970:10.1.1.28.2970
differentiating logarithm show ic block diag di di di kd information matrix distribution parameters 
size ds number parameters jl respectively 
note information bernoulli distribution parameter write log jg xd xk log xk log jl xd log log xd law simultaneous feature selection clustering mixture models prior densities parameters assume different groups parameters independent 
specifically jg different values jl different values different values independent 
furthermore knowledge parameters adopt noninformative jeffrey priors see details proportional square root determinant corresponding information matrices :10.1.1.28.2970:10.1.1.28.2970
substitute ji drop order term obtain final criterion arg min log yj ds log log log acknowledgments research supported onr number 
figueiredo research supported foundation science technology project posi sri 
agrawal gehrke gunopulos raghavan automatic subspace clustering high dimensional data data mining applications proc :10.1.1.131.5152
acm sigmod int conf 
management data pp 

arabie hubert cluster analysis marketing research advanced methods marketing research ed pp 

baldi dna microarrays gene expression 
cambridge univ press 
battiti mutual information selecting features supervised neural net learning ieee trans 
neural networks vol 
pp 

bhatia conceptual clustering information retrieval ieee trans 
systems man cybernetics part vol 
pp 

bins draper feature selection huge feature sets proc 
eighth int conf 
computer vision pp 

blum langley selection relevant features examples machine learning artificial intelligence vol 
nos 
pp 

bradley fayyad reina clustering large database em mixture models proc 
th int conf 
pattern recognition icpr pp 

caruana freitag greedy attribute selection proc 
th int conf 
machine learning pp 

celeux chr tien forbes component wise em algorithm mixtures computational graphical statistics vol 
pp 

carroll feature approach market segmentation overlapping centroids clustering marketing research vol 
pp 

bishop variational bayesian model selection mixture distributions proc 
eighth int conf 
artificial intelligence statistics pp 

dash liu feature selection clustering proc 
pacific asia conf 
knowledge discovery data mining 
devaney ram efficient feature selection conceptual clustering proc 
th int conf 
machine learning pp 

duda hart stork pattern classification 
new york john wiley sons 
dy brodley feature subset selection order identification unsupervised learning proc :10.1.1.135.3582
th int conf 
machine learning pp 

dy brodley kak unsupervised feature selection applied content retrieval lung images ieee trans 
pattern analysis machine intelligence vol 
pp 
mar 
figueiredo jain unsupervised learning finite mixture models ieee trans :10.1.1.28.2970:10.1.1.28.2970
pattern analysis machine intelligence vol 
pp 
mar 
figueiredo jain law feature selection wrapper mixtures proc 
conf 
pattern recognition image analysis pp 

freund schapire decision theoretic generalization line learning application boosting computational graphical statistics vol 
pp 

krishnapuram robust competitive clustering algorithm applications computer vision ieee trans 
pattern analysis machine intelligence vol 
pp 
may 
gustafson thompson de freitas bayesian feature weighting unsupervised learning application object recognition proc 
ninth int workshop artificial intelligence statistics 
iwayama tokunaga cluster text categorization comparison category search strategies proc 
th acm int conf 
research development information retrieval pp 

jain zongker feature selection evaluation application small sample performance ieee trans 
pattern analysis machine intelligence vol 
pp 
feb 
jain dubes algorithms clustering data 
prentice hall 
jain duin mao statistical pattern recognition review ieee trans 
pattern analysis machine intelligence vol 
pp 
jan 
jain unsupervised texture segmentation gabor filters pattern recognition vol 
pp 

jain flynn image segmentation clustering advances image understanding pp 

jain murty flynn data clustering review acm computing surveys vol 
pp 
sept 
kapur measures information applications 
new delhi india wiley 
kim street menczer feature selection unsupervised learning evolutionary search proc 
sixth acm sigkdd int conf 
knowledge discovery data mining pp 

kira rendell feature selection problem traditional methods new algorithm proc 
th nat conf 
artificial intelligence aaai pp 

kohavi john wrappers feature subset selection artificial intelligence vol :10.1.1.30.525
nos 
pp 

koller sahami optimal feature selection proc 
th int conf 
machine learning pp 

komorowski rough sets tutorial rough fuzzy hybridization new method decision making singapore springer verlag 
kononenko estimating attributes analysis extensions relief proc 
seventh european conf 
machine learning pp 


choi input feature selection mutual information parzen window ieee trans 
pattern analysis machine intelligence vol 
pp 
dec 
law jain figueiredo feature selection mixture clustering advances neural information processing systems pp 
cambridge mass mit press 
mclachlan basford mixture models inference application clustering 
new york marcel dekker 
miller subset selection regression 
london chapman hall 
concept learning feature selection square error clustering machine learning vol 
pp 

mitra murthy unsupervised feature selection feature similarity ieee trans 
pattern analysis machine intelligence vol 
pp 
mar 
modha scott feature weighting means clustering machine learning vol 
pp 

ieee transactions pattern analysis machine intelligence vol 
september kittler divergence feature selection multimodal class densities ieee trans 
pattern analysis machine intelligence vol 
pp 
feb 
pelleg moore means extending means efficient estimation number clusters proc :10.1.1.19.3377
th int conf 
machine learning pp 

pena lozano dimensionality reduction unsupervised learning conditional gaussian networks ieee trans 
pattern analysis machine intelligence vol 
pp 
june 
kittler floating search methods feature selection pattern recognition letters vol 
pp 

kittler feature selection approximation class densities finite mixtures special type pattern recognition vol 
pp 

jain small sample size effects statistical pattern recognition recommendations practitioners ieee trans 
pattern analysis machine intelligence vol 
pp 
mar 
rissanen stochastic complexity inquiry 
singapore world scientific 
roberts maximum certainty data partitioning pattern recognition vol 
pp 

roth lange feature selection clustering problems advances neural information processing systems cambridge mass mit press 
sahami machine learning improve information access phd thesis computer science dept stanford univ 
sand moore repairing faulty mixture models density estimation proc 
th int conf 
machine learning pp 

shi malik normalized cuts image segmentation ieee trans :10.1.1.160.2324
pattern analysis machine intelligence vol 
pp 
aug 
dependency feature selection clustering symbolic data intelligent data analysis vol 
pp 

trunk problem dimensionality simple example ieee trans 
pattern analysis machine intelligence vol 
pp 

vaithyanathan dom generalized model selection unsupervised learning high dimensions advances neural information processing systems pp 
cambridge mass mit press 
viola jones rapid object detection boosted cascade simple features proc 
ieee conf 
computer vision pattern recognition vol 
pp 

wallace dowe mml clustering multi state poisson von mises circular gaussian distributions statistics computing vol 
pp 

wallace freeman estimation inference compact coding royal statistical soc 
vol 
pp 

xing jordan karp feature selection high dimensional genomic microarray data proc :10.1.1.141.6372
th int conf 
machine learning pp 

yang honavar feature subset selection genetic algorithm ieee intelligent systems vol :10.1.1.30.3522
pp 

martin law received beng degree degree computer science hong kong university science technology 
working hong kong university hong kong university science technology years moved currently phd candidate department computer science engineering michigan state university 
research interests include data clustering mixture models manifold learning dimensionality reduction kernel methods 
student member ieee 
rio figueiredo sm received ee msc phd degrees electrical computer engineering instituto superior cnico engineering school technical university lisbon portugal respectively 
department electrical computer engineering researcher area coordinator institute telecommunications lisbon 
held visiting position department computer science engineering michigan state university east lansing michigan 
scientific interests include image processing analysis computer vision statistical pattern recognition statistical learning 
received portuguese ibm scientific prize unsupervised image restoration 
associate editor journals pattern recognition letters ieee transactions image processing ieee transactions mobile computing 
guest special issues journals ieee transactions pattern analysis machine intelligence ieee transactions signal processing 
workshops energy minimization methods computer vision pattern recognition local chair joint iapr international workshops syntactical structural pattern recognition statistical pattern recognition 
member program committees international conferences including cvpr icassp icpr 
senior member ieee 
anil jain university distinguished professor departments computer science engineering electrical computer engineering michigan state university 
department chair 
research interests include statistical pattern recognition exploratory pattern analysis texture analysis document image analysis biometric authentication 
papers reprinted edited volumes image processing pattern recognition 
received best awards received certificates outstanding contributions pattern recognition society 
received ieee transactions neural networks outstanding award 
fellow ieee acm international association pattern recognition iapr 
received fulbright research award fellowship alexander von humboldt research award 
delivered pierre devijver lecture sponsored iapr 
holds patents area fingerprint matching 
author books handbook fingerprint recognition springer received psp award association american publishers bio metrics personal identification networked society kluwer object recognition systems elsevier markov random fields theory applications academic press neural networks statistical pattern recognition north holland analysis interpretation range images springer verlag algorithms clustering data prentice hall real time object measurement classification springer verlag 
information computing topic please visit digital library www computer org publications dlib 
