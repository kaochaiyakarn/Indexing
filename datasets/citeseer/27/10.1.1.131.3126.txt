study gram features text categorization johannes rnkranz austrian research institute artificial intelligence wien austria mail ai univie ac technical report tr study effect grams sequences words length text categorization 
efficient algorithm generating gram features benchmark domains newsgroups data set reuters newswire articles 
results rule learning algorithm ripper indicate removal words word sequences length useful 
longer sequences reduces classification performance 
lewis influential thesis lewis machine learning techniques text categorization gained popularity see hearst hirsh sahami 
requirement machine learning algorithms training data represented set feature vectors 
straight forward approach representing text feature vectors set words approach document represented feature vector contains boolean attribute word occurs training collection documents 
word occurs particular training document corresponding attribute set set 
document represented set words consists 
study effect generalizing set words approach word sequences called grams features 
describe algorithm efficient generation frequency pruning gram features section 
section results benchmark tasks ken lang newsgroups data set reuters newswire articles 
results indicate word sequences length usually improve classification related approach bag words approach uses frequencies occurrence individual words feature values 
differences approaches context naive bayes classifiers studied mccallum nigam 
performance longer sequences useful 
show moderate pruning feature set useful heavy frequency pruning results performance decrease studied datasets 
efficiently generating gram features small values number different gram features discovered collection documents increases monotonically gram gram gram starting sequence 
exception final sequence document 
grams occur produce gram different occurrences gram followed different words 
hand similar reasons number occurrences grams decrease increasing number features grows linearly number features certain minimum frequency grow slower 
efficient algorithm generating feature sets avoid generate grams 
implemented algorithm apriori algorithm efficiently generating association rules agrawal 
proposed technique quite similar identical independently developed mladeni grobelnik 
basic idea algorithm utilize user specified lower bound minimum number occurrences feature 
grams occur frequently bound features learning algorithm 
generating pruned feature sets efficiently algorithm exploits simple property sub sequence property number occurrences sequence words document collection bounded number occurrences subsequences 
property exploited order obtain simple efficient algorithm 
gram features generated different passes documents 
pass number occurrences feature counted user specified threshold prune infrequent features 
order avoid combinatorial explosion feature space subsequence property pruning search space count sequences words sequences words previously passed frequency threshold 
sequences ignored 
shows resulting algorithm 
takes parameters collection documents maximum length features lower bound number occurrences feature 
algorithm computes features length occur times documents 
computing result performs passes document collection possible feature length 
principle pass database sufficient 
merely counting occurrences word algorithm keep pointers positions feature text occurs 
computing list procedure documents features candidates features foreach doc documents foreach ngram ngrams doc ngram ngram ngram ngram features features counter ngram counter ngram candidates candidates ngram foreach ngram candidates counter ngram features features ngram return features efficiently generating features apriori algorithm 
pointers pass documents feature set length computed feature set length algorithm 
find pairs features intersect find pairs features 
pair compute intersection position pointers features 
defined subset position pointers feature pointer immediately position contained set position pointers second feature 

discard features number associated position pointers frequency threshold 
algorithm inspired aprioritid algorithm described agrawal 
read documents memory requirements higher algorithm store list position pointers feature counter 
iteration number accesses hash table stores position pointers quadratic number features previous iteration linear size document collection apriori algorithm 
consequently additional passes document collection cheaper number features large 
higher gram sizes size feature sets small ca 
position pointers begins pay 
implemented algorithms 
implementation additional parameter specify iteration mode switch making additional passes document collection position indices 
parameter allows user specify minimum term frequency number times feature occurs collection minimum document frequency minimum number documents feature appear 
feature accepted thresholds 
experimental results inductive rule learning algorithm ripper experiments domains reuters newswire data ken lang newsgroups data set 
briefly describe ripper experimental setup results domains 
ripper william cohen ripper cohen efficient noise tolerant rule learning algorithm incremental reduced error pruning algorithm rnkranz widmer rnkranz 
ripper particularly suited text categorization problems ability set valued features cohen 
conventional machine learning algorithms document typically represented set boolean features encoding presence absence particular word gram document 
results inefficient encoding training examples space wasted specifying absence words document 
ripper allows represent document single set valued feature simply lists words occurring text 
conceptually differ boolean features conventional learning algorithms ripper clever optimizations 
remainder frequently continue refer gram separate boolean feature 
experimental setup datasets represented document set valued features gram size 
means experiments grams included grams bigrams grams unigrams 
generated different versions datasets various settings parameters df minimum document frequency tf minimum term frequency described section 
important note list order reduce number grams 
frequent grams consist concatenation frequent uninformative prepositions articles avoided way 
mentioned evidence important information thrown away technique see riloff 
ignored sentence boundaries converted characters lower case replaced digits special characters 
available list publicly available newsgroups dataset experimented ken lang newsgroups data 
collection netnews articles different newsgroups 
dataset available task identify newsgroup article belongs 
evaluated ripper various feature sets built cross validation 
complexity chose folds 
note procedure problematic characteristics newsgroup articles happens quite frequently portions articles quoted subsequent articles newsgroup 
related articles may appear training test sets danger optimistic accuracy estimates 
believe estimates comparing different versions learning setup 
table shows results 
column shows pruning parameters 
measured average error rate average run time learning algorithm cpu seconds include time needed generating feature set cumulative number generated features different settings algorithm parameters different maximal gram sizes 
df tf stand minimum document frequency minimum term frequency respectively 
set words setting refers conventional text learning setting word treated separate boolean feature 
best results obtained fairly moderate frequency pruning features occur times documents admitted sequences maximum size 
groups identical pruning parameters ones heavy pruning grams improves results 
sequences length longer improve results worse cases 
frequency pruning works parameter settings fairly low results get worse increasing amounts pruning 
obviously features fairly low coverage thrown away higher settings pruning parameters 
look highest ranked features shows indicative classes 
top features frequencies shown 
obviously words predictive classes 
word predictive classes soc talk religion misc soc religion christian alt atheism god ranked occurrences 
higher gram sizes situation similar 
problems alleviated tailoring list domain specifics 
requires considerable effort solve problems repetitive nature domain entire paragraphs may repeated documents may lead overfitting 
example fragment closed roads mountain passes serve ways escape produced highest ranked grams contain numerical patterns special characters occurring times 
article contains passage quoted times 
pruning grams error rate cpu secs 
features set words df tf df tf df tf df tf df tf df tf table results newsgroups domain 
reuters newswire data reuters newswire dataset frequently benchmark text categorization tasks 
version documents evaluated called modapte split uses documents training testing remaining documents 
standard evaluation procedure consists sequence binary classification tasks category 
results tasks combined microaveraging 
detailed description setup lewis 
shows results 
report recall precision value geometric mean recall precision predictive accuracy number features 
feature frequency ax dd ddd dddd writes article dont table frequent features newsgroups domain 
representations case bigrams results highest recall lowest precision 
terms predictive accuracy bigrams clear advantage moderate pruning heavy pruning unigrams representation catch 
obvious precision correlated number features 
unigrams give higher precision lower recall multi grams increase minimum frequency requirements increases precision 
interpreting results remembered domain fairly simple classes occurrence single word sufficient classify articles 
look features different results newsgroups domain frequent features bear obvious relationship classes 
interesting comparison number features reuters contains slightly articles compared newsgroups dataset number features differs order magnitude 
think reasons phenomenon newsgroups articles slightly longer average originate variety authors diverse vocabulary diversity topics newsgroups newsgroups articles produces grams repetition entire paragraphs article 
tables exhibit sub linear growth number features 
algorithm effectively avoids super linear growth number features see section 
related feature generation feature selection important topics information retrieval 
lewis emphasized importance studied techniques reuters newswire data 
contrary results gram features particular bigrams lewis concludes reuters dataset phrasal features term clustering pruning grams recall precision accuracy 
features set words df tf df tf df tf table results reuters newswire domain 
provide advantage conventional set words features 
notwithstanding results rnkranz mitchell riloff show phrases yield precision gains low levels recall 
mladeni grobelnik performed similar study naive bayesian classifier classifying www documents hierarchy yahoo com 
conclude sequences length improve performance longer sequences improve performance 
main difference study different classifier different domain differences setup experiments mladeni grobelnik fixed number features frequency threshold determining number features 
discussion simple efficient algorithm generating gram features investigated utility benchmark domains 
algorithm apriori algorithm discovering frequent item subsets databases 
similar adaptation algorithm independently developed studied mladeni grobelnik 
studies results indicate addition grams set words representation frequently text categorization systems improves performance 
sequences length useful may decrease performance 
note results obtained simple frequency feature subset selection 
evidence frequency pruning feature sets quite competitive text categorization domains yang pedersen mladeni worth study sophisticated pruning techniques take class information account 
hand yang pedersen lewis report heavy pruning may improve performance consistent results 
main reason choice frequency pruning easily integrated apriori feature generation algorithm 
principle feature subset selection technique post processor algorithm 
furthermore techniques directly integrated algorithm 
condition algorithm imposes feature acceptable pruning criterion subsequences acceptable 
measures implement condition upper lower bounds measures implemented allow weed unpromising candidates techniques pruning candidate conditions unpromising information gain bounds quinlan foil quinlan 
extending feature generation techniques direction subject research 
performed author stay carnegie mellon university funded austrian fonds zur der wissenschaftlichen forschung fwf number inf schr dinger 
agrawal mannila srikant toivonen verkamo 
fast discovery association rules 
fayyad piatetsky shapiro smyth uthurusamy eds advances knowledge discovery data mining pp 

aaai press 
cohen 

fast effective rule induction 
prieditis russell eds proceedings th international conference machine learning ml lake tahoe ca pp 

morgan kaufmann 
cohen 

learning trees rules set valued features 
proceedings th national conference artificial aaai pp 

aaai press 
rnkranz 

pruning algorithms rule learning 
machine learning 
rnkranz mitchell riloff 
case study linguistic phrases text categorization www 
sahami ed learning text categorization proceedings aaai icml workshop madison wi pp 

aaai press 
technical report ws 
rnkranz widmer 
incremental reduced error pruning 
cohen hirsh eds proceedings th international conference machine learning ml new brunswick nj pp 

morgan kaufmann 
hearst hirsh eds 

proceedings aaai spring symposium machine learning information access 
aaai press 
technical report ss 
lewis 

evaluation phrasal clustered representations text categorization task 
proceedings th annual international acm sigir conference research information retrieval pp 

lewis 

feature selection feature extraction text categorization 
proceedings workshop speech natural language ny pp 

lewis 
february 
representation learning information retrieval 
ph 
thesis university massachusetts department computer information science ma 
lewis 
september 
reuters text categorization test collection 
readme file available mccallum nigam 
comparison event models naive bayes text classification 
sahami ed learning text categorization proceedings aaai icml workshop madison wi pp 

aaai press 
mladeni 

feature subset selection text learning 
proceedings th european conference machine learning ecml 
springer verlag 
mladeni grobelnik 
word sequences features text learning 
proceedings th electrotechnical computer science conference erk ljubljana slovenia 
ieee section 
quinlan 

learning logical definitions relations 
machine learning 
quinlan 

programs machine learning 
san mateo ca morgan kaufmann 
riloff 

little words big difference text classification 
proceedings th annual international acm sigir conference research development information retrieval pp 

sahami 
ed 

learning text categorization proceedings aaai icml workshop 
aaai press 
technical report ws 
yang pedersen 
comparative study feature selection text categorization 
fisher ed proceedings th international conference machine learning icml pp 

morgan kaufmann 

