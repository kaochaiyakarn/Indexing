guy blelloch past years progress developing analyzing parallel 
researchers developed efficient parallel algorithms solve problems efficient sequential solutions known 
ofthese algorithms efficient theoretical framework quite efficient practice key ideas efficient implementations 
research parallel algorithms improved general understanding cases led improvements sequential algorithms 
unf success developing languages parallel particularly languages suited teaching prototyping algorithms 
large gap languages low level requiring specification details obscure meaning ofthe algorithm languages high level making performance implications various constructs unclear 
sequential computing standard languages pascal reasonable ob gap parallel languages building bridge significantly difficult 
communications acm march vol 
research involves developing parallel language useful teaching implementing parallel algorithms 
achieve important goal develop language allows high level descriptions algorithms understood mapping performance model bridges gap 
research believe features important achieving goal language performance model uses depth machine model uses running time support nested data constructs 
ability apply function parallel element collection data ability nest parallel calls 
article describe features explain important programming parallel algorithms 
ideas concrete describe programming language nesl designed features go examples program analyze parallel algorithms language 
nesl years undergraduate graduate courses parallel algorithms 
algorithms cover article relatively straightforward 
algorithms web version ofthis article available web scandal cs cmu edu www cacm html 

diagram ofa parallel random access machine pram 
assumed model processors access memory locations shared memory simultaneously 
depth total 
summing numbers tree 
total depth longest chain dependencies total number operations 
total depth analyzing performance key part studying algorithms 
analysis predict exact running time algorithm particular machine important determining running time grows function ofthe input size 
analyze performance formal model needed account costs 
parallel computing common models set processors connected shared memory parallel random access machines pram see network hypercube grid models 
models performance calculated terms number instruction cycles computation takes running time usually expressed function input size number processors 
important advance parallel computing notion models 
virtual model performance model attempt represent machine build higher level model mapped various real machines 
example pram viewed virtual model :10.1.1.160.9052
viewpoint agreed pram built directly practice unreasonable assume processor access shared memory unit time 
pram treated virtual machine mapped realistic machines efficiently simulating multiple processors pram single processor host machine 
simulation imposes slowdown requires factor fewer processors total cost processor time product remains 
advantage virtual models physical machine models easier program 
virtual models taken step define performance measures just running time particular machine 
pair measures depth defined total number operations executed computation depth defined longest chain sequential dependencies computation 
consider example summing numbers balanced binary tree see required computation operations additions 
depth computation operations longest chain dependencies depth summation tree sums need calculated starting leaves going level time 
general summing numbers balanced tree requires log depth 
usually viewed measure total cost computation integral resources time specifies running time ifthe algorithm executed sequential processor 
depth represents best possible running time assuming ideal machine unlimited number processors 
depth informally years describe performance parallel algorithms especially teaching 
claim easier describe think analyze algo march vol 
commu ica idn 

em scalar memory vector memory parallel vector processor procedure sum length log yo odd elts elts vector add return rithms terms depth terms running time processor model model processors 
furthermore depth tell lot expected performance various machines 
return points describe detail depth incorporated computational model 
basically classes models circuit models vector machine models language models briefly describe 
circuit models 
circuit models algorithm specified designing circuit gates solve problem 
circuits restricted cycles 
example view circuit inputs top adder circuit ofthe lines adders bundle 
final sum returned bottom 
circuit models circuit size number gates corresponds longest path input output corresponds depth 
particular input size build circuit implement algorithm general circuit models viewed virtual models size depth designs tell performance algorithms real machines 
models years study various theoretical aspects parallelism example prove certain problems hard solve parallel see overview 
models suited theoretical analysis convenient model programming parallel algorithms 
vector machine models 
programmable machine model depth vector random access machine 
model sequential random access machine ram extended set instructions operate vectors see 
location ofthe memory contains vector vectors vary size computation 
vector instructions include elementwise operations adding corresponding elements vectors aggregate operations extracting elements vector vector 
depth computation simply number instructions executed machine 
diagram ofa vector random access machine pseudocode summing numbers machine 
vector processor acts slave scalar processor 
functions od odd elements vector respectively 
function vector add elementwise adds vectors 
iteration loop length ofthe vector halves 
code assumes power hard generalize code total done bythe computation nl 
depth constant times number iterations log nl 
calculated summing lengths vectors computation operates 
example shows code sum values 
code executes summation tree loop iteration moves tree level 
virtual model impractical build vector memory dynamic nature 
model describing algorithms vectors arrays ideal model directly expressing algorithms complicated data structures trees graphs 
language models 
third choice defining model terms ofwork depth define directly terms constructs 
language performance model specifies costs ofthe primitive instructions set rules composing costs program expressions 
language models certainly new 
aho ullman popular introductory textbook foundations computer science define model deriving running times sequential algorithms 
approach allows discuss running time algorithms introducing machine model 
similar approach taken define model depth 
approach depth costs assigned primitive instruction language rules specified combining parallel sequential expressions 
roughly speaking executing set parallel total sum 
aem march tasks total depth maximum depth tasks 
executing tasks sequentially depth summed 
rules concrete describe nesl performance model section algorithms article illustrate examples rules applied 
note language performance models significantly important parallel algorithms sequential algorithms 
aho ullman sequential model corresponds directly machine model ram defined purely convenience satisfactory machine model captures notion depth general way 
depth 
return question models depth better processor models programming analyzing parallel algorithms 
motivate claim consider particular algorithm quicksort compare code performance analysis parallel version algorithm types models 
argue depth model code simple performance analysis closely related code code captures notion parallelism quicksort high level 
true processor model 
start reviewing sequential quicksort pseudocode shown 
standard performance analysis proves keys algorithm runs log time average expected case 
similar analysis proves maximum depth calls og expected case fact 
quicksort hard parallelize 
particular execute recursive calls parallel furthermore single quicksort compare elements pivot parallel elements sl similarly questions remain program parallel version performance 
consider programming analyzing parallel procedure quicksort element return choose element randomly sl sequences equal greater respectively return quicksort si followed followed quicksort 
pseudocode quicksort aho hopcroft ullman 
originally described sequential algorithm algorithm stated hard parallelize 
quicksort model depth 
illustrates nesl code algorithm 
code compared sequential pseudocode significant difference esl code specifies recursive calls quicksort executed parallel nesl curly brackets signify parallel execution 
parallel algorithm basically operations sequential version cost parallel version small constant factor time sequential version log expected case 
depth cost ofthe algorithm analyzed examining recursion tree 
depth blocks represents sum depths operations single call quicksort including recursive calls 
operations test termination finding pivot generation appends 
discussed detail section nesl ofthese operations constant depth fully parallel 
depth ofeach block constant total depth constant times maximum number levels recursion mentioned earlier log expected case 
completes analysis quicksort says quicksort log depth log expected case 
note derived performance measures algorithm high level code talking processors 
consider code analysis parallel quicksort parallel machine model processors 
claim model code long obscure high level intuition algorithm hard analyze performance algorithm 
particular code ll specify sequence partitioned processor general input length equal needs broken parts implemented parallel generating sl parallel recursive calls get partitioned processors load balanced synchronized details 
complicated fact quicksort recursive calls typically equal sizes recursion tree balanced sets reinserted way back recursion 
coding details help optimize algorithm particular machine little core ideas 
assume simplest processor model unit time access shared memory built synchronization primitives fully parallel code quicksort just language require hundreds code 
just question question think algorithm 
relationship depth running time 
depth viewed running time algo note parallel version quicksort requires memory implementation sequential version 
particular sequential version implemented place parallel version requires scratch space 
march vol 

gill aem function quicksort rand si si si sl log expected depth log expected quicksort quicksort rithm limits processor unlimited number processors depth 
fact costs referred 
practice want know running time fixed number sol 
simple important result brent showed knowing limits place reasonable bounds running time fixed number processors 
particular showed ifwe know computation wand depth run processors time result assumptions communication scheduling costs equation modified assumptions change 
example machine memory latency time making remote request receiving reply equation return example summing 
brent equation previous analysis depth log tells numbers summed processors time bounds log 
example elements summed processors log cycles assuming count cycle addition 
parallel machine models pram set processors connected hypercube network case 
implement addition assign elements processor sum take cycles 

nesl 
operator returns length sequence 
function rand returns random number expression rand returns random element 
notation read parallel find elements 
operation constant depth proportional length 
notation quicksort read parallel 
results returned pair 
function appends sequences 
sum processors tree log total number add cycles bounds 
communication costs 
problem depth cost measures directly account communication costs lead bad predictions running time machines communication bottleneck 
address question separate communication costs parts latency defined previously bandwidth rate processor access memory 
assume processor may multiple outstanding requests latency problem 
particular latency accounted mapping depth time machine see preceding simulation remains efficient processor time product proportional total 
hiding latency processors average processor multiple parallel tasks threads execute plenty waiting replies 
bandwidth serious problem 
machines bandwidth processors ca io em march voi bandwidth local memory depth general give predictions time 
network bandwidth available parallel machines cray sci power challenge great give reasonable predictions expect situation improve rapidly improving network technology 
nested data parallelism nesl constructs suggested expressing parallelism programming languages including fork constructs data parallel constructs futures 
question useful specifying parallel algorithms 
ifwe look parallel algorithms described literature pseudocode find nearly described parallel operations collections 
example parallel vertex graph find minimum neighbor parallel row matrix sum row 
course algorithms usually consist parallel calls interleaved operations rearrange order collection called recursively parallel quicksort 
ability operate parallel sets data referred data parallelism languages referred data parallel languages collection oriented languages 
note parallel languages data parallel features conjunction forms parallelism 
come data parallel languages panacea programming parallel algorithms distinction flat nested data parallel languages 
flat data parallel languages function applied parallel set function sequential 
nested data parallel languages function including parallel functions applied set 
example summation row matrix mentioned previously execute parallel tree sum 
claim ability nest parallel calls critical expressing algorithms way matches highlevel intuition 
particular nested parallelism implement nested loops divide conquer algorithms parallel 
algorithms described article nesting crucial way 
importance allowing nesting dataparallel languages observed 
existing data parallel languages high performance fortran hpf direct support nesting 
nesl article uses nesl example nested dataparallel language 
section gives overview language section gives examples parallel algorithms described analyzed nesl 
current hpf limited support nested calls versions significantly better support 
nesl designed express nested parallelism simple way minimum set structures designed language extension existing sequential language 
ideas clearly languages 
nesl loosely ml language powerful type system setl language designed concisely expressing sequential algorithms 
ml nesl functional limited forms effects feature tangential points article 
nesl supports data parallelism means operations sequences dimensional arrays 
elements sequence type sequence indices zero extracts element sequence 
main data parallel construct apply uses set notation 
example expression squares element sequence returning sequence 
read parallel sequence square 
apply multiple sequences 
expression adds sequences elementwise returning 
apply construct provides ability elements sequence filter 
example 
read parallel em sequence greater square 
returns sequence 
elements remain maintain relative order 
filtering quicksort example 
function primitive user defined may applied element ofa sequence 
example define function factorial en en factorial en apply elements sequence returns sequence 
addition parallelism supplied apply esl provides set functions sequences implemented parallel 
example function sum adds elements sequence function reverse reverses elements sequence 
important function sequences write supplies mechanism modify multiple values sequence parallel 
function write takes arguments sequence modify second sequence integer value pairs march vol 
aem specify modify 
pair value inserted position destination sequence 
example write inserts sequence locations respectively returning 
index repeated value written nondeterministically 
readers familiar variants pram model note write function analogous arbitrary concurrent write 
nesl includes function write allow repeated indices analogous exclusive write 
indices write current implementation reports error 
nested parallelism supplied nesl allowing sequences nested allowing parallel functions apply 
example apply sum function parallel nested sequence sum jj return 
parallelism sum sums 
quicksort algorithm showed example nested calls algorithm apply invoke recursive calls parallel 
performance model return issue performance models time context nesl 
mentioned earlier nesl defines depth terms ofthe depth primitive operations rules composing measures expressions 
refer depth expression cases depth expression sums ofthe depth ofthe subexpressions 
example expression el el subexpressions expression sum fact fact sum depth max fact fact max 
calculating depth cost ofthe add 
similar rule depth 
interesting rules concerning parallelism rules apply expression el max el 
ti 

list ofthe sequence functions supplied nesl 
required function column refers length ofthe sequence ofthe write function depends argument needs copied examples article difference effect 
operation description fork depth dist sum write drop interleave flatten create sequence 
return length return element position ofa 
return integer return integer uy return sum place elements append sequences drop elements interleave elements flatten nested sequence result result result log com op em ar iv 
ot procedure primes array length set true ifa true set multiples false 
pseudocode sieve eratosthenes rule specifies sum applications ej element plus plus account overheads 
rule depth similar takes maximum depth application ej 
supports intuition applications executed parallel evaluation apply completes call completes 
interesting rules rules expression ei ej true el similar rule depth 
depth function call scalar primitives costs esl functions sequences summarized 
note performance rules precisely defined operational semantics 
example composing depth consider evaluating expression 
rules code factorial earlier write equation unit constants come cost function call rule terms solving recurrence gives 
parallelism factorial function depth 
calculate depth full expression equations 
calculation shown 
examples parallel algorithms nesl parallel algorithms described analyzed providing examples analyze algorithms terms depth nested dataparallel constructs 
introduce important ideas concerning parallel algorithms 
main goals code closely match high level intuition algorithm easy analyze asymptotic performance code 
primes algorithm finds prime numbers example demonstrates common technique parallel algorithms solving smaller case problem speed solution ofthe full problem 
example introduce notion efficiency 
important aspect developing parallel algorithm designing close time sequential algorithm solves problem 
condition hope get speedup parallel algorithm sequential algorithm 
parallel algorithms referred efficient relative sequential algorithm constant factor ofthe time sequential algorithm 
algorithms discussed far efficient relative best sequential algorithms 
particular numbers took parallel quicksort took log expected required sequentially 
finding primes goal develop efficient algorithm 
start looking efficient sequential algorithms 
common sequential algorithm finding primes sieve eratosthenes specified 
algorithm returns array th position set true ifi prime false 
algorithm works initializing array true setting false multiples prime finds 
starts prime works vn 
algorithm needs go vn composite numbers factor equal yn 
implemented looping multiples algorithm shown take log log time constant small 
sieve eratosthenes theoretically best algorithm finding primes close happy derive parallel algorithm efficient relative log log 
turns algorithm described easy parallelism 
particular line implemented parallel 
esl multiples value generated parallel expression written array parallel write function 
rules costs see depth operations constant number multiples time sequential version 
parallel implementation line total ofthe algorithm sequential algorithm number operations depth ofthe algorithm vn march voi cd 
acm 
mining iteration loop lines constant depth number hi 
note thinking algorithm terms ofwork depth allows simple analysis assuming know running time sequential algorithm having worry parallelism maps machine 
particular amount varies greatly iteration multiples knock parallel iteration hi multiples 
varying parallelism messy program analyze processor model 
consider improving depth ofthe algorithm giving 
note ifwe primes hi generate multiples primes 
esl code generating multiples pin sqr primes sqr primes sequence containing primes hi 
computation nested parallelism parallelism sqr primes outer parallelism generating multiples prime inner parallelism 
depth computation constant constant depth log log total number multiples summed number multiples sequential version 
assumed sqr primes generate primes simply call algorithm recursively hi 
shows full algorithm finding primes idea 
returning sequence flags algorithm returns sequence values primes 
example return sequence 
algorithm recursively calls problem size hi terminates problem size reached 
depth analyzed looking picture bottom 
clearly ofthe done top level log log 
total log log 
consider depth 
recursion level constant depth total depth proportional number levels 
calculate number note size problem level size algorithm terminates 
gives equation 
code primes algorithm example level ofthe recursion diagram ofthe depth 
code int indicates empty sequence integers 
function takes square root ofan integer 
function flatten takes nested sequence flattens 
function dist distributes value sequence length expression nl fl ifl read fl flags return ifthe corresponding fl true 
function drop drops elements ofthe sequence function primes int sqr primes primes composites sqr primes flat camps flatten composites flags write dist true false flat comps indices ii fl flags fl drop indices example primes sqr primes composites flat camps flags indices result depth primes io io primes nl log log primes ni primes com un cano em march vol 


depth seek 
solving method gives log log costs log log log log algorithm remains efficient relative sequential sieve eratosthenes greatly improves depth 
sparse matrix multiplication sparse matrices common scientific applications matrices elements zero 
save space running critical store nonzero elements 
standard representation sparse matrices sequential languages array element row contains linked list nonzero values row column number 
similar representation parallel 
nesl sparse matrix represented sequence rows sequence column number value pairs ofthe nonzero values row 
matrix represented way nested sequence 
representation matrices arbitrary patterns nonzero elements subsequence different size 
common operation sparse matrices multiply dense vector 
operation result dot product sparse row matrix dense vector 
nesl code dot product sparse row dense vector sum row code takes index value pair sparse row multiplies th value sums results 
depth easily calculated performance rules 
number nonzero elements row depth ofthe computation depth sum log 
full code multiplying sparse matrix represented dense vector requires apply code row parallel gives sum row row 
nested parallelism rows row dot products 
total depth ofthe code maximum ofthe depth ofthe dot products logarithm size largest row 
total proportional total number nonzero elements 
planar convex hull example solves planar convex hull problem points plane find lie perimeter smallest convex region contains points 
example shows nested parallelism divide conquer algorithms 
algorithm parallel quickhull named similarity quicksort algorithm :10.1.1.141.5884
quicksort strategy pick pivot element split data pivot recurse split sets 
quicksort pivot element guaranteed split data equally sized sets worst case algorithm requires practice algorithm efficient 
shows code example ofthe quickhull algorithm 
algorithm recursive routine 
function takes set points plane coordinates points pi known lie convex hull returns points lie hull clockwise pi inclusive pi ofp 
points 
pi return sequence 
order pi matters switch return hull direction 
function removes elements hull lie line pi denote pl 
done removing elements cross product line pi negative 
case pi points remain placed sequence packed 
algorithm finds point pm farthest line plp 
point pm hull line infinity parallel pl moves pl hit pm 
point pm running example point cross product 
pm calls twice recursively points pm pm example 
recursive calls return flattens result appending 
convex hull algorithm works finding points maximum coordinates points hull find upper lower hull 
recursive call constant depth 
points deleted step significantly 
quicksort worst case costs 
hull points best case times log depth 
hard state average case time depends distribution inputs 
parallel algorithms convex hull problem run log worst case larger constants 
march voi 
op acm algorithms conclude examples brief discussions algorithms fast fourier transform fft scan operation prefix sums algorithm finding th smallest element set 
code shown ii 
algorithms demonstrate conciseness nested data parallel constructs 
standard recursive version fft 
second argument sequence length function cross product line xo yo xl yl line xl yo yl xo function points pl cross pi points packed points cross packed pi packed pm points max index cross flatten packed pl pi pi pm pm function convex hull points ix points minx points min index maxx points max index points minx maxx points maxx minx containing complex nth roots 
fft called recursively odd elements results combined cadd complex addition multiplication 
assuming cadd take constant depth recursion gives costs kn log log 
plus scan operation called prefix sums takes sequence returns sequence length element sum previous elements original sequence 
example executing sequence returns :10.1.1.40.6866
implemented shown algorithm works elementwise adding odd elements recursively solving problem sums 
result recursive call generate prefix sums 
costs kn log particular code shown works sequences length equal power oftwo hard generalize sequences length 
function fft elts bin elts odd elts bin nlog depth jog function scan elts odd elts scan oj interleave ls depth jog ab cd fgr kl mn fj 
code example ofthe quickhull algorithm 
sequence example shows step ofthe algorithm 
extrema line ap original split line 
farthest points subspace ap level splits 
values outside brackets hull points 
function kth smallest pivot lesser sl pivot greater sl pivot kth greater kth smallest greater greater pivot expected depth jog expected 
code fast fourier transforms scan operation finding smallest element ofa set mun ca 
cm march vol 
variation quicksort find kth smallest element sequence 
algorithm calls set containing result 
consider parallel version algorithm 
selecting lesser elements lesser greater th smallest element belong set 
case algorithm calls kth smallest recursively lesser algorithm selects elements greater pivot similarly find th element belongs greater 
belong greater algorithm calls recursively subtracting number elements equal pivot 
th element belongs lesser greater pivot algorithm returns value 
sequences expected ofthis algorithm time serial version 
expected depth log expected depth recursion log 
summary esl language designed useful programming teaching parallel algorithms 
purposes important allow simple descriptions match high level intuition supply defined model analyzing performance 
believe language successfully achieved goals 
aspects esl purpose article extract features important programming parallel algorithms 
performance model depth 
important aspect model defined directly terms language constructs trying appeal intuition machine 
discussed model virtual give mappings running times various physical machine models 
parallel constructs expressing parallelism ability nest constructs 
certainly mean exclude parallel constructs having way mapping function set parallel critical expressing parallel algorithms 
article suggesting change underlying models analyzing parallel algorithms 
particular suggests move away theoretical performance models machines models languages 
mentioned article works informally analyze parallel algorithms terms depth mapping pram 
suggest extra step taken formalizing model depth 
formal model pram cut loop directly mapping model realistic machines 
furthermore argue language models reasonable way define depth 
full esl currently available world wide web 
compiler technique called flattening nested parallelism compiles intermediate language called vcode 
benchmark results implementation connection machines cm cm cray described :10.1.1.40.6866
results show nesl performance competitive machine specific codes benchmarks 
acknowledgments marco zagha vishkin jay margaret reid miller metaxas bob harper jonathan john greiner jacques cohen siddhartha chatterjee helpful comments article 
siddhartha chatterjee jonathan jay marco zagha helped design nesl implementing intermediate languages vcode 
research sponsored part advanced research projects agency arpa number part nsf young investigator award 

aho ullman foundations science 
computer science press new york 

aho hopcroft ullman computer addison wesley reading mass 

arvind nikhil pingali structures data structures parallel computing 
acm trans 
program 
oct 

blelloch data parallel mit press cambridge mass 

blelloch nesl nested data parallel language version 
tech 
rep cmu cs school computer science carnegie mellon univ 

blelloch greiner parallelism sequential 
functional computer architecture june 

blelloch class notes programming parallel algorithms 
tech 
rep cmu cs school computer science carnegie mellon univ 

blelloch chatterjee zagha implementation portable nested dataparallel language 
parallel 
apr 

brent parallel evaluation arithmetic expressions 
acm 

chandy misra foundation 
addison wesley reading mass 

cormen leiserson rivest 
cambridge mass 

feo report sisal language project 
parallel distrib 
comput 
dec 

hatcher tichy philippsen critique ofthe programming language 
commun 
acm june 

high performance fortran forum 
high fortran language may 

hillis steele jr data parallel algorithms 
commun 
acm dec 
march voi 
commu tn aem 
ija parallel algorithms 
addison wesley reading mass 

karp ramachandran parallel algorithms shared memory machines 
handbook computer science volume algorithms complexity van leeuwen ed 
mit press cambridge mass 

mills prins jf reif jh wagner prototyping parallel distributed programs proteus 
tech 
rep unc ch tr computer science dept univ north carolina 

milner tofte harper definition standard ml 
mit press cambridge mass 

preparata shamos computational geometry 
springer verlag new york 

rose jr steele jr extended language data parallel programming 
proceedings international conference supercomputing vol 
may pp 


schwartz jt schonberg programming sets setl 
springer verlag new york 

vishkin log parallel max flow algorithm 
algorithms 

blelloch collection oriented languages 
proceedings ofthe ieee apr pp 


vishkin parallel design distributed implementation general purpose computer 
theor 
comput 
sci 
pp 

author guy blelloch associate professor computer science carnegie mellon university 
author address department computer science carnegie mellon university pittsburgh pa email blelloch cs cmu edu permission digital hard copy part personal classroom granted fee provided copies distributed profit commercial advantage copyright notice title publication date appear notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee 
acm call acm fellows designation acm fellow may conferred acm members distinguished outstanding technical professional achievements information technology current voting members acm voting members preceding years 
voting member acm may member distinction 
received acm fellows committee august year delivered committee forms provided purpose see 
information organized principal nominator includes excerpts candidate current curriculum 
listing selected publications patents technical achievements honors awards 
description nominee drawing attention contributions merit designation fellow 
supporting endorsements acm members 
acm fellows forms endorsement forms may obtained acm writing acm fellows committee acm headquarters broadway new york new york nom fellows acm rg forms accessed www acm org awards fellows packet completed forms sent august acm fellows committee acm headquarters broadway new york new york fellows acm org fax ions op cm march vol 

