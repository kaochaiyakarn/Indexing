bayesian gaussian process models pac bayesian generalisation error bounds sparse approximations matthias seeger doctor philosophy institute adaptive neural computation division informatics university edinburgh ii non parametric models techniques enjoy growing popularity field machine learning bayesian inference gaussian process gp models received significant attention 
feel gp priors part standard toolbox constructing models relevant machine learning way parametric linear models results thesis help remove obstacles way goal 
main chapter provide distribution free finite sample bound difference generalisation empirical training error gp classification methods 
general theorem pac bayesian bound new give simplified somewhat generalised derivation point underlying core technique convex duality explicitly 
furthermore application gp models novel knowledge 
central feature bound quality depends crucially task knowledge encoded faithfully model prior distributions mutual benefit sharp theoretical guarantee empirically established statistical practices 
extensive simulations real world classification tasks indicate impressive tightness bound spite fact previous bounds related kernel machines fail give non trivial guarantees practically relevant regime 
second main chapter sparse approximations developed address problem unfavourable scaling gp techniques large training sets 
due high importance practice problem received lot attention 
demonstrate tractability usefulness simple greedy forward selection information theoretic criteria previously active learning sequential design develop generic schemes automatic model selection hyper parameters 
suggest new generic schemes evaluate variants large real world classification regression tasks 
schemes underlying principles clearly stated analysed applied obtain sparse approximations wide regime gp models far special cases studied 
iii course phd studies fortunate benefit inspiring interactions people machine learning community am able acknowledge 
go supervisor christopher williams guided research comments sparked deeper interest directions ignored forced re consider intuitive carefully 
benefitted wide overview relevant literature areas machine learning interested 
members anc group edinburgh inspiring discussions interesting talks covering wide range topics 
especially amos david barber having shared knowledge pointing interesting areas necessarily narrow scope thesis 
gratefully acknowledge support research studentship microsoft research random walk postgraduate studies led terrain quite unfamiliar harder certainly boring journey people met way 
ralf herbrich shared deep pragmatic insights data dependent bounds areas learning theory gave helpful comments enjoyed time spent working hugo zaragoza internship ms research cambridge autumn late night pub sessions eagle 
am grateful discussions neil lawrence michael tipping bernhard sch lkopf antonio time 
fortunate john langford diverse interests learning theory applications refinements pac bayesian theorem 
learned lot pragmatic approach learning theoretical problems enjoy joint 
am grateful manfred opper sharing enormous knowledge learning theoretical analyses bayesian procedures 
manfred got interested parts reminded studies learning curves bayesian methods done time ago david haussler invited aston university birmingham days 
discussions invaluable helped details see big picture pac bayesian technique recognising time huge versatility convex inequalities 
david mcallester proposing proving remarkable pac bayesian theorems place interesting discussions met windsor uk nips 
interest sparse gaussian process approximations sparked iv chris williams see section frustration running time memory consumption experiments pac bayesian theorem 
goal provide pac result practically useful course calls method practitioners really large datasets 
got interested informative vector machine ivm neil lawrence ralf herbrich nips workshop contribution worked generalisations details expectation propagation ep foundation strategy implementation able handle datasets size required pac bayesian experiments 
enjoyed collaboration led joint neil lawrence 
john platt christopher burges giving opportunity spend summer doing research microsoft redmond 
enjoyed chris liked spend time john unfortunately fortunately father time 
patrice simard interesting discussions great football soccer matches youth stamina prevailed experience wisdom 
alex joining great hiking trips mt fishing lake washington unexpected manoeuvre persistence getting photos mind white water day 
hope meet hiking prefer driving 
friends time beautiful town edinburgh 
year explored historical sites fascinating country gave lively account truths myths scottish history 
great time thomas bj rn build artificial worms save rain forest 
amazing scenery scottish sparked interest hiking mountain walking karsten bj rn david steve great experiences views vast wilderness 
am indebted sharing years going separate ways wish best 
special go mother sisters supported years especially final hardest 
thesis written memory father 
declaration declare thesis composed contained explicitly stated text submitted degree professional qualification specified 
vi matthias seeger table contents declaration previous collaborations 
publications postgraduate studies 
background bayesian gaussian processes 
gaussian processes process weight space view gaussian process models 
approximate inference learning 
reproducing kernel hilbert spaces 
penalised likelihood 
spline smoothing 
maximum entropy discrimination 
large margin classifiers kriging 
choice kernel 
kernel design 
learning theory 
probably approximately correct 
concentration inequalities 
vapnik chervonenkis theory 
pac bounds model selection 
pac bayesian bounds gaussian process methods data dependent pac bounds pac bayesian theorems 
need data dependent bounds 
bayesian classifiers 
pac bayesian theorems 
pac bayesian theorem gibbs classifiers 
binary classification case 
confusion distributions multiple classes 
comments 
case general bounded loss 
extension bayes classifier 
speculative extensions 
application gaussian process classification 
vii pac bayesian theorem gp classification 
laplace gaussian process classification 
sparse greedy gaussian process classification 
minimum relative entropy discrimination 
related 
theorem meir zhang 
experiments 
setup mnist 
experiments laplace gpc 
experiments sparse greedy gpc 
comparison pac compression bound 
bounds model selection 
comparing gibbs bayes bounds 
discussion 
sparse gaussian process methods 
likelihood approximations greedy selection criteria 
likelihood approximations 
greedy selection criteria 
expectation propagation gaussian process models 
sparse gaussian process methods conditional inference 
informative vector machine 
projected latent variables 
model selection 
model selection plv 
model selection ivm 
details optimisation 
related 

nystr approximations 
importance estimating predictive variances 
experiments 
ivm classification digit recognition 
plv regression robot arm dynamics 
ivm regression robot arm dynamics 
ivm classification digit recognition ii 
discussion 
viii pac bayesian bounds gaussian process methods 
suggestions 
sparse gaussian process methods 
suggestions 
general appendix notation 
linear algebra 
probability 
miscellaneous 
linear algebra 
useful formulae 
partitioned matrix inverses 
woodbury formula 
schur complements 
update cholesky decomposition 
useful formulae 
convex functions 
exponential families 
gaussians 
exponential families 
projections 
gaussian variables 
pattern recognition 
bayesian inference approximations 
probabilistic modelling 
bayesian analysis 
approximations bayesian inference 
lower bound maximisation 
expectation maximisation large deviation inequalities 
appendix chapter extended pac bayesian theorem example 
details proof theorem 
proof theorem 
efficient evaluation laplace gp gibbs classifier 
proof theorem 
case regression 
proof pac compression bound 
examples compression schemes 
appendix chapter expectation propagation 
expectation propagation exponential families 
ix marginal likelihood approximation 
adf update noise models 
likelihood approximations 
selection criteria 
optimal sparse likelihood approximations 
relaxed likelihood approximations 
cheap versus expensive selection criteria 
derivations informative vector machine 
update representation 
exchange moves 
model selection criterion gradient 
derivations projected latent variables 
site approximation updates 
point inclusions 
information gain criterion 
extended information gain criterion 
gradient model selection criterion 
bibliography list figures sample paths gaussian covariance function 
sample paths mat rn covariance function 
sample paths exponential covariance function 
sample paths polynomial covariance function 
relation margin gap bound part 
contribution pattern gibbs error bayes margin loss upper bound values expected test errors 
upper bound values expected test errors regression 
upper bound values expected test errors gap bound values expected training errors expected test errors regression expected test errors upper bound values mnist ivm test error rejection rate mnist vs rest 
ivm comparing different criteria rank predictions mnist vs rest 
ivm predictive std dev 
predictive mean mnist vs rest 
test error rejection rate svm ivm mnist vs rest learning curves sparse regression methods kin 
learning curves sparse regression methods pumadyn nm model selection criterion full sparse gp regression methods model selection runs full sparse gp regression methods ivm test error rejection rate usps classes 
illustrations convex duality 
xi xii list tables experimental results laplace gpc 
experimental results sparse gpc ivm 
experimental results compression bound ivm 
experimental results compression bound svm 
test errors training times ivm vs svm mnist vs rest ivm vs svm mnist tasks full size images 
training times sparse regression methods 
model selection full sparse gp regression methods 
ivm test error rates model selection times usps vs rest xiii xiv chapter order solve real world machine learning problems need principled way representing uncertainty quantitatively manipulating updating representation light observed data 
observed data corrupted noise subject measurement error aspects phenomenon generating data usually imperfectly known 
concept uncertainty useful justify necessary abstraction mapping complex problem tractable domain variables accounting ignorance details 
uncertainty conveniently represented probability distribution domain variables interest 
model induces family global distributions structural conditional independence constraints specifications local distribution families simple understood form 
model specified completely generate data just true source bayes formula compute marginal query probabilities conditioned observed data 
distinguish parametric non parametric models 
data generation mechanism completely specified finite set parameters learning data amounts reducing initial uncertainty fitting model observations adjusting parameters 
thesis mainly concerned non parametric methods setup quite different 
important special case serves illustrate general idea smoothing data proposing underlying curve optimally solves trade fitting data minimising complexity quantified example terms average bending energy 
non parametric models constructed random fields contrast parametric situation typically determined finite number parameters 
interested finite dimensional projection field inference finite problem analytically tractable amenable common approximation techniques 
typically note domain includes variables observed latent variables lead decompositions simplification model description 
chapter 
dimensionality order training set size marked contrast parametric situation parameter dimensionality usually independent training data 
thesis contributes quite different aspects field nonparametric modelling accurately gaussian process models introduced section 
briefly motivated subsections 
pac bayesian bounds gaussian process methods chapter chapter generalised slightly improved version mcallester pac bayesian distribution free generalisation error bound provide simplified proof pointing basic concept clearly 
apply result gaussian process classification methods 
consider binary classification pattern recognition problem data xi yi 
sampled independent identically distributed unknown joint data distribution input points targets 
classification rule rs aims minimise generalisation error gen pr rs sampled data distribution independently statistical learning theory aim find distribution free upper bounds bound gen computable independent data distribution unknown probability random draws bound violation 
important point statement holds matter data distribution typically bound converges empirical error emp rs yi bound statement implies convergence probability emp gen bound rate convergence 
comparing rate bounds different methods method different configurations help design choices drive model selection 
critical high security applications existence useful distribution free bound reassuring 
thesis generalisations simplifications powerful general data dependent technique prove distribution free upper bounds bayesian type classification rules apply technique nonparametric bayesian gaussian process classification methods 
main concept require inequality coming notion duality convex analysis 
compared techniques past prove similar distribution free bounds proof methods surprisingly light weight efficient 
opposed uniform results bounds strongly depend prior knowledge encoded procedure predictors learned samples prior assumptions judged complex suffer larger penalty bound predictors samples conforming assumptions 
sparse gaussian process methods chapter chapter sparse approximation schemes gaussian process methods show performance similar standard gp inference approximations vastly reduced time memory requirements 
recall practical non parametric methods finite dimensional uncertainty representations usually scale training set size leading memory training time scaling superlinear typically quadratic prediction time scaling 
hope useful generalisation source data exhibit redundancies grows large second part thesis describe general way detecting redundancies sparse variants non parametric methods achieve favourable scaling time memory requirements 
concepts generic framework approximate inference gaussian process gp models combined greedy forward selection informative cases information theoretic criteria active learning sequential design 
contributions appeal practitioner theoretician 
clear simple discussion concepts underlying sparse approximations 
concrete algorithms modifications non sparse generic approximate inference technique motivated analysis 
span interesting cases arising practice primitives specialised obtain new variants pointed clearly 
place special emphasis numerically stable algorithms favourable tradeoff memory running time requirements dominating operations highly efficient numerical linear algebra software 
importantly framework complete includes automatic empirical bayesian model selection 
tractability efficiency demonstrated significant number large tasks 
background chapter apart scientifically novel contributions included range tutorial material hope reader find useful contribution 
thesis entirely self contained main branches 
chapter contains background material notably extensive discussion section notions gaussian processes relevant machine learning techniques 
section chapter 
gives brief description key concepts learning theory tried emphasise intuitive ideas technical details 
software large amount software developed order run experiments thesis 
strictly speaking complete contribution time package released public domain 
initial reason implement system frustration widely matlab prototyping system 
main drawbacks matlab apart high pricing poor memory management incredible slowness iterative code 
circumvented implementing plug ins impossible sort non trivial operations matrices producing copies 
order circumvent slow iterative code artificial matrices simple low rank structure created temporarily able operations 
matlab contains host powerful functions easily extensible 
system allow proper object oriented development life easier extensibility 
system contains extensive matrix vector library allows simple manipulations parts permutations matrices need copies 
module generating pseudo random variates commonly distributions available 
powerful gradient nonlinear included allow direct control optimisation process non standard scenarios required section easily accommodated 
module dataset import export different formats designed allow virtual views large datasets accessed intermediate memory buffers virtual datasets represented indices views 
generic implementations frequently experimental setups example model selection cross validation arbitrary learning methods produce prediction value vectors arbitrary format evaluated test dataset arbitrary loss functions 
data file formats matlab functions read write access available results visualised matlab 
implemented rudimentary hopefully happen near way code matlab 
stand small things 
code unfortunately uses numerical recipes distribution proprietary 
important point re design matrix library conform fortran storage model optimised numerical software linked 
example essential basic features packaged bought separately 
basic installation sample gamma variables basic data analysis 
features fully implemented 

declaration previous collaborations allows system controlled interactively matlab 
interface object oriented works file transfer file semaphores running system server process 
declaration previous collaborations parts material thesis done collaboration researchers previously published list publications related thesis 
material chapter appeared previously accompanying report 
interested pac bayesian theorem john langford nimrod megiddo 
am grateful manfred opper inviting seminar aston university birmingham 
discussions manfred led great simplifications argumentation accessibility chapter owes lot 
john shawe taylor organising neurocolt workshop bounds windsor inviting david mcallester john langford manfred opper inspiring discussions royal venue 
ralf herbrich discussions learning theory issues time ms research cambridge 
material chapter appeared previously significant part especially model selection plv novel 
published done collaboration neil lawrence ralf herbrich ivm presentation christopher williams neil lawrence plv regression scheme 
chris williams sparked interest sparse approximations nystr approximations 
discussed detail chapter schemes closely related body done csat manfred opper 
publications postgraduate studies section list published postgraduate studies chronological order 
described thesis cases added short abstracts 
matthias seeger christopher williams neil lawrence fast forward selection speed sparse gaussian process regression workshop ai statistics 
particular bear responsibility name informative vector machine chosen longer boring 
chapter 
neil lawrence matthias seeger ralf herbrich fast sparse gaussian process methods informative vector machine neural information processing systems 
matthias seeger pac bayesian generalization error bounds gaussian process classification journal machine learning research 
matthias seeger covariance kernels bayesian generative models neural information processing systems 
propose mutual information kernels general framework learning covariance functions gp models labelled unlabelled task data related fisher kernel haussler hmrf kernels discussed section 
show kernels evaluated approximately variational techniques experiments variational bayesian mixtures factor analysers 
matthias seeger john langford nimrod megiddo improved predictive accuracy bound averaging classifiers international conference machine learning 
combines mcallester pac bayesian theorem approximation technique mixture discriminants suggested obtain pac bayesian bound bayes type classifiers 
christopher williams matthias seeger nystr method speed kernel machines neural information processing systems 
christopher williams matthias seeger effect input density distribution kernel classifiers international conference machine learning 
matthias seeger learning labelled unlabelled data technical report university edinburgh 
see www dai ed ac uk seeger 
provides rigorous formal definition problem learning mixture labelled unlabelled data literature review fairly complete time development framework input dependent regularisation 

declaration previous collaborations matthias seeger input dependent regularization conditional density models technical report university edinburgh 
see www dai ed ac uk seeger 
proposes general statistical framework learning labelled unlabelled data ideas algorithms standard latent variable techniques similar em 
matthias seeger annealed expectation maximization entropy projection technical report university edinburgh 
see www dai ed ac uk seeger 
complements frequently technique deterministic annealing step annealing learn assignment structure general resources components models 
application gaussian mixture modelling tied covariance matrix parameters 
chapter 
chapter background statistical models concerned thesis non parametric 
employing gaussian processes gps prior distributions latent fields models inference techniques understood conceptually simple bayesian viewpoint 
section give gps role non parametric regression models 
chapter thesis concerned learning theoretical finite sample size analyses bayesian gp methods concepts methodology required introduced section 
hope sections useful readers familiar subjects grasp basic concepts 
details largely omitted provided 
bayesian gaussian processes gaussian processes gps natural generalisations multivariate gaussian random variables infinite countably continuous index sets 
gps applied large number fields diverse range ends deep theoretical analyses various properties available 
hand thesis require quite elementary properties deep theory gps 
exposition section remains selective fairly elementary non rigorous level contains material required follow remainder thesis 
readers machine learning community find valuable tries connect different branches similar ideas investigated phrases concepts familiar vocabulary statistical modelling 
hand section safely skipped reading visited 
introductions gp models machine learning context 
chapter 
background gaussian processes process weight space view gaussian process gp models constructed classical statistical models replacing latent functions parametric form random processes gaussian prior 
section introduce gps highlight aspects relevant thesis 
develop simple views gps pointing similarities key differences distributions induced parametric models 
follow chap 

concepts required study gp prediction chap 

concepts vocabulary general probability theory refer 
section skipped readers interested details familiar gps quick glance mean process weight space view 
xn sequence complex valued random variables recall xn quadratic mean mean square xn 
convergence weaker sure convergence turns useful mode discussing gp aspects require 
general equivalent 
nonempty index set 
main parts thesis arbitrary assume group assume 
nutshell random process collection random variables common probability space 
measure theoretic definition awkward basically single variable 
process defined finite dimensional distributions induces finite subset kolmogorov proved specification exists random process measure theoretic sense iff symmetric consistent 

xn borel sets 
bn permutation 
bn bn bn 
question uniqueness random processes tricky processes equivalent surely equivalent processes called versions differ significantly sure properties sample paths 
require study sample path properties thesis see information 
second order statistics mean function covariance function addition origin negation 
ib ib denotes complex conjugate 

bayesian gaussian processes central study characteristics process mean square sense 
positive semidefinite function sense 
xn 
zn xi xj 
clear zi xi xi 
note implies 
called positive definite holds 
positive leads important spectral decomposition discussed section 
positive semidefinite referred kernel pointing role kernel linear integral operator see section 
stationary processes situations behaviour process depend location observer restriction rich theory developed linking local properties process behaviour close origin 
process called strictly homogeneous strictly stationary invariant simultaneous translation variables 
implies constant function write case 
process fulfilling conditions called weakly homogeneous weakly stationary 
stationary process choice origin reflected statistics second order 
called correlation function 
stationary process spectral representation stochastic fourier integral chap 
chap 
theorem rg asserts positive semidefinite furthermore uniformly continuous iff characteristic function variable df probability distribution function 
density lebesgue measure called spectral density 
theorem allows prove term uniquely literature replaced non negative definite positive definite different meaning 
different weaker property conditional positive see chap 
important discussed 
spectral distribution measure total mass 
chapter 
background positive computing fourier transform checking non negative 
proportional spectral density 
note real function spectral distribution symmetric exists 
process determine mean square properties true general sure properties 
stronger zero mean process properties usually determined entirely covariance function 
stationary processes merely behaviour origin counts derivative exists iff exists 
example process rbf gaussian covariance function analytic analytic differentiable order 
isotropic processes stationary process called isotropic covariance function depends 
case spectral distribution invariant isotropic isomorphisms rotations 
loosely speaking second order characteristics isotropic process position direction observed 
simpler characterise isotropic correlation functions stationary ones general 

spectral decomposition simplifies df df distribution function bessel function kind see sect 

right hand side hankel transform order see sect 

alternatively spectral density exists df ag easily convert spectral representation terms 
denote set corresponding isotropic correlation functions rg dg note characterises dg theorem 
clear dg dg dg dx denotes differential functional 
ag surface area unit sphere beware depend dimension induce correlation function 

bayesian gaussian processes show iff exp df result due schoenberg 
note assumption isotropy puts strong constraints correlation function especially large example inf large negative correlations ruled 
nonnegative 
furthermore large smooth may jump additive white noise 
nonsingular bx correlation function called anisotropic 
examples isotropic covariance functions section 
views gaussian processes gaussian process gp process gaussian 
gaussian determined second order cumulants involve pairwise interactions completely determined mean covariance function 
gps far accessible understood processes uncountable index sets 
clear positive semidefinite function exists zero mean gp covariance function gps modelling tool flexible 
conjunction latent variable modelling techniques wide variety non parametric models constructed see section 
fact gaussian covariance matrices induced obtain approximations bayesian inference fairly straightforwardly see section 
interesting note derivatives gp gps exist derivative observations incorporated model way function value observations applications see 
characteristics differentiability order controlled covariance function example section 
example thoroughly studied gps wiener process brownian motion continuous random walk covariance function min multivariate generalisations brownian sheets see chap 

continuous differentiable 
fact version wiener process constructed continuous sample paths version sample paths differentiable probability 
wiener process construct gps means stochastic integrals chap 

develop elementary views gaussian processes process weight space view 
usually simpler chapter 
background exclusively thesis allows relate gp models parametric linear models directly 
follow 
process view zero mean gp covariance function spirit gp definition 
defined implicitly finite subset induces vector process values points xi xj 
xn 
kolmogorov theorem guarantees existence gp family practice modelling problems involving unknown functional relationship formulated finite number linear characteristics evaluations derivatives linked observations predictive queries cases process view boils dealing projection gp multivariate gaussian distribution simple linear algebra quadratic forms 
gps seen weight space viewpoint relating linear model 
bayesian context view suggested hagan localized regression model weight space finite dimensional generalisation arbitrary gp priors developed uses process view 
address gp regression rigorous bayesian context equivalence spline smoothing bayesian estimation processes noticed earlier kimeldorf wahba see section 
recall linear model feature map covariate independent gaussian noise 
gp covariance function satisfies weak constraints written albeit possibly infinite dimensional weight space 
develop view details discussed detail section 
mild conditions covariance function construct sequence converges quadratic mean 
variables 
orthonormal eigenfunctions operator induced corresponding eigenvalues sense term process view function space view employed 
relationship gps associated spaces smooth functions bit subtle introduced section 
continuous version exists continuous sample paths require 
need pointwise convergence stronger statements possible mild assumptions sect 


bayesian gaussian processes precise section 
quadratic mean 
weight space view gps allows view non parametric regression model direct infinite dimensional generalisation linear model spherical gaussian prior 
say maps feature space typically countably infinite dimensional 
important note construction feature map individual components scaling sense norm hilbert 
space drawn operates comparable different rkhs norm scales roughness function 
intuitively graph increasingly complicated see section details 
inference purposes concerned derivatives linear functionals process weight space view equivalent lead identical results 
feel process view simpler avoiding spurious relying familiar gaussian manipulations 
hand weight space view frequently machine learning literature peculiarities may reason perception gp models difficult interpret 
danger false intuitions developed interpolating geometrical arguments low dimensional euclidean space feature space 
note weight space representation gp terms feature map unique 
route eigenfunctions covariance operator way establish 
invariant 
gaussian processes limit priors parametric models conclude section mentioning prime reasons focusing current machine learning interest gp models highly original different way establishing weight space view proposed 
consider model cancel magically weight space viewpoint occur process view 
example section discuss role reproducing kernel sense hilbert space inner product define map hilbert space weight space 
chapter 
background multi layer perceptron mlp hidden layer functions weights output layer weights suppose independent identical priors resulting bounded surely compact region interest 
vj independently 
converges quadratic mean zero mean gp covariance function 
stronger conditions assure sure convergence uniformly compact region 
bottom line take conventional parametric model linearly combines outputs large number feature detectors scale outputs isolation negligible contribution response just corresponding gaussian process model 
neal shows non zero number non gaussian feature outputs significant impact response non zero probability limit process typically gaussian 
conclude weight space view relate non parametric gp models parametric linear models fairly directly 
important differences general 
neal showed gps obtained limit distributions large linear combinations features feature contribution negligible output distributions architectures fit strong feature detectors typically gaussian 
predictions gp model smoothed versions data sense concrete section interpolate minimising general smoothness constraints encoded gp prior opposed parametric models predict focusing functions family consistent data 
hagan discusses differences optimal design 
gaussian process models simplest gaussian process model priori zero mean gaussian process covariance function independent noise 
inference model simple analytically tractable observation process gaussian covariance 
data xi yi 
xi xj context model interesting note stationary continuous sum continuous stationary covariance white noise covariance 
furthermore sch conjectured isotropic bounded covariance function continuous possibly 

bayesian gaussian processes xi test point distinct training points du xi tower properties lemma gaussian formulae section 
see model posterior predictive process gaussian mean function covariance function 
practice posterior mean predictions required prediction vector computed linear conjugate gradients solver runs eigenvalue spectrum shows fast decay 
predictive variances test points required cholesky decomposition definition ll computed variance computation requires single back substitution 
pointwise predictive variance larger corresponding prior variance shrinkage decreases increasing noise level result derived weight space view applying standard derivation bayesian linear regression 
note just parametric linear regression smoothed prediction linear function observations mean function predictive process see section 
note gets big predictive mean variance points far data tend prior mean prior variance 
second level inference problems selecting values hyperparameters parameters integrating analytically tractable approximations applied 
approximate model selection discussed section 
generalise model allowing arbitrary noise distribution retaining gp prior 
generative view sample process prior yi yi xi independent 
likelihood function factors product univariate terms yi ui 
generalised easily allow bounded linear functionals latent process evaluation functional xi discussed section 
chapter 
background likelihood depends finite set predictive posterior process written dp dp prior measure shifted multiplication depending process values recall notation section 
predictive process gaussian general mean covariance function obtained knowledge posterior mean covariance matrix discussed section 
test point expectation predictive distribution 
general model level inference analytically tractable 
section general approximate inference framework discussed 
mcmc methods see section applied fairly straightforwardly example gibbs sampling latent variables 
methods attractive marginalisation hyperparameters dealt framework 
naive realisations may prohibitive running time due large number correlated latent variables advanced techniques difficult handle practice 
mcmc advanced widely class approximate inference techniques discussed detail thesis 
generalised linear models 
binary classification large class models kind obtained starting generalised linear models replacing parametric linear function process gp prior 
seen direct infinite dimensional generalisation employing weight space view see section 
spline smoothing context framework detail 
employs noise distributions exp exponential family natural parameter sufficient statistics log partition function see section 
scale hyperparameter 
linear model special case eu technically attractive feature framework log strictly concave leading strictly concave unimodal posterior 

bayesian gaussian processes binary classification glm binomial noise distribution logistic regression logit noise 
log cosh 
frequently binary classification noise model probit noise seen noisy heaviside step exponential family 
noise models strictly log concave 
models latent processes allow fixed number latent variables case processes uc 
likelihood factors yi uc xi uc zero mean gaussian priori covariance function theoretically possible cross covariance functions prior covariances uc different may hard come suitable class functions 
furthermore assumption processes uc independent priori leads large computational savings joint covariance matrix data assumes block diagonal structure 
note structure separate different block diagonal structures coming factorised likelihood separate cases important example latent processes class classification 
likelihood comes multinomial glm multiple logistic regression 
convenient binary encoding class labels class 

noise multinomial softmax exp exp 
called softmax mapping 
note mapping invertible add changing 
terms section parameterisation multinomial overcomplete due hyperparameters may shared prior processes making marginally dependent 
vector notation associated single case 
confused vector notation rn group variables cases 
chapter 
background linear constraint corresponding glm log partition function log exp strictly convex 
usual remedy constrain example fixing uc 
fine context fitting parameters maximum likelihood may problematic bayesian inference 
mentioned typically priors uc fix uc induced prior exchangeable distribution component permutations different distributions singled technical reasons 
think preferable bayesian context retain symmetry accept 
dealing non identifiability inference approximations hard softmax invertible plane orthogonal strictly convex 
anyway detail different blocking structures mentioned renders implementations approximate inference class model somewhat involved binary case 
furthermore turns straightforward extension inference approximations thesis sections grows factor running time requirements compared binary case 
contrast gp laplace approximation see section scales linearly 
possible approximations desirable scaling attained methods pursued thesis 
examples process models ordinal regression ranking models see likelihood suggestions multivariate regression 
note straightforward fit class model uncertain class observations 
example target distribution 
corresponding log likelihood factor chosen log softmax reduces normal case note concave strictly concave plane orthogonal 
model useful problems points belong multiple classes multi label problems 
robust regression gp regression gaussian noise lead poor results data prone outliers due light tails noise distribution 
robust gp regression model obtained heavy tailed noise distribution laplace student distribution 
interesting idea fact obtained starting integrate precision gamma distribution 
robust model written 
bayesian gaussian processes drawn gamma distribution parameters hyperparameters 
posterior conditioned precision values gaussian computed way case 
sampled mcmc may chosen maximise posterior 
marginal likelihood gaussian computed easily 
note case number hyperparameters grows invalidate usual justification marginal likelihood maximisation see section 
approximate inference learning seen previous section posterior process likelihood general form written shifted version prior 
processes context dealt feasibly gaussian ones general way obtaining gp approximation posterior process approximate gaussian leading process dq dp gaussian 
optimal way choosing minimise 
equality follows fact dp dq dp dq recall notation section 
minimum point unique mean covariance function 
equivalent moment matching see section requires find mean covariance matrix 
unfortunately intractable general large datasets non gaussian noise 
gaussian approximation leads gp posterior approximation intractable valuable guideline example ep algorithm see section 
thesis primarily interested approximate inference methods gp models employ gp approximations posterior processes 
depend data covariance function kernel matrix hyperparameters 
class contains variety chapter 
background methods proposed literature briefly discussed see section 
virtually reduced parameterisation restricted form diagonal positive entries 
recall notation section 
methods mentioned section sparse approximations chapter case simplicity replacing 
approximate predictive posterior distribution test point determined easily ki ki ki kid 
ki xi generally gp posterior approximation mean function covariance function ki ki 
predictive distribution obtained averaging 
expectation analytically tractable done gaussian quadrature see section smooth grow faster polynomial 
simple numerically stable way determine predictive variances compute cholesky decomposition ll definition variance requires refer prediction vector 
generally mentioned section derivative information bounded linear functionals latent process likelihood variables predicted fact corresponding finite set scalar variables multivariate gaussian prior covariance matrix derived covariance function discussed detail section 
generalisation multi process models section straightforward principle 
dimension restricted form merely block diagonal blocks diagonal 
processes priori independent consist blocks diagonal 
general formulae prediction modified efficiency 
details slightly involved may depend concrete approximation method process models discussed detail thesis 

bayesian gaussian processes examples simple efficient way obtaining gaussian approximation laplace method see section proposed binary classification logit noise :10.1.1.18.3953
find posterior mode done variant newton raphson fisher scoring see 
iteration consists weighted regression problem requires solution positive definite linear system 
done approximately conjugate gradients solver 
mode diag diag logistic function diag diagonal elements positive 
recall laplace approximation replaces log posterior quadratic fitted local curvature mode 
logit noise log posterior strictly concave dominated gaussian prior far general gaussian approximation fairly accurate 
hand true posterior significantly skewed meaning mode quite distant mean optimal covariance approximation local curvature mode poor 
expectation propagation ep algorithm gp models described section employed sparse approximations chapter 
significantly outperform laplace gp approximation terms prediction accuracy costly 
somewhat harder ensure numerical stability 
hand ep general example deal discontinuous non differentiable log likelihoods 
description ep applies process models just unfortunate scaling slight technical difficulty having approximate dimensional gaussian expectations see section 
range different variational approximations see section suggested 
note variational method chosen minimise easy see best gaussian variational distribution covariance matrix form sect 

approximations mentioned far training time scaling prohibitive large datasets 
sparse inference approximations reduce scaling controllable topic chapter 
model selection far concerned level inference conditioned fixed hyperparameters 
useful general method provide means select partly due complex iterative structure elementary steps smaller laplace technique efficiently 
chapter 
background values parameters marginalise see section 
thesis focus marginal likelihood maximisation general model selection technique see section 
denote hyperparameters log marginal likelihood log difficult compute posterior approximated general 
variational lower bound described section employ lower bound maximisation model selection 
case lower bound log eq log log eq log 
differential entropy relative entropy defined section 
note posterior approximation depends feasible general obtain exact gradient 
variational em important special case lower bound maximisation iterative turn freezing maximising lower bound chosen family variational distributions 
alternatively chosen different way approximation posterior 
deviation variational choice criticised ground choices lead decreases lower bound algorithm increase criterion strictly monotonically 
hand chosen different way may lie outside families lower bound maximised efficiently may result larger value family 
furthermore lower bound criterion motivated fact gradient eq log ignoring dependence approximates true gradient log ep log point see section 
context approximate gp inference methods dependence gp prior quite explicit example covariance depends strongly kernel matrix 
argue keeping fixed gradient computation merely ignore dependence essential parameters 
typically leads involved gradient computation potentially closer true gradient 
alternatively computation analytically tractable gaussian likelihood example case gp regression gaussian noise discussed log 
example bound gaussians covariance matrix form finding prohibitively costly practice proposed variational schemes restricted subfamilies 

bayesian gaussian processes resource limits indirect dependencies may ignored 
refer section concrete example section details optimisation problem slightly non standard due lack strict monotonicity 
reproducing kernel hilbert spaces theory reproducing kernel hilbert spaces rkhs characterise space random variables obtained bounded linear functionals gp method prediction finite information 
apart rkhs provide unification ideas wide area mathematics mentioned 
interested reader may consult 
exposition taken 
theory discussed essential arguments thesis section skipped readers interested details 
reproducing kernel hilbert space rkhs hilbert space functions evaluation functionals bounded 
implies exists kernel inner product specific hilbert space vector space inner product complete sense cauchy sequence converges element space 
example hilbert space generated inner product space functions adjoining limits cauchy sequences note operation adjoined objects need functions usual sense 
example obtained completing vector space functions shown contain functions defined pointwise 
rkhs anomalies occur functionals bounded 
existence functions means expressions interpreted care 
element defined set equivalent cauchy sequences define cauchy sequences equivalent sequence obtained interleaving cauchy 
expression understood limit limn fn gn fn gn sequel convention 
bounded functionals called continuous 
chapter 
background riesz representation theorem exists unique representer holds easy see kernel positive semidefinite 
called reproducing kernel rk note 
important note rkhs norm convergence implies pointwise convergence pointwise defined function fm fm fm 
hand positive semidefinite exists unique rkhs rk set finite linear combinations xi xi aik xi bjk xi inner product space extended hilbert space adjoining limits cauchy sequences 
norm convergence implies pointwise convergence inner product space adjoined limits pointwise defined functions rkhs rk conclude rkhs properties nicer general hilbert space 
functions pointwise defined representer evaluation functional explicitly 
rkhs mercer eigendecomposition 
karhunen loeve expansion mentioned rkhs general kernels contains unique rkhs subspace 
recall contains functions holds 
standard inner product 
taken indicator function compact set unit hypercube 
positive semidefinite regarded kernel representer positive semidefinite linear operator sense kf 
eigenfunction eigenvalue 
eigenvalues real non negative 
furthermore suppose continuous 

bayesian gaussian processes mercer hilbert schmidt theorems exists countable orthonormal sequence continuous eigenfunctions eigenvalues expanded terms 
seen generalisation eigendecomposition positive semidefinite hermitian matrix 
reproducing property positive semidefinite kernels recognised moore develop notion general positive hermitian matrices 
case characterise rkhs embedded explicitly 
define fourier coefficients 
consider subspace hk 
hk hilbert space inner product fourier series converges pointwise fourier coefficients equation rk hk 
important distinguish clearly inner products hk see details relationship inner products 
measures expected squared distance measure roughness function 
example eigenfunctions increasingly rough 
spectral decomposition leads important representation zero mean gp covariance function karhunen loeve expansion 
sequence uk operators kernels called compact general discrete spectrum countable number eigenvalues 
sense high frequency components usual fourier transform 
chapter 
background independent variables converges quadratic mean stronger statement additional conditions 
defined quadratic mean 
expansion section introduce weight space view 
note variances decay gp approximated finite partial sums expansion see 
duality rkhs gaussian process zero mean gp covariance function exact relationship rkhs rk 
think seen distribution hk wrong pointed sect 

fact version sample functions process hk probability 
seen noting partial sums uk 
roughly speaking hk contains smooth non erratic functions characteristics expect sample paths random process 
better intuition hk turn contain expected values conditioned finite amount information 
important duality hk hilbert space noticed 
construct hilbert space hgp way starting positive semidefinite replace xi xi inner product gp ab xi gp xi 
hgp space random variables functions isometrically isomorphic hk mapping xi xi gp purposes regard hgp rkhs rk bounded linear functional hk representer hk lk 
bayesian gaussian processes isometry maps random variable hgp formally denote lu 
note lu lk generally functionals hk 
clear lu general different process obtained applying sample paths 
fact surely hk apply general 
correct interpretation quadratic mean isometry hgp hk 
example suppose differential functional evaluated retrieve observations section derivatives gp 
space hgp central importance sort inference gp models interested contains exactly random variables condition predict situations finite amount information available observations linear functionals process 
penalised likelihood 
spline smoothing gp models interested origin spline smoothing techniques penalised likelihood estimation low dimensional input spaces spline kernels widely due favourable approximation properties splines algorithmic advantages 
comprehensive account spline smoothing relations bayesian estimation gp models exposition mainly 
spline smoothing special case penalised likelihood methods giving view reproducing kernel green function regularisation operator introduced 
section included mainly illustrative purposes 
section discussed duality gaussian process rkhs covariance function 
apart bayesian viewpoint gp models different direct approach estimation non parametric models penalised likelihood approach oldest widely incarnations spline smoothing methods 
introduce basic ideas dimensional model leads general notion regularisation operators penalty functionals connections rkhs 
omit details important computational issues multidimensional generalisations see details 
elementary account 
sketch ideas rigorous details see 
interpolation smoothing splines originates sch 
natural chapter 
background spline order defined knots xn 
denotes set polynomials order xi xi xn 
natural cubic splines obtained 
define roughness penalty jm dx 
jm penalises large derivatives order large value example large functions large curvature 
fixed function values interpolant minimising jm defined spline order consider related smoothing problem minimising penalised empirical risk yi xi jm wm clear natural spline order wm replaced spline values knots change risk term increase jm 
taylor theorem 
gm dt gm 
xi 

gm gm green function boundary value problem functions form hilbert space inner product rkhs rk dx gm gm du 
interesting note zero mean gp covariance function obtained fold integrated wiener process 
zero mean gp wiener process independent increments 
covariance function min 
obviously process defined stochastic integral gm dw precisely wm called sobolev space absolutely continuous 

bayesian gaussian processes zero mean gp covariance function chosen sample paths continuous wm dgm dx gm equivalent 
note written dw dx 
xm 
boundary values satisfied direct sum space trivially rkhs choose orthonormal basis define kernel sum outer products basis functions 
kernel direct sum sum finite dimensional kernel 
note full space general see duality rkhs regularisation operator 
hilbert space pf 
consider operator null space example restrict orthogonal complement 
operator positive definite inverse green function kernel rk inner product pf pg penalty functional simply squared rkhs norm 

pf rk 
hand start rkhs rk derive corresponding regularisation operator give additional insight meaning covariance function see 
fact stationary continuous theorem 
spectral density take spectrum dimensional example readily generalised splines unit sphere thin plate splines details get quite involved see chap 

kimeldorf wahba generalised setup general variational problem rkhs allowing general bounded linear functionals lif xi 
determined coefficients dimension null space differential operator associated spline case 
computed direct formulae sect 

general penalised likelihood approach adjoint 
pg 
uniquely defined spectrum 
chapter 
background function values linear functionals latent variables likelihood see section obtain example non parametric extensions 
penalised likelihood obtained adding penalty functional likelihood just determined coefficients representer theorem proved argument spline case 
general iterative methods required find values coefficients 
bayesian view spline smoothing close section reviewing equivalence spline smoothing bayesian estimation gp model pointed kimeldorf wahba 
positive semidefinite kernel corresponding operator dimensional null space construct rkhs follows 
null space represented orthonormal basis rkhs direct sum 
consider model yi xi zero mean gp covariance function independent 
furthermore ai priori 
hand regularised risk functional yi xi orthogonal projection 
kimeldorf wahba show lies span 
xi 
give numerical procedure computing coefficients 
define fa 
yn show lim fa fixed proof see chap 
straightforward application duality rkhs hilbert space described section 
procedure dealing improper prior awkward necessary rkhs induced rich 
case spline kernels constrained boundary conditions 

bayesian gaussian processes note parametric extension non parametric gp model sensible rich principle leading semiparametric models partial splines 
details models refer chap 
chap 

maximum entropy discrimination 
large margin classifiers regard gps building blocks statistical models way parametric family distributions see section examples 
statistical methods estimate unknown parameters models follow different paradigms machine learning popular 

probabilistic bayesian paradigm introduced section 
noted section intractable posterior process typically approximated gp 

large margin discriminative paradigm posterior process obtained associating margin constraints observed data searching process fulfils soft constraints time close prior gp sense concrete section 
constraints linear latent outputs posterior process gp covariance prior 
relationship bayesian methods penalised likelihood generalised spline smoothing methods discussed section 
large margin methods special cases spline smoothing models particular loss function correspond probabilistic noise model 
attempts express large margin discrimination methods approximations bayesian inference paradigm separation suggested somewhat convincing 
connection paradigms formulated section exposition 
large margin paradigm popular empirical success support vector machine svm introduced 
bayesian gp setting see section likelihood observed data seen impose soft constraints predictive distribution sense functions significant volume posterior violate strongly 
large margin paradigm probabilistic view called minimum relative entropy discrimination chapter 
background mred constraints enforced explicitly 
introduce set latent margin variables datapoint 
gp prior latent function choose prior 
margin prior encourages large margins discussed detail 
minimum relative entropy distribution dq defined subject soft margin constraints xi 

just case likelihood function constraints depend values xi random process 
known information theory sect 
solution constrained problem dq dp exp exp value lagrange multipliers obtained minimising convex function log called dual criterion constraints 
right hand side holds prior see way 
furthermore immediate gaussian process covariance kernel mean function diag yi due factorised form zu zu form depends choice prior margin variables 
jaakkola give examples priors encourage large margins 
example drop quickly order penalise small especially negative margins empirical errors 
order soft constraint margin violations mimic svm situation 
notational simplicity bias term 
modifications straightforward 
original svm formulation seen uniform improper prior 
svm setup choice margin width arbitrary distance re scaled terms prior variance 

bayesian gaussian processes complete dual criterion log log ky 
potential term log identical svm dual objective see 
called hard margin svm margin constraints enforced allowing violations obtained 
converges training data separable prone complicated solutions 
effect potential term solution limited see 
keeps saturating exactly happens svm misclassified patterns 
dual criterion optimised efficient algorithms smo nonlinear potential term introduces minor complications 
just svm sparsity encouraged observed practice 
conclude mred gives complete probabilistic interpretation svm close approximation thereof 
note svm classification seen map approximation bayesian inference probabilistic model loss function correspond proper negative log likelihood 
interestingly mred view points limitations framework opposed bayesian treatment gaussian process model proper likelihood 
recall margin constraints linear latent outputs leading fact mred posterior process covariance kernel prior 
constraints enforce predictive mean move priori predictive variances simply prior ones independent data 
suggests predictive variances error bars estimated simply performing discrimination svms large margin discriminative methods may appropriate probabilistic gp models point discussed detail section 
important lack practical methods model selection svm 
bayesian gp methods general model selection strategy detailed section 
alternatively hyperparameters marginalised approximately mcmc techniques 
model selection techniques sparse potential term acts logarithmic barrier enforce constraints 
smo fact svm criterion quadratic linear constraints 
chapter 
background bayesian gp methods discussed section 
contrast model selection svm typically done variants cross validation severely limits number free parameters adapted 
support vector machine sake completeness briefly review svm binary classification 
details see 
detailed account history svm methods 
algorithm binary classification proposed originally 
svm seen special case penalised risk minimisation generalised smoothing splines see section novelty risk functional continuous 
leads sparse predictors sense coefficients requires constrained optimisation techniques fitting 
recall weight space view section 
simplicity suppress feature mapping write reader keep mind linear functions discussed may live spaces higher dimension input space 
hyperplane separates training data correctly xi 

distance xi hyperplane ui euclidean space hyperplane maximises minimal margin training set solution min subj 

hard margin support vector machine problem 
data separable neglecting points may lead larger margin 
important especially high dimensional feature spaces separations typically possible may small margins due outliers 
soft margin support vector machine problem relaxes constraints introducing slack variables min subj 

term penalises slack value linearly 
introducing lagrange multipliers margin constraints nonnegativity minimising lagrangian primal parameters obtain data matrix contains xi rows furthermore equality constraint 
dual variables eliminated adding readers may parameter front penalty allowing adjust trade prefer re scale feature space inner product equivalent multiplying kernel called variance parameter see section 
applications sense different penalty coefficients ci required modifications straightforward 

bayesian gaussian processes upper bound constraints 
recall lagrange multiplier theory choose dual parameters maximising lagrangian leading dual problem min ky subj 


final discriminant function xx xi lagrange multiplier theorem renders called karush kuhn tucker kkt conditions hold solution corresponding constraint hold equality yi ui 
corresponding patterns called margin support vectors distance separating hyperplane minimal 
yi ui patterns bound support vectors 
support vectors patterns dual variables non zero 
bias parameter determined set margin support vectors 
conclude section mentioning known fact svm compression scheme sense defined section 
words identified set support vectors retraining machine reduced set gives back solution 
follows fact kkt conditions reduced problem full problem non sv dual coefficient set 
follows easily mred simply dropped criterion unique global minimum full reduced problem 
kriging important early application gaussian random field models termed kriging south african mining engineer developed methods predicting spatial ore grade distributions sampled ore grades 
optimal spatial linear prediction roots earlier wiener kolmogorov closeness space may replaced closeness time mainly concerned time series 
fundamental ideas developed fields kriging meteorology name objective analysis see chap 

go details refer chap 
follow 
basic model semiparametric smoothing known feature map zero mean random field covariance function nutshell kriging minimum mean prediction method linear functionals observations chapter 
background 
xn spatial locations xi rg example measures ore grade interested predicting dx area focus error properties general kriging methods typically depend second order properties process assumed gaussian field 
furthermore restrict linear predictors optimal predictor error sense conditional expectation linear gaussian known xi xj xi 
xn unknown simple procedure plug generalised squares estimate 
procedure motivated angles 
restrict attention linear predictors unbiased sense suggested approach minimises error unbiased predictors 
called best linear unbiased predictor 
bayesian motivation constructed way mentioned section 
gaussian prior covariance matrix scales priori gaussian 
posterior mean converges prior uninformative 
equations known long rediscovered areas statistics 
practice kriging methods concerned inducing appropriate covariance function observed data 
empirical frequently method estimating covariance function close origin 
theoretical side stein advocates usefulness fixed domain asymptotics understand relationship covariance model behaviour kriging predictors 
theorem stationary covariance function characterised spectral stein restricts analysis interpolation situations predictions required locations principle supported observations contrast extrapolation studied time series context 
confused distinction interpolation smoothing section 
non trivial kriging techniques smoothing methods 

bayesian gaussian processes distribution 
stein points fixed domain asymptotics depend strongly spectral masses large high frequency components low frequency ones mean function smooth polynomials 
spectral density fourier transform 
general lighter tails smoother sense 
stein advocates expected smoothness central parameter gp prior smooth analytic covariance functions rbf gaussian kernel see section 
important concept highlighted stein see chap 
equivalence orthogonality gps 
essentially gps covariance functions different form equivalent case possible unambiguously decide infinite amount observations fixed region 
basis argue parametric family covariance functions inducing equivalent gps parameters just fixed priori consistent estimation possible 
hand parameters different values lead orthogonal gps learned data fixed priori 
note kriging models generally concerned intrinsic random functions irf generalisations stationary processes frequently spline smoothing context 
nutshell irf non stationary random field spectral density integral diverges environment origin infinite pointwise variance 
generalised divided difference 
xn sense cip xi polynomials total degree variance ciu xi finite serves define covariance function conditionally positive semidefinite xi xj practice uses semi parametric models latent process interest sum irf polynomial total degree coefficients parametric latent variables 
aside note presence bias term uninformative prior svm see section means algorithm works just way conditionally positive semidefinite order 
long equality constraint holds ky non negative 
probability measure equivalent null sets mutually absolutely continuous see section 
orthogonal null set mass 
gaussian measures orthogonal equivalent 
fact maps basis mentioned obtained posterior expectation uninformative prior parametric coefficients 
chapter 
background fact add generality high frequency behaviour process integrable complement environment irf written uncorrelated sum stationary nonstationary part outside environment smooth 
discussed detail thesis see 
choice kernel 
kernel design tendency machine learning community treat kernel methods black box techniques sense covariance functions chosen small set candidates 
family kernels typically comes small number free parameters model selection techniques cross validation applied 
approaches surprisingly problems interest machine learning experience invariably shown gained choosing designing covariance functions carefully depending known characteristics problem example see sect 

establishing clear link kernel functions consequences predictions non trivial theoretical results typically asymptotic arguments 
opposed finite dimensional parametric models process prior affects predictions non parametric model fixed domain asymptotic situations see section 
sole aim section introduce range frequently kernel functions characteristics give methods constructing covariance functions simpler elements show techniques obtain insight behaviour corresponding gp 
contains extensive material accessible review 
final part discuss kernel methods discrete spaces noted positive definiteness arbitrary symmetric form function hard establish general 
example sensible approach constructing distance patterns depending prior knowledge proposing covariance function general need positive semidefinite simple criterion prove disprove covariance function 
represented euclidean space kernel see 
form kernel 
kernels property called infinitely divisible 
sch managed characterise infinitely divisible kernels stationary try compute spectral density analytically tractable general 
true general see 

bayesian gaussian processes property unfortunately just hard handle positive 
standard kernels provide list frequently standard kernels 
variance scaling parameter practice offset parameter vb uses vb 
scales variance process vb comes uncertainty bias parameter added process 
applications kernel matrix directly linear systems advised add jitter term kernel improve condition number amounts small amount additive white noise chosen quite small confused measurement noise modelled separately see section 
modifications omitted sequel simplicity 
gaussian rbf covariance function exp isotropic 
inverse squared length scale parameter sense determines scale expected change significantly 
analytic analytic 
stein points xj 
quadratic mean similar formula holds predicted perfectly knowing derivatives depend environment 
wide spread gaussian covariance function strong smoothness assumptions unrealistic physical processes particular predictive variances unreasonably small data 
spectral density exp light tails 
hand smola recommend gaussian covariance function high dimensional kernel classification methods high degree smoothness 
interesting note conditionally positive semidefinite degree see section 
reasons numerical stability vb large 
context kriging see section adding proposed math ron model called effect see sect 
authors criticised practice 
probit noise model obtained adding noise passing result heaviside step 
experiments thesis gaussian covariance function 
chapter 
background context gps time series prediction girard report problems unreasonably small predictive variances gaussian covariance function consider kernels comparison 
shows smoothed plots sample paths 
note effect length scale high degree smoothness 
gaussian rbf smoothed sample paths gp gaussian covariance function 
variance 
dash dotted 
solid dashed consider anisotropic version called squared exponential covariance function exp 
positive definite 
typically diagonal matrix inverse squared length scale parameter wj dimension 
full matrices considered factor analysis type matrices useful intermediate 
important application additional compared gaussian kernel automatic relevance determination ard discussed 
note squared exponential covariance function diagonal seen product dimensional gaussian kernels different length scales corresponding rkhs tensor product space built rkhs dimensional functions see section 

bayesian gaussian processes mat rn class covariance functions called modified bessel covariance functions modified bessel function sect 

show isotropic important feature class smoothness regulated directly 
example times differentiable iff spectral density obtain process rational spectral density continuous time analogue ar time series model 
defines ornstein uhlenbeck process stationary analogue wiener process independent increments 
general polynomial order constant coefficient sect 

note converges gaussian covariance function appropriate rescaling 
mat rn class generalised anisotropic family way gaussian kernel 
show sample function plots values 
note effect roughness sample paths 
paths erratic length scale horizontal region shown 
process differentiable twice 
exponential class covariance functions 
positive definiteness proved mat rn class see sect 

ornstein uhlenbeck covariance function gaussian 
kernel varies smoothly processes quite different properties regimes 
continuous sample paths ensured differentiable sample paths obtained case analytic 
positive definite 
shows sample path plots 
statements hold probability usual 
chapter 
background nu ornstein uhlenbeck nu nu nu smoothed sample paths gp mat rn covariance function 
variance 
upper left ornstein uhlenbeck mat rn 
upper right mat rn dash dotted solid 
lower left mat rn dash dotted solid 
lower right mat rn dash dotted solid 
derived spline covariance function principles 
kernel interest posterior mean functions gp models variational problem rkhs splines order piecewise polynomials see section associated computations number training points knots 
hand technical complications arise spline kernels rks subspaces wm functions satisfy boundary conditions see section 
operator induced spline kernel null space spanned polynomials practice necessary adjoin corresponding finite dimensional space 
spline kernels stationary supported obtain spline kernels circle imposing periodic boundary conditions wm leading stationary 
bayesian gaussian processes exponential smoothed sample paths gp exponential covariance function 
variance solid 
dashed 
gaussian 
kernel cos 
representation follows spectral density discrete 
note sample functions periodic probability 
wahba chap 
shown construct splines sphere iterated laplacian quite involved 
equivalent splines sense defined thin plate spline conditionally positive definite functions see section see details 
kernel discrimination methods polynomial covariance functions popular unsuitable regression problems 
polynomial kernels seen induce finite dimensional feature space polynomials chapter 
background total degree 
interesting note exactly rkhs adjoin conditionally positive definite kernel order thin plate spline covariance function 
hand spline case polynomial parts usually regularised 
karhunen loeve expansion see section write expansion monomials total degree gaussian random coefficients 
regularisation operator see section polynomial kernels worked 
note covariance function kernel infinitely divisible 
shows sample path plots 
polynomials analytic 
polynomial sample paths gp polynomial covariance function 
variance 
solid 
dashed 
euclidean inner product referred linear kernel machine learning literature 
gp models kernel straightforward linear models linear regression logistic regression 
clear weight space view see section linear model regarded gp model kernel technique sense number training points 
furthermore svm linear kernel variant perceptron method maximal running kernel algorithm wasteful awkward due singular kernel matrix 

bayesian gaussian processes stability studied statistical physics 
give example function covariance function called sigmoid kernel tanh ax positive semidefinite see shipped svm packages know 
springs desire kernel expansions look restricted layer neural networks 
correct link mlps gp models neal see section involves limit infinitely large networks 
covariance function corresponding layer mlp limit williams 
practice course possible fit expansions kernels data covariance functions 
underlying theory minimisation rkhs see sections breaks view inference gp model 
practical side flawed results negative predictive variances pop expected 
worse optimisation techniques including svm algorithms rely positive matrices may break 
fact svm optimisation problem convex local minima general constructing kernels elementary parts construct complicated covariance functions simple restricted ones easier characterise stationary isotropic covariance functions see section 
large number families elementary covariance functions known reviewed section 
generalisation stationary kernels conditionally positive semidefinite ones stationary fields frequently models see section discussed 
class positive semidefinite forms formidable closure properties 
closed positive linear combinations pointwise product pointwise limit 
covariance function finite 
important special case 
example kernel positive variance modified constant diagonal choosing note hagan localized regression model section special case 
general way creating non stationary covariance function parametric model linear assume gp prior chapter 
background integrate parameters see details 
furthermore suppose sequence models priors obtain sequence kernels 
priors appropriately scaled pointwise limit exists kernel 
standard kernels obtained way 
neal showed model size goes infinity prior variances tend accordingly layered models non gaussian priors tend gp due central limit theorem see section 
important modification embedding 
covariance function arbitrary map covariance function special case 
example euclidean space valid kernel induced gaussian rbf kernel 
fisher kernel mutual information kernels examples 
embedding put rigid constraints gp 
example stationary surely 
cos sin sample paths periodic functions 
embedding create non stationary kernels elementary stationary ones 
powerful mechanism starts viewing different way 
squared exponential kernel suppose input subject noise 
different observed locations independent noise variables independent process 
process gaussian mean covariance function determined easily form squared exponential kernel covariance matrix depends similar construction create non stationary covariance functions 
idea generalised considerably shown 
define 
note mahalanobis distance covariance matrix depends isotropic correlation function recall correlation fixed variables mean variance 

bayesian gaussian processes section shown valid correlation function 
proof uses characterisation df see section df 
integral written dr df positive semi definite special case 
equation create new families non stationary kernels isotropic ones 
note fields estimate 
principle specified gps see inference costly 
hand simpler parametric models may sufficient 
unlabelled data abundant possible learn second field source see 
interesting note smooth properties deducible transferred gp correlation function 
guidelines kernel choice choosing kernel task depends intuition experience 
highdimensional tasks suitable prior knowledge available best option may explore simple combinations standard kernels listed 
invariances known may encoded methods described sect 

approximate bayesian gp inference methods discussed thesis principle combinations different kernels lot free hyper parameters adapted automatically 
applies especially diagonal kernels positive 
case chapter 
background low dimensional obtain insight 
stein points usefulness studying fixed domain asymptotics see section 
respect tail behaviour spectral density see section important 
degree differentiability degree smoothness process depends rate decay 
stein recommends kernel families mat rn class come degree smoothness parameter 
stresses importance concept equivalence orthogonality gps see section 
arguments asymptotic nature example clear mat rn class learned accurately limited amount data 
predictions equivalent processes different kernels different 
ways getting feeling behaviour process visualisation option low dimensional 
draw samples process plot follows plots section produced way 
fine grid domain interest 
sample lv ll cholesky decomposition definition 
large approximated incomplete cholesky factorisation see 
process isotropic grid regularly spaced toeplitz structure cholesky decomposition computed see 
repeatedly sampling plotting give idea degree smoothness average length scales euclidean distance expected vary significantly special features learning kernel promising approach choosing covariance function learn data prior knowledge 
example parametric family covariance functions choose parameters order corresponding process model observed data 
model selection fixed family done empirical bayesian method marginal likelihood maximisation see section generic approximation case gp models section 
stein argues citing jeffreys differences important lead consistency large data limit fixed domain 
fine grids smooth kernels gaussian cholesky technique described fails due round errors 
singular value decomposition svd case concentrating leading determined reliably 
matrix toeplitz diagonals main diagonals constant 
proper bayesian solution integrate parameters see section approximated mcmc techniques outcome mixture covariance functions leading expensive predictors 

bayesian gaussian processes procedure typically scales linearly number hyperparameters elaborate heavily parameterised families employed 
important special case termed automatic relevance determination ard mackay neal 
idea introduce hyperparameter determines scale variability related variable interesting prior mean 
example set linear model throwing host different features components place weights diagonal matrix positive hyperparameters 
place hyperprior diag encourages small values priori incentive di di small inducing variance close effectively switches effect predictions 
balanced need components model fit data leading automatic discrimination relevant irrelevant components 
context covariance functions implement ard anisotropic kernel see section form isotropic diagonal positive definite 
example squared exponential covariance function 
wi determines scale variability prior field moves th coordinate axis 
imagine field restricted line parallel axis length scale restriction distance expected change process significant 
wi length scale large field constant direction regions interest 
ard discriminate relevant irrelevant dimensions input variable automatically predictions influenced significantly 
section provides example important ard context difficult real world gp regression problems high dimensional input spaces 
spatial statistics techniques see sect 
frequently 
stationary process semi var 
estimated averaged squared distances groups datapoints roughly distance apart fitted parametric families maximum likelihood 
stein empirical single input choosing covariance function impossible field differentiable gives range techniques including empirical bayesian approach mentioned 
classification models idea local invariance certain groups transformations important 
example recognition handwritten digits influenced translations small angle rotations chapter 
background bitmap 
process latent function classification problem representing log probability ratio classes see section starting applying small transformations group discrimination remain invariant lead significant changes process output sense 
relate notion ard varying invariant directions induce coordinate non linear general irrelevant prediction 
chapter gives number methods modifying covariance function order incorporate invariance knowledge degree reviewing direction omit 
minka pointed instances learning learn prior learning paradigm seen learning gp prior multi task data see 
fact setup standard hierarchical model frequently bayesian statistics implement realistic prior distributions 
access noisy samples assumption sampled different realisations latent process turn sampled process prior 
data sort valuable inferring aspects underlying covariance function 
simple multi task scenario multi layer perceptron fit samples penalised maximum likelihood sharing input hidden weights different sets hidden output weights sample 
idea hidden units discover features important general combination uppermost layer specific 
place gaussian priors hidden output weights gp model covariance function determined hidden units 
generally start parametric family covariance functions learn hyperparameters multi task data marginal likelihood maximisation hierarchical sampling model 
approximate implementation idea reported 
kernels discrete objects mentioned section principle input space restricted group 
example gaussian processes lattices important vision applications form gaussian markov random field sparse structured inverse covariance matrix 
gaussian likelihoods posterior mean determined efficiently conjugate gradients solver embedded trees algorithm wainwright sudderth willsky compute marginal variances 
kernel methods degree rotation results 
loopy belief propagation renders correct mean converges slower numerically unstable 

bayesian gaussian processes methods covariance matrices variables determined spatial relationship associated covariates proposed number problems involving discrete spaces finite countably infinite 
aim section give selected examples 
kernels defined set finite length strings finite alphabet 
string kernels proposed try review 
important applications string kernels distance measures sequences arise problems dna rna biology statistical models build nucleotide sequences 
proposed string kernels special cases convolution kernels introduced haussler 
interesting case discussed extension hidden markov random field hmrf 
markov random field mrf observed variables latent variables clique potentials cd xd ud xd ud subsets components marginalised 
replace clique potential positive definite kernels kd xd ud marginalise result covariance kernel seen unnormalised joint generative distribution 
original mrf structure allows tractable computation algorithm evaluate covariance function efficiently 
example hidden markov model hmm sequences extended pair hmm way emitting observed sequences sharing latent sequence string kernels arise special cases construction 
practice string kernels generally kernels obtained joint probabilities pair suffer ridge problem larger priori attain significant correlation especially long sequences compared 
example models involving dna sequences sequences correlate strongly encode proteins similar function 
standard string kernel sequences strongly correlated obtained common ancestor latent sequence operations insertions substitutions ancestor model motivated evolution genes gives example pair hmm setup 
sequences differ quite substantially regions structure functional part protein depend strongly 
remote homologies really interesting ones close homologies detected simpler statistical techniques process models string kernels 
hand may possible spot homologies going string kernels pair hmrf constructions example building general framework kernels obtained finite transducers 
conceptually simple way obtain kernel embed euclidean space concatenate embedding known chapter 
background kernels example gaussian 
example fisher kernel maps datapoints fisher scores parametric model 
surge interest automatic methods parameterising lowdimensional non linear manifolds local euclidean coordinates 
methods non parametric fit conventional parametric mixture models order obtain parametric embedding obtain kernel 
kondor lafferty proposed kernels discrete objects concepts spectral graph theory diffusion graphs 
finite covariance function simply positive semidefinite matrix 
symmetric generator matrix corresponding exponential kernel defined exp 
hj 
define elements indices matrix positive definite 
fact eigenvectors transformed exp 
practice general exponential kernels computed feasibly large especially general efficient way computing kernel matrices points interest 
possible approximate sampling 
kernel generator matrices linked heat equation hk interesting note infinitely divisible covariance function covariance matrix form 
kondor lafferty interested diffusion kernels graphs special cases exponential kernels 
generator negative called graph laplacian 
construction seen stationary markov chain random walk continuous time vertices graph 
kernel probability time state time interpretation requires true negative graph laplacian implies doubly stochastic 
equation describes heat flow diffusion initial distribution 
idea describe structure sense closeness close points highly correlated covariance function terms local neighbourhood association induce weighted unweighted undirected graph 
correlation points proportional distribution random walk started time 
similar ideas effectively non parametric clustering classification partially 
bayesian gaussian processes labelled data 
kondor lafferty give examples graphs special regular structures diffusion kernel determined efficiently 
include certain special cases string kernels infinite analogue markov chains treated carefully 
situations determined known simple recursive formulae represent representative sample including training set unlabelled data 
generator matrix underlying graph projected representative sample sensible way sparse leading eigenvectors eigenvalues approximated sparse lead approximation low rank optimal frobenius norm 
kondor lafferty note graph regular grid generator matrix converges usual laplacian operator gaussian kernel mesh size approaches 
chapter 
background learning theory field computational learning theory colt concerned basic questions 
particular problem setup described typically set related random variables goal predict outcomes particular variable interest 
accuracy prediction quantified frequentist way imagining variable prediction sampled repeatedly independently counting number times prediction correct 
suppose observe sample problem hand 
sample points need learn predictor attains desired accuracy 
computation spend learn predictor 
question asks sample complexity problem 
second question major importance practice discussed detail 
subfield colt ignores running time complexity issues called statistical learning theory slt 
thesis describes application learning theoretical result nonparametric classification see chapter turns order understand general result specialisation case interest mathematical concepts learning theory today required 
aspects learning theory aims giving flavour inner workings pac vc results 
readers familiar topics need spend time may want glance section notation defined 
sources interested reader tap thorough introductions learning theory 
excellent book distribution free analyses learning algorithms devroye see 
annual colt conference way get familiar 
clear aspects slt implicit wider mathematical field empirical process theory 
reader interested specific treatment kernel algorithms svm number books available 
structure section follows 
section introduce pac framework case interested 
section contains short account concentration inequalities 
section give brief account notions vc theory binary classification 
section comment applicability pac results statistical model selection 
note exposition parts follows closely excellent tutorial learning theory classification bor lugosi 
target variable infinite set specify loss function threshold advance define prediction correct iff loss falls threshold 
framework extended non sampling known structure marko discussed 

learning theory probably approximately correct section set formally define framework colt problems studied 
restrict binary classification problem introduced section training sample xi yi 
xi yi 
concentrate problem finding pac results specific learning method interest 
trivially result gives upper bound sample complexity problem 
corresponding lower bounds typically scope 
pac analyses distribution free assume generated unknown data distribution pac analysis conditioned assumptions distribution result hold source 
variable interested generalisation error gen see section error predictor learned test case drawn independently data distribution 
probably approximately correct pac framework introduced valiant context colt 
suppose predictor computed learning method specified parameter 
pac bound statement form prs gen bound 
called confidence parameter 
upper bound statistic bound independent data distribution computed sample course particular choice learning method depends sample write gen bound 
fact bound guaranteed hold probability random draws accounts probably pac 
learning method chose predictor independent empirical error emp emp converge gen surely law large numbers strong large deviation inequalities obtain rate convergence fact large deviation statements heart pac theory urge reader glance exposition section 
reasonable write bound emp gap gap bound term gap positive typically converges uniformly 
note unusual possible range values defined depends sample size statistics 
thesis refer variable gen emp gap 
approximately pac accounts fact gap positive sample size asymptotically pac bound usually implies sure convergence typically employs borel cantelli lemma show sect 

chapter 
background certain estimator generalisation error empirical error uniformly internal choices algorithm upper bound uniform convergence rate pac statement somewhat stronger holds pac sense finite sample sizes seen dumb algorithm chooses predictor independent sample large deviation inequalities imply strong pac bound 
example gen emp chernoff inequality theorem implies prs log 
algorithm choice restrict finite set values called hypothesis space size say union bound pr pr ei order obtain pac statement prs log log 
ei large hypothesis space infinite union bound applied directly art proving pac bounds begins 
example classical vc theory discussed section essentially gives argument union bound applied context infinite hypothesis spaces classification problems spaces restricted different way 
give rough categorisation pac results 
recall general form 
gap bound term gap independent concrete choice course usually depend characteristics hypothesis space bound called uniform 
words uniform pac bound deals variables sup gen emp supremum hypothesis space 
fixation empirical error statistic bound depends concrete choice motivated law large numbers 
fact statistic converges surely gen fixed class pac bounds focusses cross validation error estimates emp 
historically empirical risk minimisation erm paradigm requires choose predictor minimise empirical risk training sample 
learning theory justified uniform gap bounds difference generalisation empirical error 
uniform bound gives strongest possible statement fixed hypothesis class argued section restriction gap bound term lead unnecessarily loose bounds practice strong statement required 
non uniform bounds tighter practice uniform ones incorporate prior knowledge problem hand nature indifferent sense chosen data distribution maliciously contradict prior assumptions 
gap depend concrete sample apart simple statistics gap bound called data independent priori opposed data dependent posteriori bounds 
distinction somewhat artificial empirical error bound term depends potential merits posteriori bounds discussed section 
notion nonuniform posteriori bounds formalised luckiness framework shawe taylor 
statement pac theorem interpreted care 
pac bound joint statement generalisation error bound term bound 
imagine large number statisticians handed sample drawn independently data distribution 
training testing method evaluating bound term generalisation error method smaller bound term value 
rule possibility example gap heavily correlated empirical error large high probability emp small 
joint pac statement hold probability small empirical error small happen observe sample 
possibility ruled truly having access repeated samples making assumptions data distribution 
phrased differently stronger conditional statement prs gen bound emp non trivial interval chosen priori directly inferred pac statement 
concentration inequalities term concentration inequality touches large diverse field empirical process theory fairly elementary large deviation results martingale convergence theorems deep mathematics 
needless say applications inequalities reach far learning theoretical questions radford neal pointing 
case course wasteful training 
chapter 
background applied 
position provide brief incomplete account borrowing heavily 
interested reader may consult 
neglect cite results properly see original 
concentration inequalities seen generalisations laws large numbers 
state mild conditions sum independent variables close expectation high probability 
closeness measured scale expectation 
concentration phenomenon true wide classes general functions independent variables 
usefulness concentration result study complicated random variable function independent ones replaced studying expected value simpler 
simplest concentration results sums efron stein inequality variance 
xn xi independent variables 
write 
xi xi 
xn furthermore ghost sample distribution xi independent 
furthermore 
xi xi 
xn 
inequality asserts var proof 
proof idea borrows martingale theory considering martingale difference sequence vi 
xi 
xi 
si 
xi martingale called martingale see chap 
si 
xi si vi 
xi 
crucially vi vi vj var loosely speaking cross talk elements martingale difference sequence 
rest proof jensen inequality theorem convexity note efron stein inequality equality sum xi sense sums concentrated 
inequality weakened replacing arbitrary measurable functions 
xi xi 
xn 
useful 
learning theory practice guarantee bounded ci say surely certainly true value change ci ith argument modified arbitrarily 
example ci var concentrated order stronger exponential concentration inequality derived starting bounded differences assumption known hoeffding inequality sect 

pr ez exp fi 
xi 
central proof vi fi conditioned fi vi lies interval size ci 
convexity sx hard see exp vi fi sci chernoff bounding technique see section conditioning fi 
turn observation finish proof 
large number non trivial combinatorial problems lead variables bounded differences 
purposes consider sup emp gen 
bounded differences ci concentrated expected value bounded example vc theory see section 
hoeffding inequality improved lines simple large deviation version bernstein inequality relaxation chernoff bound depends average variances xi improves hoeffding bound variances small technically quite complicated 
rough idea impose self bounding property leading var ez 
details obtain sharper inequalities 
different avenue information theoretic concepts inequalities conditioning reduces entropy convexity relative entropy decoupling martingale arguments 
ideas see 
vapnik chervonenkis theory section task proving pac result decomposed showing variable quantifying maximum deviation chapter 
background generalisation error bound term concentrated bounding expectation 
done hoeffding inequality refinements 
method vapnik chervonenkis classical 
section motivate ideas 
binary classification problem generalised follows 

xn variables set events denote pr xi 
variable interest sup bounded differences ci hoeffding pr ez vc theory allows bound ez terms combinatorial characteristics infinite 
define shatter coefficient sa max 
xn xn largest number subsets obtain intersecting set points events case interested determined classification function hypothesis space xi contains input class label furthermore xi iff classifier determined mistake xi 
sa maximum number different labellings classifiers realise set input points 
expressive sets larger shatter coefficient 
theorem shows ez log sa 
give proof mention central ideas 
introduce ghost sample independent copy xi 
jensen inequality lemma ez sup empirical measure 
crucial step introduce rademacher variables 
independent pr 
pair xi xi distribution sup sup xi 
learning theory purpose introducing condition xi concentrate bounding variable 
function variables conditioned worst case bounded independent values shatter coefficient 
key observation supremum replaced supremum finite representer set possible intersections values xi conditioned size representer set bounded sa 
hard see large deviation inequality rademacher variables leads statement see details 
central ideas ghost sample rademacher variables conditioning replacing finite representer set essentially union bound set size bounded combinatorial shatter coefficient independent random variables 
sense vc technique allows application union bound combinatorial restriction class infinite expressiveness terms different classifications represent data limited 
shatter coefficient relevant defining dimension size dimensionality parameter space 
shatter coefficient hard compute complicated classes bounded terms vc dimension sa bounded sa say shatters subset noted shatter coefficient sa maximum number different labellings yi impose subset classifiers note sa sa simply trivially sa sa 
sa define largest value sa size largest subset shattered 
set 
example consider vector space functions dimension define 
seen follows 
points 
xm embedding rm xi dimension exists rm xi negative 
labelling yi obtained find points 
xm xi rm exist gj 
gj xi labelling achieved 
vc dimension sauer lemma states sa ne chapter 
background second inequality holds clear converges probability data distribution rate convergence bounded terms vc dimension 
case called pac learnable 
hand vapnik chervonenkis showed data distributions converge probability 
results imply method deciding uniform consistency erm paradigm arbitrary hypothesis spaces prove disprove finiteness vc dimension 
detailed account vc theory application linear classifiers svms 
pac bounds model selection suppose set hypothesis spaces models wish select aim method example erm restricted attains small generalisation error 
reason pursuing union may large space avoid overfitting problem see section method directly applied 
principle pac result holds spaces select argued example 
countable set possible invoke multiple testing lemma essentially union bound show minimising pac bound term plus log factor accounting multiple bound application select close optimal large see details 
clear model selection pac bounds justified small sample sizes encountered practice common pac results frequently give tight guarantees values 
define slack bound difference bound value generalisation error 
arguments certainly plausible 
colt publications virtue pac bounds model selection suggested sample sizes slack obviously order magnitude generalisation error 
done plotting bound values generalisation error estimates independent data curves noting respective minimum points somewhat close corresponding minima 
regarded empirical observation way techniques cross validation marginal likelihood maximisation observed fine practice 
example popular svm claimed theoretically justified vc theorems 
known bounds svm trivial region sample sizes svm algorithm feasibly run model ignore issues computational complexity 
practice neglects multiple testing issue allow apply bound simultaneously different values accounting point 

learning theory selection practice typically done minimising bounds 
model selection bound minimisation case sense slack shown smaller variability different values generalisation error 
true infer corresponding minimum points close 
proving robustness slack changing harder show pac result toy situations simply hold 
aware results direction 
order prove modern pac result typically host bounding steps different mathematical ideas applied contribute slack 
bound term usually represents trade fit complexity predictor complexity penalty weighting terms strongly depends bounding techniques 
resulting slack depend significantly 
chapter 
background chapter pac bayesian bounds gaussian process methods pac bayesian theorem mcallester unusual result field statistical learning theory 
theorems employ heavy concepts empirical process theory pac bayesian theorem proved effort simple properties convex functions contributions chapter give simple proof 
mcallester result powerful applicable large number bayes learning schemes highly configurable available task prior knowledge characteristics pac bounds lack extent 
simple direct proof pac bayesian theorem tight practically relevant situations heavy weight theorems struggle give non trivial guarantees 
chapter various extensions remarkable result simple proof points convex legendre duality core principle contrast pac results union bound combinatorics 
apply theorem approximate bayesian gaussian process classification obtaining tight bounds judged experimental studies 
fairly small sample sizes results highly nontrivial outperform kernel classifier bounds compared wide margin 
structure chapter follows 
section introductory stresses need practice bounds strongly dependent learning method training sample task prior knowledge 
section introduce variants pac bayesian theorem proof consider extensions 
theorems applied gaussian process classification section 
section mention related section experimental results handwritten digits recognition task 
chapter closed discussion section 
parts results published previously de chapter 
pac bayesian bounds gaussian process methods tailed section 
data dependent pac bounds pac bayesian theorems section discuss number shortcomings classical vapnik chervonenkis generalisation error bounds tightness practical applications show directions improvement 
introduce number bayesian types classifiers 
pac bayesian theorems motivated way address shortcomings classical results 
need data dependent bounds recall binary classification problem introduced section training sample xi yi 
xi yi 
recall notion pac bound generalisation error introduced section provides estimated upper bound function training sample confidence parameter event bound violated probability random draws useful pac bounds proved practically relevant situations complexity range classification functions response finite training data limited regularisation see section 
limitation problem learning finite data ill posed 
ideas classical vc bounds introduced section 
important note classical theory necessarily developed practical applications mind answer question conditions principle empirical risk minimisation erm hypothesis space uniformly consistent 
question solved completely vc theory learnable iff vc dimension finite 
results linked practically successful schemes svm see section 
applied real world situations reasonable samples size non trivial models classical vc bounds developments low zero impact bound values typically lie orders magnitude truth 
theoreticians tend brush away objections citing minimax lower bounds match vc upper bounds fairly closely asymptotically 
argument holds think pac bound statement written blindly uniformly applied statistical problem hand 
misleading context real world statistics setup lower bounds different pac scenario 
pac bound holds data distributions value depend strongly 
data dependent pac bounds pac bayesian theorems data distribution 
modern bounds orders magnitude smaller data distribution corresponds prior assumptions encode constructed malicious way 
practitioners may concerned issue statistics natural science nature malicious data distribution constructor 
exactly setup minimax lower bounds particular statistical method distributions lead slow learning 
worst case distributions unusual constructed order exploit weaknesses learning method 
note researchers interested bounding rate convergence gap precise gap bounds 
problem insist rate bounds independent data distribution worst case distributions sensible distribution light model 
order obtain useful rate bounds general show consistency model restricted regularised appropriately ideally theoretical results guide limiting choices practice 
result depend true source general support single important principle statistics obtain information possible task adjust model compatible information 
suggest hedge worst case scenarios choosing unrealistically simple models 
fact typically indifferent try encode prior information faithfully worse vote efforts increase method vulnerability worst case scenarios 
possible improve classical vc results practice 
recall gap difference generalisation error empirical error 
classical vc theorem typically hypothesis space finite vc dimension bound gap worst choice bound course hold particular algorithm really time holds just method choosing maximally malicious algorithm knows data distribution perfectly selects hypothesis maximal gap 
may overly ambitious words vapnik solving problem avoid solving general problem intermediate step 
possible shift particularities algorithm definition better solution consider complexity measures vc dimension scale sensitive versions thereof specific algorithm 
alleviate problem classical vc bounds gap bound depends sample size recall section exhibit characteristics typically model learning method completely useless 
chapter 
pac bayesian bounds gaussian process methods bounds called data independent priori contrast posteriori bounds 
fixed predictor empirical error converges true surely empirical error certainly major ingredient bound expression limitation statistic sensible classical setting erm analysed 
bounds interested additional statistics drive data dependent complexity measures potentially information sample merely empirical error size 
classical vc bounds restricted prior knowledge task encoded bound 
possible degree creating hierarchy nested hypothesis spaces growing vc dimension corresponding prior occam razor process complicated practice 
summarise vc dimension hypothesis space unsuitable complexity measure practice 
flexible fine grained hardly adjusted algorithms models prior knowledge 
depend sample shortcomings mentioned luckiness framework proposed alternative 
pac bayesian bounds discussed chapter seen strong realisations framework formally simpler established framework bayesian inference 
important point compromise basic validity pac statements way order construct classification method data dependent bound follow bayesian modelling assumptions heuristics available prior knowledge encoded feasibility constraints probabilistic model prior distributions algorithm prediction derived different heuristic way 
distribution free bound general depend algorithm prior assumptions sample just empirical error bound generalisation error 
extent unknown data distribution compatible assumptions general determine accuracy method observed tightness bound compromise validity theorem 
statement bound holds standard pac assumptions training sample data distribution completely unknown 
data distribution agreement prior assumptions violates influence validity statement 
structural risk minimisation approach deal hypothesis spaces infinite vc dimension example svms feature space 

data dependent pac bounds pac bayesian theorems bayesian classifiers 
pac bayesian theorems recall setup binary classification model defined section family parameterised noise distribution 
assume form bias hyperparameter symmetric 
note need finite dimensional vector 
example application non parametric models identify process 
bayesian analysis model see section requires specification prior distribution 
posterior training sample target probability new point predicted eq predictive classifier sgn 
number related classifiers studied 
bayes classifier predicts sgn eq bayes voting classifier outputs sgn eq sgn 
note terminology non standard authors refer predictive classifier bayes classifier term bayes classifier denote optimal classifier data distribution 
general classifiers predictive bayes bayes voting different distribution symmetric mean agree 
type classifier called gibbs classifier studied learning theory 
test point predicts corresponding target sampling returning sgn plugging sampled parameter vector 
note gibbs classifier probabilistic element requires coin tosses prediction 
note targets test points predicted parameter vectors sampled purpose independent 
situations interested practice gibbs classifier performs somewhat worse corresponding bayes classifier 
discuss relationship detail section 
gibbs classifier method choice reason restricted single prediction 
mcallester proved number pac bayesian theorems applicable gibbs classifiers 
general pac bayesian theorem simply pac bound deals bayes classifiers constructed expectations hypotheses discriminants respect posterior distribution important note need bayesian posterior distribution model chosen learning algorithm 
mcallester readers familiar markov chain monte carlo methods see section note similarity mcmc approximation sample corresponding bayes classifier 
difference typically mcmc sample representing posterior retained predictions gibbs classifier posterior sample 
chapter 
pac bayesian bounds gaussian process methods theorem incorporates directions improvement mentioned section suggests new complexity measure data algorithm dependent adjusted prior knowledge 
importantly measure compatible aims bayesian modelling prior assessment theorem especially suitable applications approximate bayesian algorithms 
section theorem range extensions simple intuitive proof 
pac bayesian theorem gibbs classifiers section prove range pac bayesian theorems gibbs classifiers binary classification problem general multi class scenarios arbitrary bounded loss functions 
simple extension binary bayes classifiers motivated 
binary classification case section mcallester pac bayesian theorem binary classification 
theorem deals gibbs classifier see section mixture distribution may depend training sample referred posterior distribution 
order eliminate probabilistic element gibbs classifier bound gap expected generalisation error expected empirical error expectation 
theorem configured prior distribution parameter vectors 
gap bound term depends strongly relative entropy definition prior 
assume absolutely continuous positive measure 
recall bernoulli relative entropy 
convex furthermore strictly decreasing strictly increasing mapping ber pl pu pl pu pl pu defined 
define ber 
ber seen relative entropy ball radius note due convexity compute interval easily newton algorithm 
clear definition ber limits ber ber 
confidence parameter result 

pac bayesian theorem gibbs classifiers theorem pac bayesian theorem data distribution bound holds probability random samples xi yi 
size drawn data distribution prs gen emp 
arbitrary posterior distribution parameter vectors may depend sample prior furthermore emp sgn yi gen sgn log emp expected empirical error gen expected generalisation error gibbs classifier note probability gen drawn data distribution independently sample 
ber easy compute may awkward certain sit 
frequently approximated lower bounds fairly tight seen derivatives sides follows leading gen emp emp shows gap bound scales roughly empirical error small 
needless say additional lower bound recommended practice 
note mcallester theorem applies generally bounded loss functions hoeffding inequality bounded variables 
generalisation shown section 
special case zero loss techniques tailored binomial variables give considerably tighter results hoeffding bound expected empirical error gibbs classifier small 
theorem generalised various ways 
section multi class version encompasses binary classification case proof serve prove theorem 
slightly shorter direct proof 
complexity measures chapter 
pac bayesian bounds gaussian process methods relative entropy may considered discussed section 
simple weak extension bayes classifier section 
stress line section pac bayesian theorem require true data distribution constrained way depending prior model class 
example analyses pac try characterise learning curves generated model prior analyse generalisation error gibbs bayes classifier averaged data distribution 
generated fixed member family derive convergence rate distance product data distribution bayesian marginal likelihood ep 
clear restrictive assumptions start stronger results achieved pac bayesian theorem 
confusion distributions multiple classes practice classification problems come classes 
tackle problems sufficient number binary classifiers principled data economic multi class model see section 
subsection assume targets predicted class labels 

probabilistic rules mapping input points distributions 
considered 
rule predict test point sampling distribution 

restricting delta distributions recover special case purely deterministic rules 
application pac bound really statistical test embedded experimental design 
binary case interested bounding aspects data distribution generalisation error probability false positives grows number possible questions tested training sample 
especially multi class case useful pac bound support individual designs concerned general properties generalisation error including special case 
hard extend pac bayesian theorem respect 
capture notion probabilistic rules introducing variable 
source random coins required sample 
independent variables fixed independent distributions 
effectively pr deterministic rules mapping 

note corresponding gibbs rule distribution evaluated sampling independently outputting 
experimental design implies finite set 
pac bayesian theorem gibbs classifiers size distribution related unknown data distribution posterior distribution follows sampled data distribution independently variable distribution known function 
example pr expected generalisation error gibbs rule 
generally may interested joint confusion distribution denotes conditional data distribution example order bound probabilities false positives true negatives case binary classification 


coincides joint confusion distribution short general pac bound allows inferences components observed sample state generalisation pac bayesian theorem binary classification 
suppose set size mapping furthermore arbitrary prior distribution parameter vectors choose confidence parameter 
result holds 
theorem extended pac bayesian theorem data distribution bound holds probability random samples xi yi 
size drawn data distribution prs 
arbitrary posterior distribution parameter vectors may depend sample prior furthermore distribution induced data distribution follows pr drawn data distribution independently 
empirical estimate furthermore pr xi yi log log chapter 
pac bayesian bounds gaussian process methods proof theorem section 
note theorem special case theorem obtained set sgn 
theorem renders level high confidence 
words unknown vector rl lies closed convex set 
seen relative entropy ball radius centre multidimensional generalisation ber see equation 
cutting ball planes derive bounds projections ct appendix provide explicit example theorem practice 
major contributions chapter proof theorem 
mcallester theorem special case theorem proof considerably simpler original leads possible avenues generalisation 
bound tighter special case binary classification 
recall notation introduced section 
notational simplicity define pairing possible random sources randomised gibbs rule 
extend prior posterior setting dp dp dr dq dq dr product measures 
define yi expectation unknown data distribution sample xi yi 


simply measures fixed instance fixed rule divergence empirical estimate convenient way 
fix write 
fixed independent sample strong law large numbers asserts converges surely derive strong large deviation inequality 
may expect guarantee remains valid fixing priori sample prior distribution depend sample 
part proof argument sound 
reason particular choice corresponding large deviation inequality tight proved easily 
fixed multinomial distributed 
csisz rner refer type underlying sequence xi yi 
elegant method rules parameterised deterministic set forget altogether 

pac bayesian theorem gibbs classifiers types see sect 
part proof 
shown appendix es nd 
average markov inequality theorem obtain prs 
course essential depend sample words easy prove strong large deviation bounds dumb gibbs rules depend training sample 
example concavity log convexity jensen inequality lemma see log log probability random draws unfortunately quite uninteresting practice 
cornerstone pac bayesian theorem generic way converting bounds dumb priori gibbs rules useful bounds posteriori rules method applied bayes rules see section 
take statement dumb gibbs rule prior ask happens replace posterior fix arbitrary sample show log 
log 
note relative entropy identical relative entropy factor cancels radon nikodym derivative recall definition 
notion convex legendre duality see section prove 
convex convex dual log partition function log ep exp see equation replaced chapter 
pac bayesian bounds gaussian process methods dual parameter measurable dp 
see consider ep exp 
candidate define gibbs measure dpg ep dp probability measure relative 
relative entropy nonnegative see section ep pg log dq dq dp log ep eq 
furthermore finite density dq dp exists inequality equality log dq dp equation follows 
conclude proof noting convexity relative entropy see section jensen inequality 
holds log log compare inequality inequality dumb classifier see pay penalty replacing prior posterior altogether combine fact fixed implies order conclude hold 
theorem special case theorem obtained setting 
easy see gen gen emp emp emp gen 
theorem follows 
comments seen section proof pac bayesian theorem naturally decomposes parts 
part specific setting consists proving large deviation inequality style dumb gibbs classifier selects rule prior independent sample second part generic quantifies slack inequality pay want replace dumb classifier gibbs rule interested posterior slack quantified directly relative entropy posterior prior 

pac bayesian theorem gibbs classifiers learning method requires select posterior distribution concentrated region deemed high cost paid bound 
prior distribution enter pac bayesian theorem thin air role central parameter tuned priori potentially tighten bound 
choice course constrained requirement independence sample chosen give rise small samples believe observed task 
pac bayesian theorem applied approximate bayesian technique coincides typical bayesian objective coding available task prior knowledge prior distribution model class 
interesting compare terms pac bayesian theorem frequently bayesian model selection criterion log marginal likelihood log log dp see sections 
employ true posterior pac bayesian theorem log dp log dp log log log yi xi normalised negative log marginal likelihood minimised bayesian model selection sum complexity term employed pac bayesian theorem sample average log 
term refer average likelihood penalises training errors 
log marginal likelihood incorporates similar trade pac bayesian theorem complexity term log factors 
just approximation posterior see lower bound log 
broad model class average likelihood small small model class posterior gibbs rule mis classify points training sample leading larger average likelihood 
large model class prior necessarily broad derivative dq dp large region posterior concentrated leading large complexity term 
seen strong instance called stratification technique average continuum possible union bound see section 
chapter 
pac bayesian bounds gaussian process methods put simply density ratio discriminants consider sensible having seen data increase growth model class 
stress number parameters model class sensible measure complexity 
fairly obvious take model class create new adding large number parameters slightly influence rules 
introducing notion effective number parameters helps number defined unambiguously computed feasibly 
example non parametric methods consider section seen having infinite number parameters parameters regularised prior conditioning finite amount data render finite number parameters significant influence predictions 
complexity measure behaves correctly situations argument suggests 
suppose influence rules 
dp dp dp complexity measure ignores distribution 
note proof theorem require union bound see section distinctive advantage pac bayesian method 
nutshell traditional techniques perform sort sphere covering hypothesis space employ loose covering number arguments union bound pac bayesian theorem employs expectations prior changing posterior cost 
slack comes fact linear lower bound convex function see section 
case general bounded loss stated proved pac bayesian theorem case zero loss sets size subsection provide generalisation general bounded loss functions theorem 
proof appendix uses water filling argument due 
adopt notation section assume bounded loss function quantifies loss occurs rule order predict target corresponding true target 
example zero loss binary classification 
notation expectation drawn data distribution notation confused notation tion 

pac bayesian theorem gibbs classifiers xi yi 
fixed surely bounded convergence rate exponential typical large deviation inequality see section looks follows 
nonnegative function fixed pr pr 
require nondecreasing furthermore convex 
simplicity require differentiable 
give examples possible inequalities moment 
choose 
theorem pac bayesian theorem bounded loss functions data distribution bound holds probability random samples xi yi 
size drawn data distribution prs log 
proof appendix 
note function inverted just easily see equation order obtain upper lower bounds 
instantiate theorem zero loss chernoff bound essentially obtain theorem 
fact chernoff bound holds just general bounded loss form theorem 
original formulation uses hoeffding bound see appendix significantly tight chernoff bound expected empirical error far 
specialised large deviation inequalities come form proof probably adapted 
note constraint convex crucial step proof 
specifically upper bound eq eq terms bound eq trivial convex 
extension bayes classifier practice comparing gibbs bayes classifier posterior distribution directly turns bayes variant performs better gibbs variant hardly practice reason 
chapter 
pac bayesian bounds gaussian process methods section closer look relationship suggest simple extension pac bayesian theorem binary bayes classifiers 
introduced gibbs bayes bayes voting classifiers section 
recall thesis assume noise model symmetric sense function yu 
simplicity assume section bias parameter added general bayes bayes voting predictive classifier different interested exclusively case distribution induced symmetric mean case bayes classifier variants agree 
fact easy see nondecreasing classifiers sgn eq identical 
note assumption violated concepts gibbs bayes bayes voting classifiers questionable anyway predictive classifier 
fortunately approximate bayesian methods discussed thesis fulfil assumption 
define errors eq sgn sgn eq 
furthermore gibbs bayes define ea ea expectation data distribution 
intuitively bayes classifier outperform gibbs variant trained model represents data distribution 
assume data distribution identical condition write eq density function 
simplicity write meaning corresponding distributions induced fixed 
consider sampling independently furthermore 
gibbs bayes classifier err sgn sgn sgn sgn sgn differ depending relationship sgn 
sgn sgn pr pr sgn pr sgn pr probability tail note event sgn independent 
conditional difference errors positive sgn showing bayes classifier outperforms gibbs variant situation 
hand easy construct malicious setting gibbs classifier better bayes variant see recall section relevance arguments may limited practice 
known data distribution relate coarser sense noting 
pac bayesian theorem gibbs classifiers remarked lemma 
theorem applies bayes classifiers 
obtained extension efforts really ideally looking 
bound bayes classifier error certainly pessimistic 
second generalisation error bound bayes classifier terms expected empirical error gibbs classifier prefer bayes classifier practice evaluate gibbs variant order obtain performance guarantees 
third argument carry classes 
meir zhang obtained pac bayesian margin bound bayes voting classifier combining new inequality rademacher complexities convex duality step 
discuss result section 
meir zhang result criticised grounds see section render non trivial guarantees experiments see section gibbs theorem certainly 
light practical evidence aim provide pac bayesian theorem bayes type classifiers tight gibbs theorem practically relevant situations knowledge remains open problem 
speculative extensions section take liberty collecting speculative thoughts driven definite 
section note principle complexity measure replaced measures long convex section show essential slack pac bayesian bound written explicitly 
gaining knowledge behaviour slack especially degree dependence hyperparameters may key issue pac bounds model selection practice 
generalisation complexity measures nature proof section allows think generalisations 
recall key transforming uninteresting bound statement pac bayesian theorem characterisation relative entropy terms convex dual see section 
suppose divergence measure convex assumption distributions true bayes voting gibbs classifier 
construct situations show tight principle recall section tightness arguments may misleading practice 
chapter 
pac bayesian bounds gaussian process methods exists convex dual max convex general divergence measure program may feasible entertain 
sure convex second determine convex dual 
third prove large deviation bound form prs polynomial serve equivalent generic part proof followed order obtain pac bayesian theorem asserts log log probability draws feasibility approach depends factors 
course convex feasible compute 
determine convex dual closed form 
second probably difficult prove bound prior hand 
note necessarily constrained particular divergence defined subsection chosen convenience proving 
convex sample pursued program divergence relative entropy 
slack term pac bayesian theorem authors suggested minimise pac upper bounds order model selection 
section argue theoretically justified prove slack bound difference bound value true generalisation error significantly variable hyperparameters selected range interest generalisation error 
straightforward formalise requirement strict pac sense proving probably harder bound 
phenomenon occurs practice non trivial real world examples see section valuable obtain analytical results order understand toy models 
interesting consequence simple proof pac bayesian theorem section essentially write slack analytically 
simplicity deal deterministic rules 
application gaussian process classification inspecting proof see sources slack initial arguments leading plugging final application jensen inequality 
bound leads typically minor contribution pac bayesian gap bound final application jensen inequality fairly tight posterior concentrated leaving typically dominating slack coming wrong choice 
see slack right hand side pg pg gibbs measure defined 
suppose form dq dp xi yi 
example true bayesian posterior obtained xi yi log yi xi 
pg log dq dpg log write pg log 
admittedly expressions useful se expect showing slack small close 
application gaussian process classification section apply pac bayesian theorem wide class gaussian process models binary classification 
class defined section section show bound terms computed member 
sections give specific examples known gp approximations see section 
pac bayesian theorem gp classification section specialise pac bayesian theorem gaussian process models binary classification incorporating wide class approximate inference methods 
gp binary classification model introduced tested random sampling particular architecture 
chapter 
pac bayesian bounds gaussian process methods section logit probit noise model discussed satisfy symmetry condition required 
recall rn denotes covariance matrix evaluated training input points xi ui rn latent outputs points 
non parametric model seen special case scenario section weights complete latent process 
alternatively develop process eigensystem covariance kernel parameterise terms countable number weights weight space view section presentation process view turns simpler 
general class approximation methods interested defined section 
posterior approximated gaussian general form 
schemes covariance matrix restricted form approximation defined terms parameters approximate predictive distribution gaussian mean variance 
order apply pac bayesian theorem case show compute terms defining gap bound value expected empirical error relative entropy 
just empirical average sgn denotes 
relative entropy determined 
obtain log tr ki 
special form simplifies log tr ki defined 
formulae depend generic parameters posterior approximation 
sections show compute relative entropy range concrete gpc approximations 
simple way compute cholesky decomposition ll see appendix 
log log diag tr computed number operations required simply plug terms theorem order obtain pac bayesian theorem gp binary gibbs classifiers 
important note theorem valid prior 
application gaussian process classification model fixed priori free hyperparameters covariance function noise model chosen independently training sample example widely practice choosing parameters maximising approximation log marginal likelihood log see sections compatible theorem statistical test 
easy modify theorem allow model selection finite set size polynomial hyperparameter candidates straightforward application union bound see section 
introduces log term numerator 
choosing hyperparameters continuous optimisation admissible 
note theorem principle applies multi class gp methods see section 
mentioned implementation methods quite involved methods additional approximations required avoid scaling quadratic number classes 
terms theorem computation may analytically tractable special approximations may considered order compromise bound statement 
program subject 
laplace gaussian process classification specialise generic framework previous section laplace gp binary classification discussed briefly section :10.1.1.18.3953
gaussian posterior approximation parameters covariance depend posterior mode 
note order evaluate gibbs classifier predictive variance computed approximate bayes classifier sgn independent see equation 
downside gibbs classifier requires evaluation bayes classifier uncertainty estimate required 
show appendix evaluate gibbs classifier efficiently sparse approximations rejection sampling techniques resulting average case prediction 
experimental results section 
gain insight gap bound value analysing relative entropy term see equation theorem applied laplace gpc 
normal situations extremely small expression dominated term 
assume logit noise 
see yi yi yi yi recall section symmetric bayes bayes voting predictive classifiers predict target 
independence classifiers interpreted weakness gaussian approximation true posterior skew 
error bars classifiers course depend predictive variance 
chapter 
pac bayesian bounds gaussian process methods sigma margin relation margin gap bound part margin example xi yi 
plotted 
maximal converges exponentially quickly behaves 
third term classification mistakes yi render negative contribution gap bound value 
expect bayesian architecture 
method simple solution expense making mistake goal prevent disastrous fitting 
bound penalised higher expected empirical error rewarded smaller gap bound term 
remaining terms 
matrix kd positive definite eigenvalues 
furthermore unit vector fisher courant min max characterisation eigenvalue spectrum hermitian matrices sect 
bz dz max max largest eigenvalue implies coefficients diag 
eigenvalues margin learning theoretical concept frequently context pac bounds averaging bayes classifiers see example section 

application gaussian process classification lie max 
analysing log tr general spectral decomposition see term lie log max max bounds tight 
sparse greedy gaussian process classification practice applicability gaussian process kernel methods severely restricted unfortunate scaling time level inference space 
sparse approximations gp models improve vastly figures considerable practical importance 
chapter discuss sparse approximations detail range different schemes approximate inference 
lie class described section theorem applies gibbs classifier versions 
section focus simple ivm scheme introduced section efficient algorithms discussed thesis 
parametric representation ivm described section appendix 
terms placeholders section active set furthermore 
relative entropy term log tr predictive distribution derived section 
evaluation gibbs classifier costs back substitution computation costs 
expected empirical error computed 
discussed section non sparse equivalent ivm 
cavity tap method theorem applied approximation formulae 
note ivm compression scheme sense defined appendix pac bayesian theorem compression bound holds sparse methods plv see section just 
see section pac bayesian theorem significantly tighter practice standard pac compression bound applied compression scheme 
means positive semidefinite 
see details generalised inequalities 
chapter 
pac bayesian bounds gaussian process methods minimum relative entropy discrimination mred framework probabilistic interpretation large margin discrimination methods svm introduced discussed section 
formulation allows direct application pac bayesian theorem gibbs version corresponding classifiers 
recall prior process mred posterior process covariance ky 
application theorem mred straightforward 
note gibbs variant mred classifier typically useful method practice due failure mred large margin methods general provide non trivial estimates predictive variance 
predictive variance latent variable corresponding prior variance large gibbs variant perform worse bayes averaging points close decision boundary lie close training points 
related section mention closely related 
focus result proved giving pac bayesian theorem bayes voting classifiers opposed theorem holds gibbs classifiers 
section show possible route extending result case gp regression 
literature pac bounds kernel methods large reviewed reader may consult 
provide brief pac bounds section 
shawe taylor williamson pac analysis bayesian estimator 
notion pac bayesian theorems developed mcallester gave proof slightly tight version theorem 
theorems seen special case restricted class cut posteriors log likelihood indicator function 
mcallester theorems number papers 
herbrich graepel theorems obtain margin bound svm 
seeger langford megiddo combine sampling approach pac bayesian theorem obtain bayes classifier version 
langford caruana apply mcallester theorem multi layer perceptrons 
langford shawe taylor provide extension theorem bayes classifiers drawbacks section 
interesting feature shows 
related apply pac bayesian theorem averaging bayes classifiers provide estimates predictive variances 
similar idea proposed unfortunate dependence dimensionality weight space 
considerable literature pac bounds combined mixture classifiers multi layered classifiers 
results see 
theorem meir zhang pac bayesian theorem holds gibbs classifiers practice bayes bayes voting classifiers frequently due typically better performance 
meir zhang pac bayesian margin bound applies bayes voting classifiers recall approximations interested bayes bayes voting classifiers identical see section 
obtain result combining bound proved convex duality step identical second part proof 
section result compare pac bayesian theorem 
fact derive slightly different theorem closer relevant theorem 
meir zhang consider functions sgn 
allowing choice prior define qa fa qa note meir zhang claim theorem holds arbitrary classes fa theorems able reproduce result additional condition fa closed multiplication 
words require fa fa 
similar structural risk minimisation bounds nested hierarchy defined priori unbounded sequence 
distribution pj function equal constant constant 
choose 
theorem data distribution bound holds probability random samples problem removed longer version tong zhang personal communication 
chapter 
pac bayesian bounds gaussian process methods xi yi 
size drawn data distribution prs pr inf log pj log log yif xi min aj 

aj log log proof theorem appendix 
note search equivalent optimisation margin loss functions 
hierarchy aj pj chosen priori 
long remain sensible limits effect precise choice insignificant bound value typically dominated terms 
may chose aj pj grid size 
comparing pac bayesian theorems theorem quite different form theorem corresponding statement bayes classifier mentioned section 
direct analytical comparison difficult expect theorems show merits lucky cases frequently occur practice 
section give comparative arguments pointing serious lack tightness result meir zhang 
empirical comparison section 
interested gp binary classification situation 
ri xi xi 
know gibbs 
easy see xi ri yif xi bayes classifier mistake case independent contribution gibbs 
related empirical error close ri small 
reason bayes variant outperforms gibbs variant lie patterns ri fairly small gibbs high probability prediction bayes ri 
situations patterns small ri significantly right side 
contribution margin loss smaller gibbs error region margin loss drops zero quickly slope gibbs contribution ri see 
loss contribution margin loss margin loss gibbs error normalised margin contribution single pattern expected gibbs error bayes margin loss respectively 
horizontal unit normalised margin yi xi xi 
advantage margin loss expected gibbs error grows smaller dominant term bound theorem scales 
second term poses serious problem bayes bound scales gibbs bound 
issue tightness empirical bayes error empirical margin loss smaller usually case practice 
interesting exactly case authors put forward contrast chapter 
pac bayesian bounds gaussian process methods result mcallester point rightly focus pac studies bayes gibbs classifier usually lower error rates practice 
bound fails exploit properly case low error rates far 
classical vc bounds deal zero loss problem elegantly circumvented bounding relative deviations see 
bound implies bound gap worst case smaller small 
interesting find technique carries pac bayesian theorem 
fact exponential concentration inequalities provided exploit small variance unknown process possibly serve substitute bounded difference inequality proof 
furthermore critical term drops may happen practice see section 
pac bayesian theorem regression obtain pac bayesian theorem regression model 
aware existence theorem comparable tightness theorem theorem regression setting 
note gibbs versions classifiers show acceptable performance practice gibbs variant regression estimator sensible estimate required smooth 
section thoughts pac bayesian theorem regression coming definite 
readers interested speculative material jump section 
details appendix 
recall gp regression model section 
latent process priori gaussian kernel obscured independent noise observation 
non negative loss function lipschitz maximum slope 
loss quantified example empirical risk yi xi 
note convex huber laplace loss examples lipschitz convex see section obtain bound risk proof zero loss situation carry straightforwardly 
ghost sample done clear get rid margin loss function proof theorem done contraction principle see section 

experiments bound risk useful theoretically possible gibbs regression estimate bound probably terms empirical risk gibbs estimate significantly larger empirical risk bayes estimate 
try move lines theorem 
steps proof theorem hoeffding bounded differences inequality see section directly applicable regression setting unbounded loss 
order prove theorem regression setting different concentration argument 
probably necessary sort assumptions bounded variance distribution targets 
remaining part proof bounding term pn done shown appendix 
consists functions qu qa pn es tr 
expectation data distribution 
furthermore tr concentrate mild conditions data distribution kernel function 
example stationary kernel see section tr prior process variance 
show concentration pn boundedness condition pac bayesian theorem gp regression model 
experiments section experiments testing instantiations pac bayesian theorem laplace gp gibbs classifier section sparse greedy gp ivm gibbs classifier section special cases bound described sections 
results indicate bounds tight training samples moderate sizes 
section compare bound state art pac compression bound ivm bayes classifier compression bound soft margin support vector classifier section 
section try evaluate model selection qualities pac bayesian bound applied ivm gibbs classifier 
section strengthen results repeating experiments different setup comparisons bound meir zhang see section 
chapter 
pac bayesian bounds gaussian process methods setup mnist describe experimental design mnist sections come 
design applies sections 
real world binary classification task created basis known mnist handwritten digits database follows 
mnist comes training set test set handwritten digits represented pixel bitmaps pixel intensities quantised bit values 
input dimensionality reduced cutting away pixel margin averaging intensities pixel blocks resulting pixel bitmaps 
concentrated binary subtask discriminating training pool cases test set cases created 
gaussian process prior model parameterised gaussian rbf kernel variance parameter inverse squared length scale 
tasks considered balanced employ bias parameter noise model 
experimental setup follows 
experiment consisted independent iterations 
iteration datasets sampled independently replacement training pool model selection ms training set size nms ms validation set size lms task training sample size note set sampled independently model selection sets ensuring prior theorem independent task training sample 
model selection performed list candidates classifier trained ms training set evaluated ms validation set ms score expected empirical error gibbs classifier ms validation set 
winner trained task training set evaluated test set 
alongside upper bound value theorem evaluated 
confidence parameter fixed 
quote total running times experiments 
timing figures obtained machine implementation 
experiments laplace gpc laplace approximation framework gp binary classification described section application pac bayesian theorem method section 
logit noise model bias term logistic function 
note computation relative entropy term expected empirical error require predictions available online www research att com yann exdb mnist index html 
note smaller values altering upper bound values significantly 

experiments pattern employ sampling techniques appendix 
note complete kernel matrix evaluated stored 
implementation requires buffer size specifications results experiments section listed table 
experiments chose model selection validation set size lms recall test set fixed size 
experiments growing sample sizes corresponding ms training set sizes nms experiments nms experiment 
note nms experiments chosen computational feasibility due considerable size candidate list 
nms emp gen upper gen bayes time table experimental results laplace gpc 
task training set size nms model selection training set size 
emp expected empirical error gen expected generalisation error estimated average test set 
upper upper bound expected generalisation error theorem 
gen bayes test error corr 
bayes classifier 
time total running time run secs 
figures mean width test confidence interval 
note resource requirements experiments today desktop machines computational capabilities 
example experiment completed total time hours memory requirements 
setting expected empirical error estimate test set expected generalisation error lie pac bound expected generalisation error theorem impressive highly non trivial result samples size 
largest experiment done mainly comparison experiment ivm see section 
total computation time hours iteration memory requirements 
note slight improvement test errors upper bound values lie chapter 
pac bayesian bounds gaussian process methods 
gen bayes column table contains test error bayes classifier approximate posterior gibbs classifier attains see section 
note necessarily best obtain bayes classifier model selection done specifically gibbs variant 
laplace gpc case note bayes gibbs variants perform comparably bayes classifier attains slightly better results mentioned section evaluated efficiently 
include results comparison gibbs result implies bound generalisation error bayes classifier see section link weak render sufficiently tight result 
experiments sparse greedy gpc sparse ivm approximation gp binary classification discussed section corresponding pac bayesian theorem section 
comparison laplace gpc training evaluation faster respectively 
bound value computed 
experiments final active set size fixed priori 
training done way discussed section 
experiments reported chose ms training size nms ms validation size lms dms 
note experiments nms lms constellation laplace gpc experiments data subsets order facilitate direct comparisons 
results listed table 
emp gen upper gen bayes time table experimental results sparse gpc 
task training set size final active set size 
emp expected empirical error gen expected generalisation error estimated average test set 
upper upper bound expected generalisation error theorem 
gen bayes test error corr 
bayes classifier 
time total running time run secs 
figures mean width test confidence interval 
compare results ones obtained laplace gpc 
sparse gpc gibbs classifier trained examples attains expected test error upper bound evaluates 
laplace gpc variant significantly lower 
ratio 
experiments upper bound expected test error ratio gap bound expected test error 
note experiment sparse gpc completed total time minutes times faster laplace gpc experiment 
interesting observe sample size results significantly better full laplace gpc task experiment section 
note try optimise final active set size simply fixed priori 
automatic choice heuristics evaluate error datapoints outside active set see section 
gen bayes column table serves purpose column table 
case sparse greedy gpc results show bayes classifier performs significantly better gibbs variant attains competitive results 
possible explanation difference observed laplace gpc obtained inspecting kernel parameters values preferred sparse greedy gpc 
parameter larger sparse gpc latent process larger priori variance 
typically leads increase predictive variances turn introduce sampling errors gibbs predictions 
comparison pac compression bound section experiments order compare result sparse gpc section state art pac compression bound 
note employ bayes gp classifiers gibbs gp classifiers fair compare gibbs specific bound artificially gibbs ified version result typically bayes averaging classifiers 
compression bound applies learning algorithms means selecting subsample si size sample size hypothesis output independent si si 
details appendix state prove compression bound theorem 
examples compression schemes perceptron learning algorithm support vector machine svm sparse greedy ivm 
pac compression bound theorem depends training error remaining patterns outside active set called emp repeated experimental setup section note comparing quite different ways approximating true posterior gaussian laplace approximation mode different posterior mean holy grail bayesian logistic regression see section approximation repeated moment matching see section 
meaningful direct comparison involve cavity tap method see section costly compute laplace approximation 
chapter 
pac bayesian bounds gaussian process methods employed dataset splits 
results table 
emp gen upper table experimental results pac compression bound sparse gp bayes classifier 
task training set size final active set size 
emp empirical error full training set gen error test set 
upper upper bound generalisation error pac compression bound 
figures mean width test confidence interval 
experiments emp achieved runs compression bound tightest case 
experiment upper bound generalisation error factor estimate test set 
ratio worse experiment 
task pac bayesian theorem produces tighter upper bound values pac compression bound 
note compression bound restrictive applies compression schemes 
hand pac bayesian theorem depends strongly model prior assumptions inference approximation algorithm compression bound differentiate algorithms attain level compression empirical error emp 
reader may wonder generalisation errors slightly lower ones reported table 
due fact section evaluated bayes classifier hyperparameter values optimised gibbs variant performed model selection bayes classifier explicitly 
comparison compression bound support vector classifiers compare results sparse gp gibbs classifiers state theart bounds popular soft margin support vector machine svm 
introduced section discuss similarities differences proper gp classification models 
context important note svm algorithm produces sparse predictors 
degree sparseness directly controllable parameter furthermore explicit algorithmic goal svm algorithm maximally sparse expansion 
aim maximise soft minimal empirical margin see section 
svm compression scheme see section 
experiments number support vectors vectors misclassified lie margin tube decision hyperplane feature space see section 
note svm emp misclassified points support vectors 
experimental results sv classifiers pac compression theorem table 
employed dataset splits section 
emp gen upper num sv table experimental results pac compression bound sv classifiers 
task training set size 
emp empirical error full training set gen error test set 
upper upper bound generalisation error pac compression bound 
figures mean width test confidence interval 
experiments higher degree sparsity attained chosen experiments sparse gpc mentioned try optimise degree sparse gpc case leading somewhat better values pac compression bound 
values experiment experiment factors estimates computed test set useful practice 
compression bound applies svm certainly specifically tailored algorithm depend empirical margin distribution 
get better results svm specific bounds 
margin bound commonly justify data dependent structural risk minimisation svm non trivial smaller see 
algorithmic stability bound support vector classification 
fact gap bound value converges zero rate variance parameter goes zero rate 
log correspond severe smoothing 
herbrich graepel older pac bayesian theorems prove bound depends minimal normalised empirical margin 
theorem applies hard margin svc non trivial minimal normalised hard margin feature space dimension hard margin svms svm literature common practice separate covariance kernel write front sum slack variables 
parameter cn gap bound behaves 
view svc linear method feature space induced covariance kernel chapter 
pac bayesian bounds gaussian process methods tend overfit noisy real world data small normalised margins points practice soft margin variant typically preferred 
separate experiment setup dataset splits section training sample size training hard margin svms bias parameter obtained generalisation error estimates test set gen minimum normalised margins generalisation upper bound values upper theorem 
results back simple observation minimum normalised hard margin suitable pac gap bound statistic probably replaced robust noise soft margin sparsity degree combinations thereof 
surprise able find proposed svc specific bound tighter task simple pac compression bound theorem 
bounds model selection results model selection 
opinion issue approached care 
obvious generalisation error bound model selection real world task far reasonable estimates generalisation error task 
argument discussed detail section 
pac bayesian theorem applied gp gibbs classifiers offers highly non trivial generalisation error guarantees real world task described section lie factor estimates test set 
spite fact follow usual conventions results experiment trying assess model selection qualities bound 
sparse greedy gpc 
experiment consisted independent runs 
fixed grid values run sample size drawn training pool method trained configuration fixed evaluated test set 
results shown 
translated upper bound values expected test errors subtracting constant determined average distance points curves 
graph scale left hand side expected test error scale right hand side upper bound value 
plot expected test errors horizontal vs upper bound values vertical 
type plot minimal normalised margin arc cosine maximal angle normal vector separating plane input points mapped feature space 
minimal normalised margin close means mapped input points lie double cone narrow angle line normal vector 
noisy data situation arguably quite happen 

experiments monotonically increasing relationship ideally expect 
dotted curves lines slope fitted corresponding solid curves minimum squares 
ordering 
particular experiment surprisingly monotonically increasing linear correlation upper bound values expected test errors model selection minimising upper bound value worked case 
note constants subtract upper bound curves order bring close expected generalisation error estimates visual inspection order magnitude larger range variation individual curves shown plots 
important direction research gain understanding empirically analytically phenomenon observed slack term significantly larger expected test error variable range hyperparameter values interest see section 
experiments section conclusive show hyperparameters bound may predictive 
suspect really expected empirical error follows expected test error closely difference remains fairly constant 
points empirical error evaluated active set 
argument course ignores fact dependence posterior patterns outside active set com exp gen error upper bound exp gen error upper bound exp gen error upper bound exp gen error upper bound exp gen error upper bound exp gen error upper bound comparing upper bound values expected test errors 
solid line expected test error scale left 
dashed line upper bound value translated scale right 
respective minimum points marked asterisk 
chapter 
pac bayesian bounds gaussian process methods upper bound exp gen error upper bound exp gen error upper bound exp gen error upper bound exp gen error upper bound exp gen error upper bound exp gen error comparing upper bound values vertical axis expected test errors horizontal axis 
dotted line fitted regression line slope 
comparing upper bound values expected test errors upper parts gap bound values expected training errors lower parts 
solid expected test error scale left side 
dashed upper bound value translated 
dash dotted expected training error scale left side 
dotted gap bound value translated 
respective minimum points marked asterisk 
pression scheme ivm chosen favour active set 
theory pac bound tells dependencies 
experiments accounted additional complexity term crude union bound type pac compression bound theorem refined type pac bayesian theorem 
splitting upper bound curves expected empirical errors gap bound values see extension necessary plotted curves common graphs ordering runs graphs 
subplot curves top upper bound dashed expected test error solid curves bottom expected empirical error dash dotted gap bound dotted 
scale correct expected empirical test error curves gap upper bound curves translated downwards different constants 
scale curves omitted 
furthermore graphs show linear correlation expected training test errors poor graphs exhibit fitting model selection minimising expected training error choose large values corresponding narrow kernel width 
exp 
error exp gen error exp 
error exp gen error exp 
error exp gen error exp 
error exp gen error exp 
error exp gen error exp 
error exp gen error comparing expected training errors vertical axis expected test errors horizontal axis 
dotted line fitted regression line slope 
comparing gibbs bayes bounds aim section twofold 
order strengthen results obtained sections repeat experiments different binary classification task 
second planned compare pac bayesian theorem gibbs classifiers implications gp bayes chapter 
pac bayesian bounds gaussian process methods classifier theorem obtained meir zhang 
turned theorem render non trivial bounds task highlighting problem pointed section 
setup mnist similar mnist introduced section differs points 
consider mnist subtask discriminating nines 
available images official training set training pool cases tested images official test set full pixel bitmaps served input points 
repeated experiments section single training sample size fixing active size furthermore nms lms dms 
outcome emp gen upper quite line earlier figures 
order compare pac bayesian theorems gp bayes classifier re ran experiments bayes gibbs variant model selection 
noted section twice value gibbs bound upper bound bayes classifier unsatisfactory apparent superior performance bayes classifier task 
tried compare results bound theorem meir zhang ran problem mentioned section term larger runs 
situation bound give non trivial results prior configuration 
suspect cases bound non trivial scaling square root render suboptimal practice 
note task upper range sample sizes considered experiments 
experimental findings conclude problem practically satisfying pac bayesian bound gp bayes classifiers resolved 
results section pac bayesian bound quite suitable model selection due excellent monotonically increasing linear correlation expected test errors upper bound values different values repeated experiments mnist resulting 
results show upper bound value shortcomings predictor expected test error 
plots left middle columns fixed varying values bail linear relationship notably upper bound underestimates sharp increase test error small recall small translate large length scale smooth discriminant functions decision boundaries 
preference selection results quite different 
gibbs variant preferred larger values bayes variant required larger values 

discussion exp gen error upper bound upper bound varying exp gen error exp gen error upper bound upper bound varying exp gen error upper bound exp gen error upper bound varying exp gen error comparing expected test errors upper bound values mnist 
upper row hyperparameter values vs test errors scale left translated bound values scale right 
lower row test errors vs bound values dotted fitted regression line 
left varying 
middle varying 
right varying 
smooth solutions hints bound placing weight complexity penalty 
fact large values bound grows faster expected test error points direction 
right column plots show upper bound fails predict test error varying fixed favouring smooth solutions prior process variance 
findings somewhat back objections raise section showing model selection current pac bounds risky best 
discussion chapter shown apply mcallester pac bayesian theorem approximate bayesian gaussian process classification methods 
generic bound applies wide class gp inference approximations gp approximate predictive process see sections 
simplified original proof pac bayesian theorem considerably pointing convex duality see section principal underlying technique 
shown generalise theorem multi class settings problem bounding linear functions confusion matrix 
focused gaussian process models pac bayesian theorem principle applied approximate bayesian chapter 
pac bayesian bounds gaussian process methods classification technique 
terms bound depends typically fall simple inference approximation 
additional suggestions section 
chapter sparse gaussian process methods approaching statistical learning problem non parametric method truly data speak 
prior knowledge problem smooth data visualisation prediction need constrain data generation process compatible pre built model 
general principal drawback non parametric methods practice heavy scaling size training sample learned represented virtually uncompressed sample 
predictions require associations test point training points incremental learning 
generally infeasible large samples 
sparse approximations try filter discard redundant points sample notion redundancy depends current belief source 
chapter clarify nature approximations gaussian process inference new schemes employ greedy forward selection filter sample 
show schemes improve scaling non sparse counterparts orders magnitude large samples widening range problems attacked successfully gaussian process models 
structure chapter follows 
section contrasts parametric non parametric methods sets goals 
section develop general theory sparse gp approximations central notion likelihood approximations discussion greedy selection criteria borrowed active learning meet sample subset selection requirements 
schemes focus sparse variants expectation propagation ep algorithm gp models introduced section 
schemes conditional sparse inference developed section simple fast ivm accurate plv 
approximate inference techniques incomplete dealing model selection issue section 
field sparse gp approximations received lot attention relations clarified section 
empirical studies section binary classification chapter 
sparse gaussian process methods regression estimation ivm regression estimation plv 
close chapter discussion section 
parts results obtained collaborations researchers previously published conference proceedings detailed section 
bayesian inference certainly appealing principle simple consistent rules core see section 
computational burden intractable challenge find suitable approximations tailored particular situation hand 
seen section schemes exact approximate bayesian inference gp models typically suffer heavy scaling number training points time training memory training memory represent predictor time prediction test point 
figures characteristic non parametric methods training data uncompressed form represent belief 
contrast parametric methods logistic regression see section multi layer perceptrons typically training time memory test time requirements independent represent information training sample affects belief empirical statistic size independent belief parameterised way scale advantage parametric methods belief typically updated fairly easily time independent new data comes 
drawback approach hard come sensible model hard wired statistics confronted understood source inferences may biased artifacts shortcomings parametric model little actual observations 
furthermore non parametric methods universally consistent represent true source limit infinite data typically case parametric methods size sufficient statistic grow 
relationship parameters represented distributions complicated multi layer perceptron case current approximation techniques bayesian inference give poor results may difficult handle time consuming 
sparse approximations gp models sense try combine flexibility simplicity non parametric methods favourable scaling parametric ones 
priori fixing statistics applied training sample try directly approximate inference full gp model sparse representation posterior belief size controlled inde 
likelihood approximations greedy selection criteria doing try retain desirable properties scheme full gp model 
example sparse approximations discussed retain property predictive posterior process latent outputs gaussian 
likelihood approximations greedy selection criteria section show sparse gp approximations understood likelihood approximations 
introduce number greedy selection criteria discuss potential significance sparse gp techniques 
likelihood approximations motivated section sparse approximation gp inference lead training time memory requirements scale essentially linearly number training points ideally retain desirable property full inference schemes predictive posterior process gaussian see section 
section discuss general framework obtaining approximations 
order retain predictive posterior gp property necessary follow procedure general case see section approximate gaussian define approximate predictive process 
course possible principle replace conditional prior process different gp depending obtain gaussian 
approximation violate property true process sensible 
true posterior approximated gaussian function unnormalised gaussian definition 
suppose really depends subset ui components 
case prediction new datapoint eq eq ui ui ui ui ui 
allow du exist long resulting posterior properly normalised 
chapter 
sparse gaussian process methods ui straightforward compute marginal ui predictions done requiring precomputation see section 
hand gaussian posterior approximation written ar symmetric 
write du diagonal full rank 
general special structure perform inference 
furthermore write 
plugging kt ku xi see order obtain expression predictive variance requires evaluation components need index set size exact conditions sparse clear restrictive constraints iv feasible 
note exactly precision matrix gaussian likelihood approximation constraints iv equivalent depending ui 
restricted likelihood approximation necessary achieving desired scaling general 
interesting note non sparse predictor requiring acceptable training scale restriction iv strictly required shown section 
follows additional influence predictive variance approximations 
choose suitable likelihood approximation model 
example note ep approximation general gp model completely factorised likelihood discussed section implies likelihood approximation diagonal precision matrix diag range gp approximations see section called 
straightforward idea sparse likelihood approximation simply clamp zero 
develop corresponding sparse method section 
general non diagonal may preferable correct errors sparse approximation 
sparse line method see section efficient examples 
optimal form sparse likelihood approximation 
optimality defined relatively relative spline smoothing low dimensional input spaces see section relevant matrices banded sparsity pattern perform inference efficiently 

likelihood approximations greedy selection criteria entropy definition corresponding posteriors certainly sensible criterion 
lemma likelihood prior posterior exists 
need gaussian 
consider set positive functions ui ui property ui exists 
argmin ep ui argmin exp ep log ui 
gaussian unnormalised gaussian ui ui ui conditional mean 
case unnormalised gaussian parameterised terms block inverse covariance matrix 
proof appendix 
optimal likelihood approximation sense minimal intractable context sparse methods optimal approximation criterion tractable form conditional mean ep ui ik ui computed inverting ki 
define ki ep ui ui seen reconstruction orthogonal projection termed kl optimal projection 
section show kl optimal projections combined expectation propagation obtain powerful sparse approximation method gp inference 
greedy selection criteria likelihood approximation fundamentally depends active set 
central part sparse approximation algorithm procedure selecting appropriate finding globally optimal optimal defined non trivial way combinatorial problem heuristics 
pick random general perform poorly difficult classification tasks due resource limitations restrict methods active sets size dmax say time example starting 
shrinking desired size option 
consider methods change size iteration 
advantages include updates posterior approximation done easily scoring chapter 
sparse gaussian process methods methods evaluate candidates updates done larger blocks 
methods greedy forward selection simplest elements included option remove 
methods allow removals may result better final selections typically slower forward selection context cause numerical problems 
optimal greedy selection criterion lead active set decision boundary final predictor close optimal close corresponding non sparse predictor 
shape boundary depends patterns close mis classified ones 
course option train non sparse predictor identify points greedily approximate goal including points judged uncertain mis classified current sparse predictor words prefer patterns compatible learned far 
idea widely related active learning sequential design 
scenario assume massive amount unlabelled data covariates available required select training sample small possible learn classification regression function 
distinguish different problems depending labels available time selection supplied inclusion decision 
facing practical training data prohibitively expensive 
somewhat simpler query studied 
example subset selection scenario 
shown representative unlabelled sample necessary successful fast learning typically requesting labels points extremely occur achilles heel mentioned related problem 
methods regimes related query scheme derived subset selection scheme additional expectation unknown label example sampling current predictor 
criteria consider ones suggested expectation chosen target left away 
current belief having included patterns new updated belief including pattern additionally 
measure amount information gained including decrease entropy ent negative relative entropy info qnew new 

likelihood approximations greedy selection criteria interpretation reduction coding loss switching wrong model source qnew correct see section 
entropy reduction quantifies posterior spread shrinks due new inclusion relative entropy generally quantifies changes posterior 
especially large new qnew puts significant weight low density regions note active learning fast shrinkage version space main aim 
certain sense posterior entropy seen proxy version space volume bayesian context 
mackay notes qnew bayesian posteriors si si respectively ent info expectation yi xi si 
quite different functions yi especially crucial case yi different expect yi xi si 
refer ent differential entropy score info instantaneous information gain score 
note scores typically negative lower values desirable 
criteria widely interesting note hard obtain useful approximation context nonlinear parametric methods multi layer perceptrons mlps straightforward compute case gaussian process models usual gaussian approximations see sections 
example case mlps gaussian posterior approximations poor selection information criteria take approximations truth suffer badly 
gp case gaussian approximations quite criteria evaluated having approximations 
criteria query active learning gp models equally simple subject 
hagan early discussing optimal design gp models assumption large pool unlabelled points available input distribution considered known 
tong koller give method active learning svms see section 
conclude section stressing important point subset selection methods criterion score inclusion candidates able evaluate fast significantly faster computing qnew 
mentioned time train available data significant fraction thereof subset selection required 
presence expensive selection criterion iteration draw small subsample pool pick top scorer 
training time strongly dominated score evaluations version space collects hypotheses space consistent data far 
volume measured relative full hypothesis space see 
cohn discuss active learning mixtures gaussians locally weighted linear regression criteria computed equally easily 
chapter 
sparse gaussian process methods dominant cost scales size subsamples 
looking qnew required compute need case see section 
recommend look cheaper approximation criteria general approximation suggested section 
conjecture sensible criteria cheap expensive preferred time issue 
intuition wrong general simple example appendix shows 
expectation propagation gaussian process models section introduce expectation propagation ep scheme approximate inference gaussian process models 
parametric representations update equations ep heart sparse gp algorithms discussed 
general ep scheme exponential families described appendix urge reader glance material 
short ep builds idea 
intractable posterior belief written normalised product positive sites approximated member exponential family done replacing site site approximation corresponding unnormalised family definition 
site parameters approximations updated sequentially iteratively 
ep update definition exchanges site approximation true site obtain tilted exponential distribution definition outside distribution projected back preserving moments defined 
feasible depends crucially structure sites 
called cavity distribution proportional current belief divided th site approximation tilted distribution proportional times th site 
suppose gaussian process model completely factorised likelihood sense observed output yi depends corresponding latent output ui xi 
case seen section inference conditioned hyperparameters requires merely computation posterior 
un collects latent outputs training points xi yi 

case approximate posterior yi ui note direct projection posterior belief considered intractable iterative approximation ep required 

expectation propagation gaussian process models prior kernel matrix training set matches situation section family gaussians sites ti ui yi ui 
clear sites gaussian gp regression gaussian noise see section posterior worked directly need ep approximation 
note site depends component allows locality property ep lemma 
principle straightforward develop ep gaussian process models site depends small number components case example multi class gp classification see section 
example see sect 

recall notion unnormalised gaussians definition 
due locality property write ti ui ui bi bi diag site parameters site approximation unnormalised dimensional gaussian 
set bi 
reasons numerical stability sensible require bounded away zero threshold 
define mi bi mean ti ui taken gaussian 
set mi 
note ep posterior approximation depends parameters addition kernel matrix 
fact ab lemma 
algorithm starts iterates ep updates definition convergence 
order ep update require marginal ui hi ai 
details update gaussian case log partition function log zi log yi ui appendix 
locality property update change site parameters equations appendix see log yi ui concave ui possible yi tilted marginal ui positive variance 
new value sign second derivative log zi mean cavity marginal ui guaranteed positive log partition function concave 
note update results marginal posterior approximation ui exhibits increase variance 
bi updated adjust aside note setting include gp models 
observed represents certain independent component analysis ica models presence gaussian noise observed mixed signals 
case ui correspond latent sources covariance function inverse mixing matrix noise variances 
chapter 
sparse gaussian process methods diag 
updates dominate cost running ep applications interested follow sparsity pattern burden eased approximations 
element changed updates done woodbury formula lemma 
practice recommend improve condition kernel matrix adding small multiple identity see 
light definition recommended reasons stability restrict components nonnegative 
context different approximation problem minka reports restrictions lead worse results gp binary classification encountered case negative far 
ep gaussian process models proposed 
fact hard see fixed points ep case coincide cavity tap approximation suggested earlier opper winther 
authors derived set self consistent equations statistical physics ansatz solved iteration ep provides simpler typically somewhat faster stable algorithm purpose 
algorithm time complexity requires memory prohibitive large training set sizes see provides basis efficient sparse algorithms developed 
sparse gaussian process methods conditional inference section discuss schemes sparse approximate bayesian inference gp models conditional fixed set hyperparameters 
applied principle arbitrary likelihoods concentrate case completely factorised likelihoods sites arbitrary positive functions component latent vector 
informative vector machine ivm simpler basic scheme seen simplified version sparse line method 
ivm greedy algorithm algorithmically quite different line scheme key merits compared sparse methods high efficiency simplicity implementation 
early version proposed fully developed scheme appears 
projected latent variables plv algorithm expensive potentially accurate scheme 
seen greedy version batch algorithm randomly running training data including note problem ill conditioned plague sparse gp approximations developed 

sparse gaussian process methods conditional inference new points line fashion try obtain efficient scheme ordering candidates heuristic manner 
quite bit challenging simpler ivm accurate likelihood approximation employed plv may render method suitable model selection see section 
informative vector machine nutshell informative vector machine ivm employs likelihood approximation see section follows ep see section leaving site parameters clamped zero 
patterns adf updates done chosen greedy forward selection differential criteria see section 
chief merits method simplicity high efficiency 
mentioned section simple way obtaining likelihood approximation context ep approximation gp models clamp site parameters zero bi 
called active set discussed section greedy forward selection select new patterns included compute score function points selection index 
pick winner 
representation maintain update order run ivm scheme 
easy see chosen purely random representation sufficient training updates predictions require parameters training algorithm run time depending 
random selection strategies perform poorly especially case difficult binary classification regression estimation tasks greedy selection strategy obvious order statement potential value inclusion point need know current marginal ui 
minimal representation provide marginal demand cost evaluation prohibitive selections done large set candidates 
representation allows efficient update marginals ui large number points updated numerically stable way inclusions described appendix 
storage requirements dominated matrix 
fixed active set posterior approximation ui ki ki bi confused active sets defined context constrained optimisation problems see 
polynomial time convex problems task choosing optimal way typically combinatorial complexity 
chapter 
sparse gaussian process methods ll defined 
recall bi parameters active site approximations 
predictive distribution obtained averaging ui ui resulting gaussian process mean variance ki ki xi see equation 
predictive variance required sensible pre compute prediction vector ki 
predictive target distribution obtained averaging 
binary classification probit noise bias hyperparameter 
noise models onedimensional integral approximated gaussian quadrature see section 
fact commonly noise models bayes classifier sgn independent 
follows fact symmetric 
option rejecting fraction test points decision size 
generally uneven loss function decision theoretic sense appropriate task decision minimising posterior risk expected loss 
important fact distinguishes ivm likelihood approximation expensive ones knowledge marginal ui sufficient evaluating range sensible selection scores direct consequence locality property ep models factorised likelihood lemma 
recall differential entropy score information gain score section 
due locality property simply difference marginal entropies inclusion see lemma ent new ui ui new log ai log log ai ui hi ai recall notation definitions section appendix 
marginal known term computed 
note ent proportional reduction log variance ui qnew ui 
information gain score computed equally easily exploiting lemma info new new ui ui 

sparse gaussian process methods conditional inference equation info new ui ui log new ai new ai ai hnew hi iai see appendix 
marginal moments hi ai see compu tation ui hinges knowledge column matrix ki equation called stub completes description ivm scheme algorithm 
algorithm basic informative vector machine algorithm require desired sparsity threshold numerical stability 
diag diag diag 

repeat compute ent equation info equation 
new new bnew computed adf update see appendix 
site updates new bi bnew update representation shown appendix notably diag 
representation described appendix allows keep marginals ui date time algorithm cost inclusion leading total cost small constant 
setup order decide point include pick top scorer points outside strategy wasteful especially iterations impact new inclusions diminishes 
randomised greedy version algorithm selection index strict subset 
iterations marginals corresponding points available scoring 
refer procedure randomised greedy selection rgs 
appendix describe updating scheme representation exploits selection constraint delaying computation stubs long patterns selection index 
inertia selection index exhibits iterations cheaper computational cost 
supported retaining fraction top previous iteration filling new points drawn random 
options range conservative large small risky chapter 
sparse gaussian process methods small large 
early iterations sensible full greedy selections early inclusions typically larger impact final ones complete update marginals cheaper active set small 
implementation allows memory usage limited smaller final active set size 
large necessary add refinements cache hierarchy see section ideas 
may want address problem points include final active set 
important advantages ivm sparse methods svm see section controlled 
algorithm remain resource constraints running time predictable priori 
apart number heuristics may select appropriate resource constrained range 
example error independent validation set monitored predictions requiring number validation points compute error inclusion 
idea monitor error predictive log likelihood remaining training sample outside current active set 
typically underestimates true error selection criteria prefer patterns wrongly classified current predictor may useful deciding 
evaluating stopping heuristics empirically subject 
ivm scheme decision include final 
easy modify scheme allow exchange moves move exchanges ep update step modify site parameters 
appendix give details exchange moves showing representation updated differential scores modified score exchange pairs 
exploring usefulness exchange moves practice scope thesis subject 
potential difficulties include numerical instabilities updating representation larger number exchange candidate pairs scored iteration 
reader may wonder run ep updates convergence final active set chosen 
preliminary experiments frequently noted decline performance additional ep update iterations 
conclusive explanation effect moment merely offer plausible argument subsequent ep iterations improve approximation posterior si si worsening approximation full posterior 
running time complexity attractive feature ivm scheme contrast known kernel methods svm running time requirements estimated 
sparse gaussian process methods conditional inference controlled priori 
ivm scheme rgs requires inclusion current active set size total 
dominating operation inclusion matrix vector multiplication matrix size rgs dominating operations stub updates needs order extend stub size cost inclusion current selection index 
exact cost depends details setup notably retain fraction cache mechanism stubs practical applications involving numerical mathematics gained organising computations dominant inner loops consists simple vector operations large vectors possible 
lies advantage ivm schemes svm trained popular smo algorithm 
dominating computation ivm rgs consists single matrix vector multiplication inclusion smo typically dominated large number elementary operations covariance function evaluations efficiently 
ivm run rgs larger size decreased 
individual stub updates require inner products vectors size sense bundle attain size supported implementation moment 
high efficiency crucial conditional inference described subroutine model selection see section 
measurements show time requirements conditional inference dominate total time computing model selection criterion gradient especially rgs involved 
reason remaining dominating computations gradient efficiently 
efforts speed conditional inference concentrate reducing overhead stub cache bundle stub updates way increase size 
implementation includes feature worth mentioning 
recall small initial active set sizes selection index 

obtain efficient require rows accessible linear vectors 
rgs columns need accessed linear vectors efficient stub updates 
rgs activated principal buffer matrix storing full initially fed stub cache transposed 
matrix large sensible essential transposition place requiring second large buffer 
place transposition non square matrices non trivial efficient algorithms exist 
certainly speed techniques applied code blas operation blas suite provides highly efficient implementations architectures 
vector vector operations supported blas 
version incorporate highly efficient numerical packages blas chapter 
sparse gaussian process methods leave 
projected latent variables contrast ivm employs factorised approximation likelihood plv scheme uses approximation latent variable ui coupled training datapoints called kl optimal projection motivated lemma 
unnormalised gaussian say corresponding likelihood approximation ui ui ui prior conditional expectation recall ki 
gaussian embed mapping scheme renders gaussian posterior approximations ep depend training targets 
apart kl optimality approximation intuitively understood modification sampling model targets draw ui ui ui sampling replace conditional distribution mean distribution ui sampled 
seen section order obtain sparse approximation constrain likelihood depend ui method replacing remaining variables likelihood conditional mean way intrusive 
compared ivm plv scheme somewhat complicated develop implement running time memory requirements larger 
furthermore non gaussian likelihood case tensions arise fact principle underlying approximations kl optimal projections ep incompatible sense discussed suitable forward selection criteria harder compute additional approximations required 
potential gain preferring plv ivm 
experiments done compare methods sections see plv successfully especially conjunction automatic model selection smaller active set sizes intuitive explanation due coupling plv likelihood approximation points active set summarise information neighbouring points accurately 
ivm points outside active set influence predictions weak sense shaping selection plv point takes influence predictions directly ones strongly 
due elaborate likelihood approximation representations updates somewhat complicated plv ivm final algorithm expensive run 
significant simplification lapack done 
best linear predictor squares sense 

sparse gaussian process methods conditional inference obtained special case gp regression gaussian noise see section 
special case fix general site parameters adjusted iteratively ep updates 
address issues 
representation allows stable efficient updates include new points update site parameters 
predictions representation 

update site parameters bi point may may 
include point active set 

score point potential value including 
delving details helpful state key differences simpler ivm scheme serve motivate differences representation updates 
plv ui coupled sites 
means time new index included incentive update site parameters 
second coupling matrix ki ui ui 
coefficients called projected latent variables fulfil roughly role ui ivm scheme 
replaced mechanics plv quite similar ivm 
example order update th site approximation require marginal ui ivm 
form maintained explicitly order map ui 
note ui 
adf updates plv done way ivm pseudo variables ui blurred sites ti ti ui ui ui dui ti ui ui dui ki pi ki ki ik ki pi introduced 
suitable representation allows efficient stable updates core algorithm 
noted form stored 
please note overlaps notation ivm unavoidable reasons simplicity 
variables defined inner representations ivm plv meant local definitions transferred contexts 
fixed ll decomposed cholesky factors ki note ki convenient representation projection matrix 
practice recommend jitter chapter 
sparse gaussian process methods factor covariance function see leads replaced small 
sure cholesky factor computed problems 
ui 
easy see ui lm lm 
qq decomposed cholesky factors 
define posterior written ui ui lq lm predictive posterior obtained 
new datapoint predictive mean variance xi prediction requires ivm 
note predictive mean required pre compute prediction vector 
likelihood yi ui gaussian adjust site parameters iteratively obtain gaussian approximation true posterior 
plv done ep updates 
define diag diag 
update formulae derived appendix 
noted virtually identical updates ivm replaced pseudo variable vector true site ti ui blurred site ti 
note meaning components ui ui ui ui ui ki pi qi 
note independent site parameters plv components computed demand 
change site parameters modify related variables updated 
new new viv vi done technique described section 
order include new point maximum efficiency reduced single cholesky factor training 
numerically stable way doing uses singular value decomposition svd possibly treating eigenvalues close special way 
case full gp inference gibbs reports stability problems precomputation approach 
expect problems alleviated case sparse approximations especially jitter factor 
sparse gaussian process methods conditional inference representation updated appendix contains details 
immediately inclusion site parameters updated 
general updating site parameters inclusion ones probably visited times 
note var measuring covariance define var ci qi ci columns play important role updating scoring equivalents stubs ivm 
opposed maintained updated incrementally columns computed demand 
note update site parameters inclusion new point cost inclusion leading prohibitive cost 
csat circumvents problem choosing line scheme cycling random data deciding individually threshold include point merely update site parameters 
scheme site parameter updates separated inclusion decisions greedy selection 
inclusion set points picked random site parameters updated 
details appendix cost update sweep sites note subtle difference scheme run site parameters fixed gp regression gaussian noise 
update site parameters way general case different fixed values true likelihood gaussian 
kl optimal projection obtain likelihood approximation perfectly compatible ep scheme 
consequence adapted site parameters ep setting final worse approximation true gaussian posterior sense kl optimality due lemma fixed 
scoring points inclusion selection heuristics discussed section applied plv 
ivm differential criteria turned depend old new marginal distributions point question consequence locality property ep factorised likelihood 
plv likelihood approximation factorised anymore sites depend ui coupled 
order score point criteria recall section kl optimal approximation uses projection see section ep uses projections gaussian family 
chapter 
sparse gaussian process methods spend time required include point dismissed infeasible section 
progress considering approximations criteria ivm 
problem included ui coupled sites immediately 
simple approximation ignore couplings site specific new full criterion current approximate posterior new obtained inclusion new computed faster updating representation new cavity distribution obtained removing site pseudo variable ui define approximation new ui bi bi obtained adf update added detailed section general different bi just updated 
furthermore 
contrast computed representation shown appendix special case information gain criterion derivations criteria similar 
fact may coarse approximation cases vastly reduced cost strong argument explore usefulness thoroughly 
intermediates example choose small index set set ui ui 
rd denotes projection matrix inclusion bh may different old values bh defining small criterion computed feasibly shown appendix 
note full information gain criterion obtained 
coupling free 
course raises new questions example choose update parameters bh new coupling evaluating score 
ideas discussed 
sensible update site parameters ep steps evaluating criterion may poorly adapted new situation increased coupling may pay 
hard details developed appendix 
dominating ignored couplings site ui 
note included likelihood approximation depends ui direct coupling variables 
sparse gaussian process methods conditional inference computation updates score computation evaluation need covariance 
holds require elements ki kernel matrix elements new row cost soon prohibitive 
note special case bh bh updating cost evaluating criterion drops 
simple coupling free approximation efficient interleave score computations site parameter updates 
inclusion new point update site parameters new active set randomly chosen points subsequent systematic update run compute score pattern immediately update site parameters 
slightly problematic comparing score values different site parameter settings leads significant gain efficiency 
overview algorithm section give overview algorithm simple coupling free score approximation see section 
algorithm consists stages 
stage active set chosen greedy forward selection desired size attained 
second stage site parameters thoroughly updated resource constraints 
stage consists iterations leads inclusion new pattern initialised picking points random initial setting diag 
algorithm shows schema iteration 
initial update run required site parameters updated inclusion run patterns especially included probably visited times 
resource limits permit recommend note score computations site updates interleaved 
reasons stability update site parameters immediately inclusion new threshold inclusion rejected 
sensible give inertia new iteration contain fraction top previous iteration 
long small 

cost scoring patterns 
extended score approximation extensions simple coupling free selection score approximation motivated section derived appendix 
lead better selections require additional algorithmic details chapter 
sparse gaussian process methods algorithm plv stage iteration leading inclusion pattern 
require representation current active set date 
site parameters fixed pick 
update site parameters see appendix 
pick 

update site parameters compute coupling free score approximation 
pick 

compute coupling free score approximation 
argmax update representation shown appendix 
worked exploration option practice left 
collect remarks 
extended approximations require neighbourhood set pattern ideally chosen uh correlated strongly ui posterior computing optimal neighbourhood structure infeasible 
simple idea determine structure priori correlations gp prior 
choice neighbourhood structure plays subdominant role plv efficient clustering technique prior correlation affinity measure probably sufficient 
order compromise running time requirements limited hierarchical clustering schemes represent subclusters appropriate sufficient statistics 
conditional inference algorithm embedded model selection sensible re select structure depends kernel hyperparameters 
extended case efficient interleave parameter updates score computations 
scored neighbour set 
compute update parameters sites comes keeping date 
extended score computed passing 
interleaving implies compare inclusion candidates 
model selection different site parameter configurations keeping parameter updates scoring separate efficient 
chosen number required column computations acceptable 
contain fraction top previous iteration 
special case site parameters fixed gaussian likelihood simpler 
required score computation see appendix need back substitution saved maintained explicitly see appendix 
model selection far discussed inference conditioned free hyperparameters 
distinctive advantage gp methods kernel algorithms hyperparameters adapted bayesian marginal likelihood maximisation see sections allowing kernels large number free parameters 
section show model selection done sparse gp approximations 
model selection plv follow generic model selection scheme outlined section hyperparameters denoted 
note plv scheme combines posterior approximations different optimality criteria 
ep scheme moment matching approximation marginal likelihood consistent ep fixed points ep scheme correspond saddle points approximation 
ep applied full gp models criterion constructed generalising bethe free energy analogous criterion loopy belief propagation see 
case gaussian field dense prior completely factorised likelihood section criterion called free energy 
plv uses kl optimal projections projections free energy apply strong way sparse case 
usage kl optimal projections suggests standard variational lower bound approximation simpler derive handle free energy 
derivation section fact ui ui easy see eq log ui ui log du 
note minimise upper bound maximising lower bound order remain compatible standard formulations optimisation algorithms 
chapter 
sparse gaussian process methods eq log eq log ti ui ui ui qi integrating ui 
practice minimise sum negative log priors hyperparameters suppressed 
ui lq lm ui ll lemma obtain ui ui log tr 
gradient parameters covariance function noise model derived appendix 
complete plv scheme including model selection works follows 
outer loop minimise hyperparameters gradient optimiser quasi newton see 
optimiser requires evaluations criterion gradient different purposes select new search directions drive line search fixed search direction 
computations done calling plv conditional inference subroutine see section new search direction determined 
line searches configuration kept fixed 
optimisation scenario discussed detail section 
true likelihood gaussian say simpler approximation log log ep 
criterion experiments section equivalent variational details gradient computation criterion 
true likelihood gaussian approximation apply likelihood approximations ep need correspond proper conditional distribution observables model selection ivm comparison plv ivm approximation true posterior crude model selection may suffer fact 
hand ivm applied significantly larger tasks principled approximate bayesian model selection rarely 
principle run scheme plv section maximisation variational lower bound log marginal likelihood 
doing straightforwardly requires complete matrix equation 
model selection randomised greedy selection rgs applicable 
retain advantage approximate criterion 

principle replace true posterior ul ignoring data outside 
full criterion 
ideally chosen corresponding datapoints determine shape true posterior 
luckily greedy selection strategies ivm exactly goal information gathered selection order choose recall section rgs works storing initial parts stubs columns pattern scored stub complete 
stub cache organised ideally contain long stubs patterns render best score values sensible include points largest available stubs course minimises time required complete need criterion computation 
important advantage ivm scheme running time iteration controlled limiting size details developed appendix 
details optimisation subsection comment criteria approximate log marginal likelihood type suggested optimised standard techniques 
model selection problem setup section boils minimise function vector hyperparameters assumed real unconstrained vector 
collects site parameters operates choosing site parameters conditioned fixed plv ivm 
strictly speaking random element computations involves choice assume smooth behaviour 
active set fixed hard compute gradient mapping complicated 
straightforward gradient optimisation attractive 
approach construct sequence argmin minimisation line search search direction constructed gradient sequence standard method conjugate gradients quasi newton see 
motivated approximation see section 
positivity constraints enforced log transformations 
chapter 
sparse gaussian process methods approach closely related variational em variant lower bound maximisation see section 
case nicer properties due fact argmin 
gradients exact case furthermore step guaranteed decrease objective search directions positive inner product gradients 
properties true general approach 
encounter practice oscillatory behaviour observed stages convergence high relative accuracy expected 
note variational em approach minimise line searches 
methods constructing sequence search directions rely fairly accurate line conjugate gradients modified form 
example frequent restarts gradient search direction recommended 
quasi newton methods known rely accurate line searches 
approach share monotone convergence properties local minimum variational em optimisation problem hard 
search directions gradient approximations efficiently practice 
optimisation code incorporates recover logic restarts line search fails proposed direction descent 
critical problem practice choice suitable stopping criterion 
relative improvement criterion value successive iterations suitable measure context due oscillatory behaviour 
choose lags define monitor min 
non increasing sequence non negative 
context convergence assessed size sign small lags possibly combination checking small 
stopping criterion practice means trading unnecessarily long running time danger early spurious convergence see section examples 
line searches efficient updating 
lead problems practice dependence ignored gradient computation line search finite differences small steps match corresponding derivatives projections gradient 
optimisation behaves stable close optimum 
case experiments 

related empirical study find efficient stopping criterion model selection problems section subject 
conclude fact standard optimisation packages may directly applicable model selection problem may result unnecessary inefficiencies certainly negative aspect strategy 
hand modifications required relatively minor essentially possible different criteria line searches computing new search directions assessing convergence robust behaviour presence minor oscillations needed 
furthermore nowadays shelf programming packages rarely solve problems particular structure posed kernel methods svm specialised code run faster 
related sparse approximations kernel methods received lot attention machine learning community 
section review schemes related chapter 
mentioned methods closely related csat opper 
plv scheme differs algorithm greedy forward selection decide inclusions iterates data line fashion algorithm line scheme deciding inclusions removals approximation error criteria 
model selection minimising free energy reported done 
greedy forward selection efficient random traversal approach efficient selection criterion able pick informative points early 
schemes repeatedly remove include points may prone numerical instability matrix update formulae woodbury lemma notoriously ill behaved especially rank addition negative definite 
ivm related sparse line scheme simpler slightly efficient 
csat opper seemingly direct representation see section 
representations equivalent terms running time storage requirements numerical stability advantage allowing randomised greedy selections speed algorithm significantly see section 
factorised likelihood approximation greedy selection criteria section computed representation ivm 
contrast line scheme uses likelihood couples sites may lead accurate approximations 
ivm line scheme 
csat applies sparse gp approximation technique wide range applications including binary classification density estimation wind field prediction 
chapter 
sparse gaussian process methods special case plv regression gaussian noise related sparse greedy gp regression hybrid adaptive splines 
methods greedy forward selection choose patterns corresponding basis functions xi model active set optimal predictor derived straightforwardly important difference sparse greedy gp regression methods lies selection criterion 
smola bartlett luo wahba called expensive criteria section scoring point scales way inclusion 
opinion prohibitively expensive round small number candidates scored total training time strongly dominated score computations 
contrast approximate information gain score see sections evaluated chosen couplings new point ignored 
furthermore smola bartlett luo wahba consider generalisations arbitrary gp models recommend cross validation schemes select free hyperparameters prohibitive 
smola bartlett scheme easily described notation transforming dual variables recall notation section 
scheme sparse map approximation minimise ui ut lm ui sparse sense 
proportional negative log posterior log ui minimised posterior mean lm minimum value min selection heuristic point 
sb decrease obtained allowing component non zero 
shown section computation requires computations 
smola bartlett consider randomised scheme evaluated small random subset size authors sb note regression case update site parameters 
hyperparameter vector section 

related recommend leading training complexity times expensive plv regression 
random subsets fluctuate freely 
typically ends evaluating large fraction full kernel matrix problematic kernel evaluations expensive 
contrast plv requires final active set 
number authors suggest low rank matrix approximations sparse versions conventional kernel algorithms 
mentioned section gp approximations terms matrix diagonal positive holds svm solvers 
active set 
smola sch lkopf williams seeger suggest replacing low rank approximation ik ki giving different theoretical justifications choice see section 
requiring cholesky decomposition 
plugging approximation algorithm renders scaling 
williams seeger suggested random choice see smola sch lkopf greedy forward selection frobenius norm criterion expensive criterion sense section 
different approach cholesky decomposition target matrix row pivoting stopped early obtain low rank approximation 
advantage method finding largest pivot fast corresponds greedy forward selection criterion tr denotes low rank approximation 
trace criterion non negative clear means terms approximation quality authors bounds extremely loose 
contrast frobenius norm tr comes strong approximation guarantees lead efficient method 
incomplete cholesky approximation variants routinely interior points code linear programming 
noted methods share common problem choice depend input datapoints target values completely ignored fact low rank decompositions computed knowing 
powerful technique sparse bayesian gp learning tipping relevance vector machine rvm 
idea consider kernel expansion weight vector automatic relevance determination ard see section 
placing prior diag improper uniform prior compo row pivoting essential feature 
note pivoting required complete cholesky decomposition improve numerical stability 
chapter 
sparse gaussian process methods nents log learning maximising variant marginal likelihood maximisation see section finds real world data components tend fixing corresponding components approximate posterior 
original rvm considered sparse approximation technique sense section requires training time expensive train methods mentioned section refinements shown rvm training incrementally 
authors recommend joint maximisation marginal likelihood posterior sparsity hyperparameters additional kernel parameters provide experimental results 
sampling mechanism motivate rvm correspond proper gp model fact process model 
implies diag conditioned consistent sense kolmogorov criterion see section 
view rvm integrated search coefficients particular way construct sparse gaussian posterior approximation posterior approximation just example section 
viewed rvm falls class schemes described chapter bayesian ard motivation selecting weakened 
clear priori selection greedy maximisation marginal likelihood compares information criteria section experimental comparison may required gain clarity 
version rvm binary classification suggested laplace approximation see section 
older scheme related rvm subset regressors sr model consider sparse kernel expansion wt ki 
bayesian viewpoint model unusual prior depends input points selection particular detail inference approximation 
fact predictive variance var tends far training inputs isotropic standard kernels stark contrast proper gp methods variance maximal 
fair sr model probably intended bayesian 
finite dimensional bayesian formulation generalised spline smoothing method green silverman discussed section suffers problem 
bayesian committee machine bcm different approach training sample split number blocks size say predictors subsamples combined somewhat bayesian way 
argument conditioned query points may test set part subsamples approximately conditionally independent 
assumption committee renders approximation joint posterior query points 
advantage bcm sparse methods discussed far may data 
related prediction 
render approximation predictive distribution gaussian process 
single test point query set case conditional independence assumption strong 
prediction requires bundling test points query sets inconvenient 
furthermore predictions usually interested marginal distributions test points bcm tries approximate joint distribution query points 
bcm scales size query set infinite mixture gaussian process experts method uses committee small gp predictors 
mcmc method sampling dirichlet process see theory leads correct bayesian inference limit 
high dimensional real world data expect markov chain model mix reasonable time example incentive indicator variable switch experts low 
authors experimental results low dimensional data 
tong koller consider active learning strategies support vector machines svms 
noted see section sect 
sparse gp approximations ivm plv applied active learning easily due simple forms criteria section 
contrast tong koller version space oriented approach fairly elaborate 
maxmin approach interesting requires retraining svm twice including point order evaluate criterion 
sparse approximations solution smoothing spline problems see section discussed see chap 

optimisation penalised risk functional restricted functions span finite number basis functions chosen data dependent way 
set representative input points si chosen size smaller basis functions constructed linear combinations si kernel rkhs 
coincides principle methods discussed chapter long si chosen training set bcm includes test input points arguably strategy little known input point distribution 
special spline kernels low dimensional input spaces basis function chosen compact support called splines lead advantages numerical optimisation si spread 
approximation gp linear model components studied 
method requires computations zhu propose priori optimal approximation context version space part hypersphere corresponding normal vectors hyperplanes classify data points correctly 
si need come training set denoted different symbols 
chapter 
sparse gaussian process methods mercer eigen expansion kernel see section input distribution assumed known 
mentioned section gp priors drive methods quite different paradigms probabilistic bayesian large margin discriminative 
chapter focussed sparse bayesian gp learning ideas applied sparse variants large margin algorithms svm 
example framework developed chapter applied straightforwardly mred variant svm see section 
worry estimating predictive probabilities resulting method somewhat simpler implement may argue estimating probabilities effective scores select new inclusion candidates 
standard svm incremental learning proposed 
idea restore kkt conditions see section including new point dual variable adiabatic increments order retain conditions dual variables working satisfying new pattern 
method rely crucially dual quadratic function gradient linear dual variables allows compute exact effects non infinitesimal variations 
authors aim learn support vectors eventually report forward selection strategies straightforward normalised margin ones tong koller suggest criteria svm active learning adapted 
information theoretic criteria applicable due failure svm estimate predictive variances 
section collect additional material complements main parts chapter 
nystr approximations section briefly review basic ideas related details 
recall section positive definite kernel exists uniquely determined hilbert space hs hk admitting reproducing kernel rk reproducing kernel hilbert space rkhs recall terms orthonormal eigenfunctions 
note reason cross validation score generalised cv score computed re training times generalised spline smoothing procedure section possible general penalised likelihood methods 

free choosing weighting measure 
positive density function consider modified inner product dx 
eigen expansion exists tr dxdy 
hilbert space inner product isometrically isomorphic lebesgue measure mapping easy see rk rk 
usual applications gaussian processes spline smoothing methods necessary know eigen representation kernel inner product original underlying variational problem approximated concentrating efforts lower dimensional span basis functions eigenfunctions important 
order able estimate eigenfunctions data certain knowledge true data distribution xk xk large standard approach plug xj obtain dimensional eigenproblem ku diag 
approximate xj nu xk xk expect approximation accurate small deteriorate close plugging back obtain nystr approximation xk xj known fixed surely details see 
knowledge dominating eigenfunctions useful approximation 
recall karhunen loeve expansion zero mean gaussian process covariance note variability prior ensemble concentrated space eigenfunctions leading eigenvalues spectrum decays reasonably quickly 
argued gp regression setting coefficient eigenfunction predictive posterior process damped close zero high probability smaller noise variance data sampled model :10.1.1.18.3953
eigenfunctions associated leading chapter 
sparse gaussian process methods eigenvalues typically dominate predictive estimates prime candidate building basis reduced approximation 
thorough explicit analysis 
spectrum eigenfunctions depend crucially distribution data leading entries estimated reliably large samples mentioned 
dependence spectrum eigenfunctions illustrated case gaussian kernel mixture separated gaussians 
nystr approximation suggests approximating gp covariance linear model basis functions 
equivalent approximating kernel function pu pk means kernel matrix training data approximated pu closest frobenius norm rank matrices 
approximation somewhat related method suggested lanczos algorithm find approximately 
problem finding hard extremely small case requires computation storage complete suggested nystr approximation approximate subset xi 
eigen decomposition ki remaining values filled approximation xk 
equivalent replacing covariance matrix ik ki motivated angles example see section 
index set picked random contrast emphasis greedy forward selection strategies chapter 
low rank approximations kernel matrices provide efficient approximations mutual information 
accuracy hinges sufficiently fast decay kernel matrix authors review tries quantify decay 
stationary kernels rate decay covariance operator spectrum depends tail behaviour spectral density see section input density 
contain empirical studies comparing theoretical decay rates operator eigenvalues nystr approximations computed actual kernel matrices find approximations tend underestimate process eigenvalues 
theoretical rates stationary kernels higher dimensions predict slower decay 
real world data high dimensions lies close low dimensional manifold necessarily imply slow decay kernel matrix spectra data 
essentially argument motivate pca 
frobenius norm tr 
shawe taylor williams provide concentration results lead gap bounds matrix process eigenvalues 
idea relate sums leading eigenvalues covariance matrix covariance operator respective courant fisher characterisation 
recall section eigen decomposition inner product rkhs admitting embedded characterised fourier coefficient vectors fi fi courant fisher min max theorems relate leading ues leading process eigenvalues 
maximum dimensional subspaces feature space denotes projection subspace feature space 
maximising subspace spanned leading eigenfunctions 
relate matrix eigenvalues observe 
xn data matrix feature space xx leading eigenvalues courant fisher theorem feature space just population case simply replacing expectations empirical distribution 
maximising subspace different population case constructed matrix eigen decomposition 
min max characterisation allows infer various inequalities fixed subspace process eigenfunctions concentration results bound difference empirical population expectation 
importance estimating predictive variances section focus binary classification problem 
recall section class approximations bayesian inference gp models interested aims approximate posterior process gaussian 
second order approach naturally leads predictions variance process quantifying local uncertainty posterior mean prediction 
hand valid estimates predictive uncertainty obtained large margin discriminative methods section svm 
svm discriminant output say related decisions svm ivm sgn sgn respectively svm constitute proper probabilistic model see section reason useful posterior mean prediction 
fact svm loss peculiar characteristics essential large margin discriminative method see section differentiable zero curvature flat zero 
corre bias parameter assume added sense point predictions sgn 
chapter 
sparse gaussian process methods negative log likelihood consider sensible model classification noise 
ability produce valid uncertainty estimates comes additional cost training prediction time somewhat longer bayesian gp methods comparable svm techniques binary classification 
platt proposed obtain predictive probabilities ad hoc post processing 
pr logistic function see equation 
scalars fitted post hoc regularised maximum likelihood unbiased version training set 
clearly outputs sold probabilities validity method remains unclear 
svm assumed perform sensible choose obvious decision rule predictive probabilities different svm decision usually perform worse 
suppose intended predictive probability strictly increasing 
define pr uncertainty point prediction decreases increasing 
ranking test set uncertainty exactly 
ranking important applications predictive probabilities see platt estimate provide new 
fact order related problems fully transferred 
apart obvious problems number additional heuristics required practice 
tipping pointed similar weaknesses 
point predictions uncertainty estimates considered incomplete analysis statisticians classification 
hand predictive probabilities important simplest decision problems 
example required decisions result unequal losses utilities 
important argument proper probabilistic models combined integrated simple consistent way larger systems simply consistency elementary probability laws 
concentrate simple decision problem reject option fraction set test points 
experimental results described section hardest binary task 
shows error reject curve ivm svm obtained averaging curves runs 
error rate rejection significantly lower ivm 
shown variability curves simple example multi class classification process model independent gp priors see section 

runs larger svm 
recall test points ranked svm ivm 
probit noise model define odd ivm ranking 
note explicit role predictive variance larger uncertainty prediction shrunk 
believe svm provides prediction posterior mean lead similar ranking ivm 
shows error reject curves ranking criteria 
test error rate ivm svm ivm mean rejected fraction test error rate increasing rejection rate mnist vs rest averaged runs 
see svm performs worst closer invalid ranking ivm 
hand induced test set orderings behave quite differently giving evidence usefulness estimate 
role predictive variances understood clearly plotting misclassified points ordering predictive probabilities done runs 
chapter 
sparse gaussian process methods pred 
mean pred 
std dev 
ratio absolute predictive mean standard deviation ratio misclassified points ivm mnist vs rest 
see selection points large wrong values accompanied large values 
lead suboptimal ordering rejection scenario combination behaves better 
shows relationship test set sorted points predictive mean value 
right hand plot shows mean standard deviation computed local averaging 
region sparsely populated due simplicity task 
noisy roughly linear relationship predictive mean standard deviation hold area real interest 
fluctuates strongly larger seen fluctuations simply random systematically correct predictive means ranking 
conclude shown predicting posterior variances approximate bayesian way provide complete analysis quantifying uncertainty quite essential decision theoretic setups discrimination reject option 
hand uncertainty estimates come small non negligible extra cost compared optimised large margin discrimination methods 
box window size 
run 
platt post processing method requires train svm times leave data validation total cost quite bit higher simply training svm 

experiments predictive std dev 
predictive mean predictive std dev 
predictive mean predictive std dev 
predictive mean test points ivm mnist vs rest 
right plot shows mean standard deviation computed local averaging 
experiments section give results range experiments testing ivm plv schemes real world tasks 
ivm classification digit recognition material part section previously appeared 
mnist handwritten digits database comparing ivm see section svm algorithm see section 
considered unbalanced binary tasks form rest 

mapped 
sampled bitmaps size split mnist training set new training set size validation set size test set size 
run consisted model selection training testing results averaged runs 
employed rbf gaussian kernel variance parameter inverse squared length scale 
model selection done minimising validation set error training random training set subsets size 
goal compare methods performance running time 
svm chose smo algorithm fast elaborate kernel matrix cache see details 
ivm available online www research att com yann exdb mnist index html 
model selection training set run tested methods 
list kernel parameters considered selection size methods 
chapter 
sparse gaussian process methods employed randomised greedy selections fairly conservative settings 
binary digit classification task unbalanced bias parameter gpc model chosen non zero 
simply fixed fraction patterns training set added constant vb kernel account prior variance bias hyper parameter 
ideally vb chosen model selection initial experiments different values vb exhibited significant fluctuations validation errors 
ensure fair comparison initial svm runs initialised active set size average number svs independently re ran svm experiments allowing cache space 
table shows results 
svm ivm gen time gen time table test error rates gen training times time binary mnist tasks 
svm support vector machine smo average number svs 
ivm sparse gpc randomised greedy selections final active set size 
figures means runs 
note ivm shows comparable performance svm achieving lower training times 
conservative settings randomised selection parameters speed ups realisable 
registered shown significant fluctuations training time svm runs stable priori predictable ivm 
ivm obtain estimates predictive probabilities test points quantifying prediction uncertainties 
produced hardest task reject fractions test set examples size 
svm size discriminant output quantify predictive selections random full greedy selection index size retained fraction 

experiments error rate svm ivm rejected fraction test error rate increasing rejection rate svm dashed ivm solid task vs rest 
svm reject distance separating plane ivm estimates predictive probabilities 
ivm line runs svm line exhibiting lower classification errors identical rejection rates 
uncertainty heuristically 
clearly inferior difference pronounced simpler binary tasks 
svm community common combine rest classifiers obtain multi class discriminant follows test point decide class associated classifier highest real valued output 
ivm equivalent compare estimates log predictor pick maximising suboptimal different predictors trained jointly constraints hold latent processes noise model parameters order obtain consistent distribution classes enforced joint model properly calibrated 
ivm estimates log depend predictive variances measure uncertainty predictive mean properly obtained svm framework 
probit noise model done log lies depending value variance shows class largest log need largest value points discussed detail section 
combination scheme results test errors ivm svm figures mean standard deviation runs 
comparing results literature recall experiments images sub sampled size usual 
repeated hardest tasks full mnist input images kernel implementation exploits chapter 
sparse gaussian process methods high ratio zero valued pixels bitmaps 
apart changes setup 
compared svm variants ivm differed parameters randomised greedy selection mechanism size full greedy selection run size selection set retain fraction ivm 
ivm 
ivm 
ivm 
results shown table 
svm ivm ivm ivm ivm versus non gen time versus non gen time versus non gen time table comparison svm vs ivm mnist tasks full size images 
note memory usage limited maximum training set size 
harder tasks svm slightly outperforms ivm variants 
hand nature ivm algorithm allows priori prediction required running time depending parameters ones controlling rgs 
plv regression robot arm dynamics material section previously appeared 
aim study behaviour plv gp regression gaussian noise variance hyperparameter especially model selection marginal likelihood maximisation discussed section 
note employ approximation general variational recall section special case gaussian likelihood plv scheme simplifies considerably site parameters fixed adapted true full posterior conditioned hyperparameters gaussian ep approximation required 
experiments 
experiments simple form approximate information gain selection score see section ignores couplings 
version called info gain 
compared version selects random random method smola bartlett smo bart see section 
deviated author suggestions choose smaller random selection set size 
note priori info gain random complexity smo bart times slower 
chose datasets kin training test cases attributes pumadyn nm training test cases attributes artificially created robot arm simulator highly non linear low noise 
pre processed subtracting linear regression fit normalising attributes unit variance 
linear regression tasks result averaged squared errors 
better approach semiparametric model see sections tried 
cases different random splits training test sets run 
fixed run split experiments order facilitate cross comparisons 
squared exponential kernel positive hyperparameters diag wj vb employed 
model selection kernel successful switch input attributes relevant inference driving wj automatic relevance determination ard see section 
stated experiments repeated times quote medians quartiles plots 
predictive accuracy measured terms average squared error 
averaged test set note depend estimates predictive variance 
learning curves compare full gpr info gain random smo bart 
hyperparameters adjusted maximising marginal log posterior full gpr random subset size nms training set 
shows learning curves kin nms note plot contains upper lower quartiles full gpr horizontal lines table gives corresponding training times 
small sizes smo bart outperforms methods larger random effective 
see curves right sparse methods full training set significantly outperform full gpr subset employ sparser expansions 
case freedom selecting kin anton www igi data html 
pumadyn nm zoubin ghahramani www cs toronto delve 
chapter 
sparse gaussian process methods info gain random smo bart info gain random smo bart learning curves sparse gpr kin 
left sparse methods model selection training sets full gpr 
right sparse methods full training set 
axis active set size axis average squared error 
horizontal lines quartiles full gpr 
points outweighs advantage larger expansion 
random 
experiments time secs act 
set size kin info gain random smo bart kin info gain random smo bart table training time secs methods info gain random smo bart 
info gain similar training times smo bart times slower probably precludes subroutine model selection 
fair compare info gain smo bart equal amounts running time case performed better 
set pumadyn nm lot irrelevant attributes context gpr identified order achieve performance 
adapting hyperparameters nms full gpr leads attributes singled wj 
median average squared test errors 
note traditional techniques cross validation applicable situation due large number hyperparameters 
demonstrate significance ard ran experiment gaussian rbf kernel resulting squared error 
relevant attributes full gpr rbf kernel attains squared error 
see covariance functions hyperparameters essential decent performance stresses importance model selection techniques handle covariance functions 
shows learning curves sparse methods 
methods achieve accuracy comparable full gpr inclusion training cases training time median full gpr info gain random smo bart require respectively 
unreasonably small values info gain exhibits fluctuations leading worse performance random 
fact smo bart times slower info gain weighted slightly faster decay learning curve 
chapter 
sparse gaussian process methods info gain smo bart random learning curves sparse gpr pumadyn nm 
axis active set size axis average squared error 
sparse model selection test selection kernel parameters maximising approximation marginal described section 
smo bart considered due excessive running time 
experiment followed trajectory hyperparameter space full gpr training pumadyn nm nms computed corresponding approximate criterion values info gain random different shows criterion curves info gain left random right 
critical parts trajectory approximation random fairly poor info gain approximation acceptable 
note methods re selected iteration poor approximation random attributed random fluctuations 
larger experiment trained full gpr pumadyn nm 
experiments full gpr info info info full gpr random random random criterion full gpr sparse approximations evaluated hyperparameters 
axis iterations 
nms 
ran identical setup sparse approximate criterion info gain random variant named fixed selected random kept fixed optimisation 
active set sizes 
different profiles observed relevant attributes mentioned singled ard squared error method failed error close 
fact random fixed failed runs info gain successful just 
combinations medians average squared error number iterations optimisation running time successful runs table 
model selection info gain reliable small sizes runs random converged high accuracy poor spurious minima criterion approximation 
successful runs flat plateaus traversed new downwards direction see left 
somewhat surprisingly info gain results larger squared errors scheme smaller compare criterion curves full gpr sparse methods run successful 
right plot sizes largest inverse length scales hyperparameter vectors chosen different methods recall positions initial hyperparameter values chosen recommended 
close useful minimum point task note sharp initial drop criterion values 
optimiser stopped relative improvement gradient size fell small thresholds iterations 
somewhat surprisingly random behave better fixed spurious converge fail run 
expected random escape spurious minima readily re selected random iteration 
chapter 
sparse gaussian process methods method succ 
squared iter 
ms time runs error optim 
secs full gpr info gain info gain info gain random fixed table comparison model selection full sparse gpr methods dataset pumadyn nm nms 
methods 
full gpr info info info random full gpr info info info random left criterion curves model selection optimisations axis iterations optimiser lower solid line full gpr upper solid line random 
right largest values inverse squared length scales wj selected hyperparameter vector 
note dangerously flat plateau curve random fixed similar curve shown 
suboptimal performance info gain may suspect overfitting behaviour 
curve runs full gpr concrete values significantly larger ones chosen 
training full gpr hyperparameters random results squared error suggesting hyperparameters suboptimal 
general empirical bayesian methods lead overfitting 

experiments ivm regression robot arm dynamics aim empirical studies section test model selection strategy ivm scheme proposed section regression estimation task pumadyn nm section direct comparison model selection expensive plv scheme 
setup follows 
dataset splits training test cases 
opposed plv experiments model selection ms done randomly chosen subsets training set size nms employ full training sets ms nms 
employed squared exponential covariance function initial values 
recall sections additional parameters specified ivm scheme 
denotes size ms criterion index approximate full likelihood ms criterion 
randomised greedy selection rgs comes parameters see section 
note cost criterion gradient computation see section 
recall section decent performance pumadyn nm gpr discovery relevant input dimensions essential 
set experiments asked ard effect reproduced reliably model selection faster ivm scheme 
concentrated cases 
full training set employ rgs 
quasi newton optimiser run parameters maximum number iterations limited adopted conservative bound 
effort choose stopping criterion suitable deal final oscillatory behaviour optimiser see remarks section stopped relative improvement adjacent steps fell iteration bound reached 
bimodal behaviour terms predictive squared error observed 
runs failed detect relevant dimensions squared errors 
runs terminated prematurely due small relative improvements 
solution provided runs invariably drive small value value producing predictive means variances quite close respectively 
close result linear regression obtain constant 
repeated experiments lower accuracy thresholds forcing optimiser run full iterations qualitative difference outcomes 
situation different runs successful detecting relevant components 
median averaged squared error median ms running time secs 
figures chapter 
sparse gaussian process methods compared info gain table keeping mind ran twice number optimiser iterations 
ivm slightly outperformed variants listed table runs somewhat longer plv 
note ivm operates larger training set vs plv uses rgs ms selection index size avoid scaling 
repeated experiment conservative rgs setting median squared error runs successful median ms time secs significant speedup decrease performance 
ran ivm full greedy selection ms training subsets nms plv experiments 
runs failed squared error median remaining runs median ms time secs 
course just plv experiments final training run selected parameters done full training set patterns 
see ivm fairly aggressive rgs run large training set robust alternative subsampling training set model selection 
conclude particular task ivm regression ms full training set slightly outperformed plv regression ms subset training set running times comparable 
ivm rgs somewhat complicated especially bookkeeping larger gain probably realised careful implementation 
kept mind ivm running time complexity implementation simple gp regression gaussian noise non gaussian models binary classification see section plv complicated costly case 
hand accurate likelihood approximation plv allows smaller active set sizes inference approximation coarse drive model selection pumadyn nm regression task solved successfully plv see section required ivm 
direct comparison ivm plv complicated fact different approximations marginal likelihood model selection 
ivm variational lower bound section plv employed simpler direct approximation 
probably sensible ivm depends datapoints 
ivm classification digit recognition ii section focus model selection ivm see section binary classification task 
extracted usps handwritten digits database see chap 
containing grey scale patterns size attribute values quantised bits values 
attained values 
database comes 
experiments split training set test set patterns 
class prior distributions somewhat nonuniform represented 
corresponding recognition task quoted hard human error rates estimated 
just section employed rbf kernel variance parameter inverse squared length scale 
hyperprior log gamma mean degrees freedom 
probit noise model unconstrained bias parameter hyperprior computed training data 
focussed binary tasks form rest 

set experiments employed ms selection index size rgs parameters furthermore active set size 
results binary rest tasks shown table 
gen time gen time gen time table test error rates gen model selection times time binary usps tasks 
figures medians runs 
csat quotes results sparse ep algorithm version plv line selection usps rest test error dropping sweeps 
slightly worse result probably hyperparameters csat uses covariance function noise model set hand 
curve suggests choice conservative 
combined binary predictors heuristic manner section deciding class maximum log estimate resulting test error rate comparable results kernel classifiers black box covariance functions see table 
allowed reject fraction test cases sensible avoid patterns maximum log smallest 
doing obtain error reject curve 
csat reports errors single sweep sweeps task subscribing restrictions example single set classifiers selected 
chapter 
sparse gaussian process methods rem 
error rate rejected fraction test error rate increasing rejection rate ivm model selection combined usps task 
see text details 
repeated hardest binary task rest covariance function 
median test error slightly dropped ms running time doubled secs 
inverse squared length scales ard effect visible particular kernel input components irrelevant ms method single 
sharp increase running time due part gradient term computed hyperparameters 
discussion chapter discussed develop sparse approximations bayesian nonparametric gp methods principled manner 
lead algorithms training time scaling prediction controllable parameter converge full bayesian counterparts described generic schemes implement sparse greedy gp approximations greedy forward selection informationtheoretic criteria simple efficient ivm see section potentially accurate plv see section 
schemes demonstrated automatic model selection large number hyperparameters maximising sparse approximations marginal likelihood data mod 
discussion standard optimisation techniques 
schemes provide estimates predictive variances predictive means allowing quantifying uncertainty error bars dealing general decision theoretic setups balanced discrimination 
established empirically methods chapter fit models involving priors hyperparameters large datasets difficult real world tasks 
suggestions section 
chapter 
sparse gaussian process methods chapter final chapter main contributions highlight unresolved issues give suggestions valuable 
done separately main chapters section pac bayesian theorems section sparse gp approximations 
pac bayesian bounds gaussian process methods chapter simple direct proof pac bayesian generalisation error bound generalises mcallester original result 
pointed convex legendre duality see section core technique proof giving example amazing scope power duality principle convex analysis applications relevant machine learning include em algorithm number variational approximations bayesian inference convex notably linear programming 
certain classes bayes type classifiers convex duality provides direct tighter approach global uniform bounds mathematically involved techniques combinatorial dimensions covering numbers 
shown apply pac bayesian theorems large generic class nonparametric gaussian process methods spanning schemes practice having resort heavy weight mathematical concepts practically tight results powerful learning methods notoriously difficult handle conventional concepts 
pac bayesian theorems certainly constitute application convex duality proving distribution free bounds hope examples thesis spark new interest establishing convex duality core technique developing full potential learning theory 
experimental results indicate pac bayesian bounds chapter 
tight practically relevant situations giving useful results stateof art pac bounds kernel classifiers considered 
discussed possible reasons lack tightness classical vc bounds section apply principle current kernel classifier bounds know dependence algorithm data sample typically weak restricted statistics large empirical margin certain degree sparsity bounds hardly configurable task prior knowledge 
contrast pac bayesian complexity measure relative entropy posterior prior flexible configurable depends strongly particular algorithm employed kernel classifier bounds know 
gp case prior knowledge encoded general way choice covariance function 
dual relationship worth pointing 
approximate bayesian techniques simplifications overcome intractability exact bayesian analysis model factorisation gaussian assumptions exactly simplifications allow feasibly pac analysis technique 
fact approximate intractable posterior process gaussian allows compute pac bayesian bound gp models 
sparse gpc approximation computational complexity evaluating bound drops accordingly 
relations gibbs bayes classifiers see section inferred symmetry properties approximate gaussian predictive distribution probably hold true predictive distribution 
example suggests pac average case analyses simplified tightened exact intractable bayesian inference focussed tractable approximations practice example 
course analyses type higher interest practitioners 
suggestions pac bayesian theorem applies widely approximate bayesian inference techniques 
example application parametric models fairly straightforward standard techniques variational mean field laplace approximations 
concrete example easy apply bayesian multi layer perceptrons inference approximations laplace variational mean field 
langford caruana consider different application mlps 
note gaussian approximations employed approximate gp inference natural gp prior excellent existing approximations posterior complicated parametric models mlps poor 
theorem due extreme multimodality posterior degeneracies plateaus ridges contrast commonly noise models 
sparse gaussian process methods applied non bayesian methods resulting classifiers considered probabilistic mixtures ones constructed boosting methods 
gibbs versions classifiers sample component classifiers mixture distribution prediction 
furthermore convex duality decouple posterior mixture classifier defined errors individual component classifiers wider applicability technique proving pac results mixture classifiers mixtures hierarchical mixture experts architecture 
open problems remain knowledge 
meir zhang obtained pac bayesian theorem bayes classifiers see theorem result typically tight theorem gibbs classifiers apparent contradiction observation practice bayes version typically outperforms gibbs version experiments theorem give non trivial guarantees 
refinements result may lead satisfying theorem bayes version 
mentioned section problem scaling square root large leading constant 
case zero loss possible derive variants vc bounds scale small empirical error straightforward apply technique margin loss functions theorem 
second pac bayesian theorem restricted unbounded loss functions required deal regression estimation problem 
bound necessarily depend statistics data distribution variance apply statistics controlled certainty 
third pac bayesian theorems allow model selection training sample continuum models stratification procedures justify model selection countable families unnatural compared elegant union bound free approach pac bayesian technique 
pac bayesian theorem applies hierarchical priors integrating hyperparameters approximately admissible 
empirical bayesian model selection translates integrating spike approximate invalidates bound relative entropy term infinite 
sparse gaussian process methods chapter discussed sparse approximations bayesian nonparametric gp methods developed principled manner 
strict requirements approximations imply conditional inference training gp posterior unimodal approximated gaussian 
chapter 
scales running time memory sparsity parameter controllable chosen smaller negligible loss prediction performance 
time prediction independent scales 
placed special emphasis solutions generic implemented reasonably simply numerically stable way having resort complicated heuristics come favourable running time memory trade time provide range services predictive variances model selection non sparse relatives 
shown sparse gp approximations understood naturally conventional non sparse technique suitable likelihood approximation loosely speaking creates bottleneck reduced number active latent variables 
aim design generic schemes fit large number models interest minimal specific primitives added generic implementation 
achieved building ep algorithm generic simple computations involving low dimensional gaussian integrals likelihood factors 
active set subset active latent variables likelihood approximation depends determined sequentially greedy forward selection 
sophisticated expensive evolutions exchange moves suggested discussed detail 
selection criteria see section proposed active learning sequential design application sparse gp methods novel knowledge 
context gaussian approximations employed bayesian gp techniques computed easily analytically application complicated nonlinear parametric architectures attractive 
computation fast likelihood approximation way similar true likelihood proposed simple decoupling criterion approximation likelihood approximation fully coupled 
importantly methods proposed complete include generic empirical bayesian model selection technique shown effectively experiments difficult large real world tasks heavily parameterised covariance functions setup rarely considered kernel machines literature 
surprisingly basic ingredients methods entirely new 
sparse approximations nonparametric techniques focussing subsets dataset proposed times owing high significance practice 
selection criteria employed established active learning sequential design 
sparse approximation gp expectation propagation scheme developed memory requirements reduced ivm scheme expense slightly increased running time see discussion stub caching trimming section 

sparse gaussian process methods albeit different strategy choosing stress methods rely special features covariance function low input dimensionality accuracy usefulness methods discussed depends covariance matrix data approximated matrix fairly low rank 
opinion prior distributions covariance functions case chosen task prior knowledge constraints technical convenience possible 
inference approximation ideally flexible respect allowing arbitrary covariance functions noise models providing automatic model selection scales linearly number hyperparameters 
suggestions methods chapter generic applicable nonparametric models binary classification regression gaussian noise 
robust regression distributed noise straightforward 
multi class classification ordinal regression ranking require model see section running ep likelihood factors groups latent variables 
implicit general development caveat running time complexity increases factor approximations reduce factor may applicable example laplace gp approximation section scales linearly 
applications nonparametric kernel methods restricted standard supervised statistical tasks classification regression estimation 
csat sparse gp techniques approximate density estimation 
bach jordan employ low rank kernel matrix approximations context approximations mutual information apply contrast function independent components analysis 
friedman nachman propose gaussian process networks model sets random variables complicated nonlinear interactions vague prior knowledge characteristics show learn network structure 
interesting find generic sparse methods developed applicable ambitious expensive applications nonparametric smoothing 
code experiments released public domain pressing issue 
written easily extensible unfortunately user friendly matlab implementation 
decided matlab implementation fairly early implementation allows plug arbitrary noise models long certain primitives implemented 
arbitrary covariance functions 
contains proprietary code numerical recipes moment replaced version released 
chapter 
matlab known poor memory management inability handling iterative code 
project wrap code mex functions called matlab 
appendix general appendix chapter provides general background draw remainder thesis included mainly render self contained 
readers familiar material invited consult chapter match notational conventions 
section defines general notational conventions 
section state properties linear algebra collect useful formulae 
section discuss properties convex functions central part thesis 
section introduces exponential families particular gaussian properties relevant 
section briefly define notions pattern recognition statistical problem thesis concerned 
bayesian inference approximations topic section 
section states basics large deviation inequalities 
notation linear algebra vectors ai 
column default matrices ai written bold face 
matrix aj denotes th column ai th entry 


index sets ai denotes sub matrix formed selecting corresponding entries short notations ak ai 
complement sorted ascending order 
example ai ai aj special vectors matrices defined follows index sets sets data points assumed ordered notation known unordered sets 
appendix general appendix vectors zero ones th standard unit vector 
kronecker symbol 
furthermore identity matrix 
matrices size clear context 
superscript denotes transposition 
diag matrix diagonal 
diag vector containing diagonal diag diagonal matrix diagonal tr sum diagonal elements tr diag 
diag tr operators lower priority multiplication 
example diag diagonal matrix inner product diag denotes determinant square matrix denotes norm vector ai said euclidean norm 
size hadamard product direct product component wise product defined ai jbi function sym 
relations matlab style scalar functions means ai bi ai probability 
miscellaneous distinguish notationally random variable possible values 
vector matrix random variables written way vectors matrices 
distribution density generally notation distribution density function 
random variable denotes expectation expected value event pr denotes probability 
probability space usually clear context clarity additional subscript prs 
abbreviated ep 
ia denote indicator function event ia true ia 
note pr ia 
delta distribution places mass point mass 
sets random variables non empty 
write denote conditional independence conditional distribution depend log denotes logarithm euler base notation means cg constant notation left hand side density 
sgn denote sign sgn sgn sgn 
landau notation defined iff exists constant probability theoretic concepts notation unfamiliar reader 
measure denoted lebesgue measure denoted dx 
measurable set denotes 
linear algebra 
useful formulae mass 
measure finite mass space finite probability measure mass 
probability measure denote distribution 
sets mass called null sets 
called absolutely continuous null sets null sets notation 
theorem radon nikodym states density measurable iff 
case called radon nikodym derivative simply density 
linear algebra 
useful formulae partitioned matrix inverses 
woodbury formula 
schur complements definition schur complement nonsingular matrix 
ordered index set 


complement assume ai nonsingular schur complement ai defined ai ar ar ai ai note ai nonsingular schur complement ai ai furthermore ar nonsingular relation holds ar special case ai ar different size useful relation determinants exist 
assume underlying probability space complete field contains subsets null sets 
generalisations possible pseudo inverses require 
appendix general appendix lemma partitioned inverse equations assume ai nonsingular 
nonsingular iff schur complement ai invertible case ai ai ar ai ai ai ai ai ai ai ai ar ai course simply interchange equations renders woodbury formula useful corollary 
lemma woodbury formula assume ai ar nonsingular 
nonsingular iff schur complements ai ar invertible 
case complements nonsingular ar ai ai ra ar sides equal ai ai ai ai ar ai woodbury formula useful smaller 
straightforward application leads severe numerical instabilities symbolic rewriting tool doing actual computations 
incremental cholesky decomposition see section preferred 
details schur products see 
update cholesky decomposition context linear systems symmetric positive definite matrices cholesky decomposition method choice exact treatment due high numerical stability efficiency simplicity 
definition cholesky decomposition symmetric matrix 
cholesky decomposition exists iff positive definite 
defined ll cholesky factor lower triangular positive elements diagonal 

linear algebra 
useful formulae defined uniquely relation 
computing cholesky factor algorithmically straightforward contrast decompositions pivoting row column permutations necessary 
decomposition costs twice fast matrix inversion 
cholesky factor stored system matrix matrix plus additional vector 
computed systems solved lu furthermore 
back substitutions come cost general matrix vector multiplication 
woodbury formula useful symbolic rewriting tool notoriously unstable presence round errors 
opinion explicit computations cholesky techniques preferred 
update cholesky factor rank changes system matrix 
sequel slightly different variant cholesky decomposition ldl diagonal positive entries diag suppose want compute decomposition vv solve lp back substitution vv pp find ld pp new factor new diagonal matrix turns really depends parameters computed pi 
show parameters come recurrence start iterate 
ti di ti di ti pi needed 
overwrite want update method applies resulting matrix positive definite recurrence start iterate 
di pia si si due simple form back substitutions multiplications factor done 
example matrices form updated sense replaced new cholesky factor done efficiently 
modifications case ll obvious replace right multiply fact proper fastest way invert compute 
unfortunately techniques poorly supported standard matlab 
command performs rank update returns new cholesky factor explicitly allowing implicit form 
appendix general appendix order physically update cholesky factor place scheme described algorithm 
ll 
technique routinely numerical programming come 
algorithm update cholesky factor 
li ipi 
li 

li jpj 
li useful formulae section collect formulae fit 
lemma tower properties random variables yy exists 
var var var 
var var 
convex functions properties convex functions sets central importance thesis immense number applications machine learning statistics 
convex function globally lower bounded hyperplane bound locally tight desired 
powerful consequences simple fact developed comprehensive treatment subject 
literature convex optimisation large refer 
definition convexity subset vector space convex function convex convex 
note equivalent set epi convex 
called strictly convex inequality strict 

convex functions note open twice continuously differentiable strictly convex iff hessian positive definite convex iff hessian positive semidefinite 
suppose inner product space dual space linear functions written half space defined 
convex convex set epi represented intersection half spaces contain property convex sets 
half space contains epi iff sup fact represent epi need consider halfspaces sup domain set supremum exists 
called legendre dual intuitively nicely behaved dual represents terms tangent planes maps gradients offsets 
dom tight affine global lower bound 
expect sup case furthermore convex 
bound 
relationship called legendre convex duality 
open strictly convex continuously differentiable solve differentiating 
due strict convexity equation solution 
gradient space defines mapping legendre pairs 
mapping inverted equality iff legendre pair 
duality illustrated 
equality iff legendre pair 
error local approximation lower bound quantified legendre pair 
called bregman divergence strictly convex iff 
note bregman appendix general appendix sup sup slope slope bregman illustrations convex legendre duality 
left affine tight lower bounds right reconstruction lower bounds bregman divergence 
divergence convex applying procedure dual obtain bregman divergence elements important bregman divergence relative entropy introduced 
see graphical illustration 
examples convex dualities norms 
duals 
note self dual 
important duality featuring relative entropy 
useful inequality directly deduced legendre duality lemma jensen inequality convex random variable finite mean strictly convex equality implies surely 


take expectations 
remainder section introduce information theoretic functions convexity properties extensively thesis 
excellent information theory see 
definition relative entropy probability measures space density dq dp exists 
relative entropy defined eq log dq dp log dq dq 
dp 
convex functions absolutely continuous set 
non negative equal iff function strictly convex 
non negativity follows directly strict convexity log jensen inequality 
called information inequality 
convexity follows log sum inequality see theorem 
note metric symmetric general satisfy triangle inequality usual form 
intuitive motivation terms coding cost 
encode symbols drawn code optimal lose average nats symbol compared optimal code may viewed true distribution approximation applications roles flipped 
distributions absolutely continuous dominating positive measure 
mass concentrated finite set size say 
dominating measure counting measure putting weight 
finite distributions prq log prq 
prp rm dominating measure typically lebesgue measure dw log recall notation distribution density dw dq dw 
special case bernoulli distributions skew coins probabilities heads frequently thesis log log 
convex furthermore strictly monotonically increasing mapping strictly decreasing mapping 
convex representation legendre dual max log maximum measurable defined additive constant 
legendre dual log partition function appendix general appendix log ep exp 
dual parameter log dq dp shown section proof theorem 
relative entropy emerges bregman divergence 
log dp choice legendre pair corresponding easy see including case bregman divergence 
relative entropy derive information theoretic functions 
fix base measure finite need probability entropy defined 
case finite distributions equation normally uses counting measure prq log prq 
case log continuous distributions uniform lebesgue measure finite 
usual remedy subtract infinite part entropy depend argument differential entropy log dw 
entropy differential entropy concave functions negative convex ones 
convex duality representation negentropy max eq log dw obtained lebesgue measure subtracting infinite part sides 
corresponding bregman divergence legendre pair satisfies exp 
exponential families 
gaussians section collect definitions useful properties general exponential families distributions gaussian family 
exponential families definition exponential family set distributions densities exp log exp 
exponential families 
gaussians base measure called exponential family 
called natural parameters natural parameter space sufficient statistics log partition function 
furthermore called moment parameters denotes expectation 
important reasons considering exponential families likelihood function data function sample average sufficient statistics fixed dimensionality independent sample size 
model give rise posteriors exponential family see section members approximating distributions new information incorporated increasing size parametric representation 
familiar distributions form exponential families gaussians see section multinomials gammas natural parameter space convex 
linear affine dependencies components components redundant representation called overcomplete 
called minimal 
note useful properties hold general minimal representations useful practice notationally clumsy 
approach state general properties minimal representations properties special overcomplete representations occasionally 
justified adding linear constraints destroy convexity 
remainder section assume representation minimal 
log partition function closely related cumulant generating function log exp exists iff 
interior cumulants obtained derivatives especially var 
representation minimal see strictly convex legendre duality section obtain lemma natural moment parameters exponential family minimal representation bijective mapping natural parameters moment parameters 
log partition function strictly convex legendre dual log denotes expectation 
conversions done follows 
appendix general appendix convex function moment parameters strictly convex minimal representation cases simple explicit form legendre pair order evaluate 
note called parameters known mean parameters 
conditional distribution exponential family sufficient statistics natural parameters family 
new family typically overcomplete representation minimal re parameterisation necessary want properties minimally parameterised families 
note class exponential family distributions closed marginalisation 
example joint continuous gaussian discrete multinomial variable results mixture gaussians general exponential family 
number exponential subfamilies multivariate gaussian multinomial ones closed marginalisation 
lemma product exponential distributions product densities unnormalised member exp lies 
exponential family convexity implies log concave strictly minimal representation 
words exponential family densities log concave 
positive function induce tilted exponential family modifying base measure recomputing log partition function 
definition tilted exponential family exponential family natural parameter positive function log exists tilted exponential family ff induced contains densities pf exp modified base measure 
ff natural parameter space 
exponential families 
gaussians ff proper exponential family moment parameter pf computed derivatives epf log 
note ff set distributions iff log vector 
update distribution multiplying positive factor iff update factor structure ratio members definition unnormalised exponential family exponential family natural parameter set functions exp referred unnormalised exponential family associated note members general probability densities may normalisable 
proportional member iff 
note lemma relative entropy relative entropy log log see definition denoted 
see convex convex 
projections relative entropy definition natural replacement euclidean distance square dealing manifolds probability distributions allowing generalisations important geometrical concepts orthogonal projection triangle inequalities pythagorean theorems 
equivalent orthogonal projection introduced csisz projections 
appendix general appendix definition projections closed convex set see section distributions distribution 
projection projection examples projections 
argmin argmin 
note projections defined convex relative entropy convex arguments 
project iff suppose exponential family definition 
eq moment parameter eq 
important special case projection equivalent moment matching example section 
gaussian variables definition gaussian distribution variable non degenerate gaussian distribution mean covariance matrix iff exp stringent definition gaussian distribution vector variable non degenerate gaussian distribution surely constant 
allows degenerate distributions supports confined affine subspaces gaussian family important family continuous distributions theory practice number reasons 
superb closeness properties linear transformations conditioning marginalisation 
gaussian arises ideal noise error distribution central limit theorem 
distributions mean covariance matrix structure maximises differential entropy 
cumulants order higher vanish cumulant generating function quadratic 
gaussian variables correlated independent 
thesis gaussians play large role 
gaussian processes gps see section induce gaussian distributions evaluated finite point sets 
remainder section introduce notation 
exponential families 
gaussians easier manipulate gaussian expressions 
section collect number gaussian formulae thesis 
gaussian family exponential family definition definition unnormalised gaussian exp xt symmetric need positive definite 
furthermore 
centred version uc exp note may exist uc proportional 
proportional gaussian density iff positive definite 
natural parameters sufficient statistics xx parameterisation minimal due symmetry constraint easier minimal 
corresponding moment parameters mean covariance matrix gaussian 
conversion natural moment parameters requires inversion positive definite matrix 
log partition function log set associated unnormalised exponential family 
conversion formulae lemma conversions conversions unnormalised gaussians uc uc uc pos 
def 
uc uc uc exp strictly speaking second moment parameter xx 
appendix general appendix specialisation definition positive function 
normalisable positive definite form tilted exponential family compute moments derivatives log log log var log see section 
manipulating gaussians sense forming linear combinations marginalisation conditioning boils simple linear algebra summarised lemmas 
lemma conditional gaussian distributed 


xr xi gaussian covariance matrix mean xi xi 
shown function xr lemma 
note covariance matrix xr xi depend xi 
lemma sums affine transforms xi independent gaussian variables aixi ai ai ia marginalisation special case 
contain duplicates xi 
unnormalised gaussians note ax products unnormalised gaussians unnormalised gaussians natural parameters combine additively see lemma 

exponential families 
gaussians lemma products unnormalised gaussians form uc ri ci 
convert ri ci define ri assume invertible 
ri ci ci uc exp rt ci 
positive definite factor right simply additional conditions obtain recursive formula state larger lemma combine factors 
lemma products recursive formulation conditions lemma assume invertible 
uc uc uc uc 
furthermore positive definite familiar equation 
lemma differential relative entropy differential entropy gaussian log relative entropy see definition gaussians log single need invertible 
tr 
appendix general appendix pattern recognition section briefly introduce general problem thesis mainly concerned 
classification pattern recognition problem data xi yi 
xi yi sampled independently identically distributed unknown data distribution finite set 
assume 

situation called binary classification problem case referred multi class problem 
sample called training set 
goal compute classification function classifier small generalisation error gen pr sampled data distribution independently computed definite knowledge data distribution 
process called training 
ambitious goal estimate conditional data distribution related moments thereof directly leading called predictors 
context binary classification classifiers defined terms discriminants real valued functions obtain classifiers thresholding sgn threshold 
notion generalised multi class problem discriminant fy class setting argmax fy 
functions training sample referred empirical statistics 
important statistic empirical training error emp yi 
note generalisation empirical error defined expectations called zero loss 
scenario generalised non negative loss functions predictors case expected loss data distribution called true risk expected loss yi called empirical risk 
framework empirical distribution encompasses statistical problems regression estimation task estimate function fixed classifier empirical error converge generalisation error surely strong law large numbers 
chosen depending minimise empirical error broad set candidates typically best candidate gen fact acceptable performance 
problem known overfitting effect attributed fact trying fit training data overly leads choice dependent noise way 
bayesian inference approximations penalise supposedly candidates choice called regularisation 
principled costly solution bayesian analysis see section classifier constructed expectation set candidates models 
candidates usually low weights expectation 
key points thesis notion complexity defined universal practically feasible way 
depend context statistical problem available prior knowledge characteristics constraints 
bayesian inference approximations section introduce aspects bayesian analysis relevant thesis 
detailed accounts see 
briefly sketch basics general approximation techniques bayesian inference relevant machine learning 
excellent date 
probabilistic modelling probabilistic model simplified description partially observable data source 
consider binary classification problem defined section 
observed variables domain input points targets labels 
simple model obtained introducing latent variable assumption motivated variety reasons 
prior assumptions include notion smoothness targets close input points priori high probability 
express need include probability variables observed 
second easier model independently directly 
model description completed choosing prior distributions 
specification called systematic component model specification random component 
specified parametric non parametric way postulate parametric family prior dimensionality independent dataset sizes place prior distribution directly probabilistic relationship penalises low probability assignment violating prior assumptions behaving non smooth universal definition kolmogorov complexity chap 
practically feasible 
appendix general appendix ous non parametric modelling relationships parametric models discussed detail section 
semi parametric models combine ways example modelling sum parametric non parametric residual part 
semi parametric models useful test parametric modelling assumption observing relevance non parametric residual part prediction noise distributions standard models section 
depend parameters called hyperparameters prior distributions called 
note parametric model primary parameter vector treated hyperparameter 
note said distribution input points examples covariates variables considered time prediction target variables required 
sum domain partitions observed latent variables observed target variables covariates latent target variables called nuisance variables 
interested predicting target variables covariates training sample example target variable nuisance variable 
main assumption model different cases xi yi sampled common latent parameter latent process sampled prior data distribution satisfies exchangeability de finetti theorem see 
bayesian analysis bayesian inference prediction central part bayesian analysis follows 
complete model variables covariates condition observed data marginalise nuisance latent variables obtain posterior distribution dh 
nuisance variables called complete data likelihood called observed data likelihood 
example exchangeability assumption yi xi yi xi yi ui ui xi dui 
care required general deterministic function random process characteristics smoothness continuity defined differently see section 

bayesian inference approximations reason predictive distribution test case 
bayesian inference amounts updating prior belief posterior belief 
predictions obtained posterior averages 
computational requirements bayesian inference prohibitive techniques approximate bayesian computations posterior expectations essential bayesian toolbox 
discuss section 
frequently general empirical bayesian method nuisance hyperparameters marginal likelihood maximisation maximum likelihood ii 
split primary parameters hyperparameters 
typically smaller dimension posterior 
sufficiently large usually highly peaked mode due central limit behaviour replace 
example maximum posteriori map approximation 
posterior expectations approximated ep ep log log log dw called marginal data likelihood 
log typically simple function main task optimise log 
situations log marginal likelihood computed tractably due integral approximations considered 
finding plugging special case bayesian model selection 
approximations bayesian inference positive function simplicity assume dom rg general approximation technique render furthermore approximation ep eq functions expectation computed feasibly 
focus methods large fairly peaked heavy tails 
oldest approximation technique bayesian inference introduced laplace approximate posterior binomial model called laplace saddle point approximation 
suppose log unique minimum appendix general appendix point twice continuously differentiable open set containing 
hessian positive definite approximate exp log normalising approximation log log 
note approximation entirely local depending value curvature mode 
special situations laplace approximation applied multimodal modes clearly separated local shape 
hand perform poor unimodal distributions due local nature applicable twice continuously differentiable mode 
variational approximations reformulating approximation problem optimisation problem typically maximisation lower bound 
approach bound expectations done feasibly 

optimisation problem choose order maximise lower bound resulting 
different global variational approach derived convex duality negentropy log partition function substitute log leading log log log eq log distribution 
difference log lower bound bregman divergence note log legendre pair 
choose tractable family order maximise lower bound 
contrast variational technique described minimises time relative entropy bringing closer target distribution note opposed laplace approximation variational techniques render guaranteed lower bound reassuring goal maximise parameters long corresponding upper bound available method checking tightness lower bound considerably easier original approximation task general method upper bound log partition function sparsely connected graphical models method layer networks binary variables 
goal approximate insisting lower bound fairly restrictive 
variant second variational method called variational bayes received lot attention 
variational bayes essentially special case variational mean field approximation family draw 
bayesian inference approximations defined factorisation constraints 
allows simple analytic update factors fixed process iterated 
note historically methods called mean field approximations completely factorised variational distributions obvious extension partial called generalised mean field structural mean field follow nomenclature 
general framework variational bayes general user friendly software available 
variational mean field approximations lead poor results range improvements suggested 
general ansatz look approximations called negative free energy log exp 
section know convex dual negentropy log exp max eq optimal legendre pair exp desired target distribution 
mean field approximation maximises lower bound factorised distributions goal approximate marginals target distribution may corresponding marginals different approach approximate 
particular intractable entropy term function certain marginals disregarding fact sets marginals deal optimisation consistent single joint choose overlapping marginals approximation significant improvement mean field general lower bound anymore 
algorithms maximise constraint marginals locally consistent marginalise correctly overlaps 
structured graphs known loopy belief propagation generalised belief propagation 
interested section variant belief propagation called expectation propagation applied dense sparse graphs 
developed class approximate inference techniques bayesian statistics markov chain monte carlo mcmc methods 
idea able obtain independent samples target distribution approximate expectation corresponding empirical average samples 
law large numbers convergence average roughly number samples constant depends smoothness properties function average largely independent dimensionality 
usually possible directly obtain independent samples complicated posteriors high dimensional possible define ergodic markov chain posterior equilibrium distribution run chain long discard initial part trajectory burn sample path average converge appendix general appendix just principle independent dimensionality 
enormous number sampling techniques choose excellent review consult see 
powerful software packages mcmc available 
lower bound maximisation 
expectation maximisation important concept classical statistics maximum likelihood ml estimation 
suppose data 
xn model exponential family definition 
log likelihood function log log xi xi constant 
note depends empirical statistic name sufficient statistic 
unique maximum point moment parameter maximum likelihood estimate 
important note projection moment matching see section empirical distribution family 
family exponential example dh nuisance variables 
instance general problem maximising function log exp dh 
general hard task may content finding local maximum 
know fixed convex function find global lower bound linear tight eq exp legendre pair 
maximise right hand side sides equal improve 
furthermore gradients lower bound identical eq 
guaranteed convergence local maximum procedure relies eq simple optimise exp dh 
refer method lower bound maximisation 
important special case follows 
log exponential family distribution additional observed variables values plugged natural parameters 
gradient computed straightforwardly 
bayesian inference approximations eq corresponding conditional distribution different exponential family 
lower bound maximised eq form maximum likelihood estimate latent marginalised expectation maximisation em algorithm maximum likelihood estimation presence nuisance variables special case algorithm 
hn nuisance variables gi hi complete data log likelihood decomposes 
note posterior product distribution 
hn factors simple compute 
models complete data likelihood exponential family maximisation lower bound simply corresponds projection definition way ml estimation nuisance variables distribution projected simply product empirical distribution observed variables 
historically computation called step lower bound maximisation called step 
important interpretation em algorithm comes information theory 
suppose complete data likelihood comes exponential family observe instance sample variable step corresponds projection define manifold distributions constraint easy see step corresponds projection definition projection results 
em special case alternating minimisation procedure class introduced csisz show common roots number algorithms statistics information theory 
showed distance convex closed convex sets unique global minimum minp corresponding minimum point alternating minimisation procedure 
wrong em get trapped local minima 
problem exponential families commonly em convex set distributions 
projections steps defined local minima exist real problem 
simple cases em convex model families blahut algorithm cases lower bound computed optimised feasibly 
case course free choose different lower bound suboptimal distribution optimal globally maximises lower bound search thing restricted family bound evaluated feasibly 
seen variational generalisation em see section computes maximum likelihood mixing proportions mixture model fixed component distributions see sect 

appendix general appendix pointed hinton neal 
note generalised algorithm guaranteed converge local minimum offers general way doing marginal likelihood maximisation see section 
numerous generalisations em deterministic step annealing step annealing sequential em direct approach maximising gradient optimisation 
shown gradient computed easily maximising lower bound 
furthermore gradient quasi newton conjugate gradients restricted memory quasi newton exhibit faster convergence lower bound maximisation techniques typically order convergence 
convergence properties em studied compared gradient optimisation :10.1.1.18.5213
consensus fairly close minimum point modern gradient methods clearly exhibit faster convergence high accuracy 
hand em attain point reasonably low significantly faster gradient due ability large steps 
advantage em implemented easily packages gradient optimisation publicly available 
simple hybrid employed thesis uses modern gradient method keeps fixed line searches descent directions 
changes lead discontinuous behaviour lower bound variant promises higher stability 
em important place modern optimisation techniques steps done efficiently lower bound maximisation algorithms clearly inferior gradient optimisation areas application 
example generalised iterative scaling algorithm applied log linear models sequence tagging 
sequence lower bounds applied achieve decoupling model parameters 
quoted advantage authors gis questionable thing opinion 
optimisation literature decoupling techniques avoided lead phenomenon called zig function correlates variables significantly optimisation function turn updating variable keeping fixed leads slow progress total characteristic zig zag trajectory 
iterative scaling useful maximum likelihood fitting undirected graphical models clique sizes small 
improved variant parameter updates inference interleaved 

large deviation inequalities large deviation inequalities section describe chernoff bounding technique deriving exponential inequalities basic markov inequality 
follow exposition 
theorem markov inequality surely non negative random variable finite mean 
pr proof assume note pr 
inequality variable zero mean finite variance var pr follows markov setting order bound probabilities events exponentially require inequalities exponential nature 
standard technique obtain tighter large deviation results apply markov inequality exponentiated large deviation 
technique accredited chernoff cram pr pr 
bound holds pr replaced 
remainder section focus upper tail 
theorem chernoff inequality random variable finite mean 
pr sup log log cumulant generating function definition cumulants exist obtained derivatives evaluated 
note implies convex dual convex see section 
distribution define tilted distribution dp exp dp 
easy see legendre pair see section 
cumulant generating function cumulants derivatives evaluated 
known computed noting supremum attained xi appendix general appendix xi distributed log 
case look 
cram shown bound theorem asymptotically tight case fixed lim log pr 
example xi bernoulli variables log see equation 
theorem chernoff inequality binomial variables 
xn bernoulli variables xi 
pr bound holds furthermore pr holds 
bounds hold 
xn bounded variables xi xi tightest sums bernoulli variables 
xi xi convex cumulant generating function bernoulli variable 
sum bernoulli variables 
appearance inconvenient quadratic lower bound order derive hoeffding inequality pr quadratic bound fairly poor far usually interested hoeffding inequality just convenience recommended 
appendix appendix chapter chapter provide derivations additional details order complete presentation chapter 
extended pac bayesian theorem example theorem provides powerful extension pac bayesian theorem multiple classes confusion distribution 
section example order see practice 
binary classification case interested upper bounding probability gibbs rule outputs true class probability false positive 
probability written joint confusion distribution 
chernoff inequality theorem obtain lower bound 
yi prs chosen log probability 
note left limit interval ber log defined 
show moment theorem order obtain upper bound holds confidence 
applying union bound conclude bounds hold time confidence resulting upper bound ratio 
confidence parameters chosen priori 
bound tighter may want choose 
apply theorem setup introduced 
note 

entry representing values 
writing appendix appendix chapter upper bound maximum value lies convex bounded feasible set defined 
optimisation problem linear criterion convex constraints solved easily assumption components positive 
shown subsection solution form pl pl 
lagrange multiplier 
denote solution fixed multiplier function strictly monotonically decreasing simple dimensional search find unique multiplier value 
solution value component desired upper bound 
remains show solution convex problem stated 
straightforward exercise lagrange multipliers 
primal problem minimise subject feasible set 
assume components positive 
introduce lagrange multipliers constraints 
lagrangian 
setting gradient equal zero obtain pl 
pl pl pl positive violate constraint 
pl pl 
summing constraints see pl pl quadratic unique nonnegative solution 
easy see dp 
pl 
details proof theorem strictly decreasing 
simple dimensional search reveals unique solution looking 
details proof theorem give proof inequality see sect 

recall notation defined proof theorem section 
probability sequence 
ln depends type type sufficient statistic sequence pli en recall entropy equation 
second number sequences type upper bounded see consider sequences sampled 
distribution probability arbitrary sequence type nh 
sum probabilities obtain bound number sequences 
note bound tight lower bound number see theorem 
third number different types upper bounded es nd nd nh lower bound number sequences type easy show es exp grows exponentially sense exponent optimal 
proof theorem give proof pac bayesian theorem arbitrary bounded loss functions theorem 
done starting inequality bounding probability large deviations expected empirical loss fixed rules order obtain exponential statement plugged generic part proof section 
implication proved beautiful water filling argument due 
recall notation defined sections 
loss function risk defined empirical risk xi yi 
nonnegative function appendix appendix chapter assume inequalities hold nondecreasing convex differentiable 
fix 
show es take expectation exchange order expectations apply markov inequality statement prs 
note probability trivially true 
order prove look distribution maximises left hand side subject constraints imposed nondecreasing obvious fulfil constraints equality measure density fulfilling constraints equality obtained differentiating constraints obtain sgn straightforward check nonnegative integrates 
may value consider measure sum measure density point measure mass expectation measure upper bound expectation probability distribution fulfilling constraints sgn sgn sgn proving statement 
applying generic part proof section convexity completes proof theorem 
water filling argument 
density function fulfilling constraints equality may integrate 
case maximising probability density obtained shrinking initial part zero 
inequality stick 

efficient evaluation laplace gp gibbs classifier efficient evaluation laplace gp gibbs classifier recall section order evaluate laplace gp gibbs classifier test point require predictive variance computed back substitution 
true non sparse members family approximation methods mentioned section 
note required order compute uncertainty estimate bayes gibbs classifier required rejection sampling technique developed efficient predictions 
describe technique case laplace gpc development non sparse method clear notation introduced sections 
fix 
possible obtain upper lower bounds critical term having compute factorisation lanczos methods 
bounds due skilling context gp methods 
idea maximise quadratic function ut bu approximately 
sparse greedy maximisation allowing controlled number components nonzero compute upper lower bounds see having know rows alternatively faster incomplete cholesky techniques create envelope 
suppose functions simply setting exp exp 
note compute ratio exp knowledge facts sample rejection method follows independently sample uniformly 
return 
ratio 
appendix appendix chapter compute variance exactly 
return 
sample return 
note method produces sample having compute variance algorithm returns second step 
denote event denote du du 
du 
bounds tight probability close 
example 
proof theorem theorem pac bayesian theorem bayes voting classifiers 
section replicate proof notation 
section contains concepts 
proof uses bound 
need machinery empirical process theory 
set classifiers 
xi yi sample define empirical rademacher complexity rn sup xi vector rademacher variables unbiased coins sides independent note rn random variable independent targets yi 
shown concentrated rn es rn rademacher complexity drawn usual fact rn bounded differences size hoeffding inequality applies see section 
margin loss function defined section choose 
course measurability issues refer original 

proof theorem theorem theorem data distribution prs pr inf yif xi log rn 
log log log log proof theorem hard 
bounded loss allows apply hoeffding inequality bounded martingale difference sequences see section 
ledoux contraction principle get rid margin loss function exploiting fact contraction 
details 
recall definitions qa fa section 
convex duality order bound empirical rademacher complexity rn fa getting rid supremum fa 
note class fa closed multiplication remove absolute value operator 
recall measurable log see equation 
write short xi 
choosing iy rn fa log exp iy 
jensen inequality pull expectation inside log evaluate moment generating function log exp iy log cosh note log cosh concave function sect 
upper bounded 
log cosh log cosh log exp contraction appendix appendix chapter minimisation leads rn fa bound holds rn fa 
fixed theorem holds fa rn replaced replace aj pj 
conclude proof theorem applying union bound 
case regression section mentioned possible route pac bayesian theorem gp regression 
give details argumentation 
starting point theorem second part prove bound pn terms rademacher complexity rn fa 
case interested consists functions fa 
recall eq qa lipschitz loss function 
obviously part proof remains valid pn rn fa rademacher complexity rn fa es rn fa rn fa sup yi xi fa denote ui xi xi eq ui 
convex duality yi ui resulting rn fa max log ep exp note 
setting log ep exp max log ep 

proof pac compression bound max log ep exp minimisation results rn fa tr 
tr making jensen inequality concave square root function yt es follows easily 
proof pac compression bound section empirically compare pac bayesian theorem binary gp classification standard pac compression bound 
section state bound theorem give proof 
theorem slight refinement theorem turn results 
interesting note spite extremely simple derivation compression bound tightest known bounds popular support vector machine practically relevant regimes way ahead results employing deeper mathematics 
define mean compression scheme 
suppose hypothesis space binary classifiers learning algorithm mapping training samples unknown data distribution hypotheses usual define generalisation empirical error hypothesis gen pr emp yi xi yi drawn data distribution independently compression scheme size learning algorithm find mapping samples ordered subsets 
algorithm mapping samples size hypotheses sample si si xi yi 
means extract sample subsample si algorithm essentially trained si order produce result full sample appendix appendix chapter chosen si ignored 
data compressed presenting learner intuitive characteristic exploited prove generalisation error bounds schemes 
characteristic somewhat exploited permutation invariance scheme data sample 
compression scheme associated mappings call permutation invariant sample feeding sample obtained si permuting points leads result independent permutation 
special cases allowed permute points allowed permute points 
herbrich sect 
provides motivation gives examples compression schemes 
fix choose 

priori relaxed shown 
theorem pac compression bound suppose algorithm samples size permutation invariant compression scheme associated mappings 
data distribution bound holds probability samples xi yi 
size drawn data distribution gen emp prs emp max ber log recall definition ber 
log equation 
note error remaining patterns emp denoted emp section section 
proof fix 

goal upper bound prs emp gen 
recall si 
apply simple union bound argument summing possible values order obtain upper bound prs emp si gen si 
proof pac compression bound determine range number admissible 
fix upper bound addend corresponding esi prs si denote gen si emp si 
si depends just si conditioned si binomial distributed 
chernoff bound theorem obtain prs si 
depend si upper bound 
independent bound simply counting number admissible leading potentially different si 
permutation invariant permute points si changing si 

distinguishable values altogether prs emp gen 
equating solving leads unique solution 
fixed prs emp gen 
noting different values see gen emp prs emp emp prs gen 
concludes proof theorem 
note proved theorem assumption fixed priori 
relaxed allowing dependence sample appendix appendix chapter union bound argument 
theorem exactly form theorem replaced 
assume function reasonable relaxed desired 
note main contribution gap bound value comes binomial coefficient small introduced crude union bound argument summing 
necessary assumptions mapping 
particular algorithm may possible come informed weighting uniform possible sets tighten bound significantly 
note special case attain emp solve analytically log see equation 
note compression schemes index mapping partly randomised function sample random coin tosses 
independent poses problem theorem holds possible outcomes coin tosses course bound value depends outcomes 
examples compression schemes herbrich sect 
gives range examples compression schemes 
shown perceptron learning algorithm rosenblatt permutation invariant compression scheme number patterns algorithm selects update weight vector 
note due perceptron convergence theorem possible upper bound terms margin data 
furthermore popular support vector machine seen permutation invariant compression scheme number support vectors follows discussion section 
application compression bound section justified 
final svm discriminant mistakes support vectors emp 
note fixed priori theorem 
furthermore ivm see section compression scheme 
seen noting patterns selected inclusion active set update model parameters 
experiments see section fixed priori theorem applies original form set 
note variants scheme chosen depending sample cases modified 
appendix appendix chapter chapter provide derivations additional details order complete presentation chapter 
expectation propagation ep algorithm realises general purpose framework approximating posterior beliefs exponential family distributions 
generic section serving develop gaussian case section 
details doing adf ep updates case section 
expectation propagation exponential families section introduce general expectation propagation ep algorithm specialisation gp models require chapter see section 
suppose statistical model observables latent variables prior distribution exponential family properties exponential families require section 
likelihood function factors particular way ti example case data bayesian networks 
refer ti sites 
true posterior ti appendix appendix chapter analytically intractable may approximate distribution exp 
tractable way choosing start incorporate sites ti sequential ordering 
order incorporate ti compute true bayesian update ti zi ti project see section obtain updated belief qnew new argmin recall projection equivalent match moments new qnew qnew new new dual moment parameters new see section new 
refer process inclusion site ti belief inclusion different true bayesian update full updated belief collapsed qnew member allows inclusions chained 
lives tilted exponential family induced ti definition moments computed feasible amenable numerical approximations moments full posterior remain intractable 
simple idea extensively example context bayesian line learning switching linear dynamical systems see exhaustive 
known assumed density filtering adf 
site may included context dynamical systems restricted updates direction backbone chain filtering bidirectional smoothing improve approximation 
new view adf established allows shortcomings removed 
process including ti results replaced qnew seen multiplying ratio ti qnew 
operation particularly simple natural parameters new new 
ratio ti member unnormalised exponential family associated definition form similar qnew new general lie natural parameter space view motivates representing ti example family gaussians ti may correspond gaussian negative variance 

expectation propagation ti ti referred site approximations natural parameters site parameters 
denotes parameters note allow ti fact site approximations constant leading 
adf update inclusion site ti seen follows note ti definition adf update inclusion 
compute moments ti pick new moments 

order replace qnew replace ti new qnew 
viewpoint clear adf generalised iterative approximation scheme allowing multiple iterations sites 
ep update inclusion deletion site ti works follows definition ep update inclusion deletion 
delete site approximation ti ti obtaining tj 
natural parameters 
ti compute new log zi zi ti pick new moments 

replace ti new new 
natural parameters new appendix appendix chapter line refer cavity distribution 
networks discrete nodes gaussian markov random fields ep seen generalisation loopy belief propagation allowing flexible choice approximating structure distribution family see chap 

algorithm converge fixed point saddle points approximation free energy generalisation bethe free energy 
double loop concave convex algorithms applied order ensure convergence 
problems convergence overcome damped updates new update convex combination new updates lead new outside close boundary rejected 
practice important address issue numerical stability conversion natural moment parameters typically stable operation 
possible implementation remain moment parameters entirely fold conversions update operations single mapping stabilised 
course generic presentation clear damped updates convex combinations natural parameters way 
important note ep simply local approximation sites ti corresponding ti global fit distribution obtained replacing ti ti current belief 
fact sites may continuous functions ep applied approximate inference different regimes sparsely connected bayesian markov networks models fully connected gaussian prior 
regime single ti depends small number components markov network case small cliques underlying graph 
choosing special structure approximating distribution tractable subgraph decomposable extension model graph requiring prior follows structure run ep message passing scheme updating parameters certain small extensions thereof 
notion developed generalised chap 
discrete variables address issue represent 
second regime interested family gaussians typically densely connected likelihood factors ti local 
special case completely factorised likelihood 
manipulations gaussians general cost converting natural moment parameters 
locality property ep order simplify ep updates representation belief ut suppose site ti depends uj example gaussian family invert matrix 

expectation propagation ti ti uj 
uj uj new uj new uj uj new uj order minimise expression set new uj uj match moments marginals uj new uj 
follows ti ti uj site approximations inherit locality corresponding sites parameterised economically sense components clamped zero 
furthermore ti uj see uj uj ti uj order update site approximation ti need access marginal uj ep global approximation updates local 
note change ti uj general affects marginals due densely connected prior 
locality property furthermore lemma 
lemma gaussians uj uj ut proof note uj uj 
uj uj 
uj uj uj entropy gaussian distribution depends covariance matrix lemma covariance matrix uj depend uj special property gaussian distributions lemma 
take expectation just uj concludes proof part lemma 
second part straightforward 
marginal likelihood approximation marginal likelihood ti appendix appendix chapter approximated ep allows optimize free hyperparameters 
long ep approximate posterior matter site approximations ti normalized normalization explicit site approximations ci ti ti exp idea match normalization constants way moments making cavity distributions 
zi ti zi ti exp require cavity expectations ti ci ti means zi ci zi log ci log zi 
approximation log obtained replacing sites ti approximations ci ti log exp log ci log ci 
order maximize require gradient hyperparameters 
computation simplified greatly making fixed point conditions hold convergence ep 
update site pi eq pi ti ti 
remarks tilted exponential families section log zi log zi pi eq furthermore log zi log ci 
log exp 
expectation propagation jacobian computed general drops eq 
parameter site tj dependence direct assume 
show dependence ignored 
ignore direct dependence log zi log zi log ci log exp 
direct dependence counts log cj log zj log tj pj tj 
adf update noise models show adf updates definition ep updates definition noise models thesis case gaussian beliefs 
general ti ti uj 

order update site approximation ti uj uj bi require marginal uj hj aj 
compute uj converting natural parameters subtracting bi converting back 
results aj hj bi adf update aj hj site parameters bi zero 
need moments tilted gaussian uj ti uj uj computed 
zi eq ti uj 
gaussian context direct compute derivatives log zi mean vector natural parameter readily converted 
log zi log zi 
appendix appendix chapter new log zi new 
compute new site parameters convert natural parameters subtract natural parameters new new new note likelihood model completely matrices vectors simplify scalars 
case binary classification probit noise model yi ui yi ui zi yi ui yi ui dui see equation 
yi zi yin zi zi practice direct evaluation zi zi instable zi implementation evaluates linear lower bound 
general sites ti uj gaussian integrals log zi derivatives analytically tractable 
method choice approximating gaussian integrals gaussian quadrature gauss hermite type sect 

method delivers approximations integrals dx requiring evaluation chosen points 
nutshell gaussian quadrature constructs unique set polynomials orthonormal inner product dx uses zeros degree member adjusts weights rule exact polynomial order 
turns corresponding rule exact polynomials order owing free choice 
general accuracy hinges smooth approximated polynomial region significantly different 
hold true 
likelihood approximations 
selection criteria case suitable linear transformation ti ui may necessary split integral parts deviate standard quadrature rules 
gaussian quadrature attractive number grid points required grows exponentially 
relevant literature problem reviewed 
general large may resort monte carlo mc techniques accuracy small poor compared quadrature techniques 
method exact monomials suitable context exploiting symmetry orthogonal transformation weight function 
likelihood approximations 
selection criteria section provide additional details concerning likelihood approximations greedy selection criteria sparse gp approximations 
optimal sparse likelihood approximations recall notations section 
section give proof lemma 
part note ui determined constant factor 
remove indeterminacy requiring ui ui 
problem minimising subject constraint lagrangian ep log ui ep ui leading ui ui ui 
ui du ui ep ui ui ep ui 
second part note relative entropy term rewritten constant eq ui log exp ep log ui ui fact prior terms cancel 
minimised making sure numerator denominator proportional 
note essentially argument derive freeform optimal smoothing distributions variational approximations see section 
suppose denote recall relative entropy definition non negative iff arguments identical distributions 
appendix appendix chapter ep ui conditional mean ui prior ep log ui simply log quadratic conditional joint gaussian distribution conditional covariance matrix depend variable condition lemma 
optimal likelihood approximation sense statement gaussian case ui ep ui 
important fact context chapter conditional mean computed inverting marginal covariance ui 
unfortunately case statement form optimal likelihood approximation requires computation block inverse covariance matrix feasible sparse approximations see lemma 
relaxed likelihood approximations recall discussion section 
clear general computation avoided symmetric nonsingular allowed depend restriction dropped 
prediction vector kif ki computed resource constraints allow evaluation ki 
note parameterisation translates directly form likelihood approximation 
brought centred form uc iff exists case 
existence strict requirement likelihood approximation 
example consider mred view large margin classifiers section log likelihood linear additional constrained margin variables adding nonlinear dependencies 
likelihood approximation depends log linear way ui ri exp iu additional obtain accurate approximations true posterior remains open 

derivations informative vector machine cheap versus expensive selection criteria section noted task subset selection cheap criteria generally preferred expensive ones 
context expensive means score evaluation costly actual inclusion computation qnew time issue sensible expensive criterion preferable cheap 
give simple example need case 
resource constraints criteria chosen characteristics requirements task 
symmetric positive definite linear system ax find sparse approximate solution size available memory computed demand 
minimising directly feasible incrementally try minimise cg xt ax ls ax greedy forward selection iteration scored maximum reduction criterion achievable component allowed non zero 
score cg requires evaluation column ls cg cheap ls expensive 
bu cg equal constants ls corresponds 
goal approximation cg preferred criterion case cg penalises errors high eigenvalues ls stronger 
particular case task requirements lead prefer cheaper criterion afford expensive 
derivations informative vector machine section collect details concerning ivm approximation introduced section 
update representation recall notation definitions section recall order decide point include active set score remaining points significant fraction thereof 
order compute score point need know corresponding marginal uj corresponding elements diag describe efficient stable scheme maintaining information 
assume marginals kept date inclusions 
relax requirement randomised greedy scheme 
recall woodbury formula lemma appendix appendix chapter write ki ki 
maintain cholesky decomposition ll see section ki matrix maintain memory 
note 
short knowledge marginal ui hinges column suppose new point included site parameters bi computed 
update li ki furthermore li iki li xi li furthermore diag new diag update note new new new new component new compute directly observing new ai new aia bnew factor determined solving hnew hi ai resulting see section recall doing adf updates 
easy see ai li new ili new ili storage requirements dominated running time requirements computation new row evaluation column kernel matrix 
randomised greedy selection recall section randomised greedy selection rgs keep subset marginals date corresponding selection index speed comes fact columns kept date time delayed updating scheme 
maintain integer vector nn gi iteration number inclusion column date 
denote rd rd matrices inclusion points 
know gi gi gi current buffer matrix 
call vector stub suppose included points include point selection index modified order update components diag corresponding go sure stubs corresponding new full size 
central note back substitution required naturally broken steps 
step uses th row produces element stub 
save 
derivations informative vector machine gj steps procedure directly start stub steps gj 
described set gj 
note order randomised scheme efficient significantly smaller 
exhibit inertia time 
large simple implementation just memory 
long possible exhausting system ram efficient option terms running time 
cache structure 
implementation uses simple structure consisting buffers 
smaller keeps size larger columns size chosen large possible resource constraints 
replaced exchange stubs corresponding buffers 
may lead stub trimmed size 
case chop gj elements re computed faster return time 
scheme fairly simple implement may need modified large 
case process selecting new patterns inclusion uniform prefer patterns 
retain fraction grows considered 
implement cache hierarchy leads trimming recomputation 
general problem practice rgs dominating computational steps scale full selection done 
full greedy selection large back substitutions done en block having inner loops scale making optimised software blas randomised selection possible 
ignore issues painfully important dealing large scope thesis 
exchange moves section derive update equations exchange moves time remove active set include leaving total size constant 
exchange candidates scored way inclusion candidates 
move score computation require knowledge marginal ui uj 
locality property ep lemma qnew ui ui new ui qnew ui ui 
ui ui uj uj new ui uj ui uj denotes components ui uj 
ui 
implementation include packets 
appendix appendix chapter ai ja jaj hi ai hj mj jaj clear adf update remains simple inclusion replacing hi ai note variance new qnew ui new 
representation discussed section updated follows 
sup pose position replace furthermore ki ii ki due change replaced note new kq noting arrive easy see set ki new lv ii ki iki jkj ki 
arrive new technique described section order update denote diag changes imi jmj changes note new ai ij ij ij new ij ij ij ij ij definition see arrive new ij ij 
updated steps 
compute ki kl 

derivations informative vector machine second note new cholesky factor written ll factors special structure allowing back substitutions 
new matrix computed 
note order exchange move require element ai computed ki known 
memory available ki stored alongside algorithm stage new inclusions required sensible convert explicit representation updated woodbury formula kernel evaluations inexpensive required elements re evaluated needed 
differential entropy score computed lemma twice new ui ui uj uj ent qnew ui ui uj uj log new aj log log aj 
information gain score apply lemma obtain new new ui uj ui uj 
ui uj ij ij qnew ui uj new ij new 
arrive info log tr ij model selection criterion gradient ij section suggested model selection criterion ivm works efficiently randomised greedy selection rgs 

buffer size stored 
order compute criterion gradient evaluate scratch include patterns largest stubs rgs cache evaluation significantly cheaper 
strategy choosing discussed 
criterion defined way mf plv see section true likelihood replaced ul 
leads eq log ul ui ui may numerically stable cholesky updates 
appendix appendix chapter ui ui log tr diag diag eq log yl ul eq log yi ui ui hi qi 
define ci eq log yi ui 
bi define algebra gives ki db dki log tr tr dki variation dk 
dki dki dki furthermore hl kl diag kl dhl dki dki dq diag diag dki diag dki 
deq log yl ul eq log yi ui log ui ai eq log yi ui ui hi qi bi eq log yi ui ui hi qi furthermore di ci qi bi deq log ul dq 
computation done numerical quadrature 
need inner product relation diag db tr diag 
derivations projected latent variables matrices vector diag dl diag dl deq log yl ul ld diag tr dki tr dki eal dki noise model depends hyperparameters variation results deq log yl ul eq log yi ui 
hyperparameter say location parameter sense log yi ui dui log yi ui partial integration see log yi ui ui hi eq eq log yi ui ai 
eq log ul 
dominating precomputations require matrix overwrites 
computed efficiently completing stubs stored rgs cache see section 
form patterns largest stubs cache contains discussed section objectives keeping pattern stub cache having virtually identical 
precomputations computational cost gradient component 
cost precomputation violates resource constraints dependence may ignored obtain approximate gradient clear criterion approximate gradient fed custom optimiser typically requires finite differences converge gradients 
implementation requires matrix buffer allowing derivative matrices ki computed fly 
manages complete stubs available rgs having allocate second large buffer moment 
derivations projected latent variables section collect derivations required develop projected latent variables plv scheme introduced section 
qi appendix appendix chapter site approximation updates 
point inclusions recall ui ui 
eq ui qi 

mark variables update inclusion prime 
due likelihood approximation update site parameters point slightly involved ivm case 
bi ti ui site parameters adjusted mean variance 
ti ti ui ui ki pi ti ui ui ui ui ui 
tilted gaussian derivation section sub ui ti ui ti 
require bi application lemma 
note 
see iv positive definite 
qi vt vi ivi ivi 
note update requires applying dimensional gaussian expectations twice diagonal case may raise problems expectations done numerical quadrature 
probit classification noise model ti ui yi ui yi ui arrive formulae section yi replaced yi mean variance 
easier update site parameters ui normal ep update described section just replace ai qi hi 
ui qi require qi 
recall note qi ci ci need compute ci 
update related representation variables updated 
suppose bi viv vi back substitutions see section 
computed ci available update 
obtained 
update bci 

derivations projected latent variables reasons numerical stability refreshed procedure 
fixed number site parameter updates old old entries non zero computed 
recompute 
implementation see include new point update ki vi li ki vi ki pi furthermore vi zt update adding component vt 
running time dominated computations vi furthermore compute kernel matrix column site parameters updated immediately inclusion procedure described 
update unstable small recommend simply skip updates 
special case site parameters fixed gaussian likelihood see section vectors means variances marginals maintained updated explicitly new row matrix need stored explicitly 
information gain criterion suppose want score pattern value inclusion recall section order obtain cheap approximation expensive score new new posterior approximation inclusion try replace new defined 
show corresponding approximation negative information gain score computed 
step update site parameters bi included done adf update subtle point note outcome general different update described section likelihood approximation depends ui 
marginal cavity ui ui ui bi derived way section noting ui ui ui ui appendix appendix chapter ui 
adf update done section rendering new values bi denote old values bi obtained multiplying bi bi ui 
denote normalisation constant zi 
sequel denote expectations conditional expectations marginalise log zi mi ui mi recall mi bi 
order compute zi marginalise ui 
bi bi zi exp mi bi uc recall qi define qi algebra obtain log zi log mi bi log 
second part 
mi ui mi ui ui mi ui ui var ui ui ui ui ui bi ui ui ui ui bi var ui ui mi ui mi mi mi var 
application know var qi qi sufficient computing 
simplifications possible bi bi case bi mi define mi ui mi qi mi tedious algebra reveals log li log mi 

derivations projected latent variables extended information gain criterion suppose want score inclusion 
recall section obtain successively accurate approximations full information gain criterion allowing couplings points 

recall ui ui 
denote projection matrix inclusion ui ui 
definition see vt new row including furthermore ui 
mh bh bh usual convention mj bi 
section defined obtained multiplying uc mh uc 
note allow bh different bh 
recall 
computation covariance matrix available 
criterion derived section obtained special case 
approach section 
integrate determine log partition function zi parameters ui ui 
compute zi marginalise ui 
ui ui uc ui second uc uc ui ba factor uc ui exp 
obtain zi uc uc mh 
integrand function 
simplify define bh 
bee 
note 
furthermore 
noting changed multiplication obtain bee define bh bh mh appendix appendix chapter note log log 
second factor uc equal uc 
implies multiplying factor remaining ones results replaced 
zi obtain zi 

log zi log li log rt denote expectation compute second part criterion log uc log uc mh 
seen ui ui variance mean 
condition ui integrate ui 
affects log uc ui results addend just log uc 
merge representation define 
evaluates log tr define note 
combining parts log log rt tr 
derivations projected latent variables bring different form log log log rt tr log site parameters change note bh bh criterion expression simplified considerably 
furthermore bli case tedious algebra gives log lib log 
expression easy recognise special case 
note need case gp regression gaussian noise site parameters fixed algorithm 
serves starting point general case start bh update sequentially described 
note order compute need compute explicitly 
sufficient compute ut ut 
available costs back substitution dominates evaluation identify special case qi 
updating site parameters criterion evaluated sensible update site parameters ep steps 
describe sequential scheme updates associated variables alongside necessary order compute marginals required ep updates 
located position require cavity marginal uj obtained setting corresponding elements zero 
simplicity refer cavity marginal recall ui gaussian variable recall mean ui ui ba appendix appendix chapter variance 

var ui obtain var recall section adf update need cavity marginal read marginal 
le le 
recall require kj pj li recall vh 
sequential updating scheme implemented woodbury formula lemma reasons numerical stability incremental cholesky techniques see section 
eet cholesky factor written type discussed section back substitutions 
compute 
factors type called cheap stored 
ease notation define vector cholesky factor note ab cheap computed linear time 
furthermore bh bh 
representation maintain variables date 
furthermore log 
update procedure stages modify furthermore define change associated variables mark corresponding new versions prime example 
cholesky factor compute cheap 
allows update variables 
compute new cheap 
define 

derivations projected latent variables note 
note computed 
required fairly sense store explicitly 
easy see furthermore ee ee computed mentioned 
stage compute cavity marginal order perform ep update bj need wt wt 

inner product computed described paragraph 
cavity marginal apart drop values ep step obtain new values computed stage 
significantly different enter second stage representation updated explicitly 
inner products done follows 
note recall 
inner products 
updated 
second stage bj accordingly 
compute 
back substitutions furthermore see 
criterion computed representation variables suffices update 
convergence criterion stopping criterion update representation somewhat cheaper full criterion computation 
cases update works 
log log log complete second stage updating bj compute notation 
representation primed log log log appendix appendix chapter shown 
tr tr diag diagonal 
elements diag squared norms vectors computed 

require 
computed usual 
cost updating site parameters evaluating dominated computation requires back substitutions compute 
depending update strategy concrete implementation larger part complete may available 
case far silently assumed case derivation modified follows 
ui ui lm go far take integrand function ui ui zi bh bh 
ui degenerate leading re definitions log log tr 
derivations projected latent variables equivalently log log log tr log simpler form valid case re define fact easy see computation form exactly cases compute cost 
denote columns wm sequential updating scheme described stands substitutions implied re definitions replace wm 
computation 
keeping wm date maintain explicitly allows compute wl directly iteration 
final criterion evaluation done efficiently simply copy scheme evaluating example tr cost 
efficient evaluation possible representation extended considered 
gradient model selection criterion recall definition plv model selection criterion section 
practice minimise sum negative log omitted simplicity 
section derive gradient hyperparameters 
ui depends kernel hyperparameters site parameters correct gradient take dependence account 
fact assumption site parameters independent hyperparameters 
recall eq log ui ui appendix appendix chapter relative entropy term 
variation dk dd ui ui log tr gradient computations quite involved strategy follows 
aim write dg tr dki tr dki symmetric 
show accumulate define lq ki ki 
note de dki sym dki tr de tr dki tr aki dki symmetric frequently 
define ki 
note ib ki eb ki 
kie ki define kie ki dki de dki 
ib ib algebra gives dki dki dki accumulation sym el log log log ki tr dki tr dki 
derivations projected latent variables tr tr ki define ki 
tr tr dki tr kie de 
ek lq lq tr tr dki tr ki dki ki leads accumulations recall 
eq log eq log ti ui ui ui qi ki pi know eq log dq rn defined see section replace hi qi relation 
qi diag ki diag diag dki diag dki gives diag diag diag 
diag diag ki defining dq diag dki diag de 
diag appendix appendix chapter obtain ki diag ki dki de dki sym sym ki compute gradient efficiently general expect fairly large number hyperparameters 
straightforward accumulation wasteful due different matrices related back substitutions 
settle method problematic numerical stability 
fact easy see diag computed cheaply collecting noting lv ki sym computed computed overwriting need second large buffer 
note computed explicitly notational convenience 
sophisticated implementation requires matrix holding followed derivative matrices dk computed fly 
gradient computation precomputations 
likelihood factors ti associated hyperparameter variation leads dg eq log ti ui may require numerical quadrature 
special case hyperparameter location parameter sense log ti ui log ti ui dui 
derivations projected latent variables partial integration see log ti ui ui eq eq qi log ti ui ai dg 
note bias parameters classification noise models probit logit location parameters 
appendix appendix chapter bibliography 
review gaussian random fields correlation functions 
technical report norwegian computing centre 
adler 
geometry random fields 
john wiley sons 
anthony bartlett 
neural network learning theoretical foundations 
cambridge university press 
anthony biggs 
computational learning theory 
cambridge university press 
anthony shawe taylor 
result vapnik applications 
discrete applied mathematics 

theory reproducing kernels 
trans 
amer 
math 
soc 
attias 
variational bayesian framework graphical models 
solla pages 
bach jordan 
kernel independent component analysis 
journal machine learning research 
baker 
numerical treatment integral equations 
clarendon press oxford 
bar shalom li 
estimation tracking principles techniques software 
artech house 
barber bishop 
ensemble learning multi layer networks 
jordan kearns solla editors advances neural information processing systems pages 
mit press 
bartlett lugosi 
model selection error estimation 
cesa bianchi goldman editors conference computational learning theory pages 
morgan kaufmann 
bibliography becker thrun obermayer editors 
advances neural information processing systems 
mit press 
berger 
statistical decision theory bayesian analysis 
springer nd edition 
jos bernardo adrian smith 
bayesian theory 
john wiley sons st edition 
billingsley 
probability measure 
john wiley sons rd edition 
bishop 
neural networks pattern recognition 
oxford university press st edition 
bishop frey editors 
workshop artificial intelligence statistics 
electronic proceedings isbn 
bishop 
structured variational distributions vibes 
bishop frey pages 
electronic proceedings isbn 
lugosi massart 
sharp concentration inequality applications 
random structures algorithms 
bousquet 
bennett concentration inequality application suprema empirical processes 
acad 
sci 
paris 
olivier bousquet andr elisseeff 
stability generalization 
journal machine learning research 
box tiao 
bayesian inference statistical analysis 
john wiley sons 
boyd vandenberghe 
convex optimization 
cambridge university press 
available online www stanford edu boyd html 
boyen koller 
tractable inference complex stochastic processes 
cooper moral editors uncertainty artificial intelligence 
morgan kaufmann 
brenner 
matrix transposition place 
communications acm 
cacm algorithm 
poggio 
incremental decremental support vector machine learning 
leen pages 
bibliography chernoff 
measure asymptotic efficiency tests hypothesis sum observations 
annals mathematical statistics 
chung 
course probability theory 
academic press nd edition 
clarke barron 
information theoretic asymptotics bayes methods 
ieee transactions information theory 
cohn ghahramani jordan 
active learning statistical models 
journal artificial intelligence research 
cortes haffner mohri 
rational kernels 
becker 
cortes vapnik 
support vector networks 
machine learning 
thomas cover joy thomas 
elements information theory 
series telecommunications 
john wiley sons st edition 
cram sur un th de la th orie des probabilit sci 

cressie 
statistics spatial data 
john wiley sons nd edition 
cristianini shawe taylor 
support vector machines kernel methods 
cambridge university press 
csat 
gaussian processes iterative sparse approximations 
phd thesis aston university birmingham uk march 
csat manfred opper 
sparse representations gaussian processes 
leen pages 
csat manfred opper 
sparse online gaussian processes 
neural computation 
csat manfred opper ole winther 
tap gibbs free energy belief propagation sparsity 
dietterich pages 
csisz rner 
information theory coding theorems discrete memoryless systems 
academic press 
bibliography csisz dy 
information geometry alternating minimization procedures 
editor statistics decisions pages 
oldenburg verlag munich 
cybenko berry 
hyperbolic householder algorithms factoring structured matrices 
siam matrix anal 
appl 
darroch ratcliff 
generalized iterative scaling log linear models 
annals mathematical statistics 
darwiche friedman editors 
uncertainty artificial intelligence 
morgan kaufmann 
davis 
methods numerical integration 
academic press 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal roy 
stat 
soc 

devroye gy rfi lugosi 
probabilistic theory pattern recognition 
applications mathematics stochastic modelling applied probability 
springer st edition 
dietterich becker ghahramani editors 
advances neural information processing systems 
mit press 
dongarra du hammarling hanson 
extended set fortran basic linear algebra subprograms 
acm trans 
math 
soft 
tipping 
analysis sparse bayesian learning 
dietterich pages 
fedorov 
theory optimal experiments 
academic press 
fine 
efficient svm training low rank kernel representations 
journal machine learning research 
roger fletcher 
practical methods optimization unconstrained optimization volume 
john wiley sons 
roger fletcher 
practical methods optimization constrained optimization volume 
john wiley sons 
freund schapire 
experiments new boosting algorithm 
saitta editor international conference machine learning pages 
morgan kaufmann 
bibliography freund seung shamir tishby 
selective sampling query committee algorithm 
machine learning 
friedman hastie tibshirani 
additive logistic regression statistical view boosting 
technical report department statistics stanford university 
friedman nachman 
gaussian process networks 
boutilier goldszmidt editors uncertainty artificial intelligence pages 
morgan kaufmann 
frieze kannan vempala 
fast monte carlo algorithms finding low rank approximations 
foundations computer science pages 
ghahramani beal 
propagation algorithms variational bayesian learning 
leen pages 
mark gibbs 
bayesian gaussian processes regression classification 
phd thesis university cambridge 
gilks richardson spiegelhalter editors 
markov chain monte carlo practice 
chapman hall st edition 
girard rasmussen murray smith 
gaussian process priors uncertain inputs application multiple step ahead time series forecasting 
becker 
green bernhard silverman 
nonparametric regression generalized linear models 
monographs statistics probability 
chapman hall 
geoffrey grimmett david 
probability random processes 
oxford university press rd edition 
gy rfi editor 
principles nonparametric learning 
springer 
david haussler 
convolution kernels discrete structures 
technical report ucsc crl university california santa cruz july 
see www cse ucsc edu haussler pubs html 
david haussler michael kearns robert schapire 
bounds sample complexity bayesian learning information theory vc dimension 
machine learning 
bibliography david haussler manfred opper 
mutual information metric entropy cumulative relative entropy risk 
annals statistics 
ralf herbrich 
learning kernel classifiers 
mit press st edition 
ralf herbrich graepel 
pac bayesian margin bound linear classifiers svms 
leen pages 
tom heskes 
expectation propagation approximate inference dynamic bayesian networks 
darwiche friedman 
hinton neal 
new view em algorithm justifies incremental variants 
jordan 

chal 
convex analysis minimization algorithms number der mathematischen wissenschaften 
springer 

chal 
convex analysis minimization algorithms ii 
number der mathematischen wissenschaften 
springer 
roger horn charles johnson 
matrix analysis 
cambridge university press st edition 

information theory continuous systems 
world scientific st edition 
jaakkola 
variational methods inference estimation graphical models 
phd thesis massachusetts institute technology 
jaakkola haussler 
exploiting generative models discriminative classifiers 
kearns pages 
tommi jaakkola david haussler 
probabilistic kernel regression models 
heckerman whittaker editors workshop artificial intelligence statistics 
morgan kaufmann 
tommi jaakkola marina meila tony jebara 
maximum entropy discrimination 
solla pages 
jordan 
probabilistic graphical models 
preparation 
jordan editor 
learning graphical models 
kluwer 
bibliography jordan jacobs 
hierarchical mixtures experts em algorithm 
neural computation 
kearns solla cohn editors 
advances neural information processing systems 
mit press 
kearns vazirani 
computational learning theory 
mit press 
kimeldorf wahba 
correspondence bayesian estimation stochastic processes smoothing splines 
annals mathematical statistics 
kolmogorov 
foundations theory probability 
chelsea new york nd edition 
trans 
morrison 

empirical margin distributions bounding generalization error combined classifiers 
annals statistics 
kondor lafferty 
diffusion kernels graphs discrete input spaces 
sammut hofmann editors international conference machine learning 
morgan kaufmann 

statistical approach basic mine valuation problems 
journal chemical mining society south africa 
langford shawe taylor 
pac bayes margin 
becker 
john langford rich caruana 
bounding true error 
dietterich pages 
langley editor 
international conference machine learning 
morgan kaufmann 
lawrence herbrich 
sparse bayesian compression scheme informative vector machine 
nips workshop kernel methods 
lawrence seeger herbrich 
fast sparse gaussian process methods informative vector machine 
becker pages 
bibliography lecun boser denker henderson howard hubbard jackel 
backpropagation applied handwritten zip code recognition 
neural computation 
ledoux talagrand 
probability banach spaces 
springer 
leen dietterich tresp editors 
advances neural information processing systems 
mit press 
lerner 
hybrid bayesian networks 
phd thesis stanford university 
nick littlestone manfred warmuth 
relating data compression learnability 
technical report university california santa cruz 
jun liu 
markov chain monte carlo related topics 
technical report department statistics stanford university 
lugosi 
pattern classification learning theory 
gy rfi 
lugosi 
concentration measure inequalities 
technical report 
summer school machine learning anu canberra 
luo wahba 
hybrid adaptive splines 
journal american statistical association 
mackay 
information objective functions active data selection 
neural computation 
mackay 
bayesian non linear modeling energy prediction competition 
ashrae transactions volume pages 
mackay 
probable networks plausible predictions review practical bayesian methods supervised neural networks 
network computation neural systems 
mackay 
gaussian processes 
technical report cambridge university uk 
see wol ra phy cam ac uk mackay readme html 

principles 
economic geology 

intrinsic random functions applications 
journal applied probability 
bibliography mcallester ortiz 
concentration inequalities missing mass histogram rule error 
becker 
david mcallester 
pac bayesian model averaging 
conference computational learning theory pages 
david mcallester 
pac bayesian theorems 
machine learning 
david mcallester 
pac bayesian stochastic model selection 
machine learning 
nelder 
generalized linear models 
number monographs statistics applied probability 
chapman hall st edition 
mceliece mackay 
cheng 
turbo decoding instance pearl belief propagation algorithm 
ieee journal selected areas communications 
mcnamee 
construction fully symmetric numerical integration formulas 
numerische mathematik 
ron meir tong zhang 
data dependent bounds bayesian mixture methods 
becker 
see citeseer nj nec com html 
minka picard 
learning learn learning point sets 
unpublished manuscript 
available media mit edu papers learning html 
thomas minka 
ep energy function minimization schemes 
see www stat cmu edu minka papers learning html august 
thomas minka 
expectation propagation approximate bayesian inference 
breese koller editors uncertainty artificial intelligence 
morgan kaufmann 
thomas minka 
family algorithms approximate bayesian inference 
phd thesis massachusetts institute technology january 
moore 
properly positive hermitian matrices 
bull 
amer 
math 
soc 
murphy 
dynamic bayesian networks representation inference learning 
phd thesis university california berkeley 
bibliography murphy weiss jordan 
loopy belief propagation approximate inference empirical study 
laskey prade editors uncertainty artificial intelligence pages 
morgan kaufmann 

sparse approximate solutions linear systems 
siam journal computing 
neal 
probabilistic inference markov chain monte carlo methods 
technical report crg tr university toronto 
see www cs toronto edu radford 
neal 
bayesian learning neural networks 
number lecture notes statistics 
springer 
radford neal 
monte carlo implementation gaussian process models bayesian classification regression 
technical report department statistics university toronto january 
nelder 
generalized linear models 
journal roy 
stat 
soc 

wahba goldfarb pugh 
cross validated spline methods estimation dimensional tumor size distributions observations dimensional cross sections 
journal american statistical association 
hagan 
curve fitting optimal design 
journal roy 
stat 
soc 

hagan 
bayesian numerical analysis 
bernardo berger dawid smith editors bayesian statistics pages 
oxford university press 
opper winther 
gaussian process classification svm mean field results leave estimator 
smola 
manfred opper 
bayesian approach line learning 
saad editor line learning neural networks 
cambridge university press 
manfred opper ole winther 
gaussian processes classification mean field algorithms 
neural computation 

nonstationary gaussian processes regression spatial modelling 
phd thesis carnegie mellon university pittsburg 
bibliography platt 
probabilistic outputs support vector machines comparisons regularized likelihood methods 
smola 
platt burges weare zheng 
learning gaussian process prior automatically generating music playlists 
dietterich pages 
john platt 
fast training support vector machines sequential minimal optimization 
sch lkopf pages 
white 
selecting concise training sets clean data 
ieee transactions neural networks 
poggio girosi 
networks approximation learning 
proceedings ieee 
william press saul teukolsky william vetterling brian flannery 
numerical recipes cambridge university press nd edition 
rasmussen 
evaluation gaussian processes methods nonlinear regression 
phd thesis university toronto 
rasmussen 
infinite gaussian mixture model 
solla pages 
rasmussen ghahramani 
infinite mixtures gaussian process experts 
dietterich pages 
redner walker 
mixture densities maximum likelihood em algorithm 
siam review 
ripley 
stochastic simulation 
john wiley sons 
christian robert george casella 
monte carlo statistical methods 
texts statistics 
springer st edition 
rockafellar 
convex analysis 
princeton university press 
rose 
deterministic annealing clustering compression classification regression related optimization problems 
proceedings ieee 
rosenblatt 
perceptron probabilistic model information storage organization brain 
psychological review 
bibliography roweis saul 
nonlinear dimensionality reduction locally linear embedding 
science 

fast method calculating perceptron maximal stability 
journal de physique 
saad 
iterative methods sparse linear systems 
international thomson publishing st edition 
schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods 
annals statistics 
schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods 
annals statistics 
sch lkopf burges smola editors 
advances kernel methods support vector learning 
mit press 
bernhard sch lkopf alexander smola 
learning kernels 
mit press st edition 
sch 
metric spaces completely monotone functions 
proc 
nat 
acad 
sci volume pages 
sch 
spline functions problem graduation 
annals 
seeger 
bayesian model selection support vector machines gaussian processes kernel classifiers 
solla pages 
seeger 
covariance kernels bayesian generative models 
dietterich pages 
seeger 
pac bayesian generalization error bounds gaussian process classification 
journal machine learning research october 
seeger langford megiddo 
improved predictive accuracy bound averaging classifiers 
brodley editors international conference machine learning pages 
morgan kaufmann 
seeger lawrence herbrich 
sparse bayesian learning informative vector machine 
technical report department computer science sheffield uk 
see www dcs shef ac uk neil papers 
bibliography seeger williams lawrence 
fast forward selection speed sparse gaussian process regression 
bishop frey pages 
electronic proceedings isbn 
matthias seeger 
bayesian methods support vector machines gaussian processes 
master thesis university karlsruhe germany 
see www cs berkeley edu 
matthias seeger 
annealed expectation maximization entropy projection 
technical report institute anc edinburgh uk 
see www cs berkeley edu 
matthias seeger 
covariance kernels bayesian generative models 
technical report institute anc edinburgh uk 
see www cs berkeley edu 
matthias seeger 
pac bayesian generalization error bounds gaussian process classification 
technical report edi inf rr division informatics university edinburgh 
see www cs berkeley edu 
seung opper sompolinsky 
query committee 
conference computational learning theory pages 
morgan kaufmann 
shawe taylor williams 
stability kernel principal components analysis relation process 
becker 
shawe taylor williamson 
pac analysis bayesian estimator 
conference computational learning theory pages 
john shawe taylor peter bartlett robert williamson martin anthony 
structural risk minimization data dependent hierarchies 
ieee transactions information theory 
skilling 
bayesian numerical analysis 
john skilling editor maximum entropy bayesian methods 
cambridge university press 
smola bartlett sch lkopf schuurmans editors 
advances large margin classifiers 
mit press 
smola ri williamson 
regularization dot product kernels 
leen pages 
bibliography smola sch lkopf 
sparse greedy matrix approximation machine learning 
langley pages 
smola sch lkopf 
ller 
connection regularization operators support vector kernels 
neural networks 
alex smola peter bartlett 
sparse greedy gaussian process regression 
leen pages 
murray smith rasmussen 
derivative observations gaussian process models dynamic systems 
becker 
solla leen 
ller editors 
advances neural information processing systems 
mit press 
peter 
learning curves gaussian processes 
kearns pages 
peter 
probabilistic methods support vector machines 
solla pages 
spiegelhalter thomas best gilks 
bugs bayesian inference gibbs sampling 
technical report mrc biostatistics unit cambridge university 
stein 
interpolation spatial data theory kriging 
springer 
martin szummer tommi jaakkola 
partially labeled classification markov random walks 
dietterich pages 
teh welling 
improving efficiency iterative proportional fitting procedure 
bishop frey pages 
electronic proceedings isbn 
tenenbaum de silva langford 
global geometric framework nonlinear dimensionality reduction 
science 
michael tipping 
sparse bayesian learning relevance vector machine 
journal machine learning research 
simon tong daphne koller 
support vector machine active learning applications text classification 
journal machine learning research 
bibliography volker tresp 
bayesian committee machine 
neural computation 
valiant 
theory learnable 
communications acm 
vapnik chervonenkis 
uniform convergence relative frequencies events probabilities 
theory probability applications 
vladimir vapnik 
statistical learning theory 
wiley st edition 
williams 
discovering hidden features gaussian process regression 
kearns 
van der wellner 
weak convergence empirical processes 
springer 
grace wahba 
spline models observational data 
cbms nsf regional conference series 
siam society industrial applied mathematics 
grace wahba 
support vector machines reproducing kernel hilbert spaces randomized 
sch lkopf pages 
wainwright sudderth willsky 
tree modeling estimation gaussian processes graphs cycles 
leen pages 
wainwright 
stochastic processes graphs cycles 
phd thesis massachusetts institute technology january 
wainwright jaakkola willsky 
new class upper bounds log partition function 
darwiche friedman 
waterhouse robinson 
classification hierarchical mixtures experts 
ieee workshop neural networks signal processing pages 
weiss freeman 
correctness belief propagation gaussian graphical models arbitrary topology 
solla pages 
williams 
computation infinite neural networks 
neural computation 
christopher williams matthias seeger 
effect input density distribution kernel classifiers 
langley pages 
bibliography christopher williams 
prediction gaussian processes linear regression linear prediction 
jordan 
christopher williams david barber :10.1.1.18.3953
bayesian classification gaussian processes 
ieee transactions pattern analysis machine intelligence 
christopher williams matthias seeger 
nystr method speed kernel machines 
leen pages 
wright 
modified cholesky factorizations interior point algorithms linear programming 
siam journal optimization 
xu jordan 
convergence properties em algorithm gaussian mixtures 
neural computation 

correlation theory stationary related random functions volume springer 
yedidia freeman weiss 
generalized belief propagation 
leen pages 
zhu williams rohwer 
gaussian regression optimal finite dimensional linear models 
bishop editor neural networks machine learning volume nato asi series 
springer 
