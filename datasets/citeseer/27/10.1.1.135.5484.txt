statistical learning kernel methods bernhard sch olkopf microsoft research limited street cambridge cb nh uk bsc microsoft com research microsoft com bsc february technical report msr tr microsoft research microsoft microsoft way redmond wa lecture notes course taught interdisciplinary college germany march 
briefly describe main ideas statistical learning theory support vector machines kernel feature spaces 
contents introductory example learning pattern recognition examples hyperplane classifiers support vector classifiers support vector regression developments kernels representing similarities linear spaces examples kernels dissimilarities linear spaces introductory example suppose empirical data xm ym theta sigma domain nonempty set patterns xi taken yi called labels targets 
stated indices understood run training set note assumptions domain set 
order study problem learning need additional structure 
learning want able generalize unseen data points 
case pattern recognition means new pattern want predict corresponding sigma 
mean loosely speaking choose sense similar training examples 
need similarity measures inf sigma 
easy target values identical different 
require similarity measure theta 
function examples returns real number characterizing similarity 
reasons clear function called kernel :10.1.1.103.1189
type similarity measure particular mathematical appeal dot products 
instance vectors rn canonical dot product defined delta nx denotes th entry geometrical interpretation dot product computes cosine angle vectors provided normalized length 
allows computation length vector asp delta distance vectors length difference vector 
able compute dot products amounts able carry geometrical constructions formulated terms angles distances 
note assumption patterns live dot product space 
order able dot product similarity measure need embed dot product space need identical rn map phi 
space called feature space 
summarize embedding data benefits 

lets define similarity measure dot product delta phi delta phi 
allows deal patterns geometrically lets study learning algorithm linear algebra analytic geometry 

freedom choose mapping phi enable design large variety learning algorithms 
instance consider situation inputs live dot product space 
case directly define similarity measure dot product 
choose apply nonlinear map phi change representation suitable problem learning algorithm 
position describe pattern recognition learning algorithm arguably simplest possible 
basic idea compute means classes feature space xfi xi xfi gamma xi number examples positive negative labels respectively 
assign new point class mean closer 
geometrical construction formulated terms dot products 
half way lies point 
compute class checking vector connecting encloses angle smaller ss vector gamma connecting class means words sgn gamma delta sgn gamma delta gamma sgn delta gamma delta defined offset gamma kc gamma kc delta prove instructive rewrite expression terms patterns xi input domain note dot product similarity measure cf 

need rewrite terms kernel evaluated input patterns 
substitute get decision function sgn xfi delta xi gamma xfi gamma delta xi sgn xfi xi gamma xfi gamma xi similarly offset xf yi yj gamma xj gamma xf yi yj xj consider known special case type classifier 
assume class means distance origin viewed density positive integral dx order state assumption require define integral holds true corresponds called bayes decision boundary separating classes subject assumption classes generated probability distributions correctly estimated parzen windows estimators classes xfi xi xfi gamma xi point label simply computed checking larger directly leads 
note decision best prior information probabilities classes 
classifier quite close types learning machines interested 
linear feature space input domain represented kernel expansion 
example sense kernels centered training examples arguments kernels training example 
main point sophisticated techniques discussed deviate selection examples kernels centered weight put individual kernels decision function 
longer case training examples appear kernel expansion weights kernels expansion longer uniform 
feature space representation statement corresponds saying study normal vectors decision hyperplanes represented linear combinations training examples 
instance want remove influence patterns far away decision boundary expect improve generalization error decision function reduce computational cost evaluating decision function cf 

hyperplane depend subset training examples called support vectors 
learning pattern recognition examples example mind consider problem pattern recognition formal setting 
class pattern recognition seek estimate function sigma input output training data 
assume data generated independently unknown fixed probability distribution 
goal learn function correctly classify unseen examples want examples generated 
put restriction class functions choose estimate function training data satisfying xi yi need generalize unseen examples 
see note function test set rn theta sigma satisfying mg fx fg exists function lambda lambda xi xi lambda xi xi 
training data means selecting functions completely different sets test label predictions preferable 
minimizing training error empirical risk remp mx jf xi gamma yij imply small test error called risk averaged test examples drawn underlying distribution jf gamma yj dp statistical learning theory vc vapnik chervonenkis theory shows imperative restrict class functions chosen capacity suitable amount available training data 
vc theory provides bounds test error 
minimization bounds depend empirical risk capacity function class leads principle structural risk minimization 
best known capacity concept vc theory vc dimension defined largest number points separated possible ways functions class 
example vc bound vc dimension class functions learning machine implement functions class probability gamma bound ff remp ff oe hm log holds confidence term oe defined oe hm log gamma log delta gamma log tighter bounds formulated terms concepts annealed vc entropy growth function 
usually considered harder evaluate play fundamental role conceptual part vc theory 
alternative capacity concepts formulate bounds include fat shattering dimension 
bound deserves explanatory remarks 
suppose wanted learn dependency delta pattern contains information label uniform 
training sample fixed size surely come learning machine achieves zero training error provided examples contradicting 
order reproduce random labellings machine necessarily require large vc dimension confidence term increasing monotonically large bound support possible hopes due small training error expect small test error 
understandable hold independent assumptions underlying distribution holds provided nontrivial prediction bound error rate void larger maximum error rate 
order get nontrivial predictions function space restricted capacity vc dimension small relation available amount data 
hyperplane classifiers section shall describe hyperplane learning algorithm performed dot product space feature space introduced previously 
described previous section design learning algorithms needs come class functions capacity computed 
considered class hyperplanes delta rn corresponding decision functions sgn delta proposed learning algorithm separable problems termed generalized portrait constructing empirical data 
facts 
hyperplanes separating data exists unique yielding maximum margin separation classes maxw gamma xik delta mg second capacity decreases increasing margin 
construct optimal hyperplane cf 
solves optimization problem minimize kwk subject yi delta delta xi constrained optimization problem dealt introducing lagrange multipliers ffi lagrangian ff kwk gamma mx ffi yi delta xi delta gamma lagrangian minimized respect primal variables maximized respect dual variables ffi saddle point 
try get intuition 
constraint violated yi delta delta xi gamma case increased increasing corresponding ffi 
time change decreases 
prevent gamma ffi yi delta delta xi gamma arbitrarily large change ensure provided problem separable constraint eventually satisfied 
similarly understand constraints precisely met equalities yi delta delta xi gamma corresponding ffi value ffi maximizes statement karush kuhn tucker complementarity conditions optimization theory 
condition saddle point derivatives respect primal variables vanish ff wl ff 


note 
yi yi binary classification toy problem separate balls diamonds 
optimal hyperplane orthogonal shortest line connecting convex hulls classes dotted intersects half way classes 
problem separable exists weight vector threshold yi delta delta xi 
rescaling point closest hyperplane satisfy delta xi bj obtain canonical form hyperplane satisfying yi delta delta xi 
note case margin measured perpendicularly hyperplane equals kwk 
seen considering points opposite sides margin 
delta delta gamma projecting hyperplane normal vector kwk 
leads mx mx solution vector expansion terms subset training patterns patterns ffi non zero called support vectors 
karush kuhn tucker complementarity conditions ffi delta yi xi delta gamma support vectors lie margin cf 

remaining examples training set irrelevant constraint play role optimization appear expansion 
nicely captures intuition problem hyperplane cf 
completely determined patterns closest solution depend examples 
substituting eliminates primal variables arrives wolfe dual optimization problem find multipliers ffi maximize ff mx ffi gamma mx xi delta xj subject ffi mx hyperplane decision function written sgn mx delta delta xi 
computed 
structure optimization problem closely resembles typically arise lagrange formulation mechanics 
subset constraints active 
instance keep ball box typically roll corners 
constraints corresponding walls touched ball irrelevant walls just removed 
seen light surprising possible give mechanical interpretation optimal margin hyperplanes assume support vector xi exerts perpendicular force size ffi sign yi solid plane sheet lying hyperplane solution satisfies requirements mechanical stability 
constraint states forces sheet sum zero implies torques sum zero pi xi theta delta kwk theta kwk 
theoretical arguments supporting generalization performance optimal hyperplane 
addition computationally attractive constructed solving quadratic programming problem 
support vector classifiers tools describe support vector machines 
section formulated dot product space 
think space feature space described section 
express formulas terms input patterns living need employ expresses dot product bold face feature vectors terms kernel evaluated input patterns delta feature space phi mm idea sv machines map training data higherdimensional feature space phi construct separating hyperplane maximum margin 
yields nonlinear decision boundary input space 
kernel function possible compute separating hyperplane explicitly carrying map feature space 
done feature vectors occured dot products 
weight vector cf 
expansion feature space typically correspond image single vector input space 
obtain decision functions general form cf 
sgn mx delta phi delta phi xi 
sgn mx delta xi 
quadratic program cf 
maximize ff mx ffi gamma mx xi xj subject ffi mx practice separating hyperplane may exist high noise level causes large overlap classes 
allow possibility examples violating introduces slack variables order relax constraints yi delta delta xi gamma classifier generalizes controlling classifier capacity kwk sum slacks pi done example support vector classifier radial basis function kernel exp gamma kx gamma :10.1.1.2.6040
coordinate axes range 
circles disks classes training examples middle line decision surface outer lines precisely meet constraint 
note support vectors algorithm marked extra circles centers clusters examples critical classification task 
grey values code modulus argument pmi delta xi decision function 
shown provide upper bound number training errors leads convex optimization problem 
possible realization soft margin classifier minimizing objective function kwk mx subject constraints value constant determining trade 
boldface greek letters shorthand corresponding vectors 
incorporating kernels rewriting terms lagrange multipliers leads problem maximizing subject constraints ffi mx difference separable case upper bound lagrange multipliers ffi 
way influence individual patterns xxx sv regression tube radius fitted data 
trade model complexity points lying outside tube positive slack variables determined minimizing 
outliers gets limited 
solution takes form 
threshold computed exploiting fact svs xi ffi slack variable zero follows karush complementarity conditions mx delta xi xj yi possible realization soft margin variant optimal hyperplane uses parametrization :10.1.1.2.6040
replaced parameter shown lower upper bound number examples svs come lie wrong side hyperplane respectively 
uses primal objective function error term pi gamma ae separation constraints yi delta delta xi ae gamma margin parameter ae variable optimization problem 
dual shown consist maximizing quadratic part subject ffi pi additional constraint pi ffi 
support vector regression concept margin specific pattern recognition 
generalize sv algorithm regression estimation analogue margin constructed space target values note regression vapnik insensitive loss function jy gamma maxf jy gamma gamma estimate linear regression delta precision minimizes kwk mx yi gamma xi written constrained optimization problem reads minimize lambda kwk mx lambda subject delta xi gamma yi yi gamma delta xi lambda lambda note error smaller require nonzero lambda enter objective function 
generalization kernel regression estimation carried complete analogy case pattern recognition 
introducing lagrange multipliers arrives optimization problem chosen priori maximize ff ff lambda gamma mx ff lambda ffi mx ff lambda gamma ffi yi gamma mx ff lambda gamma ffi ff lambda gamma ffj xi xj subject ffi ff lambda mx ffi gamma ff lambda regression estimate takes form mx ff lambda gamma ffi xi computed fact equality ffi equality lambda ff lambda extensions algorithm possible 
point view just need target function depends vector cf 

multiple degrees freedom constructing including freedom penalize regularize different parts vector freedom kernel trick 
instance general loss sigma 
output sigma ui xi um 
test vector support vectors xn mapped vectors phi xi phi phi phi xn dot product phi phi xi xi phi phi architecture sv machines 
input support vectors xi nonlinearly mapped phi feature space dot products computed 
kernel layers practice computed single step 
results linearly combined weights aei solving quadratic program pattern recognition aei regression estimation aei ff lambda gamma ffi 
linear combination fed function oe pattern recognition oe sgn regression estimation oe 
functions leading problems solved efficiently 
norms norm regularize solution 
example polynomial kernels incorporated consist multiple layers layer computes products certain specified subsets entries 
algorithm modified need specified priori 
specifies upper bound fraction points allowed lie outside tube asymptotically number svs corresponding computed automatically 
achieved primal objective function kwk mx yi gamma xi 
treating parameter minimize :10.1.1.2.6040
developments having described basics sv machines summarize empirical findings theoretical developments follow 
kernels optimal margin classifier turned classifier serious competitor high performance classifiers 
surprisingly noticed different kernel functions sv machines empirically lead similar classification accuracies sv sets 
sense sv set characterize compress task manner certain degree independent type kernel type classifier 
initial bell labs focused ocr optical character recognition problem main issues classification accuracy classification speed 
consequently effort went improvement sv machines issues leading virtual sv method incorporating prior knowledge transformation invariances transforming svs reduced set method speeding classification 
way sv machines competitive best available classifiers ocr object recognition tasks 
initial weakness sv machines apparent ocr applications characterized low noise levels size quadratic programming problem scaled number support vectors 
due fact quadratic part contained svs common practice extract svs going training data chunks regularly testing possibility patterns initially identified svs turn svs stage note chunking size matrix theta number training examples 
happens high noise problem 
case slack variables nonzero corresponding examples svs 
case decomposition algorithm proposed observation leave non sv examples xi ffi current chunk svs especially hit upper boundary ffi 
fact chunks contain svs maximize corresponding sub problems 
smo explores extreme case sub problems chosen small solve analytically 
public domain sv packages optimizers listed web page www kernel machines org 
details optimization problem see 
theoretical side understood part sv algorithm initially precise role kernel certain kernel choice influence generalization ability 
respect connection regularization theory provided insight 
kernel function expansions show regularization operator mapping functions learning machine dot product space problem minimizing regularized risk remp kp regularization parameter written constrained optimization problem 
particular choices loss function reduces sv type quadratic programming problem 
specific sv machines common wider class approaches 
gets lost general case fact solution usually expressed terms small number svs 
specific feature sv machines due fact type regularization class functions estimate chosen intimately related sv algorithm equivalent minimizing regularized risk set functions ffik xi provided interrelated xi xj xi delta xj chosen green function lambda case right hand side equals xi delta lambda xj xi delta xi xj instance gaussian rbf kernel corresponds regularization functional containing specific differential operator 
sv machines kernel plays dual role firstly determines class functions solution taken secondly kernel determines type regularization 
conclude section noticing kernel method computing dot products feature spaces restricted sv machines 
pointed develop nonlinear generalizations algorithm cast terms dot products principal component analysis number developments followed example 
kernels take closer look issue similarity measure kernel section think subset vector space rn endowed canonical dot product 
product features suppose patterns rn information contained th order products monomials entries delta delta jd jd case prefer extract product features feature space products entries 
visual recognition problems images represented vectors amount extracting features products individual pixels 
instance collect monomial feature extractors degree nonlinear map phi 
approach works fine small toy examples fails realistically sized problems dimensional input patterns exist nf gamma gamma 
different monomials comprising feature space dimensionality nf instance theta pixel input images monomial degree yield dimensionality 
certain cases described exists way computing dot products high dimensional feature spaces explicitely mapping means kernels nonlinear input space rn subsequent processing carried dot products exclusively able deal high dimensionality 
section describes dot products polynomial feature spaces computed efficiently 
polynomial feature spaces induced kernels order compute dot products form phi delta phi employ kernel representations form phi delta phi allow compute value dot product having carry map phi method extend generalized portrait hyperplane classifier nonlinear support vector machines :10.1.1.103.1189
termed linearization space context potential function classification method express dot product elements terms elements input space 
look case polynomial features 
start giving example 
map 
dot products take form delta delta desired kernel simply square dot product input space 
works arbitrary straightforward generalization result proved context polynomial approximation lemma proposition define cd map rn vector cd entries possible th degree ordered products entries corresponding kernel computing dot product vectors mapped cd cd delta cd delta proof :10.1.1.103.1189
directly compute cd delta cd nx jd delta delta jd delta delta delta jd nx delta delta ordered products unordered ones obtain map phi yields value dot product 
compensate multiple occurence certain monomials cd scaling respective entries phi square roots numbers occurence 
definition phi phi delta phi cd delta cd delta instance ji equal remaining ones different coefficient corresponding component phi gamma 
general case cf 

phi simply means phi represents image entries pixel values kernel delta space spanned products pixels provided able solely terms dot products explicit usage mapped pattern phi 
kernels form take account higher order statistics combinatorial explosion cf 
time memory complexity goes moderately high conclude section note possible modify maps space monomials degree defining delta representing similarities linear spaces follows look things way round start kernel 
kernel function construct feature space kernel computes dot product feature space 
question brought attention machine learning community 
functional analysis problem studied heading hilbert space representations kernels 
monograph functional analytic theory kernels large part material section 
aspect section differs previous dealt vectorial data 
results current section contrast hold data drawn domains need additional structure nonempty sets generalizes kernel learning algorithms large number situations vectorial representation readily available :10.1.1.21.2820
start basic definitions results 
definition gram matrix kernel patterns xm theta matrix xi xj ij called gram matrix kernel matrix respect xm 
definition positive matrix theta matrix kij ci ci called positive definition positive definite kernel nonempty set 
function theta xi gives rise positive gram matrix called positive definite kernel 
shall refer simply kernel 
term kernel stems type function study integral operators 
function gives rise operator dx called kernel argue term positive definite kernel slightly misleading 
matrix theory term definite usually denote case equality occurs cm 
simply term positive kernel hand confused kernel values positive 
literature number different terms bar cj denotes complex conjugation 
positive definite kernels reproducing kernel mercer kernel support vector kernel 
definitions positive definite kernels positive matrices differ fact case free choose points kernel evaluated 
positive implies positivity diagonal symmetry xi xj xj xi note complex valued case definition symmetry includes complex conjugation depicted bar 
definition symmetry matrices analogous kij kji 
obviously real valued kernels mainly concerned contained definition special case require kernel take values sufficient require hold real coefficients ci 
want get away real coefficients additionally require kernel symmetric xi xj xj xi shown complex valued positive definite kernel real part real valued positive definite kernel 
kernels regarded generalized dot products 
dot product shown kernel linearity carry dot products general kernels 
property dot products inequality natural generalization kernels proposition positive definite kernel jk delta proof 
sake brevity give non elementary proof basic facts linear algebra 
theta gram matrix entries kij xi xj positive 
eigenvalues nonnegative product determinant gamma gamma gamma jk substituting xi xj kij get desired inequality 
position construct feature space associated kernel define map space functions mapping denoted phi 
phi denotes function assigns value turned pattern function domain sense pattern represented similarity points input domain rich representation turn kernel allows computation dot product representation 
shall construct dot product space containing images input patterns phi need endow linear structure vector space 
done forming linear combinations form mx ffik xi ffi xi arbitrary 
define dot product function fij hf gi mx xi see defined explicitly contains expansion coefficients need unique note hf gi xi xi 
depend particular expansion similarly note hf gi mx ffig xi equations show argument linear second 
symmetric hf gi hg functions fn coefficients fl fln nx fji positive definite kernel function space 
step proving dot product interesting property phi follows directly definition functions hk representer evaluation 
particular hk virtue properties positive kernels called reproducing kernels 
proposition jf jhk ij delta hf hf directly implies property left prove order establish dot product 
complete space functions norm corresponding dot product add limit points sequences convergent norm gets hilbert space usually called reproducing kernel hilbert space case real valued kernels included case chosen real hilbert space 
examples kernels suggest usage gaussian radial basis function kernels exp gamma kx gamma oe sigmoid kernels tanh delta theta note kernels convenient property unitary invariance gamma consider complex numbers lambda :10.1.1.103.1189
radial basis function kernel additionally translation invariant 
satisfies mapped example unit length phi 
addition points lie inside orthant feature space 
see recall unit lenght vectors dot product equals cosine enclosed angle 
cos phi phi phi delta phi hilbert space defined complete dot product space 
completeness means sequences convergent norm corresponding dot product limits 
amounts saying enclosed angle mapped examples smaller ss 
examples far apply case vectorial data 
give example vector space 
example similarity probabilistic events oe algebra probability measure gamma positive definite kernel 
examples include kernels string matching proposed :10.1.1.21.2820
dissimilarities linear spaces move larger class kernels 
interesting regards 
turn kernel algorithms larger class kernels positive definite kernels 
second relationship positive definite kernels interesting number connections classes provide understanding kernels general 
third intimately related question variation central aspect positive definite kernels thought dot products feature spaces hand embedded distance measures arising norms feature spaces 
definition differs additional constraint sum ci definition 
definition conditionally positive matrix symmetric theta matrix kij satisfying mx ci ci mx ci called conditionally positive 
definition conditionally positive definite kernel nonempty set 
function theta xi gives rise conditionally positive gram matrix called conditionally positive definite kernel 
definitions real valued case look exactly 
note symmetry required complex case 
due additional constraint coefficients ci follow automatically anymore 
trivially true positive definite conditionally positive definite 
strictly weaker conditionally positive definite conditionally positive definite 
see simply apply definition get pi ci ci cj xi xj ci cjk xi xj ci cjk xi xj standard example conditionally positive definite kernel positive definite gamma kx gamma dot product space 
see simply compute pattern set xm xi xj gamma gamma xjk gamma gamma gamma xi delta xj delta gamma ci gamma cj xi delta xj xi delta xj line follows fact delta positive definite kernel 
note negative put cm kernel positive definite 
proof add fact gamma kx gamma kfi conditionally positive definite fi 
consider kernel considered canonical conditionally positive kernel dot product space see related dot product 
clearly distance induced norm invariant translations gamma gamma gamma gamma words complete knowledge kx gamma completely determine underlying dot product reason dot product invariant translations 
needs fix origin going distance measure dot product 
need write dot product gamma gamma terms distances gamma delta gamma delta kx delta delta gamma gamma kx gamma kx gamma kx gamma delta construction result positive definite kernel dot product positive definite kernel translated inputs 
established connection class positive definite kernels corresponding dot product different coordinate systems related translations 
fact similar connection holds wide class kernels proposition symmetric kernel theta satisfying 
gamma gamma positive definite conditionally positive definite 
result generalized 
case simply need add right hand side 
necessary contradicting 
proof state sufficient 
result prove interesting connection classes kernels proposition kernel conditionally positive definite exp tk positive definite 
positive definite kernels form exp tk interesting property th root positive definite kernel 
kernels called infinitely divisible 
show disregarding technicalities logarithm infinitely divisible positive definite kernel mapping conditionally positive definite kernel 
conditionally positive definite kernels natural choice dealing translation invariant problem support vector machine maximization margin classes data independent origin position 
seen dual optimization problem constraint pmi projects subspace definition conditionally positive matrices 
seen positive definite kernels conditionally positive definite kernels closely related 
represented dot products hilbert spaces 
turns essentially correspond distance measures associated norms hilbert spaces proposition real valued conditionally positive definite kernel satisfying exists hilbert space real valued functions mapping phi gamma phi gamma phi exist generalizations case maps cases representation looks slightly complicated 
significance proposition conditionally positive definite kernels generalize algorithms distances corresponding algorithms operating feature spaces 
analogue kernel trick distances dot products dissimilarities similarities 

smola williamson discussions watkins pointing nips svm workshop talk distances dot products differ way deal origin 
aizerman braverman er 
theoretical foundations potential function method pattern recognition learning 
automation remote control 
alon ben david cesa bianchi haussler 
scale sensitive dimensions uniform convergence learnability 
journal acm 

theory reproducing kernels 
trans 
amer 
math 
soc 
bartlett shawe taylor 
generalization performance support vector machines pattern classifiers 
sch olkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
berg christensen 
harmonic analysis semigroups 
springer verlag new york 
bertsekas 
nonlinear programming 
athena scientific belmont ma 
blanz sch olkopf burges vapnik vetter 
comparison view object recognition algorithms realistic models 
von der malsburg von seelen sendhoff editors artificial neural networks icann pages berlin 
springer lecture notes computer science vol 

boser guyon vapnik :10.1.1.103.1189
training algorithm optimal margin classifiers 
haussler editor proceedings th annual acm workshop computational learning theory pages pittsburgh pa july 
acm press 
burges sch olkopf 
improving accuracy speed support vector learning machines 
mozer jordan petsche editors advances neural information processing systems pages cambridge ma 
mit press 
cortes vapnik 
support vector networks 
machine learning 
girosi jones poggio 
regularization theory neural networks architectures 
neural computation 
haussler 
convolutional kernels discrete structures 
technical report ucsc crl computer science department university california santa cruz 
mercer 
functions positive negative type connection theory integral equations 
philos 
trans 
roy 
soc 
london 
osuna freund girosi 
improved training algorithm support vector machines 
principe morgan wilson editors neural networks signal processing vii proceedings ieee workshop pages new york 
ieee 
platt 
fast training support vector machines sequential minimal optimization 
sch olkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 
poggio 
optimal nonlinear associative recall 
biological cybernetics 
sch olkopf 
support vector learning 
oldenbourg verlag unchen 
tu berlin 
sch olkopf burges vapnik 
extracting support data task 
fayyad uthurusamy editors proceedings international conference knowledge discovery data mining menlo park 
aaai press 
sch olkopf burges smola 
advances kernel methods support vector learning 
mit press cambridge ma 
sch olkopf platt shawe taylor smola williamson 
estimating support high dimensional distribution 
tr msr microsoft research redmond wa 
sch olkopf smola 
uller 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
sch olkopf smola williamson bartlett :10.1.1.2.6040
new support vector algorithms 
neural computation 
smola sch olkopf 
uller 
connection regularization operators support vector kernels 
neural networks 
smola sch olkopf 
kernel method pattern recognition regression approximation operator inversion 
algorithmica 
smola sch olkopf 
tutorial support vector regression 
neurocolt technical report nc tr royal holloway college university london uk 
smola bartlett sch olkopf schuurmans 
advances large margin classifiers 
mit press cambridge ma 
vapnik 
estimation dependences empirical data russian 
nauka moscow 
english translation springer verlag new york 
vapnik 
nature statistical learning theory 
springer 
vapnik 
statistical learning theory 
wiley 
vapnik chervonenkis 
note class perceptrons 
automation remote control 
vapnik chervonenkis 
theory pattern recognition russian 
nauka moscow 
german translation theorie der akademie verlag berlin 
vapnik lerner 
pattern recognition generalized portrait method 
automation remote control 
wahba 
spline models observational data volume cbms nsf regional conference series applied mathematics 
siam philadelphia 
watkins 
dynamic alignment kernels 
smola bartlett sch olkopf schuurmans editors advances large margin classifiers pages cambridge ma 
mit press 
williamson smola sch olkopf 
generalization performance regularization networks support vector machines entropy numbers compact operators 
technical report neurocolt www neurocolt com 
accepted publication ieee transactions information theory 

