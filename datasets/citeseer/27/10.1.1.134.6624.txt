pattern discovery entropy minimization matthew brand merl cambridge ma usa mar revised oct 
appear proceedings uncertainty ai statistics 
propose framework learning models optimizing entropies entropy minimization posterior maximization free energy minimization equivalent 
solutions maximum posteriori map estimator yield powerful learning algorithms combine expectation maximization deterministic annealing 
contained special cases methods maximum entropy maximum likelihood new method maximum structure 
focus maximum structure case entropy minimization maximizes amount evidence supporting parameter minimizing uncertainty sufficient statistics cross entropy model data 
iterative estimation map estimator gradually excess parameters sculpting model structure reflects hidden structures data 
models highly resistant fitting particular virtue easy interpret yielding insights hidden causes generate data 
motivation pattern discovery seek model reflects structure data hope turn reflects structure generating process 
premise universe constructed relatively small processes order produce object dataset larger small process kind repetition structure loops 
dataset unrolled record process internal structure 
find compact model data feel increasingly email brand merl com brand media mit edu 
confident interpret model doing learn process predictions model consistent new samples taken process 
know founded theorems algorithmic complexity assure possible models possible turing machines yielding smallest part encoding model plus data relative model best strategy hypothesis identification vitanyi li best strategy prediction vit nyi li 
sadly function yields compact model computable 
furthermore know search efficiently accurate models disregarding compactness quite restricted spaces locally optimal parameterizations stochastic models kind structure fix advance 
broaden scope estimating model structure parameters manner reveals salient structures data 
framework yields fast hillclimbing procedure finding nearly global optima 
setting interested learning point estimating single model highly predictive structure reflects structure generating process 
address important limits finite data finite time finite precision 
particular want extract essential structure possible dataset modeling accidental structure noise sampling artifacts want wasted speculative computation models discarded selection process want maximize information content parameters bits evidence supporting parameter realistic information capacity digitally represented numbers 
bayesian setting shall show framework clearest interpretation learning considered problem minimizing entropies 
set observations hypothesis class models likelihoods approximated practice products densities drawn family having minimal sufficient statistics parameter vectors equal dimension hidden variables assume data latent structure incomplete 
vector parameterizes model specifies structure sparse encoding 
starting random parameterized model seek optimal embedded model maximizes posterior bayes rule likelihood measures accuracy modeling data prior measures consistency background knowledge 
maximum structure priors background knowledge 
assert vector model parameters taken random variable generates possible processes 
know average processes infinitely unpredictable learning impossible 
background knowledge average learning succeed 
formally say expected entropy processes finite 
write prior knowledge entropy measure assessed model specified expectation regard finite value 
classic maximum entropy method jaynes allows derive mathematical form distribution knowledge expectations euler lagrange equations yielding lagrange multiplier depends unknown 
shall find meaningful interpretations values shall concentrate assumption shall call entropic prior assume henceforth drop notation 
note passing making similar assertion expected perplexity assuming measure possible integrate lagrange multiplier arrive directly eqn 

call reader attention properties derive definition entropy bias compact models having ambiguity determinism maximum entropy distributions assertion random variable characterized expectations structure 
invariant model entropy defined terms model joint factored distributions 
turn inferential principles optimization problems 
models hidden variables difficult fit notoriously prone fitting 
eqn 
remarkable form shall exploit simultaneously estimate structure parameters complex probability models mixed combinatorial structure continuous parameter optimization problem 
shall develop map estimators expectation maximization em prior drives weakly supported parameters extinction allowing removed model loss posterior probability 
resulting models quite combinatorial optimization problems notoriously rough energy surfaces em merely local hill climber 
presenting estimators shall develop technique uses entropic prior improve quality local optima em finds quasi global optimum limiting case 
prior balancing introduce generalization posterior rewriting bayes rule manipulation prior normally positive driving term monotonically increases approaches zero 
varying balances prior likelihood useful iterative parameter estimation allows get right neighborhood respect constraint attempting satisfy 
course obtain proper probability obliged converge meaningful value done gradient iteratively tracking gradient map estimate map estimate shorthand gaussian driving function decay rate controlled variance term choice different driving function 
barrier term 
known gap equilibrium desired bridged heuristic schedule 
may sided introduce optimization technique unclear useful interpretation inference principle 
prior balancing take physical meaning entropic prior 
negated logarithm eqn 
entropic prior obtain error energy cost parameterization understood temperature 
notation indicates equality assuming additive constants normalizing terms omitted 
upper bound helmholtz free energy equation statistical physics equality maximizing modified posterior minimizes free energy analogous finding equilibrium configuration complex model different parts compete explain data 
models likelihoods priors prior independent parameter balanced separately allowing different parts model mature different rates 
interesting cases immediately fall iteratively re estimating gives deterministic annealing da rose miller hofmann pseudo global optimizer energy surfaces 
da belongs family continuation techniques convolve smooth energy surface globally convex track optimum gradual deconvolution surface texture illustration shows tracking local minimum leads global minimum function forefront log posterior temperature optimization continuation parameters da entropy high temperatures keeps system prematurely committing nearby local optima forces explore energy surface large scale statistical analogue robust estimation huber shrinking scale parameter li 
structure 
hidden variable model equivalent maximizing equally possible hypotheses model possible paths hidden markov model concentrating promising hypotheses temperature declines 
ll introduce map estimators fold da em additional computational cost 
note modified posterior gives useful amendment da automatic annealing schedule tracks quality inversely entropy model gradients map estimates remaining cases interest arise different terminal temperatures obtain maximum entropy solution 
obtain maximum likelihood ml solution 
obtain maximum structure solution corresponding entropic prior eqn 

conveniently derived single map estimator cases 
map estimators obtain map parameter estimators solving maxima log balanced posterior lead systems transcendental equations obtained solutions simple distributions subadditivity principles models composed thereof 
multivariate mean gaussian distribution matrix covariance entropy entropic prior uniform inversely proportional volume find entropic map estimator set gradient log posterior zero 
loss generality assume zero mean simplify derivation anonymous reviewers pointing developed special case directly maximum entropy considerations albeit estimators annealing schedule 
left right multiplying reveals entropic map estimator covariances essentially normalization scatter samples note maximum structure map estimator best mean squared error estimate map estimator best unbiased estimate 
similarly normalized estimators give scale parameters related distributions exponential laplace gamma 
multinomial distribution entropy entropic prior map estimator vector evidence alternative set derivative log posterior zero lagrange multiplier ensure solve system simultaneous transcendental equations lambert function inverse mapping satisfying setting working backwards eqn 
setting simplifies eqn 

implies eqn 
eqn 
eqn 
form set fix point equations typically converge iterations 
derivation generalizes brand 
details computing brand 
differentiating eqn 
find posterior concave parameter provided true near degenerate cases evidence 
note assumed likelihood prior fortunately case popular hidden variable models 
minimization entropies flat exponential distributions principle maximum likelihood equivalent minimizing cross entropy directed kullback leibler distance data sufficient statistics distribution estimated parameters 
framework identifies entropies associated modeling map estimator minimizes sum 
clearly seen considering negated log posterior multinomial parameter maximum structure case entropies useful interpretation measures uncertainty expected sufficient statistics linearly related coding length 
cross entropy measures error model fit data linearly related coding costs aspects data captured model 
ideally purely random noise 
terms give expected coding length data relative model 
measures uncertainty model component distributions coding length 
short map estimation reduces entropies relative entropies model data sufficient statistics 
practice parts model ill matched structure data weakly supported 
evidence concentrates surviving parameters equivalently cases may slightly elaborate methods find global optimum simply accept local optimum ignore typically negligible effect joint distribution 
minimized information content maximized 
variations may introduce entropies eqn 
cross entropy equiv 
kl divergence want conserve information contained previous estimates stay close far model happily small variations map formul accommodate combination constraints 
simplicity exposition identified entropy model component distributions leading extremely efficient form description length minimization 
may choose entropy measures example may prefer minimize joint entropy fixed probabilities hidden variable values data 
estimated data step em cases computed parameters markov models stationary probability state 
case leads small variation map estimator result approximate entropy minimized calculated previous current parameter estimates 
find practice case behaved especially deterministic annealing changes quite gradually 
trimming problem entropy minimization digital computers numerical error parameters fully extinguished 
eqn 
add numbers logarithms troublesome parameter values near 
fortunately prior allows identify excess parameters trimmed model loss posterior probability expanding bayes rule logarithms rearranging obtain operationally parameter trimmed setting target value model simpler easier interpret 
typically target minimal entropy extinction value setting multinomial parameter may simply usefully interpreted value setting variance parameter equivalently 
zeroing eqn 
approximated differentials yielding may observe parameter trimmed varying increases entropy faster log likelihood 
sides eqns 
mixed matched obtain mathematically convenient forms 
multinomial trim test set eqn 
eqn 
obtain gradient log likelihood normally computed re estimation trimming tests cheap 
solving eqn 
gives variance trimming criteria trimming test covariances scope 
trimming local heuristic increases maximize posterior 
sense iterative estimation models containing hidden variables radically model turn reduces ambiguity data expected sufficient statistics 
short accelerates learning 
experiments trimming mainly speed parameter extinction happen gradually map estimation 
noted particularly desirable parameters near extinction floating point calculations introduce numerical round underflow errors 
addition trimming near convergence bump model local local probability maximum parameter subspace simpler geometry enabling training 
parameter extinction trimming protect fitting sculpting model fit data reveal simple machine explains data finite state machine hmm circuit nn 
shall give examples 
related ideas model selection criteria mml wallace boulton wallace freeman aic mdl rissanen bic schwartz stochastic complexity rissanen rich literature model selection usually leveraged priors estimators force separate treatments model structure parameter values 
example bic mdl asymptotic approximations marginal likelihood assuming structure typically ignoring prior typically formulated level granularity number parameters prevents continuous sample spaces 
entropic prior bears resemblance selection criteria provides unified treatment structure parameter values 
stochastic complexity occupies interesting middle ground natural interpretations bit level granularity leading approximate estimators occasionally yielding point estimator discrete sample spaces 
vovk derived binomial parameter estimator showed minimizes stochastic complexity remarkably mdl strong sense vit nyi li probability close 
unfortunately mdl discretization continuous values severe consequences contrasts discontinuous vovk estimator equivalent entropic map estimator 
estimate bernoulli parameters selection criteria vovk rissanen entropic estimates evidence ratios entropic estimate vovk rissanen estimate total mass evidence computer scientists tried get leverage model selection criteria proposing search heuristics reduce computation dominates generate test structure learning algorithms 
ikeda stolcke omohundro search algorithms hmm structure generate rival models splitting merging states reduce description length 
run times reported days entropic estimation induce hmm topologies similar datasets matter minutes 
friedman advertised structural em algorithm bayesian networks step approximate penalized generate test search inside em loop :10.1.1.24.1555
propose entropy minimization algorithm bayesian networks parameter extinction subgraphs deterministic 
unfortunately step dense graphs computationally prohibitive starting complete structure problematic 
exponential density forms conjugate priors obtain estimators reducing properties 
example multinomial parameters dirichlet priors exponents course choose values parameters priori equivalent inventing extra observations actual samples taken potentially dangerous business 
examples entropic estimation obtain low entropy models variety model classes 
tested benchmark datasets real time data obtained directly computer vision speech analysis systems 
resulting models consistently smaller discriminative better generalizing predictive conventionally ml estimated models fit 
typically resulting model interpretable providing insight causal structure process generated data 
give variety examples mixture models mixture model fitted ring cartesian samples taken uniform distribution polar coordi figures show model estimated conventionally entropically trimming entropically trimming deterministic annealing 
initial conditions identical cases 
ellipses indicate iso density contours gaussian components 
conventional entropic annealed entropic case fitting resulted model accidental properties data sample 
second case trimming removed excess components resulting model looks essential structure data large sampled region near top affects model 
third case da circumvents local optimum finds model generalizes better 
radial basis function networks vowel classification obtained british english vowel recognition dataset cmu neural bench archive 
example vowel characterized lpc coefficients time frames 
dataset treated times perceptrons neural networks kanerva models radial basis function networks non parametric methods nearest neighbor 
yielded best classification test set peaked gaussian basis functions robinson 
combined entropically estimated mixture models vowel training data rbfn obtained correct classification test set 
entropic estimation mixtures components vowel resulting rbfn basis functions 
home value prediction obtained publicly available boston home value database statlib set predict median home value potentially relevant attributes 
features boolean valued different arbitrary dynamic ranges normalized dimension unit variance mixture modeling 
trial models identically initialized trained half examples randomly selected predict home values remaining test examples 
performance ml model baseline trial measure performance entropically estimated model reduced mean squared error predictions test set 
table shows mean improvement rbfn prediction accuracy maximum likelihood models function rbfs initialization 
rbfs entropic map 
trimming 
annealing values level statistical significance marked 
improvement quite significant large initialization recall scores terms unit variance prices 
attributed entropic estimation resistance fitting 
advantage disappears statistically insignificant initializations small notice fitted models annealing finding superior local optima 
hidden markov models hmms handwriting analysis classification obtained handwriting samples writers archive 
diagrams show entropic maximumlikelihood models pen strokes digit estimated pen position data taken msec intervals different individuals electro magnetic resonance sensing tablet 
initial conditions identical 
ellipses indicate show iso density contours state arcs indicate state dwell transition probabilities respectively thicknesses 
entropic estimation induces interpretable automaton captures essential structure timing pen strokes variations ordering writers 
original dynamical parameters trimmed 
estimation entropic prior results wholly opaque model original dynamical parameters trimmed 
trials different parts database held entropic models yielded correct classification ml models yielded 
video analysis example gigabyte video randomly taken office setting filtered extract motion vectors modeled entropically hidden markov model brand 
resulting hmm state machine compact read enter exit whiteboard phone computer roughly transitions trimmed 
taken liberty labeling states forwardbackward analysis find support new video 
states mapped nicely typical activities office occupant 
hmm conventionally estimated initial conditions fully connected bushy profitably illustrate interpret 
prediction bach chorales obtained dataset melodic lines bach surviving chorales uci repository merz murphy transposed key compared entropically conventionally estimated hmms prediction classification tasks training identical random initial conditions trying variety different initial state counts 
trained chorales testing remaining 
trials chorales rotated test set 
results neatly chart sparsification classification prediction superiority entropically estimated hmms 
parameters zeroed entropic versus ml hmm models bach chorales notes correctly predicted states initialization sequences correctly classified lines indicate mean performance trials error bars standard deviations 
clear despite substantial loss parameters sparsification entropically estimated hmms average better predictors notes 
test sequence truncated random length hmms predict missing note 
better discriminating test chorales temporally reversed test chorales challenging bach employed melodic reversal compositional device 
larger models parameter trimming state trimming average states state models incoming transitions deleted 
conventionally estimated hmms wholly uninterpretable entropically estimated hmms discern basic musical structures 
sampling high probability states subgraphs interest entropically estimated state hmm 
tones output state listed order probability 
extraneous arcs removed clarity 
ced include self transitioning states output tonic dominant triads lower diatonic tones 
chordal state sequences states lead tonic leading tone 
forward backward analysis chorales confirms subgraphs explaining musical structures 
facial animation case learned dynamical model signals acoustic features voice visual features face video 
inspection revealed model organized facial configuration space prototypes strongly resemble visemes 
technically recognition model sparse near deterministic generate realistic facial motions accompany new voice track near photorealistic face 
empirical entropy rate model indicates propagating context effects average msec forward backward time consistent vocal coarticulation effects context propagating msec probably facial articulation effects typically occur longer time scales due agile facial tissue 
surprisingly model able predict upper facial motion accurately motion mouth exploiting prosodic information acoustic signal 
recurrent neural networks rnns speculative example entropic prior designed weights recurrent neural network assuming generates neural networks weights gaussian distribution functionally related mean variance 
yields modified back propagation rule trim test 
figures show semi recurrent neural network dag self connections memory activation propagates link cycle trained compute xor conventional back prop weight decay left entropic back prop trimming right 
initial conditions identical 
bias note entropically estimated topology amalgam minimal feed forward xor circuits 
bias open questions framework agnostic regard pressing questions firstly criteria motivate choice entropy measure discussed different choices lead mdl entropy minimax 
cases identical 
consistent results upper bound analytic forms unavailable 
secondly remove parameters 

near convergence 
da 
various combinations entropic training trimming annealing intriguing physical analogues including processes casting tempering 
entropic prior pose severe challenges integration bayesian methods model averaging 
integrating hidden variable models intractable bayesians typically fall back marginal likelihoods single model structure approximation 
classes models marginal likelihoods predictive posterior densities general reason believe approximation hmms model structure having greatest marginal likelihood shares probability mass posterior mode 
integration desirable infeasible propose falling back weighted responses population entropically estimated models 
course depending choice integral normalizes prior problem remains outstanding 
require solving worth inquiring framework extended purely discrete optimization problems 
preliminary results encouraging possible generalize multinomial map estimator compute doubly stochastic matrices 
allows entertain graphtheoretic problems solutions formulated permutation matrices zeros single row column 
problems doubly stochastic matrix weighted super imposition possible solutions high temperature hardened single quasi optimal solution entropy algorithmically minimized 
initial experiments traveling salesman problems quite promising 
summary develops efficient method finding compact hidden variable probability models entropy minimization 
models highly predictive interpretable theories identify relationships hidden causes observed effects 
main results entropic prior favors small unambiguous maximally structured models 
prior balancing manipulation bayes rule allows gradually introduce remove constraints course iterative re estimation 
combined yields posterior negative logarithm equals upper bounds helmholtz free energy model 
posterior contains special cases methods maximum entropy maximum likelihood new method maximum structure 
map estimators entropy optimization deterministic annealing performed wholly em 
maximum structure case map estimator smoothly excess parameters simplifying model preventing fitting 
trimming tests identify excess parameters removal increase posterior 
accelerate learning 
combined result class fast exact hill climbing algorithms mix continuous combinatoric optimization evade suboptimal equilibria 
acknowledgments reviewers clarifying questions making interesting connections statistics literature 


information theory extension maximum likelihood principle 
petrov cs ki editors proc nd international symp 
inference theory pages 
akad 
brand 

learning concise models human activity ambient video 
technical report mitsubishi electric research labs 
brand 

structure discovery conditional probability models entropic prior parameter extinction 
neural computation accepted 
gonnet hare jeffrey knuth 

lambert function 
advances computational mathematics 
friedman 

bayesian structural em algorithm 
proc 
th conf 
uncertainty artificial intelligence 
hofmann puzicha buhmann 

unsupervised texture segmentation deterministic annealing framework 
ieee transactions pattern analysis machine intelligence pami 
huber 

robust statistics 
wiley sons 
ikeda 

construction phoneme models model search hidden markov models 
international workshop intelligent signal processing communication systems sendai 
jaynes 

papers probability statistics statistical mechanics chapter brandeis lectures pages 
kluwer academic 
li 

robust estimation deterministic annealing 
pattern recognition 
merz murphy 

uci repository machine learning databases 
miller rao rose gersho 

global optimization technique statistical classifier design 
ieee transactions signal processing 
rissanen 

modeling shortest data description 
automatica 
rissanen 

stochastic complexity statistical inquiry 
world scientific 
robinson 

dynamic error propagation networks 
phd thesis cambridge university engineering department 
see www cs cmu edu benchmarks vowel html updated 
rose gurewitz fox 

deterministic annealing approach clustering 
pattern recognition letters 
schwartz 

estimating dimension model 
annals statistics 
stolcke omohundro 

best model merging hidden markov model induction 
technical report tr international computer science institute center st berkeley ca usa 

deterministic annealing em 
tesauro touretzky leen editors advances neural information processing systems volume 
mit press 
vitanyi li 

ideal mdl relation bayesianism 
isis information statistics induction science pages 
world scientific singapore 
vit nyi li 

minimum description length induction bayesianism kolmogorov complexity 
submitted ieee information theory 
revised version dec available www cwi nl selection html 
vovk 

minimum description length estimators optimal coding scheme 
vit nyi editor proceedings computational learning theory europe pages 
springer verlag 
wallace boulton 

information measure classification 
computing journal 
wallace freeman 

estimation inference compact coding 
journal royal statistical society series 
deterministic annealing paths illustrate different annealing methods associated continuation shown 
figures show iso contours surface possible paths minimum 
goal travel correct annealing path computing points possible 
regard gradient descent finds local minimum temperature moving fixed increments lowers temperature fixed schedule 
lead disaster temperature declines fast expectation maximization usually efficient way get neighborhood local optimum exploit machinery calculate safe jumps temperature practice find sufficient just move local optimum temperature interleaving parameter temperature re estimation obtain fast trajectories optimum 
