parallelism utilization embedded reconfigurable computing systems survey trends koen computer engineering laboratory faculty electrical engineering mathematics computer science delft university technology netherlands koen ce tudelft nl embedded reconfigurable computing attracted great attention due potential accelerate application execution 
key feature ability perform computations hardware increase performance retaining flexibility software solution 
researchers field reported substantial performance improvements variety different applications cryptography multimedia processing genetics networking dsp 
embedded reconfigurable computing systems lend high performance computing advantage parallelism different levels granularity ranging fine grained instruction level coarse grained process task level parallelism 
necessary different parallel processing techniques fully take advantage systems 
survey explore enhancements new field computing considering embedded reconfigurable hardware architectures software facilities targeting systems 
focus survey employment parallelism key feature application development embedded reconfigurable systems 
precisely different levels parallelism indicated instruction level data loop level task level process thread level introduced distinguished properties identified category 
various reconfigurable systems incorporating combination attributes investigated 
generally try identify major problems limit embedded reconfigurable computing systems reaching maximum potentials 
keywords embedded reconfigurable computing fpga parallelism hardware software design design performance evaluation tuning applications optimum performance elusive 
reconfigurable computing rc attracted great attention potential accelerate application execution 
key feature ability perform computations hardware increase performance retaining flexibility software solution 
traditionally methods implementing algorithms 
hardwired technology application specific integrated circuit asic separate components combined board 
asics dedicated components designed specific computations prove quite fast efficient executing exact computation designed 
flexible method employment software programmable microprocessors processors execute set instructions perform computations 
simply changing software instructions result system functionality alternation hardware manipulation 
flexibility going compensated degradation performance 
basically dynamic nature reconfigurable devices addressed collection computational elements logic blocks functionality decided means configuration patterns 
logic blocks composed commercial fpgas custom configurable hardware 
new field computing attracted great attention shown efficient variety applications name network intrusion prevention data encryption image processing digital signal processing sophisticated applications face recognition 
reconfigurable systems usually employing general purpose processor gpp addition logic blocks exhibit better flexibility executing wide range applications achieve better performances 
gpps usually conduct hard map kernels codes easily implemented software control constructs reconfigurable units concerned computations 
compilation environments reconfigurable device quite diverse ranging tools hand mapping assistance digital unit hardware fully automated utilities fed circuit description high level language vhdl transformed configuration reconfigurable device 
noted reconfigurable devices demand fixed structure setting compiletime space availability source concern system accelerate program fits start run time reconfigurable devices possible reuse space program execution going introduce tradeoff extra reconfigurability time increased efficiency arising potential acceleration larger part program 
reconfigurable systems run time ones generally shown achieve high performance speedups traditional microprocessor systems limited cost configuration hardware 
current re configurable systems suffer significant overhead due time takes reconfigure hardware 
order deal overhead increase computing power reconfigurable systems important develop hardware software systems reduce eliminate delay 
reduce overhead configuration techniques configuration compression configuration prefetching technique partial reuse programmed configurations employed 
rapidly evolving field reconfigurable computing background techniques systems current researches hardware software rc techniques run time reconfigurability 
commercial dynamically reconfigurable processors systems array structures pes interconnection architectures classified 
rest organized follows 
section ii introduce parallelism utilization different levels granularity works embedded reconfigurable systems 
section iii modern concept high performance reconfigurable computing concisely described 
limiting factors current embedded systems discussed section iv 
section devoted summary concluding remarks 
stressed primarily presents outline developments techniques reconfigurable computing field intended introductory text detailed descriptions ongoing research projects 
ii 
parallel computing reconfigurable logic embedded reconfigurable computing bound exploit parallelism different levels granularity 
section going take brief look reconfigurable devices roles contributions context 
regarding standard parallel computing terminology reconfigurable hardware generally provide parallelism finer granularity compared distributed systems task process level 
upper level coarse grain parallelism commonly practiced distributed systems layer reconfigurable logic 
introduce different levels parallelism quickly survey embedded reconfigurable systems various granularity parallelism ranging instruction process level 
depicts concept parallelism different levels embedded reconfigurable computing systems 
instruction level parallelism ilp lowest level granularity ilp exploited high performance microprocessors 
instruction level parallelism purest form truly instruction level load stores alu process process task task task task task task task task task task task fig 

different levels parallelism embedded reconfigurable computing systems operations 
instruction level operations visible source code manipulated programmer 
conventional microprocessors ilp exploited micro architecture superscalar processor 
having multiple instructions progress different stages completion superscalar processor able complete instruction clock cycle 
long instruction word vliw processors offer method fine grained parallel operation 
vliw processor contains multiple functional units operating parallel 
key difference instruction stream 
vliw processing elements different instructions executed simd executes instruction processing elements 
parallelism vliw limited scalability register file shared processing elements limited instruction level parallelism source code 
vliw processors started gaining acceptance embedded systems domain 
monolithic register file vliw processors large number functional units practical 
simple solution utilization architectures termed clustered vliw processors 
rc simd type simd architecture reconfigurable communication network 
uses delay line instruction bus causing accesses communication network distributed time 
architecture requires cheap communication network performing expensive fully connected simd architectures 
conflict model employed deal irregular resource conflicts architecture 
runtime reconfiguration refinement simd architecture 
multiprocessor platform high throughput decoding configurable asip combined efficient memory communication interconnect scheme 
asip simd architecture specialized extensible instruction set stages pipeline control 
attached memories communication interfaces enable design efficient multiprocessor architectures 
multiprocessor architectures benefit shuffling technique introduced field reduce communication latency 
major characteristics proposed platform addressed flexibility scalability reusable various operating modes 
adve vector llva virtual instruction set architecture visa exposes extensive static information vector parallelism avoiding hardware specific parameters 
provides arbitrary length vectors fixed length vectors subword simd extensions set operations vector types 
translators compile vector llva written arbitrary length vectors motorola rsvp architecture vector llva written fixed length vectors altivec intel sse implemented 
experiments show visa design captures vector parallelism quite different classes architectures provides virtual object code portability class subword simd architectures 
modern microprocessors order execution mechanism keep multiple execution units busy possible achieve higher performance 
achieved allowing instructions issued completed original program sequence means exposing concurrency sequential instruction stream 
instruction issued cycle independent instructions executed parallel instructions kept waiting circumstances proceed speculatively 
speculative execution order issue superscalar processors expose concurrency sequential binary code 
sun chip multiprocessors way issue vliw pipelines 
architecture provides set predicated instructions support control speculation 
intel explicitly parallel instruction computing epic architecture speculative evolution vliw 
compile superscalar processor compiler simply generates sequential instruction stream processor instruction stream run time 
contrast vliw processor executes instruction word generated compiler requiring compiler schedule concurrent operations compile time 
legacy techniques compiling sequential code reconfigurable processors concerning ilp reviewed 
explicit dataflow graph execution edge architectures aim exploit fine grained concurrency single thread 
break program sequence multi instruction blocks commit atomically 
trips prototype edge architecture compiler assigns instruction numbers determine placement alu substrate 
exploit instruction level parallelism trips microarchitecture implements order execution 
assigning ids instructions trips scheduler statically places instruction array alus hardware dynamically issues instructions operands ready 
differs vliw approach uses static placement static issue order superscalar approach uses dynamic placement dynamic issue 
vliw architectures large hardware resources supporting multiple parallel operations 
performance improvements easily obtained defining complex instruction multiple parallel operations sequence simple instructions typical risc architecture little instruction level parallelism benefit parallel operations combined complex instruction 
risc architectures explicit instruction level parallelism exploit parallelism pipeline stages 
possibilities combine multiple operations different pipeline stages feasible contribute performance improvements processor native instruction set 
lee instruction set synthesis framework optimizes instruction set efficient instruction encoding application data path architecture 
aimed modern risc pipelined architectures multi cycle instruction support representative current configurable processors despite existing methodologies applicable vliw processors 
library new instructions created various encoding alternatives account data path architecture constraints best set instructions selected satisfying instruction bitwidth constraint 
paolucci proposed tiled architectural strategy employs scalable building blocks 
shapes typical dsp oriented tile composed risc vliw dsp distributed network processor tile memories set tile peripherals pot 
tile equipped distributed external memory 
tile evolution atmel multiprocessor soc includes risc plus floating point vliw magic dsp 
shapes routing fabric connects onchip chip tiles weaving distributed packet switching network 
shapes investigate layered system software destroy algorithmic distribution info provided programmer fully aware hardware paradigm 
parameterizable hierarchical instruction scheduling tiled processors explored 
scheduler employed determine contention latency sweet spot generates best instruction schedule application 
avoid application specific tuning parameters produce best performance applications determined 
adaptive explicitly parallel instruction computing stylized form reconfigurable system chip designed enable compiler control reconfigurable resources 
architectural model extends explicitly parallel instruction computing architectures epic design outline instruction level parallelism explicitly communicated compiler hardware 
extends model reconfigurable socs 
support reconfigurable fabric dynamically configured collection adaptable functional units 
configured stream bits called configuration 
specific instructions executed epic processor send configuration stream adaptive fabric 
similar instructions executed manage configurations configuration memory hierarchy 
vliw architecture execution model adopts template large pieces computation running matrix functional units connected set local register spread matrix 
experiments show performance gain comparing epic architecture number registers functional units 
jones proposed super complex instruction set computing embedded processor architecture particularly investigated performance power consumption device compared traditional processor architecture execution 
heterogeneous processor architecture designed exceed performance traditional embedded processors maintaining reduced power budget compared low power embedded processors 
processors tightly integrated shared register file eliminating need overheads associated busses multiprocessor chips 
heart processor vliw containing homogeneous execution cores functional units 
code generation embedded reconfigurable archi efficiently exploit multiple memory banks demonstrate instruction level parallelism 
basically code generation process contain intermediate representation code compaction instruction scheduling memory bank assignment register accumulator assignment 
phases performed various sequences logically independent 
number researches investigated multi bank memory achieve maximum instruction level parallelism 
works focus designing variable partitioning mechanisms try evenly distribute memory accesses explore potential higher memory bandwidth 
heterogeneous register sets zhuang specific register allocation algorithms fit irregularities 
detailed information 
data level parallelism dlp data level parallelism referred loop level parallelism llp usually revealed exploited sequential programs optimizing compilers 
data parallelism small scale basic blocks program 
larger scale data parallelism nest loops perform array vector calculations 
loop parallelization common technique embedded reconfigurable computing primary target exploit parallelism 
generally automatic parallelization loops planned hardware compilers attempt maximize reconfigurable hardware 
loop unrolling done different styles various scales 
compilers select innermost loop level completely unrolled parallel execution hardware potentially creating heavily pipelined structure 
cases outer loops may multiple iterations executing simultaneously 
loop reordering improve parallelism utilization reconfigurable device basically done programmer preprocessing step 
hand compiler systems taken procedure step focus parallelization loops program just inner loops 
type compilers generate analyze different data control flow graph representations entire program source code needed clear data control dependencies iterations schedule parallel operations hardware 
evaluated performance potential different types parallelism executing loops 
applications embedded benchmark suites analyzed intel compiler 
ziegler hall set measurements characterize design space auto matically mapping high level algorithms consisting multiple loop nests fpga 
focused space time tradeoffs associated sharing constrained chip area multiple computations represented asynchronous pipeline 
analyses transformations associated parallelizing compiler technology perform highlevel optimization designs 
amount parallelism individual loop nests controlled goal deriving design effective chip resources 
described heuristics automatically searching space set metrics evaluating comparing designs 
traditional schedulers mapping software implementations compute intensive loops array profitable take account explicit routing operand values 
essence problem binding operations time slots resources extended include explicit routing operands producers consumers 
software pipelining technique referred modulo graph embedding proposed mapping loop bodies reconfigurable architectures 
leverages graph embedding graph theory draw graphs target space 
advantage technique considers communication structure loop body mapping 
dou speculative execution mechanism dynamic loop scheduling goal iteration cycle 
technique exploits data dependences intra iteration inter iteration 
instructions special data reuses case loop carried dependences designed 
experimental results show substantial reduction memory accesses 
task level parallelism tlp task level parallelism usually regarded potential parallelism brought attention applications result considerable performance gains 
particularly important embedded reconfigurable systems systems perform different types computation data streams 
conceptually task level parallelism looks easy exploit tasks allocated processors 
task structure easily exposed ordinary programming languages programmers face difficulties clarify bounds tasks 
programming models may help clarify task structure application 
embedded processes consist multiple sub tasks different source code partly executed parallel 
subtask level parallelism inside single task limited fully utilize parallel processors results slacks processors 
subtasks multiple tasks executed interleaving fashion efficient processors 
design time algorithms proposed interleave subtasks separated schedules tasks 
interleaver considered part hierarchical scheduler steer code generation complex applications tasks 
proposed fpga partition architecture methodology set supporting tools enable partial reconfiguration directions re allocation tasks slot fpga arrangement reconfiguration communication infrastructure tasks external processor 
embedded reconfigurable devices operate autonomously adapt receive new task group tasks optimizing task allocation intra task communications 
type communication structures supported combination buses point point connections networks chip noc variable width sharing fixed set intra task communication channels 
process vs thread level parallelism parallel computing systems major parallel activity process level finer granularity thread level 
process level process separate address space 
order communicate state process send message destination process es explicitly receive message 
different sorts messaging protocols may 
messages may buffered asynchronous mpi 
thread level parallel processing threads share address space communicate shared memory messaging 
signaling synchronization mechanisms processing include critical sections mutual exclusion barriers 
multithreaded processor able pursue threads control parallel processor pipeline 
contexts threads control stored separate chip register sets 
unused instruction slots arise latencies pipelined execution single threaded programs contemporary microprocessor filled instructions threads multithreaded processor 
execution units multiplexed thread contexts loaded register sets 
underutilization superscalar processor due missing ilp overcome simultaneous multithreading processor issue multiple instructions multiple threads cycle 
simultaneous multithreaded processors combine multithreading technique wide issue superscalar processor utilize larger part issue bandwidth issuing instructions different threads 
explicit multithreaded processors multithreaded processors apply processes operating system threads hardware thread slots 
processors optimize throughput multiprogramming workloads single thread performance 
distinguish processors implicit multithreaded processors utilize speculation speculatively executing machine generated threads control part single sequential program 
forms explicit multi threading techniques described interleaved multi threading imt blocked multithreading smt 
simultaneous multithreading smt architectural technique improves resource utilization allowing instructions multiple threads coexist processor share resources afforded systems tight energy budgets 
exploit data level parallel hardware exploit available information threads 
earlier studies shown performance smt architecture begins saturate number coexisting threads increases 
shin shown single fetch policy best solution entire execution time significant performance improvement attained dynamically switching fetch policies 
proposed implementation method includes extremely lightweight thread control fetch policies detector thread processor architecture run detector thread impact user application threads 
evaluated various heuristics detector thread determine best fetch policies 
software smt sw smt technique exploit task level parallelism improve utilization instruction level data level parallel hardware improving performance 
technique performs simultaneous compilation multiple threads design time includes run time selection efficient mixes 
survey multithreading classifications explicit multithreading techniques 
alternative avoids complexity instruction issue elimination speculative execution model 
model fragments sequential code compile time executes fragments order maintaining order execution fragments 
constraints execution fragments dependencies managed distributed scalable manner synchronizing registers 
fragments code called capture instruction level parallelism 
proposed reconfigurable architectural template exploits mixed coarse grained fine grained reconfigurable data path control elements obtain performances asics level computational tasks repetitive execution reduced set operations multidimensional array data 
architectural template determines execution partitioning dominant non dominant kernels processor coprocessor interaction 
system integrability scalability improved memory mapping coprocessor processor communications 
way coprocessor coupled kind existing processor cluster coprocessors parallel implement thread level parallelism 
main innovation lies reconfigurable coprocessor core architecture optimized execution kernels 
alp architecture efficiently integrates different forms including ilp tlp evolutionary changes programming model hardware 
novel part alp data level parallelism technique called simd vectors streams integrated conventional superscalar cmp smt architecture subword simd 
technique lies subword simd vectors providing significant benefits lower cost 
atlas prototype chip multiprocessors hardware support transactional memory tm technology aiming simplify parallel programming 
atlas uses bee multi fpga board provide system powerpc cores running linux 
aimed provide benefits chip multiprocessors research performance improvement software simulator visibility helps software tuning architectural enhancements 
certain issues building fpga framework chip multiprocessors addressed performance challenges mapping asic style chip multiprocessors rtl fpgas software support selection criteria base processor challenges pre designed ip libraries 
currently researchers mit investigating vector thread architectures 
architectures unify data level thread level instruction level parallelism providing new insights parallelization programs difficult vectorize incur excessive synchronization costs multithreaded 
developed scale processor example vector thread architecture designed lowpower high performance embedded systems 
prototype includes single issue bit risc control processor vector thread unit supports virtual processor threads execute instructions cycle kb shared primary cache 
design thread associative memory chitecture multithreaded processor investigated 
considering fact memory contention concurrent threads chip multithreaded processing limiting factor performance improvement proposed memory addresses challenge incorporating thread specific information explicitly onchip memory hardware utilized different levels memory hierarchy 
coarse grained reconfigurable architecture architecture dynamically reconfigurable embedded systems compiler offer high instruction level parallelism applications means sparsely interconnected array functional units register files 
wu proposed extension mt multi threaded exploit thread level parallelism 
mt architectures array partitioned multiple smaller arrays execute threads parallel 
partition changed dynamically extension provides flexibility multi core approach 
anderson unifying programming model specifying application threads running hybrid cpu fpga system 
threads specified single pthreads multithreaded application program compiled run cpu synthesized run fpga 
system abstracts cpu fpga components unified custom threaded multiprocessor architecture platform 
support abstraction cpu fpga component boundary created hardware thread interface hwti component frees designer having specify embed platform specific instructions form customized hardware software interactions 
hardware thread interface supports generalized pthreads api semantics allows passing data types hardware software threads 
hardware thread interface provides platform independent compilation target enables thread instruction level parallelism software hardware boundary 
proposed lightweight subset implementation mpi standard called tmd mpi 
tmd mpi provides programming model capable multiple fpgas embedded processors hiding hardware complexities programmer facilitating development parallel code promoting code portability 
message passing engine tmd mpe developed encapsulate tmd mpi functionality hardware 
tmd mpe enables communication hardware engines embedded processors 
addition chip designed enable intra fpga inter fpga communications 
tmd mpi tmd mpe network provide flexible design flow multiprocessor system chip design 
iii 
high performance reconfigurable computing high performance reconfigurable computing uniting conventional processors gate arrays 
new powerful computing facility basically looks appealing highperformance computing community primarily systems potential exploit coarsegrained process task level parallelism finegrained instruction level parallelism direct hardware execution fpgas dynamically tune architecture fit various applications 
illustrates outline typical highperformance reconfigurable architecture 
high performance reconfigurable computers known reconfigurable supercomputers shown considerable improvements performance power size cost conventional highperformance computers 
traditionally benefits utilized compute intensive integer applications 
doubt benefits attained general scientific applications 
programming straightforward depending programming tool range designing hardware software programming may require partial substantial hardware knowledge 
fortunately trend reconfigurable chip sizes diversity resources may relieve concerns 
hardware reconfigurability feared users learn design hardware employ machines effectively 
src src parallel architectures src computers developed gear scalability 
addition traditional highperformance computing vendors utilized fp gas parallel architectures 
silicon graphics 
sgi introduced sgi reconfigurable application specific computing technology 
building success cray xt system cray xt system brings new levels scalability sustained performance high performance computing 
linux working fpga accelerated system tailored specifically high performance multi paradigm computing 
aims help accelerate adoption emerging computing paradigm offering powerful platform opensource tools 
keeping linux spirit driver user space api verilog interfacing ip open source transparent 
claimed system powerful reconfigurable device available market 
far software development concerned src provides semi integrated solution addresses hardware software sides application sepa conventional processors reconfigurable device computing node 
hardware side expressed carte carte fortran separate function compiled separately linked compiled fortran software side form application 
hardware vendors third party software tool impulse handel rc toolbox 
tools handle fpga side application machine application interface call functions 
handel support sgi impulse rc toolbox support cray xd 
currently library parallel tool mpi handle scaling application node parallel system 
attempt develop system patel proposed architecture scalable computing machine built entirely fpga computing nodes 
machine enables designers implement large scale computing applications heterogeneous combination hardware accelerators embedded microprocessors spread fpgas interconnected flexible communication network 
parallelism multiple levels granularity application exploited obtain maximum computational throughput 
providing simple abstracted communication interface objective able scale thousands fpga nodes proposed architecture appears programmer unified extensible fpga fabric 
programming model mpi support partitioning application independent computing tasks implemented architecture 
iv 
parallelism limitations embedded systems main problem traditional parallel processing requires user handle parallelization synchronization communica computing node fig 

high performance reconfigurable architecture reconfigurable device tion 
essence major problems implementing multiprocessor applications placed user 
embedded reconfigurable architectures replicate custom semi custom hardware components chip 
necessary utilize different parallel processing techniques take advantage multi core resources 
unfortunately problems limit multi core embedded computing systems reaching potential 
parallel processing hard achieve 
writing parallel programs extremely tedious error prone 
automation tools parallel programming coarse grain parallelizing compilers fine grain instruction level parallelism limited success 
shared memory resource vliw symmetric architectures popular particularly embedded dsp architectures 
similar concept vector processing cores processors pentium mmx powerpc altivec 
way applications written inadvertently hide parallelism compiler 
particular instruction level parallelism highly parallel codes ultimately limited compiler find extremely low 
shared memory systems scale large number processor cores ilp increased 
second available sequential programs contain extremely limited parallelism making cores sit idle eliminating benefits 
third parallel execution lead tremendous overheads coherence mechanisms additional code required manage parallel execution 
task level parallelism workload partitioning certainly continue dominant software development issues reconfigurable platforms heterogeneous homogeneous architectures 
issues critical heterogeneous architectures specialized processors may additional constraints 
evolutionary stage embedded application development language refinement 
significant amount done parallelizing functional languages existing easily adapted embedded reconfigurable computing may 
partly addressed developer supervised parallelism exploitation 
openmp mpi evident examples 
embedded computing languages need offer hybrid approach identify task level parallelism originally developed general purpose computations 
put words need achieve high performance challenges understanding pes architecture sophisticated parallel programming techniques 
research platforms aimed achieve goal intends programming processors easy single threaded single core programming full advantage available resources 
summary consider scalability architecture appropriate programming models task control management primary challenges embedded system designs coming years 
summary concluding remarks embedded computing research gaining popularity years 
innovative concepts embedded languages exploit parallelism tuning performance crucial take evolutionary step convenient application development 
embedded reconfigurable systems potential demonstrate computing power overcoming hard solve problems 
concept demanding fact steered direction high performance computing specific application accelerations 
described latest contributions embedded reconfigurable computing concerning parallelism hardware platforms software facilities targeting systems 
summary reconfigurable computing systems lend high performance computing advantage parallelism multiple levels granularity ranging instruction process level parallelism 
considering parallelism utilization inherent current applications note types parallelism statically just examining program 
opportunities dynamically executing program 
static parallelism easier implement cover important sources parallelism maximum parallelism achieved 
dynamic discovery parallelism particular application performed levels abstraction ranging instruction task level 
authors believe major benefit parallel programming paradigm language tailored embedded reconfigurable computing ease identifying extracting opportunities parallelism application result maximal performance gain 
compton hauck reconfigurable computing survey systems software acm comput 
surv vol 
pp 

luk cheung reconfigurable computing architectures design methods iee proceedings computers digital techniques vol 
pp 

weaver paxson gonzalez fpga accelerator network intrusion prevention fpga proceedings acm sigda th international symposium field programmable gate arrays 
new york ny usa acm press pp 

comparative survey high performance cryptographic algorithm implementations fpgas iee proceedings information security vol 
pp 

vijaykrishnan brooks kandemir irwin symmetric encryption reconfigurable custom hardware international journal embedded systems vol 
pp 


jing 
chen 
chen 
chen reconfigurable system high speed diversified aes fpga 
vol 
pp 

de lima da silva mapping image processing systems fpga computer temporal partitioning design space exploration proceedings th annual symposium integrated circuits systems design 
new york ny usa acm press pp 

graham nelson reconfigurable processors high performance embedded digital signal processing fpl proceedings th international workshop field programmable logic applications 
london uk springer verlag pp 

tessier reconfigurable computing digital signal processing survey vlsi signal process 
syst vol 
pp 

run time reconfigurable systems digital signal processing applications survey vlsi signal process 
syst vol 
pp 

delgado level reconfigurable architecture digital signal processing 
eng vol 
pp 

kumar das system chip implementation line face recognition pattern recogn 
lett vol 
pp 

cardoso architectures compilers support reconfigurable computing crossroads vol 
pp 

li hauck configuration compression virtex fpgas fccm proceedings th annual ieee symposium field programmable custom computing machines 
washington dc usa ieee computer society pp 

new approach compress configuration information programmable devices date proceedings conference design automation test europe 
leuven belgium belgium european design automation association pp 

li hauck configuration prefetching techniques partial reconfigurable coprocessor relocation fpga proceedings acm sigda tenth international symposium gate arrays 
new york ny usa acm press pp 

robertson irvine design flow partially reconfigurable hardware trans 
embedded computing sys vol 
pp 

carvalho ao framework design implementation dynamically partially reconfigurable systems proceedings th symposium integrated circuits system design 
new york ny usa acm press pp 

vikram mapping data parallel tasks partially reconfigurable hybrid processor architectures ieee trans 
large scale 
syst vol 
pp 

bner becker exploiting dynamic partial reconfiguration fpgas architecture system integration proceedings th annual symposium integrated circuits systems design 
new york ny usa acm press pp 

hauck agarwal software technologies reconfigurable systems dept ece northwestern univ tech 
rep 
vuillemin bertin shand touati programmable active memories reconfigurable systems come age ieee trans 
large scale 
syst vol 
pp 

mangione smith hutchings andrews de hon ebeling hartenstein morris prasanna seeking solutions configurable computing computer vol 
pp 

dehon wawrzynek reconfigurable computing implications design automation dac proceedings th acm ieee conference design automation 
new york ny usa acm press pp 

gokhale graham reconfigurable computing accelerating computation field programmable gate arrays 
springer 
nelson mythical ccm search usable fpga general computing machines asap proceedings ieee th international conference application specific systems architectures processors asap 
washington dc usa ieee computer society pp 

survey dynamically reconfigurable processors ieice trans commun vol 
pp 

jones hoare mehta foster reducing power increasing performance trans 
embedded computing sys vol 
pp 

balakrishnan kumar impact intercluster communication mechanisms ilp clustered vliw architectures acm trans 
des 
autom 
electron 
syst vol 

fatemi basten rc simd reconfigurable communication simd architecture image processing applications journal embedded computing vol 
pp 

fatemi basten jonker run time reconfiguration communication simd architectures ipdps 
ieee 
muller quel asip multiprocessor soc design simple double binary turbo decoding date proceedings conference design automation test europe 
leuven belgium belgium european design automation association pp 

robert adve vector llva virtual vector instruction set media processing vee proceedings second international conference virtual execution environments 
new york ny usa acm press pp 

callahan wawrzynek instruction level parallelism reconfigurable computing fpl proceedings th international workshop field programmable logic applications fpgas computing paradigm 
london uk springer verlag pp 

chen burger mckinley spatial path scheduling algorithm edge architectures sigarch comput 

news vol 
pp 


lee choi dutt instruction set synthesis efficient instruction encoding configurable processors acm trans 
des 
autom 
electron 
syst vol 

choi 
kim 
yoon 
park hwang 
synthesis application specific instructions embedded dsp software ieee trans 
comput vol 
pp 

arnold designing domain specific processors codes proceedings ninth international symposium hardware software codesign 
new york ny usa acm press pp 

paolucci leupers thiele shapes tiled scalable software hardware architecture platform embedded systems codes isss proceedings th international conference hardware software codesign system synthesis 
new york ny usa acm press pp 

shapes project website shapes bin view 
swanson petersen putnam eggers instruction scheduling tiled dataflow architecture asplos xii proceedings th international conference architectural support programming languages operating systems 
new york ny usa acm press pp 

compiler optimization embedded applications adaptive soc architecture cases proceedings international conference compilers architecture synthesis embedded systems 
new york ny usa acm press pp 

schlansker rau epic explicitly parallel instruction computing computer vol 
pp 

santos azevedo araujo vliw architecture geometry computation asap proceedings ieee th international conference application specific systems architectures processors asap 
washington dc usa ieee computer society pp 

malik simultaneous allocation code generation dual data memory bank asips acm trans 
des 
autom 
electron 
syst vol 
pp 

leupers variable partitioning dual memory bank dsps icassp vol 
pp 

cho paek whalley fast memory bank assignment fixed point digital signal processors acm trans 
des 
autom 
electron 
syst vol 
pp 


retargetable register allocation framework embedded processors sigplan vol 
pp 

zhuang zhang pande hardware managed register allocation embedded processors proceedings acm sigplan conference languages compilers tools embedded systems 
new york ny usa acm press pp 


lee chen effective efficient code generation algorithm uniform loops non orthogonal dsp architecture syst 
softw vol 
pp 

franklin berg ebeling specifying compiling applications rapid fccm proceedings ieee symposium fp gas custom computing machines 
washington dc usa ieee computer society 
luk pipeline vectorization reconfigurable systems fccm proceedings seventh annual ieee symposium field programmable custom computing machines 
washington dc usa ieee computer society 
wang lewis automated compute accelerator design partial evaluation fccm proceedings th ieee symposium fpga custom computing machines 
washington dc usa ieee computer society pp 

budiu goldstein fast compilation pipelined reconfigurable fabrics fpga proceedings acm sigda seventh international symposium field programmable gate arrays 
new york ny usa acm press pp 

nicolau tian saito challenges exploitation loop parallelism embedded applications codes isss proceedings th international conference hardware software codesign system synthesis 
new york ny usa acm press pp 

ziegler hall evaluating heuristics automatically mapping multi loop applications fpgas fpga proceedings acm sigda th international symposium field programmable gate arrays 
new york ny usa acm press pp 

park fan mahlke modulo graph embedding mapping applications coarsegrained reconfigurable architectures cases proceedings international conference compilers architecture synthesis embedded systems 
new york ny usa acm press pp 

dou xu wu implementation coarse grained reconfigurable architecture loop self pipelining lecture notes computer science vol 
pp 

ma hierarchical task scheduler interleaving subtasks heterogeneous multiprocessor platforms asp dac proceedings conference asia south pacific design automation 
new york ny usa acm press pp 

de la torre reconfigurable heterogeneous communications core reallocation dynamic hw task management ieee international symposium circuits systems iscas pp 

message passing interface mpi forum home page www mpi forum org 
shin 
lee 
adaptive dynamic thread scheduling simultaneous multithreaded architectures detector thread parallel distrib 
comput vol 
pp 

raghavan novo software simultaneous multi threading technique exploit task level parallelism improve instruction data level parallelism lecture notes computer science vol 
pp 

survey processors explicit multithreading acm comput 
surv vol 
pp 

instruction level parallelism scalable approach chip multiprocessors computer journal vol 
pp 

reconfigurable coprocessor multimedia application domain vlsi signal process 
syst vol 
pp 


li adve 
chen alp efficient support levels parallelism complex media applications acm trans 

code optim vol 

wee casper ge olukotun practical fpga framework novel cmp research fpga proceedings acm sigda th international symposium field programmable gate arrays 
new york ny usa acm press pp 

scale control processor test chip massachusetts institute technology tech 
rep mit csail tr january 
wang wang thread associative memory multithreaded computing islped proceedings international symposium low power electronics design 
new york ny usa acm press pp 

wu madsen mt multithreading coarse grained reconfigurable architecture lecture notes com puter science vol 
pp 

anderson peck stevens sass andrews enabling uniform programming model software hardware boundary fccm proceedings th annual ieee symposium field programmable custom computing machines fccm 
washington dc usa ieee computer society pp 

parallel programming model multi fpga multiprocessor machine master thesis university toronto 
el buell reconfigurable supercomputing sc proceedings acm ieee conference supercomputing 
new york ny usa acm press 
el bennett underwood pennington buell george reconfigurable supercomputing high performance reconfigurable computing supercomputing paradigm sc proceedings acm ieee conference supercomputing 
new york ny usa acm press 
src computers programmer friendly reconfigurable computing systems www com 
sgi technology www sgi com products 
cray website www cray com 
linux linux supercomputing www com buell el guest editors high performance reconfigurable computing computer vol 
pp 

patel chow scalable fpga multiprocessor fccm vol 
pp 

openmp official website www openmp org 
platform www net 

