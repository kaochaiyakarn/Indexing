markov games framework multi agent reinforcement learning michael littman brown university bellcore department computer science brown university providence ri cs brown edu definitions markov decision process mdp mdp howard defined set states tion reinforcement learning single adaptive actions transition function sa 
agent interacts environment defined pd defines effects various actions probabilistic transition function 
state environment 
pd represents set tic view secondary agents part discrete probability distributions sets 
reward environment fixed function sa specifies agent task 
havior 
framework markov games allows broad terms agent objective find policy widen view include multiple adap mapping interaction history current choice action tive agents interacting competing goals 
maximize expected sum discounted reward considers step direction efp jg jis reward exactly agents diametrically op 
discount factor controls posed goals share environment 
describes effect rewards optimal decisions learning algorithm finding optimal small values emphasizing near term gain policies demonstrates application sim larger values giving significant weight rewards 
ple player game optimal policy probabilistic 
general form markov game called stochastic game owen defined set states collection action sets ak agent environment 
state transitions controlled current state action agent agent agent lives vacuum interact agents associated reward function ri sa ak achieve goals 
reinforcement learning attempts maximize expected sum ing technique creating agents exist tan discounted rewards efp jri jg jis yanco stein mathematical frame reward 
justifies inappropriate multi agent environments 
theory markov decision processes consider studied specialization mdp barto howard agents diametrically lies reinforcement learning opposed goals 
allows single reward func assumes agent environment stationary tion agent tries maximize called contains adaptive agents 
opponent tries minimize 
denote agent action set denote opponent theory games von neumann morgenstern action set andr denote immediate reward explicitly designed reasoning multi agent systems 
markov games see van agent actiona ain states der wal opponent takes 
extension game theory mdp environments 
considers consequences markov adopting specialization call player game framework place mdp reinforcement learn zero sum markov game simplifies mathematics ing 
specific case player zero sum games impossible consider important phenomena addressed restricted version cooperation 
step consid insights applied open questions field ered strict generalization mdp reinforcement learning 
matrix games 
sa ak pd 
mdp discount factor thought probability game allowed continue current move 
possible define notion undiscounted rewards schwartz markov games optimal strategies undiscounted case owen 
games best postpone risky actions indefinitely 
current purposes discount factor desirable effect players trying win sooner 
optimal policies previous section defined agent objective maximizing expected sum discounted reward 
subtleties applying definition markov games 
consider parallel scenario mdp mdp optimal policy maximizes expected sum discounted reward meaning state policy achieve better expected sum discounted reward 
mdp optimal policy optimal policies mdp stationary deterministic 
means mdp policy optimal 
policy called stationary change function time called deterministic action chosen agent states alls 
markov games policy performance depends critically choice opponent 
game theory literature resolution dilemma eliminate choice evaluate policy respect opponent look worst 
performance measure prefers conservative strategies force opponent draw ones accrue great deal reward opponents lose great deal 
essence minimax behave maximize reward worst case 
definition optimality markov games important properties 
mdp markov game non empty set optimal policies stationary 
mdp need deterministic optimal policy 
optimal stationary policy probabilistic mapping states discrete probability distributions actions pd 
classic example rock scissors deterministic policy consistently defeated 
idea optimal policies stochastic may strange readers familiar mdp games alternating turns backgammon tic tac toe frameworks deterministic policy worse best probabilistic 
need probabilistic action choice stems agent uncertainty opponent current move requirement avoid second guessed agent rock scissors rock opponent scissors table matrix game rock scissors vs rock rock vs rock vs scissors rock scissors table linear constraints matrix game 
finding optimal policies section reviews methods finding optimal policies matrix games mdp markov games 
uses uniform notation intended emphasize similarities frameworks 
avoid confusion function names appear appear different numbers arguments time 
matrix games core theory games matrix game defined matrix instantaneous rewards 
reward agent choosing opponent chooses 
agent strives maximize expected reward opponent tries minimize 
table gives matrix game corresponding rock scissors agent policy probability distribution actions pd 
rock scissors components rock scissors 
notion optimality discussed aro aa earlier optimal agent minimum expected reward large possible 
find policy achieves 
imagine satisfied policy guaranteed expected score matter action opponent chooses 
inequalities table constrain components represent exactly policies solution inequalities suffice 
optimal identify value constraints hold 
linear programming see strang general technique solving problems kind 
example linear programming finds value abbreviate linear program max pd min expected reward agent policy opponent 
mdp host methods solving mdp section describes general method known value iteration bertsekas 
value state total expected discounted reward attained optimal policy starting state 
states large smart agent collect great deal reward starting states 
quality state action pair total expected discounted reward attained nonstationary policy takes actiona states follows optimal policy 
functions satisfy recursive relationship xs st max aq says quality state action pair immediate reward plus discounted value succeeding states weighted likelihood 
value state quality best action state 
follows specify optimal policy state choose action highest value 
method value iteration starts estimates generates new estimates treating equal signs equations assignment operators 
shown estimated values true values bertsekas 
markov games agent maximize reward greedy strategy choosing action value 
strategy greedy treats surrogate immediate reward acts maximize immediate gain 
optimal function accurate summary rewards 
similar observation markov games expected reward optimal policy starting states andq expected reward opponent xs aq continuing optimally 
treat values immediate payoffs unrelated sequence matrix games state solved optimally techniques section 
value states sin markov game max pd min quality resulting recursive equations look equations analogous value iteration algorithm shown converge correct values owen 
worth noting games alternating turns value function need computed linear programming optimal deterministic policy 
case 
learning optimal policies traditionally solving mdp value iteration involves applying equations simultaneously alls 
watkins watkins proposed alternative approach involves performing updates asynchronously transition function learning formulation update performed agent receives reward making transition actiona 
update takes place equation 
probability happens possible agent carry appropriate update explicitly learning rule converges correct values assuming action tried state infinitely new estimates blended previous ones slow exponentially weighted average watkins dayan 
straightforward seemingly novel apply technique solving markov games 
completely specified version algorithm 
variables warrant explanation algorithm part environment internal algorithm parameters algorithm 
variables environment state set action set opponent action set discount factor gamma 
variables internal learner learning rate alpha initialized decays time agent estimate function agent estimate function agent current policy states pi 
remaining variables parameters algorithm controls agent deviate current policy ensure state space adequately explored controls rate learning rate decays 
algorithm called minimax essentially identical standard learning algorithm minimax replacing max 
experiments section demonstrates minimax learning algorithm simple player zero sum markov game modeled game soccer 
initialize ando ino letv choose action return action uniformly random 
current state iss return 
learn receiving moving opponent alpha alpha rew gamma linear programming 
pi 
pi ggg letv pi gg alpha decay minimax algorithm 
initial board left situation requiring probabilistic choice right 
soccer game played grid depicted 
players occupy distinct squares grid choose actions turn stand 
players selected actions moves executed random order 
circle figures represents ball player ball steps appropriate goal left right player scores point board reset configuration shown left half 
possession ball goes player random 
player executes action take square occupied player possession ball goes stationary player move take place 
defensive maneuver stand player wants go 
goals worth point discount factor set scoring sooner somewhat better scoring 
agent offensive better breaking unknown defender agent probabilistic policy 
instance example situation shown right half deterministic choice blocked indefinitely clever opponent 
choosing randomly stand agent guarantee opening score 
training testing different policies learned minimax algorithm learning 
learning algorithm learner trained random opponent learner identical design 
resulting policies named mm qr qq minimax trained random minimax trained minimax trained random trained minimax algorithm mm described learning took place steps 
value chosen learning rate reached run 
learning algorithm qr qq identical max operator place minimax table keep information opponent action 
parameters set identically minimax case 
log qr opponent training fixed policy chose actions uniformly random 
mm qq opponent learner identical tables 
resulting policies evaluated ways 
policy run head head random policy steps 
emulate discount factor step probability declared draw 
wins losses random opponent tabulated 
second test head head competition hand built policy 
policy deterministic simple rules scoring blocking 
steps completed games random opponent won 
third test learning train challenger opponent mm qr qq 
training procedure followed qr champion policy held fixed challenger trained 
resulting policies evaluated respective champions 
test repeated times ensure stability reported 
evaluations repeated times averaged 
results table summarizes results 
columns marked games list number completed games steps columns marked won list percentage won associated policy 
percentages close indicate contest nearly draw 
policies quite tested random opponent 
qr policy performance quite remarkable completed games policies won nearly 
expected qr trained specifically beat opponent trained competition random policy chooses actions idealized opponent mind 
hand built policy mm roughly breaking 
mm policy marginally better 
limit case agent trained minimax algorithm insensitive opponent trained behave maximize score worst case 
fact difference suggests algorithm converged optimal policy 
prior convergence opponent big difference behavior minimax agent playing strong opponent means training take place important parts state space 
performance qq qr policies hand built policy strikingly different 
points important consequence minimax criterion 
close look policies indicated qq luck implemented defense perfect policy 
qr policy hand happened converge strategy appropriate 
slightly different opponent tables turned 
fact qq policy random hand built opponents especially compared mm qr qq won games won games won games won games vs random vs hand built vs challenger vs mm challenger vs qr challenger vs qq challenger table results policies trained minimax mm learning qr qq 
minimax policies somewhat surprising 
simultaneously training adaptive agents learning mathematically justified practice prone locking reaching mutual local maximum agents learning prematurely see boyan 
spite researchers reported amazing success approach tesauro boyan successful instance 
third experiment intended measure worst case performance policies 
learned policies held fixed challenger trained beat 
precisely scenario minimax policies designed fair game break strongest challenger 
policy quite achieve level performance indicating steps random opponent insufficient convergence optimal strategy 
mm policy slightly better winning challenger barely 
algorithms trained learning significantly worse incapable scoring 
due great part fact learning designed find deterministic policies deterministic game perfect defense rock scissors 
correctly extending learning find optimal probabilistic policies exactly minimax designed 
discussion explores markov game formalism mathematical framework reasoning multi agent environments 
particular describes reinforcement learning approach solving player zero sum games max operator update step standard learning algorithm replaced minimax operator evaluated solving linear program 
linear programming innermost loop learning algorithm somewhat problematic computational complexity step large typically steps needed system reaches convergence 
possible approximate solutions linear programs suffice 
iterative methods quite promising relevant linear programs change slowly time 
applications reinforcement learning zero sum games impediment 
games checkers samuel tic tac toe boyan backgammon tesauro go schraudolph consist series alternating moves games minimax operator implemented extremely efficiently 
strength minimax criterion allows agent converge fixed strategy guaranteed safe possible worst possible opponent 
argued unnecessary agent allowed adapt continually opponent 
certainly true extent agent principle vulnerable form opponent leads agent learn poor policy exploits 
identifying opponent type learning agent described interesting topic research 
minimax criterion probabilistic policies closely connected current research 
minimax criterion single agent environments produce risk averse behavior 
random transitions environment play role opponent 
secondly probabilistic policies context acting optimally environments agent perception incomplete singh 
environments random actions combat agent uncertainty true state environment random actions games help deal agent uncertainty opponent move 
player markov games fairly restricted class multi agent environments independent interest include markov decision processes special case 
applying insights theory cooperative multi player games prove fruitful finding useful connections may challenging 
acknowledgments david ackley justin boyan tony cassandra leslie kaelbling ideas suggestions 
barto barto sutton watkins 
learning sequential decision making 
technical report department computer information science university massachusetts amherst massachusetts 
published learning computational neuroscience foundations adaptive networks michael gabriel john moore editors 
mit press cambridge massachusetts 
bertsekas bertsekas 
dynamic programming deterministic stochastic models 
prentice hall 
boyan boyan justin 
modular neural networks learning context dependent game strategies 
master thesis department engineering computer laboratory university cambridge cambridge england 
matthias 
consideration risk reinforcement learning 
proceedings machine learning conference 
appear 
howard howard ronald 
dynamic programming markov processes 
mit press cambridge massachusetts 
owen owen guillermo 
game theory second edition 
academic press orlando florida 
samuel samuel 
studies machine learning game checkers 
ibm journal research development 
reprinted feigenbaum feldman editors computers thought mcgraw hill new york 
schraudolph schraudolph nicol dayan peter sejnowski terrence 
td lambda algorithm learn evaluation function game go 
advances neural information processing systems san mateo ca 
morgan kaufman 
appear 
schwartz schwartz anton 
reinforcement learning method maximizing undiscounted rewards 
proceedings tenth international conference machine learning amherst massachusetts 
morgan kaufmann 

singh singh satinder pal jaakkola tommi jordan michael 
model free reinforcement learning non markovian decision problems 
proceedings machine learning conference 
appear 
strang strang gilbert 
linear algebra applications second edition 
academic press orlando florida 
tan tan 
multi agent reinforcement learning independent vs cooperative agents 
proceedings tenth international conference machine learning amherst massachusetts 
morgan kaufmann 
tesauro tesauro 
practical issues temporal difference 
moody lippman hanson editors advances neural information processing systems san mateo ca 
morgan kaufman 

van der wal van der wal 
stochastic dynamic programming 
mathematical centre tracts 
morgan kaufmann amsterdam 
von neumann morgenstern von neumann morgenstern 
theory games economic behavior 
princeton university press princeton new jersey 
watkins dayan watkins dayan 
learning 
machine learning 
watkins watkins 
learning delayed rewards 
ph dissertation cambridge university 
yanco stein yanco holly stein lynn andrea 
adaptive communication protocol cooperating mobile robots 
meyer jean roitblat wilson stewart editors animals animats proceedings second international conference adaptive behavior 
mit press bradford books 

