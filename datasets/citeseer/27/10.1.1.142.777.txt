streaming data algorithms high quality clustering callaghan lambda nina mishra adam meyerson sudipto guha rajeev motwani october data gathering grows easier researchers discover new ways interpret data algorithms essential fields 
data stream computation precludes algorithms require random access large memory 
consider problem clustering data streams important analysis variety sources data streams routing data telephone records web documents 
provide new clustering algorithms theoretical guarantees performance 
give empirical evidence superiority commonly means algorithm 
adapt algorithm able operate data streams experimentally demonstrate superior performance context 
applications concept data stream appropriate data set 
nature stored data set appropriate model significant portions data queried updates small relatively infrequent 
contrast data stream appropriate model large volume data arriving continuously unnecessary impractical store data form memory 
data streams appropriate model access large data sets stored secondary memory performance requirements necessitate access linear scans 
data stream model data points accessed order arrive 
random access data allowed memory assumed small relative number points limited amount information stored 
general algorithms operating streams restricted fairly simple calculations time space constraints 
challenge facing algorithm designers perform meaningful computation restrictions 
applications naturally generate data streams opposed simple data sets 
astronomers telecommunications companies banks stock market analysts news organizations example lambda contact author mail loc cs stanford edu authors mails hpl hp com cs stanford edu sudipto research att com rajeev cs stanford edu 
vast amounts data arriving continuously 
telecommunications example call records generated continuously 
typically processing done examining call record records archived examined 
example cortes report working long distance call records consisting records day customers 
applications traditional non streaming data treated stream due performance constraints 
researchers mining medical marketing data example volume data stored disk large possible pass small number passes data 
research data stream computation includes sampling finding quantiles stream points calculating difference streams :10.1.1.6.6513
common form data analysis applications involves clustering partitioning data set subsets clusters members cluster similar members distinct clusters dissimilar 
typically cluster characterized canonical element representative called cluster center 
goal determine cluster centers compute clustered partition data set 
concerned challenging problem clustering data arriving form stream 
provide new clustering algorithm theoretical guarantees performance 
give empirical evidence superiority commonly means algorithm 
adapt algorithm able operate data streams experimentally demonstrate superior performance context 
follows describe clustering problem greater detail give high level overview results organization followed discussion earlier related 
clustering problem different variants clustering problem literature field spans large variety application areas including data mining data compression pattern recognition machine learning :10.1.1.152.7115
focus version problem integer collection points metric space find medians cluster centers metric space point assigned cluster defined median point nearest 
quality clustering measured sum squared distances ssq data points assigned medians 
goal find set medians minimize ssq measure 
generalized optimization problem distance metric substitutes squared euclidean distance known median problem 
variants clustering problem may involve centers may employ measure ssq may consider special cases metric space dimensional euclidean space 
variants known np hard goal devise algorithms produce solutions near optimal solutions near linear running time 
finding optimal solution median problem known np hard useful heuristics including means proposed 
means algorithm enjoyed considerable practical success solution produces guaranteed local optimum :10.1.1.44.5872
hand algorithms literature approximation algorithms squared euclidean distance metric obeys relaxed triangle inequality behaves metric 
proposed median 
approximation algorithm guaranteed find solution ssq factor optimal ssq worst case input 
jain vazirani charikar guha provided approximation algorithms constants :10.1.1.129.4996
algorithms typically run time space omega require random access data 
space requirements need random access render algorithms inapplicable data streams 
heuristics including means infeasible data streams require random access 
result heuristics proposed scaling clustering algorithms example :10.1.1.157.392
database literature birch system commonly considered provide competitive heuristic problem :10.1.1.152.7115
heuristics tested real synthetic datasets guarantees ssq performance 
overview results fast median algorithm called localsearch uses local search techniques give theoretical justification success experimental results showing performs means 
convert algorithm streaming algorithm technique :10.1.1.32.1927
conversion decrease asymptotic running time drastically cut memory usage remove random access requirement making algorithm suitable streams 
experimental results showing stream algorithm performs popular heuristics 
rest organized follows 
section formally defining stream model median problem 
solution median obtained variant called facility location specify advance number clusters desired evaluates algorithm performance combination ssq number centers 
knowledge time approach attack median problem 
discussion streaming section 
streaming algorithm section shown enjoy theoretical quality guarantees 
described empirical results reveal synthetic real streams theoretically sound algorithm achieves dramatically better clustering quality ssq birch takes longer run 
section describes new critical conceptualizations characterize instances facility location solve obtain median solution subset feasible facilities sufficient obtain approximate solution 
localsearch detailed section 
performed extensive series experiments comparing localsearch means algorithm numerous low high dimensional data 
results section uncover interesting trade cluster quality running time 
ssq means worse localsearch localsearch typically optimum solution 
algorithms randomized ran times dataset course multiple runs large variance performance means localsearch consistently 
localsearch took longer run trial datasets near optimal answer means equally solution 
datasets course means solution 
view means birch algorithms quick dirty job obtaining solution 
finding higher quality solution takes time expect algorithms require running time appear find exceptionally solutions 
related means algorithm birch relevant results :10.1.1.152.7115
discuss detail section 
previous clustering offer scalability required fast streaming algorithm directly optimize ssq 
briefly review results thorough treatment han book 
partitioning methods subdivide dataset groups 
example medoids algorithm selects initial centers repeatedly chooses data point randomly replaces existing center improvement ssq 
medoids related cg algorithm section cg solves facility location variant desirable practice know exact number clusters facility location allows input range number centers 
choosing new medoid remaining points time consuming address problem clara sampling reduce number feasible centers 
technique similar propose theorem 
distinguishing feature approach careful understanding sample size affects clustering quality 
clarans draws fresh sample feasible centers calculation ssq improvement :10.1.1.13.4395
medoid types approaches including pam clara clarans known scalable appropriate streaming context 
examples partitioning methods include bradley subsequent improvement repeatedly takes weighted centers initially chosen randomly weight data fit main memory computes clustering :10.1.1.157.392
new centers obtained weighted number points assigned data memory discarded process repeats remaining data 
key difference approach algorithm places higher significance points data set assume data stream sorted way 
furthermore approaches known outperform popular birch algorithm 
hierarchical methods decompose dataset tree structure 
hac common treats point cluster repeatedly merges closest ones 
number clusters known merging stops separate clusters remain 
hierarchical algorithms known suffer problem hierarchical merge split operations irrevocable 
hierarchical technique cure represents cluster multiple points initially scattered cluster shrunk cluster center certain fraction 
hac cure designed discover clusters arbitrary shape necessarily optimize ssq 
density methods define clusters dense regions space separated dense regions 
methods typically continue grow clusters long density exceeds specified threshold 
algorithms dbscan optics explicitly designed minimize ssq :10.1.1.121.9220
grid methods typically quantize space fixed number cells form structure 
clustering performed structure 
examples algorithms include sting clique wave cluster :10.1.1.106.7154
cluster boundaries methods axis aligned hyperplanes diagonal separations allowed grid approaches designed optimize ssq 
preliminaries defining stream formally 
data stream finite set points xi xn read increasing order indices example points vectors random access data allowed 
stream algorithm allowed remember small amount information data seen far 
performance stream algorithm measured amount information retains data streamed usual measures case clustering algorithm example ssq running time 
follows give detailed formal definition median problem related facility location problem 
suppose set objects 
notion similarity objects goal group objects exactly clusters similar objects belong cluster dissimilar objects different clusters 
assume objects represented points metric space distance function conveys similarity 
median problem may formally defined follows definition median problem set data points metric space distance function theta number choice fc ae cluster centers define partition clusters nk ni contains points closer ci center 
goal select minimize sum squared distance ssq measure ssq kx ni ci assuming points drawn metric space means distance function reflexive iff symmetric satisfies triangle inequality 
property natural similarity measure 
second property valid commonly similarity measures euclidean distance norm norm measures holds approximate sense 
approximate triangle inequality sufficient purposes enable prove bounds performance algorithms possibly increase constant factors 
clear experiments actual performance algorithms close optimal provable constants large 
domains real space euclidean metric relax restriction subset allow medians chosen larger set 
turn closely related problem called facility location 
facility location problem lagrangian relaxation median 
data points grouped clusters restricting number clusters simply impose cost cluster 
fact cost associated cluster center called facility cost viewed facility cost 
facility cost prevents solution having large number clusters actual number clusters depend relationship facility cost distances data points 
problem may formally stated follows facility cost definition facility location problem set data points metric space distance function theta parameter choice fc ae cluster centers define partition clusters nk ni contains points closer ci center 
goal select value set centers minimize facility clustering fc cost function kx ni delta ci problem known np hard theoretical approximation algorithms known :10.1.1.129.4996
precise statements relationship facility location median generally characterize examples 
assume particular point set wish solve facility location 
opening facility free solution cluster center facility point optimal 
costly open facility optimal solution afford single cluster 
facility cost increases number centers tends decrease 
clustering streaming data algorithm clustering streaming data uses subroutine moment call localsearch 
explanation motivation subroutine somewhat involved addressed shortly 
streaming algorithm prior clustering research largely focused static datasets 
consider clustering continuously arriving data streams 
stream model restricts memory small relative possibly large data size algorithms means require random access suitable 
assume data arrives chunks xn fits main memory 
turn stream chunked stream simply waiting points arrive 
streaming algorithm follows 
chunk determine chunk consists set fewer points repeated 
re represent chunk weighted data set distinct point appears weight equal original frequency chunk original points weights new weight total weight point chunk 
cluster gamma localsearch 
purge memory retaining weighted cluster centers localsearch xi weight cluster center sum weights members center clustering 
apply localsearch weighted centers retained xi obtain set weighted centers entire stream delta delta delta xi 
algorithm stream chunk xi stream 
sample size ffl log contains fewer distinct points ffl xi weighted representation 
cluster xi localsearch 
ik centers obtained chunks iterations stream center obtained clustering xi weighted number points xi assigned 
output centers obtained clustering localsearch algorithm stream memory efficient ith point stream retains ik points 
long streams retaining ik points may get prohibitively large 
case cluster weighted ik centers retain just centers 
real streams may include chunks consisting points repeated 
situation clustering algorithms may speed considerably run compact weighted representation making multiple passes highly redundant data chunk repeatedly processing point wasteful 
solution step represent chunk weighted dataset weight corresponds number times point appears 
creating weighted data set requires time nq number distinct values wish create weighted dataset data redundant small example distinct values omega time needed create weighted dataset 
simple claim shows small sample determine distinct values weight ffl 
claim dataset contains distinct values weight ffl high probability sample size ffl log contains distinct values 
proof pl distinct values weight ffl 
probability sample missed distinct values say pk pki gamma pi gamma ffl ffi choice high probability weighted dataset created step algorithm stream distinct values 
introduce dataset experiments section points network intrusions dataset represent normal computer system rest represent instances malicious behavior 
chunks stream composed entirely normal points distinct vectors 
step affects speed clustering quality steps accomplish actual clustering provably previously shown stream produces solution cost constant times cost get applying localsearch directly entire stream supposing fit main memory 
theorem iteration approximation algorithm run steps algorithm stream centers output approximation optimum ssq delta delta delta xi assuming distance metric rd theorem intended show algorithm stream output arbitrarily bad solution :10.1.1.32.1927
practice expect solution stream finds significantly better times worse optimum 
synthetic stream experimental results consistent expectation 
localsearch algorithm stream needs simple fast constant factor approximation median subroutine 
believe algorithm guarantees flexibility describe new algorithm median concept local search start initial solution refine making local improvements 
section refer cost set medians meaning cost facility location definition 
previous facility location assume fast algorithm computes approximation facility location 
describing simple algorithm cg solving facility location problem set points metric space metric relaxed metric delta delta facility cost definition 
assume feasible solution facility location delta delta set currently open facilities assignment point necessarily closest open facility 
define gain amount cost save expend open facility exist perform possible advantageous facility subject constraints points reassigned second facility closed members reassigned members course closed 
net savings improvements 
pay open open 
certainly reassign point current assignment distance greater total savings say euclidean assumption algorithm stream approximation 
shown charikar guha algorithm achieve approximation facility location facility cost close facilities members closure centers result savings uz 
find open facilities closing reassigning members produce net savings 
case merely performing reassignment closure certainly produce expenditure savings members reassigned earlier 
reassignment cost lower operation produce savings 
close centers savings vz added assignment cost call 
gain net savings steps gamma uz vz gamma gamma omitted open 
algorithm cg data set facility cost 
run algorithm compute set facilities gives approximation facility location facility cost 
repeat omega log times ffl randomly order ffl random order calculate gain gain add facility perform allowed closures 
new algorithm algorithm directly solve median subroutine median algorithm follows 
set initial range facility cost easy calculate upper bound perform binary search range find value gives desired number facilities value try call algorithm cg get solution 
binary search questions spring mind binary search technique second algorithm uses algorithm cg subroutine find solutions quickly 
examine binary search idea trying understand facility cost affects character optimal facility location solution 
consider dataset points distance function delta delta wish solve facility location 
show facility cost increases number centers optimal solution increase ssq decrease total solution cost increases 
opening new facility expensive forced close centers give solutions higher assignment distances cost 
proof justify performing binary search theorem assume set points metric theta 
facility costs set centers optimal set centers optimal true 

ssq ssq 
ssq ssq furthermore ssq ssq iff 
proof ssq ssq 
note 
facility cost facility cost 
optimality optimality ii 
gamma gamma 

see 
combining fact 

know combine ii show 

note combine ii show 
iff 
instance facility location may exist facility cost optimal solution exactly medians 
dataset naturally algorithm find centers 
allowing centers gamma gives significant decrease assignment cost optimal facility location solution centers 
theorem defines term significant decrease theorem dataset ai denote best assignment cost achievable instance allow medians 
property ak ajk gamma facility location solution centers 
proof fl solution yield centers fl solution centers centers worse total cost fl solution centers aj ak ak 
words aj gamma ak gamma ak gamma gamma ak gamma gamma ak 
order facility cost exist need ak aj gamma ak gamma result follows assumption statement theorem 
example realize improvement going gamma centers form ak ak gamma facility location algorithm find centers 
question answer method calling cg subroutine binary search gives solutions median timely fashion 
experimented extensively algorithm gives excellent ssq achieves median solutions close optimal 
practice prohibitively slow 
analyze running time find gain calculation takes theta time log iterations requires gain computations running time particular value facility cost theta log 
solve median perform binary search facility cost find cost gives desired number clusters theta log time operation step binary search 
may non linear relationship facility cost consider reassigning point closing facilities points reassigned number clusters solution get binary search require iterations median algorithm may slow 
furthermore implementations keep theta distance matrix memory speed algorithm constant factor making running time memory usage omega probably large considering data stream model computation 
describe new local search algorithm relies correctness algorithm avoids super quadratic running time advantage structure local search certain ways 
finding initial solution observe cg begins quickly calculated initial solution guaranteed approximation 
better initial solution require fewer iterations step 
particular iteration expect cost decrease constant fraction way best achievable cost initial solution constant factor approximation approximation reduce number iterations theta log theta 
initial solution algorithm takes input dataset points facility cost algorithm data set facility cost 
reorder data points randomly 
create cluster center point 
point ffl distance current data point nearest existing cluster center ffl probability create new cluster center current data point add current point best current cluster algorithm runs time 
shown algorithm obtains expected approximation optimum 
sampling obtain feasible centers theorem motivate new way looking local search 
assume points ck constitute optimum choice medians dataset ci set points assigned ci ri average distance point ci ci assume jn ffi constant set log points drawn independently uniformly random theorem regarding solutions median restricted medians merely large theorem high probability optimum median solution medians constrained cost times cost optimum unconstrained median solution medians arbitrary points 
proof sufficiently large sample show ci obtain log points sufficiently close optimum center ci 
sample size log probability obtain fewer mp points cluster ffi chernoff bounds 
union bound probability obtain fewer mp points cluster ffi 
high probability obtain mp points cluster ci chance points far ci small 
particular probability point distance ri optimum center ci mp log ffi ffi markov inequality 
probability cluster ci 
ck point distance ri ffi union bound 
combining probability fail obtain mp points cluster fail find sufficiently close center ffi 
cluster ci sample contains point xi ri ci cost median set fx times cost optimal median solution triangle inequality assignment distance triple 
sense assuming smallest cluster small 
example smallest cluster contains just point jn clearly point overlooked feasible center 
view small subset points outliers clusters 
assume outliers removed 
theorem leads new local search paradigm searching entire space possible median set choices search restricted space solutions 
evaluating gain point evaluate gain randomly chosen set theta log points choose points evaluation points cluster centers relative optimal centers finish computation sooner 
fact select set theta log points front feasible centers compute gain points addition advantage solutions nearby values ought different 
finding facility cost faster third recall successive set gain operations tends bring closer best achievable cost constant fraction 
total gain achieved set gain computations associated additions deletions centers smaller previous total gain constant fraction absolute sense 
assume current value cost changing little current solution far centers abort change value knowing best possible solution centers 
trying improve solution get solution centers find solution cost improving small amount new set gain operations 
current number centers far definition small improvement probably broad exactly centers 
case simply observing current value probably quite far eventually want arrive narrowing interval possible values search 
second case happy solution number centers change value want get solution possible wasting iterations improving insignificant fraction 
give facility location subroutine median algorithm call take parameter ffl controls soon stops trying improve solution 
parameters data set size metric relaxed metric delta delta facility cost initial solution set facilities assignment function 
algorithm fl delta delta ffl 
current solution 
cost current solution consider feasible centers random order feasible center gain perform advantageous closures gain description obtain new solution assign point closest center 
cost new solution gamma ffl return step give median algorithm 
algorithm localsearch delta delta ffl ffl ffl 
read data points 

set zmin 
set zmax px arbitrary point 
set zmax zmin 
obtain initial solution algorithm 

select theta log random points serve feasible centers 
fewer centers zmin 
gamma ffl zmax ffl current solution ffl run ffl obtain new solution ffl jf run ffl obtain new solution reset new solution ffl jf set zmin zmax zmin jf set zmax zmax zmin 
simulate continuous space move cluster center center mass cluster 
return solution third step loop jf means gamma jf jf initial value zmax chosen trivial upper bound value trying find 
running time localsearch nm nk log number facilities opened 
depends properties dataset usually small running time significant improvement previous algorithms 
facility cost sum assignment costs open particular facility solution opens facility cost 
may equally cheap solution opens exactly facilities zero assignment cost point located exactly facility may cheaper solutions open facility different facility sum assignment costs exist optimal solution facilities 
value upper bound median trivial 
experiments empirical evaluation localsearch results experiments comparing performance means localsearch 
describe precise implementation means experiments 
compare behavior means localsearch variety synthetic datasets 
algorithm means data set integer 
pick points ck 
assign point closest ci 
say point member cluster assigned cj 

repeat point changes membership maximum number iterations ffl recalculate ci euclidean mean points assigned ffl reassign point closest updated ci 
initial centers usually chosen randomly values random points centers chosen random points value ith coordinate jth center chosen uniformly random interval mi mi mi mi respectively minimum resp 
maximum value ith coordinate point second initialization method center generally value point chosen uniformly random methods widely practice 
may better datasets points tight cluster significant number points populous clusters parts space 
centers chosen data points algorithm pick center tight cluster settle local optimum centers represent tight cluster leaving rest clusters grouped incorrectly 
second method may better datasets spread span region sparse 
experiments initialization method 
conducted experiments sun ultra mhz processors mb ram gb swap space running sunos 
experiments low dimensional datasets generated small datasets ran localsearch means 
put results algorithms perspective relative earlier ran algorithms dataset distributed authors birch consists dimensional gaussians grid overlap processes processor went swap :10.1.1.152.7115
datasets described section clusters approximately volume number points 
noted theorem clusters need regular localsearch able find 
section report experiments synthetic dataset clusters varying density volume number points accordance theorem localsearch performs experiments 
datasets generated consist uniform density radius spheres points percent random noise 
noise uniform smallest dimensional rectangle aligned axes contains spheres 
dataset calculated ssq sphere centers dataset value upper bound optimal ssq set medians dataset 
dataset relatively little noise case gaussian dataset vast majority points relatively tight spheres sphere centers close optimal centers 
cases localsearch means better solutions calculated optimal 
dataset recorded best known ssq dataset simply minimum best ssq experimentally pre calculated upper bound 
generated types datasets grid datasets shifted center datasets random center datasets 
grid datasets spheres centered regular nearly regular intervals spheres separated dimension 
grid dataset shifted grid dataset centers spheres grid dataset shifted dimension direction shift positive negative chosen uniformly random 
spheres sets regular intervals slight skew due random shifting 
random center datasets centers chosen uniformly random range 
cases spheres overlap somewhat cases separated 
datasets symmetrical shifted grid sets 
datasets low dimension cases clusters sixteen points 
dataset gaussians points clusters dimensional 
dataset ran means times allow different random initializations 
median recalculation step part step preceding description means medians members assigned values chosen uniformly random initialization 
ran localsearch times dataset 
dataset spheres asked algorithm find exactly clusters 
execution measured running time ssq medians grid gaussians average ssq means solutions standard deviation 
localsearch ssq standard deviation 
means ran average standard deviation localsearch ran average standard deviation datasets average ssq algorithm normalized division best known ssq set 
shows normalized ssq shifted center random center datasets error bars represent standard deviations algorithm want space omit results grid datasets similar corresponding shifted center datasets 
general performance means degraded datasets got progressively symmetric performance random center sets worse shifted center worse grid datasets 
localsearch examples 
set normalized best known ssq 
aa ab estimated approximation ratio dataset name local search vs means ratio best known ssq local means estimated approximation ratio dataset name local search vs means ratio best known ssq local means means vs localsearch shifted center left random center datasets best means centers worst means centers means centers dataset datasets average solution localsearch essentially best known 
average ssq close fact best known localsearch pre calculated optimal worse 
comparatively high variance localsearch ssq datasets probably reflects way generated sets randomly centered spheres overlapped entirely fewer true clusters dataset asked algorithms find 
localsearch trouble usual datasets considerably better means 
behavior means far erratic error bars show 
average cost means solutions particular dataset usually fairly close optimal invariably worse average localsearch cost means quality varied widely 
figures show variance 
shows dataset best worst medians means lowest highest ssq respectively 
shows localsearch 
localsearch solutions barely differ means quite different 
means falls suboptimal local optima median distracted random noise forcing cover spheres time 
localsearch easily best local search centers worst local search centers localsearch centers dataset distracted exchange center middle noise middle tight cluster 
coordinates points dataset best local search means medians coordinates points dataset worst local search means medians histogram dataset means localsearch centers plots localsearch means centers dimensional dataset 
show dataset histogram clusters clearly visible high spikes density 
means centers shown coordinates give position similarly localsearch centers displayed plus signs 
see examples means pitfalls losing medians noise covering multiple clusters median putting medians spot 
contrast best worst case localsearch hits clusters squarely noise assigned median pulls slightly away finding near optimal solution 
shows average running times algorithms 
running times generally comparable 
occasionally means faster localsearch slowdown rarely factor average localsearch averaged seconds dataset 
standard deviation localsearch running time high 
localsearch tries adjust cluster facility cost gets correct number clusters running time vary random decisions convergence harder easier unpredictable way 
concerned localsearch running times reasons 
localsearch consistently better solutions 
second variance localsearch solution quality low need run repeatedly finding answer contrast means produced poor answers finding 
revisit issue section address timing scalability issues 
aa ab cpu time seconds dataset name local search vs means cpu running time local means cpu time seconds dataset name local search vs means cpu running time local means means vs localsearch shifted center left random center datasets ac ad ae af ag ah ai aj ak dataset name estimated approximation ratio local search vs means ratio best known ssq local means ac ad ae af ag ah ai aj ak dataset name cpu time seconds local search vs means cpu running time local means means vs localsearch high dimensional datasets results characterize differences localsearch means 
algorithms decisions local information localsearch uses global information 
allows trade medians median different location tend get stuck local minima plague means 
reason random initialization means greater effect solution finds random choices localsearch solutions 
sophisticated decision making comes slight running time cost consider effective running time time necessary algorithm finds solution extra cost essentially vanishes 
experiments small high dimensional datasets ran means localsearch times high dimensional datasets 
points consist uniform density randomly centered dimensional hypercubes edge length percent noise 
dimensionalities datasets ac ad ae af ag ah ai aj ak ran algorithms times dataset averaged solutions 
slightly changed implementation means 
initialized medians changed way dealt medians members 
median iteration members setting position chosen uniformly random data range reset position data point chosen uniformly random 
choice medians chance get members medians members 
median random position close data point win points away medians eventually find clustering fewer clusters 
contrast position random data point point member probably set medians data points evenly distributed 
shows average ssq calculated algorithm dataset normalized division best known ssq 
error bars represent normalized standard deviations 
datasets answer means average times average cost answer localsearch close best known ssq 
standard deviations means costs typically orders magnitude larger localsearch higher relative best known cost lowdimensional dataset experiments 
increased unpredictability may indicate means sensitive dimensionality 
initialization means vital quality solution obtains variable higher dimensions leading variable solution quality 
shows running times experiments differences timing pronounced 
localsearch consistently slower running time low variance 
localsearch appears run approximately times long means improvement quality longer running time 
count amount time takes algorithm find answer localsearch competitive running time excels solution quality 
clustering streams stream means theorem guarantees performance stream constant factor approximation algorithm run steps stream 
despite fact means guarantees due popularity experiment running means clustering algorithm steps 
modifications needed means algorithm context stream real data 
modification means weighted point sets 
computation new mean place weights need taken account 
points pj assigned cluster usual means algorithm creates cluster center mean cluster pi 
weights wj points shifts location center points higher weights wi second modification involves problem previously reported literature empty cluster problem means returns fewer centers :10.1.1.44.5872:10.1.1.44.5872
recall phases means algorithm involve assigning points nearest centers computing new means 
points assigned center result means loses cluster 
implementation lost cluster randomly choose point space new cluster center 
approach worked synthetic experiments 
kdd dataset sparse high dimensional space 
high probability random point space close points dataset 
get problem lose center randomly choose point dataset 
experiments compare performance stream localsearch stream means birch 
sake completeness give quick overview birch 
birch compresses large dataset smaller clustering feature tree 
leaf tree captures sufficient statistics second moments subset points 
internal nodes capture sufficient statistics leaves 
algorithm computing tree repeatedly inserts points leaves provided radius set points associated leaf exceed certain threshold 
threshold exceeded new leaf created tree appropriately balanced 
tree fit main memory new threshold create smaller tree 
numerous heuristics employed decisions refer details :10.1.1.152.7115
constructed weighted points leaves tree passed clustering algorithm obtain centers 
birch chunked stream generated iteration gamma starting point construction iteration stream birch common method attack cluster 
stream uses clustering birch involves creating 
similarity enables directly compare methods 
give experimental results synthetic real kdd cup data comparing methods chunk stream 
put results equal footing gave algorithms amount space retaining information stream 
results compare ssq running time 
synthetic data stream generated stream approximately mb size consisting points dimensional euclidean space 
stream generated similarly datasets described diameter clusters varied factor number points factor 
divided point set consecutive chunks size mb modification equivalent running means larger multiset points point pi repeated wi times 
calculated upper bound ssq previous experiments finding ssq centers generate set 
ran experiments prefixes induced segmentation chunks prefix consisting chunk consisting second chunk appended prefix consisting concatenation chunks prefix consisting entire stream 
prefix ran birch generate birch cf tree ran birch followed hierarchical agglomerative clustering hac cf tree 
applied streamed localsearch algorithm prefix 
compare quality obtained streamed localsearch algorithm obtained popular algorithms ran localsearch cf tree experiments show localsearch better job birch data set 
examine general applicability streaming method ran streamed means ran means cf tree streaming method gave better results streamed localsearch algorithm 
birch hac combination ran comparison gave poorest clustering quality 
previous experiments ran localsearch means times dataset cf tree tested 
birch hac randomized repetition necessary generated cf trees ran hac 
left hand side chart show average clustering quality ssq achieved streamed localsearch alongside average ssq localsearch applied cf tree 
compare best known value calculated section 
localsearch streaming algorithm consistently birch localsearch combination prefix best known solution solution streamed localsearch 
particular outperformed birch localsearch algorithm factor ssq 
reason performance gap birch cf trees usually mega center point high weight points small weight words birch data stream grouping large number points mean 
way generated stream set points large relative cluster size include points far apart clusters 
summarizing group points point introduces error level computation separate points forced give high ssq answer 
right hand side chart compares average clustering quality streamed means birch means combination 
streaming method outperformed cf tree summarization method dramatically ssq achieved streamed means lower factor somewhat 
running means cf trees input localsearch results reflected incorrect summarizations data cf tree 
means able localsearch handle high dimensional data set summarization initial clusters strong localsearch consequently difference final clusterings streamed means birch means dramatic 
means localsearch benefits ssq came equal cost terms running time streamed localsearch times slower birch localsearch streamed means approximately twice slow birch means 
synthetic stream local search ssq stream ave rag ss synthetic stream means ssq stream ave rag ss birch vs stream ssq localsearch means synthetic stream local search cpu time stream cpu sec ond synthetic stream means cpu time stream cpu sec ond birch vs stream cpu time localsearch means network intrusions trends commerce created need corporations protect cyber attacks 
manually detecting attacks daunting task quantity network data 
result new algorithms proposed automatically detecting attacks 
clustering particular algorithms minimize ssq popular techniques detecting intrusions 
detecting intrusions moment happen tantamount protecting network attack intrusions particularly fitting application streaming 
offline algorithms simply offer immediacy required successful network protection 
experiments kdd cup intrusion detection dataset consists weeks raw tcp dump data local area network simulating true air force environment attacks 
features collected connection include duration connection number bytes transmitted source destination vice versa number failed login attempts continuous attributes total attributes available selected kdd ics uci edu databases kddcup kddcup html clustering 
outlier point removed 
dataset treated stream mbyte sized chunks 
data clustered clusters types possible attacks plus attack 
attacks included denial service unauthorized access remote machine guessing password unauthorized access root probing port scanning 
intrusion detection stream local search stream ave rag ss intrusion detection stream means stream ave rag ss intrusion detection memory stream po ints leaves stream centers left birch vs stream ssq localsearch means right memory usage leftmost chart compares ssq birch ls middle chart comparison birch means stream means 
birch performance th th chunks explained number leaves birch appears third chart 
stream birch amount memory birch fully take advantage 
birch leaves respectively allowed leaves respectively 
believe source problem lies birch global decision increase radius points allowed leaf size exceeds constraints 
datasets birch decision increase radius probably certainly reduces size tree 
global decision fuse points separate clusters cf leaf 
running clustering algorithm fused leaves yield poor clustering quality effects dramatic 
intrusion detection cpu time stream cpu sec ond intrusion detection cpu time stream cpu sec ond birch vs stream cpu time localsearch means terms cumulative average running time birch faster 
stream ls varies running time due creation weighted dataset step 
fourth sixth chunks stream algorithm created weighted dataset fast shown twelve second running time 
algorithm generated weighted dataset shouldn enforced claim time generated weighted dataset didn 
fifth chunk algorithm created weighted dataset runs 
runs algorithm ran quickly 
tenth run algorithm performed gain calculations cpu performance sharply degraded causing average running time slow 
results point cluster quality vs running time tradeoff 
applications speed essence clustering web search results birch appears reasonable quick dirty job 
applications intrusion detection target marketing mistakes costly stream algorithm exhibits superior ssq performance 
agrawal gehrke gunopulos raghavan 
automatic subspace clustering high dimensional data data mining applications 
proc 
sigmod pages 
ankerst breunig kriegel sander 
optics ordering points identify clustering structure 
proc 
sigmod 
bradley fayyad :10.1.1.44.5872
refining initial points means clustering 
proc 
th intl 
conf 
machine learning pages 
bradley fayyad reina :10.1.1.157.392
scaling clustering algorithms large databases 
proc 
kdd pages 
charikar guha 
improved combinatorial algorithms facility location median problems 
proc 
focs 
cortes fisher pregibon rogers 
hancock language extracting signatures data streams 
proc 
acm sigkdd intl 
conf 
knowledge data mining pages august 
duda hart 
pattern classification scene analysis 
wiley 
ester kriegel sander xu :10.1.1.121.9220
density algorithm discovering clusters large spatial databases 
lewis elkan 
true scalability clustering algorithms 
sigkdd explorations 
fayyad piatetsky shapiro smyth uthurusamy editors 
advances knowledge discovery data mining 
press park 
feigenbaum kannan strauss viswanathan 
approximate difference algorithm massive data streams 
gersho gray 
vector quantization signal compression 
kluwer academic publishers norwell ma 
guha mishra motwani callaghan :10.1.1.32.1927
clustering data streams 
proc 
focs pages 
guha rastogi shim 
cure efficient clustering algorithm large databases 
proc 
sigmod pages 
han kamber editors 
data mining concepts techniques 
morgan kaufman 
hartigan 
clustering algorithms 
john wiley sons new york 
henzinger raghavan rajagopalan 
computing data streams 
hinneburg keim 
efficient approach clustering large multimedia databases noise 
kdd 
hinneburg keim 
optimal grid clustering breaking curse dimensionality high dimensional clustering 
proc 
vldb 
jain vazirani :10.1.1.129.4996
primal dual approximation algorithms metric facility location median problems 
proc 
focs 
kaufman rousseeuw 
finding groups data 
cluster analysis 
wiley new york 
manku rajagopalan :10.1.1.6.6513
approximate medians quantiles pass limited memory 

statistical method profiling network traffic 
proceedings workshop intrusion detection network monitoring 
meyerson 
online facility location 
preparation 

offline network intrusion detection looking footprints 
sas white 
ng han :10.1.1.13.4395
efficient effective clustering methods spatial data mining 
proc 
vldb pages 
pitt :10.1.1.152.7115
criteria polynomial time conceptual clustering 
machine learning 
ismail 
means type algorithms generalized convergence theorem characterization local optimality 
ieee trans 
pami 
chatterjee zhang 
multi resolution clustering approach large spatial databases 
proc 
vldb pages 
vitter 
random sampling reservoir 
wang yang muntz :10.1.1.106.7154
sting statistical information grid approach spatial data mining 
zhang ramakrishnan livny :10.1.1.152.7115
birch efficient data clustering method large databases 
proc 
sigmod pages 

