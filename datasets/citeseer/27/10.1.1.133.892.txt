fourth intern 
conf 
architectural support programming languages operating systems asplos iv palo alto california april cache performance optimizations blocked algorithms monica lam edward rothberg michael wolf computer systems laboratory stanford university ca blocking known optimization technique improving effectiveness memory hierarchies 
operating entire rows columns array blocked algorithms operate submatrices blocks data loaded faster levels memory hierarchy reused 
presents cache performance data blocked programs evaluates optimizations improve performance 
data obtained theoretical model data conflicts cache validated large amounts simulation 
show degree cache interference highly sensitive stride data accesses size blocks cause wide variations machine performance different matrix sizes 
conventional wisdom trying entire cache fixed fraction cache incorrect 
fixed block size cache size block size minimizes expected number cache misses small 
tailoring block size matrix size cache parameters improve average performance reduce variance performance different matrix sizes 
possible beneficial copy non contiguous reused data consecutive locations 
due high level integration superscalar architectural designs floating point arithmetic capability microprocessors increased significantly years 
unfortunately increase processor speed accompanied similar increase memory speed 
fully realize potential processors memory hierarchy efficiently utilized 
data caches demonstrated effective general purpose applications bridging processor memory speeds effectiveness numerical code established 
distinct characteristic numerical applications tend operate large data sets 
cache may able hold small fraction matrix data reused may displaced cache time reused 
research supported part darpa contract 
blocking consider example matrix multiplication matrices size register allocated shows data access pattern code 
element iterations innermost loop register allocated fetched memory 
assuming matrix organized row major order innermost loop code accesses consecutive data matrices utilizes cache prefetch mechanism fully 
row accessed innermost loop reused iteration middle loop row reused outermost loop 
data remains cache time reuse depends size cache 
cache large hold matrix data displaced reuse 
cache hold row data data cache reused 
worst case words data need read memory iterations 
high ratio memory fetches numerical operations significantly slow machine 
jj kk data access pattern unblocked blocked matrix multiplication 
known memory hierarchy better utilized scientific algorithms blocked 
blocking known tiling 
operating individual matrix entries calculation performed submatrices 
blocking applied multiple levels memory hierarchy including virtual memory caches vector registers scalar registers 
example blocking applied register cache levels observe matrix multiplication speeds factor decstation factor ibm rs machine relatively higher performance memory subsystem 
matrix multiplication code blocked reduce cache misses looks kk jj min kk register allocated min jj shows data access pattern blocked code 
observe original data access pattern reproduced smaller scale 
blocking factor chosen submatrix row length fit cache 
way reused times time data brought 
total memory words accessed af interference cache 
blocking general optimization technique increasing effectiveness memory hierarchy 
reusing data faster level hierarchy cuts average access latency 
reduces number slower levels hierarchy 
blocking superior optimizations prefetching hides latency reduce memory bandwidth requirement 
reduction especially important multiprocessors memory bandwidth bottleneck system 
blocking shown useful algorithms linear algebra 
example latest version basic linear algebra library blas provides high level matrix operations support blocked algorithms 
lapack successor linpack example package built top blas library 
previous research blocking focused block algorithm manually automatically 
procedure consists steps 
restructure code enable blocking loops carry reuse second choose blocking factor maximizes locality 
step sensitive characteristics level memory hierarchy question 
benefit blocking increases block size previous approaches suggested choosing blocking factors faster memory hierarchy fully occupied data reused 
example optimal blocking factor roughly matrix multiplication machine local memory words 
appropriate registers local memories data placement fully controlled 
reasonable approach fully associative caches lru replacement policy 
impact cache behavior blocking practice caches direct mapped small degree set associativity 
address mapping may map multiple rows matrix cache lines making infeasible try fully cache 
address mapping significant effect performance blocked code causing deviate simple trend increased performance increased block size 
performance varies drastically small changes matrix size 
shown performance blocked matrix multiplication decstation 
unblocked matrix multiplication achieves roughly mflops 
decstation double word direct mapped cache hold words reused block 
graph plots performance levels obtained slightly different matrix sizes range blocking factors 
different codes blocks cache registers blocks cache 
performance curves matrix multiplication behaved drop sharply starting different blocking factors depending matrix size 
significant magnitude variation similarly sized matrices 
matrix multiplication block matrix runs twice rate matrix 
variation performance due interference misses cache 
shown rates runs cache blocked code obtained simulation 
decrease performance correlates perfectly increase cache misses 
sets data suggest behavior cache major impact blocked code performance considered choosing size block 
overview research focuses optimizing cache performance blocking 
approach discover behavior caches blocking improve performance software hardware techniques 
sensitivity rates size input matrix impossible purely experimental approach 
inadequate simulate sample data points infeasible simulate possibilities 
methodology combine theory experimentation understanding behavior cache 
drawing insights experimental data theory data locality compiler research developed model data conflicts validated simulating representative data points 
model able explain cache performance observed derive performance possible data sizes evaluate existing methods choose block size propose new methods optimizations fully utilize cache 
data locality blocked algorithms section cache model simple case direct mapped cache word cache lines illustrate model blocked matrix multiplication 
extension set associative caches multiple word line sizes described section 
reuse carried loop memory locations cache lines different iterations loop 
forms reuse temporal spatial reuse 
temporal reuse occurs data reused 
example matrix multiplication temporal reuse variables carried innermost middle outermost loops respectively 
case variable reused times size loop 
say reuse factor 
spatial reuse occurs data cache line mflops misses iteration cache register blocked cache blocked blocking factor blocking factor performance rates blocked matrix multiplication decstation 
cache line size reuse factor data accessed stride manner 
reuse data translates saving memory accesses intervening reuse displace data cache 
iteration count innermost loop large reuse innermost loop exploited 
blocking localizes iterations outer dimensions limiting intervening iterations executed innermost loop cached data reused replaced 
choosing blocking factor suitably reuse carried loops block exploited 
temporal reuse factor data blocked loop simply blocking factor number iterations blocked loop 
modeling cache interference data reused map different cache locations number cache misses variable simply aa total number memory variable reuse factor variable intrinsic misses 
generally interference misses reuse variable misses rate total number misses reused variable cache memory reuse occupies cache location 
assume simple model interference independent 
suppose locality variable carried loop set variables loop 
rate reusing loop probability accesses data iteration loop interferes reuse 
partition interferences cases cross interference interference different variables self interference interference elements array variable 
case cross interference assume location reuse unrelated cache locations interfering accesses 
estimate interference probability location reuse falls footprint variable 
footprint variable loop defined fraction cache variable iteration loop footprint measures number distinct elements iteration loop elements map unique positions cache 
accessed stride manner uniqueness guaranteed total size exceeds cache size 
case self interference longer ignore positioning reuse location elements array 
common cases 
accesses stride interference possible long number data accessed smaller cache capacity 
accesses elements array significantly interfere reuse 
self interference defined fraction accesses map non unique locations cache iteration loop sum rate reuse data table matrix multiplication parameters 
reuse self interference footprint fa af fa fa straightforward extract parameters reuse factors footprints code blocked algorithm 
self interference term depend cache size input matrix size factor cache behave 
extracting model parameters matrix multiplication interesting case study locality carried different loops different variables 
similar reuse patterns observed various important matrix operations including gaussian elimination pivoting cholesky factorization 
relevant parameters matrix multiplication code shown table 
example element reused loop reuse distinct words distinct words corresponding footprints 
parameters easy determine exception self interference variable loop simply represent term show derive value section 
quantities interest discussion left blank 
rates variable easily evaluated 
variable allocated register total number elements array af rate simply 
substituting parameters table equation rates total number cache misses equation af intrinsic misses misses intrinsic algorithm blocking factor avoided address mapping perfect 
factor due self interference variable 
terms due cross interference different variables 
far modeled reuse block 
theoretically variables reused second loop respectively 
reapplying modeling procedure refine estimate reflect level reuse 
reuse caches large respect block size small entail reused data surviving xf 
equation stands overestimate number misses circumstances 
interference regular stride section examine self interference common case array accessed constant non unit stride 
pattern constant stride array access regular self interference pattern 
intuitively data accessed interfere interference regular severe 
show self interference increase cache rate drastically avoided altogether possible 
developed algorithm find largest blocking factor avoids self interference matrix size 
tailoring block size matrix size yields better cache performance trying fixed block size 
computing self interference misses assume address written start address array matrix size run time constants 
words cache location mod 
valid caches physical addresses operating systems including mips sgi allocate frames physical address maps cache location virtual address 
location cache 
self interference direct mapped cache 
interference results pattern shown 
suppose block matrix reused words matrix marked fall cache location 
periodicity shaded words array upper left hand portion block interfere corresponding shaded words lower right hand corner block 
likewise word lower left hand corner block interferes word rectangle lower left hand corner interfere rectangle upper right hand corner 
case words block interfere increasing block size lead increase self interference 
consequently largest block size suffer self interference refer critical blocking factor plays important role determining total number misses 
observations developed efficient algorithm determines amount self interference matrix size cache size blocking factors 
algorithm executes xa matrix size cache size 
analyzing cache misses algorithm determine self interference parameter equation derive predicted number total cache misses 
cache rate combination kinds misses intrinsic misses self interference misses cross interference misses 
shows breakdown misses categories blocked matrix multiplication cache words 
misses ideal intrinsic misses self interference misses cross interference misses blocking factor breakdown misses word cache 
highlight effect address mapping function cache compares cache numbers obtained machine local memory size 
local memory software full control data placement storage effectively 
refer misses occurring local memory ideal number misses 
local memory size optimal blocking factor roughly component refers intrinsic misses particular block sizes 
section know number intrinsic misses inversely proportional blocking factor 
blocking factor approaches reaches ideal number misses 
self interference misses misses incurred due conflicts elements array 
discussed critical blocking factor blocking factors greater lead significant self interference 
blocking factor lower causes self interference 
critical blocking factor different similar matrix sizes identical cache sizes 
third component cross interference interference different arrays 
cross interference function footprint linear respect classify reused data item suffering self cross interference self interference cross interference curve may appear taper self interference begins 
self interference term explains cache behavior shown 
total cache curves decline smoothly blocking factor critical blocking factor reached 
critical blocking factor self interference component dominates causes rate rise sharply 
validate model compare predicted cache misses actual cache misses blocked matrix multiplication different cache sizes blocking factors 
shows model predicts cache behavior accurately cache large blocking factors small discussed section 
misses iteration actual predicted blocking factor actual versus predicted cache misses different cache sizes words 
fixed blocking factor model ready evaluate different strategies choosing blocking factor 
clearly trying entire cache case local memory disastrous 
evaluate simple strategy attempting fixed fraction cache 
assuming matrix size randomly distributed compute expected value possible blocking size averaging performance possible average value particularly interesting user se interested performance particular matrix size 
include standard deviation indicate likelihood achieving particular rate 
shows average cache behavior matrix multiplication word cache blocking factor 
average obtained calculating misses model range captures possible self interference patterns cache cache size 
misses ideal blocking factor average misses word cache 
vertical lines indicate standard deviation 
average standard deviation rates calculated blocking factor increments 
block chosen entire cache rate times optimal large standard deviation 
blocking factor gives best average blocking factor slightly higher average substantially lower standard deviation 
want express targeted block size fraction cache number extremely small blocking factor respectively 
actual rates range cache sizes shown 
blocking rate approximately misses iteration cache hold rows cache iteration 
blocking reduces rate average misses iteration relatively small word large word cache respectively 
reduction typically translates significant impact system performance 
unexpected drawn graphs 
trends curves different ideal 
ideal simply inverse function block factor larger block better performance 
curves indicate increasing blocking factor fact degrade performance significantly 
block size choice minimizes average cache rate uses small fraction cache 
fraction cache optimal case decreases increasing cache sizes word cache word cache 
importantly shown large standard deviation associated average case 
means execution time vary significantly different matrix sizes 
tailoring blocking factor rates highly sensitive problem size consider approach tailoring blocking factor problem size 
doing hope improve average rate importantly reduce variance 
model know cache rate hinges misses iteration block size fraction cache size rate averaged matrix sizes versus block size different cache sizes words 
critical blocking factor maximal factor self interference 
due periodicity addressing cache constant stride accesses relatively easy determine 
algorithm determine largest square block self interference 
algorithm integer return integer addr di dj integer min addr true addr addr di addr div dj abs addr mod di min dj return min di min dj algorithm algorithm compute largest square block self interference 
approach simple observations 
self interference pattern block computation identical conflict pair data depends difference addresses 
similarly di dj mod larger max di dj algorithm begins array word assuming mod valid absolute location array memory significant 
algorithm finds array words form di dj mapped location cache 
new array word mapping location puts restriction large block size may self interference begins 
blocking factor larger run time algorithm xa fast compiler run time matrix size known statically 
algorithm easily extended find largest rectangular block 
different maximal blocking factors entire range strides shown 
function periodic interference maximum matrix dimension multiple cache size 
explains optimal fixed block size small 
large diminishing return choosing larger larger block sizes 
small self interference large moderate block sizes 
data indicate critical blocking factor sensitive small changes matrix dimension array dimensions fairly large blocking factor 
choosing fixed block size optimize average case penalizes cases blocking factor exists 
blocking factor largest square block self interference word cache 
self interference number misses equation fc fa ga minimum ga 
exceeds ga cross interference limits optimal blocking factor ga 
case number misses times ideal number misses 
blocking factor misses times optimal self interference 
self interference begins ga self interference dominate cache behavior case shown optimal blocking factor typically 
shows ratio misses best block size ideal number misses cache size 
graph block size block size ga whichever smaller 
average number misses determine average performance obtained choosing block size depends average times ideal 
blocking factor best choice number misses ideal 
mean standard deviation misses significantly lower block sizes chosen shows results analysis variety cache sizes 
comparison includes data best data misses ideal rates best square block word cache 
dependent rectangular block self interference 
shows best square rectangular schemes outperform fixed block size approach terms average misses variance wide range cache sizes 
cache gets larger rate best fixed block size gets farther farther ideal increasing standard deviation 
considering block size average behavior block remains constant optimal 
self interference non constant strides analyzing cache behavior constant stride data accesses turn access patterns 
study cache behavior triangular matrices pushing irregularity data access limit consider case stride totally random 
triangular matrices typical data organization triangular matrix store rows data consecutively 
stride column access varies matrix dimension 
expect cache behavior entire multiplication similar average cache misses obtained square matrices 
compare cache figures multiplying triangular matrices elements side average obtained multiplication square matrices entire range strides 
simulation confirms prediction 
square matrices cache performance blocked triangular matrix multiplication algorithm predictable array size provided relatively large 
performance obtained similar average compute square matrices 
optimal block size small cache cache cache 
rate function highly sensitive choice blocking factor important choose number correctly 
variance optimal block size high small changes itis misses ideal best fixed block size best data dependent square block size best data dependent rectangular block size cache size rates best fixed block size best square block size best data dependent rectangular block size 
misses iteration square matrix average triangular matrix block size fraction cache size comparing rates triangular matrix multiplication average rates square matrix multiplication different cache sizes words 
difficult improve cache performance modifying block size different parts triangular matrix 
random stride shown caches may behave poorly regular strides wish investigate behave better rows randomly placed 
model accesses row array independent accesses 
access consecutive locations data decreases hit rate factor fa self interference accessing rows data reused af substituting self interference term equation varying cache block sizes obtain data 
differentiating expression misses respect blocking factor find asymptotic minimum number cache misses occurs fraction cache cache decreases cache case cache words 
figures just slightly lower obtained average square matrices 
misses iteration block size fraction cache size rate versus block size random stride different cache sizes words 
summary accesses random reuse data number intervening independent data accesses small 
constant non unit stride accesses commonly matrix operations general improve cache performance 
stride regular possible compiler run time library tailor blocking factor stride 
improvement significant relatively high variance cache misses different matrix sizes 
different cache parameters far considered direct mapped caches single word cache lines 
section consider common variations cache design higher degree associativity longer cache lines discuss affect cache behavior blocked algorithms 
re apply techniques analyzing direct mapped caches single word lines resulting models generate data different cache sizes matrix sizes block sizes 
compare performance strategies choosing data independent block size data dependent block size 
draw impact set associativity multiple word line sizes blocked algorithms 
set associativity conflict misses play significant part cache behavior blocked algorithms natural examine set associativity reduce misses significantly 
consider relatively large reasonable set associativity 
interference model section extends readily set associative caches 
briefly way set associative lru cache set holding block entries reused level assumed accesses entries hit 
validity model shown 
model misses cache size large block size small 
shows performance set associative cache fixed block size chosen problem sizes 
direct mapped cache average rates remain relatively flat block size increases 
critical blocking factors larger values 
fraction cache remains small larger direct mapped cache yields significantly lower average rate 
average remains fairly flat standard deviation shown graph increases steadily block size favoring smaller block size 
high standard deviation means execution time problem sizes significantly worse 
focuses averages standard deviations rates variety cache sizes 
compare strategies choosing block sizes fixed square block scheme problem size dependent square block scheme average ratio ideal increases fixed block size scheme remains quite constant vary block sizes different problem sizes 
importantly smaller standard deviation 
take strategy choosing block sizes associativity allows drop rate average cut standard deviation half relative strategy direct mapped cache 
line size exploit spatial locality caches today multi word cache lines 
spatial locality modeled manner similar temporal locality 
say spatial locality carried loop stride data access cache line size 
reuse factor simply cache line size divided stride 
rate calculated footprint self interference similar manner footprint self interference direct mapped case model shows best square block larger ga side 
set associative model precise prediction 
heuristic blocking factor larger cache associativity cross interference diminishes increasing set associativity 
actual versus predicted cache misses different cache sizes words 
misses iteration actual predicted blocking factor rate averaged matrix sizes versus block size different cache sizes words 
misses iteration block size fraction cache size rates best fixed block size best data dependent square block size 
misses ideal best fixed block size best data dependent square block size cache size data corresponding figures set associative cache single word line size 
actual versus predicted cache misses different cache sizes words 
misses iteration actual predicted blocking factor rate averaged matrix sizes versus block size different cache sizes words 
misses iteration block size fraction cache size rates best fixed block size best data dependent square block size 
misses ideal best fixed block size best data dependent square block size cache size data corresponding figures cache word line size 
terms need adjusted account fact access brings entire cache line 
show results applying model cache word line size 
ideal case line size words reduce rate factor reduction observed compare rate reduced factor just smallest caches factor largest 
reason ideal behavior follows row blocked matrix brought cache row necessarily align cache line boundaries 
results data loaded cache effectively increasing row length 
excess data knock useful data increasing rate 
particular self interference starts somewhat smaller blocking factor 
relative cost cache misalignment decreases block size increased decrease rate best block size closer larger caches smaller caches 
explains rates data dependent block strategy decrease increasing cache size alignment effect significant 
ratios rates ideal case greater rate longer line size reduced full factor copy optimization associativity multiple word line sizes eliminate large variance performance blocked algorithm 
investigate totally different approach eliminates self interference altogether guaranteeing high cache utilization problem sizes 
approach copy non contiguous data reused contiguous area 
doing word block mapped cache location making self interference block impossible 
technique applicable bound cache misses factor ideal 
reuse factor large cost incurred copying data negligible 
consider copying block data matrix multiplication contiguous locations 
setting term equation zero number misses compares cache numbers predicted model misses observed simulation 
discussed section cache function achieves minimum value minimum number misses times ideal 
self interference number misses nearly sensitive block size 
example block chosen fill fourth cache cache increasing number misses minimum 
misses reduced copying data placing contiguous block interfere data 
number misses achieves minimum resulting cache numbers twice ideal 
overhead copying destination row substantially higher copying reused misses iteration actual predicted blocking factor actual versus predicted cache misses different cache sizes words 
block 
entry reused block times copied 
entry destination row hand reused times 
number cache misses reduced copying reused block destination row overhead copying destination row overwhelm benefit fewer cache misses 
fact experience suggests copying sets data typically advantageous microprocessor systems today 
copying take full advantage cache longer line size 
half cache blocked loop nest making penalty due cache line misalignment negligible 
nearly words prefetched cache line fully copying cache line size rate reduced factor cache set associative opportunity reduce minimum rate eliminating just self interference cross interference 
set associativity cache matrix cache fetching data case cross interference occurs words rows mapped set 
footprints rows fa probability cross interference small strategy 
choosing blocking factor avoid nearly intrinsic misses 
results rate times ideal 
example rate ideal compared direct mapped cache 
copying data contiguous block fill cache removes negligible cross interference little effect 
demonstrate effect copying real machines compare absolute performance blocked matrix multiplication code copying optimization decstation 
show data case example suffered self interference see 
copying allows blocked code deliver consistently high level performance matrix sizes 
cases copying applied mflops copied copied blocking factor performance copying versus copying decstation 
applied difficulty 
reuse factor small cost copying greater misses saved copying 
small reuse factor arise locality computation minimal reuse spatial locality small constant 
second reused portion variable may exactly time block 
reused portion shifts copying implemented circular buffer additional addressing overhead entails 
lastly reasonably large fraction data fits cache paying cost copying data may incur unnecessary overhead 
presents comprehensive analysis performance blocked code machines caches 
blocking accepted important optimization scientific code performance machines caches understood 
combination theory experimentation shows blocking effective generally reducing memory access latency caches 
magnitude benefit highly sensitive problem size 
developed model understanding cache behavior blocked code 
model demonstrate cache behavior highly dependent way matrix interferes cache turn depends heavily stride accesses 
derived cache models different strides unit stride non unit constant stride triangular stride random stride 
models validated empirical results 
performance cache highly dependent problem size block size 
block size give rise widely varying cache rates similar problem sizes 
conventional wisdom entire cache fixed fraction cache incorrect 
fixed block size chosen optimal choice occupies small fraction cache typically 
fraction cache optimal block size decreases cache table summary cache rates word cache compared ideal 
basic model set associativity line size ideal method mean std 
dev 
mean std 
dev 
mean std 
dev 
fixed block size best block size copy block copy row block size increases 
importantly large variance performance obtained 
effect various optimizations studied summarized table 
multi word line size reduces number cache misses increases memory traffic 
set associativity improves average cache rate reduce large performance variations problem sizes 
regardless line size set associativity useful tailor block size problem size 
non unit constant stride case optimal block largest block interfere cache 
algorithm determines optimal block choice efficiently 
technique decreases average number cache misses variation different problem sizes substantially 
copying better technique applicable 
yields lower rate delivers high performance matrix sizes 
recommendation blocking numerical codes cache accompany blocking blocking optimization 
block copying applicable 
provides fewest cache misses robust performance options considered 
copying appropriate choose largest block size possible incur self interference array 
abu kuck lawrie 
automatic program transformations virtual memory computers 
proc 
national computer conference pages june 
anderson dongarra 
lapack working note implementation guide lapack 
technical report cs university tennessee apr 
callahan carr kennedy 
improving register allocation subscripted variables 
proceedings acm sigplan conference programming language design implementation june 
dongarra du hammarling duff 
set level basic linear algebra subprograms 
acm transactions mathematical software pages march 
gallivan jalby meier sameh 
impact hierarchical memory systems linear algebra algorithm design 
technical report university 
gannon jalby 
influence memory hierarchy algorithm organization programming ffts vector multiprocessor 
characteristics parallel algorithms 
mit press 
gannon jalby gallivan 
strategies cache local memory management global program transformation 
journal parallel distributed computing 
golub van loan 
matrix computations 
johns hopkins university press 

hong kung 
complexity red blue pebble game 
proceedings thirteenth annual acm symposium theory computing pages 
acm sigact may 
coffman 
organization matrices matrix operations paged multiprogramming environment 
cacm 
porterfield 
software methods improvement cache performance supercomputer applications 
phd thesis rice university may 
wolf lam 
data locality optimizing algorithm 
submitted publication 
