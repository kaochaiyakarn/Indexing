automatic algorithm configuration local search frank hutter holger hoos university british columbia main mall vancouver bc canada hutter hoos cs ubc ca determination appropriate values free algorithm parameters challenging tedious task design effective algorithms hard problems 
parameters include categorical choices neighborhood structure local search variable value ordering heuristics tree search numerical parameters noise restart timing 
practice tuning parameters largely carried manually applying rules thumb crude heuristics principled approaches rarely 
local search approach algorithm configuration prove convergence globally optimal parameter configuration 
approach versatile minimising run time decision problems maximising solution quality optimisation problems 
applies arbitrary algorithms including heuristic tree search local search algorithms limitation number parameters 
experiments algorithm configuration scenarios demonstrate automatically determined parameter settings outperform algorithm defaults orders magnitude 
approach shows better performance greater flexibility system 
code instructions tuning algorithms available line www cs ubc ca labs beta projects 
problem setting algorithm free parameters maximal performance class problem instances ubiquitous design empirical analysis algorithms 
examples parameterised algorithms tree search local search hoos st tzle commercial solvers ilog cplex offer plethora parameter settings 
considerable effort required find default parameter configuration yields high robust performance instances set distribution birattari diaz laguna 
automated tools finding parameter settings potential algorithm designers tedious task manually searching parameter space 
notice task constructing algorithm combining various building blocks copyright association advancement artificial intelligence www aaai org 
rights reserved 
www ilog com products cplex thomas st tzle universit libre de bruxelles iridia code av 
roosevelt brussels belgium stuetzle ulb ac seen special case algorithm configuration consider example tree search algorithms sat differ preprocessing variable ordering heuristics fact seen single algorithm nominal parameters 
algorithm configuration commonly implicitly explicitly treated optimisation problem objective function captures performance fixed set benchmark instances 
depending number type parameters methods solve optimisation problem include exhaustive enumeration beam search minton experimental design genetic programming application racing algorithms birattari birattari combinations fractional experimental design local search diaz laguna 
offers main contributions define iterated local search ils algorithm algorithm configuration problem 
approach works deterministic randomised algorithms applied regardless tuning scenario optimisation objective 
study effects confidence tuning occur algorithm tuned finite number training instances dominant approach literature 
possibility effects shown previously birattari give precise statistical arguments experimental results fact cost estimates orders magnitude optimistic leads strongly impaired performance 
extend basic ils algorithm order avoid tuning 
theoretical analysis resulting algorithm proves convergence optimal parameter configuration 
empirical study comprising tuning scenarios algorithms automatically determined parameter configurations outperform algorithms default settings orders magnitude 
experiments approach reaches exceeds performance system diaz laguna broadly applicable 
problem definition notation typical tuning scenarios algorithm parameters optimised distribution set problem instances algorithm parameters domain possible values associated parameter configuration space cross product domains subset thereof combinations parameter values excluded 
elements called parameter configurations denote algorithm parameter configuration 
objective algorithm configuration problem find parameter configuration resulting best performance distribution meaning best performance depends application scenario deliberately leave open 
assumption possible assign scalar cost sc single run instance case randomised algorithm seed cost example run time approximation error improvement achieved instance specific cost 
distribution instances case randomised algorithms distribution random seeds induce cost distribution cd 
formalised objective algorithm configuration minimise statistic cost distribution mean median quantile 
stochastic optimisation problem cost distributions cd typically unknown acquire approximations statistics limited number samples cost single executions 
denote approximation samples 
birattari proved statistic interest expected cost variance minimised sampling instances performing single run 
follow approach finite training set instances base approximation run instances 
deterministic algorithms interest randomised algorithms loop instances multiple runs instance compute 
iterated local search parameter configuration space depending number algorithm parameters type categorical ordinal continuous task finding best parameter configuration algorithm easy hard 
parameters parameter tuning tends easy 
case convenient allow certain number values parameter discretization try combination parameter values approach known full factorial design 
algorithm parameters reverts local optimisation full factorial design intractable number possible configurations grows exponentially number parameters 
situations manual optimisation process typically started parameter configuration parameter values modified time new configurations accepted result improved performance 
algorithm outline iterated local search parameter configuration space specific variants study differ procedure better 
input parameter configuration space neighbourhood relation procedure better compares 
output best parameter configuration 
default parameter configuration random better ils ils perturbation random localsearch better ils ils probability ils random return best procedure repeat foreach randomised order better break return sequential modifications typically continued change single parameter yields improvement anymore 
widely adopted procedure corresponds manually executed local search parameter configuration space search space set possible configurations objective function quantifies performance achieved configuration initial configuration start manual search process neighbourhood exchange neighbourhood parameter modified search step search strategy iterative improvement 
viewing manual procedure local search process advantageous reasons suggests automation procedure improvements ideas stochastic local search community 
simple iterative improvement search terminates local optimum encounters parameter configuration improved modifying single parameter value overcome problem iterated local search ils martin st tzle search performance optimising parameter configurations 
ils stochastic local search method builds chain local optima iterating main loop consisting order solution perturbation escape local optima ii subsidiary local search procedure iii acceptance criterion decide keep reject newly obtained candidate solution 
algorithm ils method searches parameter configuration space 
uses combination default random settings initialisation employs iterative improvement subsidiary local search procedure uses fixed number random moves perturbation accepts better equally parameter configurations re initialises search random probability 
furthermore exchange neighbourhood neighbouring parameter configurations differ parameter value 
practice heuristic algorithms number parameters relevant higherlevel parameters take certain values call parameters conditional 
example sat implements different clause learning mechanisms parameterised conditional parameters governing percentages clause lengths 
deals conditional parameters excluding configurations neighbourhood configuration differ conditional parameter relevant 
basic variant procedure better compares cost approximations exactly samples respective cost distributions cd cd 
configuration sample costs gathered executing instances recording costs reused computations 
confidence tuning approaches literature minton diaz laguna employs optimisation approach trying minimise cost approximations fixed number samples 
analogy machine learning call samples training set test set consists additional samples cost additional runs independently sampled instances 
main problems associated fixed size training set 
refer confidence intuitively problem concerns fact cost estimated best training configuration train underestimates cost configuration test low set 
essentially happens train est observed cost training set train min 
biased estimator argmin 

fact min 
equality holding pathological cases birattari 
result optimisation typically better performance achieve independent test set 
consider example 
objective minimise average cost budget allows single run configuration assume exponential cost distribution cd configuration assumption case run time distributions far reality see hoos st tzle 
express distribution train closed form due lemma 
choices somewhat arbitrary experimented settings expect performance fairly robust respect choice 
lemma bias exponential distributions 
algorithm parameter configuration space cd exp 
train exp 
training cost train train underestimate real cost train far 
extreme case lemma cost approximation samples underestimates true mean cost factor expectation number parameter configurations evaluated 
effect increases number configurations evaluated variance cost distributions decreases increased 
confidence undesirable effects empirical studies algorithm development 
consequence algorithm costs reported best parameter configurations results typically underestimates 
issue high chance best training configuration just lucky resulted large cost underestimate performs best training configuration train worse true best configuration lead second problem encountered typical parameter optimisation approaches dubbed birattari unfortunate situation additional tuning impairs test performance 
analogous fitting supervised machine learning lead arbitrarily poor test results 
earlier study birattari demonstrate small tuning effect extremely large number pseudo runs analysis qualitative suggest solution problem 
demonstrate confidence tuning fact severely impair performance parameter optimisation methods see figures introduce variant provably suffer effects 
approach fact consistent estimator cost approximation reliable approaches infinity eventually eliminating confidence possibility mistakes comparing parameter configurations tuning 
captured lemma 
lemma mistakes 
parameter configurations 
consistent estimators limn 
order optimise respect true objective function lemma suggests base cost approximation large number samples large sample sizes disadvantage evaluation parameter configuration cost expensive 
focused ils section introduce variant moves configuration space quickly avoids problems confidence tuning 
achieves goal focusing samples promising parameter configurations 
denote straight forward proofs lemmata provided companion technical report 
recall consistent estimator iff limn 
number samples approximate cost distribution cd 
procedure better somewhat complicated 
say dominates performance samples better formally 
procedure better starts acquiring sample configuration smaller case ties 
iteratively acquires samples configuration smaller configuration dominates returns true dominates 
keep track total number samples acquired improving step time procedure better returned true 
better returns true acquire bonus samples cd reset mechanism ensures acquire samples configurations error comparison configurations decreases expectation 
lemma eventually truly better preferred fact prove convergence true best parameter configuration 
lemma unbounded number evaluations 
denote number samples iteration approximate 
constant configuration finite 
proof lemma essentially exploits probabilistic random restarts performed effect number samples configuration grows arbitrarily large number iterations approaches infinity 
theorem convergence 
number iterations approaches infinity converges true optimal parameter configuration proof 
lemma grows unboundedly 
approach infinity lemma states pairwise comparison truly better configuration preferred 
theorem follows fact eventually visits finitely parameter configurations prefers best probability 
empirical evaluation report computational experiments tuning different algorithms sat www sat org tree search solver sat saps hutter hoos local search algorithm sat gls local search algorithm solving probable explanation mpe problem hutter hoos st tzle 
respective parameters summarised table details see original publications 
discretisation picked lower upper bounds parameter cali bra discretised interval uniformly uniformly note samples cd comparison heterogeneous instance distributions robust base comparison sampled instances configurations evaluated 
parameter default value values considered tuning saps parameter configurations wp gls parameter configurations init mb random mb sat conf 
variable order categorical values parameterised learning strategy minisat categorical values parameterised data structure mixed categorical values clause minimisation expensive cat 
values simple expensive variable decay restart restart multiplication table algorithms parameters parameter values considered tuning 
sat parameterised variable order learning strategy means values parameter conditional continuous parameter relevant consider values conditional parameter 
log scale 
sat algorithm developer provided discretisation 
carried experiments tuning sat saps variety instance distributions including instances previous sat competitions instance mix satlib single quasi group holes instance set graph colouring instances small world graphs sw gcp gent 
due space limitations report results fairly easy instance allowing perform runs sw gcp 
constructed subclasses sw gcp sw containing instances sat takes seconds sw gcp saps containing satisfiable instances median saps run time seconds 
gls optimise performance instance set grid set grid structured bayesian networks binary variables random conditional probability tables evidence variables 
gls best performing algorithm instances hutter hoos st tzle 
instance set exception split independent training test sets test performance computed independent runs instance 
consider parameter optimisation scenarios sat sw minimise sat median run time set sw gcp sat cutoff seconds 
saps sw minimise saps median run time set sw gcp saps cutoff seconds 
saps minimise median run length saps single instance cutoff second 
gls grid maximise average probability gls finds networks grid seconds relative best solution gls finds default settings experiments carried cluster dual ghz intel xeon pcs mb cache gb ram running linux 
reported runtimes cpu times 
scenario default value gls grid saps steps saps sw seconds sat sw seconds parameters table performance tuning scenarios 
scenario approach list training test performance top bottom row respectively mean stddev independent repetitions refers value wilcoxon signed rank test testing null hypothesis difference results vs sat 
hour 
equivalently minimise reciprocal value values mean tuned parameters find better solutions seconds default finds hour 
compare results algorithms default parameters parameter configurations system diaz laguna 
optimisation scenario perform multiple independent training repetitions repetition consisted independent execution algorithm tuned 
test performance approach repetition computed independent test set 
cost approximation samples configuration 
repetition terminated configurations visited default 
leads total algorithm executions repetition took saps sw saps gls grid cutoff times 
sat sw chose cutoff time 
table shows lead large improvements algorithm defaults slight edge main advantage supports arbitrary numbers types parameters 
contrast limited total continuous integer parameters rendering inapplicable tuning sat 
cost approximations samples notice significant difference training test performance 
judges progress realistically showing similar training test performance importantly best test performance 
practical applications typically afford algorithm executions may take hours may tempted lower value 
demonstrate small values lead poor behaviour due confidence tuning employed tuning scenario saps 
repetition evaluated visited configuration single run saps instance keeping track incumbent configuration shortest run length configurations visited time constitutes training performance 
line time step computed test performance independent runs saps 
repeated process times different runs training leading training test performance measurements time step shows median quantiles 
training test performance diverge especially quantile test performance see evidence tuning 
tuning effect occurs 
avoids problem tuning gracefully performing runs promising parameter configurations seconds training performance saps virtually identical test performance 
may expect come cost slow movement configuration space demonstrate case starts progress quickly performs better basic ils 
studied parameter setting tuned easy subclass sw gcp saps fairly homogeneous problem class sw gcp lead performance entire class 
shows particular example automatically tuned parameters sw gcp saps hour drastically outperform saps default setting entire set speedups orders magnitude 
extremely encouraging result automatic parameter tuning homogeneous instance classes 
tune replica exchange monte carlo algorithm protein structure prediction hoos achieved performance improvements 
applied solvers submitted sat competition achieved speedup order magnitude saps random instances fold speedup tuning new state art tree search algorithm sat encoded software verification problems hutter 
related exist approaches automatic algorithm configuration problem distribution minton diaz laguna 
techniques subject tuning 
methods aware avoid tuning time racing algorithms birattari birattari limited tuning tasks fairly small number configurations 
finding best parameter configuration instance basis see hutter 
principle powerful distribution approach follow general requires instance features various assumptions shape run time distributions 
orthogonal line tuning approach idea tuning algorithm parameters online execution time 
strategy followed called reactive search methods battiti 
line offline strategies seen complementary reactive search methods tend number parameters remain fixed search tuned line approaches 
median quantiles cpu time test train tuning saps 
median quantiles cpu time test performance saps 
runtime automated parameters runtime default saps speedup sw gcp 
confidence tuning effects tuning scenario saps 
tuning causes perform poorly 
time point plot training test performance incumbent training parameter configuration far 
dominates versions 
performance saps wp tuned sw gcp saps vs saps defaults randomly sampled satisfiable instances data set sw gcp 
average speedup orders magnitude average run time vs seconds median vs seconds 
powerful new method automated configuration tuning parameterised algorithms 
underlying principles easy understand versatile tool applied effectively configure wide variety algorithms including parameters conditional 
particular variant method provably converges true optimal parameter configuration effectively avoids problems confidence tuning 
demonstrated practical usefulness param ils number tuning scenarios outperforms algorithm defaults orders magnitude 
performs better par system contrast directly applicable tuning scenarios involving large number parameters 
proved helpful various parameter tuning tasks development new algorithms sat protein structure prediction 
number avenues research 
integrating statistical testing procedures algorithm unnecessary evaluations poorly performing configurations avoided 
furthermore interesting consider ways searching neighbourhood investigate specialised local search methods continuous parameters 
need discretisation limitation investigate limitation overcome response surface models 
convinced increasingly effective tools produced line research allow researchers focus essential scientifically valuable tasks designing algorithms solving hard problems help practitioners solve application problems effectively 
kevin leyton brown kevin murphy discussions helpful comments earlier version manuscript 
diaz laguna 
fine tuning algorithms fractional experimental design local search 
op erations research 

experimental research evolutionary computation 
springer verlag 
battiti 
reactive search machine learning memory heuristics 
technical report dit universit degli studi di trento trento italy 
birattari st tzle paquete 
racing algorithm configuring metaheuristics 
proc 
gecco 
birattari 
problem tuning metaheuristics seen machine learning perspective 
ph dissertation universit libre de bruxelles brussels belgium 
golden 
experimental design find effective parameter settings heuristics 
journal heuristics 

extensible sat solver 
proc 
sat 
gent hoos prosser walsh 
morphing combining structure randomness 
proc 
aaai 
hoos st tzle 
stochastic local search foundations applications 
morgan kaufmann 
hutter leyton brown hoos 
performance prediction automated tuning randomized parametric algorithms 
proc 
cp 
hutter hoos hu 
boosting verification automatic tuning decision procedures 
manuscript preparation 
hutter hoos st tzle 
efficient stochastic local search mpe solving 
proc 
ijcai 
hutter hoos 
scaling probabilistic smoothing efficient dynamic local search sat 
proc 
cp 
martin st tzle 
iterated local search 
handbook metaheuristics 
kluwer 
minton 
automatically configuring constraint satisfaction programs case study 
constraints 

evolving evolutionary algorithms linear genetic programming 
evolutionary computation 
hoos 
replica exchange monte carlo protein folding hp model 
submitted bmc bioinformatics 
