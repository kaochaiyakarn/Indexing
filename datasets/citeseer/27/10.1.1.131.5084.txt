latent semantic indexing probabilistic analysis christos papadimitriou prabhakar raghavan santosh vempala november hisao tamaki latent semantic indexing lsi information retrieval technique spectral analysis term document matrix empirical success heretofore rigorous prediction explanation 
prove certain conditions lsi succeed capturing underlying semantics corpus achieves improved retrieval performance 
propose technique random projection way speeding lsi 
complement theorems encouraging experimental results 
argue results may viewed general framework theoretical basis spectral methods wider class applications collaborative ltering 
computer science division berkeley berkeley ca 
email christos cs berkeley edu 
ibm almaden research center harry road san jose ca 
email almaden ibm com 
computer science department university tokyo japan 
email cs ac jp 
department mathematics cambridge ma 
email vempala math mit edu 
eld information retrieval traditionally considered outside scope database ory 
database theory deals queries precise predicates called employee manager salary paradigm information retrieval nebulous ill de ned concept relevance depends intricate ways intent user nature corpus 
evidently little theory built basis 
increasing volume applied database research see instance sigmod proceedings focusing methods managing text 
see surveys information retrieval including discussions technique focus database theoretical points view respectively classical texts subject information retrieval 
eld information retrieval evolving directions bring closer databases 
information retrieval systems increasingly built relational object relational database systems text index les 
important change dramatic expansion scope information retrieval advent internet information database concepts theory started nd fertile ground see example record number information retrieval papers sigmod proceedings secondly techniques employed information retrieval mathematical sophisticated plausibly amenable analytical treatment :10.1.1.120.3875
attempt treat rigorously technique latent semantic indexing lsi introduced 
thirdly information retrieval systems increasingly built relational object relational database systems text index les 
advent web enabled powerful new applications ltering known target personalized systems tackled techniques inspired part information retrieval section 
ir lsi complexity information retrieval best illustrated nasty classical problems synonymy missing documents automobile querying car polysemy retrieving documents internet querying sur ng 
deal similar problems ideally represent documents queries terms conventional vector methods underlying latent hidden concepts referred terms 
hidden structure xed mapping importantly stakes high database theory lightly pass eld 
terms concepts depends critically corpus document collection hand term correlations embodies 
latent semantic indexing information retrieval method attempts capture hidden structure techniques linear algebra 
brie see section detailed description vectors representing documents projected new low dimensional space obtained singular value decomposition term document matrix see subsection 
low dimensional space spanned eigenvectors correspond largest eigenvalues presumably striking correlations terms 
queries projected processed low dimensional space 
results great savings storage query time expense considerable preprocessing empirical evidence reported literature improved information retrieval :10.1.1.117.3373
repeatedly reported lsi outperforms regard precision recall standard collections query workloads conventional vector methods address problems polysemy synonymy 
little literature way mathematical theory predicts improved performance 
interesting mathematical fact due eckart young stated theorem cited explanation improved performance lsi states informally lsi retains possible relative position document vectors 
may provide explanation lsi deteriorate performance conventional vector space methods fails justify observed improvement precision recall 
rst attempt mathematical techniques rigorously explain ically observed improved performance lsi 
lsi exploit reveal statistical properties corpus start rigorous probabilistic model corpus say mathematical model corpora generated section 
brie model topics probability distributions terms 
document probability distribution convex combination small number topics 
include framework style authorship model matrix modi es term distribution 
corpus collection documents obtained repeatedly drawing sample documents 
section brie discuss alternative probabilistic model motivated part applications collaborative ltering 
corpus model determine conditions lsi results enhanced retrieval 
theorem stating essentially corpus focused collection meaningfully correlated documents lsi performs 
problem de ne terms reasonably close correspondence mean intuitively practice theorem proved 
section prove results quite general liked de nitely point direction 
particular show special case style modi er document single topic terms partitioned topics topic distribution high probability terms low probability lsi projecting subspace dimension equal number topics discover topics exactly high probability theorem 
section show project term document random low dimensional subspace high probability wehave distance preservation property akin enjoyed lsi 
suggests random projection may yield interesting improvement lsi perform lsi precomputation original term document matrix dimensional projection great computational savings great loss accuracy theorem 
random projection seen alternative justi cation sampling lsi 
reports lsi experiments literature suggest lsi done entire corpus randomly selected terms documents may sampled appears documents 
little non empirical evidence accuracy sampling 
result suggests di erent elaborate computa intensive approach projection random low dimensional subspace rigorously proved accurate 
lsi background corpus collection documents 
document collection terms universe terms 
represented vector axis represents term 
ith coordinate vector represents function number times ith term occurs document represented vector 
candidates right function frequency precise choice ect results 
matrix rank rows represent terms columns represent doc uments 
singular values eigenvalues aa necessarily distinct 
singular value decomposition expresses product matrices diag matrix ur matrix columns orthonormal vr matrix column orthonormal 
lsi works omitting largest singular values decomposition appropriate dimension low dimensional space alluded informal description page 
small enable fast retrieval large adequately capture structure corpus practice hundreds compared thousands 
dk diag uk uk vk vk 
ak matrix rank approximation rows represent documents 
words column vectors documents projected dimensional space spanned column vectors uk call space lsi space approximation 
known theorem gives idea subscript denotes frobenius norm 
theorem eckart young see 
matrices ak minimizes ka ck ai ci lsi preserves extent possible relative distances presumably retrieval capabilities term document matrix projecting lower dimensional space 
remains seen way retrieval capabilities 
probabilistic corpus model useful formal models ir literature probability plays major role see instance surveys comparisons :10.1.1.33.4684
approach body formulate information retrieval problem learning concept relevance relates documents queries 
corpus correlations plays central role 
contrast focus probabilistic properties corpus 
lsi supposed exploit bring structure corpus fare meaningful collection strongly correlated documents produce noise random set unrelated documents 
order study dependence performance lsi statistical properties corpus start probabilistic model corpus 
state basic probabilistic model 
universe terms topic probability distribution meaningful topic di erent uniform distribution concentrated terms talk particular subject 
example topic space travel favor terms galaxy rarely mentioning spider 
possible criticism model take account correlations terms topic example document topic internet contain term search contains term engine 
structure documents heavily ected authorship style 
juj juj stochastic matrix matrix nonnegative entries row sums equal denoting way style modi es frequency terms 
example formal style may map car automobile vehicle seldom car wheels 
admittedly comprehensive treatment example assumption valid uence independent underlying topic 
corpus model quadruple universe terms set topics set styles probability distribution denote set convex combinations topics set convex combinations styles set positive integers integers represent lengths documents 
corpus model probability distribution topic combinations intuitively favoring combinations related topics style combinations document lengths total number term occurrences document 
document generated corpus model step sampling process 
rst step convex combination topics combination styles positive integer sampled distribution terms sampled times form document time distribution size collection documents generated repeating step sampling process times 
analysis lsi lsi bring semantically related documents 
deal ectively problem synonymy 
theoretical evidence 
results assume corpus model particularly simple structure 
show case lsi discover structure corpus handles synonymy 
results taken indications kinds results proved hope lead elaborate techniques lsi proved realistic assumptions 
rst need useful lemma formalizes intuition largest singular values matrix separated remaining singular values subspace spanned corresponding singular vectors preserved small perturbation added lemma ann matrix rank singular value decomposition diag 
suppose su ciently large constant arbitrary matrix kf su ciently small positive constant 
singular value decomposition 
uk matrices consisting rst columns respectively 
orthonormal matrix matrix kgk 
proof lemma appendix relies theorem stewart perturbing symmetric matrix 
corpus model 
pure document involves single topic 
call separable set terms ut associated topic ut mutually disjoint total probability assigns terms ut primary set terms topic assumption corpus model style free pure probably strong elimination addressed investigations 
hand assumption corpus separable small value may reasonably realistic documents usually preprocessed eliminate commonly occurring words 
pure corpus model jt denote number topics pure document generated fact generated single topic wesay document belongs topic corpus generated document denote vector assigned rank lsi performed wesay rank lsi skewed corpus instance pair documents vd belong di erent topics vd belong topic 
informally rank lsi skewed corpus small assigns nearly orthogonal vectors documents di erent topics nearly parallel vectors documents single topic lsi particularly job classifying documents applied corpus 
theorem states large corpus speci cally greater number terms generated restricted corpus model nice property high probability 
theorem separable corpus model topics probability topic assigns term su ciently small constant 
corpus documents rank lsi skewed onc probability 
proof 
ci denote subset generated corpus consisting documents belonging topic ti see main idea rst assume 
document ci consists terms ui primary set terms associated topic ti 
term document matrix representing corpus consists blocks bi rows bi correspond terms ui columns bi correspond documents ci matrix non zero entries rows columns bi 
block diagonal blocks bi focus particular block bi denote largest second largest eigenvalues bt bi 
intuitively matrix bi essentially adjacency matrix random bipartite multigraph standard theory spectra graphs probability 
formal justi cation showing quantity captures property conductance equivalently expansion ofb bi high 
conductance undirected edge weighted graph min wt random documents picked topic ti 
show conductance jtj jt ij number terms topic ti 
graph induced adjacency matrix bi 
subset vertices documents wt assume jsj jsj 
ps probability th term ti 
estimate term xjs ps probability independence xj simple application cherno hoe ding bound lower bound weight cut ps jsj high probability second application cherno hoe ding bound 
desired bound conductance follows 
sample size jcj su ciently large maximum term probability su ciently small note implies size primary set terms topic su ciently large largest eigenvalues high probability 
suppose sample enjoys property 
ui denote eigenvector bi corresponding eigenvalue space coordinates indexed terms ti ui extension full term space obtained padding zero entries terms ti 
dimensional lsi space corpus spanned mutually orthogonal vectors ui vector vd representing document ci projected space projection scalar multiple ui orthogonal uj term document matrix consists blocks bi matrix small klk norm exceeding high probability 
observed analysis case invariant subspace wk corresponding largest eigenvalues ideal representation space representing documents topics 
hope small perturbation prevent lsi identifying wk small errors 
apply lemma 
denote dimensional space rank lsi identi es 
separability corpus model implies norm perturbation document term matrix lemma norm di erence matrix representations wk 
wk small perturbation wk projecting vector representing document inci yields vector close direction ui dominating eigenvector bt bi 
lsi representations documents direction belong topic nearly orthogonal belong di erent topics 
quantitative analysis lemma shows rank lsi skewed high probability 
experiments theorem gives asymptotic result claims probability approaches size parameters grow phenomenon indicates observed corpora modest sizes seen experiment 
generated documents terms long corpus model terms topics 
topic assigned disjoint set terms primary set 
probability distribution topic probability density equally distributed terms primary set remaining equally distributed terms 
corpus model separable 
measured angle function angle cosine pairs documents original space rank lsi space 
typical result similar results obtained repeated trials 
call pair documents intra topic documents generated topic inter topic 
intra topic inter topic min max average std min max average std original space lsi space angles measured radians 
seen angles intra topic pairs dramatically reduced lsi space 
minimum inter topic angle small indicating inter topic pairs close confused average standard deviation show pairs extremely rare 
results experiments di erent size parameters similar spirit 
experiments reported singular value decomposition 
synonymy section brief discussion synonymy context lsi 
consider simple model terms identical occurrences generalizes synonymy applies pairs terms demand war peace 
furthermore small occurrence probability 
term term autocorrelation matrix aa rows columns corresponding terms nearly identical 
avery small eigenvalue corresponding pair presumably smallest eigenvalue aa corresponding eigenvector vector terms say di erence terms 
intuitively version lsi project weak eigenvector corresponds presumably insigni cant semantic di erences synonymous terms 
exactly expect method claims bring hidden semantics corpus 
lsi random projection result johnson lindenstrauss states points vector space projected random subspace suitably high dimension distances points ap preserved 
random projection reduce dimension document space bring semantically related documents 
lsi hand achieve computation time bottleneck 
naturally suggests step approach 
apply random projection initial corpus dimensions small obtain high probability smaller representation close terms distances angles original corpus 

apply rank lsi random projection number singular values kept may increased little 
section establish approach works sense nal repre sentation close get directly applying lsi 
way view result random projection gives fast way eigenspace eigenvalues eigenvectors matrix 
rst state johnson lindenstrauss lemma 
lemma johnson lindenstrauss see 
dimensional subspace origin random variable denote square length projection suppose log pr jx nj le result infer high probability pairwise euclidean distances log approximately maintained projection random subspace 
choosing lemma high probability projected vectors scaling factor fv ig satisfy kv jk similarly inner products preserved approximately vi vj projected vectors satisfy vi vj vi vj vi vj 
particular vi length inner product vi vj changes atmost 
consider term document matrix generated corpus model 
random column orthonormal matrix rows columns project dimensional space 
rt matrix random projection scaling svd respectively 
rx lemma arbitrary positive constant 
log su ciently large constant kx tx proof lemma appendix 
corollary corollary rank approximation original matrix kx get main result section proof appears appendix theorem jja kjj bib jja jjajj measure jja tells original matrix recovered direct lsi 
words theorem says matrix obtained random projection followed lsi expanded twice rank recovers matrix obtained direct lsi 
computational savings achieved step method 
matrix 
time compute lsi mnc ifa sparse non zero entries column average number terms document 
time needed compute random projection dimensions mcl 
projection time compute lsi ml 
log total time ml 
obtain approximation 
running time step method asymptotically superior log log compared mnc 
theoretician rst reaction unexpected positive negative empirical phenomenon understand terms mathematical models rigorously proved theorems precisely tried substantial partial success 
able prove seen mere indication hold expect true positive properties lsi go far theorems proving 
speci technical issues pursued 
theorem extended model documents belong topics term occurrences independent 
lsi address polysemy spectral techniques slightly di erent see :10.1.1.120.3875
seen evidence handle synonymy 
theory ideally go ex post facto justi cation methods explanation positive phenomena point exploiting improving 
section propose random projection technique way speeding lsi possibly alternative attempt direction 
infact vaithyanathan modha ibm almaden private communication applied random projection lsi real life corpora encouraging initial results 
important role theory unify generalize 
spectral techniques con ned vector space model strict context information retrieval 
furthermore spectral analysis similar graph theoretic model world wide web shown exper succeed identifying topics substantially increase precision recall web searches databases law decisions service logs patents :10.1.1.120.3875
itis clear spectral techniques theoretical analysis may prove method domains current interest data mining spectral techniques discover correlations relational databases collaborative ltering personalizing sub preferences interests 
rows columns general terms documents consumers products viewers movies components systems 
conclude section brief description promising alternative graph theoretic corpus model 
suppose documents nodes graph weights edges capture conceptual proximity oftwo documents example distance matrix derived fact coincide aa 
topic de ned implicitly subgraph high conductance concept connectivity appropriate context 
prove assumption similar separability spectral analysis graph identify topics model proof omitted theorem corpus consists disjoint subgraphs high conductance joined edges total weight bounded fraction distance matrix rank lsi discover subgraphs 
berry dumais brien 
linear algebra intelligent information retrieval 
siam review 
berry brien krishna varadhan 
version user guide 
university april 
brewer 
invited talk pods sigmod 
chakrabarti dom gibson kleinberg raghavan rajagopalan 
mining information networks spectral methods 
preparation ibm almaden research center 
sachs spectra graphs academic press 
deerwester dumais landauer furnas harshman 
indexing latent semantic analysis 
journal society information science 
dumais furnas landauer deerwester 
latent semantic analysis improve information retrieval 
proceedings chi conference human factors computing new york acm 
dumais 
improving retrieval information external sources 
behavior research methods instruments computers 
fagin 
combining fuzzy information multiple sources 
proc 
pods pp 

faloutsos oard 
survey information retrieval ltering methods 
technical report university maryland computer science dept college park md frankl 
johnson lindenstrauss lemma sphericity graphs comb 
theory 
fuhr probabilistic models information retrieval computer journal pp 

gibson kleinberg raghavan 
nonlinear dynamical systems mine categorical data 
submitted publication 
golub 
handbook matrix computation ii linear algebra 
springer verlag new york 
golub van loan 
matrix computations 
johns hopkins university press london 
hoe ding 
probability inequalities sums bounded random variables journal american statistical association 
kleinberg authoritative sources hyperlinked environment proc :10.1.1.120.3875
acm siam sym discrete algorithms appear 
available ibm research report rj may 
johnson lindenstrauss 
extensions mapping hilbert space contemp 
math 

jerrum sinclair 
approximating permanent 
siam comp 
pp 

raghavan 
information retrieval algorithms survey 
proceedings acm symposium discrete algorithms 
van rijsbergen information retrieval butterworths london 
salton mcgill 
modern information retrieval 
mcgraw hill new york 
turtle croft comparison text retrieval methods computer journal pp 

wilkinson 
algebraic eigenvalue problem 
oxford university press london 
appendix proofs lemmas version lemma take speci values constants facilitate proof 
lemma ann matrix rank singular value decomposition diag 
suppose arbitrary matrix kf 
matrices singular value decomposition 
uk consisting rst columns respectively 
orthonormal matrix matrix kgk proof lemma relies theorem stewart perturbing symmetric matrix 
theorem suppose symmetric matrices orthogonal matrix range invariant subspace partition matrices bq eq follows matrices bq eq min max ke ke min smallest eigenvalue max largest eigenvalue exists real matrix kp ke columns form orthonormal basis subspace invariant proof lemma 
apply theorem aa block diagonalizing matrix theorem followed zero columns 
write uk rst columns andq consists remaining columns followed zero columns 
bu diagonal matrix bq diagonal matrix 
eq decomposed blocks eij theorem 
apply theorem need bound kei jk simply bounding kek 
aat af fat fft kak kf kf 
non zero eigenvalues 
min max ke ke positive 
assumptions theorem satis ed 
follows exists matrix satisfying kp ke forms orthonormal basis subspace invariant forb invariant subspace corresponds largest singular values column vectors rst eigenvectors span invariant subspace spanned column vectors 
orthonormal matrix 
kq kq kp follows khk hr claimed 
lemma proof theorem 
lemma matrix orthonormal columns matrix kw uk ku uk ku vk ku wk arbitrary vectors ku kv kw kw kw ii proof lemma th eigenvalue written ajb ajb consider expression vk rst eigenvectors th eigenvector vi reduced summing kx bt ju rj kx vi bj vi bj vi bj vi vi orthogonal bj unit vectors proof theorem 
maxv nx kx ak iii kx kx kx bj vi lx bn orthonormal set vectors hand show jja kjj jja kjj nx kx bib bij bi abi abi bi abi nx jja jjajj nx kx nx kx jjajj jja kjj jja jja kjj kx jja kx kx kx iv kx rt abi log lemma large high probability high probability corollary kx substituting kx kx kx jja kjj jja kjj abi kx jja jja jjajj 
