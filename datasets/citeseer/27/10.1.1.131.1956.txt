annals statistics 
discussion arcing classifiers leo breiman yoav freund robert schapire labs park avenue florham park nj usa yoav schapire research att com september leo breiman interest boosting extensive experiments adaboost algorithm calls arc fs generous exposition statistics community 
breiman experiments intensive email communication years inspired think boosting new ways 
new ways thinking turn led consider new ways measuring performance boosting algorithm predicting performance sample instances 
exciting communication channel prominent practical statistician 
computer scientists try derive algorithms theoretical frameworks 
frameworks capture prior beliefs nature real world problems capture important aspects problem new useful ways 
case boosting originally derived answer theoretical question posed kearns valiant pac framework model study theoretical machine learning proposed valiant 
probably thought algorithms theoretical question posed 
hand experimental statistician leo usually interested actual behavior algorithms existing data sets pays lot attention actual values various variables run algorithm 
running adaboost synthetic real world datasets breiman observed algorithm surprisingly low generalization error consistent theory time predicted 
challenge experiments breiman reported drucker cortes quinlan prompted think harder problem come new theoretical explanation surprising behavior describe bartlett lee 
explanation suggests new measurable parameters tested experiments adventure continues 
theory suggests new algorithms experiments experiments give rise new observations challenge theory come tighter bounds 
communication leo challenging exciting 
hope see communication developing researchers computational learning theory statistics 
boosting bagging breiman improving performance learning algorithm called prediction algorithm classification method 
algorithm operates set instances theories usually give upper lower bounds actual performance algorithms 
gap bounds reflection degree theories fail reflect world shortcomings mathematical analysis 
theoretical bounds refer discussion non asymptotic generate specific numerical bounds finite sample sizes 
bounds numerically pretty loose 
cases produce classification rule refer hypothesis 
goal learning algorithm find hypothesis low generalization prediction error low misclassification rate separate test set 
bagging boosting general methods improving performance learning algorithm call base learning algorithm 
certain level algorithms similar feeding perturbed versions training set base learning algorithm combining resulting rules majority vote 
similarities apparent important differences algorithms 
probably important difference perturbations introduced bagging random independent perturbations introduced boosting training set chosen deterministically serially th perturbation depending strongly previously generated rules 
breiman uses boosting resampling boosting reweighting way combines methods 
section breiman reports results deterministic version boosting results indicate experiments reported randomization important element 
hand breiman informed unpublished experimental results regarding boosting unpruned decision trees boosting resampling advantage boosting reweighting 
difference boosting reweighting bagging overlooked 
breiman analyzes bagging boosting terms bias variance decomposition error 
argue analysis appropriate boosting 
proposed different analysis appropriate boosting leaves effects randomization 
giving theory randomization effects characterization cases advantageous interesting open problem 
rest discussion organized follows 
section give historical perspective development boosting algorithms 
section summarize arguments explaining boosting bias variance decomposition 
section sketch explanation small generalization error boosting full description analysis appears bartlett lee 
conclude describing practical theoretical open questions 
boosting pac framework differences boosting bagging reflect different frameworks algorithms developed 
breiman developed bagging context reducing variance learning algorithms boosting developed answer theoretical question posed kearns valiant pac learning literature 
stated somewhat informally question suppose computationally efficient learning algorithm generate hypothesis slightly better random guessing distribution inputs 
existence weak learning algorithm imply existence efficient strong learning algorithm generate arbitrarily accurate hypotheses 
answer question schapire proof constructive describes efficient algorithm transforms efficient weak learning algorithm efficient strong :10.1.1.153.7626
freund described simpler considerably efficient boosting algorithm called boost majority 
adaboost algorithm proposed boosting boosting reweighting assume learning algorithm directly weighted training sample boosting resampling training samples generated picking examples random distribution training set 
algorithms 
nearly efficient boost majority adaboost certain practical advantages preceding boosting algorithms discuss 
goal boosting algorithms starting schapire generate combined hypothesis generalization error small 
means achieving reduction reduce error combined hypothesis training set 
expectation reducing training error boosted hypothesis reduce test error justified appealing uniform convergence theory vc theory arguments rely sample compression 
result analysis expectation point running boosting point training error combined hypothesis zero 
experimental drucker cortes quinlan breiman reported realized worthwhile run boosting algorithm point error combined hypothesis zero 
fact doing cause test error decrease surprising completely contradicted intuitions relations training error test error learning algorithms 
fact discoveries latest boosting algorithm adaboost previous boosting algorithms possibly fact algorithm efficient practice generalization properties 
previous boosting algorithms classified arcing algorithms breiman terminology 
interesting see previous boosting algorithms decrease test error training error zero reached 
addition boost majority quite different adaboost way treats outliers suggests worth exploring experimentally 
reason adaboost especially efficient practice adaptive 
previous boosting algorithms receive input advance observing data parameter 
parameter amount believe classification method better random guessing 
formally order proof prediction error combined rule hypotheses generated weak learning algorithm error smaller practice hard know set advance 
aspects problem discuss turn 
breiman remarks appendix case distributions inputs hypothesis error smaller 
reflects limitation original pac framework boosting originally analyzed assumption weak learning uniformly respect distributions 
boosting algorithms analyzed outside framework done adaboost theoretical framework analysis expanded better reflect real world problems 
cost expansion general framework lose clear simple definition weak learner words degree learning algorithm boosted characterized algorithm gives small errors run adaboost unsatisfying characterization involves base classification method boosting algorithm characterization involves classification method 

may possible boost majority cases weak learning algorithm depends input distribution 
fact need know advance may really big problem able run algorithm times perform binary search largest value works 
important computational efficiency problem 
problem number iterations required boost majority grows somewhat different framework give extended analysis boost majority degree overcomes difficulty section 
kong dietterich definitions breiman definitions stumps stumps name boost bag boost bag boost bag boost bag waveform bias var error twonorm bias var error bias var error ringnorm bias var error table bias variance experiments boosting bagging synthetic data :10.1.1.57.5909
columns labeled dash indicate base learning algorithm run just 
example need tens thousands boosting iterations 
problem fixed adaboost take advantage iterations error weak hypotheses smaller gain large computational efficiency 
point subtle main reason adaboost efficient real world problems ultimately reason far played dominant role application boosting real world problems 
development boosting algorithms result continuous interaction practical theoretical considerations demonstrates importance interaction modes research 
breiman response represent chapter interaction 
bias variance explanation breiman analysis bagging boosting decomposition expected error combined classifier bias term variance term 
difficulties analysis boosting details bartlett lee 
bias variance decomposition originates analysis quadratic regression 
application classification problems problematic reflected large number suggested decompositions addition breiman :10.1.1.57.5909
unavoidable problem voting independently generated rules increase decrease expected error 

cases voting independent classifiers guaranteed decrease expected error case voting bagged classifiers 
analysis bootstrap estimation underlies bagging asymptotic guarantees hold real world sample sizes 

performed analysis behavior bagging boosting top quinlan decision tree algorithm algorithm call stumps generates best rule test single feature level decision tree decision stump computed bias variance methods synthetic problems breiman 
definitions bias variance kong dietterich breiman :10.1.1.57.5909
results summarized table 
clear bagging error curves cumulative margin distribution graph boosting letters dataset 
variance reducing procedure boosting reduce variance bias 
evident experiments stumps learning algorithm high bias 
experiment ringnorm data boosting stumps increases variance time decreases bias sufficiently reduce final error 
experiments demonstrate variance reduction completely explain performance boosting 
margins explanation bartlett lee describe alternative explanation fact boosting decrease test error training error zero 
sketch explanation 
assume sake simplicity problem binary classification problem possible labels sign 
denote input prediction th rule correct label weighted vote rule generated adaboost written 
natural think 
vote correct example measure confidence prediction 
intuition mind define large positive margin indicates confident correct prediction large negative margin indicates confident incorrect prediction small margin indicates predictions 
claim explanation boosting achieves zero training error goes generate combined hypothesis margin large examples training set large margin causes decrease generalization error 
example experiment ran adaboost top letters dataset breiman 
left shown training test error curves lower upper curves respectively combined hypothesis function number trees combined 
test error dataset run just 
test error boosting trees 
error rates indicated horizontal grid lines 
just trees combined training error combined hypothesis dropped zero test error continues drop round round 
indicated explanation phenomenon distribution margins training examples 
visualize margins plotting cumulative distribution plot fraction examples margin function 
right side show cumulative margin distributions correspond experiment described 
graphs show margin distributions iterations indicated margin short dashed long dashed hidden solid curves respectively 
main observation boosting tends significantly increase margins training examples training error reaches zero 
case training error remains unchanged zero round margin distribution changes quite significantly iterations examples margin larger 
comparison round examples margin 
experimental theoretical evidence margin explanation effectiveness boosting 
examining margin distributions variety problems algorithms demonstrate connection generalization error distribution margins training set 
back empirical observations theoretical explanation parts prove sufficiently large training sets bound generalization error function margin depend number base hypotheses combined boosted hypothesis 
second prove training errors base hypotheses sufficiently small boosting guaranteed generate combined hypothesis large positive margins examples 
open problems breiman demonstrates effectiveness perturb combine methods reducing classification error 
lot understanding gained questions remain 
questions particularly interesting relation randomized effect bagging deterministic effect boosting 
effects separated experiments 
alternatively unified theory provable theorems explains boosting bagging single framework 
characterize learning algorithms data generation processes benefit boosting bagging combination 
resampling best way randomly perturbing training data 
adding random noise label features 
analyze effects 
quinlan reported unusual cases boosting increase generalization error base learning algorithm small amount 
characterize predict boosting fail manner 
section discussed explanation theoretically bounding generalization error voting methods bagging boosting 
practical accurate methods estimating generalization error 
leo breiman 
bagging predictors 
machine learning 
leo breiman 
heuristics instability model selection 
annals statistics 
harris drucker corinna cortes 
boosting decision trees 
advances neural information processing systems pages 
sally floyd manfred warmuth 
sample compression learnability vapnik chervonenkis dimension 
machine learning 
yoav freund 
boosting weak learning algorithm majority 
information computation 
yoav freund robert schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences august 
michael kearns leslie valiant 
cryptographic limitations learning boolean formulae finite automata 
journal association computing machinery january 
ron kohavi david wolpert 
bias plus variance decomposition zero loss functions 
machine learning proceedings thirteenth international conference pages 
eun bae kong thomas dietterich :10.1.1.57.5909
error correcting output coding corrects bias variance 
proceedings twelfth international conference machine learning pages 
quinlan 
bagging boosting 
proceedings thirteenth national conference artificial intelligence pages 
ross quinlan 
programs machine learning 
morgan kaufmann 
robert schapire :10.1.1.153.7626
strength weak learnability 
machine learning 
robert schapire yoav freund peter bartlett wee sun lee 
boosting margin new explanation effectiveness voting methods 
machine learning proceedings fourteenth international conference 
robert tibshirani 
bias variance prediction error classification rules 
technical report university toronto november 
valiant 
theory learnable 
communications acm november 
vapnik 
estimation dependences empirical data 
springer verlag 

