supervised learning neural networks feedback networks robert brandt feng lin study supervised learning neural networks 
common practice back propagating error feedback bya separate feedback network topology synapse strengths feed forward network propose new adaptation algorithm supervised learning accomplished back propagation algorithm achieved separate feedback network 
elimination feedback network biological neural systems achieve adaptation means retrograde regulatory mechanisms may exist biological neural systems 
advantage new algorithm simpli es hardware implementation arti cial neural networks 
research supported part national science foundation ecs 
feng lin department electrical computer engineering wayne state university detroit mi tel fax email ece eng wayne edu 
robert brandt intelligent devices avenue glen il 
currently visiting beckman institute university illinois urbana il 
version april 
back propagation algorithm arti cial neural networks proposed ago see parker pineda rumelhart werbos williams researchers speculated analogous adaptation mechanism biological neural systems zipser rumelhart 
consensus crick 
main reason belief back propagation usually described requiring dedicated companion feedback network propagate error feedback see example hertz illustrated 
requirement separated feedback network met biological neural system 
say reciprocal connections rare biological neural systems fact ubiquitous biological neural system satisfy strict requirement exists correspondence connections feed forward feedback networks corresponding connections networks maintain identical strengths adapt 
fact biological neural systems connection neurons composed hundreds synapses 
hand discovered retrograde regulatory mechanisms biological neural systems modulation adaptation means synaptic feedback di usion molecules oxide carbon madison de action second messengers schmajuk schmajuk blair retrograde axonal transport bloom side ects synapse related rna transcription 
question explain discrepancy back propagation algorithm happen biological neural systems 
particular feedback network absolutely necessary adaptation take place suggested back propagation algorithm 
put way error feedback proportional propagated feedback network retrograde mechanism similar discovered 
obtain question focused simple observation relevant error feedback implicit strengths synapses rates change 
precisely show appropriate error feedback proportional derivative sum squares strengths synapses 
observation weintroduce new adaptation algorithm uses implicit error feedback 
algorithm need construct separated feedback network 
con guration neural networks algorithm illustrated 
word algorithm generalized sense mean mathematical description model generating error feedback updating synapse strengths 
property algorithm implementation especially hardware implementation arti cial neural networks simpler previously considered implementation various adaptation algorithms see example lansner lehmann hollis card 
particular absence means adding design involve additional wiring layout neurons 
trainable neuron designed standard unit considering network topology adaptation coe cient 
trainable neurons connected way designer wants 
obviously increases potential designing networks dynamically recon gurable topologies 
algorithm stated discrete time systems di erence equations continuous time systems di erential equation 
algorithm continuous time systems biological neural systems generally run continuous asynchronous concurrent fashion 
algorithm discrete time systems stated proved similar manner di erence equations 
due similarity continuous time version repeat 
preliminary results algorithm brandt lin brandt lin 
adaptation algorithm section main result new adaptation algorithm hierarchical non hierarchical called recurrent networks 
networks considered interconnected arbitrarily standard notation layers suitable notations specify network con guration label particular neuron label particular synapse dn set dendritic synapses neuron set feedback generating synapses neuron pres presynaptic neuron corresponding synapse posts postsynaptic neuron corresponding synapse subscripts associate variables particular synapse neuron ws strength synapse pn membrane potential neuron rn ring rate neuron neuron transmits signal dendrites axon 
signal ow involves generation postsynaptic potentials activation synapse neurotransmitters spatial temporal integration postsynaptic potentials soma triggering action potentials biological neural system connection neurons may consist synapses 
algorithm applies general case 
arti cial neural network synapse connection generally su cient 
case synapse equivalent connection 
axon propagation action potentials axon release neurotransmitters presynaptic terminals 
ring rate rn neuron de ned reciprocal interval 
synapse strength ws synapse assumed proportional quantity neurotransmitters released spike arrives synapse 
assumed short time average postsynaptic potential proportional product synapse strength ws presynaptic ring rate 
membrane potential axon pn thought sum postsynaptic potentials pn dn set membrane potential neuron ring rate rn neuron assumed sigmoidal function membrane potential rn npn sigmoidal function maximal ring rate neuron measure steepness sigmoidal characteristic neuron neurons adapt assume sort direct feedback isavailable neurons 
neurons viewed output neurons 
de ne feedback generating synapse synapse receive direct feedback signal 
set synapses denoted include synapses receiving direct feedback 
error function membrane potentials output neurons 
assume direct feedback signal fn known square error fn pn fn tn pn tn target potential neuron minimize wewould adapt ws ws tn pn de dws postsynaptic adaptation coe cient synapse direct feedback coe cient neurons 
achieved adaptation algorithm 
hierarchical networks 
rst consider hierarchical networks hierarchical network neuron may receive signals preceding neurons 
describe network denote set neurons network introduce partial order exits synapses dn 
assumption state ring rate neuron stable unique 
learning algorithm requiring feedback network provides way minimize error 
theorem consider hierarchical neural network neurons ring rates satisfy equilibrium equations 
adaptation law ensure error decreases monotonically time equation satis ed 
ws posts posts ws ws proof adaptation law satis es ws de dws distinction total partial derivatives total derivative rate changes respect changes independent parameter including changes mediated feedback generating synapses partial derivative include changes 
particular pn fn prove inductively follows 
base need prove holds dendritic synapses output neurons feedback generating synapses 
partial order set neurons de ned non empty 
consists maximal elements respect case total partial derivatives equal posts 
de dws de dws de ws dws induction step consider neurons synapses connected neurons holds 
set neurons de ned non empty satis ed synapses partial order need show holds feedback generating synapses neuron possesses holds dendritic synapses neuron 
denote neuron induction hypothesis ws ws ws dependency ws form de dws posts ws de dws de dws posts 
de rn rn de rn rn ws rn de dws ws de dws posts ws uence de dws de dws de posts hand substituting induction hypothesis ws posts ws de dws ws posts de posts de ws posts de dws non hierarchical networks non hierarchical recurrent networks 
non hierarchical network neuron may receive signals neurons 
assume network converges stable states equilibrium equations xed point stable attractor 
adaptation law equations non hierarchical networks 
network non hierarchical equations give implicit functions ws 
ensure existence implicit functions certain conditions satis ed 
state conditions label neurons synapses proposition subset synaptic adaptations ws mgg implicitly de ned function remaining synaptic adaptations ws fm lgg equations jacobian determinant nonzero det ss gss ss gss matrix mg ss gss posts ws proof de ne hs lg 
combining equations particular mg ws mx mx ws lx gss ws hs gss ws hs ss gss ws hs lx lx gss ws gss ws implicitly de ned mg det ss gss observation proposition corollary 
corollary condition satis ed hierarchical network 
proof hierarchical network label synapses gss ifs matrix ss gss matrix diagonal elements 
det ss gss condition satis ed adaptation law equations minimize error stated 
theorem consider non hierarchical neural network reached equilibrium described equations 
assume jacobian determinant vanish 
synapse strengths network adapt ws posts posts ws ws mg ws fm lg error decrease monotonically time 
fact satis ed synapses 
proof condition satis ed unique solution equation exists 
show ws satis es equation satis ed synapses solution equation unique 
rst take derivative respect ws ri ipi de dws di ws assume dn posts 
dri dws ipi di ws dws kx xij dws xij ipi aj di ws dri dws kx xij dws ys ys posts 
matrix equation 
xk xk dr dws dr dws dr dws ys nth low zk zk dr dws dr dws dr dws zk zk xk xk ys words dri dws show ws satis es 
substituting dpi dri de nition ys de ne ws dri dws wehave ws de nition de dws ws posts ws kx fi posts fi fi gn posts kx ipi kx ipi kx dpi fi dri dri dws fi ipi gk zk zk fk de nition zij gk xk xk gk fk words kx kx ipi di ws gi substituting back wehave hand ws pk posts posts post ipi di ws gi post ppost post ws gi ws post post ppost post posts posts ws posts ws ws theorems see adaptation law independent network topologies 
notations proposed written form hierarchical non hierarchical networks 
furthermore algorithm neuron synapse respond adapt fashion processing asynchronously concurrently 
issue related adaptation neurons close output neurons may correct error feedback delay due delays neurons consider 
assume time constant adaptation determined adaptation coe cient su ciently larger time constant network dynamics delay error feedback little ect adaptation 
noteworthy adaptation coe cient needs set output neurons selected synapses 
corollary synapses reduced ws posts posts ws ws clearly direct feedback signal particular neuron posts adaptation coe cient absent 
noteworthy posts output neuron adaptation algorithm written ws posts ws ws compare algorithm back propagation algorithm ws posts easy see error posts corresponds posts ws ws back propagation algorithm posts generated propagated feedback network 
see unnecessary posts calculated ws ws 
simulation algorithm rigorously proved simulation needed verify proven result issues rate convergence basin attraction readily addressed means simulations 
section discuss general approach simulate neural network algorithm 
assume goal train neural set nonlinear functions yi vi xl domain denote xl ym 
construct network inputs outputs 
number neurons topology connections determine achievable accuracy approximation 
general neurons connections network result accurate approximation 
de ne topological capacity total number connections 
constructing network having largest topological capacity number neurons refer reader brandt lin 
train network vary input time function trajectory repeatedly visit regions uniformly 
denote input neurons output neurons rj xj outputs neural network functions xj ri hi rl hi xl denote rn rn want approximate adapt synapse strengths minimize error nx yi ri simplicity equilibrium equations pn dn rn pn adaptation law isgiven ws ws yn rn fn ws posts formulas train neural network learn illustrate approach considering example 
example consider problem approximating nonlinear function strictly layered network hidden neurons 
hidden neurons connected input neurons connections denoted output neuron connected hidden neurons connections denoted 
see 
note xor problem expressed function 
equilibrium equations adaptation governed simulation take cos cos simulation results shown figures 
hardware implementation eliminating feedback network algorithm allows simpler hardware implementation trainable neural networks 
fact build trainable neuron standard unit shown 
unit realizes equilibrium equation pn adaptation law dt dn rn pn ws ws ws posts standard unit trainable neurons build neural network topology 
proposed new algorithm supervised learning neural networks 
distinct feature algorithm requires separate feedback propagate error feedback 
achieved sacri cing nice properties known back propagation algorithm 
fact shown algorithm appropriate form mathematically isomorphic back propagation algorithm 
algorithm overcome drawbacks back propagation algorithm related biological plausibility hardware implementation 
almeida 
backpropagation perceptrons feedback 
neural computers pp 


oxide retrograde messenger 
science pp 

brandt lin 
supervised learning neural networks explicit error back propagation 
proceedings nd annual allerton conference communication control computing pp 

brandt lin 
supervised learning achieved explicit error back propagation 
proceedings international conference neural networks pp 

brandt lin 
optimal layering neurons 
proceedings ieee international symposium intelligent control pp 

brown keenan kelso 
longterm potentiation synaptic systems hippocampal brain slice 
neural models plasticity byrne berry eds academic press pp 

brown keenan 
hebbian synapses biophysical mechanisms algorithms 
annual review neuroscience pp 

crick excitement neural networks 
nature pp 

de 
pathways regulated neurons 
annual review physiology pp 


biological learning signaling 
proceedings ieee inns international joint conference neural networks vol 
pp 

card 
tolerance analog hardware chip learning backpropagation networks 
ieee transactions neural networks pp 

gardner 
neurobiology neural networks mit press 
hawkins kandel 
learning modulate transmitter release themes variations synaptic plasticity 
annual review neuroscience pp 

hertz krogh palmer 
theory neural computation addison wesley 
hollis 
neural network learning algorithm tailored vlsi implementation 
ieee transactions neural networks pp 

lansner lehmann 
analog cmos chip set neural networks arbitrary topologies 
ieee transactions neural networks pp 

sanchez rodriguez 
cmos adaptive bam chip learning weight refreshing 
ieee transactions neural networks pp 

madison 
mechanisms underlying longterm potentiation synaptic transmission 
annual review neuroscience pp 

mel 
pattern discrimination modeled cortical neuron 
neural computation pp 

parker 
optimal algorithms adaptive networks second order back propagation second order direct propagation second order hebbian learning 
proceedings ieee international conference networks pp 

pineda 
propagation dynamic approach adaptive neural computation neural computation pp 

volterra dale kandel schwartz 
acid second messengers presynaptic inhibition sensory cells 
nature pp 

rethinking neural networks quantum fields biological data lawrence erlbaum associates publishers 
rall 
core conductor theory cable properties neurons 
handbook physiology vol 
eds williams wilkins baltimore pp 

rumelhart hinton williams 
learning internal representations error propagation 
parallel distributed processing explorations microstructure vol foundations rumelhart mcclelland eds mit press cambridge pp 

schmajuk 
stimulus con guration spatial learning hippocampal function 
psychological review pp 

schmajuk blair 
stimulus con guration classical conditioning hippocampal function 
behavioral brain research submitted 
madison 
requirement intercellular messenger oxide long term potentiation 
science pp 

schwartz 
molecular mechanisms memory second messenger induced modi cations protein kinase nerve cells 
annual review neuroscience pp 


neural models classical conditioning theoretical viewpoint 
connectionist modeling brain function developing interface hanson olson eds mit press cambridge pp 

villa de greengard 
redistribution induced release neuro transmitter junction 
journal cell biology pp 

linial richter 
cellular molecular biology presynaptic nerve terminal 
annual review neuroscience pp 

bloom 
mechanisms fast slow axonal transport 
annual review neuroscience pp 

werbos 
regression new tools prediction analysis behavioral sciences 
ph thesis harvard university 
williams 
back propagation associative reinforcement learning 
proceedings ieee international conference neural networks pp 

williams 
theory reinforcement learning systems 
technical report nu ccs northeastern university 
williams lynch bliss 
acid induces long term activity dependent enhancement synaptic transmission hippocampus 
nature pp 


wu 
lan 
cmos current mode neural associative memory design chop learning 
ieee transactions neural networks pp 


acute regulation tyrosine nerve neurotransmitters phosphorylation 
annual review neuroscience pp 

zipser rumelhart 
neurobiological signi cance new learning models 
computational neuroscience schwartz eds mit press cambridge pp 


network con guration back propagation algorithm 
network con guration algorithm network example output error signals error example adaptation example 




block diagram trainable neuron 
