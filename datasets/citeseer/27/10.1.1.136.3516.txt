arxiv cmp lg aug harvard university technical report tr similarity approaches natural language processing thesis lillian jane lee division engineering applied sciences partial fulfillment requirements degree doctor philosophy subject computer science harvard university cambridge massachusetts may lillian jane lee rights reserved 
ii statistical methods automatically extracting information associations words documents large collections text potential considerable impact number areas information retrieval natural language user interfaces 
huge bodies text yield highly unreliable estimates probability relatively common events fact perfectly reasonable events may occur training data 
known sparse data problem 
traditional approaches sparse data problem crude approximations 
propose different solution able organize data classes similar events information event lacking estimate behavior information similar events 
thesis presents similarity approaches general measure similarity kullback leibler divergence information theoretic quantity 
approach build soft hierarchical clusters soft event belongs cluster probability hierarchical cluster centroids iteratively split model finer distinctions 
clustering method uses technique deterministic annealing represents knowledge application soft clustering problems natural language processing 
method cluster words drawn words associated press newswire words encyclopedia find language models built clusters substantial predictive power 
algorithm extends modification domains document clustering 
second approach nearest neighbor approach calculating centroid class essence build cluster word 
compare nearest neighbor approaches word sense disambiguation task find performance far superior standard methods 
set experiments show estimation techniques nearest neighbor model enables achieve perplexity reductions percent standard techniques prediction low frequency events statistically significant speech recognition error rate reduction 
iii years graduate student harvard noticed trend 
aiken computation lab spent days nights currently 
worked bell labs summers institution longer exists 
began catch repetitive strain injury cases broke harvard fellow graduate students called center disease control suggest cause 
aside feel incredibly fortunate 
dream team nlp committee 
stuart shieber advisor absolutely 
best way sum interactions get away 
stuff produced clearer better 
barbara grosz supportive throws mean 
just way fernando pereira 
started research enterprise place truly deserves title mentor 
harry lewis cs experience les valiant alan yuille oral exam committee margo seltzer advice encouragement 
number people grad school process easier deal mike re history bailey baker michael bender alan stan chen ric adam joshua goodman colon carol harlow horwitz andy kehler anne bobby kleinberg david mazieres jeff miller christine nakatani wheeler kathy job buddy ben rocco jen smith nadia carol chris small sophie best listeners entire universe peg schafer keith smith chris thorpe tony yan 
wonderful place am quite grateful having opportunity talk people alshawi ido dagan don hindle julia hirschberg yoram singer tishby david yarowsky 
mention underscores summer students difficult get done david ahn tom chou charles isbell iyer kim knowles mason diana meadows andrew ng marianne shaw ben 
rebecca hwa deserves special mention 
say just calm lillian got stressed 
trips trying break aiken rule 
day ll get giving latex 
deepest mom dad sister charlotte jon kleinberg believed didn described thesis supported part national science foundation 
iri 
gratefully acknowledge support nsf graduate fellowship labs fellowship program bell labs graduate research program women 
iv bibliographic notes portions thesis joint appeared 
chapter distributional clustering english words fernando pereira naftali tishby pereira tishby lee appeared proceedings st meeting acl 
don hindle making available associated press verb object data set fidditch parser verb object structure filter mats rooth selecting data set consisting objects fire discussions david yarowsky help stemming tools ido dagan suggesting ways test cluster models 
chapter portions chapter adapted similarity methods word sense disambiguation 
written ido dagan fernando pereira appear proceedings th meeting acl dagan lee pereira 
alshawi joshua goodman rebecca hwa stuart shieber yoram singer helpful comments discussions 
chapter ido dagan fernando pereira described similarity estimation word cooccurrence probabilities appeared proceedings nd annual meeting acl dagan pereira lee 
slava katz discussions topic doug mcilroy detailed comments doug paul help baseline back model andre michael riley providing word lattices experiments 
contents distributional similarity 








distributional clustering 


maximum 
minimum 







probabilistic clustering methods 

similarity estimation 






confusion probability 


pseudo 
data 
vi 


similarity estimation speech recognition 

evaluation 


vii list figures 


direct object clusters verb fire 

noun clusters associated press newswire 
model evaluation associated press object verb pairs 
pairwise verb comparisons associated press object verb pairs 



effect 
viii list tables 




guy mle 


guy mle 


chapter shall know word keeps firth pg 
considering problem predicting string probabilities 
suppose strings 
grill doctoral candidates 
grill doctoral updates asked determine string 
notice asking strings grammatical 
fact constitute legitimate english sentences 
sentence command ask graduating ph student difficult questions second order take lists people just received throw 
methods assigning probabilities strings called language models 
thesis abuse term somewhat refer methods assign probabilities word associations language models 
consider methods estimate probability word cooccurrence relations methods need defined sentences 
example chapters concerned problem estimating probability noun transitive verb appear sentence head noun direct object important application language modeling error correction 
current speech recognizers achieve perfect recognition rates easy imagine situation speech recognizer decide speaker said grill doctoral candidates grill doctoral updates 
language model provide speech recognizer information sentence information help recognizer right choice 
similar situations arise handwriting recognition spelling correction optical character recognition physical evidence may determine corresponding string 
formally physical evidence suppose wish know string message conveyed encoded bayes rule combine estimate acoustic model probability plm assigned language model find posterior probability true string evidence hand plm evidence fixed hypothesized string generally ignored practice 
situation hypothesized strings distin lists basis physical evidence language model provide information necessary disambiguation 
application language modeling machine translation 
suppose needs translate phrase grill doctoral candidates language 
possible target sentences ask applicants questions applicants 
language model furnishes information sentence second absence context providing evidence contrary pick sentence correct translation 
thesis concerned statistical approaches problems natural language processing 
typically statistical approaches take input large sample text may may annotated fashion attempt learn characteristics language statistics sample 
may auxiliary information gained sources online dictionaries wordnet miller 
important advantage statistical approaches traditional linguistic models statistical methods yield probabilities 
probabilities easily combined estimates components equation 
traditional linguistic models hand describe string grammatical 
information coarse grained practical tasks instance grill doctoral candidates grill doctoral updates valid sentences know string far second 
simplest statistical approach language modeling maximum likelihood estimate mle simply counts number times string interest occurs training sample normalizes sample size 
grill doctoral candidates estimate takes form pmle grill doctoral candidates grill doctoral candidates grill doctoral candidates number times phrase occurred number word triples number sentences measure 
notice event interest unseen occur maximum likelihood estimate assigns probability zero 
terms practicality turns fatal flaw sparse data problem quite big large number possible events appear assigning unseen events probability zero mle amounts declaring perfectly reasonable strings zero probability occurring clearly unsatisfactory 
illustrate nature sparse data problem example 
consider set text contained pages indexed altavista digital web search engine digital equipment 
currently set consists web pages extremely conservative estimate means contains words 
time writing phrase grill doctoral candidates occur words mle rule sentence absolutely impossible 
sparse data problem affects low frequency events incorrect infer important 
attempt claim event low probability occur large sample estimating probability zero major error 
aggregate probability unseen events big percentage test data means quite important treat unseen events carefully 
brown 
instance studied word sample english text estimated new sample drawn source distribution trigrams sequences consecutive words occurred large text 
speech recognizer refused accept sentences completely unusable 
historical aside observe noam chomsky declared sparse data problems insurmountable 
fair assume sentence green ideas sleep sleep ideas green occurred 
statistical model sentences ruled identical grounds equally remote english 
nonsensical grammatical chomsky pg 
thought experiment helped field notion interest statistical models language abney 
years chomsky wrote progress sparse data problem 
chomsky statement false assumption statistical model maximum likelihood estimate 
certainly case 
standard language modeling techniques speech recognition jelinek mercer smoothing katz back smoothing estimator guaranteed non zero 
case probability unseen word pair estimated methods incorporate probability word details section 
adequate example word updates appears web pages indexed altavista candidates 
key idea thesis similarity information sophisticated probability estimates sparse data problems occur 
idea intuitively appealing know word candidates similar word occurrence sentence grill doctoral lead believe grill doctoral candidates 
notion similarity explore distributional similarity represent words distributions contexts occur implied quotation opens chapter 
concerned measures distance probability mass functions 
discuss measures chapter main focus kullback leibler divergence information theoretic quantity 
thesis divided parts 
development distributional clustering method grouping similar words 
method builds probabilistic hierarchical clusters objects belong cluster probability clusters broken subclusters hierarchy results 
derive method exhibit clusters method order provide qualitative sense method performs show effective language models constructed clusters produced method 
knowledge probabilistic clustering method applied natural language processing 
second part development computationally efficient way take incorporate similarity information nearest neighbor similar neighbor language model combines estimates specific objects classes 
compare different implementations type model standard smoothing methods find similarity information leads far better estimates 
thesis organized follows 
chapter describes theoretical results employed chapters 
discuss standard language modeling techniques study properties distributional similarity functions 
chapter presents distributional clustering method 
chapter develops nearest neighbor approach compares performance implementations pseudo word disambiguation task 
chapter considers extension nearest neighbor approach studies performance realistic tasks 
conclude brief summary thesis indicate directions chapter 
ironically known false altavista reveals time writing web pages contain sentence contain second 
chapter distributional similarity chapter presents background material underlying thesis 
section argue representing objects distributions natural useful 
section reviews common methods estimating distributions sample 
methods provide initial distributions algorithms standards compare performance similaritybased estimates 
section studies various functions measuring similarity distributions 
pay particular attention kullback leibler divergence cover thomas plays central role 
objects distributions issue address representation objects wish cluster compare 
moment vague sorts objects considering researchers clustered documents salton cutting irises fisher cheeseman :10.1.1.34.6746

general apply different types objects 
second particular object representation easy calculate samples want outside sources information line dictionaries 
second condition expresses preference algorithms adaptable rely knowledge hard computers derive training data algorithms new domains expending considerable effort re acquiring requisite knowledge 
furthermore large samples annotations far common readily obtainable large highly annotated samples working representations adhering second condition tends convenient 
clustering schemes represent objects terms set attributes kaufman rousseeuw 
object associated attribute vector values attributes 
attributes take infinite number values example mean normal distribution real number 
attributes sex patient range finite set 
usually assumptions relationship different attributes 
thesis restricted version attribute representation 
objects equated probability mass functions distributions attribute nonnegative real value require object attribute vectors satisfy constraint ai 
probability object assigns ai 
distributional representation objects particularly appropriate situations arising unsupervised learning learning algorithm infer properties events sample unannotated data 
chapters unannotated data data tagged parts speech 
situations define attributes contexts events occur value ai particular event proportion time event occurred context example suppose wish learn word usage small sample english text rose rose nose 
events words 
define context word word possible contexts nose rose 
attribute vector word occurs nose third attribute times rose attribute twice 
accordance requirement representations distributional representation fairly general 
instance demonstrated words represented distributions subsequent words just easily represent documents distributions words occur customers distributions products buy 
reasonable representation data consists set events words occurring measurements properties list word part speech 
compliance second requirement distributions distributional representation object trivial calculate long contexts easily recognizable 
furthermore wish apply techniques language modeling task probability distributions produced 
constraint components attribute vectors sum unity calculations seen chapter 
initial estimates distributions remainder thesis concerned object distributions estimated object context pairs 
formally set objects consideration set possible contexts yn 
assume data consists pairs counts times occurred training sample 
counts individual objects contexts readily attained counts pairs loss generality assume object context occurs 
wish represent object conditional distribution ally distribution estimated data pairs 
course goal thesis develop estimates need initial distributions start 
particularly simple estimation method maximum likelihood estimate mle pmle pmle 
notice joint event occurs pmle equivalent saying event occur training sample impossible 
noted chapter maximum likelihood estimate tends grossly underestimate probability low frequency events 
alternatives mle jelinek mercer katz church gale take mle initial estimate adjust total estimated probability pairs occurring sample leaving probability mass unseen pairs 
techniques known smoothing methods smooth zeroes distributions 
typically adjustment involves interpolation new estimator weighted combination mle estimator guaranteed nonzero unseen pairs discounting mle decreased create leftover probability mass unseen pairs 
jelinek mercer classic interpolation method 
produce estimate linearly interpolating mle conditional probability object context pair maximum likelihood estimate pmle probability context pmle pmle 
function ranges reflects confidence available data regarding ifx occurs relatively frequently reason believe mle pair reliable 
give high value depends pmle 
hand relatively rare pmle accurate 
case decide rely pmle counts single event higher counts joint event 
set relatively low value 
method training described bahl jelinek mercer 
popular alternative speech recognition literature back discounting method katz 
provides clear separation frequent events observed frequencies reliable probability estimators low frequency events prediction involve additional information sources 
furthermore back model require complex estimation calculations interpolation parameters case jelinek mercer method 
katz uses turing formula replace actual frequency object context pair discounted frequency 
nm denote number pairs occurred times sample 
turing estimate defines nc nc discounted frequency way true frequency mle equation pd consequence estimated conditional probability unseen pair pd probability mass assigned unseen pairs involving object distributed uniformly 
total mass assigned unseen pairs involving simply complement mass assigned seen pairs involving pd 
details see das presents different derivations empirical bayesian empirical turing estimate 
katz alters turing treatment pd unseen pairs 
bases estimate conditional probability unseen pair estimate probability amounts assuming behavior independent behavior jelinek mercer similar assumption set low value equation 
formally write estimate arbitrary pair form katz original presentation convenient chapters note asymmetrical treatment seen unseen pairs pd pr unseen 
pr model probability redistribution unseen pairs 
katz implicitly defines pr probability context pr pd 
chapters take advantage placeholder pr insert similarity probability redistribution models 
quantity normalization factor required ensure pr pr second formulation normalization computationally preferable generally case total number possible pairs far exceeds number observed pairs 
jelinek mercer smoothing katz back smoothing technique 
thorough study chen goodman showed back jelinek mercer smoothing perform consistently back generally yielding better results modeling pairs 
back formulation contains placeholder apply similarity estimates katz estimation method smoothed distributions required 
measures distributional similarity section consider theoretical computational properties functions measuring similarity distributions 
refer functions distance functions similarity functions achieve minimum distributions compared maximally similar identical 
described chapters uses negative exponentials distance functions true similarity functions functions increase similarity increases required 
certainly intend give exhaustive listing distance functions 
see anderberg extensive survey 
purpose simply examine important properties functions commonly employed researchers natural language processing machine learning 
discuss kl divergence section detail forms basis thesis 
describe distance functions including total divergence mean section various geometric norms section similarity statistics section 
pay particular attention computational requirements functions 
view fact wish large data sets require time needed calculate distance distributions linear near linear number attributes 
demand strictly necessary described thesis clustering chapter depends kl divergence similarity computations chapters done preprocessing phrase 
goals find adaptive versions algorithms case functions computed efficiently 
defer discussion confusion probability defined essen steinbiss chapter 
function great importance essen steinbiss occurrence smoothing method quite similar language modeling 
reason include confusion probability chapter function distributions object described conditional probability marginal probability comparing objects involves distributions 
remainder section andx objects associated distributions respectively 
doesn matter distributions estimated 
notational convenience call distributions ands 
occasionally refer distribution corresponding attribute vector yn 
kl divergence define function log specify base logarithm 
limiting arguments lead set log zero 
function goes names literature including information gain nyi error relative entropy cross entropy kullback leibler distance cover thomas 
kullback refers function information discrimination reserving term divergence symmetric function kullback 
name kullback leibler kl divergence thesis 
kl divergence standard information theoretic measure dissimilarity probability mass functions applied natural language processing described thesis machine learning statistical physics 
metric technical sense symmetric obey triangle inequality see theorem cover thomas 
non negative shown theorem 
theorem information inequality equality holding proof 
authors prove theorem jensen inequality deals expectations convex functions notice expected value respect quantity log 
short proof attributed elizabeth thompson green 
ln denote natural logarithm base logarithm 
observe ln equality holding 
write logb ln ln ln ln ln equality holding kl divergence distributions exactly greater really measure dissimilarity mentioned similarity 
yields intuitive explanation expect kl divergence obey triangle inequality hatzivassiloglou mckeown observe dissimilarity transitive 
motivates kl divergence true distance metric 
appeal statistics information theory maximum entropy principle 
statistician kullback derives kl divergence bayesian perspective 
random variable values suppose considering exactly hypotheses hq hypothesis distributed hypothesis distributed bayes rule write posterior probabilities hypotheses hq hq hq hr hr hr hq hr logs equations subtracting obtain log hq hq logp log hr hr consider log information supplies choosing hq hr difference logarithms posterior odds ratio prior odds ratio 
average information choosing hq hr 
measure dissimilarity distributions greater divergence easier average distinguish 
statistical rationale kl divergence cover thomas 
empirical frequency distribution sample length probability mass function py simply number times showed sample divided theorem hypothesized source distribution 
probability observing sample length empirical frequency distribution approximately nd 
see trying decide hypotheses rk empirical frequency distribution observed sample ri gives relative weight evidence favor hypothesis ri 
kl divergence arises information theory measure coding inefficiency 
distributed average codeword length best code entropy ofq logq 
distribution mistakenly encode average codeword length resulting code increase 
divergence large dissimilar inefficient average place look maximum entropy argument 
entropy distribution considered measure uncertainty distributions outcomes uncertain outcome occur described relatively complicated codes 
maximum entropy principle stated jaynes assume distribution underlying observed data distribution highest entropy consistent data pick distribution fewest assumptions necessary 
accepts maximum entropy principle motivate kl divergence manner 
distribution certainly priori maximum entropy distribution 
write logq log log log 
maximizing entropy equivalent minimizing kl divergence prior subject constraint choose distribution fits data 
summarize described motivations kl divergence 
sake broad acceptability bayesian arguments refer priors non bayesian ones 
means reasons 
background see cover thomas kullback general information acz dar axiomatic development nyi description information theory uses kl divergence starting point 
authors brown church hanks dagan marcus markovitch luk mutual information kl divergence joint distribution random variables product distributions 
random variables probability mass functions andg respectively joint distribution function 
log denote sets possible values respectively 
mutual information measures dependence independent implies kl divergence zero information inequality theorem 
give mutual information consideration wish attempt estimate joint distributions 
church hanks consider words associated words occur near sample text hatzivassiloglou mckeown note occurrence adjectives noun phrase means adjectives similar 
information joint distributions carry similarity varies widely different applications generally useful notion 
theoretical reasons justifying kl divergence problem employing practice 
recall distributions infinite nonzero 
know exactly sensible value allows distinguish absolute confidence 
case estimates careful estimates may erroneously set zero effect infinite isnot 
ways problem 
smoothed estimates described approach taken chapter 
calculate kl divergence distributions average distributions 
described chapter computes divergences cluster centroids created averaging class objects 
chapter describes experiments calculate total divergence average examine properties total divergence subsection 
total divergence mean equation gives definition total kl divergence mean appears dagan lee pereira stands average 
empirical frequency distributions defined just theorem test statistic hypothesis drawn distribution 
theorem see equality clearly symmetric function obey triangle inequality shown 
heated debates bayesians non bayesians known 
example skilling pg 
writes valid defence sic non bayesian methods incompetence write convenient form observing log log log sum broken parts sum greater zero sum greater zero 
call sets respectively 
log log log log log log log log similar decomposition holds 
write log log log 
equation computationally convenient involves sums elements elements typically consider situations estimated smaller ratios sum elements negative 
reaches maximum set empty case log 
observation easy see obey triangle inequality 

consider distributions 
log log log log log log log supports disjoint 
violating triangle inequality 
geometric distances think probability mass functions vectors distribution associated vector yn measure distance distributions various geometrically motivated functions including norms cosine function 
functions appear quite commonly clustering literature kaufman rousseeuw cutting sch tze :10.1.1.34.6746
functions true metrics name norm suggests 
norm called manhattan taxi cab distance defined 
clearly interestingly bears relation discovered independently csisz base logarithm function 
consequently convergence kl divergence implies convergence norm 
find tighter bound follows 
dividing sum equation sums defined section obtain 
express form depending elements 
applying triangle inequality see equality set empty 
convenient expression computational point view need sum elements describe experiments distance function chapter 
norm euclidean distance vectors 
denote usual norm function norm bounds norm inequality equation applies norm 
norm appears quite literature kaufman rousseeuw write branches univariate multivariate statistics known long time methods minimization sums averages dissimilarities absolute residuals called methods robust methods sums squares called methods 
computational simplicity methods fact extremely sensitive effect outliers 
pg 
give consideration norm thesis 
turn cosine function 
symmetric function related angle vectors closer vectors smaller angle 
cos notice cosine inverse distance function achieves maximum zero supports disjoint 
functions described just opposite zero greater zero 
analysis geometric properties cosine function geometric similarity functions information retrieval jones furnas 
cosine function efficient compute functions discussed 
numerator requires summing elements elements taken account calculating denominator 
may desirable calculate norms distributions preprocessing step just normalize vectors violate constraint attribute vector components sum 
similarity statistics correlation statistics measuring association random variables anderberg chapter 
known pearson correlation coefficient non parametric measures gamma statistic spearman correlation coefficient kendall coefficient gibbons 
spearman statistic finch chater find syntactic categories kendall statistic appears hatzivassiloglou mckeown henceforth clustering adjectives 
concentrate statistic discuss detail chapter 
kendall coefficient pairwise comparisons 
pair contexts yi yj consider quantities ij yi yj ij yi yj 
pair concordance ij ij sign signs differ quantities zero pair tie concordance 
difference probability observing concordance probability observing ranges 
value corresponds perfect concordance necessarily equality corresponds perfect correlation 
unbiased estimator number observed concordances number observed terms computational efficiency slightly expensive total divergence mean norm 
order calculate number order probabilities assigned rerank probabilities assigned number exactly number discrepancies orderings 
need sort set calculate number discrepancies orderings spend log time calculate similarity optimization noted pair concordance tie 
need sort log operation 
case sparse data significant time savings linear time 
example aid visualizing behavior salient functions described consider twodimensional example 
situation distribution need know value distribution 
plotted values various distance functions respect fixed distribution 
horizontal axis represents probability horizontal axis means distribution 
fixed distribution horizontal axis 
distance distances distribution cos comparison distance functions observed kl divergences total divergence mean norm zero increase travels away cosine function hand decreases travels away demonstrates kl divergence symmetric curve lies curve 
general kl divergence sharp flat distribution divergence flat sharp distribution sharp distribution relatively high values attributes flat distribution resembles uniform distribution 
intuition behavior follows 
assume source distribution second argument flat somewhat odd observe sharp sample distribution 
surprising observe flat sample believe source distribution sharp 
instance suppose source distribution 
probability observing sample length sharp empirical distribution 
source distribution probability observing flat empirical distribution 
interesting feature note curve total divergence mean lower kl divergence curves turn part lower curve 
speculate flatness anda point indicates functions somewhat robust sampling error small results greater change value norm value kl divergence total divergence mean 
summary preview established groundwork results thesis 
explained want distributions represent objects described ways estimate distributions measure similarity distributions 
working conditional probabilities induced objects contexts 
mentioned objects contexts fairly general notions instance object document contexts set words occur document 
confine attention modeling pairs words sets words 
chapters set nouns set transitive verbs indicates number times direct object verb chapter considers bigram case set possible words andc denotes number times word occurred immediately word chapter distributional clustering chapter describes similarity methods estimating probabilities 
probabilistic hierarchical distributional clustering scheme detailed model approach behavior objects modeled class behavior 
chapters describe nearest neighbor approach base estimate object behavior behavior objects similar class construction involved 
attention devoted study clustering techniques books written subject anderberg hartigan kaufman rousseeuw 
traditional applications clustering include discovering structure data providing summaries data 
propose clustering solution sparse data problems grouping data similarity classes create new generalized sources information may consulted information specific events lacking 
wish estimate probability event occurs rarely sample base estimate average behavior events class es class encompasses data points estimates class probability data estimates probability single event 
example suppose wish estimate graduation rate asian american females enrolled high school ohio 
asian american female data infer right rate probably guess 
suppose consider group high schools similar public high schools areas ohio 
average information asian american females attending schools group better estimate 
knowledge clustering algorithms natural language processing literature create hard boolean classes data point belonging class 
words algorithms build partitions data space 
combinatorial demands hard clustering schemes enormous ways group observations non empty sets 
stirling number second kind knuth 
huge number possible groupings small values hatzivassiloglou mckeown observe divide points sets approximately ways 
turns problem finding partition minimizes optimization function np complete brucker surprisingly hard clustering algorithms resort greedy hill climbing search find partition 
greedy hill climbing approaches create initial clustering iteratively local changes clustering order improve value optimization function 
desired number clusters 
update methods initial classes chosen fashion repeatedly move data points class 
number clusters stays iteration 
special cases update methods medoid centroid methods represent clusters data points 
medoids actual data points centroids imaginary data points created averaging object distributions kaufman rousseeuw 
cluster membership decided assigning object closest cluster representative closeness measured distance function 
iteration step consists moving representative order improve value optimization criterion updating cluster memberships 
non update methods number clusters varies course clustering include divisive agglomerative clustering 
divisive algorithms start universal class data points belong iteration involves choosing current set classes split new classes 
agglomerative algorithms contrast data point belonging class iteration step pair current classes merged form new larger class 
case choice class split classes merge generally picking class classes division combination results largest improvement optimization function process stops clusters formed 
divisive algorithms agglomerative algorithms allowed run classes merged readily yield hierarchical clusterings represented dendrograms essentially binary trees 
root dendrogram class containing data points class considered divisive case class formed agglomerative case 
node dendrogram represents class denoted class 
nodes children node iteration step class divided class class class class depending type clustering algorithm 
class hierarchy produced may course interest appealing aspect hierarchical clustering provides attractive solution problem deciding right number clusters 
partitioning methods mentioned generally take number clusters input parameter deciding right number clusters anderberg writes hierarchical clustering methods give configuration number clusters entire data set number entities cluster member pg 

anderberg kaufman rousseeuw express reservations hierarchical methods hierarchical method suffers defect repair done previous steps 
agglomerative algorithm joined objects separated divisive algorithm split 
rigidity hierarchical methods key success leads small computation times main disadvantage inability correct erroneous decisions kaufman rousseeuw pp 
propose novel soft probabilistic hierarchical clustering method overcomes rigidity problem 
data point belonging class assign probabilities class membership data point belonging class positive probability 
reestimate membership probabilities iteration sense data points permanently assigned separate classes 
probabilistic clusterings advantage provide descriptive summary data 
consider situation depicted circle halfway suppose clusters desired 
hard clustering forced associate circles say reports partition convey information just grouped soft clustering hand state belongs cluster cluster equal probability express ambiguity situation 
ambiguous case brief clustering method centroid probabilistic divisive hierarchical algorithm associating objects learning distributions 
class represented centroid placed cluster weighted center mass object centroid calculate membership probability belongs method begins creating single centroid object belonging centroid probability iteratively splits current centroids membership probabilities 
creation child centroids parent centroids creates hierarchy classes obvious way 
decided section objects data points centroids represented distributions set contexts 
kl divergence discussed length section distance function 
optimization function free energy quantity motivated statistical physics algorithm uses deterministic annealing find phase transitions free energy splits cluster centroids transitions 
time update annealing parameter reestimate location cluster centroids membership probabilities object 
shall especially interested problem clustering words theoretical results described general fashion 
re emphasize clustering method clustering objects described distributions involves employing techniques clustering documents 
evaluate method tasks involving prediction object verb pairs find greatly reduces error rate especially cases traditional methods katz back method see section fail 
word clustering methods automatically classifying words contexts scientific practical interest 
scientific questions arise connection distributional views linguistic particularly lexical structure relation question lexical acquisition 
practical point view word classification addresses issues data sparsity generalization statistical language models especially models decide alternative analyses proposed grammar 
known simple tabulation frequencies certain words participating certain configurations example frequencies pairs transitive main verbs head nouns verbs direct objects reliably comparing likelihoods different alternative configurations 
problem large samples number possible joint events larger number event occurrences sample events occur rarely 
frequency counts yield unreliable estimates probabilities 
hindle proposed dealing data sparseness problem estimating likelihood unseen events similar events seen 
instance may estimate likelihood particular adjective modifying noun likelihoods adjective modifying similar nouns 
requires reasonable definition noun similarity method incorporating similarity probability estimate 
hindle proposal words similar strong statistical evidence tend participate events 
notion similarity agree intuitions cases clear notion construct word classes corresponding models association 
chapter build similarity probability model parts model association words certain hidden classes model behavior classes 
researchers built models preexisting sense classes constructed humans example resnik uses wordnet yarowsky works roget thesaurus 
mentioned chapter interested ways derive classes directly distributional data 
resnik thesis contains discussion relative advantages approaches resnik 
follows consider sets words set nouns set transitive verbs 
interested object verb relation pair denotes event noun occurred head noun direct object verb raw knowledge relation consists frequencies particular pairs required configuration training corpus 
form text analysis required collect pairs 
counts experiment derived newswire text automatically parsed hindle parser fidditch hindle 
constructed similar frequency tables help statistical part speech tagger church tools regular expression pattern matching tagged corpora yarowsky 
compared accuracy coverage methods studied biases introduce took care filter certain systematic errors instance subjects complement clauses report verbs say incorrectly parsed direct objects 
consider problem classifying nouns distribution direct objects verbs converse problem formally similar 
noun classification problem empirical distribution noun conditional density pmle denotes number times event occurred training corpus 
problem study pmle classify classification method construct clusters cluster membership probabilities 
cluster associated cluster centroid distribution discrete density obtained computing weighted average noun distributions pmle 
move freely describing noun centroid 
cluster nouns conditional verb distributions pmle need measure similarity distributions 
purpose kl divergence section log kl divergence natural choice variety reasons discussed section 
mentioned measures inefficient average code encode variable distributed respect problem pmle gives loss information centroid distribution empirical distribution pmle modeling noun furthermore minimizing kl divergence yields cluster centroids simple weighted average member distributions shall see 
technical difficulty infinite 
due sparse data problems case pmle zero particular pair 
sidestep problem smoothing zero frequencies methods described section 
satisfactory goals precisely avoid data sparsity problems grouping words classes 
turns difficulty avoided clustering technique computing kl divergence individual word distributions calculate divergences word distributions cluster centroids 
centroids average distributions guaranteed nonzero word distributions 
useful advantage method techniques need compare pairs individual objects estimates individual objects prone inaccuracies due data sparseness 
organization rest chapter follows 
develop theoretical basis clustering algorithm section 
example clusterings section order get sense qualitative performance algorithm 
section presents evaluations ability cluster probability estimation method estimate word pair probabilities especially situations data sparse show method job modeling 
section review nlp community clustering words briefly touch soft clustering methods fields 
theoretical basis general problem seen learning joint distribution large sample 
training data sample independently drawn pairs si xji assume xj yl sample train model information 
line argument section proceeds follows 
set general form cluster probability model 
determine principles minimum distortion maximum entropy guide search proper parameter settings model combine principles free energy function 
sections go details set parameters maximizing entropy minimizing distortion 
section describes searching phase transitions free energy yields hierarchical clustering 
order estimate likelihood sample need probability model 
find set clusters represented cluster centroid conditional distribution decomposed 
probability belongs probability centroid distribution stated centroids representative objects form distribution just objects 
ideally objects belong strongly cluster similar 
equation estimate probability average centroid distributions weighting probability belongs markovian assumption association solely clusters conditionally independent cluster model drastically reduces dimension model space number pairs lower number possible pairs 
decomposition equation write likelihood assigned 
assume marginals part model considered fixed indicate write 
loss generality assume greater zero order flesh equation need find suitable forms cluster membership distributions centroid distributions 
guided principles model fit data model useful second model assumptions possible model general 
goodness fit determined distortion model 
equation estimates probability randomly selecting cluster distribution estimate conditional probability recall section pmle measures inefficiency distribution maximum likelihood distribution code distortion average coding loss incurred model notational shorthand pmle 
turns distortion equation give information find closedform expressions membership probabilities 
fact constraints cluster system minimizes distortion centroid placed top object object belonging centroid coincides 
add requirement membership assignments fewest assumptions possible probability object belongs centroid higher needs 
requirement corresponds maximum entropy principle described section 
wish maximize configuration entropy log average entropy membership probabilities 
combine distortion entropy single function free energy appears statistical mechanics rose gurewitz fox 
function arbitrary maximum entropy points see section show 
special interest points represent balance force maximizing entropy ordering force minimizing distortion 
fact statistical mechanics probability finding system configuration negative exponential system minimal free energy configuration 
free parameter interpretation leave 
suppose fix number clusters 
clearly local minima occur entropy local maximum simultaneously distortion local minimum critical points need correspond critical points 
difficult jointly maximize entropy minimize distortion location cluster centroids affects membership probabilities vice versa andthe independent 
simplify search minima breaking estimation process steps 
hold distortion centroid distributions fixed maximize entropy subject constraints 
distortion regarded constant step maximizing entropy corresponds reduction free energy 
second fix membership probabilities values derived step treat entropy constant 
find critical point respect centroid distributions turns critical point fact minimum distortion free energy reduced 
moving centroid distributions may change values membership probabilities maximize entropy repeat steps stable configuration reached 
step estimation iteration reminiscent em estimation maximization algorithm dempster laird rubin commonly find maximum likelihood solutions 
continue review notation sections 
model probabilities marked tilde 
model parameters membership probabilities centroid distributions 
object marginal probabilities considered part model regarded positive constants 
centroid marginals form ensures 
empirical frequency distributions denoted pmle considered fixed data 
assumption exists object pmle 
quantity shorthand kl divergence pmle 
summarize information table 
quantity value notes 
determined 
determined fixed positive values determined determined pmle pmle fixed data pmle determined table summary common quantities natural logarithms chapter base logarithm function base substantially alter results extra constant factors expressions 
subsections assume number clusters fixed 
maximum entropy cluster membership section addresses parameter estimation step finding cluster membership probabilities maximize configuration entropy reduce free energy assuming distortion centroid distributions fixed suffice simply hold centroid distributions fixed see equation distortion depends membership probabilities 
assumption centroids assumption justified section 
implementation sets pmle interested distributional modeling regard frequencies particular nouns 
recall definition configuration entropy equation log 
wish maximize quantity subject constraints normalization constraint distortion constraint constant take variation function lagrange multipliers 
important note multiplier normalization term free energy 
calculate partial derivative respect membership probability fixing centroid distributions means independent association fixed distortion 
log log problem division assumed object marginals positive 
critical points wehavethat 
allows solve 
meant insure normalization set value satisfied def zx standard notation partition normalization functions name comes german 
closed form solution membership probabilities 
shown jaynes exponential form gives just critical point maximum entropy maximum entropy estimate membership probability desired 
expression intuitively satisfying membership probabilities dependent distance kl divergence sense farther belongs furthermore centroid distributions fixed positive values defined means membership probabilities positive object degree association cluster 
need calculate sum time update membership probabilities 
object distributions sparse computation significantly faster 
pleasing relationship expression estimate theorem restated zx theorem hypothesized source distribution 
probability observing sample length empirical frequency distribution approximately nd 
maximum entropy membership probability zx corresponds probability observing object distribution pmle source distribution assumed centroid replaced sample size regard lagrange multiplier free parameter sense control sample size 
high value express strong belief maximum likelihood estimate pmle case large sample probability belongs centroid negligible small 
conversely low value equivalent small sample case trust mle allow relatively distant section describes vary order derive hierarchical clustering 
conclude section observing maximum entropy membership probabilities free energy rewritten follows log log zx substitution step justified ensured normalization maximum entropy membership probabilities 
simple differentiation easy see set gives equations desired 
minimum distortion cluster centroids proceed second estimation step 
fix membership probabilities maximum entropy values calculated configuration entropy considered constant expression free energy equation 
membership probabilities fixed individual centroid distributions independent just need find values minimize subject constraint centroids 
find critical point equations prove lemma critical point fact minimum showing minimizes distortion order find critical point take partial derivatives lagrange multiplier expression partial derivative respect calculated follows zx zx variation respect 
pmle log pmle pmle centroid distribution term reappears 
substituting pmle pmle critical point partial derivative allows solve pmle multiplier meant enforce constraint pmle 
substitution obtain centroid distributions pmle 
natural expression cluster centroid average data points weighted bayes inverse probability belongs bayes inverses positive maximum entropy membership probabilities centroid distribution zero assume pmle nonzero clear time required update centroid distributions computation faster object distributions sparse 
expression gives unique critical point entropy held fixed free energy reduced point 
goal look minima entropy held fixed suffices show centroid distributions yield minimum distortion lemma 
lemma distortion exactly critical point respect centroid distributions critical point unique minimum assuming cluster membership probabilities fixed 
proof 
pmle pmle log centroid distributions independent membership probabilities fixed sufficient maximize centroid quantity dc pmle log log pmle log log pmle depend 
logarithm strictly increasing function need find maximum product 
observe product logarithm equation rewriting continuous domain closed bounded 
know analysis achieves maximum minimum domain 
clearly point boundary domain yields minimum value zero unique critical point maximum minimum fixed membership probabilities determine entropy critical point critical point distortion constant 
centroid distributions define unique critical point distortion application lemma tells minimum succeeded finding centroid distributions minimize distortion reduce free energy 
hierarchical clustering previous sections developed maximum entropy estimates membership probabilities minimum distortion estimates centroid distributions exp zx pmle 
search minima fixed step iteration described section 
set membership probabilities maximum entropy values current centroid distributions 
plug membership probabilities update centroid distributions 
repeat step cycle parameters converge steady states 
step iteration lets find cluster centroids membership probabilities fixed number clusters 
shown number clusters chosen 
inclusion parameter free energy expression suggests deterministic annealing procedure clustering rose gurewitz fox number clusters determined sequence phase transitions continuously increasing annealing schedule 
discussed section plays role similar sample size controls importance distance function 
fruitful think inverse temperature 
high temperature limit low entropy biggest role minimizing free energy system consisting cluster centroid preferred 
low temperature limit high distortion dominates minimum energy configuration centroid placed top data point data point belonging probability centroid coincides 
system cooled point freedom objects associate distant centroids disappeared 
extremes critical values phase transitions occur natural solution involves including centroids 
find phase transitions cluster twin centroid small random perturbation 
critical splits membership centroid iterative reestimation procedure andp converge infer really cluster 
critical value centroids diverge giving rise children sketch clustering procedure appears 
start low single cluster centroid average noun distributions guaranteed nonzero 
current set leaf clusters corresponding current free energy minimum 
refine solution search lowest causes leaf cluster split 
ideally just split critical value practical performance numerical accuracy reasons may splits new critical point 
splitting procedure repeated achieve desired number clusters model cross entropy 
create initial centroid repeat max clusters centroid create twin repeat twins split iterations estimate membership probs estimate centroids centroid split raised quickly lower centroid split raise centroid split raise delete extra twins clustering algorithm gun missile weapon rocket missile rocket bullet gun shot bullet rocket missile root officer aide chief manager clustering examples direct object clusters verb fire properties child detect input serial positions adjacency occurrence relations words general linguistically irrelevant 
pinker pg 
section describe experiments clustering words procedure described previous section 
explained clustering procedure yields value aset clusters minimizing free energy model estimate conditional probability verb noun depends 
recall pair occurred head noun direct object verb example pair thesis write extracted sentence write thesis 
experiment wanted choose small set nouns sure bore relation 
chose set consist nouns appearing frequently heads direct objects verb fire associated press newswire 
corpus chosen nouns appeared direct object heads total distinct verbs noun represented density verbs 
shows words similar cluster centroid clusters resulting cluster splits kl divergences centroids 
seen split separates objects corresponding sense fire cluster ones corresponding personnel action cluster 
second split refines sense projectile sense cluster projector sense cluster 
split somewhat sharp distinguishing contexts occur corpus 
notice rocket close centroids high probability belonging classes soft clustering scheme allows type ambiguity 
note senses refer designations clusters algorithm decide sense cluster 
number material variety mass state ally residence movement number diversity structure concentration number material mass variety speed level velocity size change failure variation structure speed zenith depth velocity number concentration strength ratio pollution failure increase infection structure relationship aspect system number comedy essay piece material salt ring number number variety material cluster essay comedy poem treatise residence state conductor teacher distinction form representation complex network community group conductor vice president editor director complex network lake region navy community network complex state people modern farmer conductor vice president director chairman improvement voyage migration progress control recognition support program operation study investigation voyage trip progress improvement form explanation care control recognition noun clusters encyclopedia second experiment performed bigger data set object verb pairs involving frequent nouns june electronic version encyclopedia words 
shows closest nouns centroid set hierarchical clusters derived corpus 
notice clusters cluster splits correspond natural sense distinctions 
observe general word number close quite cluster centroids 
model evaluation preceding qualitative discussion provides indication aspects distributional relationships may discovered clustering 
need evaluate clustering rigorously basis models distributional relationships 
look kinds measurements model quality kl divergence held data asymmetric model ii performance task deciding verbs take noun direct object data relating verbs noun withheld training data 
evaluation described performed data set extracted words associated press newswire pattern matching techniques mentioned earlier 
collection process yielded verb object pairs 
selected subset involving frequent nouns corpus clustering randomly divided training set pairs test set pairs 
shows closest nouns cluster centroids early stage hierarchical clustering training data 
kl divergence plots aggregate kl divergence data sets cluster models different sizes higher kl divergence worse coding inefficiency cluster model 
aggregate kl divergence pmle 
critical value show aggregate kl divergence respect cluster model sets training set set train randomly selected held test set set test set held data nouns clustered set new 
surprisingly training set aggregate divergence decreases monotonically 
test set aggregate divergence decreases minimum clusters starts increasing suggests larger models 
new noun test set intended evaluate clusters frequent nouns useful classifiers selectional properties nouns general 
characterize new noun maximum likelihood distribution new mle estimated new sample training data new nouns definition don appear 
corresponding cluster membership probabilities new noun form exp new mle zx model probability estimate calculated 
shows cluster model provides nat information selectional properties new nouns overtraining effect pronounced held data involving clustered nouns 
year increase number series state year woman change recommendation payment deal number increase sale recommendation change proposal decision assurance protection approval aid recommendation decision contribution announcement proposal agreement appeal pact seat break control care information assurance aid notice approval permission recognition part place advantage step attack violence failure increase number increase policy demand protest effort strike failure problem loss accident violence abuse problem violation loss failure drop accident number concern view interest policy rule program regulation fear concern number threat number increase view interest program year state week rule regulation policy ban member student woman people year city state year week today program member people student woman week year month day year state thing program member president official leader people man woman member city plane vehicle number year amount weapon gun equipment drug city area house building gun weapon drug weapon equipment material product bomb explosive rock gun weapon missile state group city area city building house number amount rate price number program system nation number production cost rate amount number share percent nation system program year state program noun clusters associated press newswire kl divergence nats decision error train test new number clusters model evaluation associated press object verb pairs exceptional number clusters pairwise verb comparisons associated press object verb pairs decision task evaluated cluster models verb decision task related applications disambiguation language analysis 
task consists judging verbs take noun object occurrences training set deliberately deleted 
test evaluates models reconstruct missing data cluster centroids interested cluster models help solve sparse data problems 
data test built training data previous way experiment dagan marcus markovitch 
randomly picked pairs verb appeared fairly frequently occurrences deleted occurrences pairs training set 
resulting training set build sequence cluster models 
create test set verb deleted pair confusion set created 
model triple asked decide appear noun course need way judging correctness having access true pair probabilities source distribution natural language presumably unknown 
fall back empirical frequencies give rough estimate correct answer 
frequencies known entirely accurate need cluster models choose create confusion sets noun pairs verbs verbs occurred twice original data set prior pair deletion 
reasonably sure whichever verb occurred training set truly higher probability occurrence 
order evaluate performance compare sign log log pmle pmle initial data set 
error rate model simply proportion sign disagreements test corpus 
shows error rates model selected just exceptional triples log frequency ratio differs log marginal frequency ratio exceptional cases especially interesting estimation methods katz backoff method just marginal frequencies initial cluster model represents consistently wrong 
see cluster model tremendously outperforms classic estimation methods exceptional cases potential provide better solution sparse data problem 
furthermore overtraining effects observed largest models considered effects appear exceptional cases 
related scope thesis provide review entire body clustering literature data clustering discussed fields ranging statistics biology 
list journals publish papers subject contains entries classification society north america summary various clustering methods thesis anderberg substantially text submitted dissertation pg 
xiii 
narrow focus subjects clustering methods appearing natural language processing literature section probabilistic clustering methods section 
clustering natural language processing quite methods distributional clustering appeared literature natural language processing community best knowledge soft clustering language processing context 
algorithms describe algorithms fall categories seek find classes corresponding human concepts create classes purpose improving language modeling 
aside note categories correspond orthogonal trends clustering general 
trend readily apparent data mining knowledge discovery find clusters formed 
vein uses optimization criteria concerning cluster structure instance distortion function measures average distance objects centroids 
trend find clusters aid performance task area uses optimization criteria likelihood performance measure 
clustering clusters sake methods goal production clusters test clusterings aid performance task geared finding semantic syntactic classes 
hatzivassiloglou mckeown henceforth notable provide way evaluate goodness semantic clusterings papers example finch chater sch tze merely example clusters state derived classes correspond intuition 
describe hard clustering scheme grouping semantically related adjectives 
treat adjectives distributions nouns modify kendall coefficient studied section measure distance distributions 
optimization function formedness rewards partitions minimize average distance adjectives cluster 
carefully delineate rigorous evaluation method comparing clusterings produced algorithm clusterings produced human judges computing precision recall fallout measure results respect average responses judges account fact humans agree 
interesting feature incorporate negative linguistic similarity information 
simply observing adjectives noun phrase variety linguistic reasons placed class get dramatically better results improvement various performance metrics 
superficial similarities clustering readily apparent 
distributional similarity component system treats adjectives distributions nouns treat nouns distributions verbs 
associated press newswire training data words opposed 
results incomparable goals differ 
explicitly aim create classes semantically related words solicit human judgments 
constrained human limitations clustering adjectives 
hand interested clusterings improve performance great deal data 
independent body seeking build classes corresponding human intuitions field language clustering 
researchers comparative study problem create hierarchical clusterings correspond evolution splitting languages time 
black kruskal give short history bibliography field 
clustering language modeling large number papers written class models improve language modeling papers appear icassp proceedings sig 
common approach group words parts speech 
reason believe classifications parts speech optimal respect language modeling performance look papers novel clustering techniques 
methods discuss attempt create probabilistic models strong predictive power surprising guided maximum likelihood principle 
known class method brown 

setting set objects set contexts pair denotes appearance word sequence training sample 
brown assume minor criticism number clusters create parameter system humans free choose number clusters wished 
boolean clustering data word belongs class isthe membership function 
class probability estimate takes form 
membership function parameters determined sample frequencies function needs estimated 
done attempting find class assignments maximize average mutual information clusters limit equivalent maximizing likelihood tn training text lc log tn entropy unigram single word distribution consider fixed 
serious problem brown face way calculate estimates 
iteration step agglomerative clustering algorithm forced try different merges classes find yielding best improvement amount care able derive algorithm takes time iteration step setting iteration steps take time significant savings number clusters small relative number words 
desired number clusters achieved brown shift words cluster cluster order compensate premature groupings words class rigidity problem referred quotation kaufman rousseeuw earlier chapter 
create soft clustering compensate words incorrectly classed 
rate brown method potentially involves wasted computation bad merges shifts tried guaranteed step take reduces free energy 
brown alternative algorithm spends time iteration 
algorithm sorts words frequency puts top classes 
iteration step consists adding frequent word clustered new class finding best merge new set classes merge taken system clusters 
hand possible heuristic narrows search missed 
may explain small perplexity reduction achieved brown method brown corpus model interpolates class model word estimators 
commonly cited class language modeling method kneser ney 
respects kneser ney quite similar brown probability model heuristics employed speed calculations 
optimization criterion differs derived maximum likelihood principle 
agglomerative clustering algorithm kneser ney start desired number clusters operation undertaken improve clustering move words cluster searching move biggest improvement 
running time iteration step 
reports kneser ney method achieves perplexity improvements wall street journal data respect katz back method 
result 
class model uses smoothing method known absolute discounting ney essen 
interesting question performance due smoothing method due clustering brown smooth data comparison done class method absolute discounting method 
probabilistic clustering methods papers discuss notion probabilistic clustering ruspini inspired zadeh fuzzy sets zadeh 
method attempts find membership probabilities calls degrees optimize certain formedness conditions similar distortion function attempt mathematically derive estimates search parameter settings consists repeatedly altering membership probability keeping fixed 
furthermore method relies distances objects objects average distributions 
poses problem case considers artificial problems true distances known 
practice estimates inter object distances quite sensitive noise centroid methods overcome problem averaging points 
fuzzy means method bezdek generalization means approach bears resemblance procedure 
centroid method euclidean distance function 
centroid distributions depend squares membership probabilities pmle membership probabilities turn depend positions centroids 
optimization function rewards clusterings minimize distance objects centroids formedness condition maximum likelihood criterion fact fuzzy means meant produce probability estimates 
meant produce hierarchical clustering number centroids kept constant iterated estimation process 
clustering procedures similar deterministic annealing approaches include rose gurewitz fox influenced approach hofmann buhmann 
find clusters minimize free energy 
important difference squared euclidean distance kl divergence distance function 
distributional setting considering kl divergence motivated entirely clear norm meaningful 
bayesian methods wallace dowe cheeseman stutz combine wellformedness constraints performance criteria 
seek find model maximum posterior probability data posterior probability product model prior likelihood model assigns data 
prior structure cluster system general encodes bias fewer clusters prior serves balance tendency maximum likelihood criteria reward systems large number clusters 
analogous inclusion maximum entropy condition derivation method maximum entropy criterion tends favor having fewer clusters 
methods wallace dowe cheeseman stutz yield cluster hierarchies number clusters allowed fluctuate iteration step iteration step 
class hierarchy described hanson stutz cheeseman consist classes attributes node dendrogram represents collection parameter settings inherited descendents node 
described novel clustering procedure probability distributions group words participation particular grammatical relations words 
method builds hierarchy probabilistic classes iterative algorithm reminiscent em 
resulting clusters intuitively informative construct class word models substantial predictive power 
clusters derived proposed method cases semantically significant intuition needs grounded rigorous assessment 
addition predictive power kind carried worthwhile compare automatically derived clusters human judgements suitable experimental setting suggested hatzivassiloglou mckeown 
general development methods directly measuring cluster quality open research area problem compounded takes hierarchical clusterings account 
possible direction take move domains 
instance document clustering studied researchers field information retrieval 
renewed interest document clustering browsing aid search tool see cutting 
short discussion way organize documents yahoo 
provides hierarchical clustering documents appear class 
situations large clustering algorithm may somewhat slow extra descriptive power provided probabilistic clustering may worth extra computational effort 
chapter similarity estimation previous chapter looked cluster centroids source information data specific event lacking 
chapter introduces alternative model look events similar convenience refer new type model similarity clustering method preceding chapter notion similarity 
previous chapter described method automatically clustering distributional data showed clusters derived construct effective models predicting probabilities situations data lacking 
clustering method divisive system started just cluster centroid temperature slowly lowered phase transitions caused cluster centroids split 
splitting centroids meant number clusters determined possible numbers clusters considered fairly efficient fashion best configuration chosen cross validation 
clustering algorithms keep number clusters constant estimation process way try different numbers clusters re run algorithm different value time 
generally case algorithms results computation aid computation different search right efficient 
interesting alternative nearest neighbor approach event probability need estimate consult events similar sense allow event form centroid class avoid having find right number clusters 
approach reduce size model parameter space class approaches avoids generalization class models fall prey 
dagan marcus markovitch argue class information model specific events may lead loss information 
probabilistic clusterings ameliorate problem somewhat combining estimates different classes membership probabilities weight class estimates appropriately concern generalization valid 
small example generalization problem clearer 
shows situation objects empty circles centroid grey circle middle 
assume trying model behavior cluster centroid model estimate behavior depends behavior centroid dependence indicated arrow centroid behavior centroid average behaviors points including andc indicated arrows pointing centroid 
estimate depends relatively close farther way 
sense points trying estimate centroid overgeneralization turn attention chapter similarity language modeling techniques require building general classes 
cluster model described previous chapter estimated conditional probability object context pair averaging class estimates weighting evidence class degree association new object centered model replaces centroids objects depends similarity originators equation 
similarity estimation language modeling cooccurrence smoothing method essen steinbiss derived acoustic model smoothing sugawara 

karov edelman develop similarity disambiguation method fit framework equation method estimate probabilities relies similarity function calculated iterative process give consideration 
chapter establish proof concept discuss compare ways instantiate equation simple decision task evaluation purposes 
kl divergence prove effective measure dissimilarity 
chapter evaluate similarity model true life tasks test utility method speech recognition complicated version model incorporating heuristics order speed computation 
chapter overview previous chapter goal estimate conditional probability object context pairs concern chapter describe similarity estimation methods general 
section develop common framework methods parameter varies method method similarity function 
section describe various similarity functions 
majority distance functions studied chapter discuss confusion probability appears essen steinbiss 
second part chapter describes evaluation similarity methods 
section introduce problem pseudo word disambiguation task related usual word sense disambiguation problem presents advantages terms ease experimentation 
discussion data construct basic language models comparison basic models look examples get qualitative sense different similarity functions perform 
section presents fold cross validation results similarity methods baseline models 
tests show similarity information quite useful sparse data situations 
particular similarity methods performed better back unigram frequency eliminated factor decision 
interesting phenomenon observe effect removing extremely rare events training set quite dramatic similarity methods 
contrary claim katz events discarded hurting language model performance katz similarity smoothing methods suffer noticeable performance degradation singletons events occur exactly omitted 
chapter base logarithm function 
distributional similarity models similarity language model consists parts scheme deciding similarity information determine probability word pair method combining information similar words course function measuring similarity words 
give details parts sections 
discounting redistribution hold best specific information available 
maximum likelihood estimate mle pmle equation chapter training data yields terrible estimate case unseen word pair pretty sufficient data exists 
katz implementation turing discounting method described chapter provides attractive framework similarity methods uses discounted mle pair occurs data different estimate pair occur pd 
pr unseen recall equation represents modification katz formulation written pr katz 
allows similarity estimates unseen word pairs simply backing probability context observe formulation means similarity estimate unseen word pairs desired 
investigate estimates pr derived averaging information objects distributionally similar combining evidence basic assumption similarity model object similar object behavior yield information behavior lacking average distributions similar objects weighting information furnished particular similarity precisely denote increasing function similarity similar larger lets denote set objects similar discuss exact form paragraph 
general form similarity model consider weighted linear combination predictions similar objects psim 
observe formula predict occur tends occur objects similar considerable latitude allowed defining set 
essen steinbiss karov edelman implicitly set large desirable restrict fashion summing time consuming 
chapter consider various heuristics choosing small set similar words 
heuristics include setting limit maximum size allowing object belong dissimilarity threshold value 
show evidence chapter limiting size set closest objects greatly degrade performance best similarity models 
approach taken chapter psim probability redistribution model equation pr psim 
chapter discuss variation pr linear combination psim estimator 
similarity functions final step defining similarity model choose similarity function 
look section functions chapter measure distance distributions 
functions necessary define weight function reverses direction distance function need weights larger values distributions distant 
section describes properties confusion probability achieve performance results essen steinbiss 
section discusses base language models object distributions computed summarizes properties similarity functions compare 
regardless similarity function chosen order computation equation efficient useful compute matrix similarities xi xj distances xi xj arbitrary distance functions 
distance functions chapter studied functions measuring distance probability distributions 
included kl divergence section total divergence mean section log denotes probability mass function norm section 
functions distance functions decrease similarity increase 
desire weight functions similarity tobe wd experimentally tuned parameter controlling relative influence objects closest high non negligible extremely close low objects somewhat distant contribute estimate 
choice negative exponential form motivated fact probability drawing sample size empirical distribution multinomial nd order exponent theorem section 
distance function total divergence mean negative exponential wa controls relative importance similar objects determined experimentally 
define weight function norm wl playing role wd wa tried exponential form yielded better performance results 
attempt normalize various weight functions take different sets values example wd wa wl normalization necessary evaluation task ignores scale factors 
confusion probability essen steinbiss introduced confusion probability context cooccurrence smoothing language modeling 
cooccurrence smoothing applied grishman sterling problem estimating likelihood selectional patterns 
similarity models essen steinbiss consider choose describe implement model equivalent model best performer 
essen steinbiss report test set perplexity reductions small corpora 
interpolation framework similarity estimate linearly interpolated estimators seen unseen events sake uniformity incorporate confusion probability back framework equation 
confusion probability represents likelihood object substituted object probability contexts pc term required ensure pc 
expression incorporates conditional probabilities marginal probabilities measure distance distributions functions described section 
confusion probability symmetric sense pc identical frequency normalization pc pc measures described may closest object may exist object pc pc shall see section 
insight behavior pc gained bayes rule rewrite expression pc 
form reveals important difference confusion probability functions andl described 
functions rate similar roughly high pc greater large notice case ratio large contradicts back assumption estimate pair unseen 
fact pc called probability implies ranges elementary calculations show fact maximum value maxy 
essen steinbiss choose weight function confusion probability including scale parameter 
base language models discussion referred quantities explaining quantities come 
provided base language model turns subtlety form base language model may take 
discussed section kl divergence undefined exists context greater zero zero 
argues language model smoothed zero 
natural choice back estimate equation 
normalization confusion probability requires base language model consistent respect joint marginal probabilities 
unfortunately back estimate property discounts conditional probabilities altering marginals 
maximum likelihood estimate base language model pc pmle directly compare performances similarity models defined require different base language models 
experimental results section chapter evaluate total divergence mean norm confusion probability pmle base language model 
chapter describes experiments kl divergence distance function back estimate base language model 
features measures similarity listed summarized table 
base lm constraints conditions satisfied probability estimates base language model 
column indicates weight associated similarity function depends parameter needs tuned experimentally 
experimental results evaluated similarity measures described word sense disambiguation task 
method noun verbs decide verb noun direct object 
measure absolute quality assignment probabilities case standard language model evaluation perplexity distance range base lm constraints tune 
ifp log pc maxy bayes consistency table summary similarity function properties reduction defined chapter merely ask method able distinguish alternatives 
able ignore constant factors need normalize similarity measures lie calculate denominator equation 
pseudo word sense disambiguation usual word sense disambiguation task method tested ambiguous word context asked context identify correct sense word 
example test instance sentence fragment bank disambiguation method decide bank refers river bank savings bank alternative 
sense disambiguation clearly important task presents numerous experimental difficulties 
notion sense clearly defined instance dictionaries may provided sense distinctions fine coarse data hand 
needs training data correct senses assigned require considerable human effort 
circumvent difficulties set pseudo word disambiguation experiment sch tze gale church yarowsky general format follows 
construct list pseudo words combination different words word contributes exactly pseudo word 
replace test set corresponding pseudo word 
example choose create pseudo word words take change test data plans take plans take action take action method tested choose words pseudo word 
pseudo word set attractive features 
alternative senses control experimenter 
test instance presents exactly alternatives disambiguation method alternatives chosen frequency part speech 
secondly pre transformation data yields correct answer hand tagging word senses necessary 
advantages pseudo word experiments elegant simple means test efficacy different language models 
data ran evaluation associated press newswire data clustering evaluation described previous chapter 
review set frequent nouns data set transitive verbs observed take noun direct object 
extraction object verb pairs performed regular pattern matching tools yarowsky words associated press newswire automatically tagged parts speech church 
admittedly regular expressions inadequate task filtered results somewhat bad pairs doubtless remained 
training data base language models singletons singletons parameter tuning test data tuning tuning tuning tuning tuning table number bigrams training parameter tuning test sets 
pairs derived building base bigram language models reserving testing purposes 
similarity measures compared require smoothed language models calculated katz back language model maximum likelihood model pmle 
furthermore wished investigate katz claim delete singletons word pairs occur training set affecting model performance katz training set contained singletons 
built base language models summarized table 
singletons pairs omit singletons pairs mle mle mle back bo bo table base language models wished test effectiveness similarity information unseen word cooccurrences removed test set object verb pairs occurred training set resulted unseen pairs occurred multiple times 
unseen pairs divided equal sized parts formed basis fold cross validation runs ti performance test set sets combined set tuning parameters necessary simple grid search 
test pseudo words created pairs verbs similar frequencies control word frequency decision task 
measure performance error rate defined number incorrect choices number ties size test corpus 
tie occurs words making pseudo word deemed equally 
look performance base language models 
error rates summarized table 
mle mle error rates exactly test sets consist unseen bigrams assigned probability maximum likelihood estimate 
chose form pseudo words verbs similar frequencies back models bo bo perform poorly 
back models consistently performed worse mle models chose mle models subsequent experiments 
ran comparisons measures utilize unsmoothed data norm total divergence mean confusion probability 
noted bo data kl divergence performed slightly better norm chapter study mle mle bo bo table base language model error rates pc guy guy role kid kid people lot thing fire thing lot guy man man man doctor mother year girl doctor lot rest friend today son boy way bit son part role rank role rank kid rank table closest words word guy mle base language model 
rank words role kid shown top 
performance kl divergence carefully 
sample closest words section examine closest words randomly selected noun guy measures 
table shows closest words order base language model mle 
overlap closest words closest words little overlap closest words measures closest words respect pc words man lot common 
observe word guy fourth list words highest confusion probability respect guy 
examine case nouns kid role closely 
similarity functions kid second closest word guy role considered relatively distant 
pc case role highest confusion probability respect guy kid th highest confusion probability 
accounts difference hand pc 
table gives verbs occur guy kid role indicates rate words similar tend cooccur words observe verbs occur kid occur guy verb play commonly occurs role guy 
sort verbs decreasing guy different order emerges table play verb cooccur role ranked higher get verb cooccur kid indicating role higher confusion probability respect guy kid 
examine effect deleting singletons base language model 
table shows closest words order base language model mle 
relative order object verbs guy see get play give catch tell pick need kid get see take help want tell teach send give love role play take lead support assume star expand accept sing limit table object verbs highest 
boldface verbs occur noun guy base language model mle 
admire bore fool play get table verbs highest guy ratios 
numbers parentheses ranks 
closest words remains words quite different mle 
data suggests effect singletons calculations similarity quite strong borne experimental evaluations described section 
conjecture effect due fact low frequency verbs data 
omitting singletons involving words drastically alter number cooccur similarity functions depend words surprising effect similarity values deleting singletons dramatic 
contrast back language model sensitive missing singletons turing discounting small counts inflation zero counts 
performance similarity methods shows error rate results test sets mle base language model 
parameter set optimal value corresponding parameter training set 
rand shown comparison purposes simply chooses weights randomly 
set equal cases 
similarity methods consistently outperform mle method recall error rate katz back method error rate huge margin conclude similarity information useful unseen word pairs unigram frequency informative 
similarity methods better pc guy guy role kid kid people lot thing fire thing lot guy reason mother break answer man ball reason lot answer doctor job tape boost thing rest ball reporter table closest words word guy mle base language model 
rand indicates simply combine information words arbitrarily quite important take word similarity account 
cases edged methods 
average improvement pc difference significant level paired test 
error rate error rates test sets base language model mle mle bo error rates test set base language model mle 
methods going left right rand pc anda performances shown settings optimal corresponding training set 
values ranged 
values ranged 
results mle case depicted 
see similarity methods achieving far lower error rates mle back rand methods performed best 
omitting singletons amplified disparity pc average difference error rates increases significant level paired test 
important observation methods including rand effective singletons included base language model case unseen word pairs clear singletons ignored similarity models 
recall experiments set point view computational efficiency may desirable sum words experimented closest words varied 
see stopping sufficient capture performance improvement 
appears closest words efficiently sum times fewer words performance penalty stopping pc result increasing error rate 
automatically derived similarity language models provide appealing approach dealing data sparseness 
suggest framework relies maximum likelihood estimates reliable statistics available uses similarity estimates situations data lacking 
described compared performance models standard estimation methods mle method katz back scheme pseudo word disambiguation task 
observed similarity methods perform better standard error rate error rates test sets base language model mle mle boo error rates test set base language model mle 
methods going left right rand pc anda performances shown settings optimal corresponding training set 
values ranged 
values ranged 
methods unseen word pairs method kl divergence mean best 
investigated katz claim build compact language models suffering significant performance degradation discarding singletons training data 
results indicate similarity language modeling singletons quite important omission leads noticeably higher error rates 
error rate error rates training set varied error rates training set varies base language model mle 
set optimal value fora 
chapter similarity estimation speech recognition previous chapter looked performance similarity methods simple disambiguation task 
chapter tackles realistic problems perplexity reduction speech recognition error reduction 
distance function kl divergence base language model back estimate 
similarity model considered chapter model developed chapter added features meant improve performance efficiency 
chapter introduced similarity methods developed general framework compared methods pseudo word disambiguation task 
pseudo word task convenient experimental point view allowed limit number senses pseudo word control probabilities different senses recall chose create pseudo words verbs similar frequencies 
able perform clean experiment demonstrate similarity methods potential outperform standard approaches sparse data problems 
admitted pseudo word disambiguation bit distant problems encountered real word applications 
chapter evaluate similarity method tasks perplexity reduction speech recognition error rate 
perplexity performance metric language modeling systems generally assumed lowering perplexity correlated better performance practice jelinek mercer roukos 
plm probability model sample text 
perplexity pp measures plm models pp plm intuition expression language model assign high probability low perplexity generated unknown source distribution language 
way look regard perplexity measuring average branching text point language model 
example suppose language models 
turns words high probability occurring word san juan jose juan jose cat dog high probability occurring san jelinek note perplexity accurate measure difficulty recognition sample large 
say better language model 
certain words follow san say 
chapter model probabilities pairs adjacent words object verb pairs refers event word sequence bigram xy occurred training sample 
tackle problem bigram language modeling special case gram language modeling grams dominant language modeling technology speech recognition today 
bigram language model probability string words factored product conditional word pair probabilities plm wn plm wi wi 
perplexity bigram model plm respect string wn plm wi wi exp log plm wi wi base logarithm exponential functions chapter chapter 
concern practicality similarity estimation consider heuristics improving efficiency performance similarity models 
particular interested effect limiting number similar words consulted making estimate particular bigram 
heuristic apply interpolate similarity information unigram single word probability katz back method 
find combining estimates improve performance best rely unigram probability result tells similarity information important unigram information 
rest chapter proceeds follows 
section explains modifications similarity model introduced previous chapter 
section presents evaluation results new similarity model achieved reduction perplexity respect katz back model unseen bigrams wall street journal data 
constituted just test sample leading reduction test set perplexity 
experimented application language modeling technique speech recognition yielded statistically significant reduction recognition error 
section points directions research 
similarity model recall general form similarity models developed chapter pd pr unseen 
defined pr psim psim 
chapter simply set set objects similar set computational standpoint somewhat unsatisfactory large 
furthermore case closest objects contribute sum 
experiment chapter limiting size 
introduce parameters define words excluding satisfy need tune experimentally 
kl divergence distance function experiments described provide performance results previous chapter 
recall weight function kl divergence defined parameter controls relative contribution words different distances increases nearest words get relatively weight 
decreases remote words larger effect sum 
tuned experimentally 
preceding chapter set pr psim shall see better smooth psim interpolating unigram probability recall katz 
linear interpolation get pr psim experimentally determined interpolation parameter 
smoothing appears compensate inaccuracies psim mainly infrequent conditioning words 
evaluation shows values small similarity model plays stronger role independence assumption 
summarize construct similarity model interpolate 
interpolated model probability redistribution model pr obtain better estimates unseen bigrams 
parameters tuned experimentally relevant process determine set similar words considered determines relative effect words determines importance similarity model 
evaluation evaluated method comparing perplexity effect speech recognition accuracy baseline bigram back model developed mit lincoln laboratories wall street journal wsj text dictation corpora provided arpa hlt program paul 
baseline back model closely follows katz design discussed section sake compactness singleton bigrams treated unseen recall omission singletons quite detrimental simple similarity models considered previous chapter 
counts model obtained words wsj text years 
perplexity evaluation tuned similarity model parameters minimizing perplexity simple grid search additional sample words wsj text drawn arpa hlt development test set 
best parameter values 
values improvement perplexity unseen bigrams held word sample bigrams unseen just 
improvement unseen bigrams corresponds test set perplexity improvement 
table shows reductions training test perplexity sorted training reduction different choices best ones equation clear computational cost applying similarity model unseen bigram 
lower values computationally preferable 
table see reducing incurs penalty perplexity improvement relatively low values appear sufficient achieve benefit similarity model 
table shows best value increases decreases arpa wsj development corpora come versions verbalized punctuation 
experiments 
training reduction test reduction table perplexity reduction unseen bigrams different model parameters lower greater weight conditioned word frequency 
suggests predictive power neighbors closest modeled fairly frequency conditioned word 
bigram similarity model tested language model speech recognition 
test data experiment pruned word lattices wsj closed vocabulary test sentences 
arc scores lattices sums acoustic score negative log likelihood score case negative log probability provided baseline bigram model 
lattices constructed new lattices arc scores modified similarity model baseline model 
compared best sentence hypothesis original lattice modified counted word disagreements hypotheses correct 
total disagreements 
similarity model correct cases back model 
advantage similarity model statistically significant level 
reduction error rate small number disagreements small compared number errors recognition setup experiments 
table shows examples speech recognition disagreements models 
hypotheses labeled back similarity bold face words errors 
similarity model better modeling regularities semantic parallelism lists avoiding past tense form hand similarity model mistakes function word inserted place punctuation written text 
research model chapter provides modification scheme similarity estimation described preceding chapter heuristics improving speed performance incorporated 
demonstrated augmented model practical speech recognition systems 
discuss possible directions explore 
may possible simplify current model parameters somewhat especially respect parameters select nearest neighbors word 
hand may words simplistic training model differ word word involve massive sparse data problems 
substantial variation base model similarity conditioned words similarity conditioning words 
example essen steinbiss variation considers confusion probability contexts objects essen steinbiss 
noted model equivalent model discussed section uses confusion probability conditioning words commitments 
leaders felt point dollars commitments 
leaders fell point dollars followed france agreed italy followed france greece 
italy aide necessity change exist necessity change exists 
additional reserves reported 
additional reserves reported darkness past church darkness passed church table speech recognition disagreements models model variation perform 
evidence may combined similarity estimate 
instance may advantageous weigh similarity estimate measure reliability similarity function neighbor distributions 
second possibility take account negative evidence hatzivassiloglou mckeown see discussion section 
example frequent followed may statistical evidence put upper bound estimate 
may require adjustment similarity estimate possibly lines rosenfeld huang 
similarity model may applied configurations bigrams 
trigrams necessary measure similarity different conditioning bigrams 
done directly measuring distance distributions form corresponding different bigrams 
alternatively practically may possible define similarity measure trigrams function similarities corresponding words 
similarity models suggest appealing approach dealing data sparseness 
corpus statistics provide analogies words agree linguistic domain intuitions 
previous chapter looked performance various instantiations simple similarity model 
chapter variant provides noticeable improvement katz back estimation method realistic evaluation tasks 
improvement achieved bigram model statistically significant modest effect small proportion unseen events 
bigrams easily accessible platform develop test model substantial improvements obtainable informative configurations 
obvious case trigrams sparse data problem severe 
example doug paul personal communication reports wsj trigrams word vocabulary test set trigrams occurred words training data 
chapter absolute 
history personal recollection occasionally exploding aimed critical bursts sweeps great wave carries pages accelerating tempo leaving sense driving energy spent 
rosenkrantz pg 
viii ways distributional similarity applications natural language processing 
distributional clustering method proved create clusters correspond intuitive sense distinctions lead language model predictive power 
clustering method yields soft hierarchical clusters 
soft clustering language processing context appears novel natural words ambiguous 
nearest neighbor approach combined estimates similar words cluster centroids 
approach advantage computational efficiency need engage iterative estimation necessary clustering 
showed methods kl divergence provided substantial improvement katz back method unseen word pairs noticeable improvement essen steinbiss confusion probability pseudo word disambiguation task 
demonstrate similarity information helpful applications showed extension similarity model produce perplexity reduction speech recognition error rate reduction 
conclude incorporation similarity information potential provide better results area language modeling 
techniques may extend farther 
clustering certainly applicable problems automatic thesaurus construction lexicon acquisition 
fact clusterings produce probabilistic may advantage instance words may appear thesaurus category 
interesting experiment applying techniques problems document clustering indexing mentioned chapter 
brings deeper question 
proper way evaluate inherent quality clusterings opposed measuring performance gain clusterings provide 
need way talk different clustering order analyze competing clustering methods 
move larger larger data sets impractical perform evaluation described hatzivassiloglou mckeown automatically derived classes compared classes created humans 
fruitful direction hierarchical clusterings look edit distances trees see kannan 
key question address formulate adaptive versions algorithms 
methods rely heavily annotated samples easy acquire new training data 
way incorporate new information having restart clustering process distributional clustering case recalculate similarity matrix nearest neighbor case 
clustering fact soft classes may provide answer reestimate membership probabilities new data comes built hard clusters adjust prematurely grouping objects splitting objects apart 
abney abney steven 

statistical methods linguistics 
judith klavans philip resnik editors balancing act 
mit press cambridge ma 
acz dar acz janos zolt dar 

measures information characterizations volume mathematics science engineering 
academic press new york 
anderberg anderberg michael 
cluster analysis applications volume mathematical statistics 
academic press new york 
ass association computational linguistics 

st annual meeting acl columbus oh june 
association computational linguistics morristown new jersey 
bahl jelinek mercer bahl frederick jelinek robert mercer 

maximum likelihood approach continuous speech recognition 
ieee transactions pattern analysis machine intelligence pami march 
bezdek bezdek james 
pattern recognition fuzzy objective function algorithms 
advanced applications pattern recognition 
plenum press new york ny 
black kruskal black paul joseph kruskal 

comparative brief history bibliography key works 
www ntu edu au education langs html 
brown brown peter vincent dellapietra peter desouza jennifer lai robert mercer 

class gram models natural language 
computational linguistics december 
brucker brucker peter 

complexity clustering problems 
rudolf bernhard korte werner editors optimization operations research number lecture notes economics mathematical systems 
springer verlag berlin 
cheeseman cheeseman peter james kelly matthew self john stutz taylor don freeman 

autoclass bayesian classification system 
john laird editor proceedings fifth international machine learning conference pages ann arbor mi june 
morgan kaufmann 
cheeseman stutz cheeseman peter john stutz 

bayesian classification auto class theory results 
usama fayyad gregory piatetsky shapiro padhraic smyth uthurusamy editors advances knowledge discovery data mining 
aaai press mit press menlo park ca pages 
chen goodman chen stanley joshua goodman 

empirical study smoothing techniques language modeling 
th annual meeting acl pages santa cruz ca june 
association computational linguistics morristown new jersey 
chomsky chomsky 
syntactic structures 
number iv 
mouton hague netherlands 
church church kenneth 

stochastic parts program noun phrase parser unrestricted text 
proceedings second conference applied natural language processing pages 
church gale church kenneth william gale 

comparison enhanced turing deleted estimation methods estimating probabilities english bigrams 
computer speech language 
church hanks church kenneth ward patrick hanks 

word association norms mutual information lexicography 
computational linguistics march 
classification society north america classification society north america 

classification literature automated search service 
www pitt edu class html 
cover thomas cover thomas joy thomas 

elements information theory 
wiley series telecommunications 
wiley interscience new york 
cutting cutting douglass david karger jan pedersen john tukey 

scatter gather cluster approach browsing large document collections 
th annual international sigir pages denmark june 
dagan lee pereira dagan ido lillian lee fernando pereira 

similaritybased methods word sense disambiguation 
th annual meeting acl pages madrid spain july 
association computational linguistics association computational linguistics morristown new jersey 
dagan marcus markovitch dagan ido shaul marcus shaul markovitch 

contextual word similarity estimation sparse data 
computer speech language 
dagan pereira lee dagan ido fernando pereira lillian lee 

similarity estimation word cooccurrence probabilities 
nd annual meeting acl pages las cruces nm june 
association computational linguistics morristown new jersey 
dempster laird rubin dempster arthur nan laird donald rubin 

maximum likelihood incomplete data em algorithm 
society 
digital equipment digital equipment 

altavista search main page 
www altavista digital com 
essen steinbiss essen ute volker steinbiss 

occurrence smoothing stochastic language modeling 
proceedings icassp volume pages 
finch chater finch steven nicholas chater 

bootstrapping syntactic categories 
proceedings th annual conference cognitive science society america pages 
firth firth john rupert 

synopsis linguistic theory 
society editor studies linguistic analysis 
blackwell oxford pages 
reprinted selected papers firth edited palmer 
longman 
fisher fisher ronald 
multiple measurements taxonomic problems 
annals 
gale church yarowsky gale william kenneth church david yarowsky 

statistical methods word sense disambiguation 
working notes aaai fall symposium series probabilistic approaches natural language pages 
gibbons gibbons jean dickinson 

nonparametric measures association volume quantitative applications social sciences 
sage publications park ca 
irving 
population frequencies species estimation population parameters 
biometrika 
green green phil 

lecture notes genome sequence analysis university washington class mbt fall 
www genome washington edu mbt lecture edit html october 
notes taken david adams 
available www eecs harvard edu green html 
grishman sterling grishman ralph john sterling 

smoothing automatically generated selectional constraints 
human language technology pages san francisco california 
advanced research projects agency software intelligent systems technology office morgan kaufmann 
hanson stutz cheeseman hanson robin john stutz peter cheeseman 

bayesian classification correlation inheritance 
proceedings th international joint conference artificial intelligence volume pages san mateo august 
international joint conferences artificial intelligence morgan kaufmann 
hartigan hartigan john 
clustering algorithms 
wiley series probability mathematical statistics 
wiley interscience new york 
hatzivassiloglou mckeown hatzivassiloglou vasileios kathleen mckeown 

automatic identification adjectival scales clustering adjectives meaning 
st annual meeting acl ass pages 
hindle hindle 
noun classification predicate argument structures 
th annual meeting acl pages 
hindle hindle donald 

parser text corpora 
sue atkins antonio editors computational approaches lexicon 
oxford university press oxford england chapter pages 
hofmann buhmann hofmann thomas joachim buhmann 

pairwise data clustering deterministic annealing 
ieee transactions pattern analysis machine intelligence january 
jaynes jaynes edwin 
information theory statistical mechanics 
physical review 
jaynes jaynes edwin 
brandeis lectures 
roger rosenkrantz editor jaynes papers probability statistics statistical physics volume synthese library 
reidel dordrecht holland chapter pages 
lectures brandeis university 
jelinek mercer jelinek frederick robert mercer 

interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice amsterdam netherlands north holland may jelinek mercer roukos jelinek frederick robert mercer salim roukos 

principles lexical language modeling speech recognition 
furui mohan sondhi editors advances speech signal processing 
mercer dekker pages 
jones furnas jones william george furnas 

pictures relevance 
journal american society information science november 
kannan kannan 

computing local consensus trees 
sixth acm siam symposium discrete algorithms pages 
karov edelman karov yael shimon edelman 

learning similarity word sense disambiguation sparse data 
rth workshop large corpora 
available cs tr weizmann institute science 
katz katz slava 
estimation probabilities sparse data language model component speech recognizer 
ieee transactions acoustics speech signal processing assp march 
kaufman rousseeuw kaufman leonard peter rousseeuw 

finding groups data cluster analysis 
wiley series probability mathematical statistics 
john wiley sons new york 
david 
inaccuracy inference 
society series 
kneser ney kneser reinhard hermann ney 

improved clustering techniques class statistical language modelling 
european conference speech communications technology pages berlin germany 
knuth knuth donald 
art computer programming volume fundamental algorithms addison wesley series computer science information processing 
addison wesley reading ma second edition 
kullback kullback solomon 

information theory statistics 
john wiley sons new york 
luk luk alpha 
statistical sense disambiguation relatively small corpora dictionary definitions 
rd annual meeting acl pages boston ma june 
association computational linguistics morristown new jersey 
miller miller george 
wordnet lexical database english 
communications acm 
das das arthur 

turing formula word probabilities 
ieee transactions acoustics speech signal processing assp december 
ney essen ney hermann ute essen 

estimating small probabilities 
european conference speech communication technology pages berlin germany 
paul paul douglas 
experience stack decoder hmm csr backoff gram language models 
proceedings speech natural language workshop pages palo alto california february 
defense advanced research projects agency information science technology office morgan kaufmann 
pereira tishby lee pereira fernando naftali tishby lillian lee 

distributional clustering english words 
st annual meeting acl ass pages 
available xxx lanl gov ps cmp lg 
pinker pinker steven 

language learnability language development 
number cognitive science series 
harvard university press cambridge ma 
nyi nyi 
probability theory 
north holland amsterdam 
resnik resnik philip 

wordnet distributional analysis class approach lexical discovery 
aaai workshop statistically natural language processing techniques pages july 
resnik resnik philip 

selection information class approach lexical relationships 
ircs report university pennsylvania philadelphia pa december 
author ph thesis 
rose gurewitz fox rose kenneth eitan gurewitz geoffrey fox 

statistical mechanics phase transitions clustering 
physical review letters 
rosenfeld huang rosenfeld ronald huang 

improvements stochastic language modeling 
darpa speech natural language workshop pages new york february 
morgan kaufmann san mateo california 
rosenkrantz rosenkrantz roger 
preface 
roger rosenkrantz editor jaynes papers probability statistics statistical physics volume synthese library 
reidel dordrecht holland pages vii ix 
ruspini ruspini 
numerical methods fuzzy clustering 
information sciences 
salton salton gerard 

automatic information organization retrieval 
mcgraw hill computer science series 
mcgraw hill new york 
sch tze sch tze hinrich 

context space 
working notes aaai fall symposium probabilistic approaches natural language 
sch tze sch tze hinrich 

part speech induction scratch 
st annual meeting acl ass pages 
sig signal processing society ieee 

icassp atlanta ga may ieee new york ny 
skilling skilling john 

fundamentals maxent data analysis 
brian buck vincent macaulay editors maximum entropy action collection expository essays 
oxford press oxford chapter pages 
sugawara sugawara nishimura kaneko 

isolated word recognition hidden markov models 
proceedings icassp pages tampa florida 
ieee 
joerg 
extended clustering algorithm statistical language models 
technical report dra cis cse rn forum technology dra december 
available xxx lanl gov ps cmp lg 
wallace dowe wallace christopher david dowe 

intrinsic classification mml program 
zhang john dickson lukose editors ai proceedings th australian joint conference artificial intelligence pages nsw australia november 
world scientific 
yahoo 
yahoo 

yahoo 
www yahoo com 
yarowsky yarowsky david 

conc tools text corpora 
technical memorandum bell laboratories 
yarowsky yarowsky david 

word sense disambiguation statistical models roget categories trained large corpora 
coling pages august 
zadeh zadeh 
fuzzy sets 
information control 

