proceedings icml workshop continuum labeled unlabeled data machine learning data mining systems pp washington dc august comparing unifying search similarity approaches semi supervised clustering basu cs utexas edu mikhail bilenko cs utexas edu raymond mooney mooney cs utexas edu department computer sciences university texas austin tx semi supervised clustering employs small amount labeled data aid unsupervised learning 
previous area employed approaches methods utilize supervised data guide search best clustering similarity methods supervised data adapt underlying similarity metric clustering algorithm 
presents unified approach means clustering algorithm incorporates techniques 
experimental results demonstrate combined approach generally produces better clusters individual approaches 

learning tasks large supply unlabeled data limited labeled data expensive generate 
consequently semi supervised learning learning combination labeled unlabeled data topic significant interest blum mitchell joachims nigam 
specifically semisupervised clustering class labels constraints examples aid standard unsupervised clustering focus projects past years wagstaff basu klein xing 
existing methods semi supervised clustering fall general approaches call search similarity 
search approaches clustering algorithm modified labels constraints bias constraints typically specify examples class link different classes link 
search appropriate partition 
done modifying objective function evaluating clusterings includes satisfying constraints demiriz enforcing constraints clustering process wagstaff initializing clusters labeled examples basu 
similarity approaches existing clustering algorithm uses similarity metric employed similarity metric trained satisfy labels constraints supervised data 
similarity metrics similarity semisupervised clustering including string edit distance trained em bilenko mooney kl divergence trained gradient descent cohn euclidean distance modified shortest path algorithm klein mahalanobis distances trained convex optimization xing 
clustering algorithms trained similarity metrics employed semi supervised clustering including single link bilenko mooney complete link klein agglomerative clustering em cohn means xing 
unfortunately similarity search approaches semi supervised clustering adequately compared experimentally relative strengths weaknesses largely unknown 
approaches incompatible applying search approach trained similarity metric clearly additional option may advantages existing approaches 
new unified semi supervised clustering algorithm derived means incorporates metric learning labeled data seeds constraints 
similaritybased search components unified method experimental results comparing combining approaches 
methods semi supervision individually improve clustering accuracy metric learning requires sufficient labeled data beneficial 
metric learning helpful combining seeding constraints results better performance approach 

problem formulation 
clustering means means clustering algorithm iterative relocation partitions dataset clusters locally minimizing total distance data points cluster centroids 
xi xi set data points xid th component xi represent cluster centroids li cluster assignment point xi li 

euclidean means algorithm creates partitioning xl objective function xi xi li locally minimized 
shown means algorithm essentially em algorithm mixture gaussians assumptions identity covariance gaussians uniform priors mixture components expectation particular conditional distribution basu 
denotes observed data denotes current estimate parameter values mixture gaussians model denotes missing data step em algorithm computes expected value log likelihood log conditional distribution bilmes 
maximizing complete data log likelihood assumptions specified shown equivalent minimizing means objective function 
euclidean means formulation distance point xi corresponding cluster centroid li calculated square euclidean distance xi li xi li xi li 
measure distance direct consequence identity covariance assumption underlying gaussians 

semi supervised clustering constraints semi supervised clustering setting small amount labeled data available aid unsupervised clustering process 
pairwise constrained clustering consider framework pairwise link constraints associated cost violating constraint points dataset addition having distances disjoint subsets union points wagstaff 
supervision form constraints generally practical providing class labels clustering framework true labels may unknown priori human expert easily specify pairs points belong cluster different clusters 
means clustering handle pairwise constraints explicitly formulate goal clustering pairwise constrained clustering framework minimizing combined objective function defined sum total square distances points cluster centroids cost violating pairwise constraints 
mathematical formulation framework motivated metric labeling problem generalized potts model kleinberg tardos boykov 
pairwise constrained clustering framework set unordered link pairs xi xj implies xi xj assigned cluster set unordered link pairs xi xj implies xi xj assigned different clusters 
wij wij sets give weights corresponding link constraints link constraints respectively 
dm dc metrics quantify cost violating link link constraints dm li lj li lj dc li lj li lj indicator function true false li cluster labels 
model problem pairwise constrained clustering link constraints formulated minimizing objective function point xi assigned partition xli centroid li xi li wij li lj xi xi xj wij li lj xi xj refer model pairwise constrained means pc kmeans model 

semi supervised clustering metric learning avenue utilizing labeled data involves adapting distance metric employed clustering algorithm 
intuitively allows capturing user view objects considered similar dissimilar 
original data representation may embedded space clusters sufficiently separated modifying distance metric transforms representation dis cluster objects minimized distances different cluster objects maximized 
result clusters discovered learned distance metrics adhere closely notion similarity expressed labeled data clusters obtained untrained distance metrics 
previous xing parameterize euclidean distance sym metric positive definite weight matrix follows xi xj xi li xi li 
restricted diagonal matrix scales axis different weight corresponds feature weighting new features created linear combinations original features 
clustering formulation matrix equivalent considering generalized version means model described section gaussians covariance matrix bilmes 
easily shown maximizing complete data log likelihood generalized means model equivalent minimizing objective function xi li log det xi second term arises due normalizing constant gaussian covariance matrix 
unifying constraints metric learning clustering previous semi supervised clustering cohn xing labeled data learning metric utilized pairwise constraint information learn weights minimize constraint violations 
propose incorporate metric learning directly clustering algorithm way allows unlabeled data influence metric learning process pairwise constraints 
combining objective functions leads objective function attempts minimize cluster dispersion learned metric minimizing number constraint violations xi li wij li lj xi xi xj wij li lj log det xi xj assume uniform weights wij wij traditionally done generalized potts model boykov problem objective function constraint violations treated equally 
cost violating link constraint close points higher cost violating link constraint points far apart 
cost assignment reflects intuition worse error violate link constraint similar points error impact metric learning framework 
multiplying weights wij penalty function fm xi xj max min max xi xj gives cost violating link constraint points xi xj min max nonnegative constants correspond minimum maximum penalties respectively 
set fractions square maximum link distance max xi xj xi xj guaranteeing penalty violating constraint positive 
formulation enables penalty violating link constraint proportional seriousness violation 
analogously cost violating link constraint distant points higher cost violating link constraint points close worse error 
multiplying weights wij fc xi xj min min xi xj max allows take seriousness constraint violation account 
combined objective function xi li log det xi xj xi li lj xi xj xj xi li lj xi xj weights wij wij provide way specify relative importance unlabeled versus labeled data allowing individual constraint weights 
objective function greedily optimized proposed metric pairwise constrained means mpc kmeans algorithm uses means type iteration 

algorithm set data points set link constraints set link constraints corresponding weight sets number clusters form metric pairwise constrained means mpc kmeans finds disjoint partitioning xh partition having centroid locally minimized 
algorithm mpc kmeans components 
utilizing constraints cluster initialization satisfaction constraints cluster assignment step constitutes search component algorithm 
learning distance metric weight matrix algorithm iteration current constraint violations similarity component 
intuitively search technique uses pairwise constraints generate seed clusters initialize clustering algorithm uses constraints guide clustering process iterations 
seeds inferred constraints bias clustering region search space possibly reducing chances getting stuck poor local optima clustering satisfies user specified constraints produced simultaneously 
similarity technique distorts metric space minimize costs violated constraints possibly removing violations subsequent iterations 
implicitly space data points embedded transformed respect user provided constraints capturing notion similarity appropriate dataset user perspective 

initialization generate seed clusters initialization step mpc kmeans take transitive closure link constraints wagstaff augment set adding entailed constraints assuming consistency constraints 
number connected components augmented set 
connected components create neighborhood sets np neighborhood set consists points connected links augmented set pair neighborhoods np np link add link constraints pair points np np augment link set entailed constraints 
overload notation point refer augmented link link sets respectively 
note neighborhood sets np contain neighborhood information inferred link constraints unchanged iterations algorithm different partition sets xh contain cluster partitioning information get updated iteration algorithm 
preprocessing step get neighborhood sets np 
neighborhoods provide initial starting point mpc kmeans algo rithm 
required number clusters select neighborhood sets largest size initialize cluster centers centroids sets 
initialize cluster centers centroids neighborhood sets 
look point connected neighborhood set 
point exists initialize th cluster 
cluster centroids left uninitialized initialize random points obtained random perturbations global centroid 
step mpc kmeans alternates cluster assignment step centroid estimation metric learning step see 
step mpc kmeans point assigned cluster sum distance cluster centroid cost constraint violations possibly incurred cluster assignment minimized 
note assignment step subsets associated cluster may change assignment point 
cluster assignment step point moves new cluster component contributed point decreases 
points new assignment decrease remain 

step step cluster centroids reestimated points xh 
result contribution cluster minimized 
pairwise constraints take part centroid re estimation step constraints explicit function centroid 
term distance component minimized step 
centroid re estimation step effectively remains means 
second part step metric learning matrix re estimated decrease objective function 
updated matrix obtained partial derivative setting zero resulting xi li xi li xi xi xj xi xj wij xi xj xi xj li lj wij xi xj xi xj li lj subsets exclude constraint pairs penalty functions fm fc take threshold values min max respectively 
see appendix details derivation 
estimating full matrix limited training data difficult limit diagonal equivalent learning metric feature weighting 
case th diagonal element add corresponds weight th feature add xid lid wij xid xjd li lj wij xid xjd li lj intuitively term sum xi xid lid scales weight feature proportionately feature contribution cluster dispersion analogously scaling performed computing mahalanobis distance 
second terms depend constraint violations xi xj wij xid xjd li lj xi xj wij xid xjd li lj respectively con tract stretch dimension attempting current violations 
metric weights adjusted iteration way contribution different attributes distance equalized constraint violations minimized 
objective function decreases cluster assignment centroid re estimation metric learning step till convergence implying mpc kmeans algorithm converge local minima 

experiments 
methodology datasets experiments conducted datasets uci repository iris wine representative randomly sampled subsets pen digits letter datasets 
pen digits letter chose sets classes letter pen digits sampling data points original datasets randomly 
classes chosen handwriting recognition datasets intuitively represent difficult visual discrimination problems 
pairwise measure evaluate clustering results 
traditional information retrieval measures adapted evaluating clustering considering cluster pairs algorithm pckmeans input set data points xi set link constraints xi xj set link constraints xi xj number clusters sets constraint weights output disjoint partitioning xh objective function locally minimized 
method 
initialize clusters 
create neighborhoods np 
sort indices decreasing size np 
initialize centroids np initialize centroids np point linked neighborhoods np initialize initialize remaining clusters random 
repeat convergence 
assign cluster assign data point xi cluster set arg min xi log det xi xj xj xi lj xi xj xi lj xj 
estimate means 
update metric xi xi li xi li xi xj wij xi xj xi xj li lj xi xj wij xi xj xi xj li lj 
recision recall 
mpc kmeans algorithm measure recision recall recision recall generated learning curves fold crossvalidation dataset determine effect utilizing pairwise constraints 
point learning curve represents particular number pairwise constraints input algorithm 
unit constraint weights datasets provide individual weights constraints 
maximum square distance link constraints value max min set 
clustering algorithm run dataset pairwise measure calculated test set 
results averaged runs folds 

results discussion figs show learning curves datasets 
dataset compared semi supervised clustering schemes pairwise measure pairwise measure number pairwise constraints mpc kmeans kmeans pc kmeans kmeans 
results iris dataset number pairwise constraints mpc kmeans kmeans pc kmeans kmeans 
results digits dataset mpc kmeans clustering involves seeding metric learning unified framework described section kmeans means clustering metric learning component described section utilizing constraints seeding pc kmeans clustering utilizes constraints seeding initial clusters forces cluster assignments respect constraints doing metric learning outlined section unsupervised means clustering 
datasets unified approach mpc kmeans outperforms individual seeding pc kmeans metric learning kmeans approaches 
learning curves illustrate providing pairwise constraints beneficial clustering quality 
pairwise measure pairwise measure number pairwise constraints mpc kmeans kmeans pc kmeans kmeans 
results wine dataset number pairwise constraints mpc kmeans kmeans pc kmeans kmeans 
results letter ijl dataset wine letter ijl datasets difference methods utilize metric learning mpc kmeans kmeans pc kmeans regular means pairwise constraints indicates absence constraints weighting features variance essentially mahalanobis distance improves clustering accuracy 
wine dataset additional constraints provide improvement cluster quality dataset shows meaningful feature weights obtained scaling variance just unlabeled data 
metric learning curves display characteristic dip clustering accuracy decreases initial constraints provided certain point starts increase eventually outperforms initial point learning curve 
conjecture phenomenon due fact feature weights learned constraints unreliable number constraints provides metric learning mechanism data estimate metric parameters 
hand seeding clusters small number pairwise constraints immediate positive effect final cluster quality providing pairwise constraints diminishing returns pc kmeans learning curves rise slowly 
seeding metric learning utilized unified approach benefits individual strengths methods seen mpc kmeans results 
results indicate unified approach utilizing pairwise constraints clustering outperforms seeding metric learning individually leads improvements cluster quality 

extending approach high dimensional datasets text euclidean distance performs poorly primary avenue research 
currently working formulation utilizes objective function similar spherical means dhillon modha 
weight matrix singular high dimensional data scenario handled regularization 
comparing unified approach search similarity techniques xing cohn area 
planning incorporate active selection pairwise constraints framework similar basu proposed approach 
situations obtaining data labels directly pairwise constraints may possible 
possible infer pairwise constraints scenarios labels number pairwise constraints grows quadratically amount labeled data making training entire set pairwise constraints intractable 
scenarios sampling mechanisms selecting small number meaningful pairwise constraints interesting topic 

new approach semisupervised clustering unifies previous similarity methods 
general formalization semi supervised learning problem allowed develop vari ation standard means clustering algorithm uses supervised data seeds constraints search adapting underlying distance metric 
individual components unified approach experimentally compared individual approaches combination 
small amounts supervised data search approach produces accurate clusters similarity approach 
combining advantages techniques unified approach generally performs better approaches individually 

acknowledgments banerjee insightful comments 
research supported part ibm phd fellowship faculty fellowship ibm nsf iis 
basu banerjee mooney 

semisupervised clustering seeding 
proceedings th international conference machine learning icml 
basu banerjee mooney 

active semi supervision pairwise constrained clustering 
submitted publication available www cs utexas edu 
bilenko mooney 

adaptive duplicate detection learnable string similarity measures 
proceedings ninth acm sigkdd international conference knowledge discovery data mining kdd 
washington dc 
bilmes 

gentle tutorial em algorithm application parameter estimation gaussian mixture hidden markov models technical report icsi tr 
icsi 
blum mitchell 

combining labeled unlabeled data training 
proceedings th annual conference computational learning theory 
madison wi 
boykov veksler zabih 

markov random fields efficient approximations 
ieee computer vision pattern recognition conf 
cohn caruana mccallum 

semi supervised clustering user feedback 
unpublished manuscript 
available www cs cmu edu mccallum 
demiriz bennett embrechts 

semi supervised clustering genetic algorithms 
annie artificial neural networks engineering 
dhillon modha 

concept decompositions large sparse text data clustering 
machine learning 
joachims 

transductive inference text classification support vector machines 
proceedings sixteenth international conference machine learning icml 
bled slovenia 
klein kamvar manning 

instance level constraints space level constraints making prior knowledge data clustering 
proceedings nineteenth international conference machine learning icml 
sydney australia 
kleinberg tardos 

approximation algorithms classification problems pairwise relationships metric labeling markov random fields 
ieee symp 
foundations comp 
sci 
nigam mccallum thrun mitchell 

text classification labeled unlabeled documents em 
machine learning 
wagstaff cardie rogers 

constrained means clustering background knowledge 
proceedings th international conference machine learning icml 
xing ng jordan russell 

distance metric learning application clustering side information 
advances neural information processing systems 
mit press 
appendix objective function xi li log det xi xj xi li lj xi xj xj xi li lj xi xj obtain step updates cluster centroids metric parameterization matrix partial derivatives setting zero 
properties linear algebra 
ax tr 
tr ab bt diag 
log det diag 
diag derivation estimating cluster centroids weight matrix xj xh xj xh tr xi li xi li xi wij max min xi xj max tr xi xj xi xj li lj wij min max xi xj min tr xi xj xi xj li lj log det prop xi xi xj xi xj xi li xi li wij xi xj xi xj li lj wij xi xj xi xj li lj diag xi li xi li xi xj xi xj xi wij xi xj xi xj li lj wij xi xj xi xj li lj xi li xi li xi xj xi wij xi xj xi xj li lj prop prop wij xi xj xi xj li lj prop xi xj 
