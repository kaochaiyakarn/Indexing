input output hmms sequence processing bengio paolo frasconi dept informatique dipartimento di sistemi recherche informatica universite de montreal universita di firenze montreal qc firenze italy september consider problems sequence processing propose solution discrete state model order represent past context 
weintroduce recurrent connectionist architecture having modular structure associates subnetwork state 
model statistical interpretation call input output hidden markov model iohmm 
trained em gem algorithms considering state trajectories missing data decouples temporal credit assignment actual parameter estimation 
model presents similarities hidden markov models hmms allows map input se quences output sequences processing style recurrent neural networks 
iohmms trained discriminant learning paradigm hmms potentially advantage em algorithm 
demonstrate iohmms suited solving grammatical inference problems benchmark problem 
experimental results tomita grammars showing adaptive models attain excellent generalization 
bell laboratories holmdel nj learning problems data interest signi cant sequential structure 
problems kind arise variety applications ranging written spoken language processing production actuator signals control tasks multivariate time series prediction 
feedforward neural networks inadequate cases absence memory mechanism retain past information exible way 
models include delays connections duration temporal contingencies captured xed priori architecture inferred data 
furthermore tasks appropriate size input window delays varies sequence sequence sequence 
recurrent neural networks hand allow model arbitrary dynamical systems store retrieve contextual information exible way durations xed priori vary sequence 
sequence analysis systems take context account exible manner recurrent neural networks hmms nds form state variable representation past context 
state space representation main computations divided updating state context variable state transition function computing predicting output current state output function 
research orts supervised learning recurrent networks exclusively focused gradient descent methods continuous state space 
numerous algorithms available computing gradient 
example back propagation time bptt algorithm straightforward generalization back propagation allows compute complete gradient fully recurrent networks 
real time recurrent learning rtrl algorithm local time produces partial gradient time step allowing line weights updating 
algorithm proposed training local feedback recurrent networks 
local time requires computation proportional number weights back propagation time 
local feedback recurrent networks suitable implementing short term memories limited representational power dealing general sequences 
practical di culties reported training recurrent neural networks perform tasks temporal contingencies input output sequences span long intervals :10.1.1.41.7128
fact proved parametric dynamical system non linear recurrence asa recurrent neural network increasingly di cult train gradient descent duration dependencies captured increases 
problem gradient error function persists regardless gradient computation algorithm bptt employed 
common heuristic solution start training shorter sequences incrementally train longer sequences 
general rules needed deal long term dependencies short sequences 
previous alternative training algorithms suggests root problem lies essentially discrete nature process storing contextual information inde nite amount time :10.1.1.41.7128
potential solution problem propagate backward time targets state space di erential error information 
order gain intuition target propagation suppose oracle available provides targets internal state variable time step 
case learning reduced static learning problem problem learning state output mappings de ne behavior dynamical system state space representation 
course general 
essentially supposes prior knowledge appropriate state representation 
conceive iterative approach repeated steps rst step approximates oracle providing pseudo targets second step ts parameters pseudo target state trajectory output targets 
absence prior knowledge pseudo target state trajectories randomly initialized 
iteration guaranteed produce improvements approximation true targets process may converge useful solution regard output targets speci ed supervision 
rst related approaches probably moving target algorithm rohwer 
moving target approach consists formulating supervised learning optimization problem joint space temporal targets adjustable parameters connection weights 
rohwer proposed solution gradient descent demonstrated experimentally di cult credit assignment tasks solved 
di cult problems method got stuck local minima useful solution obtained 
extending previous propose statistical approach target propagation em algorithm :10.1.1.56.686
consider parametric dynamical system having discrete states introduce modular architecture subnetworks associated discrete states 
architecture interpreted statistical model trained em generalized em gem algorithms dempster laird rubin considering internal state trajectories missing data 
way learning factored temporal credit assignment subproblem static learning subproblem consists tting parameters state output mappings de ned estimated trajectories 
order iteratively tune parameters em gem algorithms system propagates forward backward discrete distribution states resulting procedure similar baum welsh algorithm train standard hidden markov models hmms 
main di erence standard hmms model represent distribution output sequences yt represents condi tional distribution output sequences input sequences ut model called input output hmm iohmm 
iohmms trained maximizing conditional likelihood 
supervised learning problem output sequence plays role desired output response input sequence output represents clas si cation decisions input sequence approach discriminant standard hmms trained maximize likelihood observations 
example applications hmms isolated word recognition special case sequence classi cation separate model con structed word class trained instances class 
type training said discriminant model example word model trained independently try model type observations acoustic representative class word 
discriminant training strategies attempt build best model observations class focus di erences class order better predict observation belongs class 
models trained discriminant approaches expressed degrees freedom concentrate parameters decision surface classes distribution data 
advantage discriminant training criteria tend robust incorrectness model reason perform better 
input output sequences multivariate discrete continuous 
iohmms perform sequence regression continuous classi cation discrete 
example task phoneme recognition may sequence acoustic vectors cepstral parameters may consist discrete sequence phonetic labels 
sequence classi cation tasks isolated word recognition output label class de ned sequence 
potential elds application robot navigation system identi cation time series prediction 
example economic time series input sequences di erent economic time series output sequence prediction values variables hidden states represent di erent regimes economy business cycles 
applications handwriting speech recognition output sequence sequence characters phonemes tobe synchronized input sequence pen trajectory acoustic sequence 
hmms markov assumptions distribution outputs inputs factored sums products types factors output probabilities transition probabilities 
xt ut output distribution state xt input ut time speci es output function dynamical system 

xt xt ut matrix transition probabilities time conditioned current input ut speci es state transition function dynamical system 
simple way obtain iohmm hmm output transition probabilities function input ut time step output distribution ut obtained mixture probabilities component conditional particular discrete state mixing proportions current state probabilities conditional input 
model interesting connections mixture experts architecture jacobs jordan nowlan hinton 
mixture experts sequence regression carried associating di erent modules di erent states letting module data compute expected value output state input xt ut interval time receives credit 
mixture experts task decomposition smooth 
related approach gating switching experts provided expert modules state computing state transition distribution xt xt ut conditioned current input 
connectionist model extending hidden markov models process discrete input output streams proposed modeling distribution output sequence input sequence interesting related models various hybrids neural networks hmms proposed literature 
iohmms neural networks modeling transition output distributions models strictly neural part model feedforward short horizon hmm represent longer term temporal structure case speech prior knowledge structure 
experiments arti cial tasks shown simpli ed version approach deal long term dependencies ectively recurrent networks trained back propagation time alternative algorithms :10.1.1.56.686
model limited representational capabilities map input sequence nal discrete state :10.1.1.56.686
describe extended architecture allows fully specify input output portions data required supervised learning paradigm 
way general sequence processing tasks addressed production classi cation prediction 
organized follows 
section devoted circuit description architecture statistical interpretation 
section derive equations training iohmms 
particular learning algorithm discrete inputs linear subnetworks gem version general multilayered subnetworks 
section compare iohmms related models sequence processing standard hmms recurrent mixture experts architectures 
section analyze theoretical point view learning capabilities model presence long term dependencies arguing improvements achieved adding extra term likelihood function constraining state transitions model 
section report experimental results classical benchmark study grammatical inference tomita grammars 
results demonstrate model achieve generalization training examples 
input output hidden markov models proposed architecture recurrent networks usually continuous state space iohmms consider proba bility distribution discrete state dynamical system state space description xt xt ut xt ut ut input vector time output vector xt ng discrete state 
equations de ne generalized mealy nite state machine inputs outputs may values 
referred state transition func tion function 
consider probabilistic version dynamics current inputs current state distribution estimate output distribution state distribution time step 
admissible state transitions speci ed transition graph fv vertices correspond model states 
describes topology underlying markov model 
transition state state admissible exists edge eij de ne set successors state sj def fi eij 
proposed system illustrated 
architecture composed networks networks oj state output networks uniquely associated states networks share input ut state network nj task predicting state distribution xt xt ut current input previous state xt similarly output network oj computes parameters current input current expected output past input sequence convex weighted sum delay current state distribution convex weighted sum softmax softmax proposed architecture iohmm recurrent mixture experts 
distribution current output system xt ut current state input 
typically output networks compute expected output value xt ut 
subnetworks assumed static de ned means algebraic mappings nj ut oj ut vectors adjustable parameters connection weights 
assume functions di erentiable respect parameters 
ranges functions nj may constrained order account underlying transition graph output ij state subnetwork nj associated successors state layer nj units cardinality 
convenience notation suppose ij de ned impose condition ij belonging sj 
guarantee variables ij positive summing softmax function layer ij sj sj aij intermediate variables thought activations weighted sums output units subnetwork nx ij probabilistic interpretation 
shown outputs state networks recursively compute time step vector represents current memory system interpreted current state distribution past input sequence 
memory variable computed linear combination outputs state networks gated value previous time step nj nx output networks compete predict global output system output subnetwork oj 
nx level description need specify internal architecture state output subnetworks long compute di erentiable function parameters 
probabilistic model hinted connectionist architecture interpreted probability model 
simplify assume multinomial distribution state variable xt probability computed possible value state variable xt 
consider main variable temporal recurrence ifwe initialize vector positive numbers summing interpreted vector initial state probabilities 
convex sum vector positive numbers summing interpretation having denoted xt subsequence inputs time 
making certain conditional independence assumptions described section equation probabilistic interpretation xt nx xt xt ut xt ut subnetworks compute transition probabilities conditioned input ut ij xt xt ut neural networks trained minimize output mean squared error mse output architecture interpreted expected position parameter probability distribution output addition conditional input ut expectation conditional state xt xt ut total probability ut obtained mixture probabilities xt ut conditional state example gaussian output model subnetwork corresponding mean squared error criterion output distribution fact mixture gaussians 
general state distribution predicted set state subnetworks provides mixing proportions xt xt ut actual form output densities xt ut task 
example multinomial distribution suitable sequence classi cation symbolic mutually sive outputs 
gaussian distribution adequate producing continuous outputs 
rst case softmax function output subnetworks oj second case linear output units subnetworks oj 
conditional dependencies random variables input state output involved probabilistic interpretation proposed architecture joint probability 
conditional independency simplify notation write probability discrete random variable takes value introduces ambiguity 
similarly ifx continuous variable denote probability density 
xt xt xt ut ut ut xt xt xt bayesian network expressing conditional dependencies random variables probabilistic interpretation recurrent architecture 
bayesian network standard hmm 
sumptions amount computation necessary estimate probabilistic relationships variables quickly get intractable 
introduce independency model set random variables 
random variables wesay conditionally independent written pair 
dependency model mapping assigns truth values independence predicates form 
listing set conditional independency assumptions prefer express dependencies graphical representation 
dependency model repre sented means directed acyclic graph dag called bayesian network formal de nition bayesian networks 
practice bayesian network constructed allocating node variable creating edge variable believed direct causal impact assumption suppose dag depicted bayesian network depen dency model associated variables xt yt evident consequences independency model previous state current input relevant determine state 
step memory property analogue markov assumption hidden markov models 
fact bayesian network hmms obtained simply removing ut nodes arcs see 
basic di erences architecture standard hmms terms computing style learning 
di erences discussed 
supervised learning algorithm learning algorithm proposed architecture derived maximum likelihood principle 
extension takes account priors parameters straightforward discussed 
discuss case training data set pairs input output sequences independently sampled distribution def def tp pg denote vector parameters obtained collecting parameters archi tecture 
likelihood function def py tp output values targets may speci ed intermittently 
example sequence classi cation tasks interested output sequence 
modi cation likelihood account intermittent targets straightforward 
maximum likelihood principle optimal parameters obtained maximizing 
optimization problem arising formulation learning addressed framework parameter estimation missing data missing variables state paths fx tp pg describing path state space sequence 
rst brie describe em algorithm 
em algorithm em estimation maximization iterative approach maximum likelihood estimation mle nally proposed 
iteration composed steps estimation step maximization order simplify notation sequence index may omitted 
step 
aim maximize log likelihood function log pa rameters model data 
suppose optimization problem simpli ed knowledge additional variables known missing hidden data 
set dc referred complete data set context referred incomplete data set 
correspondingly log likelihood function lc dc referred complete data likelihood 
chosen function lc dc easily maximized known 
observable lc random variable maximized directly 
em algorithm relies integrating distribution auxiliary function ex lc dc jd expected value complete data log likelihood observed data parameters computed previous iteration 
intuitively computing corresponds lling missing data knowledge observed data previous parameters 
auxiliary function deterministic maximized 
em algorithm iterates steps local maximum likelihood estimation compute ex lc dc jd maximization update parameters arg max cases di cult analytically maximize required step algorithm able compute new value produces increase case called generalized em gem algorithm update parameters theorem guarantees convergence em gem algorithms possibly local maximum incomplete data likelihood theorem dempster gem algorithm equality holds em training iohmms order apply em iohmms noting variable representing paths state space observed 
knowledge variable allow decompose temporal learning problem static learning subproblems 
xt known state probabilities reduce possible independently train instance subnetworks di erent time steps account temporal dependency account sharing parameters di erent times 
observation allows link em learning target propagation approach discussed 
note viterbi approximation considering path static learning problems epoch 
think step em approximation oracle provides target states timet averaging values distribution conditioned values parameters previous epoch 
step ts parameters epoch estimated trajectories 
sequel section derive learning equations architecture 
de ne complete data dc corresponding complete data likelihood def def tp xtp lc dc xju decomposed follows xt py tp xtp static feedforward networks opposition dynamic involving back propagation time 
credit assigned transition network output network 
omit xt xt ut equality follows conditional independency model assumed 
iterating decomposition obtain factorization complete likelihood lc dc xju py tp xt xt ut de ne vector indicator variables zt follows zi 
state distribution multinomial zi zi xt indicator variables rewrite complete data likelihood follows lc dc py py ty yt xt ut xt xt ut ty ny ny yt xt ut zi xt xt ut zi logarithm obtain expression complete data log likelihood lc dc log lc dc px tx nx zi log xt ut nx zi log xt xt ut lc dc depends unknown state variable maximize directly 
temporal credit assignment problem solved 
complete training remain learn data static mappings produce output state transitions 
situation em helps decouple static temporal learning 
estimation step compute expected value lc dc respect distribution paths data old parameters lc dc ju px tx px nx ex zi log yt xt ut zi yt log xt xt ut tx nx gi log xt ut nx hij log xt xt ut def gi xt def hij xt xt elements autocorrelation matrix consecutive state pairs 
hat gi hij means variables computed old parameters order compute hij gi introduce probabilities borrowing notation hmm literature rewritten follows def xt def xt xt xt xt yt xt xt xt xt xt conditional independence assumptions graphically depicted previously discussed obtain yt xt ut ut xt ut speci es output distribution state ut speci es state distribution state conditioned current input 
recursion initialized initial state probabilities def xed priori learned extra parameters hmms 
general constrain model nal states set likelihood dp sequence written terms xt similarly backward recursion established xt xt xt xt xt yt xt xt xt xt conditional independence assumptions yt xt ut ut backward recursion initialized ifi fand 
useful distributions computed 
xt ut obtained running subnetwork oi input vector ut plugging target vector oi output algebraic expression output distribution 
ut simply th output subnetwork ni fed input vector ut transition posterior probabilities hij expressed terms hij xt xt yt ut xt xt yt xt xt yt yt xt xt yt xt ut xt xt yt ut ij ut equation obtained conditional independency assumptions con ditional likelihood equation 
state posterior probabilities gi obtained summing hij directly gi xt yt ut xt yt xt xt ut summarize obtain equations similar train hmms baum welch algorithm 
forward linear recursion equation compute likelihood equation 
training backward linear recursion equation performed equivalent propagating time gradients likelihood respect see 
notice sums equations constrained transition graph underlying model 
maximization step iteration em algorithm requires maximize 
explained subnet works linear done analytically example symbolic inputs 
general subnetworks hidden sigmoidal units softmax function constrain outputs sum maximum analytically 
cases resort gem algorithm simply produces increase example gradient ascent 
theorem guarantees convergence gem algorithms local maximum likelihood convergence may signi cantly slower compared em 
parameterization transition probabilities layers neural units learning algorithm smooth suitable line mode updating parameters sequence presentation accumulating parameter change information training set 
desirable property may help speed learning 
experiments noticed convergence accelerated stochastic line gradient ascent auxiliary function 
general form iohmm training algorithm general form iohmm training summarized algorithm 
algorithm foreach training sequence yt estimation step foreach state compute ij sj running forward state output subnetworks nj oj foreach compute forward backward recurrences current value parameters compute posterior probabilities hij sj gi eqs 
foreach state maximization step adjust parameters state subnetwork nj maximize increase gem algorithm function px tx nx hij log xt xt ut adjust parameters output subnetwork oj maximize increase gem algorithm function px iterate updated parameters 
tx gj log yt xt ut general allowed transitions graph states 
number weights parameters transition output models 
time complexity time step ordinary hmms simply equal number parameters output models 
total computation training epoch iso sum lengths sequences training set 
similar case recurrent networks trained backpropagation time nw 
specializations training algorithm steps algorithm corresponding maximization step em implemented di erent forms depending nature data subnetworks compose architecture 
lookup table networks symbolic data processing describe procedure applies true em algorithm inputs discrete subnetworks behave lookup tables addressed input symbols 
simplicity restrict analysis sequence classi cation tasks 
assume model able discrim di erent classes sequences simplify system associating nal state classes assuming input necessary perform classi cation 
output time step time step probability distribution nal states xt directly give probability distribution output classes ut 
output subnetworks need particular application algorithm output directly read nal state 
learning target class gives nal target state 
th sequence 
likelihood function simpli ed follows def py xt 
tp symbolic inputs encoded index vectors 
particular mg input alphabet symbol encoded vector having th position 
suppose state network single linear layer 
weights subnetwork nj take meaning probabilities transition state conditional input symbol wijk xt xt uk order preserve consistency probabilistic interpretation model weights nonnegative sj wijk constraint easily incorporated maximization likelihood introducing new function def mx mx sj jk jk lagrange multipliers 
derivatives function respect weights nd wijk px hij jk wijk second sum extended time steps input symbol takes value expression zero wijk px hij imposing constraint sj wijk obtain jk sj nonlinear subnetworks jk ppp hij consider general case nonlinear example multilayered subnetworks 
direct analytic maximization possible gem algorithm 
avery simple way producing increase gradient ascent 
derivatives respect parameters easily computed follows 
jk generic weight state subnetwork equation jk px tp hij ij sj ij jk partial derivatives ij jk computed back propagation 
similarly denoting ik generic weight output subnetwork oi wehave ik px tp rx gi log yt xt ut ik ik computed back propagation 
intuitively parameters updated estimation step em provided soft targets outputs subnetworks time comparisons standard hidden markov models model proposed natural extension hmms distribution output sequence conditioned input sequence 
furthermore propose parameterize state output distributions complex modules arti cial neural networks 
typical applications standard hmms automatic speech recognition :10.1.1.131.2084
cases lexical unit associated model mi 
recognition computes model probability jmi generated observed acoustic sequence training parameters adjusted maximize probability correct model mi generates acoustic observations associated instances th lexical unit 
training discriminant try learn decide phoneme sequence learns essentially unsupervised way distribution observations associated class phoneme 
likelihood observations maximized baum welsh algorithm em algorithm 
dynamic programming techniques may decode sequence states 
state sequence training viterbi algorithm approximate estimation step em 
architecture proposed di ers standard hmms respects computing style learning 
iohmms sequences processed similarly recurrent networks input sequence synchronously transformed output sequence 
computing style real time predictions outputs available input sequence processed 
architecture allows model transformation input sequence space output sequence space way fundamental sequence processing tasks prediction cation dealt 
standard hmms homogeneous markov chain iohmms transition probabilities conditional input depend time resulting inhomogeneous markov chain 
consequently system speci ed transition probabilities xed adapted time depending input sequence 
fundamental di erence learning procedure 
interesting capabilities modeling sequential phenomena weakness standard hmms poor discrimination power trained maximum likelihood estimation mle 
consider example application hmms speech recognition 
mle framework lexical unit model corresponding word phoneme trained distribution particular unit 
model learns positive examples informed teacher classes compete 
approach useful improve discrimination hmms maximum mutual information mmi training 
mmi parameters model adjusted account models likelihood model correct class mle criterion 
pointed supervised learning neural networks discriminant learning criteria mmi strictly related 
unfortunately mmi training standard hmms done gradient ascent 
hand iohmms parameter adjusting procedure mle em 
variable desired output response input resulting discriminant training 
furthermore discussed section iohmms better suited learning represent long term context hmms argument hinges fact iohmms non homogeneous 
mentioning approaches proposed integrate connectionist approaches hmm framework 
example observations hmm generated recurrent neural network 
bourlard feedforward network estimate state probabilities conditioned acoustic sequence 
deriving exact em gem algorithm apply viterbi decoding order estimate state trajectory target sequence feedforward network 
common feature algorithms proposed neural networks extract temporally local information markovian system integrates long term constraints 
systems iohmms represent conditional distribution desired output sequence observed input sequence model observation sequence 
furthermore classes output transition models em algorithm applied iohmms represent discriminant model hybrids neural networks hmms discriminant trained em algorithm 
second order recurrent networks rst order fully recurrent network sigmoidal nonlinearities evolves nonlinear iterated map xt xt ut xt continuous state vector weight matrices 
dynamics iohmm controlled recurrence ut updates state distribution input sequence iohmm case dynamics linear state variable nonlinear inputs may result general computational capabilities compared recurrent networks 
order gain intuition computational power iohmms useful consider limit case transition probabilities tend 
deterministic behavior corresponding equation obtained output units state networks saturated case state network partitions input space regions ij transition state time state time occurs ut ij 
multilayered state networks hidden units fact softmax function eq 
lim equals argmax 
universality results regions ij arbitrarily shaped 
output units state networks saturated transition probabilities exactly obtain similar interpretation regions ij soft boundaries 
multiplicative links analogies architecture second order recurrent networks encode discrete states 
second order network state units inputs evolves equation xt nx xj wj matrices weights 
iohmm uses layered state subnetworks evolve linear recurrence nx second order network represent discrete states hot encoding xi state time 
encoding assumption satis ed happen state units saturated equations equivalent 
second order networks encoding discrete states previous state selects weight matrix wj predict state input 
saturated second order network behaves modular architecture similar described distinct subnetworks activated time depending discrete state time 
similar interpretation second order networks limited symbolic inputs proposed 
adaptive mixtures experts adaptive mixtures experts hierarchical mixtures experts hme intro duced divide conquer approach supervised learning static connectionist models 
mixture experts composed modular set subnetworks experts compete gain responsibility modeling outputs region input space 
system output obtained convex combina tion experts outputs weights gj computed parametric function inputs gating network assigns responsibility di erent experts di erent regions input space 
ut gating network mixture controllers mc architecture cacciatore nowlan 
cacciatore nowlan proposed recurrent extension architecture called mixture mc gating network feedback connections temporal context account 
mc architecture shown 
iohmm architecture clearly interpreted special case mc architecture gating network modular structure second order connections 
practice cacciatore nowlan layer rst order gating net resulting model weaker capacity 
signi cant di erence lies organization processing 
mc architecture modularity exploited output prediction level iohmms modularity exploited state prediction level 
potentially important di erence lies presence saturating non linearity recurrence loop mc architecture recurrent networks 
recurrence loop iohmms purely linear 
shown non linearity loop di cult learning long term context see section discussion learning long term dependencies 
learning mc architecture uses approximated gradient ascent optimize likelihood contrast em supervised learning algorithm proposed jordan jacobs hme 
approx imation gradient step truncated back propagation time similar elman approach allows online updating continually running sequences useful control tasks 
shown earlier interpretation state sequences iohmms missing data yields maximization likelihood em gem algorithm 
learning temporal dependencies iohmms generally speaking sequential data presents long term dependencies data time signi cantly ected past data times accurate modeling sequences typically di cult 
case recurrent networks trained gradient descent credit assignment time represented sequence gradients error function respect state sigmoidal units 
researchers procedure ine ective assigning credit long temporal spawns :10.1.1.41.7128
case iohmms trained em algorithm credit assignment time represented sequences posterior having observed data probabilities xt ut 
summarize main results problem learning long term dependencies markovian models include iohmms hmms 
formal analysis problem 
temporal credit assignment previous theoretical reasons di culty training parametric non linear dynamical systems capture long term dependencies 
systems dynamical evolution controlled non linear iterated map ut state vector 
systems described equation include recurrent neural network architectures 
main result states long term storing gradient propagation harmed depending km norm jacobian state transition function greater 
km system endowed robust dynamical attractors reliably store pieces information arbitrary duration 
gradients vanish exponentially propagated backward time 
km gradients vanish information past inputs gradually lost easily deleted noisy events 
example element system detect particular conjunctions inputs state element latch information problem order train back propagate gradients vanish long time periods 
introduced connectionist model dynamical behavior controlled constrained linear recurrence equation ut interpreted probability distribution de ned set discrete states ut corresponds matrix transition proba bilities 
case norm jacobian state transition function constrained exactly 
recurrent networks learning non deterministic markovian models generally di cult span temporal dependencies increases 
important qualitative di erence markovian models long term storing temporal credit assignment necessarily incompatible occur impractical 
occur special case essentially deterministic model 
di culty increases model non deterministic worst completely ergodic 
eigenvalues ut correspond loss information initial conditions di usion information time 
conversely credit assignment backwards time harmed phenomenon di usion backward phase just transpose forward phase 
contrary eigenvalues ut close models tend deterministic behavior storing credit assignment ective 
order provide intuitions practical di culty learning long term dependencies markovian models consider sequence classi cation problem discrete target variable step training sequence 
sequences long contain relevant classi cation information clearly task exhibits long term dependencies 
key variables temporal credit assignment problem probabilities state time observed target shown section variables computed estimation step em 
correspond gradient likelihood respect state probabilities indicate probabilities associated state time step increase order increase total likelihood 
cases tend independent far away nal supervision 
clearly re ects situation maximum uncertainty changes required increase likelihood 
states time step equally responsible nal likelihood small change parameters increase likelihood 
represents serious di culty propagating backwards time ective temporal credit information di cult learning presence long term dependencies 
transition probabilities close long term context propagated credit assignment time performed correctly 
situation example problems grammar inference input output data essentially deterministic task studied section 
analysis problem credit assignment study problem theoretical point view applying established mathematical results markov chains problem learning long term dependencies homogeneous non homogeneous hmms 
analysis ordinary hmms iohmms important di erence simplest cure transition probabilities near 
hmm deterministic deterministic transition probabilities useful model simple cycles 
hand iohmm perform large class interesting computations grammar inference constraint transition probabilities vary time step depending input sequence 
analyses reported suggest fully connected transition graphs worst behavior point view temporal credit propagation 
transition graph constrained prior knowledge problem 
main bene ts gained introducing prior knowledge adaptive model improving generalization new instances simplifying learning 
techniques injecting prior knowledge recurrent neural networks proposed researchers 
cases domain knowledge supposed available collection transition rules nite automaton 
similar approach choose topologies transition graph markovian models hmms iohmms 
example structured left right speech recognition topology elementary considerations speech production structure language 
reducing credit di usion penalized likelihood outlined undesired di usion temporal credit depends fast convergence rank product successive matrices transition probabilities increases 
rate rank reduced controlling norm eigenvalues transition matrices ideal condition credit assignment transition matrix eigenvalues unitary complex circle 
determinant matrix equals product eigenvalues simple way reduce di usion ect add penalty term log likelihood follows px tp jdet tj constant weights uence penalty term 
case maximization step em require gradient ascent need gem algorithm 
contribution penalty term gradient computed relationship jdet tj ij jdet tj ji trick useful particularly nasty problems long sequences parity problem see section 
experimental comparisons recurrent networks results problems control span input output dependencies sequence problem parity problem 
simple benchmarks compare long term learning capabilities recurrent networks trained back propagation alternative algorithms 
sequence problem classify univariate input sequence sequence types rst elements experiments sequence carry information sequence class 
sequences constructed arti cially choosing di erent random initial pattern class 
nal time step output sequence considered classifying input sequence 
uniform noise added input sequence 
rst methods see tables fully connected recurrent network units free parameters 
iohmm state system sparse connectivity matrix initial state separate left right sub models states model types sequences shown 
output subnetworks required case supervision may expressed terms desired nal state explained section 
resulting architecture shown 
experiments full transition matrix yield worse results improvements obtained introducing penalty term 
parity problem consists producing parity input sequence produced nal output number input odd 
target sequence 
rst methods minimal size network input hidden output free parameters 
iohmm state system full connectivity matrix 
task performance drastically improved stochastic gradient ascent away helps training algorithm get local optima 
learning rate decreased likelihood improves increased likelihood remains system stuck plateau local optimum 
tasks method initial parameters chosen randomly training trials 
noise added input sequence uniformly distributed chosen independently training sequence 
considered criteria average classi cation error training stopping criterion met allowed number sequence presentations performed task learned average number function evaluations needed reach stopping criterion 
tables stands pseudo newton 
time weighted pseudo newton variation derivatives respect instantiation parameter particular time step weighted inverse corresponding second derivatives 
multigrid similar simulated annealing constant temperature 
discrete error propagation algorithm attempts propagate backwards discrete error information recurrent network discrete units 
column tables corresponds maximum sequence length set trials 
sequence length particular training sequence picked randomly andt numbers reported averages table sequences problem final classi cation error respect maximum sequence length 
back prop pseudo newton time weighted multigrid discrete error prop 
simulated annealing iohmms table sequences problem sequence presentations respect maximum sequence length 
back prop pseudo newton time weighted multigrid discrete error prop 
simulated annealing iohmms table parity problem final classi cation error respect maximum sequence length 
back prop pseudo newton time weighted multigrid discrete error prop 
simulated annealing iohmms table parity problem sequence presentations respect maximum sequence length 
back prop pseudo newton time weighted multigrid discrete error prop 
simulated annealing iohmms class class transition graph sequences problem 
corresponding recurrent architecture fully connected 
trials 
results tables clearly show iohmms achieve better performance obtained algorithms possibly discrete error propagation algorithm 
regular grammar inference section describe application architecture problem grammatical inference 
task learner set labeled strings requested infer set rules de ne formal language classify new sequence symbols part language part language 
considered prototype complex language processing problems 
simplest case regular grammars task proved np complete 
researchers approached grammatical inference recurrent networks 
studies demonstrate second order neural networks trained approximate behavior nite state automata fsa 
memories learned way appear lack robustness noisy dynamics dominant long input strings 
motivated research extract table de nitions tomita grammars grammar de nition string contain substring string contain substring string contains number number numberof automata rules trained network 
cases shown extracted automaton outperforms trained network 
fsa extraction procedures relatively easy devise symbolic inputs may di cult apply tasks involving sub symbolic continuous input space recognition 
complexity discrete state space produced fsa extraction procedure may grow continuous network learned representation involving chaotic attractors 
researchers attempted encourage nite state representation regularization clustering techniques training procedure 
report experimental results application iohmms set regular grammars introduced tomita researchers benchmark measure accuracy inference methods recurrent networks 
grammars binary alphabet reported table 
grammar tomita de ned small set labeled strings training data 
di culties task infer proper rules attain perfect generalization impoverished data 
task classify sequence classes accepted rejected strings scalar output put supervision time step nal output yt modeled bernoulli variable system nal expected output target output string rejected accepted 
test adopted criterion accepting string 
worth mentioning nal states directly targets done section accepting rejecting grammar states grammar states grammar states grammar states convergence generalization attained varying number discrete states model 
results averaged trials 
frequency convergence classi cation errors training set 
frequency convergence classi cation errors training set 
gray levels vertical bars show corresponding accuracies test data 
triangles denote generalization accuracy best worst trial 
horizontal dashed line represents best result reported watrous kuhn 
continued 
grammar states grammar states grammar states watrous kuhn best accuracy best trial accuracy worst trial convergence convergence ave accuracy ave accuracy continuation 
table summary experimental results tomita grammars see text explanation 
grammar sizes frequency accuracies fsa min convergence average worst best best state 
problem circumvented appending special string 
case increase number parameters 
task accepting strings solved moore nite state machine output function state strings accepted rejected depending nal state reached 
apply external inputs output networks reduced unit fed bias input 
way output network computes constant function state reached model 
system output combination weighted state distribution time absence prior knowledge plausible state paths transition graph transitions allowed 
state network composed single layer neurons softmax function outputs 
input symbols encoded dimensional index vectors ut symbol ut symbol 
total number free parameters experiments measured convergence generalization performance di erent sizes recurrent architecture 
setting ran trials di erent seeds initial weights 
considered trial successful trained network able correctly label training strings 
order select model size number states generated small data set composed randomly selected strings length applied cross validation criterion 
grammar trained di erent architectures having selected value yielded best average accuracy cross validation data set 
interestingly grammars obtained choosing smallest model successfully trained correctly classify learning set shown 
gure shows generalization accuracy triangles frequency convergence zero errors training set squares grammars comparison best result trials obtained watrous kuhn second order recurrent network dashed horizontal line data 
see iohmm trials performed better best recurrent network trials best iohmm trial generalized perfectly best recurrent network 
comparison table report grammar number states minimal recognizing fsa 
tested trained iohmms corpus binary strings length 
nal results numerically summarized table 
column convergence reports fraction trials succeeded separate training set 
columns report averages order statistics worst best trial fraction correctly classi ed strings measured successful trials 
grammar results refer model size selected cross validation 
generalization perfect grammars 
grammar best trial attained perfect generalization 
results compare favorably obtained second order networks trained gradient descent training sets proposed tomita 
comparison column table reproduce results reported watrous kuhn best trials 
researchers obtained interesting results directly comparable larger training sets di erent experimental conditions 
successful trials observed model learned deterministic behavior transition probabilities asymptotically converging exact require develop nite weights softmax function 
course consistent deterministic nature problem 
interesting note apart numerical precision problems trained models behave nite automata rendering trivial extraction corresponding deterministic automaton 
grammars trained iohmms behave exactly minimal training sets de ned tomita 
grammar grammar grammar grammar finite automata equivalent iohmms trained tomita grammars 
recognizing fsa see 
cases iohmm learned di erent representation 
particular grammar model states correctly classify test strings 
interesting minimal fsa grammar states 
report learned transition probabilities output probabilities 
wonder representation robust longer input strings 
investigate issue generated random strings length iohmm errors 
potential training problem presence local maxima likelihood function 
example fraction converged trials grammars small di culty discovering optimal solution serious restriction tasks involving large number states 
experiments noticed restricting connectivity transition graph signi cantly help remove problems convergence :10.1.1.56.686
course approach ectively exploited prior knowledge state space available 
example applications hmms speech recognition rely structured topologies 
conclude iohmms perform task grammar inference comparison recurrent networks 
furthermore nd solutions involving states minimum required system completely deterministic 
xt learned transition probabilities grammar transition probabilities bar chart row column represents discrete distribution xt xt ut 
probabilities accepting input string xt 
network correctly classi es test strings 
recurrent architecture suitable modeling input output relationship discrete state dynamical systems 
architecture probabilistic interpretation called input output hidden markov model iohmm trained em gem algorithm state paths missing data 
seen extension standard hidden markov models hmms conditioning input sequence extension mixture experts model constrained linear feedback loop sets experts predicting output predicting state 
test problems span temporal dependencies controlled iohmms learn long term dependencies ectively back propagation alternative algorithms described 
analysis problem credit assignment time hmms iohmms explains solve problem better recurrent networks non linearity recurrence loop revealed best results obtained transition probabilities near 
corresponds uninteresting models case hmms corresponds large class useful models case iohmms non homogeneous transition probabilities change sequence 
furthermore hmms trained cient gradient ascent em algorithm trained basically non discriminant way iohmms trained em algorithm discriminant training criterion 
results obtained experiments suggest iohmms appropriate solving grammatical inference problems 
particular benchmark problem proposed tomita iohmms compare favorably second order nets trained gradient descent terms generalization performance 
address extensions model directions 
algorithm perform larger scale tasks large state space 
dynamic programming applications speech recognition wish assign certain meaning particular states transitions 
algorithm tasks sequence prediction multivariate time series sequence production control robotics tasks 
ways improve credit assignment time data modeled non deterministic 
exploring solution hierarchical representation state 
achieved introducing sub state variables cartesian product corresponds system state 
sub state variables operate di erent time scale allowing credit propagate long temporal spans variables 
di erent option simplifying sequential learning task exploit form prior knowledge reduce number free parameters reduce di culty capturing long term dependencies 
waibel hinton shikano lang phoneme recognition time delay neural networks ieee transactions acoustics speech signal processing vol 
pp 

seidl lorenz structure recurrent neural network approximate nonlinear dynamic system proceedings international joint conference neural networks vol 
pp 
july 
sontag systems combining linearity relations neural networks tech 
rep rutgers center systems control 
rumelhart hinton williams learning internal representations error propagation parallel distributed processing rumelhart mcclelland eds vol 
ch 
pp 
cambridge mit press 
pearlmutter learning state space trajectories recurrent neural networks neural computation vol 
pp 

kuhn rst look phonetic discrimination connectionist models recurrent links 
ida working institute defense analysis princeton nj 
robinson fallside static dynamic error propagation networks application speech coding neural information processing systems anderson ed denver pp 
american institute physics new york 
williams zipser learning algorithm continually running fully recurrent neural networks neural computation vol 
pp 

gori bengio de mori bps learning algorithm capturing dynamical nature speech proceedings international joint conference networks washington pp 
ieee new york 
mozer focused back propagation algorithm temporal pattern recognition complex systems vol 
pp 

frasconi gori soda local feedback multi layered networks neural computation vol 
pp 

tsoi back locally recurrent globally feedforward networks critical review architectures ieee transactions neural networks vol 
pp 

bengio simard frasconi learning long term dependencies gradient descent di cult ieee transactions neural networks vol 
pp 

mozer induction multiscale temporal structure advances neural information processing systems moody hanson eds san mateo ca pp 
morgan kaufmann 
rohwer time dimension neural network models acm sigart vol 
pp 
july 
bengio frasconi simard problem learning long term dependencies recurrent networks ieee international conference neural networks sanfrancisco pp 
ieee press 
invited 
rohwer moving targets training algorithm advances neural information processing systems touretzky ed denver pp 
morgan kaufmann san mateo 
bengio frasconi credit assignment time alternatives backpropagation advances neural information processing systems cowan tesauro alspector eds morgan kaufmann :10.1.1.56.686
dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society vol 
pp 

baum petrie soules weiss maximization technique occuring statistical analysis probabilistic functions markov chains ann 
math 
statistic vol 
pp 

levinson rabiner sondhi application theory probabilistic functions markov process automatic speech recognition bell system technical journal vol 
pp 

hidden markov models guided tour proc 
int 
conf 
acoustics speech signal processing pp 

kehagias stochastic recurrent networks prediction classi cation time series tech 
rep brown university 
division applied mathematics providence ri 
ha ner discriminant learning minimum memory loss improved non vocabulary rejection eurospeech madrid spain 
bengio lecun henderson globally trained handwritten word recognizer spatial repre sentation space displacement neural networks hidden markov models advances neural information processing systems cowan tesauro alspector eds pp 

mclachlan basford mixture models inference applications clustering 
marcel dekker 
jacobs jordan nowlan hinton adaptive mixture local experts neural computation vol 
pp 

cacciatore nowlan mixtures controllers jump linear non linear plants advances neural information processing systems cowan tesauro alspector eds san mateo ca morgan kaufmann 
bourlard links hidden markov models multilayer perceptrons ieee transactions pattern analysis machine intelligence vol 
pp 

bourlard morgan connectionist speech recognition 
hybrid approach vol 
kluwer international series engineering computer science 
boston kluwer academic publishers 
bengio de mori global optimization neural network hidden markov model hybrid ieee transactions neural networks vol 
pp 

levin word recognition hidden control neural architecture international conference tics speech signal processing albuquerque nm pp 

bridle probabilistic interpretation feedforward classi cation network outputs relationships statistical pattern recognition neuro computing algorithms architectures applications fogelman soulie herault eds new york springer verlag 
pearl probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
bridle training stochastic model recognition algorithms networks lead maximum mutual infor mation estimation parameters advances neural information processing systems touretzky ed pp 
morgan kaufmann 
baldi chauvin smooth line learning algorithms hidden markov models neural computation vol 
pp 

rabiner tutorial hidden markov models selected applications speech recognition proceed ings ieee vol 
pp 

brown acoustic modeling problem automatic speech recognition 
phd thesis dept computer science carnegie mellon university 
hornik stinchcombe white multilayer feedforward networks universal approximators neural networks vol 
pp 

giles inserting rules recurrent neural networks neural networks signal processing ii proceedings ieee workshop kung fallside sorenson kamm eds pp 
ieee press 
sun chen lee giles recurrent neural networks hidden markov models stochastic grammars proc 
int 
joint conference neural networks vol 
san diego ca pp 

jordan jacobs hierarchies adaptive experts advances neural information processing systems moody hanson eds san mateo ca pp 
morgan kaufmann 
elman finding structure time cognitive science vol 
pp 

bengio frasconi di usion credit markovian models advances neural information pro cessing systems tesauro touretzky alspector eds san mateo ca morgan kaufmann 
seneta nonnegative matrices markov chains 
newyork springer 
abu mostafa learning hints neural networks journal complexity vol 
pp 

towell shavlik noordewier re nement approximate domain theories knowledge neural networks proceedings eighth national conference onarti cial intelligence boston ma pp 

tresp ahmad network structuring training rule knowledge ad neural information processing systems hanson cowan giles eds san mateo ca morgan kaufman publishers 
giles training second order recurrent neural networks hints machine learning proc 
ninth int 
conference sleeman edwards eds san mateo ca morgan kaufmann 
maclin re ning domain theories expressed nite state automata machine learning proceedings eighth international workshop birnbaum collins eds san mateo ca morgan kaufmann 
frasconi gori soda uni ed integration explicit rules learning example recurrent networks ieee transactions knowledge data engineering vol 
pp 

press 
continuous speech recognition acoustic states st meeting acoustic society america april 
becker lecun improving convergence back propagation learning second order methods proceedings connectionist models summer school touretzky hinton sejnowski eds pittsburg pp 
morgan kaufmann san mateo 
angluin smith inductive inference theory methods computing surveys vol 
pp 

giles miller chen chen sun lee learning extracted nite state automata second order recurrent neural networks neural computation vol 
pp 

pollack induction dynamical recognizers machine learning vol 
pp 

watrous kuhn induction nite state languages second order recurrent networks neural computation vol 
pp 

gori soda insertion nite state automata recurrent radial basis function networks tech 
rep dsi universita di firenze italy 
submitted 
das mozer uni ed gradient descent clustering architecture nite state machine induction advances neural information processing systems cowan tesauro alspector eds morgan kaufmann 
tomita dynamic construction nite state automata examples hill climbing inproceedings fourth annual cognitive science conference ann arbor mi pp 

hopcroft ullman automata theory languages computation 
reading ma addison wesley publishing 

