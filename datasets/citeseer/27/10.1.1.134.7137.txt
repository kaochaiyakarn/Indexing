comparison algorithms maximum entropy parameter estimation conditional maximum entropy models provide general purpose machine learning technique successfully applied fields diverse computer vision econometrics wide variety classification problems natural language processing 
flexibility models cost 
parameter estimation models conceptually straightforward practice models typical natural language tasks large may contain thousands free parameters 
consider number algorithms estimating parameters models including iterative scaling gradient ascent conjugate gradient variable metric methods 
surprisingly standardly iterative scaling algorithms perform quite poorly comparison test problems variable metric algorithm outperformed choices 
maximum entropy models variously known log linear gibbs exponential multinomial logit models provide general purpose machine learning technique classification prediction successfully applied fields diverse computer vision econometrics 
natural language processing years seen techniques sentence boundary detection part speech tagging parse selection ambiguity resolution stochastic attribute value grammars name just applications abney berger ratnaparkhi johnson :10.1.1.103.7637
leading advantage models flexibility allow stochastic rule systems augmented additional syntactic semantic pragmatic features 
richness robert malouf alfa informatica rijksuniversiteit groningen groningen netherlands malouf rug nl representations cost 
modest models require considerable computational resources large quantities annotated training data order accurately estimate model parameters 
parameter estimation models conceptually straightforward practice models typical natural language tasks usually quite large frequently contain hundreds thousands free parameters 
estimation large models expensive due sparsely distributed features sensitive round errors 
highly efficient accurate scalable methods required estimating parameters practical models 
consider number algorithms estimating parameters models including generalized iterative scaling improved iterative scaling general purpose optimization techniques gradient ascent conjugate gradient variable metric methods 
surprisingly widely iterative scaling algorithms perform quite poorly test problems limited memory variable metric algorithm outperformed choices 
maximum likelihood estimation suppose probability distribution set events characterized dimensional feature vector function addition set contexts function partitions members case stochastic context free grammar example set possible trees feature vectors represent number times rule applied derivation tree set possible strings words set trees yield conditional maximum entropy model parametric form berger chi johnson exp exp dimensional parameter vector inner product parameter vector feature vector :10.1.1.103.7637
parametric form model fitting model collection training data entails finding values parameter vector minimize kullback leibler divergence model empirical distribution log equivalently maximize log likelihood logq gradient log likelihood function vector derivatives respect parameter ep eq likelihood function concave parameter space global maximum gradient zero 
unfortunately simply setting solving yield closed form solution proceed iteratively 
step adjust estimate parameters new estimate divergence estimated probability distribution empirical distribution continue successive improvements fail yield sufficiently large decrease divergence 
parameter estimation algorithms consider take general form method computing updates search step differs substantially 
shall see difference dramatic impact number updates required reach convergence 
iterative scaling popular method iteratively refining model parameters generalized iterative scaling gis due darroch ratcliff 
extension iterative proportional fitting deming stephan gis scales probability distribution factor proportional ratio ep restriction event training data condition easily satisfied addition correction feature 
adapt gis estimate model parameters model probabilities yielding update rule ep log eq step size rate convergence depends constant larger value smaller step size 
case rows training data sum constant addition correction feature effectively slows convergence match difficult case 
avoid slowed convergence need correction feature della pietra 
propose improved iterative scaling iis algorithm update rule solution equation ep exp sum feature values event training data 
polynomial exp solution straightforwardly example newton raphson method 
order methods iterative scaling algorithms long tradition statistics widely analysis contingency tables 
primary strength iteration require computation expected values 
depend evaluation gradient log likelihood function depending distribution prohibitively expensive 
case models vector expected values required iterative scaling essentially gradient sense consider methods gradient directly 
obvious way making explicit gradient cauchy method method steepest ascent 
gradient function vector points direction function value increases rapidly 
goal maximize log likelihood function natural strategy shift current estimate parameters direction gradient update rule step size chosen maximize 
finding optimal step size optimization problem dimension practice approximate solution required guarantee global convergence 
log likelihood function concave method steepest ascent guaranteed find global maximum 
steps taken iteration narrow sense locally optimal global convergence rate steepest ascent poor 
new search direction orthogonal approximate line search nearly previous direction 
leads characteristic zig zag ascent convergence slowing maximum approached 
way looking problem steepest ascent considers search directions times 
prefer algorithm considered possible search direction iteration step exactly right length direction orthogonal previous search directions 
intuition underlies conjugate gradient methods choose search direction linear combination steepest ascent direction previous search direction 
step size selected approximate line search steepest ascent method 
non linear conjugate gradient methods fletcher reeves cg fr polak re positive cf prp algorithms proposed 
theoretically equivalent different update rules show different numeric properties 
second order methods way looking problem steepest ascent takes account gradient log likelihood function fails take account curvature gradient gradient 
usefulness curvature clear consider second order taylor series approximation hessian matrix log likelihood function matrix second partial derivatives respect 
set derivative zero solve get update rule newton method newton method converges quickly quadratic objective functions step requires computation inverse hessian matrix iteration 
log likelihood function models twice differentiable large scale problems evaluation hessian matrix computationally impractical newton method competitive iterative scaling order methods 
variable metric quasi newton methods avoid explicit evaluation hessian building approximation successive evaluations gradient 
replace local approximation inverse hessian positive definite matrix satisfies equation 
variable metric methods show excellent convergence properties efficient true newton updates large scale problems hundreds thousands parameters storing approximate hessian prohibitively expensive 
cases apply limited memory variable metric methods implicitly approximate hessian matrix vicinity current estimate previous values practical applications values suffice offer substantial savings storage requirements variable metric methods giving favorable convergence properties 
comparing estimation techniques performance optimization algorithms highly dependent specific properties problem solved 
worst case analysis typically space constraints preclude detailed discussion methods 
algorithmic details theoretical analysis second order methods see nocedal nocedal wright 
reflect actual behavior actual problems 
order evaluate performance optimization techniques sketched previous section applied problem parameter estimation need compare performance actual implementations realistic data sets dolan mor 
minka offers comparison iterative scaling algorithms parameter estimation logistic regression problem similar considered difficult transfer minka results models 
evaluates algorithms randomly generated training data 
performance accuracy optimization algorithms sensitive specific numerical properties function optimized results random data may may carry realistic problems 
test problems minka considers relatively small dimensions 
seen algorithms perform small medium scale problems may applicable problems thousands dimensions 
implementation basis implementation petsc portable extensible toolkit scientific computation software library designed ease development programs solve large systems partial differential equations 
petsc offers data structures routines parallel sequential storage manipulation visualization large sparse matrices 
estimation techniques expensive operation computing probability distribution expectations eq iteration 
order facilities provided petsc store training data sparse matrix rows corresponding events columns features 
parameter vector unnormalized probabilities matrix vector product expf feature expectations transposed matrix vector product eq ft expressing computations matrix vector operations take advantage high performance sparse matrix primitives petsc 
comparison implemented generalized improved iterative scaling primitives provided petsc 
optimization techniques tao toolkit advanced optimization library layered top foundation petsc solving nonlinear optimization problems benson 
tao offers building blocks writing optimization programs line searches convergence tests high quality implementations standard optimization algorithms including conjugate gradient variable metric methods 
turning results comparison additional points need 
order assure consistent comparison need stopping rule algorithm 
experiments judged convergence reached relative change loglikelihood iterations fell predetermined threshold 
run stopped relative tolerance particular application may may appropriate stopping rule purposes comparison 
noted current implementation applied possible optimizations appear literature lafferty suhm wu khudanpur lafferty speed normalization probability distribution improvements take advantage model structure simplify evaluation denominator :10.1.1.120.9821
particular data sets examined unstructured optimizations give improvement 
optimizations appropriate give proportional speed algorithms 
optimizations independent choice parameter estimation method 
experiments compare algorithms described applied implementation outlined previous section training data sets described table drawn domain natural language processing 
rules lex datasets examples dataset classes contexts features non zeros rules lex summary shallow stochastic attribute value grammars small set scfg features large set fine grained lexical features bouma 
summary dataset part sentence extraction task osborne appear shallow dataset drawn text chunking application osborne 
datasets vary widely size composition representative kinds datasets typically encountered applying models nlp classification tasks 
results applying parameter estimation algorithms datasets summarized table 
run report kl divergence fitted model training data convergence prediction accuracy fitted model held test set fraction contexts event highest probability model highest probability distribution number iterations required number log likelihood gradient evaluations required algorithms line search may require function evaluations iteration total elapsed time seconds 
things observe results 
iis converges fewer steps gis takes substantially time 
implementation additional bookkeeping overhead required iis cancels improvements speed offered accelerated convergence 
may misleading finely tuned implementation iis may take time iteration experiments 
iteration iis fast reported time include time required input training data difficult reproduce algorithms tested 
tests run cpu dual processor mhz pentium gigabytes main memory center high performance computing visualisation university groningen 
table datasets experiments iteration gis benefits iis gis cases quite modest 
second note datasets kl divergence convergence roughly algorithms 
summary dataset differ orders magnitude 
indication convergence test sensitive rate convergence choice algorithm 
degree precision desired reached algorithms appropriate value 
gis say require iterations reported table reach precision achieved limited memory variable metric algorithm 
third prediction accuracy cases algorithms 
variability expected data sets considered badly ill conditioned different models yield likelihood 
cases prediction accuracy differs substantially 
data sets rules lex gis small advantage methods 
dramatically iterative scaling methods perform poorly shallow dataset 
case training data sparse 
features nearly pseudo minimal sense johnson 
receive weights approaching 
smoothing probabilities improve results methods reduce observed differences 
suggest gradient methods robust certain problems training data 
significant lesson drawn results exception steepest ascent gradient methods outperform iterative scaling wide margin datasets measured number function evaluations total elapsed time 
case limited memory variable metric algo dataset method kl div 
acc iters evals time rules gis iis steepest ascent conjugate gradient fr conjugate gradient prp limited memory variable metric lex gis iis steepest ascent conjugate gradient fr conjugate gradient prp limited memory variable metric summary gis iis steepest ascent conjugate gradient fr conjugate gradient prp limited memory variable metric shallow gis iis steepest ascent conjugate gradient fr conjugate gradient prp limited memory variable metric rithm performs substantially better competing methods 
described experiments comparing performance number different algorithms estimating parameters conditional model 
results show variants iterative scaling algorithms widely literature perform quite poorly compared general function optimization algorithms conjugate gradient variable metric methods 
specifically nlp classification tasks considered limited memory variable metric algorithm benson mor outperforms choices substantial margin 
obvious consequences field 
modeling commonly machine learning technique application improved table results comparison 
parameter estimation algorithms practical construct larger complex models 
parameters individual models estimated quite quickly open possibility sophisticated model feature selection techniques compare large numbers alternative model specifications 
suggests comprehensive experiments compare convergence rate accuracy various algorithms wider range problems called 
addition larger lesson drawn results 
typically think computational linguistics primarily symbolic discipline 
statistical natural language processing involves non trivial numeric computations 
results show natural language processing take great advantage algorithms software libraries developed quantitatively oriented engineering computational sciences 
research dr malouf possible fellowship royal netherlands academy arts sciences nwo project algorithms linguistic processing 
stephen clark andreas detlef miles osborne van noord helpful comments test data 
steven abney 

stochastic attribute value grammars 
computational linguistics 
satish william gropp lois barry smith 

management parallelism object oriented numerical software libraries 
arge editors modern software tools scientific computing pages 
birkhauser press 
satish kris william gropp dinesh kaushik lois barry smith 

petsc home page 
www mcs anl gov petsc 
satish william gropp lois barry smith 

petsc users manual 
technical report anl revision argonne national laboratory 
steven benson jorge mor 

limited memory variable metric method bound constrained minimization 
preprint anl acs argonne national laboratory 
steven benson lois jorge mor jason 

tao users manual 
technical report anl mcs tm revision argonne national laboratory 
adam berger stephen della pietra vincent della pietra 

maximum entropy approach natural language processing 
computational linguistics 
bouma van noord robert malouf 

wide coverage computational analysis dutch 
daelemans veenstra zavrel editors computational linguistics netherlands pages 
amsterdam 
chi 

probability models complex systems 
ph thesis brown university 
darroch ratcliff 

generalized iterative scaling log linear models 
ann 
math 
statistics 
stephen della pietra vincent della pietra john lafferty 

inducing features random fields 
ieee transactions pattern analysis machine intelligence 
deming stephan 

squares adjustment sampled frequency table expected marginals known 
annals mathematical statistics 
elizabeth dolan jorge mor 

benchmarking optimization software performance profiles 
mathematical programming 
mark johnson stuart geman stephen canon chi stefan riezler 

estimators stochastic unification grammars 
proceedings th annual meeting acl pages college park maryland 
john lafferty bernhard suhm 

cluster expansions iterative scaling maximum entropy language models 
hanson silver editors maximum entropy bayesian methods 
kluwer 
john lafferty fernando pereira andrew mc 

conditional random fields probabilistic models segmenting labeling sequence data 
international conference machine learning icml 
thomas minka 

algorithms maximum likelihood logistic regression 
statistics tech report cmu 
jorge nocedal stephen wright 

numerical optimization 
springer new york 
jorge nocedal 

large scale unconstrained optimization 
watson duff editors state art numerical analysis pages 
oxford university press 
miles osborne 

shallow parsing noisy non stationary training material 
journal machine learning research 
miles osborne 
appear 
maximum entropy sentence extraction 
proceedings acl workshop automatic summarization philadelphia 
adwait ratnaparkhi 

maximum entropy models natural language ambiguity resolution 
ph thesis university pennsylvania 
jun wu sanjeev khudanpur 

efficient training methods maximum entropy language modelling 
proceedings icslp volume pages beijing 
