quantitative analysis cache policies scalable network file systems michael dahlin randolph wang thomas anderson david patterson computer science division 
university california berkeley current network file system protocols rely heavily cen server coordinate tile activity client workstations 
central server bottleneck limits environments large numbers clients 
central server systems nfs afs writes cache misses coherence messages handled server 
keep workload expensive server machines needed configured high performance cpus memory systems channels 
server stores data tt physically capable connecting disks 
reliance central server current systems inappropriate wide area network network bandwidth server may 
quantitative performance effect server responsibilities possible client workstations reduce need high performance server machines 
devised cache protocol data reside clients data transfers proceed directly client client 
server coordinate data transfers 
thm protocol incorporated part experi mental file system xfs 
results trace driven simulation study protocol traces client nfs installation 
find xfs protocol reduces server load factor compared afs significantly affecting response time file availability 
current network tile systems rely powerful central servers difficult build economical large scale file systems 
ideally network file system scale hundreds thou sands client machines commodity workstations server 
widely sun network fde system nfs sand spawned new industry dedicated building high performance multiprocessor sys tems needed scale nfs dozen clients andrew fde system afs designed reduce server load relative nfs interest mate afs relies central supported pm advanced research projects agency national science cda micro 
equipment 
fo xerox 
siemens alm supported national science dation graduate research fellowship anderson dlm supported science foundation young award dahlin tea cs berkeley edu permission copy fee part material granted provided copies distributed direct commercial advantage acm copyright notice title publication date appear notice copying permission association computing machinery 
copy se republish requires fee specific permission 
sigmetrics santa clara ca 
usa acm server receive copy modified data supply data client cache requests nfs afs generally specialized servers commodity desktop tions server machines server support disks hold copy file system desktop tions generally single scsi string commodity workstations cost effective way buy computing power bandwidth server machines designed greater development costs complicated servers amortized smaller sales volume 
instance sun product hne server costs times similarly configured tion 
trends file system promise place heavier demands central servers file systems 
baker bake report size large files grew order 
trend continues cost transferring data central server may pro 
file systems asked manage data wide area networks wans bandwidth restrictions limit amount data supplied central source time technology trends giving tre amounts disk space main memory 
processing power providing high speed low latency networks tle resources 
inexpensive disks feasible clients store large amounts data locally 
gb scsi disk currently costs workstations sold today configured significant amounts local disk 
similarly aggregate memories processing resources client dwarf capacity single server 
high speed local area networks lans allow clients access data peers local area network quickly access local data 
thn investigates quantitative benefits utilizing cache techniques oriented extreme reduce load central server 
techniques provide better cost effective file specialized server pushing responsibilities clients system exploit aggregate client disk processing memory capacities 
protocol pieces efforts achieve scalable cache coherence massively parallel processors arch 
protocol uses write policy cli ent client data transfers implements write ownership takes advantage cluster servers 
compare effects protocol baseline afs system 
base comparison event driven simula tion parameterized model service demands tile system requests decstation 
compared performance systems workload taken large nfs system berkeley file server client workstations 
principal result workload experimental protocol reduced server load factor 
addi tion show parts protocol sig impact performance protocol reduces average server load significantly reduces peak demand server 
aggregate client memories effective reducing disk server memory cluster servers isolate communication clusters desirable clusters connected wan 
study addresses number issues arise clients responsible tile system services 
problems scalable backup data availability presence client failures 
security clients supply data 
currently implementing protocol described part file system called xfs 
facilitate comparison afs assumes afs policy synchronizing file consis tency file closed writing assumes file caching 
implementation xfs restrictions 
actual xfs implementation stripes data raid distributed client disks improve bandwidth availability consider issues 
section outlines file system caching algo rithms nfs afs motivates alternative strate gies consider 
section describes workload section discuss key aspects simulation 
section details results study emphasis impact protocol server scalability network load cli ent load 
section examines potentially thorny issues backup availability security arise server survey related network file system studies section 
section summarize 
file system cache protocols important factor file system scalability caching policy 
file systems caches improve response time reduce server load 
clients access data memory caches quickly access remote data server 
file caches reduce server load satisfying requests server interaction 
caches 
introduces problem cache consistency different caches may hold copies file file changed client changes seen file read different client 
cached copies kept consistent large effect server scalability section describes cache protocols industry standard nfs emerging afs standard scalable xfs 
existing protocols nfs current industry standard distributed file system pro tocol designed provide response time moderate numbers clients provide scalability 
nfs caches file system data main memory client workstation 
nfs attempt client local disk space cache attempt keep file data strictly coherent 
periodic built special hardware allow support large number clients 
invalidations file attribute information ensure new data eventually seconds replace date cached copies 
file attributes invalidated time fik referenced client verify cached copy current client fetch new data server 
clients write modified tile data server ensure fetches clients receive new data 
nfs maintains separate caches data attributes names caches data block basis 
nfs scalability limited relatively small 
memory file caches larger caches possible disks 
nfs policy periodically invalidating attributes guarantees stream client requests server attributes expire files modified 
nfs write policy sends changes server disk clients data 
afs improves nfs large local disk cache client callbacks cache con 
afs uses level cache client 
memory file cache similar nfs file cache provides response time accesses misses memory file cache go disk file cache go server satisfied 
requiring periodic file consis tency nfs afs reduces server load call backs server maintains hst cached copies data file notifies clients client modifies file 
client fetches new data server time opens file 
afs clients send modified data server file closed guaranteeing server current version data allowing server know invalidate cached copies 
clients cache directory information write directory caches 
modifications directories sent immediately server maintains callbacks keep cached copies directory information consistent 
despite improvements afs scalability limited 
communication data transfer takes place cli ents server direct client client communication allowed 
particular 
server supplies data time client cache receives data time client closes file bas written 
central server disk space store file system data despite fact aggregate size client disks typically larger server disks 
server responsible fielding directory operations generating callback mes sages cache coherence operation 
xfs protocol consider effect separate optimiza afs protocol 
push server client machines 
collectively refer xfs protocol 
optimization proposed way improve scalability multiprocessor hardware caches suggested file systems 
section evaluate performance impact individually find important get performance 
optimization write client client data transfers 
eliminate file transfers server making server responsible coordinating data flow client client 
aspects protocol eliminate need buy specialized server machine configured large amount disk space 

write 
clients longer write data server close 
inform server update server invalidates cached copies clients callbacks 
modified files remain client local disk 
elimination write motivated number studies showing client writes tile deleted quickly rewritten client thom bake 
confirmed pattern berkeley nfs traces 
discarding writes day simulation bytes sent server afs overwritten deleted read client read client read 
write 
bytes overwritten discarded client transferred server 
note need write data server ensure data durability 
delayed writes seconds network tile systems reduce writes server putting data risk loss client crash nels xfs client disks allows complete write files written server 

client client data transfers 
client cache sends request data server server forwards request client currently caching needed data 
second client sends desired data directly wants data 
client client data transfers reduce server load replacing large server data transfer small forwarding packet 
direct client client communication permits write policy implemented significant delays incurred requested dirty data written cen server supplied server 
client client data transfers example separating control data paths suggested mass storage model 
parts protocol allow data stored client disks implementing referred sors cache memory architecture coma hage rost 
important detail approach guar clients don discard copy file 
cli ents copy tile permanent copy marked written marked copies may passed clients discarded 
client cache full lt sends marked copies normally discard ran selected client 
client notifies server transfer 
marked data copies managed way unmarked data copies 
final optimization write ownership clustering try reduce demands server coordinating cached copies files 

write ownership cache consistency 
time client closes modified file server triggering invalidation copies cached clients 
point ow arch may modify file freely notifying server copies data 
lose exclusive ownership file client opens file reading 
copy lf client acquires exclusive ownership write ownership optimization write invalidate consistency protocol afs callback mechanism 
allows eliminate messages server com mon case repeated writes client tile 

clustering 
clusters formed selecting groups stations closely cooperate near network topology instance lan 
cluster servers keep track ownership callback state clients cluster 
central server tracks file reformation cluster level relying cluster server forward requests specific clients caching data 
cluster servers isolate ownership changes data trans internal cluster central server 
instance ownership transferred clients cluster cluster server notified cen server need 
hand ownership transferred clients different clusters central server involved know new cluster server responsible tracking ownership file 
clustering dash multiprocessor architec ture clusters processing nodes busses cluster lans 
clustering improves loading central server state cluster servers isolating central server changes state affecting clients cluster sand 
clustering allows sys tem wide area network context organizing commu nication cluster lan networks wan links necessary 
xfs clustering distinct name space splitting read methods ut multiple servers nfs afs name space splitting improves tile system ity manually splitting file system logical pieces 
managed different server 
lt difficult divide files servers balance load avoid hot spots wolf 
name space splitting useful dif ferent parts tile system managed different tive xfs support splitting central servers cluster server pro combined file system 
fde systems multiple servers improve scalability reading files cost file writes expensive show section xfs style clustering reduces cost reads writes changes xfs dra scalable afs 
central server longer data transfers coordinates smaller amount control activity central server forward read requests clusters satisfied cluster central server send consistency messages clusters modification cluster cached copies central server informed files created deleted knows files exist forward requests files 
trace overview evaluate performance impact changes gathered traces nfs file system activity large nfs served file server 
system includes clients spread ethernets connects directly central server 
trace spans days noted measurements appear cover days trace day activity warm caches 
full day trace files referenced 
gathered trace monitoring network activity ethernets 
subnet placed workstation monitored network traffic built ultrix interface 
trace period reported dropped network traffic calls due buffer overflow 
nfs trace reflect semantics afs xfs protocols 
gathered traces net access nfs network traffic intro duced biases nfs raw trace 
instance nfs network visible open close calls get get file attribute calls really validating cache consistency 
step postprocessing added opens closes trace 
added file opens access file 
read read write opens signal afs xfs bring file accessed local disk cache 
inserted file closes immediately tile access long minute period inactivity tile block write block zero tile series writes parts file 
afs write close semantics close 
newly written file supplied subsequent read open 
block reads writes trace caused nfs memory cache misses 
afs version xfs simulated reads writes cause local disk traffic network activity file caching assumed tile consistency handled file opened closed 
included nfs directory reads writes afs xfs directory reads writes 
directories simulated semantics directory write immediately visible entire system 
afs implements writing directory changes server xfs uses file ownership dation mechanisms 
include nfs getattr calls simulator requests file attributes 
excluded get calls access block file assuming calls nfs cache validation packets 
simulator dynamically eliminates get calls filtering calls attribute cache 
attribute cache kept consistent way directory cache protocol 
attribute invalidated file lt written 
note simulating access time attribute updated tile read afs xfs 
resulting trace measured afs loads macro characteristics 
simulated afs server supplied average mb clients day read opens measured mb day large afs installation 
measured mb client day write back load mb client day loads measured 
trace reflects file system activity real system 
enhances confidence trace realistic capabilities traced system limit activity seen trace 
prime example limitation peak load 
trace underestimate peak server load imposed scalable system reasons 
speed traced system spread requests resulting longer periods activity lower peaks 
second 
users tend avoid operations take long time traced system lowering peak load 
sharing example system limitations may distort workload 
nfs weak data sharing semantics users attempt share data 
tiles shared afs xfs see increased server loads afs increase sharing requires data transfer server sharing xfs read forwarding invalidation packets 
simulator methodology built simulator evaluate performance afs xfs traced workload 
simulator starts wdh model clf system behavior describing actions taken implement afs xfs protocols 
parameterized system reflect performance real hardware 
subsections describe system model hardware parameters 
system model simulator provides average resource utilization detailed performance information 
simplest case determine average processor disk network utilizations simulating cache behavior trace input counting accesses different hardware resources 
get detailed perfor mance information adding event driven model cache simulation measure response time different requests monitor burstiness utilization different parts system 
event driven hardware model includes queuing delays 
rest subsection provides details simulated caches 
simulations xfs afs include ln memory client file caches 
afs simulations include memory tile cache server 
caches simulated file caching simplicity practice afs xfs cache chunks files break large transfers kb chunks realistic latency measurements 
assume memory caches mb client mb afs server give mb disk file cache 
simulations include attribute caches cie ents access file attributes fetch ng entree file 
client entry memory attribute cache backed disk server entry memory cache 
tht server supplies attributes systems attribute con protocols files 
cache behavior crucial performance large scale file systems warm caches gathering statistics 
results gathered days day trace warming caches day saturday 
plots hit rate read opens satis fied completely memory cache time indicates day hit rate fluctuates 
appears general upward trend expect caches warm 
steady state hlt rate rela low opens completely satisfied nfs local memory cache dld appear trace caches 
read opens local disk access files referenced earlier trace assumption xfs owns tiles 
arbitrary assume file unknown location stored randomly selected client disk 
assumption file opens located normally currently caching data 
hardware parameters estimate performance xfs compare lt afs absent parameterized model reflect performance mid range workstation tasks performed xfs afs implementation 
approximate performance decstation measured reported performance results subsystems summarized 
hardware resource services request time req 
ba 
approach clearly oversimplification requests piece hardware overheads band widths actual overhead exactly match current systems 

simple assumptions provide starting point system evaluation 
similar approach performance network file system simulations 
processor overhead time represents cpu memory subsystem time send receive network request small amount file system 
estimated time time decstation handle nfs get request 
cpu bandwidth large requests afs local disk hlt rate day simulation 
local hit rate time 
plot local disk cache hit rate day simulation afs suggests thai day sufficient warm caches 
hour hour fluctuations smoothed plot averaging previous hours point 
overhead bandwidth processor ms mb disk ms network ms xl mb 
service demand parameters 
time supply file system data memory file cache reported chen 
assume machines disks rotate rpm typical seek time ms disk band width mb base network topology configuration cll ents nfs trace subnets connected server 
afs subnet connects directly server 
xfs subnet connects cluster server 
cluster servers con central server fifth subnet 
network latency time transmit minimum sized ethernet packet network bandwidth optimistic estimate net bandwidth available ethernet whale performance assumptions result differ ences absolute latency burstiness numbers reported affect central xfs pro tocol scales better afs 
results section presents results simulations 
show proposed reduce server load factor compared afs 
show xfs pro tocol greatly reduces peak bursts server load 
xfs signifi cantly reduces total network load distribution traffic remains better suited mixed environment 
increased responsibility protocol places clients increase client file system load slightly extra forwarding read requests increase response time 
subsection presents results detail subsection examines individual impact main aspects xfs write pol icy transfers write ownership clustering 
find techniques significant contributions performance 
results section compares xfs protocol afs terms server cpu load server load response time load client load 
summarizes results showing xfs reduces server load bv 
factor compared afs 
load estimate total server processor demand including overhead bandwidth described section expressed fraction afs server demand 
find xfs reduces server load compared afs eliminating data server reducing number messages server handle 
server server server messages data load afs gb xfs gb 
total server load 
normalized server load expresses server cpu load simulated protocols fraction simulated server load afs 
estimate typical disk overhead differs average seek time reported manufacturers accounts locality seen real workloads manufacturer reported average seek mean time possible source destination tracks seeks average third distance disk surface details server load type operation demands 
write close operations server include write afs notifying server write file write owned xfs messages sent server invalidate cached copies 
read open operations server caused client misses include handling client request supplying data afs forwarding request xfs 
delete operations include message sent server indicate file deleted server messages notifying clients caching file 
attribute messages include packets request update invalidate file attribute information 
cate gory includes packets sent received server xfs client notifies server file potentially forwarding marked data cache room new data 
addition total server performance scalability dependant periods heavy load 
summarizes distribution time spent increasing levels server load afs xfs 
shows xfs reduc tion average load translates reduction time spent high load 
indicates afs server spends min day working loads xfs server loaded heavily 
extremely low peak demands xfs suggest scale system larger number clients afs 
note tbe absolute load level machines relatively low suggesting server probably handle berkeley workload 
noted earlier maximum load system experiences simulation limited maximum load accepted nfs system workload trace gathered 
having servers forward read requests potentially increase latency 
measurements show aggregate effect client memory caches minimizes impact extra step 
focus time spent open file reading request issued chunk kb arrives local disk 
consider requests local disk additional network communication requests satisfied network 
breaks response time files 
opens satisfied locally requiring disk accesses account opens satisfied quickly systems 
misses satisfied remote memory cache misses require remote disk accesses slightly slower xfs implementation extra step increase cost xfs misses satisfied remote client memory afs xfs write close attribute total 
server load breakdown 
portion afs server load due type request 
cache afs requests satisfied server memory cache 
higher remote client memory cache hit rate initially surprising afs server memory cache mb xfs client cache just mb 
note aggregate size client caches mb making effective file cache files stored distributed cache duplicates 
note server attempt keep track clients file cached memory disk obvious optimization increase server load 
plots remote memory hit rate xfs function client memory cache size indicates mb server cache equivalent mb client caches 
network load number bytes transferred net trace important metric scalability 
particularly concerned minimizing network usage wide area network file systems network bandwidth 
ser er load level second inter als 
cumulative distribution server load 
axis amount load server second interval load sum service demands requests arrive server second interval 
value amount time day server experienced load level 
afs server handled request second minutes day xfs server completely idle minutes day 
circled points indicate afs server load minutes day xfs server load high seconds day 
afs xfs freq 
time freq 
time local ms ms remote rns ms mem ms msl disk ms ms total ms ms 
read open response time 
response time time needed put chunk opened file local disk return 
local hits data local disk 
afs xfs data local cache fetched remote machine 
afs remote machine server xfs remote machine client 
remote machine desired data may disk memory cache 
bottleneck 
bandwidth issue mobile computing wireless interconnects 
summarizes total network traffic afs xfs assumption packet sent header bytes 
table indicates xfs reduces total network traf fic 
major difference total network bandwidth xfs elimination write traffic files modified deleted client 
xfs reduces total network traffic factor compared afs significantly changes nature traffic 
shows client client transfers reduce bytes transferred server 
reduction crucial file systems server may located wan 
clustering reduces number bytes oo xfs aggregate client cac 
afs mb server cache afs 
mb server cache afs mb server cache client memory cache size mb 
remote memory cache hit rates local misses 
afs xfs packets overhead mb mb data bytes mb mb write mb mb data mb mb total bytes mb mb 
network traffic xfs afs sources 
total bytes transferred estimate formed adding total data bytes plus bytes request reflect protocol overhead control information 
packet count xfs reported differs number messages reported considered traffic server 
cluster afs total traffic consideration clusters separated gateways wans 
note client load increased slightly clients shoulder considerably bility xfs afs instance supplying data clients 
xfs protocol increases total amount file system done clients measured workload 
increase client load small client load comes local block reads writes unchanged xfs 
xfs increases load client handle data requests clients reduced write activity largely offsets increase 
fraction smaller trace included larger amount file system activity purely local reads hit client memory cache 
shows demands clients greatly altered xfs protocol 
afs xfs central server mb mb cluster nia mb cluster mb 
total network traffic different parts network 
total includes data byte packet header 
wan large scale environment connection central server clusters may slower network cluster 
cli ent load level se ond inter als afs xfs 
cumulative distribution second load levels clients 
average client processor active doing file system activity minutes day 
xfs afs client loads indistinguishable 
protocol write read delete attr 
total afs write client client write ownership clusters full xfs 
server load type activity protocol 
afs line indicates server load stemming write closes read opens deletes attribute operations operations 
subsequent line shows breakdown total part xfs protocol added 
server loads changed significantly previous line highlighted 
protocol breakdown section considered aggregate effect studied 
consider individual effects 
conclude optimization contributes sig performance xfs protocol 
sum load part xfs protocol added system 
section explains benefits strategies detail simply eliminating write afs reduce server load workload 
server load associated closing files written reduced nearly factor rf clients need send small notification message server transmitting mod ified tile larger messages 

server load associated supplying read misses increased slightly server write backs modified files clients want read 
bytes written server afs read show increased read load 
small load category 
comes write throughs free cache space 
utilizing client data transfers reduces server load amount equal times original afs load 
reduction comes elimination data transfers server read opens 
note category increased slightly 
increase messages clients send free space discarding files caches 
server informed clean files discarded doesn forward read requests client longer ing desired data 
write ownership reduces number messages processed server server load 
additional com pared client client line 
indicates messages notifying server file closed eliminated write ownership 
cluster servers reduce messages types intercepting requests handled central server 
clus ter servers reduce server load reading files reading attributes forwarding requests satisfied cluster 
read forwarding primary benefit cluster serv ers 
number write close messages reduced reduction comes sources combine reduce server load times afs load 
write closes transfer ownership cluster 
writes handled cluster server reducing central server load times afs original load 
significantly cluster server acts fan invalidation packets central server 
invalidation packet central server cluster server sufficient invalidate data copies cluster 
notify invalidate protocol server client write ownership 
write close messages write ownership 
server messages tell server invalidate cached copies file 
invalidates files invalidate client messages 
tions reduce central server load times original load load delete messages messages processed central server reduced slightly 
delete mes sages invalidating multiple copies file caches cluster eliminated cluster server distributes messages appropriate clients cluster 
messages reduced file discarded cache cached client cluster case server may requests file cluster need notified change 

note cluster servers loads ranged fraction afs server load 
words 
load xfs central server 
considered extending strategy write ownership files include ownership directory sub trees allow avoid notifying server file creations deletions 
client owned du notify server file creations deletions ownership dn containing file lost 
strategy exploit common case files created 

deleted seen client 
simulations measure benefits directory ownership allow place upper bound benefits noting load write closes newly created files load deletes messages notifying server delete 
messages omitted 
total server load reduced umts reduction xfs protocol optimistic case 
conclude improvement justify considerable added complexity approach 
sources load reduced fur ther reduction significant fraction remaining load reexamined 
challenges decentralized operation xfs protocol reliance client disk caches improves scalability introduces potential challenges reliable operation 
provide backup scales number clients ensure files highly avail able despite distributed disks pro vide security guarantees unauthorized clients read change data store 
find data replication natural part xfs protocol backup easier increases availability 
message digests provide security data supplied clients 
backup xfs ability manage multiple data copies normal opera tion manage backup copies data 
simplifies design system allows multiple backup archives scale backup bandwidth rest system scales 
xfs treats archive client server keeps track backup copies data just lt tracks cached copies 
client cache backs file sending archive telling server new copy 
system policies frequency backup deciding potentially clients caching file responsible backing decision additional communication retrieving data backing store 
plan tertiary storage robots manage backup media 
tertiary storage robots provide hundreds gigabytes tens terabytes storage file access times measured tens seconds katz 
robots provide deep storage tradi tional tape systems added advantage files line sense user may access data human intervention 
tertiary robots requirement xfs backup done traditional line tapes 
advan tage storage robots backup data may sent retrieved tertiary storage system operator inte allowing system automatically provide services access old versions files deleted files 
note server disk need backed server reconstruct hst cached copies metadata polling cluster servers nels 
backup network exerts small additional load system 
include load simulations impact performance small 
noted section tiles overwritten deleted quickly need copied client backup archive 
fur ther backup may scheduled periods low system load avoid disturbing regular system 
availability xfs second challenge 
tile system spread machines probability machines containing file system data unavailable increases 
availability problems mitigated large client caches file rep file access patterns observed trace 
higher availability achieved explicit data replica tion 
large client caches file replication caching backup reduce xfs vulnerability unavailable clients 
large cli ent caches insulation crash machine noticed 
xfs automatically stores redundant copies shared read files different caches increasing tiles line backup pro vides added copies older files properties xfs mean files written backed read second vulnerable single point failures 
files written client seldom read 
vulnerable files seldom accessed writer 
estimate trace tile average cli ent go hundreds days noticing file unavailability stemming crash 
low rate suggests xfs protocol change data availability stall dominated availability server 
calculation assumed fail randomly exponentially distributed mean time failure days exponentially distributed mean time repair hour 
assumed data backed line tape robot am 
assumptions day average try access data cached unavailable client 
day trials confidence interval 
availability prob lem write sharing data widespread seen trace 
user machine crashes 
user may able switch alternate modified data unavailable crashed machine recovers stronger guarantees needed client data replication modified data provides scalable solu tion 
shortly client closes file writing send data 
solution scalable adds additional server messages server knows ahead time clients mirror writes 
copy delay chosen trade performance avail ability guarantees longer delays significantly reducing client bandwidth bake increasing length time file vulnerable single point failure 
plan investigate trade offs client transfers plan look striping reduce cost high availability 
current simulations additional client copies 
security xfs client disks store supply data raises security concerns data confidentiality integrity 
want clients transfer data cache authorized read data want accept altered data malicious client client client transfer 
believe trust clients cluster enforce system data access rules 
communication trusting clients steps described 
confidentiality data cached client disks addressed afs techniques apply xfs data read client 
xfs adds new way data brought client cache clients flush data caches fill 
flushing rare performance impact chosen strategy limited extreme case clients trusted data data encrypted flushed 
decrypt lf accessed 
commonly data flushed subset trusted instance clients cluster administrative domain 
xfs guarantee data integrity allowing clients accept data untrusted clients guaranteeing things secure copy file exists detect file altered secure image 
guarantee existence data system trust client created data event client permission modify data trust hne backup archive 
client created data pins copy cache file backed robotic storage 
file backed writer free flush data sion system may recover file tape robot 
client verifies untrusted data spe cial checksum calculated efficiently infeasible create different data match 
server stores bit digest kb data chunk list chunks cached clients 
wards client request data includes digest pris hne data forwarding packet 
digest forwarding packet protected encrypted digital signature nis 
protected part forwarding packet include request identifier protect playback attacks 
supplying data forwards protected digest data 
verifies data nal digest 
file corrupt asks server copy source 
computing digests done clients digests severely impact server scalability 
digests supplied clients reading data additional network pack ets updated server file write owner ship lost 
server forwards data read request encrypt short message including digest request identifier append message forwarding packet 
encrypting message small compared sending packet digests increase server load read requests 
tion hard compared sending message unprotected digest may sent directly client requesting read separate message 
simulation assumes cluster servers trusted clients cluster cluster server knows digest particular file chunk cluster server may ward digest appropriate client 
digests change file written clients calculate new digest send server lose write ownership 
simulated message digests assuming signature encryption cheap compared sending message 
case additional server load receiving digest updates write ownership lost 
increases server load afs total server load 
message digests severely impact response time 
measured bandwidth compute md digest dec alpha axp mb consistent processor speeds 
simulated md cal measured decstation md band width mb calculate message digest data received trusting clients read open time files ms just ms slower ms read open time reported section xfs message digests 
md cal bandwidth exceeds bandwidth network impact performance minimal 
approach attractive processor speed improvements continue system improvements 
related evaluates effectiveness file system combines strategies eliminating write cli ent transfers write ownership clustering 
fusion strategies produced system believe scale size wide area networks 
section surveys combinations schemes suggested meth ods achieve scalable tile systems 
andrew file system 
afs 
designed scalability main criteria 
andrew large disk client caches reduce tile reads server 
second callbacks reduce number pro tocol messages handled server 
xfs large disk client caches callbacks generalizes additional techniques 
mass storage system model ples location name service actual storage data 
model defines name server server locate storage server manages 
gold implemented afs storage system allows data reside separate 
xfs cli ent acts storage server server cluster servers act level location server 
study indicated division greatly reduced load central resource ce server 
sprite nels uses delayed writes server reduce server load 
diskless sprite clients 
write data server seconds reduce ity crashes 
xfs extends delayed writes write clients local disks 
blaze alonso suggest dynamically building hierarchies widely shared data 
server sup plied threshold number copies tile server refuse supply data clients 
request forwarded client caching file 
clients acting intermediate servers responsible keeping callback information files supplied caches 
authors suggest number strategies clients may guess client desired data going server 
hinting techniques applied xfs imple mentation 
muntz honey man studied effect putting intermediate data server central server cli ents andrew system 
hit rates inter mediate server surprisingly low 
client caches mb small disk cache reduced intermediate cache hit rate traces studied 
reason diffi cult give intermediate server big cache hold sig amounts data client caches 
xfs designed intermediate servers field consistency requests intermediate servers store data believe feasible storage servers hold cluster consistency information system pang sand implements replication files cluster servers 
accesses data remote cluster creates copy data local cluster 
clients different protocol nfs access data local cluster server 
cluster servers differ xfs clus ter servers cluster servers act intermediate data caches clients remote servers xfs cluster servers merely monitor location file copies clus ter 
concept locating server responsible tracking current owner file xfs central server 
authors studied behavior shared tiles syn workload cluster replication improved perfor mance server load shared files degree cluster locality low clusters perform files read cluster invalidated 
evaluate xfs caching proto col designed improve network file system scalability full advantage clients processors 
memories disks 
files stored clients data transfers go directly client client 
server coordinate transfers clients 
xfs reduces server load necessary coordination write ownership clustering cases allowing clients cluster servers avoid interacting central server 
evaluated performance ifs trace driven simu clients xfs reduced server load compared afs eliminating server data transfers reducing number messages server data storage data transfer responsibilities xfs possible build large network file sys tem commodity desktop workstations 
file server acknowledgments ike matt blaze providing hls tools formed basis trace pro cessing tools 
john hartman anonymous referees comments helpful improving 
arch james archibald jean baer 
cache coherence protocols evaluation multiprocessor simulation model acm transactions cm computer systems 
november 
bake mary baker john hartman michael kupfer ken 
john ousterhout measurements distributed file system 
proc 
th 
pages october 
andrew birrell andy chuck timothy mann 
swart 
echo distributed fde system technical report digital equipment systems research center matt blaze rafael alonso 
long term caching strategies large distributed fde systems 
proc 
zer usenix pages june 
matt blaze rafael alonso dynamic caching large scale distributed file systems 
oc 
oj th con dl cf systems pages june 
matt blaze 
large scale distributed file systems 
phd thesis princeton january 
chen peter chen david patterson 
new approach performance evaluation self scaling benchmarks predicted performance 
proc 
acm sigmetrics pages may 
robert coyne harry 
mass storage system model version fth ieee symposium mass storage stems pages april 
gold jonathan kathy woody brown christopher christopher maher daniel 
afs supercomputing environment 
ieee symposium mass storage systems pages april 
hage erik anders landin seif har dl 
ddm cache memory architecture 
ieee 

john hennessy patterson 
computer approach 
morgan kaufmann publishers 
hitz david hitz guy harris james lau allan schwartz unix component distributed kernel multiprocessor fde servers proc 
winter pages john howard michael kazar menees david nichols satyanarayanan robert sidebotham michael west 
scale performance distributed file system acm transactions systems 
february 
katz randy katz thomas anderson john ousterhout patterson 
line storage low latency high capacity storage systems geographically distributed networks 
sequoia technical report university 
september 
james satyanarayanan 
disconnected operation coda file system acm transactions systems february 
edward lazowska john zahorjan david cheriton zwaenepoel 
file access performance diskless workstations acm transactions computer stems 
august 
lenoski laudon gharachorloo gupta hennessy 
directory cache coherence protocol dash multiprocessor 
proc 
th international symposium pages may 
barbara sanjay ghemawat robert gruber paul johnson michael williams 
harp file system proc 
th wan operating pri es 
pages 
october 
mogul 
rashid accetta 
packet filter user level network code 
proc 
th acm operating systems es muntz honeyman 
multi level caching file systems cache ain trash 
proc 
usenix pages january 
nels michael nelson brent welch john ousterhout 
caching sprite network file system 
acm 
systems february 
nis digital signature standard proposed nist 
acm july 
pang james pang gall 
zhou implementation performance cluster file large scale distributed systems 
technical report computer science research institute toronto august 
rivest 
md message digest algorithm 
request comments network working group april 
rost smirni wagner 
ksr experimentation modeling proc 
acm sigmetrics pages 
sand russel sandberg goldberg steve kleiman dan walsh bob lyon 
design implementation sun network 
proc 
summer usenix 
pages june 
sand sandhu zhou 
cluster fde replication large scale distributed systems 
proc 
acm sigmetrics pages june 
satyanarayanan 
integrating security large distributed system 
acm transactions systems pages august 

scalable secure highly available distributed file access 
ieee computer pages may 
satyanarayanan 
usage evaluation wide area distributed file system 
proc 
winter january 
thom james gordon thompson 
analysis systems 
phd thesis california berkeley 
wolf joel wolf 
placement optimization problem practical solution tbe disk file assignment problem proc 
acm sigmetrics pages may 
