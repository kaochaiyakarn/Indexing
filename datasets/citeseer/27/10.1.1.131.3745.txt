laplacian eigenmaps dimensionality reduction data representation mikhail belkin lambda niyogi december central problems machine learning pattern recognition develop appropriate representations complex data 
consider problem constructing representation data lying low dimensional manifold embedded high dimensional space 
drawing correspondence graph laplacian laplace beltrami operator manifold connections heat equation propose geometrically motivated algorithm representing high dimensional data 
algorithm provides computationally efficient approach non linear dimensionality reduction locality preserving properties natural connection clustering 
potential applications illustrative examples discussed 
areas artificial intelligence information retrieval data mining confronted intrinsically low dimensional data lying high dimensional space 
consider example gray scale images object taken fixed lighting conditions moving camera 
image typically represented brightness value pixel 
pixels corresponding theta image image yields data point intrinsic dimensionality space images object number degrees lambda university chicago department mathematics math uchicago edu university chicago departments computer science statistics niyogi cs uchicago edu freedom camera 
case space consideration natural structure low dimensional manifold embedded renewed interest tenenbaum roweis saul problem developing low dimensional representations data arises sampling probability distribution manifold 
geometrically motivated algorithm accompanying framework analysis problem 
general problem dimensionality reduction long history 
classical approaches include principal components analysis multidimensional scaling 
various methods generate nonlinear maps considered 
self organizing maps neural network approaches see haykin set nonlinear optimization problem solution typically obtained gradient descent guaranteed produce local optimum global optima difficult attain efficient means 
note approach generalizing pca kernel techniques shortcoming 
methods explicitly consider structure manifold data may possibly reside 
explore approach builds graph incorporating neighborhood information data set 
notion laplacian graph compute low dimensional representation data set optimally preserves local neighborhood information certain sense 
representation map generated algorithm may viewed discrete approximation continuous map naturally arises geometry manifold 
worthwhile highlight aspects algorithm framework analysis 

core algorithm simple 
local computations sparse eigenvalue problem 
solution reflects intrinsic geometric structure manifold 
require search neighboring points high dimensional space 

justification algorithm comes role laplace beltrami operator providing optimal embedding manifold 
manifold approximated adjacency graph computed data points 
laplace beltrami operator approximated weighted laplacian adjacency graph weights chosen appropriately 
key role laplace beltrami operator heat equation enables heat kernel choose weights principled manner 
embedding maps data approximate eigenmaps laplace beltrami operator maps intrinsically defined entire manifold 

framework analysis explicit connections interpret dimensionality reduction algorithms geometric fashion 
addition algorithms able reinterpret proposed locally linear embedding lle procedure roweis saul framework 
connections laplace beltrami operator graph laplacian known specialists spectral graph theory see chung chung yau best knowledge aware application dimensionality reduction data representation 

locality preserving character laplacian algorithm relatively insensitive outliers noise 
show fact trying preserve local information embedding algorithm implicitly emphasizes natural clusters data 
close connections spectral clustering algorithms developed learning computer vision particular approach shi malik clear 
sense dimensionality reduction clustering sides coin explore connection detail 
contrast global methods tenenbaum show tendency cluster attempt preserve pairwise geodesic distances points 

discussion seung lee roweis saul tenenbaum motivated role non linear dimensionality reduction may possibly play human perception learning worthwhile consider implication previous context 
biological perceptual apparatus confronted high dimensional stimuli recover low dimensional structure 
approach recovering low dimensional structure inherently local example algorithm proposed natural clustering emerge may serve basis emergence categories biological perception 

approach intrinsic geometric structure manifold exhibits stability respect embedding 
example moving camera different resolutions camera different choices theta image grid lead embeddings underlying manifold spaces different dimension 
algorithm produce similar representations independently resolution 
problem dimensionality reduction generic problem dimensionality reduction 
set xk points find set points yk yi represents xi 
consider special case xk manifold embedded consider algorithm construct representative yi special case 
sense representation optimal clear 
algorithm points xk construct weighted graph nodes point set edges connecting neighboring points 
embedding map provided computing eigenvectors graph laplacian 
algorithmic procedure formally stated 

step constructing adjacency graph 
put edge nodes xi xj close 
variations ffl neighborhoods 
parameter ffl nodes connected edge gamma xjk ffl norm usual euclidean norm advantages geometrically motivated relationship naturally transitive 
disadvantages leads graphs connected components difficult choose ffl 
nearest neighbors 
parameter nodes connected edge nearest neighbors nearest neighbors advantages easier choose tend lead disconnected graphs 
disadvantages geometrically intuitive 

step 
choosing weights 
variations weighting edges heat kernel 
parameter 
nodes connected put wij gamma gamma xjk justification choice weights provided 
simple minded 
parameters 
wij vertices connected edge 
simplification avoids necessity choosing 
step 
eigenmaps assume graph constructed connected proceed step connected component 
compute eigenvalues eigenvectors generalized eigenvector problem lf df diagonal weight matrix entries column row symmetric sums dii wji 
gamma laplacian matrix 
laplacian symmetric positive semidefinite matrix thought operator functions defined vertices fk gamma solutions equation ordered eigenvalues lf df lf df delta delta delta computer implementation algorithm steps executed simultaneously 
gamma gamma gamma delta delta delta gamma leave eigenvector corresponding eigenvalue eigenvectors embedding dimensional euclidean space 
xi 
fm justification optimal embeddings show embedding provided laplacian algorithm preserves local information optimally certain sense 
section standard spectral graph theory 
see chung comprehensive 
recall data set construct weighted graph edges connecting nearby points 
purposes discussion assume graph connected 
consider problem mapping weighted graph line connected points stay close possible 
yn map 
reasonable criterion choosing map minimize objective function ij yi gamma yj wij appropriate constraints 
objective function choice weights wij incurs heavy penalty neighboring points xi xj mapped far apart 
minimizing attempt ensure xi xj close yi yj close 
turns yi gamma yj wij ly gamma see notice wij symmetric dii wij 
yi gamma yj wij gamma wij dii djj gamma ly note calculation shows positive semidefinite 
minimization problem reduces finding argmin yt dy ly constraint dy removes arbitrary scaling factor embedding 
matrix provides natural measure vertices graph 
bigger value dii corresponding ith vertex important vertex 
follows equation positive semidefinite matrix vector minimizes objective function minimum eigenvalue solution generalized eigenvalue problem ly dy constant function vertex 
easy see eigenvector eigenvalue 
graph connected eigenvector 
eliminate trivial solution collapses vertices real number put additional constraint orthogonality look argmin yt ly yt ly solution eigenvector smallest non zero eigenvalue 
consider general problem embedding graph euclidean space 
embedding theta matrix ym ith row provides embedding coordinates ith vertex 
similarly need minimize ky gamma wij tr ly ym dimensional representation ith vertex 
reduces finding argmin dy tr ly dimensional embedding problem constraint prevents collapse point 
dimensional embedding problem constraint prevents collapse subspace dimension standard methods show solution provided matrix eigenvectors corresponding lowest eigenvalues generalized eigenvalue problem ly dy 
laplace beltrami operator laplacian graph analogous laplace beltrami operator manifolds 
section provide justification eigenfunctions laplace beltrami operator properties desirable embedding 
smooth compact dimensional riemannian manifold 
manifold embedded riemannian structure metric tensor manifold induced standard riemannian structure graph looking map manifold real line points close manifold get mapped close line 
map 
assume twice differentiable 
consider neighboring points mapped respectively 
show jf gamma gradient rf vector tangent space mx vector mx df 

geodesic curve parameterized length connecting 
df dt idt schwartz inequality kc parameterized length kc 
taylor approximation 
integrating jf gamma infinitesimal sense 
isometrically embedded kx gamma kx gamma jf gamma ky gamma xk ky gamma xk see provides measure far apart maps nearby points 
look map best preserves locality average trying find argmin kfk integral taken respect standard measure riemannian manifold 
note minimizing corresponds directly minimizing lf fi gamma fj wij graph 
turns minimizing objective function eq 
reduces finding eigenfunctions laplace beltrami operator recall lf def gamma div div divergence vector field 
follows stokes theorem gamma div formally adjoint operators function vector field hx rf gamma div see positive semidefinite 
minimizes eigenfunction spectrum compact manifold known discrete 
eigenvalues increasing order fi eigenfunction corresponding eigenvalue easily seen constant function maps entire manifold single point 
avoid eventuality require just graph setting embedding map orthogonal 
immediately follows optimal embedding map 
arguments previous section see 
fm provides optimal dimensional embedding 
boundary appropriate boundary conditions need assumed 
heat kernels choice weight matrix laplace beltrami operator differentiable functions manifold intimately related heat flow 
initial heat distribution heat distribution time 
heat equation partial differential equation 
solution ht ht heat kernel green function partial differential equation lf gamma lu gamma ht turns appropriate coordinate system exponential order coincides local coordinate system tangent plane ht approximately gaussian 
ht sst gamma gamma kx gamma yk oe oe smooth function oe 
close small ht ss sst gamma gamma kx gamma yk see rosenberg details 
notice tends heat kernel ht increasingly localized tends dirac ffi function lim ht 
small definition derivative lf ss gamma sst gamma gamma kx gamma yk dy xk data points expression approximated lf xi ss xi gamma sst gamma xj gamma xik ffl gamma gamma xjk coefficient global affect eigenvectors discrete laplacian 
inherent dimensionality may unknown put ff sst gamma interesting note laplacian constant function zero immediately follows ff xj gamma xik ffl gamma gamma xjk ff xj gamma xik ffl gamma gamma xjk gamma observation leads possible approximation schemes manifold laplacian 
order ensure approximation matrix positive semidefinite compute graph laplacian weights wij gamma gamma xjk gamma xjk ffl connections spectral clustering approach dimensionality reduction considered utilizes maps provided eigenvectors graph laplacian eigenfunctions laplace beltrami operator manifold 
interestingly solution may interpreted framework clustering close ties spectrally clustering techniques image segmentation shi malik load balancing hendrickson leland circuit design hadley 
briefly outline ideas spectral clustering interest cluster set objects finite number clusters 
set objects visual perceptual linguistic may introduce matrix pair wise similarities objects 
possible formulate general graph theoretic framework clustering follows 
weighted graph matrix weights vertices numbered arbitrarily 
weight wij associated edge eij similarity vi vj 
assume matrix pairwise similarities symmetric corresponding undirected graph connected 
graph connected algorithms finding connected components 
consider clustering objects classes 
wish divide disjoint subsets flow minimized 
flow measure similarity clusters simplest definition flow cut total weight edges removed disjoint 
cut trying minimize cut favor cutting weakly connected outliers tends lead poor partitioning quality 
avoid problem measure set vertices introduced 
weight vertex importance relative vertices 
vol weight edge shi malik define normalized cut ncut cut vol vol problem formulated shi malik minimize ncut partitions vertex set turns combinatorial optimization problem stated hard proof due shi malik 
allow relaxation indicator functions real values problem reduces minimizing laplacian graph easily computed polynomial time arbitrary precision 
recall lx xi gamma xj wij similar geometrically motivated quantity cheeger constant hg min cut min vol vol complement see chung 
disjoint subsets vol vol 
put xi vol vi gamma vol vi lx xi gamma xj wij vi vj cut dx dii vi dii vi dii vol vol lx dx cut ncut notice column vector ones 
relaxed problem minimize xt lx xt dx condition 
put invertible assuming isolated vertices 
lx dx gamma ld gamma 
matrix gamma ld gamma called normalized graph laplacian 
symmetric positive semidefinite 
notice eigenvector eigenvalue smallest eigenvalue min yt ly yt achieved eigenvector corresponding second smallest eigenvalue course zero multiple eigenvalue happens connected component 
central observation process dimensionality reduction preserves locality yields solution clustering 
worthwhile compare global algorithm tenenbaum local algorithms suggested roweis saul 
approach non linear dimensionality reduction exemplified tenenbaum attempts faithfully approximate geodesic distances manifold 
may viewed global strategy 
contrast local approach roweis saul attempts approximate preserve neighborhood information 
see preceding discussion may interpreted imposing soft clustering data may converted hard clustering variety heuristic techniques 
sense local approach dimensionality reduction imposes natural clustering data 
analysis locally linear embedding algorithm provide brief analysis locally linear embedding lle algorithm proposed roweis saul exhibit connection laplacian 
brief description algorithm 
data set xk high dimensional space goal find low dimensional representation yk 
step 
discovering adjacency information xi find nearest neighbors dataset xi xin 
alternatively xi xin data points contained ffl ball xi 

step 
constructing approximation matrix wij equals orthogonal projection xi affine linear span xij 
words chooses wij minimizing gamma condition wij assume wij determined 
happens example case authors propose heuristic analyze 

step 
computing embedding compute embedding eigenvectors corresponding lowest eigenvalues matrix gamma gamma notice symmetric positive semidefinite matrix 
thought operator acting functions defined data points 
provide argument certain conditions ef ss eigenvectors course coincide eigenvectors develop argument steps 
step fix data point xi 
show gamma ss gamma wij xi gamma xij xi gamma xij function manifold defined data points hessian xi 
simplify analysis neighboring points xi assumed lie locally linear patch manifold xi 
consider coordinate system tangent plane centered xi 
vj xij gamma xi 
difference points regarded vector origin second point see vj vectors tangent plane originating ffj wij 
xi belongs affine span neighbors construction matrix xi ffj smooth function second order taylor approximation written rf hv kvk rf xn gradient hessian hij xi xj evaluated 
gamma gamma vj taylor approximation vj gamma vj ss gamma gamma rf gamma ffj see terms disappear gamma vj ss gamma hv step note ffi vi form orthonormal basis course usually case tr lf generally observe random vector distribution uniform sphere centered xi true example locally uniform measure manifold expectation hv proportional 
en form orthonormal basis corresponding eigenvalues spectral theorem hv eii eii independent put eii expression reduces hv tr step putting steps see gamma gamma ss lle attempts minimize gamma gamma reduces finding eigenfunctions gamma gamma turn interpreted trying find eigenfunctions iterated laplacian eigenfunctions coincide examples briefly consider possible applications algorithmic framework developed 
simple synthetic example swiss roll considered tenenbaum roweis saul 
consider toy example vision vertical horizontal bars visual field 
conclude low dimensional representations constructed naturally occurring data sets domains speech language respectively 
experiments simplest version algorithm wij turns practice 
synthetic swiss roll fig 
show example data lies swiss roll 
note swiss roll really flat dimensional submanifold isomap algorithm attempt isometrically embed swiss roll manages unroll swiss roll preserving geometrical characteristics surface 
worthwhile point isometric embedding preserving global distances attempted isomap theoretically possible surface flat curvature tensor zero 
example known classical result due gauss shows distance preserving map part sphere plane 
toy vision example consider binary images vertical horizontal bars located arbitrary points visual field 
image contains exactly horizontal vertical bar random location image plane 
principal may consider image represented function theta means point theta white means point black 
image vertical bar 
images vertical bars may obtained transformation vt gamma gamma swiss roll dimensional laplacian representation pca representation left right 
purposes illustration compare spectral dimensional representation swiss roll principal component analysis 
pca limited projections produce representation non linear data 
appropriate boundary conditions apply 
space images vertical bars dimensional manifold 
similarly space horizontal bars dimensional manifold 
manifolds embedded space functions theta 
notice manifolds intersect come close 
discretize problem consider theta grid image 
image may represented dimensional binary vector 
choose images containing vertical bars containing horizontal bars random 
fig 
left panel shows horizontal vertical bar give reader sense scale image 
middle panel dimensional representation set images laplacian eigenmaps 
notice local graph connected dimensional representation shows defined components 
right panel shows result principal components analysis principal directions represent data 
linguistic example experiment conducted frequent words brown corpus collection texts containing words distinct available electronic format 
word represented vector left panel shows horizontal vertical bar 
middle panel dimensional representation set images laplacian eigenmaps 
right panel shows result principal components analysis principal directions represent data 
blue dots correspond images vertical bars red signs correspond images horizontal bars 
dimensional space information frequency left right neighbors computed corpus 
precisely words 
representation wi dimensional vector vi say dimensions vi characterize left neighbor relations characterize right neighbor relations 
vi jth component vi number times sequence occurs corpus referred bigram count 
similarly vi count number times sequence occurs corpus 
vectors course claim natural low dimensional manifold structure vectors 
useful practical applications construct low dimensional representations data 
example known lsi latent semantic indexing approach uses principal components analysis represent documents vector space model purposes search information retrieval 
applying laplacian data yields low dimensional representation shown figs 
words respectively 
note words belonging similar syntactic categories cluster highlighting connections clustering dimensionality reduction discussed 
frequent words brown corpus represented spectral domain 
fragments labelled arrows form left right 
exclusively infinitives verbs second contains prepositions third modal auxiliary verbs 
see syntactic structure preserved 
may got told didn going felt began see get know go take say put find look give speech turn example human speech 
long recognized speech signal high dimensional distinctive phonetic dimensions 
important open question field develop low dimensional representation speech signal correlated phonetic content 
example consider low dimensional representations speech data points plotted dimensional laplacian spectral representation 
arise applying laplacian algorithm sentence speech sampled khz 
short time fourier transform millisecond window computed speech signal millisecond intervals 
yielded vector fourier coefficients ms chunk speech signal 
vectors 
vector set fourier coefficients 
furthermore vector labeled identity phonetic segment belonged 
labels utilized laplacian algorithm finds low dimensional representation data 
shown fig 
speech data points plotted dimensional laplacian representation 
spokes correspond predominantly fricatives closures respectively 
central portion corresponds periodic sounds vowels nasals 
natural clustering broad classes obtained fig 
shows different regions representation space 
note phonetic homogeneity data points lie regions 
points mapped region representation space share similar phonetic features points label may originate different occurrences phoneme 
introduced coherent framework dimensionality reduction case data resides low dimensional manifold embedded practice log fourier coefficients taken technical reasons scope current 
sh sh sh sh sh sh sh sh sh sh sh sh sh aa aa ao ao ao ao ao ao ao ao ao ao ao ao ao ao ao ao ao ao dcl kcl blowup selected regions left right 
notice phonetic homogeneity chosen regions 
data points corresponding region similar phonetic identity may arise occurrences phoneme different points utterance 
symbol sh stands fricative word aa ao stand vowels words dark respectively kcl dcl stand closures preceding consonants respectively 
stands silence 
higher dimensional space 
number questions remain answered 

approach utilizes properties laplace beltrami operator construct invariant embedding maps manifold 
maps locality preserving properties general provide isometric embedding 
nash embedding theorem guarantees existence isometric embedding low dimensional space remains unclear embedding easily computable discrete algorithm 
furthermore usually possible isometric embeddings manifold 
example knot isometric embedding circle 
clear discriminate bad isometric embeddings 
interesting formulate precisely properties embedding desirable pattern recognition data representation problems 

consideration geometric invariants manifold may potentially estimated data 
example unclear reliably estimate simple invariant intrinsic dimensionality manifold 

issues pertaining framework need sorted 
implicitly assumed uniform probability distribution manifold data points sampled 
second remains unclear algorithm behaves manifold question boundary 
third appropriate choices ffl effect behavior embeddings need better understood 
fourth convergence finite sample estimates embedding maps need addressed 

notion manifold structure natural data appealing really know particular empirical contexts manifold properties crucial account phenomena hand 
vastly systematic studies specific problems different application domains need conducted shed light question 
acknowledgments grateful john goldsmith motivating consider approach discussed peter bickel insightful critical comments amit babai todd dupont joshua maher scott conversations 
fan chung spectral graph theory regional conference series mathematics number fan chung yan 
yau eigenvalues isoperimetric inequalities riemannian manifolds graphs communications analysis geometry appear hadley efficient eigenvector approach finding netlist partitions 
ieee transactions computer aided design july 
simon haykin neural networks comprehensive foundation prentice hall bruce hendrickson robert leland multidimensional spectral load balancing short version proc 
th siam conf 
parallel proc 
rosenberg laplacian manifold cambridge university press sam roweis lawrence saul nonlinear dimensionality reduction locally linear embedding science vol december smola nonlinear component analysis kernel eigenvalue problem 
neural computation vol 
jianbo shi jitendra malik normalized cuts image segmentation ieee transactions pattern analysis machine intelligence vol august joshua tenenbaum vin de silva john langford global geometric framework nonlinear dimensionality reduction science vol december 
