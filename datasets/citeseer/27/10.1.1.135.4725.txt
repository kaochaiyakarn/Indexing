cache optimization memory resident decision support commercial workloads pedro josep torrellas department computer science university illinois urbana champaign il edu torrellas cs uiuc edu cs uiuc edu dramatic increases main memory size computers allowing applications shift main data storage area disk main memory result increase performance 
trend databases resulting called memory resident databases 
increasing gap processor main memory speed systems effective cache hierarchy crucial high performance 
unfortunately relatively little building cache friendly database systems 
cache oriented optimizations enable effective exploitation caches decision support databases 
main optimization involves developing query optimizer includes cost cache misses cost metrics 
optimizations sophisticated data blocking software prefetching 
optimizations require custom designed hardware support effective complicated tpc queries 
simple database queries run faster cache oriented optimizer blocking total faster addition add prefetching 
effectiveness optimizations stable range cache sizes cache line sizes penalties 
dramatic advances semiconductor technology continue increase integration level dram memory 
semiconductor industry association technology roadmap years memory density increased factor allow design gbit dram chips memory price decreased factor close 
addition time microprocessors bits enable huge address spaces 
trends combined enable applications shift main data storage area disk main memory performance benefits 
class workloads affected trends databases 
database sizes increasing increasing number databases able keep large chunks working set resident memory 
currently database systems built assumption data resides disk 
called database drdb systems 
large main memory improves performance drdb systems study supported part national science foundation nsf young investigator award mip asc mip darpa contract dabt nasa contract nag gifts intel ibm 
pedro supported portuguese government scholarship praxis xxi bd 
cyprus 
garcia molina suggested disk resident assumption inhibits higher performance 
necessary redesign database fully exploit main memory residence 
resulting systems called memory resident database systems 
systems studied researchers successfully implemented commercial system called performance software 
systems proposed assume uniform memory access latency 
current high performance machines include deep cache hierarchy latency access memory system highly variable 
furthermore widely known effective exploitation cache hierarchy key application performance 
shows case current drdb systems machine disk hardware bound 
surprisingly little attention devoted devising cache friendly database algorithms 
knowledge largely limited new sorting algorithm nyberg data blocking optimizations joins aggregates 
contributions focus optimizing single operation database queries typically composed multiple operations highly time interleaved 
cache oriented optimizations enable effective exploitation caches decision support databases 
traditional drdb systems benefit completely bound 
main optimization involves developing new query optimizer includes cost cache misses cost metrics 
optimizations sophisticated data blocking software prefetching 
optimizations require hardware support effective complicated tpc queries 
simple database queries run faster cache oriented optimizer blocking total faster addition add prefetching 
organized follows section introduces background concepts section presents proposed optimizations section describes setup experiments section presents results experiments 
background concepts database basics database management system dbms system controls storage data execution requests 
request dbms called query 
relational system data stored tables 
table composed records tuples tuple includes different attributes certain types 
queries usually written structured query language sql 
dbms execution query involves steps 
query submitted dbms parsed order check correctness 
second query decomposed set operations section 
operations implemented set different algorithms 
third step optimize query execution choosing order execution operations algorithm operation 
output step query plan query tree example 
steps known query compilation 
step query execution traverse query plan execute algorithm nodes 
results execution passed application 
parser optimizer executor compilation execution inner inner scan scan scan scan join join join steps execution query example query plan 
operations algorithms common operations scan select join sort group aggregate 
pipelined operations considered 
means operation generates result tuple flow control passes operation higher level query tree tuple processed immediately 
done reduce time taken query generate result reduce size temporary storage different operations 
scan operation retrieves tuples table 
tuple satisfies scan condition selected discarded 
implementations operation sequential scan index scan 
simply traverses tuples table sequentially selecting satisfy condition 
second uses special data structure tree called index directly access tuples satisfy scan condition 
join operation joins information tables new result table 
result tuple formed tuples input table join attributes satisfy join condition 
common algorithms join nested loop join index nested loop join hash join 
nested loop join example join starts execution requesting tuple lower level operation example join 
algorithm performs sequential scan inner table example table attempting find matching tuple matching value join attribute satisfies scan condition 
match result tuple formed returned operation higher level example join 
process repeated output tuples lower level operation join 
variation algorithm index nested loop join 
case index structure scan inner table tuple matching value join attribute 
hash join algorithm starts building hash table tuples inner table build phase 
algorithm requests tuples operation lower level uses probe hash table find matching tuples probe phase 
sort operation sorts tuples group operation groups tuples common characteristic aggregate operation accumulates tuples 
cache oriented optimizations help decision support queries exploit cache hierarchies design cache oriented optimizations 
optimizations data blocking software prefetching target algorithms operations third cache aware optimizer targets query optimizer 
data blocking data blocking consists reorganizing order execution loop iterations data accessed highly reused tiles blocks 
blocking applied scientific codes improve caches 
drdb systems exploit main memory better reduce number disk accesses 
addition applied blocking cache single operations drdb systems 
example applied blocking single nested loop join single hash join 
unfortunately indicated database queries executed pipelined manner 
blocking operation may speed operation inhibits pipelining delaying generation query result 
extend block pipelined query trees 
algorithm works follows 
operation tree takes input block tuples coming immediately lower level tree 
operation generates output tuples 
aggregate size output tuples larger block control passed operation immediately higher tree process block 
input block processed output added output previous block 
process repeated full output block generated 
point output block passed level 
operations pass blocks tuples 
coarse grain pipelining operations tends delay generation query result 
operating blocks data reuse cached data better suffer fewer cache misses result execute query faster 
data reuse occur ways depending data accessed sequentially indices hash table 
tuples accessed sequentially repeated accesses datum block algorithm 
example nested loop join tuple table checked tuple table 
shows pseudo code blocked nested loop join algorithm 
algorithm input block size si time 
block received operation immediately lower level tree function 
block kept cache reused read element table block output size sj passed level tree 
sizes blocks discussed section 
result blocking significant reduction 
suppose joining large tables la lb block sizeof block si tuple scan tuple block match tuple tuple tuple project tuple tuple add tuple block sizeof block sj block blocked nested loop join algorithm 
sa number cache lines input block respectively number cache misses decreases lb lambda la la lb lambda la sa la data accessed indices tuples reused 
reuse parts index structure 
specifically tree index organization reuse nodes close root tree 
occurs index scan index nested loop join 
consequently blocking keep reused data cache 
blocking algorithms accomplished simply invoking operation times row different tuples 
index nodes remain cache invocations 
example blocked index nested loop join shown 
blocked nested loop join algorithm starts getting block input tuples lower level tree 
tuples reused 
tuple index structure accessed find matching tuple table time select new tuple block access index structure possibly reusing nodes 
size input block determines times indices reused 
algorithm passes control level tree output block generated 
block tuple block tuple index scan tuple match tuple tuple tuple project tuple tuple add tuple block sizeof block sj block blocked index nested loop join algorithm 
blocked hash join block tuples received lower level query tree probed hashed form table algorithm similar blocked index nested loop join index structure hash table structure 
hash table dimensioned minimize hash conflicts data reuse hash structure minimal 
software prefetching software prefetching data requested needed 
goal hide latency access slower levels storage hierarchy 
prefetching successful time data needed data faster levels storage hierarchy 
researchers examined software prefetching techniques algorithms prefetch data caches 
algorithms usually cache line grain size usually applied scientific engineering applications example 
dbms systems prefetching typically bring data disk memory 
prefetching performed hide latency disk accesses typically done disk block grain size 
approach improve caches dbms systems software prefetching prefetch cache lines memory cache 
section consider types access patterns sequential indices hash table 
sequential accesses tuples amenable prefetching accesses happen rapid time succession 
occurs blocked algorithms section read block tuples coming lower level tree 
occurs blocked nested loop join accesses tuples table right table 
accesses occur sequential scan executed left table lowest join tree 
prefetching tuples accessed sequentially easy access predictable 
simply insert prefetch statement loop iteration prefetches tuple iteration 
indicated ratio latency memory access latency free execution iteration loop 
appropriate added loop 
algorithms blocked prefetching effective operation returns control level query tree soon output tuple 
point flight prefetches wasted 
consequently modify sequential accesses non blocked algorithms output tuple algorithm continues accessing checking set tuples iterations 
done prefetches iterations 
iterations new prefetches issued 
core code prefetching code shown 
algorithm nested loop join accesses tuples table right table sequential scan executed left table lowest join tree 
note algorithm needs output buffer may generate output tuple calling operation higher level tree 
size prefetch matching match 
tuple project add tuple block block buffer matching tuple project add tuple block return block 
min max index size value attribute tuple prefetch index max min value min max mid max min prefetch index mid min value prefetch index max mid value value index mid value max mid value index mid value min mid tuple project index mid tuple return tuple examples software prefetching algorithms prefetching non blocked sequential accesses prefetching blocked non blocked index accesses 
tuples accessed indices prefetching harder 
consider example tree index structure 
prefetch index nodes need 
unfortunately tuple search unknown path index structure follow 
prefetch possible paths 
environment important compute addresses index nodes low overhead 
way organization proposed whang krishnamurthy addresses shift operations 
shows pseudo code algorithm prefetches indices iteration advance index structure organized whang krishnamurthy 
prefetching follows possible paths 
algorithm right table table index nested loop join 
index scan left table lowest join tree 
algorithm applied non blocked blocked algorithms 
blocked algorithms algorithm simply called times row operation 
prefetching hash join similar prefetching index nested loop join hash table structure similar index structure 
unfortunately hard prefetch efficient conflict free hash table early 
consequently prefetching hash join blocked hash join 
cache aware optimizer third optimization proposed improve selection algorithms operations query tree order operations tree 
propose cache aware query optimizer 
typical cost driven optimizer cost model combination algorithm operation supported system 
cost model analytical function set inputs size data tables returns value represents cost algorithm operation combination certain metric 
metric usually predicted execution time 
query tree certain ordering operations certain algorithm operation optimizer adds costs produce cost query tree 
result classify query trees estimated cost 
drdb system cost model includes factors number operations io amount cpu activity cpu 
large access latency number operations dominant factor 
examples optimizers cost models system postgres 
baseline study 
system cost models include number operations io amount cpu activity cpu 
cost due cpu activity clearly dominant factor cost 
number operations largely neglected accounts just initial loading data memory independent algorithm 
system optimizer characteristics 
modern architectures cache memories cost models described systems 
models ignore memory access costs 
known modern architectures stall time due memory accesses may account large fraction execution time 
furthermore memory accesses cost 
cache hits smaller cost cache misses 
propose cache aware cost model differs plain cost model includes extra factor cost memory accesses due cache misses 
cost model cpu component io component 
components computed follows 
compute component algorithm analyzing memory access patterns predicting number misses 
actual computation component different cases section non blocked algorithms section blocked algorithms 
cpu component computed profiling execution algorithm determine number instructions executed 
give different weight factors final cost model form lambda lambda cpu formula set average latency cache set average cpi processor perfect memory system 
parameters set architecture specifications 
argued conceptually little difference cache aware query optimizer uses cpu cost components traditional drdb query optimizer uses cpu io cost components 
cache aware query optimizer tries keep subset memory data cache drdb query optimizer tries keep subset disk data memory 
significant differences systems 
major differences ratio disk memory access time higher ratio memory cache access time granularity data retrieved disk disk block coarser data retrieved memory memory line cache space managed hardware memory space managed software 
difference implies due smaller latency ratio cache oriented system may tolerate cache misses certain algorithms 
second difference implies disk resident system obtain performance spatial locality exploited necessary system caches 
third difference implies cache oriented system full software control resides cache time need careful cache space cache oriented system 
differences imply cache aware query optimizer typical drdb query optimizer choose different optimal trees query 
sections give high level overview estimate number cache read misses algorithm 
detailed discussion 
analysis assume table completely fit cache 
addition show number misses involved completing query 
get fraction query results non blocked algorithms suffer average fraction total misses blocked algorithms fraction total misses average bit higher fraction results finest grain size algorithms block tuples 
models non blocked algorithms rows table give high level notion estimated read misses non blocking algorithm 
analysis easier fact algorithms hardly exploit cached data reuse significant time periods 
sequential scan misses decomposed suffered reading selection attribute scanned tuple read input suffered forming output tuple time detect match copy output 
consist reading tuple attribute join attribute join query tree reading pointer tuple 
pieces information passed matching tu algorithm number read misses sequential scan read input copy output index scan read index read input element copy output nested loop join read outer input outer size theta sequential scan copy output index nested loop join read outer input outer size theta index scan copy output hash join build hash read outer input outer size theta probe hash copy output blocked index scan read index lambda read input element copy output cross self blocked nested loop join read outer input theta sequential scan copy output cross self blocked index nested loop join read outer input outer size theta blocked index scan copy output table modeling number read misses pipelined non blocked blocked algorithms 
sequential scan hash join algorithms data reuse save misses respective non blocked algorithms 
ple level query tree 
details 
index scan misses decomposed recorded traversing index read index reading selection attribute tuples leaf read input element forming output tuples copy output 
nested loop join receive total outer size tuples left input join query tree 
reading tuples time involves read outer input misses 
tuples perform sequential scan inner table join recording grand total outer size theta sequential scan misses 
sequential scan term copy output misses correspond selecting tuples match selection condition 
tuples may additionally match join condition 
generate output tuple 
operation involves reading pointer tuples joined reading tuple attribute join attribute join 
combined misses step extra copy output 
misses index nested loop join nested loop join incorporate index scan misses sequential scan ones 
misses hash join index nested loop join recording index scan misses time access inner table join record probe hash misses time probe element hash table inner table join 
addition record build hash misses build hash table place 
models blocked algorithms estimate number misses blocked algorithms model similar lam 
blocked operation types misses necessary execution operation basic induced reused block interference data cross induced reused block self interference self 
basic misses include start misses plus misses reused non block data displaced cache due large size tables indices 
estimated misses major blocked algorithms shown rows table 
consider blocked nested loop join 
basic misses include components 
reading tuples coming left input join operation read outer input 
tuples read blocks 
read cause lower left operation query tree flushed cache 
second blocks read record sequential scan misses scanning inner input table trying find matches 
basic misses include ones recorded generating output block tuples copy output 
cross misses induced blocks accesses inner input table join output tuples 
accesses may displace data reused block 
know exact mapping data structures estimate accesses cause cross assume accesses touch location cache uniform probability 
knowing fraction cache utilized reused block estimate misses accesses cause 
self misses zero reused block smaller cache 
complete formula number misses blocked nested loop join ends function size reused input block 
consequently choose block size minimizes number misses 
misses scan blocked index scan somewhat similar ones index scan table contain read index read input element copy output components 
call operation operate block index structure reused cache section 
result fewer read index misses represent read index lambda 
suffer cross self misses reused index structure compute similar way blocked nested loop join 
blocked index nested loop join records read outer input misses trying read outer size tuples left input join query tree 
tuples operated blocks 
tuple record blocked index scan misses trying index scan right input table 
misses assume reuse index tree 
blocked index scan misses cross misses recomputed cross interference different 
record copy output misses generate output tuples join 
experimental setup section describe setup experiments database system built query plans application evaluated platforms 
developed includes subset subsystems real database system 
essential subsystems experiments data storage query optimizer query executor 
real database system include subsystems concurrency control recovery security au 
due characteristics decision support workload studied subsystems largely unnecessary 
decision support queries need performed live data 
example queries executed day database updated short period night 
queries read need locking allow concurrent access 
furthermore keeping log operations unnecessary need recovery data modified 
optimized decision support query engine able correctly evaluate queries overheads shelf general purpose dbms 
overheads consequence asserting acid properties atomicity consistency isolation durability query absolutely necessary case 
shows block diagram 
results stats query system 
input data storage module set database data tables output collection data structures allocated heap loaded database data 
input query optimizer sql query statistics information data system 
output module query plan input query executor 
output executor result table 
query plans application optimizer uses different cost models algorithms generate query potentially different query plans pg cpu cm 
pg plan generated traditional drdb system 
optimizer uses cost models postgres database 
note optimizer assumes system disk resident define memory buffer larger total database data size 
result optimizer need perform inefficient data structure transformation put system performance disadvantage relative plans 
algorithms blocked caches allowed 
plan obtained pg plan exchanging original non blocked algorithms algorithms blocked caches 
obviously possible algorithms blocked version 
optimization better proposed indicated section blocking works pipelined environment 
cpu plan generated traditional system 
uses cost function includes cpu component 
optimizer uses non blocked algorithms aware existence caches 
plans representative proposed systems 
plan generated naive cache aware optimizer uses component cost function 
assumes misses costly operations neglects cost cpu computation 
cm plan generated cache aware optimizer uses cpu components cost function 
optimizer equal equal 
somewhat arbitrary choice means assuming observed latency cache times higher average cpi perfect memory system 
cm plans algorithms blocked caches lower cost 
table summarizes cost functions algorithms different plans 
plans software prefetching 
initially apply prefetching 
apply prefetching section 
plan system cost function algorithms pg disk resident postgres non blk disk resident postgres plus blk non blk extensions cpu mem resident cpu non blk naive cache blk non blk cm advanced theta cpu blk non blk cache theta table different approaches generate query plans 
database application experiments composed queries tpc benchmark 
queries selected complex includes joins 
complex queries larger potential improvement expressed different query trees offer choices optimizer 
time experiments conducted tpc decision support benchmark unavailable 
expect tpc queries complex better showcase techniques 
generate database data standard data generation tool provided tpc organization 
data sizes smaller standard gbyte limited resources 
experiments machine configured mbytes main memory 
result focus main memory database systems set database data set size mbytes 
architectural platforms uniprocessor architectural platforms evaluate proposed optimizations 
section processor sgi origin configured kbyte primary data instruction caches mbyte unified secondary cache mbyte main memory 
processor machine includes hardware performance counters measure execution cycles cache misses 
second platform architecture simulated software somewhat similar origin easy modify model different configurations 
execution driven simulation environment 
architecture modeled aggressive dynamic superscalar processor fetch retire mips instructions cycle 
processor instruction window entries additional registers renaming perform multiple branch predictions pending unresolved branches 
instructions take cycle complete multiply divide take cycles respectively 
perfect instruction cache assumed 
levels data cache kbyte way set associative primary cache mbyte direct mapped secondary cache 
caches non blocking support outstanding loads outstanding stores enable full load bypassing 
line size caches bytes 
contention gx pu pg gx pu pg gx pu pg gx pu pg cpu mal ized exe ime pu cm pu cm pu cm pu cm cpu cm mal ized exe ime execution time different query plans origin mbyte database data set existing schemes new schemes 
data query basis normalized cpu 
round trip access latencies processor caches memory cycle primary cache cycles secondary cache cycles main memory 
release memory consistency 
memory hierarchy parameters baseline architecture changed different experiments 
experimental results real architecture start evaluation running queries origin 
query ran potentially different plans generated optimizer 
different orders magnitude results charts chart corresponds existing approaches pg cpu chart corresponds cpu new approaches cm 
bars measure total time taken query plan execute normalized time taken cpu plan 
data query basis 
table shows absolute execution time cpu plan rate primary data cache 
query execution time seconds data rate table execution time rate kbyte primary data cache cpu 
shows queries postgres plan pg enhanced blocked algorithms performs worse cpu plan 
different behavior due fact simple query join operations 
number different plans small consequently plans differ 
queries close analysis postgres code shows reason poor performance plans hard coded condition 
condition asserts hash join algorithm inner input table smaller outer input table 
drdb condition tries avoid costly data spills disk ends disabling plans 
garcia molina salem report problem drdb systems 
conclude plans generated memory resident system deliver better performance 
shows memory resident plans plan worst 
clearly optimizing minimum number cache misses lead best performance 
involves algorithms cause misses add extra instructions 
example reduction cache misses cpu increase execution time 
effect occurs blocked nested loop joins hash joins 
case blocked nested loop joins reduce number misses cost instructions 
queries increase execution time larger 
best performance achieved considering cpu cost cm plan 
relative cpu cm runs faster faster faster 
rest queries improvements smaller 
general execution time reductions may large compared relatively small rates recorded primary data cache cpu plan shown table 
wide issue superscalar processor small rate reduction large impact execution time 
addition due regular access patterns accesses missing primary cache secondary cache costly 
possible cm cost function happens trigger choice set algorithms lower real computation cost ones chosen cpu 
conclude particular database complicated queries tested cache aware optimizer hat propose produces query plans cm faster traditional memory resident ones cpu 
queries runs faster complicated queries tpcd run average faster 
magnitude improvement requires simple software changes 
simulated architectures simulations evaluate optimizations broad set architectural configurations 
unfortunately query long execution time simulate 
execution time approximately times longer queries 
consequently simulate 
purpose baseline simulated architecture replicate real architectures pu pu pu pu pu pu pu pu pu cpu cm 
cpu cm 
cpu cm 
mal ized exe ime busy memory pu pu pu pu pu pu pu pu pu cpu cm 
cpu cm 
cpu cm 
mal ized exe ime busy memory pu pu pu pu pu pu cpu cm 
cpu cm 
mal ized exe ime busy memory pu cm pu cm cpu cm pu cm cpu cm pu cm cpu cm cpu cmp mal ized exe ime busy memory normalized execution time different cache sizes cache line sizes cache penalties 
chart shows impact software prefetching optimization execution time 
similar compare speed run queries 
architectures exactly comparing absolute values compare faster cm plan cpu plan architecture 
table shows results 
query shows sign inversion feel simulator generates qualitatively similar improvements real hardware configured imitate system 
query real simulated table speed improvement cm cpu queries real simulated architecture 
cache size parameter vary size primary data cache 
study kbyte caches 
shows relative execution times cpu cm plans 
query bars corresponding combinations cache size query plan 
execution time divided instruction execution busy memory stall time memory slots wasted reasons including structural hazards data dependencies mispredicted branches 
query bars normalized cpu kbyte cache 
shows little variation change cache size kbytes 
implies data reuse captured kbyte caches largely captured kbyte caches 
result configurations improvement cm cpu largely constant cache sizes 
cache line size second parameter vary cache line size 
examine word lines data caches 
shows execution time different cache line sizes query plans 
organized 
query bars normalized cpu word lines 
shows plans increase line size execution time queries decreases 
implies algorithms query plans exploit spatial locality 
relative improvement cm cpu changes little cache line changes examined 
change occurs improvement cm cpu slowly decreases longer cache lines 
may due large speedups delivered cm plan place hard deliver speedups long lines 
cache penalty change cache penalty 
compare baseline configuration double cache penalty 
double cm cost model 
execution time results organized previous figures 
query bars normalized cpu baseline latency 
shows execution time increases significantly configuration relative improvement cm cpu stays remarkably constant 
cache oriented optimizations stable different architectural configurations 
results sections suggest queries proposed cache aware plan cm provides improvement state theart memory resident plan cpu fairly robust changes cache size cache line size cache penalty 
software prefetching apply software prefetching optimization discussed section cpu cm plans baseline architecture 
results depicted usual manner 
query plan shows execution time prefetching cpu cm prefetching cpu cm 
query bars normalized cpu prefetching 
due space limitations summarized discussion results 
shows queries examined run significantly faster software prefetching 
prefetching reduces memory time minimal increases busy time 
relative non prefetching cpu plan queries run faster non prefetching cm plan faster prefetching cm plan 
prefetching speeds cpu 
relative improvement cm cpu stays relatively unchanged addition prefetching configurations examined 
dramatic increases main memory size computers allowing databases shift main data storage area disk main memory 
resulting systems called memory resident database systems 
performance higher systems waste lot time cache hierarchy 
evaluated cache oriented optimizations enable effective exploitation caches memory resident decision support databases 
traditional disk resident databases benefit extent 
main optimization involved developing query optimizer included cost cache misses cost metrics 
cost cache misses instruction execution optimizer chooses ordering algorithm implementation query operations 
optimizations sophisticated data blocking software prefetching 
optimizations required hardware support effective complicated tpc queries 
simple database complicated queries ran faster cache aware optimizer blocking total faster addition added prefetching 
effectiveness optimizations stable range cache sizes cache line sizes penalties 
expect optimizations effective tpc queries higher complexity 
acknowledgments sharad mehrotra long discussions daniel reed suggestions referees graduate students group feedback 
gharachorloo bugnion 
memory system characterization commercial workloads 
proceedings th annual international symposium computer architecture june 
elmasri navathe 
fundamentals database systems 
benjamin cummings publishing second edition 
garcia molina salem 
main memory database systems overview 
ieee transactions knowledge data engineering pages december 
hennessy patterson 
computer architecture quantitative approach 
morgan kaufmann publishers san francisco ca 
jagadish rastogi silberschatz 
dal high performance main memory storage manager 
proceedings th vldb conference pages september 
krishnan torrellas 
execution driven framework fast accurate simulation superscalar processors 
international conference parallel architectures compilation techniques pact october 
lam rothberg wolf 
cache performance optimizations blocked algorithms 
proceedings th international conference architectural support programming languages operating systems april 
mowry lam gupta 
design evaluation compiler algorithm prefetching 
proceedings fifth international conference architectural support programming languages operating systems pages october 
nyberg barclay gray lomet 
risc machine sort 
proceedings acm sigmod international conference management data pages may 

hsu 
data prefetching hp pa 
proceedings th annual international symposium computer architecture pages june 
selinger astrahan chamberlin lorie price 
access path selection relational dbms 
proceedings acm sigmod international conference management data june 
semiconductor industry association 
national technology roadmap semiconductors 
notes org nsf 
kant naughton 
cache conscious algorithms relational query processing 
proceedings th vldb conference pages september 
performance software 
main memory data management august 
www com customers wp html 
stonebraker 
postgres generation database management system 
communications acm october 

optimizing memory resident decision support system workloads cache memories 
phd thesis university illinois urbana champaign july 

pey zhang torrellas 
memory performance dss commercial workloads shared memory multiprocessors 
proceedings third international symposium high performance computer architecture hpca pages february 
transaction processing performance council 
tpc benchmark decision support standard specification revision june 
transaction processing performance council 
tpc benchmark decision support standard specification revision february 
veenstra fowler 
mint front efficient simulation shared memory multiprocessors 
proceedings nd international workshop modeling analysis simulation computer telecommunication systems mascots pages january 

whang krishnamurthy 
query optimization memory resident domain relational calculus database 
acm transactions database systems pages march 
zagha larson turner 
performance analysis mips performance counters 
proceedings supercomputing november 
