prioritized sweeping reinforcement learning data real time andrew moore christopher atkeson mit arti cial intelligence laboratory technology square cambridge ma new algorithm prioritized sweeping cient prediction control tic markov systems 
incremental learning methods di erencing learning fast real time performance 
classical methods slower accurate full observations 
prioritized sweeping aims best worlds 
uses previous experiences prioritize important dynamic programming sweeps guide exploration state space 
compare prioritized sweeping reinforcement learning schemes number di erent stochastic optimal control prob lems 
successfully solves large state space real time problems methods di culty 
introduces memory technique prioritized sweeping markov prediction reinforcement learning 
current model free learning algorithms perform relative real time 
classical methods matrix inversion dynamic programming perform relative number observations 
prioritized sweeping seeks achieve best worlds 
closest relation conventional ai search scheduling technique algorithm nilsson 
memory method stan ll waltz derives power explicitly remembering real world experiences 
closely related research performed peng williams similar algorithm prioritized sweeping call dyna queue 
review problems techniques markov prediction control 
thorough reviews may sutton 
sutton kaelbling barto 

discrete nite markov system states 
time passes series discrete clock ticks tick state may change 
probability possible successor states function current system state 
entire system speci ed table transition probabilities 
qs qs qss qij denotes probability state state time step 
table satisfy qij shows example states corresponding cells 
exception rightmost states time step system moves random neighbor 
example state moves directly state probability state space markov system partitioned subsets non terminal states terminal states terms 
terminal state entered left 
absorb absorb terms 
example rightmost states terminal 
state markov system 
markov system de ned absorbing non terminal state possible eventually enter terminal state 
restrict attention absorbing markov systems 
rst consider questions starting state probability eventual absorption terminal state 
write value ik 
absorption probabilities terminal state computed solving set linear equations 
assume non terminal states indexed snt snt number non terminals 

snt 
snt 
transition probabilities known easy matter compute eventual absorption probabilities 
machine learning applied case transition probabilities known advance may watch series state transitions 
series normally arranged set trials trial starts state continues system enters terminal state 
example learner 
shown learning approaches problem widely studied 
contribution great relevance elegant algorithm called temporal di erencing sutton 
temporal di erencing algorithm reviewed describe discrete state space case temporal di erencing algorithm 
td applied systems continuous state spaces long term probabilities represented parametric function approximators tesauro 
prediction process runs series epochs 
epoch ends terminal state entered 
assume passed states far current epoch 
age epoch andt global age 
observed transition 
ik estimated value ik system running state transition observations 
td algorithm discrete state spaces updates estimates rule set non terminal states terms set terminal states ik ik ink nx xi ij learning rate parameter memory constant xi ij ij practice computational trick requires considerably computation algorithm equation computes values sutton 
td algorithm requires st computation steps real observation st number termi nal states 
convergence proofs exist formulations td algorithm sutton dayan 
classical approach classical method proceeds building maximum likelihood model state transitions 
qij estimated qij number observations number occasions state observations new absorption probability estimates computed satisfy terminal state snt linear system ik succs qij jk succs set states observed immediate successors set non terminal states 
clear estimates correct solution equation solution equation 
notice values ik depend values observations de ned terms previous absorption probability estimates ik 
cient solve equation iteratively 
set intermediate iteration variables containing intermediate estimates ik 
initial estimates start iteration 
excellent answer previous absorption probability estimates ik 
complete algorithm performed real world observation shown 
transformation ik shown contraction mapping de ned section bertsekas tsitsiklis proves convergence solution satisfying equation guaranteed 
estimated transitions states reach terminal state solution unique 
inner loop terms referred probability backup operation requires st succs basic operations succs mean number observed stochastic successors 

terms 
repeat ik ik max max terms new qij jk succs new ik ik new max max max 
terms ik ik stochastic prediction full gauss seidel iteration 
gauss seidel expensive algorithm requiring snt backups real world observation inner loop 
absorption predictions observation ik mally provide excellent initial approximation iterations required 
interesting observation encountered example previously experienced transition terminal state iterations snt needed convergence 
prioritized sweeping prioritized sweeping designed perform task gauss seidel iteration careful bookkeeping concentrate computational ort interesting parts system 
operates similar computational regime dyna architecture sutton xed non trivial amount computation allowed real world observa tion 
peng williams exploring closely related approach prioritized sweeping developed dyna learning watkins 
prioritized sweeping uses value probability update step previous algorithm determine updates interesting step produces large change state absorption probabilities interesting absorption probabilities predecessors state change opportunity 
hand step produces small change assume urgency process predecessors 
predecessors state states history system performed step transition just changed absorption probabilities maximum possible processing step change predecessor caused change value priority predecessor currently priority queue placed priority queue lower priority promoted 
real world observation transition probability estimate qij updated probabilities transition previously observed successors state promoted top priority queue absorption probabilities updated immediately 
continue process states top queue 
state processed may result addition promotion predecessors queue 
loop continues preset number processing steps queue empties 
real world observation interesting predecessors earlier ancestors quickly nd near top priority queue 
hand real world observation unsurprising processing immediately proceeds important areas state space consideration previous time step 
areas may erent system currently nds 
look formal algorithm 
entry assume state transition 
drop su ik notation 
decision allowed processing start step im plemented ways 
subsequent experiments rule simply maximum backups permitted real world observation 
possible priority queue implementations including heap knuth experiments 
cost algorithm st succs snt basic operations states processed priority queue cost accessing priority queue length heap implementation log states added queue priorities threshold value close machine oating point precision 
stopping criteria fraught danger discuss dangers note experiments caused problem 
prioritized sweeping heuristic formal proof convergence conver gence rate 
expect able prove convergence techniques asynchronous dynamic programming bertsekas tsitsiklis variants temporal di erencing analysis dayan 
gives empirical experiments convergence relatively fast 

promote state top priority queue 

allowed processing priority queue empty remove top state priority queue 
call max terms new succs new ik ik new max max max preds max qij jk tiny threshold queue exceeds current priority promote new priority prioritized sweeping algorithm 
memory requirements learning transition probability matrix number states may initially appear prohibitive especially intend operate states 
need allocate memory experiences system wide class physical systems time lifetime system run memory 
similarly average number successors predecessors states estimated transition matrix assumed simple justi cation real problems fully connected deeper reason large true transition probability matrix sparse time gain experience estimated transition matrix sparse 
markov prediction experiment consider state markov system depicted complex version problem 
appendix gives details problem randomly generated 
system sixteen terminal states depicted white black circles 
prediction problem estimate non terminal state long term probability terminate black white circle 
data available learner sequence observed state transitions 
temporal di erencing classical method prioritized sweeping applied problem 
learner shown sequence 
td parameters gave best performance number manually optimized experiments 
classical method required compute date absorption probability estimates real world observation 
prioritized sweeping allowed backups real experience updated ik estimates highest priority states real world observation 
threshold ignoring tiny changes evaluated number stages learning stopping real time clock computing error estimated white absorption probabilities white true values denote white 
rms error states recorded snt snt state markov system 
erage stochastic successors 
look rms error plotted number observations 
experiences methods performing td weakest manages rms error 
look di erent measure performance plotted real time 
see great weakness classical technique 
performing gauss seidel algorithm observation gives excellent predictions time consuming seconds time process observations 
amount td time process half observations 
prioritized sweeping performs best relative real time 
takes approximately times long td process observation data ectively convergence superior 
experiments di erent random state problem run 
rms prediction error rms prediction error td classical pri 
sweep 
observations log scale td classical pri 
sweep real time seconds log scale rms prediction error true absorption probabilities pre values plotted number data points observed 
prioritized sweeping rms prediction error true absorption probabilities pre values plotted real time seconds running problem sun workstation 
td classical pri 
sweep observations seconds table rms prediction error mean standard deviation experiments 
runs nal results table indicate graphs figures atypical 
example shown general theme 
model free methods perform real time weak data 
classical methods data impractically slow 
techniques prioritized sweeping interesting may able achieve 
important footnote concerning classical method 
problem required prediction transitions observed real time cost recording transitions memory 
absorption probabilities computed individual large computation sequence giving best possible estimate relatively small time cost 
state problem estimate cost approximately seconds points 
prioritized sweeping bene required predict seeing data little advantage simpler classical algorithm 
prioritized sweeping usefully applicable class tasks prediction required time step 
furthermore remainder concerns control markov decision tasks maintenance date predictions particularly bene cial 
learning control markov decision tasks consider related stochastic prediction problem bridges gap markov prediction control 
suppose system gets rewarded entering certain states punished entering 
state ri 
important quantity expected discounted reward go state 
nite sum expected rewards term supplemented exponentially decreasing weighting factor called discount factor 
expected discounted reward go ji reward expected reward time step expected reward time steps expected reward time steps ji computed recursively function immediate successors 

sjs 
sjs js rs qs qs 
set linear equations may transition probabilities qij known 
known sequence state transitions ri observations slight modi cations td classical algorithm prioritized sweeping estimate ji 
markov decision tasks markov decision tasks extension markov model passively watching state move randomly able uence 
associated state nite discrete set actions actions 
time step controller choose action 
probabilities potential states depend current state chosen action 
supplement example problem actions actions actions actions actions actions actions 
random causes random transitions right moves probability cell immediately right stay remain state 
escape states 
notation ij probability move state com state applied action example random policy mapping states actions 
example shows policy right right stay random random stay controller chooses actions xed policy behaves markov system 
expected discounted reward go de ned computed manner equation 
right rand right rand stay absorb absorb stay 
policy de ned equation 
shown re ward function bottom left cell 
large expected reward go getting avoiding 
goal large reward go policies better 
important result theory markov decision tasks tells exists policy optimal sense 
state expected discounted reward go optimal policy worse policy 
furthermore simple algorithm computing optimal policy expected discounted reward go policy 
algorithm called dynamic programming bellman 
relationship known bellman optimality equation holds tween optimal expected discounted reward go di erent states 
ji max actions ri js dynamic programming applied example gives policy shown happens unique optimal policy 
avery important question machine learning obtain optimal near optimal policy ij transitions rewards observed 
example values known advance 
series actions state random 
random 
right 
random 
right 
random 
random 
critical di erence problem markov prediction problem earlier sections controller ects transitions seen supplies actions 
question learning systems studied eld reinforcement learning known learning control markov decision tasks 
early contributions eld checkers player samuel boxes system michie chambers 
systems may rst appear trivially small armed bandit problem berry promoted rich interesting statistics community 
technique gradient descent optimization neural networks combination ap policy reward go called adaptive heuristic critic introduced sutton 
kaelbling introduced applicable techniques including estimation algorithm 
watkins introduced important model free asynchronous dynamic programming technique called learning 
sutton extended dyna architecture 
christiansen 
applied planner closely related dynamic programming tray tilting robot 
excellent review entire eld may barto 
prioritized sweeping learning control markov decision tasks main di erences case previous application prioritized sweeping 
need estimate optimal discounted reward go ofeach state eventual absorption probabilities 

absorption probability backup equation bellman equa tion bellman bertsekas tsitsiklis ji max actions succs ij jj ji estimate optimal discounted reward starting state discount factor actions set possible actions state ij maximum likelihood estimated probability moving state state applied action estimated immediate reward computed mean reward experienced date previous applications action state 
rate learning ected considerably controller exploration strategy 
algorithm prioritized sweeping conjunction bellman equation fig ure 
substantial di erence algorithm prediction case state backup step bellman equation application step 
notice prede cessors state set state action pairs 
consider question best gain useful experience markov decision task 
formally correct method compute exploration maximizes expected reward received robot remaining life 
computation requires prior probability distribution space markov decision tasks unrealistically expensive 
computationally exponential number time steps system remain alive ii number states system iii number actions avail able berry 
exploration heuristic required 
kaelbling barto 
give excellent overviews wide range heuristics proposed 

promote state top priority queue 

allowed processing priority queue empty remove top state priority queue 
call new max actions max new ji ji new preds max succs ij jj tiny threshold queue exceeds current priority promote new priority prioritized sweeping algorithm markov decision tasks 
philosophy optimism face uncertainty method successfully developed interval estimation algorithm kaelbling exploration bonus technique dyna sutton 
philosophy thrun moller 
slightly di erent heuristic prioritized sweeping algorithm 
minor problems computational expense instability exploration bonus large state spaces 
slightly di erent optimistic heuristic follows 
absence contrary evidence action state assumed lead directly absorbing state permanent large reward opt amount evidence contrary needed quench optimism system parameter 
number occurrences state action pair assume jump state subsequent long term reward opt opt opt opt 
number occurrences true non optimistic assumption 
optimistic reward go estimate opt opt max actions opt succs opt ij number times action tried date state important feature identi ed sutton isthe planning explore behavior caused appear ance optimism sides equation 
related exploration technique christiansen 
consider situation 
top left hand corner state space looks attractive ifwe optimistic heuristic 
areas near frontiers little experience high opt turn areas near nearly high opt prioritized sweeping asynchronous dynamic programming method job start encouraged go north unknown east best reward discovered date 
system parameter opt require ne tuning 
set gross overestimate largest possible reward system simply continue exploration sampled state action combinations times 
section discusses search guiding heuristic similar heuristic heart search 
little experience start experience reward zone state space simple path planning prob parameter de nes try state action combination cease optimism certainly require human programmer 
small overlook low probability highly rewarding stochastic successor 
high system waste time needlessly resampling reliable statistics 
exploration procedure full autonomy 
arguably necessary weakness non random exploration heuristic 
dyna exploration bonus contains similar parameter relative size exploration bonus expected reward interval estimation parameter implicit optimistic con dence level 
selection appropriate hard formalize 
take account expected lifetime system measure importance stuck learning available prior knowledge stochasticity system known constraints reward function 
automatic procedure computing require formal de nition human programmer requirements prior distribution possible worlds 
lem 
experimental results section begins comparative results familiar domain stochastic dimen sional maze worlds 
examines parameter speci es amount computation number bellman equation backups allowed real world observation pa rameter de nes exploration performed 
number larger examples investigate performance range di erent discrete stochastic reinforcement tasks 
maze problems state actions direction 
blocked actions move 
goal state star subsequent gures gives units reward give discount factor 
trials start bottom left corner 
system reset start state goal state visited times reset 
reset outside learning task observed state transition 
dyna prioritized sweeping allowed bellman equation backups observa tion 
tested 
dyna pi original dyna pi sutton supplemented exploration bonus 

dyna opt original dyna pi supplemented optimistic heuristic prioritized sweeping 
table shows number observations convergence 
trial de ned converged time subsequent sequence decisions contained suboptimal decisions 
test optimality performed comparison control law obtained full dynamic programming true simulation 
results deterministic problems rst rows table 
rst dyna pi converged problems state problem 
smaller exploration bonus helped problem converge albeit slowly 
maze state state state state state det dyna pi det dyna opt det stc untested untested untested stc opt untested untested untested untested stc dyna pi stc dyna opt stc table 
number observations decisions subsequently optimal 
values rounded 
prioritized sweeping dyna applicable opt 
tabulated experiments run multiple runs optimistic dyna prioritized sweeping revealed little variance convergence rate 
see figures 
rows optimistic heuristic opt 
ther opt value overestimated best possible reward see converge accurate estimation true best possible reward 
tried optimism lost 
safe strategy deterministic environment 
learning controller clues implicit parameters opt 
ensure convergence optimum sample state action pair 
prioritized sweeping required fewer steps optimistic dyna mazes small 
learners runs took seconds observations running sun workstation 
interestingly prioritized sweeping usually took half real time dyna 
exploration surprises need full allocation bellman equation processing steps 
ect pronounced processing steps observation allowed 
example state problem optimistic dyna required observations took hours 
prioritized sweeping required observations took fteen minutes 
lower part table shows results stochastic problems mazes 
action chance corrupted random value applied 
north applied outcome movement north time direction time 
prioritized sweeping optimistic dyna value 
sampled state action combination times losing optimism 
value chosen reasonable balance exploration exploitation authors knowledge stochasticity system happily proved satisfactory 
discussed section choice automated experiments 
stochastic results include interesting incremental technique called learning watkins manages learn constructing state transition model 
addition ally tried learning optimistic heuristic prioritized sweeping 
initial values set high encourage better initial exploration random walk 
ort put tuning application 
performance worse 
particu lar optimistic heuristic disaster learning easily gets trapped learning pays attention current state system planning explore behavior requires attention paid areas state space system currently 
stochastic maze results di erence optimistic dyna prioritized sweeping pronounced 
large number predecessors quickly dilute wave interesting changes propagated back priority queue leading queue similar priorities 
prioritized sweeping required half total real time version dyna convergence 
small fully connected example results state bench mark problem described sato 
barto singh 
transition matrix results shown table 
parameter 
fact converged times average steps considered safe safety margin 
learners heavily tweaked nd best performance 
eq algorithm barto singh designed guarantee convergence costs poor comparative performance expected 
dyna pi probably small exploration bonus problem 
reduced exploration meant faster convergence occasion misleading early transitions caused get stuck suboptimal policy 
system parameters prioritized sweeping results give insight important parameters prioritized sweeping 
firstly consider performance relative observation 
exper stochastic state example table results graphed 
operation equivalent optimistic learning converge 
backups gives reasonable performance performance improves number opt dyna pi dyna opt eq failure table 
mean number observations subsequent decisions optimal 
learner run times cases bar eventual convergence optimal performance 
shown standard deviation trials 
discount factor 
optimistic methods opt prioritized sweeping dyna prioritized sweeping sa sa sa sa sa sa sa sa sa sa sa sa sa sa sa transition probabilities expected rewards state action markov control problem 
experiences converge backups obs 
log scale number experiences needed prioritized sweeping con verge plotted ups observation 
state stochastic maze table opt bored 
error bars show standard deviations runs di erent random seeds 
backups increases 
fty backups priority queue usually gets exhausted time step little improvement 
parameter 
test case inadequate exploration particularly dangerous 
maze reward states 
lesser reward comes state bottom right 
greater reward inaccessible state near top right 
trials bottom left world stochastic manner earlier examples 
trials reset goal state encountered times 
set low bad luck attempting explore near large reward state controller lose interest return spend rest days traveling inferior reward 
value run times recorded percentage runs converged correctly observations 
graphs results 
problem checked times appears su cient ensure stuck 
misleading maze 
small reward bottom right away larger reward 
shows number experiences needed convergence function set experiments 
tasks task state space quantized potential discrete states guiding rod planar maze translation rotation 
actions move forwards unit rod length move backwards unit rotate left unit rotate right percent successfully converged bored frequency correct con vergence versus bored mislead ing maze opt 
experiences converge bored mean standard de number experiences convergence independent exper iments function bored misleading maze 
parameter values 
unit 
fact action takes nearest quantized state having applied action 
position quantizations angle quantizations producing states unreachable start 
distance unit th width workspace angular unit degrees 
problem deterministic requires long speci sequence moves get goal 
shows problem obstacles shortest solution experiments 
dyna pi optimistic dyna prioritized sweeping tested 
results table 
dyna pi travel quarter way goal discover optimal path experiences 
possible chosen exploration bonus helped dyna pi di erent experiments tried value produced stable exploration 
optimistic dyna prioritized sweeping eventually converged requiring third experiences fth real time 
backups experience permitted optimistic dyna start goal dof prob lem shortest solution path 
experiences converge real time converge dyna pi optimistic dyna secs prioritized sweeping secs table 
performance deterministic rod maze task 
prioritized sweeping allowed backups experience opt 
prioritized sweeping required fewer experiences converge 
optimistic dyna took experiences took seconds twice real time 
prioritized sweeping took experiences little improvement extra time 
indicates prioritized sweeping backups observation su cient complete observations long term reward ji estimates close estimates globally consistent transition probability estimates ij 
conjecture full dynamic programming experience take days real time little better 
consider complex extension maze world invented singh consists maze extra state information dependent visited far maze 
example 
cells binary ags appended state producing total states 
ags named set cell containing corresponding letter passed 
ags cleared start state bottom left hand corner entered 
reward goal state top right entered ags set 
flag provides interest 
clear reward units 
set reward units 
task specify order visited 
controller nd optimal path 
prioritized sweeping tried deterministic stochastic maze dynamics opt 
deterministic case 
stochastic case 
cases globally optimal path ags goal avoiding ag deterministic case took observations minutes real time 
stochastic case required observations hours real time 
experiments information regarding special structure problem available learner 
example knowledge cell coordinates ag set bearing knowledge cell coordinates clear 
told learner cell transitions independent ag settings convergence rate increased considerably 
far interesting possibility automatic discovery structure inductive inference maze extra state form binary structure learned state transition matrix 
see singh current interesting direction 
third experiment familiar pole balancing problem michie chambers 
place discuss enormous number techniques applied problem equally enormous variation details task formulation 
state space cart quantized equal levels cart position cart velocity pole angular speed 
quantized equal levels pole angle 
simulation real valued state variables learner allowed base control decisions current quantized state 
actions thrust left thrust right 
problem interesting involves hidden state controller believes system markov fact 
possible values real valued state variables discretized box successor boxes partially determined real values controller 
task de ned reward units state absorbing state corresponding crash receives zero reward 
crashes occur pole angle cart position exceed limits 
discount factor trials start random survivable con gurations 
parameters opt 
simulation contains noise small amount added simulated thrust ags 
prioritized sweeping quickly usually observations crashes develops policy provides stability approximately cycles 
small amount noise stable runs approximately time steps discovered average crashes 
heuristics guide search experiments date optimistic estimate best available step reward opt set overestimate best reward available 
human programmer knows advance best possible reward go state resultant realistic optimism need experience state action pairs 
example consider maze world 
robot told location goal state previous experiments information told states blocked compute best possible reward go state 
greater reward obtained shortest possible path goal 
length path computed easily manhattan distance metric best possible reward go opt opt ropt optimistic heuristic initial exploration biased goal path discovered unexplored areas may ignored 
ignoring occurs optimistic reward go state greater obtained path 
example shows areas explored manhattan heuristic nding optimal path start state bottom leftmost cell goal state center maze 
maze states needed explored 
tasks may satis ed cease exploration obtained solution known say optimal solution 
achieved heuristic lies tells best possible reward go path twice length true shortest possible path 
discussion generalization state transition model dotted states visited manhattan heuristic derive opt bored 
concerned discrete state systems prior assumptions structure state space 
despite weakness assumptions successfully learn large stochastic tasks 
problems extra known structure state space important consider knowledge 
far common knowledge smoothness states way similar general transition probabilities similar 
td applied highly smooth problems parametric function approximator 
technique successfully large complex problem backgammon tesauro 
discrete version prioritized sweeping applied directly backgammon game states large factor method quantized space board positions sophisticated smoothing mechanism conceivably able compute near optimal strategy 
currently developing memory algorithms take advantage local smoothness assumptions 
investigations state transition models learned memory func tion approximators moore atkeson 
prioritized sweeping takes place non uniform tessellations state space partitioned variable resolution kd trees moore 
investigating role locally linear control rules reward functions partitionings bellman equation directly local linear quadratic regulators lqr see example sage white 
worth remembering system su ciently linear lqr extremely powerful technique 
pole balancer experiment inwhich local weighted regression identify local linear model lqr able create stable controller state transitions 
current investigations attempt perform generalization conjunction re inforcement learning mahadevan connell investigates clustering parts policy chapman kaelbling investigates automatic detection locally relevant state variables singh considers automatically discover structure tasks multiple ags example 
related dyna queue algorithm peng williams peng williams concurrently developing closely related algorithm call dyna queue 
conceptually similar idea discovered independently 
prioritized sweeping provides cient data processing methods learn state transition model dyna queue performs role learning watkins algorithm avoids building explicit state transition model 
dyna queue careful allows priority queue allows predecessors predicted change interestingness value greater signi cant threshold prioritized sweeping allows change times maximum reward queue 
initial experiments peng williams consist sparse deterministic maze worlds cells 
performance measured total number bellman equation processing steps convergence greatly improved conventional dyna sutton 
related sutton identi es reinforcement learning asynchronous dynamic programming computational regime prioritized sweeping 
notion optimistic heuristic guide search tree search algorithm nilsson motivated aspect prioritized sweeping schedules nodes expanded albeit di erent priority measure 
korf gives combination dynamic programming lrta algorithm 
lrta di erent prioritized sweeping concentrates search ort nite horizon set states current actual system state 
lin investigated simple technique replays backwards memorized sequence experiences controller 
circumstances may produce bene cial ects prioritized sweeping 
investigation shows prioritized sweeping solve large state space real time problems methods di culty 
bene ts memory approach described moore atkeson allow control forgetting potentially changeable environments automatically scale state variables 
prioritized sweeping heavily learning world model conclude words topic 
model world known human programmer advance adaptive system required alternatives learn model develop control rule 
learn control rule building model 
dyna prioritized sweeping fall rst category 
temporal di erences learning fall second 
motivations learning model interesting fact methods learn ii possibility accurately simulates kinds biological learning sutton barto 
third advantage touted computational bene ts learning model view dubious 
common argument real world available sensed directly bother reliable learned internal representations 
counterargument systems acting real time real experience sample millions mental experiences decisions improve control rules 
consider colorful example 
suppose anti model argument applied new arrival university campus don need map university university map 
new arrival truly university argument full exploration campus order create map 
map produced amount time saved pausing consult map traveling new location exhaustive random search real world enormous 
justi ed complain indiscriminate combinatorial search matrix version prior supposedly real time decision 
models need fashion 
prioritized sweeping algorithm just example class algorithms easily operate real time derive great power model 
mary lee rich sutton reviewers valuable com ments suggestions 
stefan schaal satinder singh useful comments early draft 
andrew moore supported fellowship serc nato 
support provided air force ce scienti research alfred sloan fellowship foundation associate biomedical en siemens national science foundation presidential young investigator award christopher atkeson 
appendix random generation stochastic problem algorithm generate stochastic systems section 
parameters snt number non terminal states st number terminal states succs mean number successors 
states position unit square 
terminal states generated equispaced circle diameter alternating black white 
non terminal states positioned uniformly random location square 
successors non terminal selected 
number successors chosen randomly random variable drawn exponential distribution mean succs 
choice successors ected locality unit square 
provides interesting system allowing successors entirely random 
empirically noted entirely random successors cause long term absorption probabilities similar set states 
locality leads varied distribution 
successors chosen simple algorithm drawn slowly growing circle centered parent state 
parent ni successors jth transition probability computed xj xk fx xn independent random variables uniformly dis tributed unit interval 
system generated check performed system absorbing non terminals eventually reach terminal state 
entirely new systems randomly generated absorbing markov system obtained 
barto singh 

computational economics reinforcement learn ing 
touretzky editor connectionist models proceedings summer school 
morgan kaufmann 
barto sutton watkins 

learning sequential decision making 
coins technical report university massachusetts amherst 
barto bradtke singh 

real time learning control chronous dynamic programming 
technical report university massachusetts amherst 
bellman 

dynamic programming 
princeton university press princeton nj 
berry 

bandit problems sequential allocation experiments 
chapman hall 
bertsekas tsitsiklis 

parallel distributed computation 
prentice hall 
chapman kaelbling 

learning delayed reinforcement complex domain 
technical report 
tr research 
christiansen mason mitchell 

learning reliable manipulation strategies initial physical models 
ieee conference automation pages 
dayan 

convergence td general machine learning 
kaelbling 

learning embedded systems 
phd 
thesis technical report 
tr stanford university department computer science 
knuth 

sorting searching 
addison wesley 
korf 

real time heuristic search 
arti cal intelligence 
lin 

programming robots reinforcement learning teaching 
proceed ings ninth international conference onarti cial intelligence aaai 
mit press 
mahadevan connell 

automatic programming behavior robots reinforcement learning 
technical report ibm watson research center ny 
michie chambers 

boxes experiment adaptive control 
dale michie editors machine intelligence 
oliver boyd 
moore atkeson 

memory function approximators learning control 
preparation 
moore 

variable resolution dynamic programming ciently learning action maps multivariate real valued state spaces 
birnbaum collins editors machine learning proceedings eighth international workshop 
morgan kaufman 
nilsson 

problem solving methods arti cial intelligence 
mcgraw hill 
peng williams 

cient search control dyna 
college computer sci ence northeastern university 
sage white 

optimum systems control 
prentice hall 
samuel 

studies machine learning game checkers 
ibm jour nal research development 
reprinted feigenbaum feldman editors computers thought mcgraw hill 
sato abe takeda 

learning control finite markov chains ex plicit trade estimation control 
ieee trans 
systems man cybernetics 
singh 

transfer learning compositions sequential tasks 
birnbaum collins editors machine learning proceedings eighth international workshop 
morgan kaufman 
stan ll waltz 

memory reasoning 
communications acm december 
sutton barto 

time derivative models reinforcement 
gabriel moore editors learning computational neuroscience foundations adaptive networks pages 
mit press 
sutton 

temporal credit assignment reinforcement learning 
phd 
thesis uni versity amherst 
sutton 

learning predict methods temporal di erences 
machine learn ing 
sutton 

integrated architecture learning planning reacting approximating dynamic programming 
proceedings th international conference machine learning 
morgan kaufman 
tesauro 

practical issues temporal di erence learning 
report rc ibm watson research center ny 
thrun moller 

active exploration dynamic environments 
moody hanson lippman editors advances neural information processing systems 
morgan kaufmann 
watkins 

learning delayed rewards 
phd 
thesis king college uni versity 

