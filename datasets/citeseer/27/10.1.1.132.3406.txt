clustering sequence data hidden markov model representation cen li gautam biswas box station department computer science vanderbilt university nashville tn usa proposed clustering methodology sequence data hidden markov model hmm representation 
proposed methodology improves existing hmm clustering methods ways enables hmms dynamically change model structure obtain better model data clustering process ii provides objective criterion function select optimal clustering partition 
algorithm terms nested levels searches search optimal number clusters partition ii search optimal structure partition iii search optimal hmm structure cluster iv search optimal hmm parameters hmm 
preliminary results support proposed methodology 
keywords clustering hidden markov model model selection bayesian information criterion bic mutual information 
clustering assumes data labeled class information 
goal create structure data objectively partitioning data homogeneous groups group object similarity group object dissimilarity optimized 
technique extensively successfully data mining researchers discovering structures databases domain knowledge available incomplete past focus clustering analysis data described static features features change observation period 
examples static features include employee educational level salary patient age gender weight 
real world systems dynamic best described temporal features values change signi cantly observation period 
examples temporal features include monthly atm transactions account balances bank customers blood pressure temperature respiratory rate patients intensive care 
addresses problem clustering data described temporal features 
clustering temporal data inherently complex clustering static data dimensionality data signi cantly larger dynamic case ii complexity cluster de nition modeling interpretation increases orders magnitude dynamic data 
choose hidden markov model representation temporal data clustering problem 
number advantages hmm representation problem direct links hmm states real world situations problem consideration 
hidden states hmm ectively model set potentially valid states dynamic process 
exact sequence stages going dynamic system may observed estimated observable behavior systems 
hmms represent awell de ned probabilistic model 
parameters hmm determined precise de ned manner methods maximal likelihood estimates maximal mutual information criterion 
hmms graphical models underlying dynamic processes govern system behavior 
graphical models may aid interpretation task 
clustering hmms rst mentioned rabiner speech recognition problems 
idea explored researchers including lee kokkinakis lee smyth 
main problems identi ed works objective criterion measure determining optimal size clustering partition ii uniform pre speci ed hmm structure assumed di erent clusters partition 
describes hmm clustering methodology tries remedy problems developing objective partition criterion measure model mutual information developing explicit hmm model re nement procedure dynamically modify hmm structures clustering process 

proposed hmm clustering methodology proposed hmm clustering method summarized terms levels nested searches 
outer inner level searches search 
optimal number clusters partition 
optimal structure partition 
optimal hmm structure cluster 
optimal hmm parameters cluster 
starting inner level search search steps described detail 

search level hmm parameter reestimation step tries nd maximal likelihood parameters hmm xed size 
known baum welch parameter reestimation procedure purpose 
baum welch procedure variation general em algorithm iterates steps expectation step step ii maximization step step 
step assumes current parameters model computes expected values necessary statistics 
step uses statistics update model parameters maximize expected likelihood parameters 
procedure implemented forward backward computations 

search level optimal hmm structure step attempts replace existing model group objects accurate re ned hmm model 
omohundro described technique inducing structure hmms data general model merging strategy 
proposed successive state splitting sss algorithm model context dependent phonetic variations 
ostendorf singer expanded basic sss algorithm choosing node candidate split time likelihood gains 
proposed derive structure hmm error correcting grammatical inference techniques 
hmm re nement procedure combines ideas past works 
start initial model con guration incrementally grow shrink model hmm state splitting merging operations choosing right size model 
goal obtain model better account data having higher model posterior probability 
merge split operations assume viterbi path change operation split operation observations state reside new states 
true merge operation 
assumption greatly simplify parameter estimation process new states 
choice state apply split merge operation dependent state emission probabilities 
split operation state highest variances split 
merge operation states closest mean vector considered merging 
describe criteria measure propose hmm model selection posterior probability measure ppm ii bayesian information criterion 

posterior probabilities hmms computation posterior probability hmm model ppm computation bayesian model merging criterion 
bayesian model merging criterion trades model likelihood bias simpler model 
assume prior probability fully parameterized model model structure model parameter uniformly distributed 
data bayes rule posterior probability model jx expressed jx likelihood function 
xj xj xj propose extend stolcke omohundro jx computation discrete density hmms continuous density hmm model 
decompose model independent components global structure transitions state trans emissions state 
assuming parameters associated state independent state model prior written structure model modeled exponential distribution explicitly bias smaller models constant 
transitions represent discrete nite probabilistic choices state dirichlet distribution calculating probability transitions state trans ty qi qi transition probabilities state ranging states follow prior weights chosen introduce bias uniform assignment parameters 
prior desirable characteristic favors state con gurations signi cant output transitions 
single component case propose je rey prior location scale parameters mean vector variance matrices associated state location scale prior shows data having smaller lead accurate determination parameter case state con guration prior awards clearly de ned states variances associated states small 

bayesian information criterion hmms problem ppm criterion depends heavily base value exponential distribution global model structure probability 
currently strategy selecting exponential base value di erent problems model selection performance deteriorates right 
alternative scheme bayesian model selection approach 
criterion bayesian model selection relative model posterior probability xj assuming uniform prior probability di erent models xj xj marginal likelihood 
goal approach select model gives highest marginal likelihood 
computing marginal likelihood complex models active research area 
established approaches include monte carlo methods gibbs sampling methods various approximation methods laplace approximation approximation bayesian information criterion 
documented monte carlo methods accurate computationally ine cient especially large databases 
shown certain regularity conditions laplace approximation quite accurate computation expensive especially component hessian matrix computation 
widely cient approximation method marginal likelihood bayesian information criterion log form marginal likelihood model data computed logp jx logp xj logn maximum likelihood ml con guration model dimensionality model parameter space number cases data 
choose bic alternative hmm model selection criterion 

search level optimal partition structure commonly distance measures context hmm representation sequence likelihood measure symmetrized distance measure pairwise models 
sequence model likelihood distance measure hmm clustering algorithm 
sequence hmm likelihood oj measures probability sequence generated model sequence hmm likelihood distance measure object cluster assignments automatically enforces maximizing group similarity criterion 
means style clustering control structure depth rst binary divisive clustering control structure proposed generate partitions having di erent number clusters 
partition initial object cluster memberships determined sequence hmm likelihood see section distance measure 
objects subsequently redistributed hmm parameter reestimation hmm model re nement applied intermediate clusters 
means algorithm redistribution global clusters 
binary hierarchical clustering redistribution carried child clusters current cluster 
algorithm guaranteed produce maximally probable partition data set 
goal single partition data means style control structure may 
wants look partitions various levels details binary divisive clustering may suitable 

search level optimal number clusters partition quality clustering measured terms cluster similarity cluster dissimilarity 
common criterion measure hmm clustering schemes likelihood data models set clusters 
distance measure maximizing homogeneity objects cluster want criterion measure comparing partitions terms cluster distances 
partition mutual information pmi measure task 
bayes rule posterior probability model trained data oi isgiven oij oi oij pj oij prior probability data coming cluster feature values inspected oij conditional probability displaying feature oi comes cluster represent average mutual information observation sequence oi complete set models logp log oij log pj oij maximizing value equivalent separating correct model models training sequence oi 
information partition models computed summing mutual information pj pn training sequences pmi nj number objects cluster andj total number clusters partition 
pmi maximized models separated set models 
object object object object feature feature 
objects generated di erent hmms 
preliminary results conducted preliminary experiments hmm clustering arti cially generated data 
nished implementing hmm re nement procedure experiments assume correct model structure known xed clustering process 
uniform prior distributions assumed hmms computation 
experiments objectives hmm clustering derive partition optimal number clusters object cluster memberships 
generate data clusters rst manually create hmms 
hmms generate nk objects described temporal sequences 
length temporal sequence total data points data set nk experiments choose nk 
hmm cluster states 
shows example data objects models 
observed feature values quite di cult di erentiate objects generated model 
fact objects generated model objects generated di erent model 

experiment experiment illustrate binary hmm clustering process ects pmi criterion measure 
rst part experiment pmi criterion measure incorporated binary clustering tree building process 
branches tree terminated objects node object redistribution process node ends cluster partition 
full binary clustering tree pmi scores intermediate nal partitions computed shown 
pmi scores right tree indicate quality current partition includes nodes frontier current tree 
example pmi score partition having clusters pmi score partition having clusters andc result clustering process cluster partition fragmented clusters cluster fragmented cluster fragmented cluster fragmented 
shows binary hmm clustering tree pmi criterion measure determining branch terminations 
dotted lines cut branches search tree split parent cluster results decrease pmi score 
clustering process correct cluster partition 
misclassification pmi pmi bef ore pmi af ter pmi bef ore pmi af ter ore pmi af ter 
binary hmm clustering tree 
means binary divisive ratio misclassification binary divisive means number states 
hmm clustering results data having di erent levels noise clustering starting di erent size hmms 
experiment experiment want study performance hmm clustering system data corrupted di erent levels noises 
white gaussian noises added data 
added noise computed di erent signal noise ratios 
noise successively added original cluster data signal noise ratio successfully decreased 
shows clustering results terms misclassi cation counts vs signal noise ratio 
observe noise ect clustering results large ndb 
clustering process fails separate objects hmms 

experiment experiment study ects di erent initial hmm structure clustering performance 
original hmms states 
initial hmms experiment number states ranging 
shows results terms misclassi cation counts versus number states initial hmms 
clustering results remains initial hmms having states 
initial hmms having states misclassi cation high 
algorithm fails separate objects hmm model 
initial hmms having states clustering partition generated close optimal 
result agree intuition initial hmms having states result worse clustering partition cases initial hmms states 
reason model small multiple state de nitions squeezed state state de nitions speci model accurate 
hand extra states model model accurate dividing single state de nition multiple state de nitions 
retain original model setting transitions extra states small ignore states 

cheeseman stutz bayesian classi cation autoclass theory results advances knowledge discovery data mining fayyad piatetsky shapiro smyth uthurusamy eds ch 
pp 
aaai mit press 

biswas weinberg li iterate conceptual clustering method knowledge discovery databases arti cial intelligence petroleum industry symbolic computational applications braunschweig day eds 

fisher knowledge acquisition incremental conceptual clustering machine learning pp 


wallace dowe intrinsic classi mml program proceedings seventh australian joint conference onarti cial intelligence pp 
world scienti 

li unsupervised classi cation temporal data 
survey department computer science vanderbilt university apr 

rabiner lee juang hmm clustering connected word recognition proceedings international conference speech signal processing 

lee context dependent phonetic hidden markov models speaker independent continuous speech recognition ieee transactions acoustics speech signal processing pp 


kokkinakis algorithm clustering continuous density recognition error ieee transactions speech audio processing pp 
may 

speaker independent phone modeling speaker dependent hmm composition clustering proceedings icassp pp 


smyth clustering sequences hidden markov models advances neural information processing 

baum petrie soules weiss maximization technique occurring statistical analysis probabilistic functions markov chains mathematical statistics pp 


dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society series methodological pp 


ghahramani jordan factorial hidden markov models tech 
rep mit cognitive science aug 

stolcke omohundro best rst model merging hidden markov model induction tech 
rep tr international computer science institute center st suite berkeley ca jan 

omohundro best rst model merging dynamic learning recognition advances neural information processing systems pp 


successive state splitting algorithm cient modeling proceedings international conference speech signal processing pp 


ostendorf singer hmm topology design maximum likelihood successive state splitting computer speech language pp 


vidal mas learning structure hmm grammatical inference techniques proceedings international conference speech signal processing pp 


box tiao bayesian inference statistical analysis addison wesley publishing 

kass raftery bayes factor journal american statistical association pp 
june 

heckerman tutorial learning bayesian networks tech 
rep msr tr microsoft research advanced technology division microsoft way redmond wa 

cooper herskovits bayesian method induction probabilistic network data machine learning pp 


chib marginal likelihood gibbs sampling journal american statistical association pp 
dec 

explaining gibbs sampler american statistician pp 
aug 

rabiner tutorial hidden markov models selected applications speech recognition proceedings ieee pp 
feb 

juang rabiner probabilistic distance measure hidden markov models technical journal pp 
feb 

bahl brown de souza mercer maximum mutual information estimation hidden markov model parameters proceedings ieee international conference speech signal processing vol 
pp 


computations evaluations optimal feature set hmm recognizer 
phd thesis brown university may 
