gesture interface human robot interaction stefan computer science department carnegie mellon university pittsburgh pa usa romero instituto de ci de ao universidade des ao paulo ao carlos sp brazil sebastian thrun computer science department carnegie mellon university pittsburgh pa usa 
service robotics currently pivotal research area robotics enormous societal potential 
service robots directly interact people nding natural easy user interfaces fundamental importance 
past predominately focussed issues manipulation relatively robotic systems equipped exible user interfaces permit controlling robot natural means 
describes gesture interface control mobile robot equipped manipulator 
interface uses camera track person recognize gestures involving arm motion 
fast adaptive tracking algorithm enables robot track follow person reliably ce environments changing lighting conditions 
alternative methods gesture recognition compared template approach neural network approach 
combined viterbi algorithm recognition gestures de ned arm motion addition static arm poses 
results reported context interactive clean task person guides robot speci locations need cleaned instructs robot pick trash 

eld robotics currently undergoing change 
past robots predominately factories purposes manufacturing transportation new generation service robots begun emerge 
service robots cooperate people assist everyday tasks 
speci examples commercial service robots include robot deployed numerous hospitals worldwide king autonomous cleaning robot kluwer academic publishers 
printed netherlands 
deployed supermarket opening hours endres www com htm robot designed life easier carrying golf clubs 
robots interact people avoiding 
near similar robots expected appear various branches entertainment recreation health care nursing expected interact closely people 
upcoming generation service robots opens new research opportunities 
issue robot navigation researched quite extensively cox wilfong kortenkamp borenstein considerably little attention paid issues human robot interaction see section discussion related literature 
service robots operated non expert users capable operating computer keyboard 
essential robots equipped natural interfaces facilitate interaction robots people 
need ective human robot interfaces recognized research community example torrance developed thesis natural language interface teaching mobile robots names places indoor environment torrance 
due lack speech recognition system interface required user operate keyboard natural language component instructing robot signi cantly easier 
asoh colleagues developed interface integrates speech recognition system phrase natural language interface asoh 
authors successfully instructed ce conversant robot navigate ce doors significant places environment verbal commands 
people communication involves spoken language 
example far easier point object verbally describe exact location 
gestures easy way give geometrical information robot 
researchers proposed vision interfaces allow people instruct mobile robots arm gestures see section 
example kortenkamp colleagues kortenkamp kahn kahn developed mobile robot systems instructed arm poses 
previous mobile robot approaches recognize static arm poses gestures recognize gestures de ned speci temporal patterns 
motion gestures commonly communication people provide additional freedom design gestures 
addition reduce chances accidentally classifying arm poses gestures intended 
appear better suited human robot interaction static pose gestures 
mobile robot applications gesture recognition impose requirements system 
gesture recognition system needs small robot processing power generally limited 
human robot may gesture shown system may assume static background xed location camera human performs gesture 
fact tasks may involve person case robot able recognize gestures tracks person adapt possibly drastic changes lighting conditions 
additionally system acceptable speed 
naturally want gesture immediately halt robot seconds 
requirements taken consideration designing system 
presents vision interface designed instruct mobile robot pose motion gestures 
lowest level adaptive dual color tracking algorithm enables robot track follow person speeds cm second avoiding collisions obstacles 
tracking algorithm quickly adapts di erent lighting conditions segmenting image nd person position relative center image pan tilt unit keep person centered 
gestures recognized phases rst individual camera images mapped vector speci es likelihood individual poses 
compare di erent approaches neural networks uses graphical correlation template matcher 
second phase viterbi algorithm employed dynamically match stream image data pre de ned temporal gesture templates 
reported goes design gesture interface 
goals research usability gesture interface context realistic service robot application 
interface integrated existing robot navigation control software 
task robot motivated clean ce task mobile robot competition simmons 
robot autonomously search ce objects scattered oor deposit nearby trash bins 
task di ers want guide robot trash instructed gestures 
robot pick trash carry dump trash bin 
remainder organized follows 
section describes approach visual servoing followed main gesture recognition algorithm described section 
experimental results discussed section 
concluded discussion related section general discussion section 

finding tracking people finding tracking people core vision gesture recognition system 
robot know image person visual tracking people studied extensively past years crowley wren 
vast majority existing approaches assumes camera mounted xed location 
approaches typically rely static background human motion detected image di erencing 
case robot gesture recognition assume background static 
robot tracks follows people background lighting conditions change considerably 
addition processing power robot usually limited imposes additional burden 
see section adaptive color approach tracking face shirt color appears capable tracking person changing lighting conditions 
approach tracks person combination colors face color shirt color face color feature tracking people leading remarkably robust face tracking long lighting conditions chance yang waibel 
advantage color complicated models head motion facial features rowley lies speed camera images processed crucial aspect implemented robotic platform 
experiments color insu cient tracking people moving robot extend algorithm tracking second color typically aligned vertically person face shirt color 
remainder section describes basic algorithm run approximately hz low pc 

color filters approach adopts basic color model commonly face detection tracking literature 
detect face color advantageous assume camera images represented rgb color space 
speci cally pixel camera image represented triplet 
rgb implicitly encodes color brightness face color distribution chromatic color space 
distribution obtained person face shown 
saturation 
pixels camera image proportional rp rp gp gp bp bp share color rp rp di er brightness 
brightness may vary drastically lighting condition common practice represent face color chromatic color space 
chromatic colors styles known pure colors de ned projection 
approach yang waibel approach represents chromatic face color model gaussian model characterized means nx nx ri gi covariance matrix gr rg rg gr nx nx nx ri gi ri gi typically color distribution human face color centered small area chromatic color space possible colors occur human face 
shows face color distribution person shown 
horizontal axes represents values respectively vertical axis response color model 
higher response better person face matches color model 
gaussian model face color mahalanobis distance th pixels chromatic value ri gi ri gi ri gi superscript refers transpose vector 
noticed approach uses distinctive colors track people face color shirt color 
consequently employs separate color models face face face face shirt shirt shirt shirt face shirt color respectively 
determine person location image pixel mapped chromatic color space responses face shirt color lter computed ri gi ri gi see equation 
raw camera image see response face color lter shirt color lter 
better pixel matches tracking person raw camera image face color ltered image shirt color ltered image 
projection ltered image horizontal axis search window 
face shirt center tracking adaptation lters 
search window person expected image 

sequence camera images recorded single run illustrates potential speed magnitude changes lighting conditions gesture recognition mobile robots di cult 
color model higher response color darker pixel image 
lter 
adaptation robot moves environment person appearance respect color changes due changing lighting conditions 
figures show images recorded robot followed person building 
note di erent illumination person lab hallway 
lighting conditions changes background colors may uence face color appearance 
similarly robot follows person apparent face color change person position changes relative camera lighting 
consequently essential tracking system cope di erent lighting conditions 
approach adapts color model particular adapts face model leaky integrator lter exponential decay face face face face face face shirt color model adapted fashion shirt shirt shirt shirt shirt shirt face shirt face face denote values obtained image face face values face model previous time step face model 
learning rate experiments determines quickly model adapts changes lighting conditions 
determines rate system forgets information 
example log log log iterations approximately initial lter coe cients forgotten 
trades algorithm ability rapidly adapt new situations close ability memorize past color observations close 
experiments reported adapting color models absolutely crucial robustly tracking person moving camera 

finding people approach detects people learn initial color mode initiate tracking 
achieved generic face color model high variance model accommodated range typical face colors 
tracked robot person center robot cameras 
robot tracking speci person constantly screens xed central region image 
region face color detected applying xed threshold face color lter response target area face center assumed centroid face color region 
initial means face model means covariance obtained small region surrounding estimated face center equations similarly shirt color model initialized pixels small rectangular image region xed distance face 
location rectangular image region corresponds region cm person face center assumption person distance robot approximately meter 
due size shirts placement rectangular region robust variations people heights distance robot camera 
practice person center robot camera image 
necessary initial step tracking person relative position camera eld view unconstrained 
examples windows face shirt depicted 
note previous knowledge person shirt color necessary order nd person 
system acquires initial shirt color model region face 
locate track people arbitrary shirt colors long su ciently coherent long vertically aligned 
uni color shirts best approach cope multicolored shirts adaptive color model tends specialize colors degradation performance 

tracking people initial task nding person center face center shirt determined 
treating lter response independently order track person advantageous combining responses lters 
consequently possible track person response lter expected 
locating person image performed steps rst step horizontal coordinates person image determined 
secondly person vertical coordinate vertical center face shirt determined 
speci cally rst phase system searches cooccurrences vertically aligned face shirt color 
step rests assumption face located person shirt 
color lter response summed horizontally neighbors average entire column computed 
shows resulting vectors expanded vertically reader convenience 
grey level regions face shirt indicates horizontal density face shirt color respectively 
darker pixel better match 
vectors multiplied component wise 
determine estimated 
example gestures gesture follow gesture 
gesture static gesture follow gesture involves motion indicated arrows 
horizontal coordinate person maximum value product determined index taken person coordinate 
remaining problem nding vertical coordinates face shirt search dimensional space 
algorithm simply returns local smoothing location highest response subject condition face shirt 
decrease amount computation image search restricted small window centered position person previously observed 
approach tracks colors search windows constructed person face body face shirt 
examples windows shown 

gesture recognition primary goal research develop evaluate vision interface capable recognizing pose gestures motion gestures 
pose gestures involve static con guration person arm gesture shown motion gestures de ned speci motion patterns arm follow gesture shown 
recognition gestures carried phases 
rst phase arm poses extracted individual camera images 
refer process pose analysis 
second phase temporal sequences arm poses analyzed occurrence gesture 
process referred temporal template matching 

examples pose templates 
excitatory region shown black inhibitory grey 
white regions considered matching process 

pose analysis approach uses di erent methods pose analysis neural networks correlation template matching 
methods motivated desire understand relative bene ts approach 
section empirical results comparing approaches 
approaches common operate sub region image contains person right side determined tracking module 
words rely tracking module segment image 
input section image contains right upper side person body computed coordinates obtained tracking person 
approaches common output probability distribution arm poses 
di er way segmented image mapped probability distribution 

graphical template matcher graphical template matcher called pose template matcher compares images set pre recorded templates arm poses called pose templates 
speci cally color ltered image correlated set pre recorded templates arm poses 
pose template consists regions excitatory region speci es arm expected speci pose inhibitory region arm pose 
shows examples pose templates 
excitatory regions shown black inhibitory regions shown grey 
templates constructed labeled examples human arm poses template excitatory region extracted largest coherent region ltered image segment simple geometric routine employed determine nearby inhibitory region 
consequently pixel values excitatory region inhibitory region irrelevant correlation ti pose template response shirt color model de ned correlation coe cient computed follows ti ti ti pi pi pi denotes pixel lter response coordinates pixel pose template 
means computed area pose template 
regions expect response shirt color model high value pixel pose template 
inhibitory regions value hand response shirt color model expected low 
note terms numerator denominator determined computations camera frame 
result correlating image segment pre recorded templates correlation vector feature vector ofr components pose template depicted 
example shown center 
component vector represented square size associated magnitude color indicates sign black negative white positive example happens non negative 
new image frame feature vector computed described 
feature vectors form basis temporal template matching 

neural network method alternatively arti cial neural network recognize poses 
neural network approach predicts angles arm segments relative person right side image segment 
input network image segment sampled toa matrix 
output corresponds angles arm segments encoded multi unit gaussian representations just alvinn pomerleau 
speci cally gaussian output encoding uses multiple units encode single scalar value generating gaussian activations array output units 
pomerleau representations gave best results ones tried course research 
jj pose template matching process response shirt color model arm position templates database 
implementation network trained backpropagation rumelhart database hand labeled training images 
training labels consisted angles arm segments relative image plane speci ed graphical interface hour 
implementation neural network possesses output units encode angle arm upper segment angle vertical axis measure angle arm lower segment angle horizontal axis 
values encoded gaussians 
example shows di erent angles angle angle encoded gaussian function yi constant experiments di angle angle 
shows example network input output target values 
input sampled color ltered image size 
output gauss encoded 
nearness outputs rst angle angle 
gaussian output encoding pair angles 
third row targets second forth row suggests example network predicts angles high accuracy 
shows ltered example image 
superimposed angle estimates generated neural network 
recover angle estimates gaussian encoding network output converted angles tting gaussians network output units described pomerleau 
speci cally sets output units approach determines best tting xed variance gaussian yi network average error angles measured independent set testing images shown table experiments di erent network topologies tested vanilla backpropagation learning rate empirically determined best 
pose analysis methods neural network method pose template matcher generate multi dimensional feature vectors image 
sure methods adhere output format essential empirical comparison network output transformed analytically output format template algorithms 
recall template matcher uses di erent pose templates section 
obtain representation neural network technique vector components constructed component corresponds euclidean distance pair angles generated input output target output target 
input neural network sampled color ltered image size outputs targets network angles 
network output pair angles pose template 
trick enables compare graphical template matcher neural network approach empirically described 

temporal template matching temporal template matcher integrates multiple camera images detect gestures de ned sequences arm poses static dynamic 
compares stream feature vectors set prototypes gesture templates individual gestures previously recorded 
shows examples gesture templates gestures follow 
templates sequence prototype feature vectors time arranged vertically 
seen gure gesture pose gesture feature vectors look alike follow gesture involves motion 
approach treats types gestures 
types gestures pose gestures motion gestures encoded temporal templates 
teach robot new gesture person presents robot executes gesture times times prespeci ed time intervals 
approach segments examples jj 
angles matching process response shirt color model pair angles arm position templates database 
pieces equal length uses average feature vector time segment prototype segment 
shows segmentation process simplicity gure depicts training example 
note process compresses observed gesture factor example gesture lasted frames represented feature vectors 
compensate di erences exact timing person performs gesture approach uses viterbi algorithm time alignment 
viterbi alignment commonly context hidden markov models rabiner juang employs dynamic programming nd best temporal alignment feature vector sequence gesture template 
highly popular speech recognition community cf 
waibel lee compensates variations timing spoken language gestures 
approach viterbi algorithm continuously analyzes stream incoming feature vectors possible presence gestures 
aligning observation stream gesture table average error obtained neural network testing data topology upper arm segment lower arm segment template size contains compressed feature vectors matrix matrix size computed top left bottom right follows ek ek ek ek ek ek ek ek ek dynamic programming equation computes likelihood gesture optimal time alignment consequently nal element determines sequence likelihood conditioned gesture template 
approaches uses euclidean distance compute di erence feature vector observation feature vector gesture template 
summarize time variations gesture template matching accommodated viterbi algorithm 
algorithm aligns actual observation sequence templates optimally compensating variations exact timing gesture 
viterbi algorithm determines likelihood observing sequence gesture template essential classi cation gestures 

experimental results approach implemented mobile robot evaluated series experiments 
integrated autonomous robot navigation system test utility context realistic service robotics task 
central question underlying experiments robustness robust system variations lighting conditions di erences people individual motion di erent shirt colors 

examples gesture templates 
gesture templates sequences prototype feature vectors 
shown gesture templates gesture involve motion follow gesture involves motion indicated change time 
usability usable interface speci cally control robot context service task 
th experimental results section address questions systematically 
reader convenience section broken parts 
tracking results performance people tracker obtained person ce building rooms hallways 
environment drastically varying lighting conditions 

gesture recognition comparative approaches gesture recognition 
additional experiments investigate utility dynamic motion gestures comparison static pose gestures 

integration results reported obtained context clean task aimed elucidate usefulness gesture interface context realistic service robotics task 
clean task involves human robot interaction mobile manipulation shows interface recognize dynamic gestures responds corresponding actions 
robot research amelia rwi robot equipped color camera mounted pan tilt unit sonar sensors sick laser range nder 
evaluate utility gesture recognition real world scenario integrated robot navigation control software developed conjunction university bonn rwi thrun 
aaaaaaaa 
creating gesture template example stream observed feature vectors left compressed gesture template right 

implementation details approach implemented low pc intel mhz pentium mb main memory grabbing images rate fps resolution pixels 
major computational bottleneck ltering image gaussian lters pose analysis template matching neural networks 
focusing computation response color models narrow window person performance roughly frames second obtained 
furthermore gaussian responses computed second pixel horizontally vertically dividing computational ort factor 
adapting covariances means regions face shirt xed size pose computational problem windows relatively small 
pj ek ek ek oj 
aligning observations gesture template 
shown left feature vectors temporally aligned matrix gesture template shown top right rotated degrees counterclockwise 
cope slightly di erent gestures kind system uses multiple prototypes gestures gesture 
time alignment performed multiple time scales corresponded speci assumed total duration gesture 
observed feature vectors constantly analyzed aligned set gesture templates viterbi algorithm described section 
gesture detected result alignment surpasses certain pre speci ed threshold 
trained system recognize di erent gestures gesture shown 
gesture static gesture moving arm right position second su cient trigger command 

amelia robot experiments 
follow follow gesture involves wave motion moving arm shown figures 
pointing vertical gesture motion gesture 
starting initial arm position depicted person moves arm position shown holds brief moment returns initial arm position 
pointing low starting initial arm position person points object oor shown returns initial arm position 
choice gestures motivated particular service task described 

tracking measured performance people tracker instructing robot follow person environment 
experiment judged successful robot managed track follow person meters processed close camera images 
testing environment contains illuminated lab corridor low hanging ceiling lights strong ect brightness person face 
conducted experiments involving subjects range di erent shirts see 
images arm positions contained example gestures 
image marks speci point time example start middle position motion gesture 
system development demonstrations 
subset nal experiments carried software development completed subjects measured success rate 
experiment labeled success robot lost track ofthe person 
results indicate extremely low frame error rate single recognition error typically leads failure tracking module 
identical software parameters tests regardless subject shirt color 
experiments subjects wore shirts di erent colors including gray red green light blue white black 
comes little surprise colored shirts maximize reliability tracking algorithm 
majority failures observed white black shirts di cult colors track 
failure rate approximately 
due limited range ccd cameras dark regions background shades perceived 
tracking person pose analysis 
shown search adaptation windows 
additionally pose template component inthe feature vector highest correlation superimposed person 
pitch black bright spots ceiling lights direct sunlight usually appear white image 
combination colors face shirt ability adapt quickly new lighting conditions failures rare black white shirts 
shows example recorded tracking person dark shirt 
shown estimated centers face shirt small rectangles location search windows large rectangles 
template superimposed 
person midst carrying pointing low gesture 

single image gesture recognition rst evaluated static gestures recognized single camera image 
goal experiment characterize accuracy gesture recognition extreme case single image available set base line subsequent analysis gesture recognition image streams 
single image recognition applies pose template matcher viterbi match temporal gesture prototypes 
gesture recognized probability speci gesture surpasses pre selected optimized threshold 
obviously natural variance images ects classication rate multi image case individual image variations lesser ect yield accurate recognition 
shows example pose template superimposed person image 
systematic experiment involving camera images correct pose identi ed cases nearby pose recognized pose template matcher produced answer wrong 
results encouraging improved considering stream camera images time shown 
misclassi cations exclusively due variations image lighting changes mandated adaptation color lters 
real world application easily happen person moves arm gesture type position intending perform gesture 
incidents undoubtedly increase misclassi cation rate making questionable single image gesture recognizer su ciently robust real world applications 
section report results dynamic gesture recognition multiple images illustrating provides better results static single image approach 

gesture recognition evaluation gestures sequences camera images interesting 
gestures may static dynamic recognized form sequence images viterbi algorithm 
section report results temporal gesture recognition system including comparison di erent methods pose analysis graphical template matcher neural network approach 
evaluated approaches database image sequences 
sequences person performed gestures de ned 
remaining sequences gesture performed 
test robustness system extreme half sequences collected robot motion 
sets experiments recognition threshold hand tuned separate data set 
false positives robot recognizes gesture shown instructor generally worse false negatives robot fails recognize gesture tuned thresholds away number false positives small 
tables ii iii show recognition accuracy obtained graphical template matcher neural network approach respectively 
approaches neural network approach pose template matcher classi ed examples correctly 
testing sequences 
template table ii 
recognition results pose template matcher 
gestures recognized follow point vert point low gesture gestures follow point vert point low gesture matcher prominent error failure recognize pointing vertical gesture attribute poor set motion templates gesture 
neural network poorest gesture follow gesture detected accidentally person gave gesture mistaken pointing low gesture 
di erent failures approaches suggest combining template matching neural networks yield better results approach isolation 
combined approach increases computational complexity reducing frequency images analyzed 
near frame rate essential successful tracking decided integrate approaches 
example shows recognition results di erent classes gestures follow pointing testing sequences 
horizontal axis depicts time frame number vertical axis shows probability estimate gesture template 
class contains di erent templates follow pointing class contain templates 
system recognized gesture point follow gesture point 
shows output pose template matcher time subset image sequence frames 
gure compared depicts prototype template gesture follow gesture 
follow gesture executed subsequence correctly detected 

clean task nal experiment evaluates gesture recognition interface context realistic real world service robot task 
task chose table iii 
recognition results neural network algorithm 
gestures recognized follow point vert point low gesture gestures follow point vert point low gesture motivated clean ce task designed aaai mobile robot competition simmons autonomous robot asked locate retrieve trash items scattered ce environment various subsequent competitions similar themes 
scenario di ers human interacts robot initiates clean task performed autonomously robot 
particular task hand involved human instructing robot gestures follow location trash 
robot picks trash delivers nearby trash bin 
detail clean task involves gesture operations perform gesture initiate follow behavior perform direct motion commands stopping robot guide robot places memorize point objects oor initiate clean tasks robot searches trash picks delivers nearest trash bin 
robot shown gesture immediately stops 
person points object ground robot starts searching object visual eld 
robot succeeds moves object picks object returns nearest trash bin 
location trash bins known robot 
perform task complexity gesture interface integrated robot control architecture previously developed lab collaboration real world interface division robotics thrun 
nutshell navigation system enables robots navigate safely acquiring maps unknown 
results matching process time normalized length template 
system recognized gesture point follow gesture point environments 
fast motion planner allows robots move point point alternatively explore unknown terrain 
collisions unexpected obstacles avoided fast reactive routine sets velocity travel direction robot periodic measurements robot various proximity sensors laser sonar infrared tactile 
robot motion probabilistic localization method continuously tracks robot position comparing sensor readings learned map environment 
permanent lead modi cations map new motion plans 
previous approach demonstrated robots navigate safely speed centimeter second densely populated environments 
previous versions software won second rst price aaai mobile robot competitions respectively 
software backbone interactive robotic tour guides rhino minerva successfully installed deutsches museum bonn germany burgard smithsonian national museum american history washington dc thrun 
robots successfully gave tours visitors average speed centimeter second 

example feature vector stream contains follow gesture 
arrow indicates point time follow recognized see 
gesture templates matching depicted output temporal template matcher shown 
primary de ciencies previous system lack natural human robot interface 
instruct robot basically program hand 
course really acceptable service robots necessarily instructed operated non expert users 
new gesture interface 
map robot operational range meters trace speci example successful clean operation 
robot waited corridor guided lab picked deposited trash bin 
combination navigation methods provides new level usability system 

integration results shows example run amelia instructed pick soda 
shown map robot environment constructed occupancy grid technique elfes moravec thrun actual path robot known location trash bin 
example robot initially waits corridor person 
person instructs robot follow lab follow gesture 
robot follows person speed cm sec simultaneously avoiding collisions nearby obstacles 
lab person stops robot invoking gesture 
second gesture recognized robot stops 
trash nearby person points piece trash soda oor 
robot uses visual routines thrun determine exact location soda 
deploys manipulator navigates soda picks navigates back corridor trash deposited trash bin 
task conveniently encoded nite state automaton 
gesture interface worked extremely reliably 
tests full clean tasks observe single failure 
subjective judgment interface easy 
operating robot low speed cm second easier instruct robot gave person time perform gesture robot motion 
tests higher speeds cm second di cult reliably position robot close soda approach manage speed 
obviously person know gestures advance 
gestures similar communication humans heavy noise environment airports making easy people learn gestures alternatively teach robot new set gestures 
assessment clean task gesture interface provides exible easy interface command control mobile service robots 

failure modes performance gesture interface encouraging reader notice possesses limitations met lead failure gesture interface 
particular tracking algorithm assumes person face visible times 
unproblematic person interacting non moving robot 
followed robot current interface necessary person walks backwards 
sophisticated tracking algorithms necessary cope people facing robot 
example tracking algorithm simultaneously learn model person hair color tracking continued person turns 
current gesture interface scaling invariant 
due extremely limited computational power mobile robot size image template xed 
result person stay constant distance robot currently meters 
recognition rate drops distance robot small high 
limitation overcome automatically scaling gesture templates zoom lens 
person distance easily recognized robot proximity sensors 
expect interface su ciently robust function dense crowds ones rhino minerva faced museums 
di culty tracking people crowds similar looking faces confuse tracking routine 
progress head tracking occlusion maccormick blake provide answer 
testing interface situations scope current 

related idea gestures instruct mobile robot new 
huber kortenkamp kortenkamp kortenkamp describe system uses stereo vision optical ow model people recognize gestures 
system capable recognizing distinct pose gestures pointing hand signals interprets gestures context intelligent agent architecture 
gestures recognized modeling person head shoulders elbows hands set proximity spaces 
proximity space small region scene measuring stereo disparity motion 
proximity spaces connected joints links constrain position relative 
robot recognizes pose gestures examining angles links connect proximity spaces 
con dence gesture build logarithmically time angles stay limits gesture 
comparison gesture recognition system kortenkamp runs completely board robot just 
bound single camera stereo vision dedicated vision hardware process camera images 
issues di erences physical setup system approach key di erence recognize motion gestures kortenkamp approach relies static gestures involve motion 
systems high level control architecture 
approach uses kortenkamp integrated system firby reactive action package rap firby 
rap system takes consideration robot task current state state world selects set skills run accomplish tasks 
kortenkamp extended recognize people faces mobile robot architecture wong 
perseus system kahn kahn firby speci cally addresses task recognizing pointing gestures 
capable nding objects people point 
perseus uses variety techniques including feature maps feature maps edge feature maps motion feature maps reliably solve visual problem non engineered worlds 
perseus provides interfaces symbolic higher level systems rap reactive execution system mentioned 
perseus applied object pickup task 
rst part task requires recognizing person scene 
perseus assumes people moving objects scene 
motion segment person background 
image segmented color clothing known may tracking person 
knowledge task environment stages processing best interpret scene current situation 
important parts body hands head tracked 
fusing abovementioned feature maps perseus able reliably track parts body 
gesture recognized person arm moved side body remained stationary short time 
person points position head hand determine area pointed 
area searched object 
perseus described franklin context building robot waiter 
task perseus extended recognize reach gesture similar pointing gesture person arm 
person hand contain soda 
depending information current context robot delivers soda person receives soda person 
approach perseus restricted domain recognizing pointing gestures 
gestures static con guration person arm include motion gestures 
system perseus able detect pointing gestures capable identifying object pointed 
approach uses color information task perseus uses multiple independent types information intensity feature maps disparity feature maps 
perseus model person arm distance robot person may vary 
colleagues developed similar system gesture control robot 
approaches eld approach recognizes static pose gestures 
approach similar di erent neural network algorithm gesture recognition 
notably goes tracking algorithm uses collection cues just color cues track person segment image 
particular uses neural networks face detection see rowley approach analyses shape head shoulder contour 
result expect approach track people robustly results reported suggest tracking color pairs adaptive color lters yields results wide array situations 
approaches recognizing hand gestures mobile robots cui weng triesch von der malsburg triesch von der malsburg 
example triesch van der approach tracks hand stereo camera system color motion depth cues localization hand 
hand posture recognized elastic graph matching algorithm 
due relatively complex scene analysis approach unable recognize gestures real time 
suited mobile robots background lighting conditions vary dramatically short periods time 
domain gesture robot arm manipulation achieve remarkable recognition results scenes cluttered background 
gesture interfaces long applied non moving systems cameras typically mounted static location pan tilt unit 
review selected number related approaches refer reader rich decade old literature topic 
wren extended arti cial live ive alive project maes produce model people variety tasks cf 
wren 
alive project creates virtual world person person interact virtual agents 
person watches image interacting animated agents television monitor alive allows person interact agent performing gestures agent come 
nder person finder models people connected sets blobs arm leg head torso lower body 
blob spatial color gaussian distribution associated 
distributions track various parts person 
interesting application blob representation described starner pentland starner pentland 
authors associated spatial statistics users hands hidden markov models interpret word subset american sign language asl 
able produce real time asl interpreter sign recognition accuracy 
abovementioned systems clear di erences domain mobile robots 
assume static background controlled lighting conditions assumptions hold robot camera moving 
furthermore processing power network bandwidth case mobile platform limited 
approach uses similar techniques coping temporal characteristics gesture 
wilson bobick wilson bobick model gestures view point dependent motions person body 
gestures intended communicate information authors assume people actively try easy understand orienting standard way respect recipient 
premise allows hidden markov model model motion gesture observing examples gesture single viewpoint 
major di erence system location person body hands known system 
example recognizing gestures performed hands input system joint angles returned dataglove 
wilson bobick introduce system uses position head hands determined vision system recognize di erent ai chi gestures campbell 
cope temporal characteristics gesture authors state linear hidden markov model 
wilson bobick wilson authors focus representing temporal characteristics gesture 
state technique summarization recognition gestures 
gestures de ned sequence states measurement con guration space 
authors assume known gesture related sensory data movement hand measured magnetic spatial sensor 
nder limitations domain mobile robots changing lighting conditions apply 
example wilson assume background change 
system dedicated vision hardware process stereo images 

article described vision gesture interface human robot interaction 
hybrid consisting adaptive color lter alternative methods recognizing human arm gestures pose template matcher neural network method 
viterbi algorithm employed recognize variable length motion gestures streams feature vectors extracted image 
interface integrated mobile robot navigation architecture evaluated context mobile manipulation task clean cooperatively carried human instructor 
interface goes previous capable recognizing static pose gestures dynamic motion gestures commonly communication people 
motion gestures de ned speci temporal patterns arm movements improve classi cation accuracy reduce chances accidentally classifying arm poses gestures intended 
experimental results illustrated high level robustness usability interface individual components 
error rate 
subjective judgment interface suited instruct mobile robot pick trash deliver trash bin 
example application designed test utility gestures human robot interaction conjecture proposed interface transcends broader range upcoming service robots 
open questions warrant research 
prominent limitations current approach addressed section 
example tracking module currently unable follow people face robot 
research address algorithms considering sources information shape texture tracking people 
limitation arose fact approach works person keeps xed distance robot 
nd severe limitation collision avoid routines maintain speci distance person 
argued believe nding natural ways communication humans robots utmost importance eld service robotics 
exclusively addresses gestures input modality believe augment interface speech interface gestures speech combined instructing mobile robot 
gestures help clarify spoken commands particularly situations noise impedes ability communicate 
acknowledgments gratefully acknowledge various fruitful discussions members cmu robot learning lab 
research sponsored part darpa contract number contract number rome labs contract number brazilian foundation research support daimlerchrysler research berlin frieder 
views contained document author interpreted necessarily representing policies endorsements expressed implied darpa rome labs united states government daimlerchrysler 
asoh hara matsui 
socially embedded learning ce conversant robot 
proceedings ijcai 
ijcai 



gross 
neural networks gesture remote control mobile robot 
proc 
ieee world congress computational intelligence ijcnn pages anchorage 
ieee computer society press 


corradini gross 
user localisation visually human machine interaction 
proc 
ieee int 
conf 
face gesture recognition pages nara japan 
borenstein everett feng 
navigating mobile robots systems techniques 
peters wellesley ma 
burgard cremers fox lakemeyer schulz steiner thrun 
experiences interactive museum tour guide robot 
arti cial intelligence 
appear 
campbell becker azarbayejani bobick pentland 
invariant features gesture recognition 
technical report media laboratory perceptual computing section 
cox wilfong editors 
autonomous robot vehicles 
springer verlag berlin 
crowley 
vision man machine interaction 
robotics autonomous systems 
cui weng 
hand sign recognition intensity image sequences complex 
proceedings second international conference automatic face gesture recognition killington vermont 
moghaddam pentland 
active face tracking pose estimation interactive room 
proceedings ieee sixth international conference computer vision pages 
elfes 
occupancy grids probabilistic framework robot perception navigation 
phd thesis department electrical computer engineering carnegie mellon university 
endres 
field test navigation system autonomous cleaning supermarkets 
proc 
ieee international conference automation icra 
firby kahn swain 
architecture active vision action 
proceedings ijcai pages 
firby 
task networks controlling continous processes 
proceedings second international conference ai planning application 
franklin kahn swain firby 
happy patrons better 
huber kortenkamp 
stereo vision pursue moving agents mobile robot 
proceedings ieee international conference automation 
kahn swain firby 
gesture recognition perseus architecture 
proceedings ieee conference computer vision pattern recognition pages san francisco ca 
kahn 
perseus extensible vision system human machine interaction 
phd thesis university chicago 
king 
autonomous mobile robot navigation system 
proceedings spie conference mobile robots pages boston ma november 
volume 
kortenkamp huber bonasso 
recognizing interpreting gestures mobile robot 
proceedings aaai pages 
aaai press mit press 
kortenkamp murphy editors 
ai mobile robots case studies successful robot systems cambridge ma 
mit press 
appear 
maccormick blake 
probabilistic exclusion principle tracking multiple objects 
proceedings international conference computer vision kerkyra 
maes blumberg pentland 
alive system full body interaction animated autonomous agents 
acm multimedia systems 
thrun 
learning locate object space sequence camera images 
proceedings international conference machine learning icml 
moravec 
sensor fusion certainty grids mobile robots 
ai magazine pages summer 
pomerleau 
neural network perception mobile robot guidance 
kluwer academic publishers boston ma 
rabiner juang 
hidden markov models 
ieee assp magazine 
rowley baluja kanade 
neural network face detection 
ieee transactions pattern analysis machine intelligence 
rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland editors parallel distributed processing 
vol 
ii 
mit press 


springer verlag 
german 
simmons 
aaai robot competition exhibition 
ai magazine spring 
starner pentland 
real time american sign language recognition video hidden markov models 
proceedings international symposium computer vision 
ieee computer society press 
thrun burgard fox hofmann 
map learning high speed navigation rhino 
kortenkamp bonasso murphy editors ai mobile robots case studies successful robot systems 
mit press cambridge ma 
thrun burgard cremers dellaert fox rosenberg roy schulte schulz 
minerva second generation mobile tour guide robot 
proceedings ieee international conference robotics automation icra 
thrun 
learning maps indoor mobile robot navigation 
arti cial intelligence 
appear 
torrance 
natural communication robots 
master thesis mit department electrical engineering computer science cambridge ma january 
triesch von der malsburg 
robotic gesture recognition 
proceedings bielefeld gesture workshop gw bielefeld germany 
springer lecture notes arti cial intelligence 
triesch von der malsburg 
gesture interface human robot interaction 
proc 
ieee int 
conf 
face gesture recognition nara japan 
waibel 
lee editors 
readings speech recognition 
morgan kaufmann publishers san mateo ca 
thrun romero 
template recognition pose motion gestures mobile robot 
proceedings fifteenth national conference onarti cial intelligence 
wilson bobick 
learning visual behavior gesture analysis 
technical report media laboratory perceptual computing section 
wilson bobick 
con guration states representation recognition gestures 
technical report media laboratory perceptual computing section 
wilson bobick cassell 
recovering temporal structure natural gesture 
technical report media laboratory perceptual computing section 
wong kortenkamp 
mobile robot recognizes people 
proceedings ieee international conference arti cial intelligence 
wren azarbayejani darrell pentland 
nder real time tracking human body 
ieee transactions pattern analysis machine learning 
styles 
color science concepts methods quantitative data formulae 
john wiley sons 
yang waibel 
tracking human faces real time 
technical report cmu cs school computer science carnegie mellon university 

