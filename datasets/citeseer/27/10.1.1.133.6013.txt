short version appears supercomputing serial algorithms described implemented metis unstructured graph partitioning sparse matrix ordering system available www url www cs umn edu karypis metis metis html parallel multilevel way partitioning scheme graphs george karypis vipin kumar university minnesota department computer science minneapolis mn technical report cs umn edu updated october am parallel formulation multilevel way graph partitioning algorithm 
multilevel way partitioning algorithm reduces size graph collapsing vertices edges coarsening phase finds way partitioning smaller graph constructs way partitioning original graph projecting refining partition successively finer graphs uncoarsening phase 
key innovative feature parallel formulation utilizes graph coloring effectively parallelize coarsening refinement uncoarsening phase 
algorithm able achieve high degree concurrency maintaining high quality partitions produced serial algorithm 
test scheme large number graphs finite element methods transportation domains 
graphs vertices parallel formulation produces high quality way partitions processors little seconds cray 
parallel algorithm feasible perform frequent dynamic graph partition adaptive computations compromising quality 
keywords parallel graph partitioning multilevel partitioning methods spectral partitioning methods kernighan lin heuristic parallel sparse matrix algorithms 
supported nsf ccr army research office contract da daah army high performance computing research center auspices department army army research laboratory cooperative daah contract number daah content necessarily reflect position policy government official endorsement inferred 
access computing facilities provided institute cray research pittsburgh supercomputing center 
related papers available www url www cs umn edu karypis graph partitioning important problem extensive applications areas including scientific computing vlsi design geographical information systems operation research task scheduling 
problem partition vertices graph roughly equal partitions number edges connecting vertices different partitions minimized 
example solution sparse system linear equations ax iterative methods parallel computer gives rise graph partitioning problem 
key step iteration methods multiplication sparse matrix dense vector 
partition graph corresponding matrix significantly reduce amount communication parallel sparse matrix vector multiplication 
graph partitioning problem np complete 
algorithms developed find reasonably partition 
number researchers investigated class algorithms multilevel graph partitioning moderate computational complexity :10.1.1.106.4101
schemes original graph successively coarsened small number vertices partition coarsened graph computed initial partition successively refined kernighan lin type heuristic projected back original graph 
multilevel schemes provide excellent partitions wide variety graphs 
schemes provide significantly better partitions provided spectral partitioning techniques generally order magnitude faster state art implementation spectral techniques 
despite small run time multilevel schemes important develop highly parallel formulations schemes reasons discussed section 
developing parallel graph partitioning algorithms received lot attention due extensive applications areas 
concentrated algorithms geometric graph partitioning algorithms high computational requirements spectral bisection 
geometric graph partitioning algorithms tend inherently parallel produce significantly worse partitions compared multilevel algorithms 
due high computational complexity underlying serial algorithm parallel spectral bisection algorithms running processors tend slower multilevel graph partitioning algorithm running single processor 
development formulations multilevel graph partitioning schemes quite challenging 
coarsening requires nodes connected edges merged 
graph distributed randomly processors parallel coarsening schemes require lot communication 
kernighan lin refinement heuristic variant uncoarsening phase appear serial nature previous attempts parallelize mixed success 
parallel formulation multilevel way partitioning algorithm 
formulation generally applicable multilevel graph partitioning algorithm coarsening graph refines partitions uncoarsening phase :10.1.1.106.4101
key feature parallel formulation utilizes graph coloring successfully parallelize coarsening refinement phases 
algorithm able achieve high degree concurrency maintains high quality partitions produced serial multilevel partitioning algorithm 
parallel refinement algorithm conjunction parallel graph partitioning algorithm requires way refinement improve quality 
test scheme large number graphs finite element methods transportation domains 
parallel formulation cray produces high quality way partitions processors small amount time 
graphs vertices partitioned second graphs vertices require little seconds 
furthermore quality produced partitions comparable edge cuts produced serial multilevel way algorithm significantly better edge cuts smaller produced multilevel spectral bisection algorithm 
remainder organized follows 
section briefly describes serial multilevel way partitioning algorithm 
section discusses importance developing parallel graph partitioning algorithms 
section details parallel formulation multilevel way partitioning algorithm 
section provides theoretical performance scalability analysis 
section presents experimental evaluation parallel algorithm compares performance serial algorithm 
multilevel way graph partitioning way graph partitioning algorithm multilevel paradigm complexity linear number vertices graph 
basic structure multilevel algorithm illustrated 
graph coarsened small number vertices way partition smaller graph computed multilevel recursive bisection partition projected back original graph finer graph periodically refining partition :10.1.1.133.6013:10.1.1.106.4101
finer graph degrees freedom refinements improve quality partitions 
experiments show algorithm produces partitions comparable better quality produced multilevel recursive bisection algorithm significantly better produced state art multilevel spectral bisection algorithm :10.1.1.133.6013:10.1.1.106.4101
furthermore way partitioning algorithm times faster multilevel recursive bisection times faster multilevel spectral bisection 
run time way partitioning algorithm comparable run time geometric recursive bisection algorithms produces partitions generally better :10.1.1.133.6013:10.1.1.106.4101
note geometric methods applicable coordinate information graph available 
gg coarsening phase uncoarsening phase initial partitioning phase multilevel way partitioning various phases multilevel way partitioning algorithm 
coarsening phase size successively decreased initial partitioning phase way partition smaller graph computed uncoarsening phase partitioning successively refined projected larger graphs 
way graph partitioning problem defined follows graph partition subsets 
vk vi vj vi si vi number edges incident vertices belong different subsets minimized 
way partitioning commonly represented partition vector length vertex integer indicating partition vertex belongs 
partitioning number edges incident vertices belong different subsets called edge cut partitioning 
consider weighted graph weights vertices edges 
multilevel way partitioning algorithm works follows coarsening phase graph transformed sequence smaller graphs 
gm vm 
partitioning phase way partitioning pm graph gm vm em computed partitions vm partitions containing roughly vertices 
uncoarsening phase partitioning pm gm projected back going intermediate partitioning pm pm 

rest section briefly describe various phases multilevel algorithm 
reader refer details 
coarsening phase coarsening phase sequence smaller graphs gi vi ei constructed original graph vi vi 
graph gi constructed gi finding maximal matching mi ei gi collapsing vertices incident edge matching 
process vertices collapsed matching graph set edges incident vertex 
vertices incident edge matching simply copied gi 
vertices vi collapsed form vertex vi weight vertex set equal sum weights vertices edges incident set equal union edges incident minus edge 
pair edges adjacent single edge created weight set equal sum weights edges 
successive coarsening levels weight vertices edges increases 
process coarsening illustrated 
vertex edge unit weight 
shows coarsened graph results contraction shaded vertices 
numbers vertices edges show resulting weights 
maximal matchings computed different ways 
method compute matching greatly affects quality partition time required uncoarsening phase 
matching scheme called heavy edge matching hem computes matching mi weight edges mi high 
heavy edge matching computed randomized algorithm follows 
vertices visited random order 
randomly matching vertex adjacent unmatched vertices hem matches unmatched vertex connected heavier edge 
result hem scheme quickly reduces sum weights edges coarser 
coarsening phase ends coarsest graph gm small number vertices 
partitioning phase second phase multilevel way partition algorithm compute way partition coarse graph gm vm em partition contains roughly vertex weight original graph 
coarsening weights vertices edges coarser graph set reflect weights vertices edges process finding maximal matching contracting graph obtain level coarser graph 
note thatthe vertices edges coarse graph weights reflect number vertices edges collapsed 
finer graph gm contains sufficient information intelligently enforce balanced partition minimum requirements 
partitioning algorithm way partition gm computed multilevel recursive bisection algorithm :10.1.1.133.6013:10.1.1.106.4101
experiments shown produces initial partitions relatively small amount time 
uncoarsening phase uncoarsening phase partitioning coarser graph gm projected back original graph going graphs gm gm 

vertex vi contains distinct subset vertices vi projection partition gi gi constructed simply assigning vertices partition gi vertex belongs gi 
partition gi local minima projected partition gi may 
gi finer degrees freedom improve partition decrease edge cut 
may possible improve projected partition local refinement heuristics 
reason projecting partition partition refinement algorithm 
basic purpose partition refinement algorithm select vertices moved partition resulting partition smaller edge cut remains balanced partition weight 
multilevel way partitioning algorithm simplified version kernighan lin algorithm extended provide way partition refinement 
algorithm called greedy refinement gr 
complexity largely independent number partitions refined 
key gr refinement algorithm concept decrease edge cut achieved moving vertex partition gain 
consider graph gi vi ei 
vertex vi define neighborhood union partitions vertices adjacent belong 
refinement move partitions 
vertex compute gains moving neighbor partitions 
particular compute external degree associated sum weights edges belongs part 
compute internal degree sum weights edges belongs partition definitions gain moving vertex partition 
gr algorithm consists number iterations iteration vertices checked random order see moved 
vertex 
boundary vertex empty moved partition leads largest reduction edge cut partition largest positive gain subject partition weight constraints 
weight constraints ensure partitions roughly weight 
movement achieve reduction edge cut moved partition improves partition weight balance leads increase edge cut 
moving vertex algorithm updates internal external degrees vertices adjacent reflect change partition 
gr algorithm converges small number iterations iterations 
gr algorithm able enforce partition balance constraints explicit balancing phase moves vertices partitions movement leads increase edge cut 
need parallel graph partitioning multilevel partitioning algorithms produce high quality partitions small amount time ability perform partitioning parallel important reasons 
amount memory serial computers allow partitioning graphs corresponding large problems solved massively parallel computers workstation clusters 
performing graph partitioning parallel algorithm take advantage significantly higher amount memory available parallel computers 
context large scale finite element simulations adaptive grid computations dynamically adjust discretization physical domain 
dynamic adjustments grid lead load imbalances require repartitioning graph efficient parallel computation 
able compute partitions fast parallel essential reducing run time type applications 
problems computational effort grid cell changes time 
example codes particles grid large temporal spatial variations particle density introduce substantial load imbalance 
dynamic repartition corresponding vertex weighted graph crucial balance computation processors 
furthermore development highly parallel formulations sparse cholesky factorization algorithms numeric factorization parallel computers take time step computing fill reducing ordering serial computer making new bottleneck 
example processor cray matrices factored seconds parallel sparse cholesky factorization algorithm serial graph partitioning needed computing fill reducing ordering takes orders magnitude time 
parallel formulation developing highly parallel formulation multilevel way partitioning algorithm particularly difficult task computing maximal matching coarsening phase task refining partition uncoarsening phase appear quite serial nature 
phases multilevel way partitioning algorithm described section coarsening uncoarsening phases require bulk computation 
critical efficient parallel formulation multilevel way partitioning algorithm successfully parallelize phases 
recall coarsening phase section matching edges computed contract graph 
possible way computing matching parallel processor compute matchings vertices stores locally local matchings contract graph 
pair matched vertices resides processor approach requires communication contraction step 
approach works long processor stores relatively connected portions entire graph 
particular graph distributed processors partitioned fashioned approach worked extremely 
realistic assumption cases finding partition graph problem trying solve multilevel way partitioner 
approach local matchings reasonably number processors small relative size graph average degree graph relatively high 
reason random partition graph small number processors leave connected components processor 
earlier parallelizing multilevel recursive bisection algorithm dimensional distribution graph required vertices graph partitioned processors 
graph distribution allowed moderate amount coarsening purely local matchings 
alternate approach allow vertices belonging different processors matched 
compared local matching schemes type matching provides matchings better quality ability contract graph depend number processors existence pre partition 
global matching significantly complicates parallel formulation requires distributed matching algorithm 
example vertices located different processors vertex matched vertex may matched different vertex furthermore processor may match vertex vertex 
correct usable distributed matching algorithm resolve conflicts efficiently 
note pairs vertices contracted reside different processors global communication required contracted graph constructed 
uncoarsening phase way partition iteratively refined projected successively finer graphs 
serial algorithm scans vertices moves vertices lead reduction edge cut 
parallel formulation algorithm need move group vertices time order speedup refinement process 
group vertices needs carefully selected vertex group contributes reduction edge cut 
example possible processor pi decides move set vertices si processor pj reduce edge cut vertices si connected set vertices located processor pj order edge cut improve moving vertices si vertices move 
pi selects si processor pj may decide move vertices processor 
consequently sets vertices moved pi pj edge cut may improve may get worse 
clearly group selection algorithm eliminate type unnecessary vertex movements 
possible way performing way refinement pairwise refine partitions 
assuming partitions having common boundaries way refinement performing way refinements 
total processors way refinements go time 
pairs refined concurrently determined matching processor graph 
parallel refinement algorithm restricts type vertex movements performed step 
lacks global view available serial refinement algorithm vertex free move partition leads maximum reduction edge cut 
developed highly parallel formulations phases multilevel way graph partitioning algorithm 
formulation utilizes graph coloring eliminate conflicts computation global matching coarsening phase eliminate unnecessary vertex movement parallel variation kernighan lin refinement uncoarsening phases 
exploit task level parallelism initial graph partitioning algorithm reduce small run time phase 
number processors compute way partition graph 
initially distributed processors dimensional distribution processor receives vertices adjacency lists 
algorithm partition number assigned vertex graph 
sections describe parallel formulations phases multilevel way partitioning algorithm described section 
computing coloring graph coloring graph assigns colors vertices adjacent vertices different color 
find coloring number distinct colors small 
parallel graph coloring algorithm con sists number iterations 
iteration maximal independent set vertices selected variation luby algorithm 
vertices independent set assigned color 
iteration begins vertices removed graph smaller graph input graph iteration 
maximal independent set set vertices computed incremental fashion luby algorithm follows 
random number assigned vertex vertex random number smaller random numbers adjacent vertices included process repeated vertices adjacent vertices augmented similarly 
incremental augmentation ends vertices inserted shown iteration luby algorithm requires total log augmentation steps find maximal independent set set implementation luby algorithm perform single augmentation step compute independent set iteration 
independent set computed maximal 
leads increase number required colors significantly reduces run time required color graph 
furthermore modification significantly impact run times coarsening uncoarsening phases number colors increases moderately 
luby algorithm implemented quite efficiently shared memory parallel computer vertex processor easily determine random value assigned smaller random values assigned adjacent vertices 
distributed memory parallel computer vertex random values associated adjacent vertices stored processor needs explicitly communicated 
implementation luby algorithm prior performing coloring parallel perform communication setup phase appropriate data structures created facilitate exchange random numbers 
particular pre determine vertices located processor boundary vertex connected vertices residing different processors internal vertices vertices connected vertices processors 
data structures phases parallel multilevel graph partitioning algorithm 
coarsening phase recall section coarsening phase sequence 
gm successively smaller graphs constructed 
graph gi derived gi finding maximal matching mi gi collapsing vertices incident edges mi matching mi maximal independent set edges luby parallel algorithm dual graph gi compute global matching parallel 
computing matching algorithm quite expensive dual graph usually significantly vertices gi somewhat denser 
reason matching algorithm coloring graph 
coloring algorithm happens essential parallelizing partitioning refinement phase 
parallel matching algorithm extension serial algorithm utilizes graph coloring structure sequence computations 
consider graph gi vi ei colored parallel formulation luby algorithm match variable associated vertex graph initially set 
computation variable match vertex stores vertex matched 
matched match simplify presentation describe algorithm assuming target parallel computer shared memory architecture show algorithm implemented distributed memory machine 
matching mi constructed iterative fashion 
cth iteration vertices color matched match select unmatched neighbors heavy edge heuristic modify match variable selected vertex setting vertex number 
vertex color edge selected color vertex selecting partner vertex iteration 
possibility vertex color may select 
vertices perform selections time way preventing 
handled follows 
vertices color select unmatched neighbor synchronize 
vertices color just selected neighbor read match variable selected vertex 
value read equal vertex number matching successful set match variable equal selected vertex matching fails vertex remains unmatched 
note vertex want match vertex writes match variable selected vertex succeed determines matching survives 
coloring restrict vertices select partner vertices iteration number conflicts significantly reduced 
note vertex color may fail matching accepted due conflicts vertex matched subsequent iteration corresponding different color 
algorithm implemented quite easily distributed memory parallel computer follows 
writes match variables gathered sent corresponding processors single message 
processor receives multiple write requests vertex corresponds heavier edge selected 
ties broken arbitrarily 
similarly reads match variables gathered processors store corresponding variables sent single message requesting processors 
furthermore read operation processors match variables determine ones storing collapsed vertex gi 
done uniformly distributed random variable 
vertex kept away probability 
experiments shown simple heuristic leads load balance 
matching mi computed processor knows vertices associated adjacency lists needs send needs receive 
processor sends receives subgraphs forms level coarser graph merging adjacency lists matched vertices 
coarsening process ends graph vertices 
partitioning phase partitioning phase way partition graph computed recursive bisection algorithm 
coarsest graph vertices step performed serially log time significantly affecting performance entire algorithm 
algorithm parallelize phase parallel algorithm recursive nature algorithm 
done follows various pieces coarse graph gathered processors broadcast operation 
point processors perform recursive bisection algorithm nested dissection greedy partition refinement 
illustrated processor explores single path recursive bisection tree 
processor stores vertices correspond partition way partition 
note initial broadcast operation algorithm proceeds communication 
uncoarsening phase uncoarsening phase partition projected coarse graph level finer graph refined greedy refinement algorithm section 
recall single phase refinement algorithm vertices randomly traversed vertices lead decrease edge cut switch partitions 
vertex movement external degrees adjacent vertices updated reflect new partition 
parallel formulation greedy refinement retain spirit serial algorithm change order vertices traversed determine moved different partitions 
particular single phase refinement algorithm broken sub phases number colors graph refined 
th phase vertices color considered movement subset vertices lead reduction edge cut improve balance increasing edge cut moved 
vertices color form independent set total reduction edge cut achieved moving vertices processor processor processor processor performing initial way partitioning parallel 
processor explores single path root recursive bisection tree 
time equal sum edge cut reductions achieved moving vertices 
performing group movement external degrees vertices adjacent group updated color considered 
parallel refinement physically move vertices change partitions 
processor initially stores vertices single part vertices move partitions refinement move corresponding processors 
context multilevel graph partitioning approach requires significant communication 
vertex coarse graph gi move need send adjacency list adjacency lists vertices collapsed higher level finer graphs gi gi 

parallel refinement algorithm solve problem follows 
vertices move processor processor partition number associated vertex changes 
vertices initially distributed random order processor stores vertices belong partitions 
ensures refinement processor boundary vertices needs moved leading generally load balanced computation 
furthermore leads simpler implementation parallel refinement algorithm vertices adjacency lists moved 
course vertices moved proper location partitioning algorithm single personalized communication 
balance conditions maintained follows 
initially processor knows weights partitions 
refinement sub phase processor enforces balance constraints partition weights 
vertex decides moves locally updates weights 
sub phase global partition weights recomputed processor knows exact weights 
scheme exact serial balance constraints experiments shown hybrid local global partition weight constraints able produce balanced partitions 
parallel refinement algorithm highly concurrent number colors small finite element meshes number vertices large 
furthermore serial parallel refinement algorithms similar spirit exhibit similar partition refinement capabilities 
furthermore coloring parallel refinement algorithm algorithm uses kernighan lin type local refinement 
performance scalability analysis parallel formulation multilevel way partitioning algorithm described section different parallel algorithms coloring matching contraction initial partitioning refinement 
algo rithms coloring matching contraction refinement similar requirements terms computation communication 
analysis assume number colors needed fixed constant irrespective size graph 
reasonable assumption graphs arise finite element applications 
amount computation performed algorithms proportional size graph stored locally processor 
furthermore size successively coarser graphs roughly decreases factor computation performed original graph dominates computation performed subsequent log coarser graphs 
amount computation performed np 
amount communication performed algorithms depends number interface vertices 
example coloring processor needs know random numbers vertices adjacent locally stored vertices 
similarly refinement time vertex moved adjacent vertices need notified update external degrees 
processor stores vertices nd edges average degree graph 
number interface vertices 
vertices initially distributed randomly interface vertices equally distributed processors 
processor needs exchange data vertices processor 
alternatively processor needs send information locally stored vertices processor 
accomplished personalized communication operation complexity 
communication complexity log coarsening levels np log size graph successively halved 
note equation valid data communicated processor roughly equally distributed 
graph randomly distributed reasonable assumption somewhat expensive generalized personalized communication needed 
furthermore successive coarser graphs graphs remains randomly distributed matching decisions regarding contracted vertex reside done randomly 
addition communication interface vertices matching refinement perform additional communication 
matching prefix sum performed determine numbering vertices coarser graph 
complexity operation log coarsening levels log log 
refinement reduction vector performed compute weights partitions 
size vector equal reduction done time 
complexity log coarsening levels log 
algorithms require global synchronizations complexity log log 
complexity communication overheads subsumed complexity sending information interface vertices equation 
initial partitioning phase graph size partitioned partitions recursive bisection 
described section graph gathered processor broadcast operation complexity 
processor performs recursive bisection keeps bisections 
computational complexity initial partition 
equations parallel run time multilevel partitioning algorithm np log 
run time parallel graph partitioning algorithm bounded amount time required permute graph partition vector 
assuming graph randomly distributed processors permutation equivalent personalized communication original graph 
run time communication operation 
run time parallel graph partitioning algorithm slightly higher factor log second term equation absolute lower bound 
sequential complexity serial algorithm isoefficiency function algorithm log 
experimental results evaluated performance parallel multilevel way graph partitioning algorithm wide range graphs arising different application domains 
characteristics graphs described table 
implemented parallel multilevel algorithm processor cray parallel computer 
processor mhz dec alpha ev 
processors interconnected dimensional torus network peak unidirectional bandwidth bytes second small latency 
message passing library communication 
experimental setup obtained peak bandwidth mbytes effective startup time microseconds 
processor mbytes memory larger graphs partitioned single processor 
reason compare parallel run time run time serial multilevel way algorithm running sgi challenge gbytes memory mhz mips 
peak integer performance lower alpha due significantly higher amount secondary cache available sgi machine mbyte sgi versus mbytes processors code running single processor slower running sgi 
nature multilevel algorithm discussed randomized performed experiments fixed seed 
graph name 
vertices 
edges description finite element mesh finite element finite element mesh brack finite element finite element mesh finite element highway network dual finite element dual finite element mesh ocean finite element finite element mesh wave finite element mesh table various graphs evaluating parallel multilevel way graph partitioning algorithm 
note algorithm computing initial partition graph parallel multilevel algorithm see section different multilevel recursive bisection serial algorithm 
multilevel algorithm produces significantly better initial partitions nested dissection requires time 
consequently initial partitioning step may bottleneck large number processors particularly smaller graphs 
due way refinement performed uncoarsening phase final partitions slightly worse produced serial way algorithm uses multilevel recursive bisection algorithm computing initial partitions 
partition quality table shows quality partitions produced parallel way algorithm amount time took produce partitions cray problems table 
partitions partitions shown produced processors respectively 
table shows way way way name time time time time auto brack map ocean rotor wave table performance parallel multilevel way partitioning algorithm cray 
graph performance way partitions processors respectively 
times seconds 
way way way name time time time time auto brack map ocean rotor wave table performance serial multilevel way partitioning algorithm 
graph performance shown way partitions 
times seconds sgi challenge 
quality partitions amount time required serial algorithm running sgi problems 
quality partitions produced parallel algorithm relative produced serial way partitioning algorithm graphically shown 
see edge cut produced parallel algorithm quite close produced serial algorithm 
graphs edge cut parallel algorithm worse serial algorithm factor graphs parallel algorithm somewhat better 
coarsening uncoarsening phases parallel algorithm similar sections reason small deviation compared serial algorithm traced back nested dissection initial partition phase 
quality differences eliminated multilevel bisection initial partitioning phase 
quality parallel partitioning algorithm relative widely multilevel spectral bisection msb shown 
msb partitions produced state art msb algorithm implemented chaco graph partitioning package 
see quality parallel multilevel way partitioning algorithm usually better msb graphs better 
parallel runtime table see run time parallel algorithm small 
graphs parallel algorithm requires second produce way partition processors 
larger graphs auto half vertices vertices requires seconds 
table analytically shows amount time required different phases parallel graph partitioning auto brack map ocean rotor wave quality parallel versus serial multilevel way partition parts parts parts parts baseline quality partitions produced parallel relative serial multilevel way partitioning algorithm 
ratio edge cut parallel serial algorithm plotted way partitions 
bars baseline indicate parallel algorithm produces partitions smaller edge cut serial algorithm 
auto brack map ocean rotor wave quality parallel multilevel way partition versus serial multilevel spectral bisection parts parts parts parts baseline quality partitions produced parallel multilevel way partitioning algorithm relative multilevel msb 
graph ratio edge cut parallel serial algorithm plotted way partitions 
bars baseline indicate parallel algorithm produces partitions smaller edge cut spectral bisection algorithm 
auto phase name pes pes pes pes pes pes communication setup graph coloring computing matching graph contraction initial partition way refinement total runtime table amount time seconds required different phases parallel partitioning algorithm graphs processors 
algorithm graphs experimental testbed 
note communication setup phase processors determine interface vertices need send receive setup appropriate data structures communication 
table see number processors increase amount time required phase decreases 
exception initial partitioning phase time increases 
size coarsest graph number partitions increases number processors 
amount time required phase small compared run time entire partitioning algorithm 
speedup achieved parallel algorithm cray serial algorithm running sgi shown 
smaller graphs parallel algorithm achieves speedup range processors size graphs increases speedup improves range 
discussed earlier due architectural differences cray sgi challenge run time multilevel partitioning code running single processor sgi somewhat smaller running single processor cray 
actual speedups respect serial algorithm running single processor cray higher factor 
furthermore discussed section parallel algorithm incurs additional computational overhead computing graph coloring coarsening phase overhead serial algorithm 
addition coloring overhead parallel algorithm requires communication setup phase exchange information interface vertices 
serial algorithm overhead 
instance auto table see run time seconds processors overheads take seconds total run time 
note map ocean overheads smaller graphs smaller average degrees achieve better speedup graphs similar number vertices 
experimental scalability table see graph run time parallel algorithm decreases number processors partitions increases 
table see run times serial multilevel way partitioning algorithm increases increases 
asymptotic complexity serial algorithm increase run time due increase number interface vertices exist number partitions increases 
refining interface vertices leads increase asymptotic complexity algorithm 
evidence increased computational requirements way refinement seen table 
table see number processors increases amount time required refinement decreases slower rate time required coloring matching contraction 
instance going processors run time matching decreased run time refinement decreased 
modest increase computational requirements hard draw experimental scalability parallel algorithm raw parallel run times table 
serial run times know computational requirements increase increases 
reason increase serial run time compute scaled relative efficiencies shown graphs 
auto brack map ocean rotor wave speedup parallel algorithm cray versus serial algorithm sgi pes pes pes pes speedup achieved parallel partitioning algorithm running cray serial algorithm running 
graph speedup processors shown 
efficiencies relative processor run times scaled reflect increase computation 
example know table auto going way way partition run time increases seconds seconds increase 
compute relative speedup parallel algorithm processors relative processors multiply run time processors seconds increase computational requirements divide run time processors seconds 
relative speedup yielding relative efficiency 
see graph number processors increases efficiency decreases 
expected non trivial parallel algorithm communication overhead increases processors increases 
similarly size graphs increases achieved efficiency improves communication overhead increases slower amount computation performed 
analysis section shown isoefficiency function parallel algorithm log 
order maintain fixed efficiency graph size increase log 
example double number processors need increase size graph factor little order achieve efficiency 
order experimentally evaluate scalability algorithm speedup obtained parallel algorithm serial algorithm running sgi 
differences serial parallel algorithm discussed earlier additional coloring parallel algorithm important compare efficiencies achieved graphs similar structure lead similar coloring overheads 
graphs experimental testbed table pairs graphs auto require number colors appropriate relative sizes 
auto times larger times larger 
see speedup achieved auto processors comparable speedup achieved processors respectively 
experiments confirm isoefficiency function parallel graph partitioning algorithm log 
effects initial graph distribution experiments shown table performed initially distributing graphs processors block distribution 
graphs read file consecutive ver efficiency processors scaled relative efficiency parallel multilevel way partition algorithm auto brack map rotor scaled efficiencies achieved parallel algorithm graphs 
efficiencies relative processor runs run times scaled reflect increase amount performed number partitions increases 
assigned processor 
refer distribution 
ordering somewhat different random distribution assumed description analysis parallel algorithm sections chosen simplicity 
study performance parallel algorithm different initial graph distribution schemes performed experiments random pre partitioned distributions 
cases permutation applied graph distributing processors 
case random distribution permutation computed randomly case pre partitioned distribution permutation computed serial way partition graph 
auto pes pes pes pes phase name random pre partitioned random pre partitioned random pre partitioned random pre setup graph coloring computing matching graph contraction initial partition way refinement total runtime table amount time seconds required different phases parallel partitioning algorithm different distributions processors 
table shows run time different distribution schemes larger graphs experimental testbed 
comparing run times shown table see little difference random distributions 
run time random distribution higher expected distributions result initial partitions cut edges 
run time significantly reduced pre partitioned distribution 
example case processors run time pre partitioned distribution half achieved random distributions 
reduction run time due reasons reduced communication requirements better cache utilization 
pre partitioned graph distribution number edges get cut result initial distribution significantly reduced auto 
consequently distributed graph significantly fewer interface vertices 
graph coloring matching contraction partition refinement algorithms communication significant fraction run time 
reduction run time due reduced communication required pre partitioned graph 
reduced communication requirements clearly seen amount time required communication setup phase especially processors complexity highly depends number interface vertices 
reducing communication overheads better data locality produced pre partitioned distribution significantly improves cache utilization 
particularly important machine cray small amount primary cache kbytes secondary cache 
improved cache reuse primary reason improvement achieved coloring matching contraction algorithms 
primary significance cache seen looking time required way refinement 
case improvements dramatic processors 
way refinement vertices get moved limited cache reuse 
related raghavan presents parallel formulation nested dissection ordering algorithm multilevel graph partitioning 
raghavan parallel algorithm uses dimensional partitioning graphs construct successive coarser graphs computing matchings different pairs processors coarsening level 
matching pairs vertices located different processors global maximal matchings processors coarsening step 
difference scheme algorithm projects bisection coarser graph directly original graph refinement 
algorithm obtains speedup range processor cm 
due limited global matching absence partition refinement orderings produced algorithm worse produced state art multilevel ordering algorithms 
speedup obtained algorithm similar obtained algorithm underlying communication coarsening phase similar different phases algorithm 
barnard developed parallel formulation multilevel spectral algorithm 
algorithm uses dimensional mapping graph processors uses parallel formulation luby algorithm compute maximal independent set vertices construct level coarser graph 
note coarsening scheme barnard algorithm multilevel graph partitioning algorithm 
reason coarsened graph multilevel spectral algorithms information enforce balance constraints partition quality 
coarsening scheme uses maximal independent set vertices maximal independent set edges multilevel graph partitioning algorithms luby algorithm directly original graph 
table shows parallel performance multilevel spectral bisection multilevel way partition problems experimental testbed 
table see parallel algorithm produces partitions quality significantly better produced parallel multilevel spectral bisection algorithm 
particular processors algorithm cuts times fewer edges spectral algorithm 
furthermore algorithm times faster spectral algorithm consistent serial computational requirements algorithms 
difference edge cuts parallel formulations similar serial counterparts 
parallel formulations multilevel spectral bisection parallel multilevel spectral bisection barnard available cray research 
multilevel way partitioning algorithms similar communication overheads proportional number interface vertices relative run time requirements change parallel computers scale similarly 
pes pes pes pes parallel algorithm edge cut time edge cut time edge cut time edge cut spectral bisection multilevel way partitioning table performance parallel multilevel spectral bisection parallel way partitioning algorithms processor cray 
parallel formulation inertial algorithm partitioning 
algorithm computes way partition inertial recursive bisection naturally parallel pairwise partition refinement kernighan lin heuristic described section 
experiments show quality partitions produced parallel inertial algorithm worse compared serial implementation inertial algorithm uses sequential kl refinement 
decrease partition quality due fact pairwise partition refinement effective coloring global refinement scheme algorithm 
karypis kumar parallel formulation serial multilevel recursive bisection algorithm graph partitioning sparse matrix ordering :10.1.1.133.6013:10.1.1.106.4101
algorithm uses dimensional distribution graph processors computes local heavy edge matching diagonal processors discussed section 
size matchings produced successive coarsening levels small graph successively folded smaller halves processor grid 
formulation minimizes communication overhead coarsening phase 
local matching produces sufficient coarsening long average degree coarse graphs sufficiently large proportional square root number processors 
degree graphs small case finite element meshes duals local matching sufficiently reduce size graph folding required resulting poor speedup 
scheme appropriate graphs relatively high degree especially target parallel computer slow communication network cluster workstations 
variety problems parallel formulation multilevel way partitioning times faster processor cray quality better compared formulation 
difference run times parallel formulations primarily algorithm recursive bisection requiring log rounds coarsening algorithm way partitioning requiring coarsening phase 
edge cuts produced parallel scheme somewhat worse matching limited local vertices 
scalable highly parallel formulation multilevel way partitioning algorithm able produce partitions large unstructured graphs small amount time 
theoretical analysis section shows run time scalability algorithm factor log theoretical lower bound parallel graph partitioning algorithm 
context repartitioning adaptively refined graphs run time parallel multilevel way partitioning algorithm reduced 
context coloring matching phases modified utilize faster serial algorithms vertices internal domains assigned processor 
requiring perform distributed coloring distributed matching interface nodes various domains run time phases reduced 
knowledge algorithm provides highly parallel effective formulation way partitioning refinement algorithm 
partition refinement relatively simple variant kernighan lin type algorithms concurrency exposed coloring implement sophisticated algorithms 
example refinement algorithms able climb local minima performing moves decrease edge cut easily implemented techniques described 
performance achieved algorithm allows development efficient scalable parallel formulations diverse problems utilize operate unstructured graphs 
domain decomposition techniques extensively scientific computing completely parallelized removing computational bottleneck created serial domain decomposition prior parallel computation 
allows creation highly parallel preconditioners iterative methods domain decomposition incomplete factorizations 
furthermore adaptive finite element methods effectively parallelized mesh fly fast 
stephen barnard 
parallel multilevel recursive spectral bisection 
supercomputing 
stephen barnard horst simon 
parallel implementation multilevel recursive spectral bisection application adaptive unstructured meshes 
inproceedings seventh siam conference parallel processing scientific computing pages 
stephen barnard horst simon 
fast multilevel implementation recursive spectral bisection partitioning unstructured problems 
proceedingsof sixth siam conference parallel processing scientific computing pages 
bui jones 
heuristic reducing fill sparse matrix factorization 
th siam conf 
parallel processing scientific computing pages 
chung cheng yen wei 
improved way partitioning algorithm stable performance 
ieee transactions computer aided design december 
pedro steve bruce hendrickson robert leland 
parallel algorithms dynamically partitioning unstructured grids 
proceedings siam conference parallel processing scientific computing pages 
steger 
finding clusters vlsi circuits 
proceedings ieee international conference computer aided design pages 
george 
nested dissection regular finite element mesh 
siam journal numerical 
gilbert 
parallel graph partitioning algorithm message passing multiprocessor 
internation journal parallel programming 
gupta 
fast effective algorithms graph partitioning sparse matrix reordering 
technical report rc ibm watson yorktown heights ny july 
gupta george karypis vipin kumar 
highly scalable parallel algorithms sparse matrix factorization 
technical report department science university minnesota minneapolis mn 
appear ieee transactions parallel distributed computing 
available www url www cs umn edu karypis papers sparse cholesky ps 
gupta vipin kumar 
scalable parallel algorithm sparse matrix factorization 
technical report department computer science minnesota minneapolis mn 
shorter version appears supercomputing 
tr available users kumar sparse cholesky ps anonymous ftp site ftp cs umn edu 
lars hagen andrew kahng 
fast spectral methods ratio cut partitioning clustering 
proceedings ieee international conference computeraided design pages 
lars hagen andrew kahng :10.1.1.133.6013
new approach effective circuit clustering 
proceedings ieee international conference computer aided design pages 
heath raghavan 
cartesian parallel nested dissection algorithm 
siam journal matrix analysis applications 
bruce hendrickson robert leland 
multilevel algorithm partitioning graphs 
technical report sand sandia national laboratories 
bruce hendrickson robert leland 
chaco user guide version 
technical report sand sandia national laboratories 
bruce hendrickson edward rothberg 
improving runtime quality nested dissection ordering 
technical report cs sandia national 
johan mathur lennart johnsson thomas hughes 
finite element methods connection machine cm system 
technical report thinking machines 
karypis kumar 
analysis multilevel graph partitioning 
technical report tr department computer science university minnesota 
available www url www cs umn edu karypis papers analysis ps 
short version appears supercomputing 
karypis kumar :10.1.1.133.6013:10.1.1.106.4101
fast high quality multilevel scheme partitioning irregular graphs 
technical report tr department university minnesota 
available www url www cs umn edu karypis papers serial ps 
short version appears intl 
conf 
parallel processing 
karypis kumar 
multilevel way partitioning scheme irregular graphs 
technical report tr department computer science minnesota 
available www url www cs umn edu karypis papers ps 
karypis kumar 
parallel algorithms multilevel graph partitioning sparse matrix ordering 
technical report tr department university minnesota 
available www url www cs umn edu karypis papers parallel ps 
short version appears intl 
parallel processing symposium 
george karypis 
graph partitioning applications scientific computing 
phd thesis university minnesota minneapolis mn 
george karypis vipin kumar 
fast sparse cholesky factorization scalable parallel computers 
technical report department computer science minnesota minneapolis mn 
short version appears eighth symposium frontiers massively parallel computation 
available www url www cs umn edu karypis papers frontiers ps 
kernighan lin 
efficient heuristic procedure partitioning graphs 
bell system technical journal 
vipin kumar gupta george karypis 
parallel computing design analysis algorithms 
benjamin redwood city ca 
michael luby 
simple parallel algorithm maximal independent set problem 
siam journal computing 
gary miller shang hua teng thurston stephen vavasis 
automatic mesh partitioning 
george john gilbert 
liu editors sparse matrix computations graph theory issues algorithms 
ima workshop volume 
springer verlag new york ny 
gary miller shang hua teng stephen vavasis 
unified geometric approach graph separators 
proceedings st annual symposium computer science pages 

solving finite element equations concurrent computers 
editor american soc 
mech 
eng pages 
mansour choudhary fox 
graph contraction physical optimization methods quality cost tradeoff mapping data computers 
international conference supercomputing 
alex pothen horst simon kang pu liou 
partitioning sparse matrices eigenvectors graphs 
siam journal matrix analysis applications 
raghavan 
line plane separators 
technical report uiucdcs department computer science university illinois urbana il february 
raghavan 
parallel ordering edge contraction 
technical report cs department computer science university tennessee 
edward rothberg 
performance panel block approaches sparse cholesky factorization ipsc paragon multicomputers 
proceedings ofthe scalable high performance computing conference may 
saad 
iterative methods sparse linear systems 
pws publishing boston ma 
ravi shankar sanjay ranka 
random data accesses coarse grained parallel machine 
journal parallel distributed computing 
appear 

