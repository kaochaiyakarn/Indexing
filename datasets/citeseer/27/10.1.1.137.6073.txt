generalization principal component analysis exponential family michael collins sanjoy dasgupta robert schapire labs research park avenue florham park nj dasgupta schapire research att com principal component analysis pca commonly applied technique dimensionality reduction 
pca implicitly minimizes squared loss function may inappropriate data real valued binary valued data 
draws ideas exponential family generalized linear models bregman distances give generalization pca loss functions argue better suited data types 
describe algorithms minimizing loss functions give examples simulated data 
principal component analysis pca popular dimensionality reduction technique attempts find low dimensional subspace passing close set specifically pca find lower dimensional subspace minimizes sum squared distances data points projections points subspace turns equivalent choosing subspace maximizes sum squared lengths projections empirical variance projections data happens centered origin 
pca convenient interpretation known 
probabilistic interpretation point thought random draw un distribution known denotes unit gaussian mean purpose pca find set parameters maximizes likelihood data subject condition parameters lie low dimensional subspace 
words considered noise corrupted versions true points lie subspace goal find true points main assumption noise gaussian 
equivalence interpretation ones follows simply fact negative log likelihood model equal ignoring constants eq 

gaussian assumption may inappropriate instance data binary valued integer valued nonnegative 
fact gaussian canonical distributions exponential family distribution tailored real valued data 
poisson better suited integer data bernoulli binary data 
natural consider variants pca founded distributions place gaussian 
extend pca rest exponential family 
parameterized set distributions exponential family natural parameter distribution 
instance dimensional poisson distribution parameterized corresponding mean distribution data goal find parameters lie low dimensional subspace log likelihood maximized 
unified approach effortlessly permits hybrid dimensionality reduction schemes different types distributions different attributes data 
data binary attributes integer valued attributes coordinates corresponding parameters binomial distributions parameters poisson distributions 
simplicity presentation assume distributions type 
dimensionality reduction schemes non gaussian distributions substantially different pca 
instance pca parameters means gaussians lie space coincides data case general parameters lie linear subspace typically correspond nonlinear surface space data 
discrepancy interaction space parameters space data central preoccupation study exponential families generalized linear models glm bregman distances 
exposition inevitably woven intimately related subjects 
particular show way generalize pca exactly analogous manner regression generalized glm respect elucidated differs variants pca proposed lee seung hofmann :10.1.1.127.6264
show optimization problem derive solved quite naturally algorithm alternately minimizes components analysis coefficients algorithm reminiscent csisz dy alternating procedures 
case side minimization simple convex program interpreted projection respect suitable bregman distance program generally convex 
case gaussian distributions algorithm coincides exactly power method computing eigenvectors sense generalization oldest algorithms pca 
omitted lack space show procedure converges limit point computed coefficients stationary point loss function 
slight modification optimization criterion guarantees existence limit point 
comments notation vectors row vectors 
matrix denote th row th element exponential family glm bregman distances exponential family generalized linear models exponential family distributions conditional probability value parameter value takes form natural parameter distribution usually take value reals 
function ensures sum integral domain 
follows denote domain sum replaced integral continuous case defines density term depends usually ignored constant estimation 
main difference different members family form see concepts pca algorithms stem directly definition example normal distribution mean unit variance density usually written verified member exponential family common case bernoulli distribution case binary outcomes 
case probability usually written parameter member exponential family critical function derivative denote 
differentiating easily verified expectation normal distribution bernoulli case general case referred expectation parameter defines function natural parameter values expectation parameter values 
generalization pca analogous manner generalized linear models glm provide unified treatment regression exponential family generalizing squares regression loss functions appropriate members family 
regression set assumes training sample pairs vector attributes response variable 
parameters model vector dot product taken approximation squares regression optimal parameters set glm taken approximate expectation parameter tial model inverse link function 
natural choice canonical link derivative case natural parameters directly approximated log likelihood simply case normal distribution fixed variance follows easily maximum likelihood cri equivalent squares criterion 
interesting case logistic regression negative log likelihood parameters bregman distances exponential family differentiable strictly convex function defined closed convex set bregman distance associated defined shown general bregman distance ative equal zero arguments equal 
exponential family log likelihood directly related bregman normal bernoulli poisson table various functions interest members exponential family distance 
specifically define dual function shown fairly general conditions application identities implies negative log likelihood point expressed bregman distance words negative log likelihood written bregman distance plus term constant respect ignored 
table summarizes various functions interest examples exponential family 
find useful extend idea bregman distances divergences vec tors matrices 
vectors matrices overload notation 
notion bregman distance generalization pca extended vectors general manner simplicity restrict attention bregman distances pca problems particular form 
pca exponential family generalize pca members exponential family 
wish find close belong lower dimensional subspace parameter space 
approach find basis represent linear combination elements closest matrix th row matrix th row matrix elements matrix th row 
matrix natural parameter values define probability point discussion section consider loss function form constant term dropped 
loss function varies depending member exponential family taken simply changes form example matrix real values normal distribution appropriate data loss criterion usual squared loss pca 
bernoulli distribution define relationship log likelihood bregman distances see eq 
loss written allow applied vectors matrices pointwise manner 
data points point th data represented vector lower dimensional space coefficients define bregman vector projection generalized form pca considered search low dimensional basis matrix defines surface close points data define set points optimal value min sum projection distances note normal distribution bregman distance euclidean distance projection operation eq 
simple linear projection 
simplified normal case simply hyperplane basis summarize member exponential family implication convex function chosen regular pca generalized way loss function negative log likelihood matrix taken matrix natural parameter values 
constant 
derivative defines matrix expectation parameters function derived bregman distance derived loss sum bregman distances elements values pca thought search matrix defines surface close data points 
normal distribution simple case divergence euclidean distance 
projection operation linear operation hyperplane basis 
generic algorithms minimizing loss function describe generic algorithm minimization loss function 
concentrate simplest case just single component 
drop subscript method iterative initial random choice value denote values th iteration initial random choice 
propose iterative updates alternately minimized respective arguments time optimizing argument keeping fixed reminiscent csisz dy alternating procedures 
useful write minimization problems follows see optimization problems essentially identical glm regression problem simple single parameter optimized 
sub problems easily solved functions convex argument optimized large literature maximumlikelihood estimation glm directly applied problem 
updates take simple form mal distribution follows scalar value 
method equiv power method see jolliffe finding eigenvector largest eigenvalue best single component solution generic algorithm generalizes oldest algorithms solving regular pca problem 
loss convex arguments fixed general convex arguments 
difficult prove convergence global minimum 
normal distribution interesting special case respect power method known converge optimal solution spite non convex nature loss surface 
simple proof comes properties eigenvectors jolliffe 
explained analysis hessian stationary point global minimum positive semi definite 
stationary points saddle points local minima 
hessian generalized loss function complex remains open problem positive semidefinite stationary points global minimum 
open determine conditions generic algorithm converge global minimum 
preliminary numerical studies algorithm behaved respect 
limit point sequence stationary point 
possible sequence diverge optimum may infinity 
avoid degenerate choices modified loss small positive constant value range finite 
roughly equivalent adding conjugate prior finding maximum posteriori solution 
proved modified loss sequence remains bounded region limit point stationary point 
proofs omitted lack space 
various ways optimize loss function component 
give algorithm cycles components optimizing turn held fixed randomly set initialization set cycle components times optimize th component components fixed initialize convergence modified bregman projections include term representing contribution fixed components 
sub problems standard optimization problem regarding bregman distances terms form prior 
data pca exp data pca exp regular pca vs pca exponential distribution 
projecting dimensional space bernoulli pca 
left points projected dimensional curve 
right point added 
illustrative examples exponential distribution 
generalization pca behaves differently different members exponential family 
interesting example exponential distributions nonnegative reals 
dimensional data densities usually written mean 
uniform system notation index distribution single natural parameter basically write density link function case mean distribution 
suppose data want find best dimensional approximation vector coefficients approximation minimum loss 
alternating minimization procedure previous sec tion simple closed form case consisting iterative update rule shorthand denotes componentwise reciprocal notice similarity update rule power method pca recover coefficients points lie line origin 
normally expect points lie straight line case point form written lie direction reasonably ask lines exponential assumption differ gaussian assumption regular pca provided data nonnegative 
simple illustration conducted toy experiments data points 
points lay close line versions pca produced similar results 
second experiment points moved farther afield outliers larger effect regular pca exponential variant 
bernoulli distribution 
bernoulli distribution linear subspace space parameters typically nonlinear surface space data 
left points dimensional hypercube mapped pca dimensional curve 
curve passes points projections indicated 
notice curve symmetric center hypercube right point added causes approximating dimensional curve closer 
relationship previous lee seung hofmann describe probabilistic alternatives pca tailored data types gaussian :10.1.1.127.6264
contrast method approximate mean parameters underlying generation data points constraints matrices ensuring elements correct domain 
choosing approximate natural parameters method matrices usually need constrained rely link function give transformed matrix lies domain data points 
specifically lee seung loss function ignoring constant factors defining 
optimized constraint positive 
method probabilistic interpretation data point generated poisson distribution mean parameter poisson distribution method uses loss function constraints matrices algorithm hofmann uses loss function matrices constrained positive bishop tipping describe probabilistic variants gaussian case 
tipping discusses model similar case bernoulli family 

builds intuitions exponential families bregman distances obtained largely interactions manfred warmuth papers 
andreas buja helpful comments 
warmuth 
relative loss bounds line density estimation exponential family distributions 
machine learning 
csisz dy 
information geometry alternating minimization procedures 
statistics decisions supplement issue 
rgen forster manfred warmuth 
relative expected instantaneous loss bounds 
journal computer system sciences appear 
thomas hofmann 
probabilistic latent semantic indexing 
proceedings nd annual international acm sigir conference research development information retrieval 
jolliffe 
principal component analysis 
springer verlag 
lee seung 
learning parts objects nonnegative matrix factorization 
nature 
daniel lee sebastian seung :10.1.1.127.6264
algorithms non negative matrix factorization 
advances neural information processing systems 
mccullagh nelder 
generalized linear models 
crc press nd edition 
tipping bishop 
probabilistic principal component analysis 
journal royal statistical society series 
michael tipping 
probabilistic visualisation high dimensional binary data 
advances neural information processing systems pages 
