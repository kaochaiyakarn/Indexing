tutorial hidden markov models selected applications speech recognition lawrence rabiner fellow ieee initially introduced studied late early statistical methods markov source hidden markov modeling increasingly popular years 
strong reasons occurred 
models rich mathematical structure form theoretical basis wide range applications 
sec ond models applied properly practice important applications 
attempt care fully methodically review theoretical aspects type statistical modeling show applied selected problems machine recognition speech 
real world processes generally produce observable puts characterized signals 
signals nature quantized vectors codebook continuous nature speech samples temperature measurements music 
signal source stationary sta tistical properties vary time nonstationary signal properties vary time 
signals pure coming strictly single source corrupted signal sources noise transmission distortions reverberation problem fundamental interest characterizing real world signals terms signal models 
sev eral reasons interested applying signal models 
signal model provide basis description signal processing system process signal provide desired put 
example interested enhancing speech signal corrupted noise transmission distortion signal model design system opti mally remove noise undo transmission tion 
second reason signal models important potentially capable letting learn great deal signal source real world process produced signal having 
property especially important cost getting signals actual source high 
manuscript received january revised october 
author bell laboratories murray hill nj usa 
ieee log number 
proceedings ieee vol 
february ieee case signal model simulate source learn possible simulations 
important reason signal models important practice enable realize important practical systems prediction systems recognition systems identification sys tems efficient manner 
possible choices type signal model characterizing properties signal 
broadly types signal models class deterministic models class statistical models 
deterministic models generally exploit known specific properties signal signal sine wave sum exponentials cases specification signal model generally straight forward required estimate values parameters signal model amplitude fre quency phase sine wave amplitudes rates expo 
second broad class signal models set statistical models tries charac statistical properties signal 
examples statistical models include gaussian processes son processes markov processes hidden markov pro cesses 
underlying assumption statistical model signal characterized parametric random process parameters stochastic process determined estimated precise defined manner 
applications interest speech process ing deterministic stochastic signal models success 
concern typeof stochastic signal model hidden markov model hmm 
models referred markov sources probabilistic functions markov chains communications literature 
review theory markov chains extend ideas class hidden markov models simple examples 
focus attention fundamental problems hmm design idea characterizing theoretical aspects hidden markov modeling terms solving fundamental problems due jack ferguson ida institute defense analysis introduced lectures writing 
evaluation probability likelihood sequence observations specific hmm determination best sequence model states adjustment model parameters best account observed signal 
show fundamental problems solved apply hmms selected prob lems speech recognition 
theory hidden markov models applications speech recognition new 
basic theory published series classic papers baum colleagues late early implemented speech processing applications baker cmu jelinek colleagues ibm 
widespread understanding application theory hmms speech processing occurred past years 
reasons case 
basic theory hidden markov models published math journals generally read engi working problems speech processing 
sec ond reason original applications theory speech processing provide sufficient tutorial material readers understand theory able apply research 
result tutorial papers written provided sufficient level detail number research labs hmms individual speech processing applications 
tutorial intended provide overview basic theory hmms originated baum colleagues provide practical details methods implementation theory describe couple selected applications theory distinct problems speech recognition 
combines results number original sources hopefully provides single source acquiring background required pursue fascinating area research 
organization follows 
section review theory discrete markov chains show concept hidden states observation probabilistic function state effec tively 
illustrate theory simple examples coin tossing classic balls urns system 
section ill discuss fundamental problems hmms give practical techniques solving problems 
section iv discuss various types hmms studied including ergodic left right models 
section discuss var ious model features including form observation density function state duration density opti mization criterion choosing optimal hmm parameter values 
section discuss issues arise imple hmms including topics scaling initial parameter estimates model size model form multiple observation sequences 
section vi isolated word speech recognizer implemented hmm ideas show performs compared alternative implementations 
section vi extend ideas section vi problem recog string spoken words concatenating individual 
section briefly outline ideas hmm applied speech recognizer sec tion ix summarize ideas discussed 

discrete markov processes consider system may described time set distinct states 
sn illustrated fig 
simplicity 
reg fig 

markov chain states labeled selected state transitions 
discrete times system state possibly back state set probabilities associated state 
denote time instants associated state changes 
denote actual state time qr 
full probabilistic description system gen eral require specification current state time predecessor states 
special case discrete order markov chain probabilistic description truncated just current pre state si qt skr 
sj 
processes right hand side independent time lead ing set state transition probabilities form state transition coefficients having properties obey standard stochastic constraints 
stochastic process called observ able markov model output process set states instant time state cor responds physical observable event 
set ideas con sider simple state markov model weather 
assume day noon weather overview discrete markov processes ch 

proceedings ieee vol 
february observed state rain snow state cloudy state sunny 
postulate weather day tis characterized single states matrix state transition probabilities lo weather day sunny state ask question probability model weather days sun sun rain rain sun cloudy sun 
stated mally define observation sequence sz corresponding 
wish determine probability model 
probability expressed evaluated rs sz modell ss rs ls ss ls 
notation az denote initial state probabilities 
interesting question ask answer model model known state stays 
probability evaluated probability observation sequence si si si dkl model ql ajd 
discrete probability density func tion duration din state exponential duration den sity characteristic state duration 
pi readily calculate expected num ber observations duration state conditioned starting state dpi 
ai expected number consecutive days sunny weather model cloudy rain 
extension hidden markov models far considered markov models state corresponded observable physical event 
model restrictive applicable problems interest 
section extend concept markov models include case observation prob function state resulting model hidden ded stochastic process underlying stochastic pro cess observable hidden observed set stochastic processes produce sequence observations 
fix ideas con sider model simple coin tossing experiments 
coin toss models assume scenario 
room barrier curtain see happening 
side barrier person performing coin mul tossing experiment 
person tell doing exactly tell result coin flip 
sequence hid den coin tossing experiments performed obser vation sequence consisting series heads tails typical observation sequence 
ot stands heads stands tails 
scenario problem interest build hmm explain model observed sequence heads tails 
problem faces deciding states model correspond deciding states model 
biased coin tossed 
case model sit uation state model state corresponds side coin heads tails 
model depicted fig 
case markov model observable issue complete specification model decide best value bias probability say heads 
interestingly equivalent hmm fig 
degenerate state model state corresponds single biased coin unknown parameter bias coin 
second form hmm explaining observed sequence coin toss outcome fig 

case states model state corre sponds different biased coin tossed 
state characterized probability distribution heads tails transitions states characterized state transition matrix 
physical mechanism accounts state transitions selected set independent coin tosses prob event 
third form hmm explaining observed sequence coin toss outcomes fig 

model corresponds biased coins choosing probabilistic event 
model fig 
memoryless process degenerate case markov model 
rabiner hidden markov models hi heads tails state pi pp pp 




fig 

possible markov models account hidden coin tossing experiments 
coin model 
coins model 
coins model 
choice models shown fig 
explaining observed sequence heads tails natural question model best matches actual observations 
simple coin model fig 
unknown parameter coin model fig 
un known parameters coin model fig 
unknown parameters 
greater degrees freedom larger hmms inherently capable modeling series coin tossing experiments equivalently smaller models 
theoretically true see practical considerations impose strong limitations size models con sider 
furthermore just case sin tossed 
coin model fig 
inappropriate actual physical event correspond model underspecified system 
urn extend ideas hmm somewhat complicated situation consider urn ball system fig 

assume arge 
number colored balls 
assume distinct balls 
physical follows 
genie room random process chooses initial urn 
urn ball chosen random color recorded ball replaced urn selected 
new urn selected urn ball model introduced jack ferguson colleagues lectures hmm theory 
os green green blue red yellow red 
blue fig 

state urn ball model illustrates general case discrete symbol hmm 
random selection process associated current urn ball selection process repeated 
entire process generates observation sequence colors model observable output hmm 
obvious simplest hmm cor responds urn ball process state corresponds specific urn ball color probability defined state 
choice urns dictated state transition matrix hmm 

elements hmm examples give pretty idea hmm applied simple sce 
formally define elements hmm explain model generates observation sequences 
hmm characterized number states model 
states hidden practical applications physical significance attached states sets states model 
coin tossing experiments state corresponded distinct biased coin 
urn ball model states corresponded urns 
generally states interconnected way state reached state ergodic model see possible interconnections states interest 
denote individual states sl 
sn state time 
number distinct observation symbols state discrete alphabet size 
observation sym correspond physical output system modeled 
coin toss experiments observation symbols simply heads tails ball urn model colors balls selected urns 
denote individual symbols vl 
vm 
state transition probability distribution lq special case state reach state single step types hmms pairs 
proceedings ieee vol 
february observation symbol probability distribution vk qt 
initial state distribution ql si 
appropriate values ir hmm generator give observation sequence observation symbols tis number observations sequence follows choose initial state si initial state distribution set 
choose vk symbol probability distribution state si 
transit new state state transition probability distribution state 
set return step ter procedure 
procedure generator observations model observation sequence generated appropriate hmm 
seen discussion complete specification hmm requires specification model parameters specification observation symbols specification probability mea sures convenience compact notation indicate complete parameter set model 
basic problems hmms form hmm previous section basic problems interest solved model useful real world applications 
prob lems problem observation sequence model ir efficiently compute probabilityof sequence model 
problem observation sequence 
model choose corresponding state sequence 
optimal meaningful sense best explains ion 
problem adjust model parameters maximize oja 
material section section ll ideas jack ferguson ida lectures bell lab 
rabiner hidden markov models problem evaluation problem model observations probability observed sequence produced model 
view problem scoring model matches observation sequence 
viewpoint extremely useful 
example consider case trying choose competing models solution problem allows choose model best matches observations 
problem attempt uncover hidden part model find correct state sequence 
clear case degenerate models correct state sequence 
practical situations usually optimality criterion solve problem best possible 
unfortunately see reasonable optimality criteria imposed choice criterion strong function intended uncovered state sequence 
typical uses learn structure model find optimal state sequences continuous speech recognition get average statistics individual states problem attempt optimize model parameters best describe observation sequence comes 
observation sequence adjust model parameters called training sequence train hmm 
training problem crucial applications hmms allows optimally adapt model parameters observed training data create best models real phenomena 
fix ideas consider simple isolated word speech recognizer 
word vocabulary want design separate state hmm 
represent speech signal word time sequence coded spectral vectors 
assume coding done spectral codebook unique spectral vectors observation index spectral vector closest spectral sense original speech signal 
vocabulary word training sequence consisting number repetitions talkers 
task build individual word models 
task done solution problem optimally estimate model parameters word model 
develop understanding physical meaning model states solution problem segment word training sequences states study properties spectral vectors lead observations occurring state 
goal refinements model states different codebook size improve capability modeling spoken word sequences 
set hmms designed optimized thoroughly studied recognition unknown word performed solution problem score word model test observation sequence select word 
highest kel hood 
section formal mathematical hmms 
shall see problems linked tightly probabilistic framework 
ill solutions basic problems hmms solution problem wish calculate probability observation sequence 
model ix 
straightforward way doing enumerating possible state sequence length number observations 
consider fixed state sequence qr initial state 
probability observation sequence state sequence ii assumed statistical independence obser 
get oi oj 
ot 
probability state sequence written qia rq aq lqr 
joint probability probability simultaneously product terms qin 
probability model obtained sum ming joint possible state sequences giving qin interpretation computation equation 
initially time state probability rq generate symbol state probability 
clock changes time transition state state probability generate symbol probability bq 
process continues manner list transition time state qt state qt probability lqr generate symbol probability 
little thought convince reader calculation direct definition involves order 
calculations 
possible states reached nr possible state sequences state sequence calculations required term sum 
precise need nr multiplications nt additions 
calculation computationally unfeasible small values states observations order computations 
clearly efficient procedure required solve problem 
fortunately procedure exists called forward backward procedure 
forward backward procedure consider forward variable defined 
oi qt probability partial observation sequence 
timet time model solve inductively follows initialization induction cy ol 
ai termination 

forward prob ability state si initial observation 
induction step heart forward calculation illus fig 

shows state observation fig 

illustration sequence operations required forward 
implementation computation terms lattice observations states speaking need forward part forward backward procedure solve problem 
introduce backward part procedure section help solve problem 
proceedings ieee vol 
february reached time possible states si timet 
probabilityof joint event 
observed state time si product probabilityof joint event 
observed state reached time state time summing product possible states si time results probabilityof time previous partial observations 
done si known easy see obtained accounting observation state multiplying summed quantity bythe performed states computation iterated 

step gives desired calculation ix sum terminal forward variables 
case definition aj qj ih just sum aj 
examine computation involved calculation see requires order calculations required direct calculation 
precise need multiplications additions 
need computations forward method versus io computations direct calculation savings orders magnitude 
forward probability calculation effect lattice trellis structure shown fig 

key states nodes time slot lattice possible state sequences nodes matter long observation sequence 
time time slot lattice need calculate values el times 
need calculate values calculation involves previous grid points previous time slot 
manner wecan defined ot 
probabilityof partial observation sequence state si time model solve inductively follows initialization induction 
pai dj si 
initialization step arbitrarily defines step illustrated fig 
shows order state si time account remind reader backward procedure solution problem required solution problem 
rabiner hidden markov models ci fig 

illustration sequence operations required computation backward variable 
observation sequence time con sider possible states time accounting transition si term obser vation state term account remaining partial observation sequence state term 
see backward forward calculations extensively help solve fundamental problems hmms 
computation fit requires order com puted lattice structure similar fig 

solution problem problem possible ways solving problem finding optimal state sequence associated observation sequence 
difficulty optimal state sequence possible 
example criterion choose states indi maximizes expected number correct individual states 
imple ment solution problem define variable rai io probability state si time observation sequence model equation expressed simply terms forward backward variables pai accounts partial observation sequence state accounts remainder observation sequence 
state si normalization factor cy pi yt probability measure 
solve individually state time argmax 
maximizes expected number correct states choosing state problems resulting state sequence 
example hmm state transitions optima state sequence may fact valid state sequence 
due fact solution simply determines state instant regard probability occurrence sequences states 
possible solution problem modify optimality criterion 
example solve state sequence maximizes expected number correct pairs states qt triples states qt reasonable applications widely criterion find single best state sequence path maximize ql equivalent maximizing 
formal technique finding single best state sequence exists dynamic programming methods called viterbi algorithm 
viterbi algorithm find single best state sequence ql 
qr observation sequence need define quantity max rql 


best score highest probability single path time accounts observations ends state si 
induction dj max 
retrieve state sequence need keep track argument maximized tand array 
complete procedure finding best state sequence stated follows initialization recursion ol 
wb cj max lb argmax 
termination max path state sequence backtracking 
noted viterbi algorithm similar backtracking step implementation forward calculation 
major difference maximization previous states place summing procedure 
clear lattice trellis structure efficiently imple ments computation viterbi procedure 
solution problem third far difficult problem hmms determine method adjust model parameters maximize probability observation sequence model 
known way analytically solve model maximizes probability observation sequence 
fact finite observation sequence training data optimal way estimating model parameters 
choose jh locally maximized iterative procedure baum welch method equivalently em expectation modification method gradient techniques 
section discuss iterative procedure primarily classic baum colleagues choosing model parameters 
order describe procedure reestimation iterative update improvement hmm parameters define probability state si time state time model observation sequence sequence events leading conditions required illustrated fig 

clear fig 

illustration sequence operations required computation joint event system state time state time 
definitions forward backward variables write form ai numerator term just qt qt si division ix gives desired probability measure 
proceedings ieee vol 
february previously defined ti probability state si time observation sequence model relate ti ff summing giving rai ai 
time index weget interpreted expected time number times state si visited equivalently expected number transitions state si exclude time slot summation 
similarly summation ft interpreted expected number transitions state si state 
yf expected number transitions ti expected number transitions si 
formulas concept counting event occurrences give method reestimation parameters hmm 
set reasonable mation formulas estimate hmm 
pointed forward backward algorithm leads local maxima problems interest zation surface complex local maxima 
reestimation formulas derived directly maximizing standard constrained opti mization techniques baum auxiliary function proven baum colleagues maximization leads increased hood max ix 
eventually likelihood function converges critical point 
notes reestimation procedure reestimation formulas readily interpreted implementation em algorithm statistics tation step calculation auxiliary function modification step maximization baum welch reestimation equations essentially identical em steps particular problem 
important aspect reestimation procedure stochastic constraints hmm parameters expected frequency number times state si time expected number transitions state si state expected number transitions state si expected number times state observing symbol vk expected number times state 
vk define current model compute right hand sides define reestimated model determined left hand sides proven baum colleagues cz initial model function model model sense jx automatically satisfied iteration 
looking new model observation sequence parameter estimation problem constrained produced 
procedure iteratively mization subject constraints techniques lagrange multipliers find place repeat reestimation calculation maximize improve probability observed tion ix short hand section 
limiting point reached 
final result ting standard lagrange optimization lagrange reestimation procedure called maximum multipliers readily rabiner hidden markov models conditions met ap ap aka tk appropriate manipulation right hand sides equation readily converted identical right hand sides part showing reestimation formulas exactly correct critical points fact form essentially reestimation formula left hand side reestimate right hand side computed current values variables 
note entire problem set optimization problem standard gradient techniques solve optimal values model parameters 
procedures tried shown yield standard reestimation procedures 
iv 
types hmms considered special case ergodic fully connected hmms state model reached single step state model 
strictly speaking ergodic model property state reached state finite number steps 
shown fig 
state model type model property aij coefficient positive 
example fig 
applications types hmms account observed properties signal mod better standard ergodic model 
model shown fig 

model called left right model model ill io underlying state sequence associated model property time increases state index increases stays states proceed left right 
clearly left right typeof hmm readily model signals properties change overtime speech 
fundamental property left right fig 

illustration distinct types hmms 
state ergodic model 
left right model 
state par path left right model 
hmms state transition coefficients prop erty transitions allowed states indices lower current state 
furthermore initial state probabilities property state sequence state state 
left right models additional con straints placed state transition coefficients sure large changes state indices occur constraint form 
particular example fig 
value jumps states allowed 
form state transition matrix example fig 
state left right model state transition coefficients specified ma 
ab proceedings ieee vol 
february hmms ergodic left right models possible variations combinations possible 
way example fig 
shows cross coupled connection parallel left right hmms 
strictly speaking model left right model obeys constraints seen certain flexibility strict left right model parallel paths 
clear imposition constraints left right model constrained jump model essentially effect reestimation pro cedure 
case hmm parameter set zero initially remain zero rees procedure see 
continuous observation densities hmms discussion point considered case observations characterized dis crete symbols chosen finite alphabet discrete probability density state model 
problem approach applications observations continuous signals vectors 
possible quantize continuous signals codebooks ous degradation associated quantization 
advantageous able hmms con observation densities 
order continuous observation density restrictions placed form model probability density function pdf insure param eters pdf reestimated consistent way 
general representation pdf rees procedure formulated finite mixture form modeled ficient mth mixture state 
log con cave symmetric density gaussian mean vector covariance matrix mth mixture component state usually gaussian density 
mixture gains satisfy stochastic constraint cl oa pdf properly normalized dx 
pdf approximate arbitrarily closely finite continuous density function 
applied wide range problems 
shown reestimation formulas coefficients mixture density form rai yt 
cl ot rf prime denotes vector transpose rt probability state time kth mixture component accounting term generalizes rt case simple mixture discrete density 
reestimation formula identical discrete observation densities 
interpretation fairly straightforward 
reestimation formula ratio number times system state kth mixture component expected number times system 
similarly reestimation formula mean vector weights numerator term observation giving expected value portion observation vector accounted kth mixture component 
similar interpretation reestimation term covariance matrix autoregressive hmms general formulation continuous density hmms applicable wide range problems interesting class hmms particularly applicable speech processing 
class auto regressive hmms 
class observation vectors drawn autoregression process 
specific consider observation vector components xo xl 
xk 
basis prob ability density function observation vector gauss ian autoregressive order components related ok ar ek ek 
gaussian independent identically distributed random variables zero mean variance autoregression predictor coefficients 
shown large density function approximately 
rabiner hidden markov models ao 
equations recognized autocorrelation observation samples autocorrelation autoregressive coefficients 
total frame prediction residual cy written cy ei variance sample error signal 
con sider normalized observation vector sample xi divided sample normalized bythe practice factor front exponential replaced effective frame length represents length 
consecutive data vectors overlapped 
contribution sample signal density counted exactly 
gaussian hmms straightforward 
assume mixture density form mb density defined auto regression vector equivalently autocorrelation vector ra reestimation formula sequence autocorrelation jth state kth mixture component derived form rai rt yt defined probability state time mixture component seen jk weighted sum probability occurrence normalized autocorrelations frames observation sequence 
ijk solve set normal equations obtain corresponding auto regressive coefficient vector kth mixture state 
new vectors autoregression ing reestimation loop 
variants hmm structures null transitions tied states considered hmms observations associated states model 
possible consider models observations associated arcs model 
type hmm extensively ibm con speech recognizer 
useful type model allow transitions produce jumps state pro duce observation 
transitions called null transitions designated dashed line symbol denote null output 
fig 
illustrates examples speech processing tasks null arcs successfully utilized 
fig 

examples networks incorporating null transi tions 
left right model 
finite state network 
gram mar network 
example part corresponds hmm left right model large number states possible omit transitions pair states 
possible generate observation sequences observation account path begins state ends state example fig 
finite state network rep resentation terms linguistic unit models sound arc hmm 
model null transition gives compact efficient way ing alternate word pronunciations symbol 
fig 
shows ability insert null transition grammar network allows relatively simple network generate arbitrarily long word digit sequences 
example shown fig 
null tran sition allows network generate arbitrary sequences digits arbitrary length returning initial state individual digit produced 
interesting variation hmm structure concept parameter 
basically idea set equivalence relation hmm parameters proceedings ieee vol 
february different states 
number independent parameters model reduced parameter esti mation somewhat simpler 
parameter cases observation density example known states 
cases occur characterizing speech sounds 
tech nique especially appropriate case insufficient training data estimate reliably large num ber model parameters 
cases appropriate tie model parameters reduce number parameters size model making parameter estimation problem somewhat simpler 
discuss method 
inclusion explicit state duration density hmm 
fw major weakness conventional hmms modeling state duration 
earlier showed inherent duration probability density associated state sf self transition coefficient form probability consecutive observations state si 
physical signals exponential state duration density inappropriate 
prefer model duration density analytic form 
fig 
fig 

illustration general connections normal hmm exponential state duration density variable duration hmm specified state densities self transitions state back 
illustrates model hmms explicit duration density 
part states exponential duration densities self transition coefficients respectively 
part self transition coefficients set explicit duration density case cases wherea type model left right models states proportional explicit inclusion state duration density necessary useful 
ideas explicit state duration densities due jack ferguson ida material section ferguson original 
rabiner hidden markov models transition appropriate number observations occurred state specified duration density 
simple model fig 
sequence events variable duration hmm follows initial state si chosen initial state distribution 
duration dl chosen state ease tion density pql dl 
implementation duration density cated maximum duration value 
observations odl chosen joint observation density bq ol 
od 
assume independent observations od ot 
state si chosen state transition probabilities constraint transition back state occur 
clearly requirement assume state exactly dl observations occur 
little thought convince reader variable duration hmm equivalent stan dard hmm setting exponential density 
formulation changes formulas section ll allow calculation ix reestimation model parameters 
par ticular assume state begins state ends entire duration intervals included observation sequence 
define forward variable 
si ends 
assume total states visited observations denote states ql 
qr durations associated state dl 

constraints qr si equation written rq pql dl od od od aq lq pq dr odl 
induction write os maximum duration state 
ini computation aai ii oz mo ad computed clear desired probability model written terms ix cy previously ordinary hmms 
reestimation formulas variable duration hmm define forward backward variables cy ot si begins ilx ends begins 
relationships cy cy follows pi bi os pi bi os 
relationships definitions rees formulas variable duration hmm cy pt oj pi 
pt os interpretation reestimation formulas fol lowing 
formula ti probability state state usual hmm uses condition alpha terms state ends join beta terms new state begins 
formula assuming discrete density expected number times observation vk occurred state normalized expected number times observation occurred state reestimation formula ratio expected number times state duration numberof times state occurred duration 
importance incorporating state duration densities reflected observation problems quality modeling significantly improved explicit state duration densities 
drawbacks variable duration model discussed section 
greatly increased computational load associated variable durations 
seen definition initialization conditions forward variable times storage times computation required 
don order reasonable speech processing problems computation increased factor 
problem variable duration models large number parameters associated state estimated addition usual hmm parameters 
furthermore fixed number observations training set average fewer state transitions data estimate standard hmm 
reestimation problem difficult variable duration hmms standard hmm 
proposal alleviate problems parametric state duration density nonparametric 
particular proposals include gaussian family pm pi parameters gamma family dv 
le tt rw parameters mean variance 
reestimation formulas derived results 
possibility success assume uniform duration distribution appropriate range durations path constrained viterbi decoding procedure 
optimization criterion ml mmi mdi basic philosophy hmms signal obser vation sequence modeled parameters hmm carefully correctly chosen 
problem philosophy inaccurate signal obey constraints hmm difficult get reliable esti mates hmm parameters 
alleviate type prob lem proposed wo alternatives standard maximum likelihood ml optimization pro cedure estimating hmm parameters 
alternative idea hmms designed wish design time way maximize dis power model model ability proceedings ieee vol 
february distinguish observation sequences generated correct model generated alternative models 
denote different hmms 
standard criterion separate training sequence observations derive model parameters model 
standard ml opti mization yields proposed alternative design criterion max imum mutual information mmi criterion average mutual information observation sequence complete set models 
maximized 
possible way implementing max log ix log xw choose separate correct model models training sequence 
summing training sequences hope attain separated set models possible 
possible implementation max log log ix various theoretical reasons analytical reestimation type solutions realized 
known way solving general optimization procedures steepest descent methods 
second alternative philosophy assume signal modeled necessarily generated obey certain constraints positive definite correlation function 
goal design procedure choose hmm parameters minimize discrimination information di cross entropy set valid satisfy measurements signal probability densities call set set hmm probability densities call set pa di scan generally written form dy probability density functions cor responding pa techniques minimizing giving mdi solution optimum values highly nontrivial generalized baum algorithm core iteration hidden markov modeling wi 
shown ml mmi mdi approaches uniformly formulated mdi approaches approaches differ probability density attributed source modeled model assume words equiprobable rabiner relations modeling approaches speech recognition appear ieee trans actions information theory 
effectively 
approaches assumes source probability distribution model 
comparison hmms interesting question associated hmms fol lowing hmms reasonable measure similarity models 
key point similarity criterion 
example consider case models xi ai bit ql 
equivalent sense having statistical properties observation symbols vk require pq rs solving get choosing arbitrarily get 
models look ostensibly different different different statistical equiv models occur 
generalize concept model distance dis similarity defining distance measure markov models 
sequence observations generated model 
basically measure model hl matches observations generated model hz relative model matches observations generated 
interpretations exist terms cross entropy divergence discrimination information 
problems distance measure nonsymmetric 
natural expression measure symmetrized version implementation issues hmms discussion previous sections primarily hmms variations form model 
section prac tical implementation issues including scaling multiple rabiner hidden markov models observation sequences initial parameter estimates ing data choice model size type 
implementation issues prescribe exact ana solutions issues provide seat pants experience gained working hmms years 
scaling order understand scaling required imple reestimation procedure hmms consider 
consists sum large number terms form si 
term generally significantly seen starts get big term starts head exponentially zero 
sufficiently large dynamic range computation exceed precision range essentially machine double precision 
reasonable way performing computation incorporating scaling procedure 
basic scaling procedure multiply scaling coefficient independent depends goal keeping scaled dynamic range computer similar scaling done coefficients tend zero exponentially fast computation scaling coefficients canceled exactly 
understand scaling procedure better consider reestimation formula forthe coefficients 
write reestimation formula directly terms forward backward variables get consider computation 
compute induction formula multiply scaling coefficient ctr fixed compute gt dj 
scaled coefficient set computed ai ti tit induction write write effectively scaled sum states 
compute terms backward recursion 
difference scale factors time betas alphas 
scaled form ba 
scale factor effectively restores magnitude ol terms magnitudes terms comparable scaling factors effective way keeping computation reasonable bounds 
furthermore terms scaled variables see reestimation equation written written written term seen form independent oft 
terms cancel numerator denominator exact reestimation equation realized 
obvious scaling procedure applies equally reestimation coefficients 
scaling procedure need applied time instant performed desired necessary prevent underflow 
scaling performed instant scaling coefficients set time conditions discussed met 
real change hmm procedure scaling procedure computing 
merely sum terms scaled 
proceedings ieee vol 
february property cf ad ct yt 
cf log log cf 
log dynamic range machine anyway 
note viterbi algorithm give maximum likelihood state sequence scaling required logarithms way 
refer back 
define initially set recursion step termination step log max 
arrive log signifi cantly computation numerical problems 
reader note terms log precomputed cost computation 
furthermore terms log precomputed finite observation symbol analysis codebook observation sequences 
multiple observation sequences section right model state proceeds state state tin sequential manner recall model fig 

discussed left right model imposes constraints state transition matrix initial state probabilities 
major problem left right models single observation sequence train model reestimation model parameters 
transient nature states model allow small number observations state tran sition successor state 
order sufficient data reliable estimates model parameters sequences 
modification reestimation procedure straightforward goes follows 
denote set observation sequences 
ofk ik 
kth observation sequence 
assume observation sequence inde pendent observation sequence goal adjust parameters model maximize ix pk 
reestimation formulas frequencies occurrence various events reestimation formulas multiple observation sequences modified adding individual frequencies occurrence sequence 
modified reestimation formulas tk reestimated xl 
proper scaling straightforward observation sequence scaling factor 
key idea remove scaling factor term summing 
accomplished writing reestimation equations terms scaled variables tk manner sequence ofk scale factors appear term sum appears pk term cancel exactly 
scaled values alphas betas results unscaled 
similar result obtained term 
initial estimates hmm parameters theory reestimation equations give values hmm parameters correspond local max choose initial estimates hmm parameters local maximum global maximum likelihood function 
basically simple straightforward answer question 
experience shown random subject stochastic nonzero value constraints uniform initial estimates rand rabiner hidden markov models parameters adequate giving useful parameters cases 
parameters experience shown initial esti mates helpful discrete symbol case essential dealing multiple mixtures con distribution case 
initial estimates obtained number ways including manual tation observation sequence states aver aging observations states maximum likelihood segmentation observations averaging seg mental means segmentation clustering dis segmentation techniques 
effects insufficient training data problem associated training hmm param eters reestimation methods observation sequence training necessity finite 
insufficient number occurrences different model events symbol occurrences states give estimates model parameters 
solution problem increase size training obser vation set 
impractical 
second possible solu tion reduce size model number states number symbols state 
possible physical reasons model model size changed 
third possible solution interpolate set param eter estimates set parameter estimates model adequate amount training data exists 
idea simultaneously design desired model smaller model amount training data adequate give parameter estimates interpolate parameter estimates models 
way smaller model chosen sets parameters initial model create smaller model 
estimates parameters model reduced size model interpolated model obtained ex represents weighting parameters full model represents weighting parameters reduced model 
key issue deter optimal value clearly func tion amount training data 
amount train ing data gets large expect tend similarly small amounts training data expect tend solution determination optimal value provided jelinek mercer showed optimal value estimated ward backward algorithm interpreting expanded hmm type shown fig 

expanded model probabilityof state transition neutral state model similarly probability state transition model 
models null transition 
model fig 
value estimated training data standard manner 
key point segment training data disjoint sets tl 
training set tl train models give estimates fig 

example process deleted lation represented state diagram 

training set give estimate assuming models fixed 
modified version training procedure called method deleted interpolation iterates pro cedure multiple partitions training set 
example consider partition training set percent remaining percent large number ways partitioning accomplished partic simple cycle data partition uses percent data second partition uses percent technique deleted interpolation suc applied number problems speech rec including estimation trigram word proba bilities language models estimation hmm output probabilities trigram phone models 
way handling effects insufficient train ing data add extra constraints model parameters insure model parameter estimate falls specified level 
example specify constraint discrete symbol model continuous distribution model 
constraints applied postprocessor reestimation equations constraint violated relevant parameter manually corrected remaining parameters rescaled densities obey required stochastic constraints 
post pro cessor techniques applied problems speech processing success 
seen procedure essentially equivalent simple form deleted interpolation model uniform distribution model interpolation value chosen fixed constant 
choice model remaining issue implementing hmms type model ergodic left right form choice model size number states choice observation symbols discrete continuous single multi mixture choice observation parameters 
simple theoretically correct way making choices 
depend ing signal modeled 
comments proceedings ieee vol 
february discussion theoretical aspects hidden markov models proceed discussion models applied selected problems speech recognition 
vi 
implementation speech recognizers hmms isto illus ideas hmms discussed sec tions applied selected problems speech recognition 
strive thorough complete descriptions done describing theory hmms 
interested reader read material io complete descriptions individual systems 
main goal show specific aspects hmm applied reader expert speech recognition technology 
recognition system fig 
shows block diagram pattern recognition approach continuous speech recognition system 
key signal processing steps include feature analysis spectral temporal analysis speech signal performed give observation vec tors train hmms charac various speech sounds 
detailed discussion type feature analysis section 
unit matching system choice speech rec unit 
possibilities include linguis sub word units phones phone units syllables derivative units acoustic units 
possibilities units units correspond group 
generally complex unit phones fewer language complicated variable struc ture continuous speech 
speech rec involving sub word speech units mandatory quite dif record hmms units size words larger 
spe applications small vocabulary constrained task practical basic speech unit 
consider systems section 
independent unit chosen recognition inventory units obtained training 
typically unit char type hmm parameters esti mated training set speech data 
unit matching system provides likelihoods match sequences fig 

block diagram continuous speech recognizer 
speech recognition units unknown input speech 
techniques providing match scores par ticular determining best match score subject lexical syntactic constraints system include stack decoding procedure various forms frame ous path decoding lexical access scoring pro cedure 
lexical decoding process places constraints unit matching system paths investigated corresponding sequences speech units word dictionary lexicon 
procedure implies speech recognition word vocabulary spec ified basic units chosen recognition 
specification deterministic finite state networks word ary statistical rep resentation words 
case chosen units combinations lexical decoding step recognizer greatly simplified 
syntactic analysis process lexical decoding places constraints unit matching system paths investigated ing speech units comprise words lexical ing words proper sequence specified word grammar 
word grammar represented finitestate network word combinations accepted grammar enumerated statistical grammar trigram word model probabilities sequences words specified order 
com mand control tasks single word set equiprobable required recognized grammar trivial unnecessary 
tasks referred isolated word speech recognition tasks 
applications digit sequences simple grammars adequate digit spoken followed digit 
tasks grammar dominant factor adds great deal constraint recognition process greatly improves recognition performance result ing restrictions sequence speech units valid recognition candidates 
semantic analysis process steps syntactic analysis lexical decoding adds con straints set recognition search paths 
way semantic constraints utilized dynamic model state recognizer 
depending recognizer state certain syntactically correct input strings eliminated consideration 
serves recognition task easier leads higher formance system 
rabiner hidden markov models additional factor significant effort implementation speech recognizer problem separating background silence input speech 
reasonable ways hi ng task explicitly detecting presence speech tech niques discriminate background speech basis signal energy signal durations 
methods template approaches inherent simplicity success low moderate noise backgrounds mi 
build model background silence sta tistical model represent incoming signal arbitrary sequence speech background signal silence speech silence silence part signal optional may speech 
extend speech unit models background silence included optionally state model silence inher gets included speech unit models 
techniques utilized speech recognition systems 
discussing general continuous speech rec system specialized appli cations illustrate hmm technology utilized 
system basic speech unit word task recognize single spoken word task syntax semantics constrain choice words 
task generally referred isolated word recognition 
discuss slightly complicated task basic speech unit word task recognize continuous utter ance consisting words 
included task problem recognizing spoken string digits 
consider case task syntax semantics constrain choice words digit 
recognition type referred connected word recognizers continuous speech recognized sequence word models 
technically mis speech signal lpc observation feature analysis vector computation fig 

block diagram isolated word hmm recognizer 
truly continuous speech recognition problem 
terminology continue 
isolated word recognition example consider hmms build isolated word recognizer 
assume vocabulary recognized word mod distinct hmm assume word vocabulary training set occurrences spoken word spoken talkers occurrence word constitutes observation sequence observations appropriate representation spectral temporal charac 
return question specific representation section 
order isolated word speech recognition perform word vocabulary build hmm estimate model parameters optimize likelihood training set observation vectors vth word 
unknown word recognized processing fig 
carried measurement observation sequence 
ot feature analysis speech corresponding word followed calculation model likelihoods possible models followed selection word model likelihood highest argmax 
probability computation step generally performed viterbi algorithm maximum likelihood path requires order 
modest vocabulary sizes state model observations excellent description isolated word large vocabulary speech recognizer sub word units isgiven descrip tion ibm system 
compares effects continuous discrete densities word vocabulary 
hmm word index recognized word proceedings ieee vol 
february unknown word total io computations required recognition computation multiply add calculation observation density 
clearly amount computation modest compared capa bilities modern signal processor chips 
lpc feature analysis way obtain observation vectors speech samples perform front spectral analysis 
assume processing speech samples cor responding spoken word background word eliminated appro priate word detection algorithm 
type spectral anal ysis describe linear lpc anda isgiven fig system block processing model frame samples processed vector features com puted 
steps processing follows digitized khz rate examples discussed speech signal processed order digital network order spectrally flatten signal 
blocking frames sections na consecutive speech samples na corresponding ms signal single frame 
consecutive frames spaced ma samples apart ma correspond ing ms frame spacing ms frame overlap 
frame windowing frame multiplied na sample window hamming window na tion running speech signal 
autocorrelation analysis windowed set speech samples autocorrelated give set cients order desired lpc analysis 
analysis frame vector lpc coefficients computed autocorrelation vector levinson durbin recursion method 
lpc derived cepstral vector computed qth component results described section 
cepstral weighting coefficient cepstral vector time frame pis weighted window form give sin 
block xt xi auto re lpc az cepstral frames frame analysis ck 
delta cepstrum time derivative sequence weighted cepstral vectors approximated order orthogonal polynomial finite length window frames centered current vector 
results frame win dow computation derivative 
cep derivative delta cepstrum vector computed ki gain term chosen variances ai equal 
value 
observation vector recognition training concatenation weighted cepstral vector delta cepstrum vector qp ae consists coefficients vector 
vector quantization case wish hmm discrete observation symbol density continuous vectors vector quantizer vq required map continuous observation vector discrete codebook index 
codebook vectors obtained mapping continuous vectors cepstral ng xp xp rp xp xp ar lpc coefficients ci cepstral coefficients cp fig 

block diagram computations required front feature analysis hmm recognizer 
rabiner hidden markov models codebook indices simple nearest neighbor com putation continuous vector assigned index nearest spectral distance sense codebook vec tor 
major issue vq design appro priate codebook quantization 
fortunately great deal gone devising excellent iterative procedure designing codebooks having representative training sequence vec tors procedure basically tors disjoint sets size code book represents set single vector generally centroid vectors training set assigned mth region iter optimizes partition codebook centroids partition 
associated vq dis penalty representing entire region vector space single vector 
clearly advan keep distortion penalty small possible 
implies large size codebook leads problems implementing hmms large number parameters 
fig 
illustrates quantization fig 

curve showing tradeoff vq average distortion function size vq shown log scale 
distortion versus log scale 
distortion steadily decreases increases seen fig 
small decreases distortion accrue value 
hmms codebook sizes vectors speech recognition experiments hmms 
choice model parameters come back issue raised sev eral times select type model choose parameters selected model 
isolated word recognition dis hmm designed word vocabulary clear left right model appropriate ergodic model associate time model states fairly straightforward manner 
fur thermore envision physical meaning model states distinct sounds phonemes syllables word modeled 
issue number states word model leads schools thought 
idea number states correspond roughly number sounds phonemes word models states appropriate 
idea number states correspond roughly aver age number observations spoken version called model ill manner state corresponds observation interval ms analysis 
results described section approach 
furthermore restrict word model number states represent words number sounds 
illustrate effect varying number states word model fig 
shows plot average word error 
number states hmm fig 

average word error rate digits vocabulary versus number states hmm 
rate versus case recognition isolated digits io word vocabulary 
seen error somewhat insensitive achieving local minimum differences error rate values close small 
issue choice observation vector way represented 
discussed sections vi vi considered lpc derived weighted cepstral coefficients weighted cepstral derivatives auto regressive hmms autocorrelation lpc cients observation vectors continuous models discrete symbol models codebook generate discrete symbols 
continuous models mixtures state discrete symbol models codebooks code words models diagonal covariance sev eral mixtures fewer mixtures full ance matrices 
reason simple dif performing reliable reestimation diagonal components covariance matrix necessarily limited training data 
illustrate need mixture densities modeling lpc observation vec tors eighth order cepstral vectors log energy appended ninth vector component fig 
shows comparison marginal distributions jo 

histogram actual observations state determined maximum likelihood segmentation training observations states 
observation vectors ninth order model density uses mixtures 
covariance matrices constrained diagonal individual mixture 
results fig 
model state word zero 
need values clearly seen histogram proceedings ieee vol 
february word zero state parameter range fig 

comparison estimated density jagged contour model density smooth contour components observation vector cepstral com ponents log energy component state digit zero 
parameter cepstral component inherently multimodal similarly second fourth cepstral parameters show need single gaussian component provide fits empirical data 
parameters appear fitted single gaussian cases mixtures provide sufficiently fit 
experimentally verified fact hmm important limit parameter estimates order prevent small 
example discrete symbol models constraint bj greater equal minimum value necessary insure kth symbol occurred set finite probability occurrence scoring unknown observation set 
illustrate point fig 
fo io fig 

average word error rate function min imum discrete density value shows curve average word error rate versus param eter log scale standard word recognition exper 
seen broad range average error rate remains constant value set io error rate increases sharply 
similarly continuous densities important constrain mixture gains clm diagonal covariance coefficients ulm greater equal minimum values cases 
segmental means segmentation states stated earlier initial estimates param eters bi densities essential rapid proper convergence reestimation formulas 
procedure providing initial estimates fig 
training model initialization state sequence segmentation estimate parameters 
segmental means model reestimation fig 

segmental means training procedure estimate parameter values optimal continuous mix number observation sequences 
procedure variant known means iterative procedure clustering data 
assume training set observations required parameter reestimation ini tial estimate model parameters 
required reestimation initial model estimate chosen randomly basis available model appropriate data 
model initialization set training obser vation sequences segmented states current model segmentation achieved finding optimum state sequence viterbi algorithm backtracking optimal path 
procedure illustrated fig 
shows log energy plot accumulated log likelihood plot state segmentation occurrence word 
seen fig 
states correspond roughly sounds spoken word 
result segmenting training sequences states maximum likelihood estimate set observations occur state current model 
case discrete symbol densities observation vectors state coded codeword code book updated estimate parameters number vectors codebook index state divided number vectors state current initial model created set talkers created uniform tation word states 
rabiner hidden markov models bi bz ba bq frame number fig 

plots log energy accumulated log likelihood state assignment occurrence word 
weare continuous observation den segmental means procedure cluster observation vectors state si set clus ters euclidean distortion measure cluster represents mixtures den sity 
clustering updated set model param eters derived follows number vectors classified cluster state divided number vectors state pi sample mean vectors classified cluster state sample covariance matrix vectors classified cluster state state segmentation updated estimates coefficients obtained counting number transitions state dividing number transitions state state including 
updated model fi obtained new model parameters formal reestimation procedure reestimate model parameters 
resulting model compared previous model computing dis tance score reflects statistical similarity hmms 
model distance score exceeds threshold old model replaced new reestimated model training loop repeated 
model distance score falls threshold model convergence assumed final model parameters saved 
incorporation state duration hmm section iv discussed theoretically correct method incorporating state duration information mechanics hmm 
showed cost including duration density high fold increase computation fold increase stor age 
value required word rec cost increased computation tended techniques worth 
alternative procedure formulated incorporating state duration information hmm 
alternative procedure state duration prob ability measured directly segmented training sequences segmental means pro previous section 
strictly heuristic ones 
typical set histograms state model word shown fig 

histograms plotted versus normalized duration absolute duration 
digit lii state state normalized duration fig 

histograms normalized duration density states digit seen fig 
states account initial third state accounts transition vowel lil fourth state accounts vowel fifth state accounts final sound 
way heuristic duration densities recognizer follows 
normal viterbi algorithm give best segmentation observation sequence unknown word states backtracking procedure 
duration state measured state segmentation 
cessor increments log likelihood score viterbi algorithm quantity log log ad log ad scaling multiplier scores duration state optimal path determined viterbi algorithm 
incremental cost postprocessor duration essentially negligible experience shown recognition performance proceedings ieee vol 
february essentially obtained cally correct duration model 
hmm performance isolated word recognition conclude section isolated word recognition hmms giving aset performance results terms average word error rate task recognizing iso lated digits speaker independent manner 
task training set consisting occurrences digit talkers single occurrence digit talker 
half talkers male half female 
test ing algorithm initial training set independent test sets characteristics ts talkers training occurrences digit ts new set talkers male female occurrences digit ts new set talkers male female occurrences digit results recognition tests table 
recognizers lpc dtw conventional template recog dynamic time warping dtw alignment lpc dtw vq conventional recognizer vector quantization feature vectors hmm codebook hmm cd hmm recognizer continuous den sity model mixtures state hmm ar hmm recognizer autoregressive observation density table average digit error rates recognizers evaluation sets evaluation set recognizer original ty pe training ts ts ts seen vq performance isolated word recognizer degrades con hmm modes 
seen conventional template recognizer hmm recognizer continuous density model comparable 
table shows sive density hmm gives poorer performance stan dard mixture density model 
vii 
connected word recognition hmms somewhat complicated problem speech rec hmms successfully applied problem connected word recognition 
basic premise connected word recognition rec individual word models opposed models speech units smaller words 
rec problem appropriate word models derived find optimum sequence nation word models best matches maximum likelihood sense unknown connected word string 
section discuss method called level build ing approach solving optimum sequences word models 
alternative method obtaining opti mum sequence words frame time synchronous viterbi search 
practical advantages frame synchronous search ease real time hardware implementation ease path pruning affect optimality methods 
convenience restrict discussion recognition strings connected digits 
connected digit recognition word hmms level building block diagram level building connected digit recognizer fig 
steps recognition process spectral analysis speech signal converted set lpc vectors set cepstral delta single digit patterns fig 

block diagram level building connected digit recognizer 
recognized cepstral vectors 
defines observation sequence unknown connected digit string 
level pattern matching sequence spectral vectors observations unknown connected digit string matched hmms process set candidate digit strings generally different lengths different number digits string ordered log probability scores 
postprocessor candidate digit strings subjected validity tests duration eliminate unreasonable candidates 
postprocessor chooses digit string remaining valid candidate strings 
individual digits characterized hmm type shown fig 

transitions words handled switch mode state word model state word model level building implementation 
parameters hmms characterizing digits states digit models trained obser single talker states position 
digit level outputs digit string 
rabiner hidden markov models fig 

hmm characterization individual digits con nected digit recognition 
digit models trained observations single talker 
continuous observation mixture densities mixtures state single talker models mixtures state multiple talker models 
energy probability pi dynamically normalized log energy frame speech give observation vector pi discrete density log energy values state density derived empirically training data 
state duration density pi 
addition observation density log energy prob ability state duration density word hmm characterized word duration form average duration word ij var duration word normal density 
level building hmms way level building hmms illustrated fig 

denote set word hmms vi hmms match maximize likelihood sequence viterbi matches performed 
hmm level viterbi match starting frame observation interval level retain possible frame accumulated log probability frame level model best path 
backpointer indicating path started level 
compute need local measure proba bility observation log energy occurred state model 
observation density function yc set log energy scaling coefficient normalization constant 
state transition cients enter calculation dynamic pro gramming optimization determining viterbi path 
level level corresponds word position string maximization test frame fig 

illustration hmms applied level building algorithm 
performed get best model frame fol lows max pp wf argmax ff wl wf records number word model gave best score frame level ff records backpointer best word model 
new level begins initial best probability preceding level increments viterbi score matching word models new initial frame 
process repeated number levels equivalent maximum expected number digits string typically 
level best string size words probability obtained backtracking backpointer array ff give words string 
best string maximum possible levels training word models key success connected word recognition derive word models representative connected word strings 
formal mation procedures developed costly terms computation equivalently parameter estimates obtained segmental means procedure type discussed section vi 
difference procedure discussed earlier training connected word strings segmented individual digits viterbi alignment procedure set digits segmented states vectors state clustered best proceedings ieee vol 
february cluster solution 
segmental means reestimation hmm parameters order magnitude faster baum welch reestimation procedure experimentation indicates resulting parameter esti mates essentially identical resulting hmms essentially likelihood values 
segmental means procedure give results section 
duration modeling connected digits forms durational information scoring connected digit sequences word duration state duration 
way word duration infor mation incorporated model scoring follows 
level frame accumulated probability modified determining word tion jf ff multiplying accumulated probability word duration probability set weighting factor word tions normalization constant 
state duration probabilities incorporated post processor 
level building recognizer provides multiple candidates level tracking multiple best scores frame level 
probability scores obtained rl strings length digits number candidates level typically 
rl strings backtracked give individual words individual states words 
word string denote duration level pas af possible string postprocessor multiplies accumulated probability bythe prob abilities giving fl af set weighting factor state durations word level normalization constant 
computation performed rl strings reordered list best strings obtained 
incremental cost postprocessor computation negligible compared computation give fl performance shown comparable performance internal duration models 
performance connected digit hmm recognizer hmm connected digit recognizer trained tested modes speaker trained talkers male female provided training set connected digit strings independent testing set digit strings 
training sets talkers merged single large training set testing sets similarly merged 
case set hmms digit hmm derived subset training utter ances 
rabiner hidden markov models speaker independent ti training testing databases 
training testing sets talkers different ones set talkers divided groups 
hmms 
databases variable length digit strings digits string 
performance hmm connected digit recog modes table entries table performance hmm connected digit recognizer modes training set testing set mode ul kl ul kl speaker trained talkers talkers speaker independent talkers table average string error rates cases string length unknown apriori ul cases string length known apriori kl 
results training set word models derived independent test set 
viii 
hmms large vocabulary speech recognition hmms successfully applied problems isolated connected word recognition anticipated payoff theory problems speech recognition application large vocabulary speech recognition recognition speech performed basic speech units smaller words 
research area far research area speech processing far extensive discuss 
section briefly outline ideas hmms applied problem 
advanced systems comparable investigation ibm bbn cmu places theory hmms applied representation phoneme sub words hmms representation words hmms representation syntax hmm 
solve speech recognition problem embedded network hmms 
leads expanded network astronomical number equivalent states alternative complete exhaustive search procedure required 
alternatives beam searches 
procedures shown capable handling large networks words efficient reliable manner 
details approaches scope 
attempt apply hmms continuous speech recognition ergodic hmm state represented acoustic phonetic unit 
states required represent sounds english 
model incorporated variable duration feature state account fact vowel sounds vastly different durational characteristics con 
conjunction standard pronouncing dictionary determine best matching word sequence put sub word hmm 
details rec system scope 
purpose brief discussion point vast potential hmms characterizing basic processes speech production problems large vocabulary speech recognition 
limitations hmms hmm technology contributed greatly advances speech recognition inherent limitations type statistical model speech 
major limitation assumption sive observations frames speech independent probability sequence observations written product probabilities individual observations 
limitation assumption distributions individual observation parameters repre sented mixture gaussian autoregressive densities 
markov assumption proba bility state time depends state timet clearly inappropriate speech sounds dependencies extend states 
spite limitations type statistical model worked certain types speech recognition problems 
ix 
summary attempted theory hidden markov models simplest concepts dis crete markov chains sophisticated models variable duration continuous density models 
purpose focus physical explanations basic mathematics avoided long drawn proofs derivations key results primarily trying interpret meaning math implemented practice real world systems 
attempted illustrate applications theory hmms simple problems speech recognition pointed techniques applied advanced speech recognition problems 
acknowledgment author gratefully acknowledges major butions colleagues hmms gen eral presentation particular 
great debt owed dr ferguson dr dr dr richter 
world ideas hmms 
addition dr levinson dr sondhi dr juang dr dembo dr contributed significantly theory hmms author perspective knowledge theory best applied problems speech recog nition 
baum petrie statistical inference finite state ann 
math 
stat vol 
pp 

baum egon inequality applications statistical estimation probabilistic functions markov process model ecology bull 
amer 

soc vol 
pp 

baum sell growth functions transformations manifolds pac 
math vol 
pp 

baum petrie soules weiss maximization technique occurring statistical analysis probabilistic functions markov chains ann 
math 
stat vol 
pp 

baum associated maximization technique statistical estimation probabilistic functions markov processes inequalities vol 
pp 

baker dragon system overview trans 
acoust 
speech signal processing vol 
asp pp 
feb 
jelinek fast sequential decoding algorithm stack ism 
res 
develop vol 
pp 

bahl jelinek decoding channels insertions deletions substitutions applications speech recognition trans 

theory vol 
pp 

jelinek bahl mercer design linguistic statistical decoder recognition continuous speech trans 

theory vol 
pp 

jelinek continuous speech recognition statistical methods proc 
vol 
pp 
apr 
continuous speech word recognition acoustic states proc 
asa meeting washington dc apr 
jelinek bahl mercer continuous speech recognition statistical methods handbook statistics ed 
amsterdam netherlands north holland 
bahl jelinek mercer maximum likelihood approach continuous speech recognition trans 
pattern anal 
machine intel vol 
pami pp 

levinson rabiner sondhi application theory probabilistic functions markov process automatic speech recognition 
tech 
vol 
pp 
apr 
juang hidden markov model dynamic time warping speech recognition unified view 
vol 
pp 
sept 
juang hidden markov models ieee assp mag vol 

pp 

bridle stochastic models template matching important relationships apparently different techniques automatic speech recognition proc 
inst 
conf pp 
nov 
makhoul gish vector quantization speech coding proc 
vol 
pp 
nov 
levinson structural methods automatic speech recognition proc 
vol 
pp 
nov 
drake discrete state markov processes chapter fundamentals probability theory 
new york ny mcgraw hill 
viterbi error bounds convolutional codes asymptotically optimal decoding algorithm trans 

theory vol 
pp 
apr 
forney viterbi algorithm proc 
vol 
pp 
mar 
dempster laird rubin maximum proceedings ieee vol 
february incomplete data em algorithm 
roy 
stat 
soc vol 
pp 

maximum likelihood estimation multivariate observations markov sources trans 

theory vol 
pp 

juang maximum likelihood estimation mixture multivariate stochastic observations markov chains tech 
vol 
pp 
july aug 
juang levinson sondhi maximum likelihood estimation multivariate mixture observations markov chains trans 

theory vol 
pp 
mar 
linear predictive hidden markov models speech signal proc 
paris france pp 
may 
juang rabiner mixture autoregressive hidden markov models speech signals trans 
acoust 
speech signal processing vol 
assp pp 
dec 
russell moore explicit modeling hidden markov models automatic speech recognition proc 
tampa fl pp 
mar 
levinson continuously variable duration hidden markov models automatic speech recognition computer speech language vol 
pp 
mar 
reddy speech understanding system trends speech recognition lea editor 
englewood cliffs ni prentice hall pp 

bahl brown de souza mercer maximum mutual information estimation hidden markov model parameters speech recognition proc 
tokyo japan pp 
apr 
dembo rabiner minimum discrimination information approach hidden markov modeling proc 
dallas tx apr 
juang rabiner probabilistic distance measure hidden markov models tech 
vol 
pp 
feb 
rabiner juang levinson sondhi properties continuous hidden markov model representations 
vol 
pp 
july aug 
jelinek mercer interpolated estimation markov source parameters sparse data pattern recognition practice kanal eds 
amsterdam netherlands north holland pp 

schwartz context dependent modeling acoustic phonetic recognition continuous speech conf 
proc 
lnt 
conf 
acoustics speech signal processing pp 
apr 
lee hon large vocabulary speaker independent continuous speech recognition conf 
proc 
lnt 
conf 
acoustics speech signal processing pp 
apr 
rabiner levinson sondhi vector quantization hidden markov models speaker independent isolated word recognition bell syst 
tech 
vol 
pp 
apr 
hidden markov models speaker independent recognition isolated words medium size vocabulary tech 
vol 
pp 
apr 
vector quantization markov source models applied speech recognition proc 
paris france pp 
may 
rabiner juang levinson sondhi recognition isolated digits hidden markov models continuous mixture densities tech 
vol 
pp 
july aug 
richter isolated word recognition proc 
tokyo japan pp 
apr 
lippmann martin paul training robust isolated word speech recognition proc 
dallas tx pp 
apr 
rabiner hidden markov models paul speaker stress resistant hmm isolated word recognizer proc 
dallas tx pp 
apr 
gupta integration acoustic information large vocabulary word recognizer conf 
proc 
lnt 
conf 
acoustics speech signal processing pp 
apr 
levinson continuous speech recognition means acoustic phonetic classification obtained hidden markov model proc 
dallas tx apr 
rabiner martin improved word detection algorithm telephone quality speech incorporating syntactic semantic constraints bell labs tech 
vol 
pp 
mar 
rabiner application hidden markov models automatic speech endpoint detection computer speech language vol 
pp 
sept dec 
experiments word speech recognizer conf 
proc 
lnt 
conf 
acoustics speech signal processing pp 
apr 
atal speech analysis synthesis linear prediction speech wave 
acoust 
soc 
am voi 
pp 

saito analysis synthesis telephony maximum likelihood method proc 
th lnt 
congress acoustics tokyo japan pp 

makhoul linear prediction tutorial review proc 
vol 
pp 

gray jr linear prediction speech 
new york ny springer verlag 
weighted cepstral distance measure speech recognition trans 
acoust 
speech signal processing vol 
assp 
io pp 
oct 
juang rabiner bandpass speech recognition trans 
acoust 
speech signal processing vol 
assp pp 
july 
furui speaker independent isolated word recognition dynamics emphasized cepstrum trans 
japan vol 
pp 
dec 
rosenberg instantaneous transitional spectral information speaker recogni tion proc 
tokyo japan pp 
apr 
rabiner juang segmental means training procedure connected word recognition tech 
vol 
pp 
may june 
rabiner levinson speaker independent syntax directed connected word recognition system hidden markov models level building trans 
acoust 
speech signal processing vol 
assp pp 
june 
rabiner juang model connected digit recognition system hidden mar kov models templates computer speech vol 
pp 
dec 
bourlard kamp ney speaker dependent connected speech recognition dynamic pro gramming statistical methods speech speaker recognition schroeder ed 
basel switzerland kar ger pp 

global connected digit recognition baum welch algorithm proc 
tokyo japan pp 
apr 
context dependent phonetic markov models large vocabulary speech recognition proc 
dallas tx pp 
apr 
merialdo speech recognition large size nary proc 
dallas tx pp 
apr 
chow eta byblos bbn continuous speech rec system proc 
dallas tx pp 
apr 
lawrence rabiner fellow ieee born military communications problems problems brooklyn ny september 
binaural hearing 
presently engaged research speech received degrees recognition digital signal processing techniques bell lab ph degree electrical murray hill 
coauthor books theory engineering massa application digital signal processing prentice hall dig institute technology cam processing speech signals prentice hall multi bridge ma 
rate digital signal processing ice 
participated dr rabiner member eta kappa nu sigma xi tau beta pi cooperative plan electrical engi national academy engineering fellow acoust neering bell laboratories ical society america 
murray hill nj 
worked digital cir proceedings ieee vol 
february 
