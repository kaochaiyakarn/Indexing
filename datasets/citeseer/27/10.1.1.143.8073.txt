generating accurate rule sets global optimization eibe frank department computer science university hamilton new zealand eibe cs waikato ac nz dominant schemes rule learning ripper operate stages 
induce initial rule set re ne complex optimization stage discards adjusts ripper individual rules better 
contrast shows rule sets learned rule time need global optimization 
algorithm inferring rules repeatedly generating partial decision trees combining major paradigms rule generation creating rules decision trees separate conquer rule learning technique 
algorithm straightforward elegant despite experiments standard datasets show produces rule sets accurate similar size generated accurate ripper 
operates ciently avoids postprocessing su er extremely slow performance pathological example sets method criticized 
rules basis popular concept description languages machine learning 
allow knowledge extracted dataset represented form easy people understand 
gives domain experts chance analyze validate knowledge combine ian witten department computer science university hamilton new zealand cs waikato ac nz previously known facts domain 
variety approaches learning rules investigated 
generating decision tree transform rule set nally simplify rules quinlan resulting rule set accurate original tree 
separate conquer strategy pagallo haussler rst applied aq family algorithms michalski subsequently basis rule learning systems furnkranz 
essence strategy determines powerful rule underlies dataset separates examples covered repeats procedure remaining examples 
dominant practical implementations emerged strands research quinlan ripper cohen 
perform global optimization process set rules induced initially 
motivation initial rule set generated decision tree unduly large redundant drops individual rules having previously optimized rules locally dropping conditions 
motivation ripper hand increase accuracy rule set replacing revising individual rules 
case stage nature algorithm remains cohen puts rules start initial model iteratively improve heuristic techniques 
experiments show size performance rule sets signi cantly improved post induction optimization 
hand process complex heuristic 
presents rule induction procedure avoids global optimization produces accurate compact rule sets 
method combines rule learning paradigms identi ed 
section discusses paradigms incarnation ripper 
section presents new algorithm call part partial decision trees 
section describes experimental evaluation standard datasets comparing part ripper commercial successor 
section summarizes ndings 
related review basic strategies producing rule sets 
rst creating decision tree transform rule set generating rule path root leaf 
rule sets derived way simpli ed dramatically losing predictive accuracy 
unnecessarily complex disjunctions imply expressed succinctly decision tree 
known replicated subtree problem pagallo haussler 
obtaining rule set rst transforms unpruned decision tree set rules aforementioned way 
rule simpli ed separately greedily deleting conditions order minimize rule estimated error rate 
rules class turn considered subset sought guided criterion minimum description length principle rissanen performed greedily replacing earlier method simulated annealing 
step ranks subsets di erent classes respect avoid con icts determines default class 
rules greedily deleted rule set long decreases rule set error training data 
process complex time consuming 
separate stages required produce nal rule set 
shown noisy datasets runtime cubic number instances cohen 
despite lengthy optimization process rules restricted conjunctions attribute value tests occur path initial decision tree 
separate conquer algorithms represent direct approach learning decision rules 
generate rule time remove instances cov test version available www com 
ered rule iteratively induce rules remaining instances 
multi class setting automatically leads ordered list rules type classi er termed decision list rivest 
various di erent pruning methods separate conquer algorithms investigated furnkranz shows prune immediately generated separate stopping criterion determine cease adding rules furnkranz widmer :10.1.1.10.1780
originally formulated class problems procedure applied directly multi class settings building rules separately class ordering appropriately cohen 
ripper implements strategy reduced error pruning quinlan sets training data aside determine drop tail rule incorporates heuristic minimum description length principle stopping criterion 
follows rule induction post processing step revises rule set closely approximate obtained expensive global pruning strategy 
considers replacing revising individual rules guided error modi ed rule set pruning data 
decides leave original rule substitute replacement revision decision minimum description length heuristic 
claimed cohen rip generates rule sets accurate 
experiments large collection standard datasets reported section con rm 
example shows basic strategy building single rule pruning back lead particularly problematic form call hasty generalization 
pruning interacts covering heuristic 
generalizations implications known covering heuristic prevents learning algorithm discovering implications 
simple example hasty generalization 
consider boolean dataset attributes built rules corrupted class noise 
assume pruning operator conservative delete single nal conjunction rule time entire tail conjunctions ripper 
assume rst rule training set coverage pruning set true false true false false target concept noisy domain 
rule generated pruned back true training data solely scenario plausible 
consider rule pruned 
error rate pruning set null rule error rate smaller 
rule set pruned back single trivial rule patently accurate rule set shown 
hasty generalization just artifact reduced error pruning happen pessimistic pruning quinlan 
variation number noisy instances data sample construct situations pruning causes rules comparatively large coverage swallow rules smaller signi cant coverage 
happen number errors committed rule large compared total number instances covered adjacent rule 
obtaining rules partial decision trees new method rule induction part combines approaches discussed section attempt avoid respective problems 
ripper need perform global optimization produce accurate rule sets added simplicity main advantage 
adopts separate conquer strategy builds rule removes instances covers continues creating rules recursively remaining instances left 
di ers standard approach way rule created 
essence asingle rule pruned decision tree built current set instances leaf largest coverage rule tree discarded 
avoids hasty generalization generalizing implications known subtrees expanded 
prospect repeatedly building decision trees discard bizarre rst 
pruned tree obtain rule building incrementally adding conjunctions time avoids pruning problem basic separate conquer rule learner 
conquer methodology conjunction decision trees adds exibility speed 
wasteful build full decision tree just obtain single rule process accelerated signi cantly sacri cing advantages 
key idea build partial decision tree fully explored 
partial decision tree ordinary decision tree contains branches unde ned subtrees 
generate tree integrate construction pruning operations order nd stable subtree simpli ed 
subtree tree building ceases single rule read tree building algorithm summarized splits set examples recursively partial tree 
rst step chooses test divides examples subsets accordingly 
implementation choice exactly way 
subsets expanded order average entropy starting smallest 
reason subsequent subsets expanded subset low average entropy result small subtree produce general rule 
continues recursively subset expanded leaf procedure expand subset choose split set examples subsets subsets expanded subsets expanded far leaves choose subset expanded expand subsets expanded leaves estimated error subtree estimated error node undo expansion subsets node leaf method expands set examples partial tree stage stage stage stage stage example algorithm builds partial tree continues backtracking 
soon internal node appears children expanded leaves pruning begins algorithm checks node better replaced single leaf 
just standard subtree replacement operation decision tree pruning implementation decision exactly way 
pruning operation subtree raising plays part algorithm 
replacement performed algorithm backtracks standard way exploring siblings newly replaced node 
backtracking node encountered children leaves happen soon potential subtree replacement isnot performed remaining subsets left unexplored corresponding subtrees left 
due recursive structure algorithm event automatically terminates tree generation 
shows step step example 
stages tree building continues recursively normal way entropy sibling chosen expansion node stages 
gray unexpanded black ones leaves 
stages black node lower entropy sibling node expanded leaf 
backtracking occurs node chosen expansion 
stage reached node node children expanded leaves triggers pruning 
subtree replacement node considered accepted leading stage 
node considered subtree replacement operation accepted 
backtracking continues node having lower entropy expanded leaves 
subtree replacement considered node suppose node replaced 
point process ectively terminates leaf ab bcd irrelevant binary attributes uniformly distributed examples part class noise part class noise log cpu time secs number examples cpu times part arti cial dataset partial tree stage 
procedure ensures pruning ect discussed section occur 
node pruned successors leaves 
happen subtrees explored leaves pruned back 
situations shown handled correctly 
dataset noise free contains instances prevent algorithm doing pruning just path full decision tree explored 
achieves greatest possible performance gain naive method builds full decision tree time 
gain decreases pruning takes place 
datasets numeric attributes asymptotic time complexity algorithm building full decision tree case complexity dominated time needed sort attribute values rst place 
partial tree built single rule extracted 
leaf corresponds possible rule seek best leaf subtrees typically small minority expanded leaves 
implementation aims general rule choosing leaf covers greatest number instances 
wehave experimented choosing assuming subtree raising 
accurate rule leaf lowest error rate error estimated bernoulli heuristic improve rule set accuracy 
datasets contain missing attribute values practical learning schemes deal 
constructing partial tree treat missing values exactly way instance assigned deterministically branch missing attribute value assigned branches weight proportional number training instances going branch normalized total number training instances known values node 
testing apply procedure separately rule associating weight application rule test instance 
weight deducted instance total weight passed rule list 
weight reduced zero predicted class probabilities combined nal classi cation weights 
algorithm runtime depends number rules generates 
decision tree built time log dataset examples attributes time taken generate rule set size 
assuming analyses cohen furnkranz size nal theory constant time complexity log compared table datasets experiments dataset instances missing numeric nominal classes values attributes attributes anneal audiology australian autos balance scale breast cancer breast german glass glass heart heart heart statlog hepatitis horse colic hypothyroid ionosphere iris kr vs kp labor lymphography mushroom pima indians primary tumor segment sick sonar soybean splice vehicle vote vowel waveform noise zoo log ripper 
practice number rules grows size training data greedy rule learning strategy pessimistic pruning 
worst case number rules increases linearly training examples complexity log 
experiments observed subquadratic run times arti cial dataset cohen show performance cubic number examples 
results timing method part dataset depicted log log scale class noise percent class noise 
case scales cube number examples 
experimental results order evaluate performance part set practical learning problems performed experiments standard datasets uci collection merz murphy 
datasets characteristics listed table 
learning algorithm part described ran ripper datasets 
results listed table 
give percentage correct classi cations averaged fold cross validation runs standard holte holte variant glass dataset classes combined classes deleted horse colic dataset attributes deleted attribute class 
deleted identi er attributes datasets 
revision 
table experimental results percentage correct classi cations standard deviation dataset part ripper anneal audiology australian autos balance scale breast cancer breast horse colic german glass glass heart heart heart statlog hepatitis hypothyroid ionosphere iris kr vs kp labor lymphography mushroom pima indians primary tumor segment sick sonar soybean splice vehicle vote vowel waveform noise zoo deviations shown 
folds scheme 
results ripper marked show significant improvement corresponding results part show signi cant degradation 
marks discussed 
speak results signi cantly di erent di erence statistically signi cant level paired sided test pair data points consisting estimates obtained fold cross validation run learning schemes compared 
table shows different methods compare 
entry results part hypothyroid data part mushroom data fact di er second decimal place 
indicates number datasets method associated column signi cantly accurate method associated row 
observe table part outperforms datasets outperforms part 
chance probability distribution sign test weak evidence part outperforms collection datasets similar 
table part cantly accurate datasets signi cantly accurate 
corresponding probability distribution providing weak evidence performs better part 
ripper situation different part outperforms fourteen datasets performs worse 
probability distribution value provides fairly strong evidence part outperforms ripper collection datasets type 
table results paired tests number indicates method column signi cantly outperforms method row part ripper part ripper accuracy size rule set important strong uence comprehensibility 
marks table give information relative size rule sets produced mark learning schemes datasets average part generates fewer rules occurs ripper 
compared average number rules generated part smaller eighteen datasets larger sixteen 
simple surprisingly ective method learning decision lists repeated generation partial decision trees separate conquer manner 
main advantage part schemes discussed performance simplicity combining paradigms rule learning produces rule sets need global optimization 
despite simplicity method produces rule sets compare favorably generated accurate larger produced rip 
interesting question research size rule sets obtained method decreased employing stopping criterion minimum description length principle done ripper reduced error pruning pessimistic pruning 
anonymous reviewers comments signi cantly improved exposition 
people donated datasets uci repository 
cohen 

fast ective rule induction 
proceedings th international conference machine learning pp 

morgan kaufmann 
furnkranz 

separate conquer rule learning 
technical report tr austrian research institute arti cial intelligence vienna 
ftp ftp ai univie ac papers tr ps 
furnkranz 

pruning algorithms rule learning 
machine learning 
furnkranz widmer 

incremental reduced error pruning 
proceedings th international conference machine learning pp 

morgan kaufmann 
holte 

simple classi cation rules perform commonly datasets 
machine learning 
merz murphy 

uci repository machine learning data bases 
irvine ca university california department information computer science 
www 
ics uci edu mlearn mlrepository html 
michalski 

quasi minimal solution covering problem 
proceedings th international symposium information processing vol 
switching circuits pp 

bled yugoslavia 
pagallo haussler 

boolean feature discovery empirical learning 
machine learning 
quinlan 

generating production rules decision trees 
proceedings th international joint conference arti cial intelligence pp 

morgan kaufmann 
quinlan 

simplifying decision trees 
international journal man machine studies 
quinlan 

programs machine learning 
san mateo ca morgan kaufmann 
rissanen 

modelling shortest data description 
automatica 
rivest 

learning decision lists 
machine learning 
