extended journal submission appeared inproceedings thirteenth annual conference theory 
logistic regression adaboost bregman distances michael collins labs research shannon laboratory park avenue room florham park nj research att com robert schapire labs research shannon laboratory park avenue room florham park nj schapire research att com yoram singer school computer science engineering hebrew university jerusalem israel singer cs huji ac il october give unified account boosting logistic regression learning problem cast terms optimization bregman distances 
striking similarity problems framework allows design analyze algorithms simultaneously easily adapt algorithms designed problem 
problems give new algorithms explain potential advantages existing methods 
algorithms divided types parameters iteratively updated sequentially time parallel 
describe parameterized family algorithms interpolates smoothly extremes 
algorithms give convergence proofs general formalization auxiliary function proof technique 
sequential update algorithms equivalent adaboost provides general proof convergence adaboost 
show algorithms generalize easily multiclass case contrast new algorithms iterative scaling 
conclude experimental results synthetic data highlight behavior old newly proposed algorithms different settings 
give unified account boosting logistic regression show learning problems cast terms optimization bregman distances 
framework problems similar real difference choice bregman distance unnormalized relative entropy boosting binary relative entropy logistic regression 
similarity problems framework allows design analyze algorithms simultaneously 
able borrow methods maximum entropy literature logistic regression apply exponential loss adaboost especially convergence proof techniques 
conversely easily adapt boosting methods problem minimizing logistic loss logistic regression 
result family new algorithms problems convergence proofs new algorithms adaboost 
adaboost logistic regression attempt choose parameters weights associated family functions called features weak hypotheses 
adaboost works sequentially updating parameters methods logistic regression notably iterative scaling update parameters parallel iteration 
new algorithm method optimizing exponential loss parallel updates 
plausible parallel update method converge faster sequential update method provided number features large parallel updates infeasible 
experiments described suggest case 
second algorithm parallel update method logistic loss 
parallel update algorithms known function updates derive new 
unified treatment give exponential logistic loss functions able prove convergence algorithms losses simultaneously 
true algorithms 
describe analyze sequential update algorithms loss functions 
exponential loss algorithm equivalent adaboost algorithm freund schapire :10.1.1.32.8918
viewing algorithm framework able prove adaboost correctly converges minimum exponential loss function 
new result kivinen warmuth mason convergence proofs adaboost proofs depend assumptions minimization problem may hold cases 
proof holds general assumptions 
unified view leads instantly sequential update algorithm logistic regression minor modification adaboost similar proposed duffy helmbold 
adaboost algorithm conjunction classification algorithm usually called weak learning algorithm accept distribution examples return weak hypothesis low error rate respect distribution 
new algorithm provably minimizes logistic loss arguably natural exponential loss adaboost 
potentially important advantage new algorithm logistic regression weights places examples bounded suggests may possible new algorithm setting boosting algorithm selects examples weak learning algorithm filtering stream examples large dataset 
pointed watanabe watanabe possible adaboost weights may extremely large :10.1.1.32.8918
provide modification adaboost purpose weights truncated new algorithm may viable cleaner alternative 
describe parameterized family iterative algorithms includes parallel algorithms interpolates smoothly extremes 
convergence proof give holds entire family algorithms 
considers binary case just possible labels associated example turns multiclass case requires additional 
algorithms convergence proofs give binary case turn directly applicable multiclass case modification 
comparison describe generalized iterative scaling algorithm darroch ratcliff 
procedure setting able relax main assumptions usually required algorithm 
organized follows section describes boosting logistic regression models usually formulated 
section gives background optimization bregman distances section describes boosting logistic regression cast framework 
section gives parallel update algorithms proofs convergence section gives algorithms convergence proofs 
parameterized family iterative algorithms described section 
extension multiclass problems section 
section contrast methods iterative scaling 
section give initial experiments demonstrate qualitative behavior various variants different settings 
previous variants sequential update algorithms fit general family arcing algorithms breiman mason anyboost family algorithms 
information geometric view take shows algorithms study including adaboost fit family algorithms described bregman satisfying set constraints directly general setting lafferty della pietra della pietra attempts solve optimization problems general bregman distances 
gave method deriving analyzing parallel update algorithms setting functions 
algorithms convergence proofs method 
builds previous papers compared boosting approaches logistic regression 
friedman hastie tibshirani noted similarity boosting logistic regression loss functions derived sequential update algorithm logitboost logistic loss :10.1.1.30.3515
algorithm theirs requires weak learner solve squares problems classification problems 
sequential update algorithm different related problem proposed cesa bianchi krogh warmuth 
duffy helmbold gave conditions loss function gives boosting algorithm 
showed minimizing logistic loss lead boosting algorithm pac sense suggests algorithm problem close theirs may turn pac boosting property 
lafferty went studying relationship logistic regression exponential loss family bregman distances :10.1.1.17.4755
setting described apparently extended precisely include exponential loss 
bregman distances describe important differences leading natural treatment exponential loss new view logistic regression 
builds heavily kivinen warmuth lafferty connection adaboost information geometry 
showed update specifically bregman describes optimization methods bregman distances constraint satisfied iteration example method constraint impact objective function greedily chosen iteration 
simplest version adaboost assumes weak hypotheses values algorithm type assume weak learner able choose weak hypothesis minimum weighted error 
adaboost form entropy projection bregman distance differed slightly chosen normalized relative entropy unnormalized relative entropy adaboost fit model quite complete particular convergence proof depended assumptions hold general 
kivinen warmuth described updates general bregman distances including examples bregman distance capture logistic regression 
boosting logistic models loss functions set training examples instance belongs domain instance space label assume set real valued functions convention maximum entropy literature call functions features boosting literature called weak base hypotheses 
note terminology literature features correspond entire space base hypotheses merely base hypotheses previously weak learner 
study problem approximating linear combination features 
interested problem finding vector parameters approximation measure goodness approximation varies task mind 
attempt classification problems natural try match sign minimize true 
minimization number classification errors may worthwhile goal general form problem intractable see instance 
advantageous minimize nonnegative loss function 
instance boosting algorithm adaboost exponential loss verified eq :10.1.1.32.8918
upper bounded eq 
loss easier demonstrated adaboost 
briefly series rounds adaboost uses oracle subroutine called weak learning algorithm pick feature weak hypothesis associated parameter updated 
noted breiman various authors steps done way approximately cause greatest decrease exponential loss :10.1.1.30.3515
show time adaboost fact provably effective method finding parameters minimize exponential loss assuming weak learner chooses best 
give entirely new algorithm minimizing exponential loss round parameters updated parallel time 
hope parallel update algorithm faster sequential update algorithm see section preliminary experiments regard 
classification rule postulate stochastically function attempt generated estimate probability associated label studied way doing pass logistic function estimate likelihood labels occuring sample maximizing likelihood equivalent minimizing log loss model generalized improved iterative scaling popular parallel update methods minimizing loss 
give alternative parallel update algorithm compare iterative scaling techniques preliminary experiments section 
bregman distance optimization section give background optimization bregman distances 
form unifying basis study boosting logistic regression 
particular set follow taken primarily lafferty della pietra della pietra 
continuously differentiable strictly convex function defined closed convex set bregman distance associated defined instance unnormalized relative entropy shown general bregman distance nonnegative equal zero arguments equal 
natural optimization problem associated bregman distance find vector closest vector subject set linear constraints 
constraints specified matrix vector vectors satisfying constraints problem find convex dual problem gives alternative formulation 
problem find vector particular form closest vector form vectors defined legendre transform written legendre transform function maps calculus seen equivalent instance unnormalized relative entropy verified calculus note order eq 
solution cases need assume bijective mapping interior assumption remainder 
eq 
bijective property shown transform useful additive property matrix vector consider vectors obtained legendre transform linear combination columns vector vectors set dual optimization problem stated problem finding closure remarkable fact optimization problems solutions solution turns unique point intersection take statement theorem lafferty della pietra della pietra 
result appears due csisz 
proof case normalized relative entropy della pietra della pietra lafferty :10.1.1.43.7345
see csisz survey article censor zenios book :10.1.1.43.7345
theorem unique 

satisfying 
assume properties determines uniquely 
exists theorem extremely useful proving convergence algorithms described 
show section boosting logistic regression viewed optimization problems type part theorem 
prove optimality need show algorithms converge point boosting logistic regression revisited return boosting logistic regression problems outlined section show cast form optimization problems outlined 
recall boosting goal find minimized precisely minimum attained finite seek procedure causes function converge infimum 
shorthand call finding sequence problem 
view problem form section vectors 
follows take eq 
noted earlier case furthermore trivial see space unnormalized relative entropy 
eq 

particular means equal eq 

minimizing lent minimizing eq 

theorem equivalent finding equiva satisfying constraints logistic regression reduced optimization problem form nearly way 
recall goal find sequence minimize shorthand call logloss problem 
define exactly exponential loss 
vector constant defined space restricted minor differences 
important difference choice function resulting bregman distance parameters satisfying assumptions input matrix output update parameters trivially parallel update optimization algorithm 
choice verified calculus constraints eq 

equivalent minimizing eq 

finding parallel optimization methods equal eq 
minimizing satisfying section describe new algorithm logloss problems iterative method weights updated iteration 
algorithm shown fig 

algorithm function satisfying certain conditions described particular see choices section 
really single algorithm loss minimization problems setting parameters appropriately 
note loss generality assume section instances algorithm simple 
iteration vector computed shown added parameter vector assume algorithms inputs infinite valued updates occur 
algorithm new minimization problems 
optimization methods notably adaboost generally involved updates feature time 
parallel update methods logloss known see example 
updates take different form usual updates derived logistic models discuss differences section 
useful point distribution simple function previous distribution eq 
gives logloss respectively 
prove algorithm fig 
converges optimality loss 
prove abstractly matrix vector function satisfying assumptions assumption assumption bounded 
set show choices section satisfy assumptions allow prove convergence logloss 
prove convergence auxiliary function technique della pietra della pietra lafferty :10.1.1.43.7345
roughly idea proof derive nonnegative lower bound called auxiliary function loss decreases iteration 
loss increases lower bounded zero auxiliary function converge zero 
final step show auxiliary function zero constraints defining set satisfied theorem converged optimality 
formally define auxiliary function sequence matrix contin uous function satisfying conditions proving convergence specific algorithms prove lemma shows roughly sequence auxiliary function sequence converges optimum point proving convergence specific algorithm reduces simply finding auxiliary function 
lemma auxiliary function matrix assume lie compact eq 
particular case assumption holds subspace proof condition nonincreasing sequence 
case bregman distances bounded zero 
sequence differences converge zero 
condition means converge zero 
assume lie compact space sequence subsequence converging point continuity eq 

hand limit sequence points theorem 
condition argument uniqueness show single limit point suppose entire sequence converge find open set containing set entire sequence converges apply lemma prove convergence algorithm fig 

contains infinitely points limit point closed different argued impossible 
theorem satisfy assumptions assume generated algorithm fig 

eq 

proof auxiliary function claim function clearly continuous nonpositive 
sequences upper bound change round follows eqs 
follow eq 
assumption respectively 
eq 
uses fact jensen inequality applied convex function eq 
uses definitions eq 
uses choice chosen specifically minimize eq 

auxiliary function theorem follows immediately lemma 
apply theorem logloss problems need verify assumptions satisfied 
starting assumption logloss parameters satisfying assumptions input matrix output update parameters sequential update optimization algorithm 
second equalities eqs 
respectively 
final inequality uses assumption holds trivially logloss clearly defines bounded subset sequential algorithms bounded 
section describe algorithm minimization problems described section 
algorithm section updates weight feature time 
parallel update algorithm may give faster convergence features sequential update algorithm large number features oracle selecting feature update 
instance adaboost essentially equivalent sequential update algorithm uses assumed weak learning algorithm select weak hypothesis features 
sequential algorithm logloss exactly way 
algorithm shown fig 

theorem assumptions theorem algorithm fig 
converges optimality sense theorem 
proof theorem auxiliary function function clearly continuous nonpositive 
eq 
uses convexity eq 
uses choice chose minimize bound eq 

auxiliary function theorem follows lemma 
mentioned algorithm essentially equivalent adaboost specifically version adaboost freund schapire :10.1.1.32.8918
adaboost iteration distribution training examples computed weak learner seeks weak hypothesis low error respect distribution 
algorithm section assumes space weak hypotheses weak learner succeeds selecting feature consists features assigned example adaboost exactly equal hypothesis equal lowest error accurately error farthest 
translating notation weight weighted error th weak theorem proof adaboost converges minimum exponential loss assuming idealized weak learner form 
note theorem tells exact form know limiting behavior know limiting behavior parameters 
section new algorithm logistic regression 
fact algorithm duffy helmbold choice practical terms little required alter existing learning system adaboost uses logistic loss exponential loss difference manner computed parameters satisfying assumptions input matrix satisfying condition define output update parameters parameterized family iterative optimization algorithms 
easily convert system slipper boostexter alternating trees logistic loss :10.1.1.33.1184
systems confidence rated boosting chosen round minimize eq 
approximation expression algorithm fig 

note proof theorem easily modified prove convergence algorithm auxiliary function 
parameterized family iterative algorithms previous sections described separate parallel update sequential update algorithms 
section describe parameterized family algorithms includes parallel update algorithm section sequential update algorithm different section 
family algorithms includes algorithms may appropriate certain situations explain 
algorithm shown fig 
similar parallel update algorithm fig 

round quantities computed vector computed computed fig 

vector added directly vector selected provides scaling features 
vector chosen maximize measure progress restricted belong set allowed form scaling vectors set parameter algorithm restriction vectors satisfying constraint parallel update algorithm fig 
obtained choosing assuming 
equivalently assumption choose obtain sequential update algorithm choosing set unit vectors update component equal equal assuming interesting case assume natural choose ensures maximization solved analytically giving update dual norms final case restrict scaling vectors choose maximization problem solved choose linear programming problem variables constraints 
prove convergence entire family algorithms 
idea generalizes easily case case theorem assumptions theorem algorithm fig 
converges optimality sense theorem 
proof auxiliary function change theorem 
function continuous nonpositive 
bound technique theorem exists implies applying lemma completes theorem 
multiclass problems section show results extended multiclass case 
generality preceding results see new algorithms need devised new convergence proofs need proved case 
preceding algorithms proofs directly applied multiclass case 
multiclass case label set cardinality feature form logistic regression model loss training set transform framework follows vectors pairs dimensional indexed convex function case denote defined space resulting bregman distance clearly shown assumption verified noting plugging definitions gives equal eq 

algorithms sections solve minimization problem corresponding convergence proofs directly applicable 
multiclass versions adaboost 
adaboost special case adaboost loss function loss similar set choice fact binary adaboost :10.1.1.32.8918
merely changed index set choosing multiclass logistic regression equal loss eq 

preceding algorithms solve multiclass problem 
particular sequential update algorithm gives adaboost 
adaboost mh multiclass version adaboost 
adaboost mh replace index set example label loss function adaboost mh multiclass version adaboost 
define binary adaboost comparison iterative scaling 
obtain section describe generalized iterative scaling gis procedure darroch ratcliff comparison algorithms 
largely follow description gis berger della pietra della pietra multiclass case :10.1.1.103.7637
comparison stark possible gis notation prove convergence methods developed previous sections 
doing able relax key assumptions traditionally studying gis 
adopt notation set multiclass logistic regression section 
knowledge analog gis exponential loss consider case logistic loss 
extend notation defining defined verified gis assumptions regarding features usually defined eq 
section prove gis converges second condition replaced milder multiclass case constant added features changing model loss function features scaled constant assumptions consider clearly hold loss generality 
improved iterative scaling algorithm della pietra della pietra lafferty requires milder assumptions complicated implement requiring numerical search newton raphson feature iteration :10.1.1.43.7345
gis works parallel update algorithm section defined multiclass logistic regression section 
difference computation vector updates gis requires direct access features specifically gis defined clearly updates quite different updates described 
notation sections reformulate framework follows prove convergence updates usual auxiliary function method 
theorem 
modified gis algorithm described converges optimality sense theorem 
proof show function vectors computed gis 
clearly continuous usual nonnegativity properties unnormalized relative entropy imply equality eq 
implies constraints proof theorem 
remains shown introduce notation rewrite gain follows eq 
plugging definitions term eq 
written derive upper bound second term eq 
eq 
follows log bound eq 
follows definition update eq 
uses eq 
assumption form combining eqs 
gives eq 
completing proof 
clear differences gis updates stem eq 
derived th term sum 
choice effectively means log bound taken different point 
general case bound exact varies updates 
experiments varying varies bound taken section briefly describe experiments synthetic data 
experiments preliminary intended suggest possibility algorithms having practical value 
systematic experiments clearly needed real world synthetic data comparing new algorithms commonly procedures 
experiments generated random data classified noisy hyperplane 
specifically class case generated random hyperplane dimensional space represented vector chosen uniformly random unit sphere 
chose points case real valued features point normally distributed case boolean features point chosen uniformly random boolean hypercube assigned label point depending fell chosen hyperplane label chosen perturbed point case real valued features adding random amount boolean features flipped coordinate independently probability note forms perturbation effect causing labels points near separating hyperplane noisy points farther 
features identified coordinates real valued features conducted similar experiment involving classes 
case generated random hyperplanes chosen uniformly random unit sphere classified point prior perturbing 
experiments limited weight vector depend just possible features 
set experiments tested algorithms see effective minimizing logistic loss training data 
ran parallel update algorithm section denoted par figures sequential update algorithm special case parameterized family described section denoted seq 
ran iterative scaling algorithm described section 
run sequential update algorithm section preliminary experiments consistently perform worse sequential update algorithm section 
noted section gis requires features nonnegative 
features satisfy constraint subtract constant feature changing model eq 
new set features new features define identical model old features result change denominator numerator eq 
multiplied constant realvalued features classes realvalued features classes boolean features classes relevant features relevant features training loss training loss training loss training loss training loss training loss training logistic loss data generated noisy hyperplanes various methods 
seq par realvalued features classes realvalued features classes boolean features classes relevant features relevant features test error test error test error test misclassification error data generated noisy hyperplanes 
test error test error test error log seq exp seq log par exp par slightly obvious approach choose feature transformation approach causes nonnegative affecting model eq 
numerator eq 
multiplied 
note case constants consequence testing ignored training complete 
preliminary version experiments approach gis performed uniformly considerably worse algorithms tested 
publication version tried method making features nonnegative obtained better performance 
experiments current approach 
results set experiments shown fig 

plot shows logistic loss training set methods function number iterations 
loss normalized plot corresponds different variation generating data described 
small number relevant features sequential update algorithms clear advantage relevant features methods best board 
course methods eventually converge level loss 
second experiment tested effective new competitors adaboost minimizing test misclassification error 
experiment parallel sequential update algorithms denoted par seq cases variants exponential loss exp logistic loss log 
fig 
shows plot classification error separate test set examples 
relevant features methods overfit data high level noise 
relevant features large difference performance exponential logistic variants algorithms parallel update variants clearly better early go right solution acknowledgments manfred warmuth teaching bregman distances comments earlier draft 
nigel duffy david helmbold raj iyer john lafferty helpful discussions suggestions 
research done yoram singer labs 
adam berger stephen della pietra vincent della pietra :10.1.1.103.7637
maximum entropy approach natural language processing 
computational linguistics 
bregman 
relaxation method finding common point convex sets application solution problems convex programming 
computational mathematics mathematical physics 
leo breiman 
arcing edge 
technical report statistics department university california berkeley 
appeared proceedings thirteenth annual conference computational learning theory 
leo breiman 
prediction games arcing classifiers 
technical report statistics department university california berkeley 
yair censor stavros zenios 
parallel optimization theory algorithms applications 
oxford university press 
nicol cesa bianchi anders krogh manfred warmuth 
bounds approximate steepest descent likelihood maximization exponential families 
ieee transactions information theory july 
william cohen yoram singer :10.1.1.33.1184
simple fast effective rule learner 
proceedings sixteenth national conference artificial intelligence 
csisz divergence geometry probability distributions minimization problems 
annals probability 
csisz sanov property generalized projection conditional limit theorem 
annals probability 
csisz maxent mathematics information theory :10.1.1.43.7345
proceedings fifteenth international workshop maximum entropy bayesian methods pages 
darroch ratcliff 
generalized iterative scaling log linear models 
annals mathematical statistics 
stephen della pietra vincent della pietra john lafferty :10.1.1.43.7345
inducing features random fields 
ieee transactions pattern analysis machine intelligence april 
carlos watanabe :10.1.1.32.8918
scaling boosting learner adaptive sampling 
proceedings fourth pacific asia conference knowledge discovery data mining 
nigel duffy david helmbold 
potential 
advances neural information processing systems 
yoav freund mason 
alternating decision tree learning algorithm 
machine learning proceedings sixteenth international conference pages 
yoav freund robert schapire :10.1.1.32.8918
decision theoretic generalization line learning application boosting 
journal computer system sciences august 
jerome friedman trevor hastie robert tibshirani :10.1.1.30.3515
additive logistic regression statistical view boosting 
annals statistics appear 
klaus 
hans 
simon 
robust single neurons 
proceedings fifth annual acm workshop computational learning theory pages july 
kivinen manfred warmuth 
boosting entropy projection 
proceedings twelfth annual conference computational learning theory pages 
john lafferty :10.1.1.17.4755
additive models boosting inference generalized divergences 
proceedings twelfth annual conference computational learning theory pages 
john lafferty stephen della pietra vincent della pietra 
statistical learning algorithms bregman distances 
proceedings canadian workshop information theory 
mason jonathan baxter peter bartlett marcus frean 
functional gradient techniques combining hypotheses 
advances large margin classifiers 
mit press 
tsch onoda 
ller 
soft margins adaboost 
machine learning appear 
robert schapire yoram singer 
improved boosting algorithms confidence rated predictions 
machine learning december 
robert schapire yoram singer 
boostexter boosting system text categorization 
machine learning may june 

information theoretical optimization techniques 

watanabe 
computational learning theory discovery science 
proceedings th international colloquium automata languages programming pages 

