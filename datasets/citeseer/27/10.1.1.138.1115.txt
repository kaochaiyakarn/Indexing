gradient learning applied document recognition yann lecun member ieee bottou bengio patrick haffner invited multilayer neural networks trained back propagation algorithm constitute best example successful learning technique 
appropriate network architecture gradient learning algorithms synthesize complex decision surface classify high dimensional patterns handwritten characters minimal preprocessing 
reviews various methods applied handwritten character recognition compares standard handwritten digit recognition task 
convolutional neural networks specifically designed deal variability dimensional shapes shown outperform techniques 
real life document recognition systems composed multiple modules including field extraction segmentation recognition language modeling 
new learning paradigm called graph transformer networks gtn allows systems trained globally gradient methods minimize performance measure 
systems online handwriting recognition described 
experiments demonstrate advantage global training flexibility graph transformer networks 
graph transformer network reading bank check described 
uses convolutional neural network character recognizers combined global training techniques provide record accuracy business personal checks 
deployed commercially reads checks day 
keywords convolutional neural networks document recognition finite state transducers gradient learning graph transformer networks machine learning neural networks optical character recognition ocr 
nomenclature gt graph transformer 
gtn graph transformer network 
hmm hidden markov model 
hos heuristic oversegmentation 
nn nearest neighbor 
manuscript received november revised april 
lecun bottou haffner speech image processing services research laboratory labs research red bank nj usa 
bengio partement informatique de recherche op universit de montr montr qu bec canada 
publisher item identifier 
ieee nn neural network 
ocr optical character recognition 
pca principal component analysis 
rbf radial basis function 
rs svm reduced set support vector method 
sdnn space displacement neural network 
svm support vector method 
tdnn time delay neural network 
svm virtual support vector method 
years machine learning techniques particularly applied nn played increasingly important role design pattern recognition systems 
fact argued availability learning techniques crucial factor success pattern recognition applications continuous speech recognition handwriting recognition 
main message better pattern recognition systems built relying automatic learning hand designed heuristics 
possible progress machine learning computer technology 
character recognition case study show hand crafted feature extraction advantageously replaced carefully designed learning machines operate directly pixel images 
document understanding case study show traditional way building recognition systems manually integrating individually designed modules replaced unified principled design paradigm called gtn allows training modules optimize global performance criterion 
early days pattern recognition known variability richness natural data speech glyphs types patterns impossible build accurate recognition system entirely hand 
consequently pattern recognition systems built combination automatic learning techniques hand crafted algorithms 
usual method proceedings ieee vol 
november fig 

traditional pattern recognition performed modules fixed feature extractor trainable classifier 
recognizing individual patterns consists dividing system main modules shown fig 

module called feature extractor transforms input patterns represented low dimensional vectors short strings symbols easily matched compared relatively invariant respect transformations distortions input patterns change nature 
feature extractor contains prior knowledge specific task 
focus design effort entirely hand crafted 
classifier hand general purpose trainable 
main problems approach recognition accuracy largely determined ability designer come appropriate set features 
turns daunting task unfortunately redone new problem 
large amount pattern recognition literature devoted describing comparing relative merits different feature sets particular tasks 
historically need appropriate feature extractors due fact learning techniques classifiers limited low dimensional spaces easily separable classes 
combination factors changed vision decade 
availability low cost machines fast arithmetic units allows reliance brute force numerical methods algorithmic refinements 
second availability large databases problems large market wide interest handwriting recognition enabled designers rely real data hand crafted feature extraction build recognition systems 
third important factor availability powerful machine learning techniques handle high dimensional inputs generate intricate decision functions fed large data sets 
argued progress accuracy speech handwriting recognition systems attributed large part increased reliance learning techniques large training data sets 
evidence fact large proportion modern commercial ocr systems form multilayer nn trained back propagation 
study consider tasks handwritten character recognition sections ii compare performance learning techniques benchmark data set handwritten digit recognition section iii 
automatic learning beneficial learning technique succeed minimal amount prior knowledge task 
case multilayer nn way incorporate knowledge tailor architecture task 
convolutional nn introduced section ii example specialized nn architectures incorporate knowledge invariances twodimensional shapes local connection patterns imposing constraints weights 
comparison methods isolated handwritten digit recognition section iii 
go recognition individual characters recognition words sentences documents idea combining multiple modules trained reduce error introduced section iv 
recognizing variable length objects handwritten words systems best done modules manipulate directed graphs 
leads concept trainable gtn introduced section iv 
section describes classical method hos recognizing words character strings 
discriminative gradient techniques training recognizer word level requiring manual segmentation labeling section vi 
section vii presents promising space displacement nn approach eliminates need segmentation heuristics scanning recognizer possible locations input 
section viii shown trainable gtn formulated multiple generalized transductions general graph composition algorithm 
connections gtn hmm commonly speech recognition treated 
section ix describes globally trained gtn system recognizing handwriting entered pen computer 
problem known online handwriting recognition machine produce immediate feedback user writes 
core system convolutional nn 
results clearly demonstrate advantages training recognizer word level training hand labeled isolated characters 
section describes complete gtn system reading handwritten machine printed bank checks 
core system convolutional nn called lenet described section ii 
system commercial ncr line check recognition systems banking industry 
reading millions checks month banks united states 
learning data approaches automatic machine learning successful approaches popularized years nn community called numerical gradient learning 
learning machine computes function th input pattern represents collection adjustable parameters system 
pattern recognition setting lecun gradient learning applied document recognition output may interpreted recognized class label pattern scores probabilities associated class 
loss function measures discrepancy correct desired output pattern output produced system 
average loss function average errors set labeled examples called training set simplest setting learning problem consists finding value minimizes practice performance system training set little interest 
relevant measure error rate system field practice 
performance estimated measuring accuracy set samples disjoint training set called test set 
theoretical experimental shown gap expected error rate test set error rate training set decreases number training samples approximately number training samples measure effective capacity complexity machine number constant 
gap decreases number training samples increases 
furthermore capacity increases decreases 
increasing capacity tradeoff decrease increase gap optimal value capacity achieves lowest generalization error learning algorithms attempt minimize estimate gap 
formal version called structural risk minimization defining sequence learning machines increasing capacity corresponding sequence subsets parameter space subset superset previous subset 
practical terms structural risk minimization implemented minimizing function called regularization function constant 
chosen takes large values parameters belong high capacity subsets parameter space 
minimizing effect limits capacity accessible subset parameter space controlling tradeoff minimizing training error minimizing expected gap training error test error 
gradient learning general problem minimizing function respect set parameters root issues computer science 
gradient learning draws fact generally easier minimize reasonably smooth continuous function discrete combinatorial function 
loss function minimized estimating impact small variations parameter values loss function 
measured gradient loss function respect parameters 
efficient learning algorithms devised gradient vector computed analytically opposed numerically perturbations 
basis numerous gradient learning algorithms continuous valued parameters 
procedures described article set parameters real valued vector respect continuous differentiable 
simplest minimization procedure setting gradient descent algorithm iteratively adjusted follows simplest case scalar constant 
sophisticated procedures variable substitute diagonal matrix substitute estimate inverse hessian matrix newton quasi newton methods 
conjugate gradient method 
appendix shows despite claims contrary literature usefulness second order methods large learning machines limited 
popular minimization procedure stochastic gradient algorithm called online update 
consists updating parameter vector noisy approximated version average gradient 
common instance updated basis single sample procedure parameter vector fluctuates average trajectory usually converges considerably faster regular gradient descent second order methods large training sets redundant samples encountered speech character recognition 
reasons explained appendix properties algorithms applied learning studied theoretically practical successes nontrivial tasks occur mid eighties 
gradient back propagation gradient learning procedures late limited linear systems 
surprising usefulness simple gradient descent techniques complex machine learning tasks widely realized events occurred 
event realization despite early warnings contrary presence local minima loss function major problem practice 
apparent noticed local minima major impediment success early nonlinear gradient learning techniques boltzmann machines 
second event rumelhart simple efficient procedure compute gradient nonlinear system composed proceedings ieee vol 
november layers processing back propagation algorithm 
third event demonstration back propagation procedure applied multilayer nn sigmoidal units solve complicated learning tasks 
basic idea back propagation gradients computed efficiently propagation output input 
idea described control theory literature early application machine learning generally realized 
interestingly early derivations back propagation context nn learning gradients virtual targets units intermediate layers minimal disturbance arguments 
lagrange formalism control theory literature provides best rigorous method deriving back propagation deriving generalizations back propagation recurrent networks networks heterogeneous modules 
simple derivation generic multilayer systems section 
fact local minima problem multilayer nn somewhat theoretical mystery 
conjectured network oversized task usually case practice presence extra dimensions parameter space reduces risk unattainable regions 
back propagation far widely neural network learning algorithm probably widely learning algorithm form 
learning real handwriting recognition systems isolated handwritten character recognition extensively studied literature see reviews early successful applications nn 
comparative experiments recognition individual handwritten digits reported section iii 
show nn trained gradient learning perform better methods tested data 
best nn called convolutional networks designed learn extract relevant features directly pixel images see section ii 
difficult problems handwriting recognition recognize individual characters separate characters neighbors word sentence process known segmentation 
technique doing standard called hos 
consists generating large number potential cuts characters heuristic image processing techniques subsequently selecting best combination cuts scores candidate character recognizer 
model accuracy system depends quality cuts generated heuristics ability recognizer distinguish correctly segmented characters pieces characters multiple characters incorrectly segmented characters 
training recognizer perform task poses major challenge difficulty creating labeled database incorrectly segmented characters 
simplest solution consists running images character strings segmenter manually labeling character hypotheses 
unfortunately extremely tedious costly task difficult labeling consistently 
example right half cut labeled 
right half cut labeled 
solution described section consists training system level strings characters character level 
notion gradient learning purpose 
system trained minimize loss function measures probability erroneous answer 
section explores various ways ensure loss function differentiable lends gradient learning methods 
section introduces directed acyclic graphs arcs carry numerical information way represent alternative hypotheses introduces idea gtn 
second solution described section vii eliminate segmentation altogether 
idea sweep recognizer possible location input image rely character spotting property recognizer ability correctly recognize centered character input field presence characters rejecting images containing centered characters 
sequence recognizer outputs obtained sweeping recognizer input fed gtn takes linguistic constraints account extracts interpretation 
gtn somewhat similar hmm approach reminiscent classical speech recognition 
technique quite expensive general case convolutional nn particularly attractive allows significant savings computational cost 
globally trainable systems stated earlier practical pattern recognition systems composed multiple modules 
example document recognition system composed field locator extracts regions interest field segmenter cuts input image images candidate characters recognizer classifies scores candidate character contextual postprocessor generally stochastic grammar selects best grammatically correct answer hypotheses generated recognizer 
cases information carried module module best represented graphs numerical information attached arcs 
example output recognizer module represented acyclic graph arc contains label score candidate character path represents alternative interpretation input string 
typically module manually optimized trained outside context 
example character recognizer trained labeled images characters 
complete system lecun gradient learning applied document recognition assembled subset parameters modules manually adjusted maximize performance 
step extremely tedious time consuming certainly suboptimal 
better alternative train entire system minimize global error measure probability character misclassifications document level 
ideally want find minimum global loss function respect parameters system 
loss function measuring performance differentiable respect system tunable parameters find local minimum gradient learning 
glance appears sheer size complexity system intractable 
ensure global loss function differentiable system built feedforward network differentiable modules 
function implemented module continuous differentiable respect internal parameters module weights nn character recognizer case character recognition module respect module inputs 
case simple generalization known backpropagation procedure efficiently compute gradients loss function respect parameters system 
example consider system built cascade modules implements function vector representing output module vector tunable parameters module subset module input vector previous module output vector 
input module input pattern partial derivative respect known partial derivatives respect computed backward recurrence jacobian respect evaluated point jacobian respect jacobian vector function matrix containing partial derivatives outputs respect inputs 
equation computes terms gradient second equation generates backward recurrence known back propagation procedure nn average gradients training patterns obtain full gradient 
interesting note instances need explicitly compute jacobian matrix 
formula uses product jacobian vector partial derivatives easier compute product directly computing jacobian 
analogy ordinary multilayer nn module called hidden layers outputs observable outside 
complex situations simple cascade modules described partial derivative notation somewhat ambiguous awkward 
completely rigorous derivation general cases done lagrange functions 
traditional multilayer nn special case state information represented fixed sized vectors modules alternated layers matrix multiplications weights component wise sigmoid functions neurons 
stated earlier state information complex recognition system best represented graphs numerical information attached arcs 
case module called gt takes graphs input produces graph output 
networks modules called gtn sections iv vi viii develop concept gtn show gradient learning train parameters modules minimize global loss function 
may paradoxical gradients computed state information represented essentially discrete objects graphs difficulty circumvented shown 
ii 
convolutional neural networks isolated character recognition ability multilayer networks trained gradient descent learn complex high dimensional nonlinear mappings large collections examples obvious candidates image recognition tasks 
traditional model pattern recognition hand designed feature extractor gathers relevant information input eliminates irrelevant variabilities 
trainable classifier categorizes resulting feature vectors classes 
scheme standard fully connected multilayer networks classifiers 
potentially interesting scheme rely possible learning feature extractor 
case character recognition network fed raw inputs size normalized images 
done ordinary fully connected feedforward network success tasks character recognition problems 
typical images large variables pixels 
fully connected layer hidden units layer contain tens thousands weights 
large number parameters increases capacity system requires larger training set 
addition memory requirement store weights may rule certain hardware implementations 
main deficiency unstructured nets image speech applications built invariance respect translations local distortions inputs 
sent fixed size input layer nn character images proceedings ieee vol 
november fig 

architecture lenet convolutional nn digits recognition 
plane feature map set units weights constrained identical 
dimensional signals approximately size normalized centered input field 
unfortunately preprocessing perfect handwriting normalized word level cause size slant position variations individual characters 
combined variability writing style cause variations position distinctive features input objects 
principle fully connected network sufficient size learn produce outputs invariant respect variations 
learning task probably result multiple units similar weight patterns positioned various locations input detect distinctive features appear input 
learning weight configurations requires large number training instances cover space possible variations 
convolutional networks described shift invariance automatically obtained forcing replication weight configurations space 
secondly deficiency fully connected architectures topology input entirely ignored 
input variables fixed order affecting outcome training 
contrary images time frequency representations speech strong local structure variables pixels spatially temporally nearby highly correlated 
local correlations reasons known advantages extracting combining local features recognizing spatial temporal objects configurations neighboring variables classified small number categories edges corners 
convolutional networks force extraction local features restricting receptive fields hidden units local 
convolutional networks convolutional networks combine architectural ideas ensure degree shift scale distortion invariance local receptive fields shared weights weight replication spatial temporal subsampling 
typical convolutional network recognizing characters dubbed lenet shown fig 

input plane receives images characters approximately size normalized centered 
unit layer receives inputs set units located small neighborhood previous layer 
idea connecting units local receptive fields input goes back perceptron early simultaneous hubel wiesel discovery locally sensitive neurons cat visual system 
local connections times neural models visual learning 
local receptive fields neurons extract elementary visual features oriented edges endpoints corners similar features signals speech spectrograms 
features combined subsequent layers order detect higher order features 
stated earlier distortions shifts input cause position salient features vary 
addition elementary feature detectors useful part image useful entire image 
knowledge applied forcing set units receptive fields located different places image identical weight vectors 
units layer organized planes units share set weights 
set outputs units plane called feature map 
units feature map constrained perform operation different parts image 
complete convolutional layer composed feature maps different weight vectors multiple features extracted location 
concrete example layer lenet shown fig 

units hidden layer lenet organized planes feature map 
unit feature map inputs connected area input called receptive field unit 
unit inputs trainable coefficients plus trainable bias 
receptive fields contiguous units feature map centered corresponding contiguous units previous layer 
receptive fields neighboring units overlap 
example hidden layer lenet receptive fields horizontally contiguous units overlap columns rows 
stated earlier units feature map share set weights bias detect feature possible locations input 
feature maps layer different sets weights biases extracting different types local features 
lecun gradient learning applied document recognition case lenet input location different types features extracted units identical locations feature maps 
sequential implementation feature map scan input image single unit local receptive field store states unit corresponding locations feature map 
operation equivalent convolution followed additive bias squashing function name convolutional network 
kernel convolution set connection weights units feature map 
interesting property convolutional layers input image shifted feature map output shifted amount left unchanged 
property basis robustness convolutional networks shifts distortions input 
feature detected exact location important 
approximate position relative features relevant 
example know input image contains endpoint roughly horizontal segment upper left area corner upper right area endpoint roughly vertical segment lower portion image tell input image 
precise position features irrelevant identifying pattern potentially harmful positions vary different instances character 
simple way reduce precision position distinctive features encoded feature map reduce spatial resolution feature map 
achieved called subsampling layer performs local averaging subsampling reducing resolution feature map reducing sensitivity output shifts distortions 
second hidden layer lenet subsampling layer 
layer comprises feature maps feature map previous layer 
receptive field unit area previous layer corresponding feature map 
unit computes average inputs multiplies trainable coefficient adds trainable bias passes result sigmoid function 
contiguous units nonoverlapping contiguous receptive fields 
consequently subsampling layer feature map half number rows columns feature maps previous layer 
trainable coefficient bias control effect sigmoid nonlinearity 
coefficient small unit operates quasi linear mode subsampling layer merely blurs input 
coefficient large subsampling units seen performing noisy noisy function depending value bias 
successive layers convolutions subsampling typically alternated resulting layer number feature maps increased spatial resolution decreased 
unit third hidden layer fig 
may input connections feature maps previous layer 
convolution subsampling combination inspired hubel wiesel notions simple complex cells implemented fukushima neocognitron globally supervised learning procedure back propagation available 
large degree invariance geometric transformations input achieved progressive reduction spatial resolution compensated progressive increase richness representation number feature maps 
weights learned back propagation convolutional networks seen synthesizing feature extractor 
weight sharing technique interesting side effect reducing number free parameters reducing capacity machine reducing gap test error training error 
network fig 
contains connections trainable free parameters weight sharing 
fixed size convolutional networks applied applications handwriting recognition machine printed character recognition online handwriting recognition face recognition 
fixed size convolutional networks share weights single temporal dimension known time delay nn tdnn 
tdnn phoneme recognition subsampling spoken word recognition subsampling online recognition isolated handwritten characters signature verification 
lenet section describes detail architecture lenet convolutional nn experiments 
lenet comprises layers counting input contain trainable parameters weights 
input pixel image 
significantly larger largest character database pixels centered field 
reason desirable potential distinctive features stroke endpoints corner appear center receptive field highest level feature detectors 
lenet set centers receptive fields convolutional layer see form area center input 
values input pixels normalized background level white corresponds value foreground black corresponds 
mean input roughly zero variance roughly accelerates learning 
convolutional layers labeled cx subsampling layers labeled sx fully connected layers labeled fx layer index 
layer convolutional layer feature maps 
unit feature map connected neighborhood input 
size feature maps prevents connection input falling boundary 
contains trainable parameters connections 
layer subsampling layer feature maps size 
unit feature map connected neighborhood corresponding feature map 
inputs unit added multiplied proceedings ieee vol 
november table column indicates feature map combined units particular feature map trainable coefficient added trainable bias 
result passed sigmoidal function 
receptive fields nonoverlapping feature maps half number rows column feature maps 
layer trainable parameters connections 
layer convolutional layer feature maps 
unit feature map connected neighborhoods identical locations subset feature maps 
table shows set feature maps combined feature map 
connect feature map feature map 
reason twofold 
connection scheme keeps number connections reasonable bounds 
importantly forces break symmetry network 
different feature maps forced extract different hopefully complementary features get different sets inputs 
rationale connection scheme table 
feature maps take inputs contiguous subsets feature maps 
take input contiguous subset 
take input discontinuous subsets 
takes input feature maps 
layer trainable parameters connections 
layer subsampling layer feature maps size 
unit feature map connected neighborhood corresponding feature map similar way 
layer trainable parameters connections 
layer convolutional layer feature maps 
unit connected neighborhood feature maps 
size size feature maps amounts full connection 
labeled convolutional layer fully connected layer lenet input bigger kept constant feature map dimension larger 
process dynamically increasing size convolutional network described section vii 
layer trainable connections 
layer contains units reason number comes design output layer explained fully connected 
trainable parameters 
classical nn units layers compute dot product input vector weight vector bias added 
weighted sum denoted unit passed sigmoid squashing function produce state unit denoted squashing function scaled hyperbolic tangent amplitude function determines slope origin 
function odd horizontal asymptotes constant chosen 
rationale choice squashing function appendix output layer composed euclidean rbf units class inputs 
outputs rbf unit computed follows words output rbf unit computes euclidean distance input vector parameter vector 
away input parameter vector larger rbf output 
output particular rbf interpreted penalty term measuring fit input pattern model class associated rbf 
probabilistic terms rbf output interpreted unnormalized negative log likelihood gaussian distribution space configurations layer 
input pattern loss function designed get configuration close possible parameter vector rbf corresponds pattern desired class 
parameter vectors units chosen hand kept fixed initially 
components parameters vectors set 
chosen random equal probabilities chosen form error correcting code suggested designed represent stylized image corresponding character class drawn bitmap number 
representation particularly useful recognizing isolated digits quite useful recognizing strings characters taken fully printable ascii set 
rationale characters similar confusable uppercase lowercase zero lowercase digit square brackets uppercase similar output codes 
particularly useful system combined linguistic postprocessor correct confusions 
codes confusable classes similar output corresponding rbf ambiguous character similar postprocessor able pick appropriate interpretation 
fig 
gives output codes full ascii set 
reason distributed codes common code called place code grandmother cell code outputs codes tend behave badly number lecun gradient learning applied document recognition fig 

initial parameters output rbf recognizing full ascii set 
classes larger dozen 
reason output units code time 
quite difficult achieve sigmoid units 
reason classifiers recognize characters reject 
rbf distributed codes appropriate purpose sigmoids activated circumscribed region input space outside patterns fall 
parameter vectors rbf play role target vectors layer 
worth pointing components vectors range sigmoid prevents sigmoids getting saturated 
fact points maximum curvature sigmoids 
forces units operate maximally nonlinear range 
saturation sigmoids avoided known lead slow convergence ill conditioning loss function 
loss function simplest output loss function network maximum likelihood estimation criterion case equivalent minimum mean squared error mse 
criterion set training samples simply output th rbf unit corresponds correct class input pattern cost function appropriate cases lacks important properties 
allow parameters rbf adapt trivial totally unacceptable solution 
solution rbf parameter vectors equal state constant equal parameter vector 
case network happily ignores input rbf outputs equal zero 
collapsing phenomenon occur rbf weights allowed adapt 
second problem competition classes 
competition obtained discriminative training criterion dubbed maximum posteriori map criterion similar maximum mutual information criterion train hmm 
corresponds maximizing posterior probability correct class minimizing logarithm probability correct class input image come classes background class label 
terms penalties means addition pushing penalty correct class mse criterion criterion pulls penalties incorrect classes negative second term plays competitive role 
necessarily smaller equal term loss function positive 
constant positive prevents penalties classes large pushed 
posterior probability class label ratio discriminative criterion prevents previously mentioned collapsing effect rbf parameters learned keeps rbf centers apart 
section vi generalization criterion systems learn classify multiple objects input characters words documents 
computing gradient loss function respect weights layers convolutional network done back propagation 
standard algorithm slightly modified take account proceedings ieee vol 
november weight sharing 
easy way implement compute partial derivatives loss function respect connection network conventional multilayer network weight sharing 
partial derivatives connections share parameter added form derivative respect parameter 
large architecture trained efficiently doing requires techniques described 
appendix describes details particular sigmoid weight initialization 
describe minimization procedure stochastic version diagonal approximation levenberg marquardt procedure 
iii 
results comparison methods recognizing individual digits problems involved designing practical recognition system excellent benchmark comparing shape recognition methods 
existing methods combine hand crafted feature extractor trainable classifier study concentrates adaptive methods operate directly size normalized images 
database modified nist set database train test systems described constructed nist special database special database containing binary images handwritten digits 
nist originally designated sd training set sd test set 
sd cleaner easier recognize sd 
reason fact sd collected census bureau employees sd collected high school students 
drawing sensible learning experiments requires result independent choice training set test complete set samples 
necessary build new database mixing nist datasets 
sd contains digit images written different writers 
contrast sd blocks data writer appeared sequence data sd scrambled 
writer identities sd available information writers 
split sd characters written writers went new training set 
remaining writers placed test set 
sets nearly examples 
new training set completed examples sd starting pattern full set training patterns 
similarly new test set completed sd examples starting pattern full set test patterns 
experiments described subset test images sd sd full training samples 
resulting database called modified nist mnist dataset 
fig 

size normalized examples mnist database 
original black white bilevel images size normalized fit pixel box preserving aspect ratio 
resulting images contain grey levels result antialiasing image interpolation technique normalization algorithm 
versions database 
version images centered image computing center mass pixels translating image position point center field 
instances field extended background pixels 
version database referred regular database 
second version database character images cropped pixels images 
computes second moments inertia pixels counting foreground pixel background pixel zero image horizontally shifting lines principal axis vertical 
version database referred database 
third version database early experiments images reduced pixels 
fig 
shows examples randomly picked test set 
results versions lenet trained regular mnist database 
iterations entire training data performed session 
values global learning rate see appendix definition decreased schedule passes 
iteration diagonal regular database training examples test examples size normalized centered center mass fields available www www research att com yann ocr mnist 
lecun gradient learning applied document recognition fig 

training test error lenet function number passes pattern training set distortions 
average training error measured fly training proceeds 
explains training error appears larger test error initially 
convergence attained passes training set 
hessian approximation reevaluated samples described appendix kept fixed entire iteration 
parameter set 
resulting effective learning rates pass varied approximately set parameters 
test error rate stabilizes passes training set 
error rate training set reaches passes 
authors reported observing common phenomenon overtraining training nn adaptive algorithms various tasks 
overtraining occurs training error keeps decreasing time test error goes minimum starts increasing certain number iterations 
phenomenon common observed case learning curves fig 
show 
possible reason learning rate kept relatively large 
effect weights settle local minimum keep oscillating randomly 
fluctuations average cost lower broader minimum 
stochastic gradient similar effect regularization term favors broader minima 
broader minima correspond solutions large entropy parameter distribution beneficial generalization error 
influence training set size measured training network examples 
resulting training error test error shown fig 

clear specialized architectures lenet training data improve accuracy 
verify hypothesis artificially generated training examples randomly distorting original training images 
increased training set composed original patterns plus instances distorted patterns randomly picked distortion parameters 
distortions combinations planar affine transformations horizontal vertical translations scaling squeezing simultaneous horizontal compression vertical elongation reverse horizontal shearing 
fig 
shows examples distorted patterns training 
distorted data training test error rate dropped deformation 
training parameters deformations 
total length training session left unchanged passes patterns 
interesting note network effectively sees individual sample twice course passes 
fig 
shows misclassified test examples 
examples genuinely ambiguous perfectly identifiable humans written represented style 
shows improvements expected training data 
comparison classifiers sake comparison variety trainable classifiers trained tested database 
early subset results 
error rates test set various methods shown fig 

linear classifier pairwise linear classifier possibly simplest classifier consider linear classifier 
input pixel value contributes weighted sum output unit 
output unit highest sum including contribution bias constant indicates class input character 
regular data error rate 
network free parameters 
images test error rate 
network free parameters 
deficiencies linear classifier documented included simply form basis comparison sophisticated classifiers 
various combinations sigmoid units linear units gradient descent learning learning directly solving linear systems gave similar results 
simple improvement basic linear classifier tested 
idea train unit network separate class class 
case layer comprises units labeled unit trained produce patterns class patterns class trained patterns 
final score class sum outputs units labeled minus sum output units labeled error rate regular test set 
baseline nearest neighbor classifier simple classifier nn classifier euclidean distance measure input images 
classifier advantage training time thought part designer required 
memory requirement recognition time large complete pixel training images megabytes byte pixel available run time 
proceedings ieee vol 
november fig 

training test errors lenet achieved training sets various sizes 
graph suggests larger training set improve performance lenet 
hollow square shows test error training patterns artificially generated random distortions 
test patterns distorted 
fig 

examples distortions training patterns 
compact representations devised modest increase error rate 
regular test set error rate 
data error rate naturally realistic euclidean distance nearest neighbor system operate feature vectors fig 

test patterns misclassified lenet 
image displayed correct answers left network answer right 
errors caused genuinely ambiguous patterns digits written style represented training set 
directly pixels systems study operate directly pixels result useful baseline comparison 
lecun gradient learning applied document recognition fig 

error rate test set various classification methods 
indicates classifier trained tested version database 
dist indicates training set augmented artificially distorted examples 
indicates system pixel images 
uncertainty quoted error rates 
pca polynomial classifier preprocessing stage constructed computes projection input pattern principal components set training vectors 
compute principal components mean input component computed subtracted training vectors 
covariance matrix resulting vectors computed diagonalized singular value decomposition 
dimensional feature vector input second degree polynomial classifier 
classifier seen linear classifier inputs preceded module computes products pairs input variables 
error regular test set 
rbf network rbf network constructed 
layer composed gaussian rbf units inputs second layer simple inputs outputs linear classifier 
rbf units divided groups 
group units trained training examples classes adaptive means algorithm 
second layer weights computed regularized pseudoinverse method 
error rate regular test set 
hidden layer fully connected multilayer nn classifier tested fully connected multilayer nn layers weights hidden layer trained version back propagation described appendix error regular test set network hidden units network hidden units 
artificial distortions generate training data brought marginal improvement hidden units hidden units 
images test error jumped network hidden units 
remains somewhat mystery networks large number free parameters manage achieve reasonably low testing errors 
conjecture dynamics gradient descent learning multilayer nets self regularization effect 
origin weight space saddle point attractive direction weights invariably shrink epochs theoretical analysis confirm 
small weights cause sigmoids operate quasi linear region making network essentially equivalent low capacity single layer network 
learning proceeds weights grow proceedings ieee vol 
november progressively increases effective capacity network 
perfect fortuitous implementation vapnik structural risk minimization principle 
better theoretical understanding phenomena empirical evidence definitely needed 
hidden layer fully connected multilayer nn see effect architecture hidden layer multilayer nn trained 
theoretical results shown function approximated layer nn 
authors observed hidden layer architectures yield better performance practical situations 
phenomenon observed 
test error rate network better result hidden layer network obtained marginally weights connections 
increasing network size yielded marginally improved error rates 
training distorted patterns improved performance somewhat error network network 
small convolutional network lenet convolutional networks attempt solve dilemma small networks learn training set large networks 
lenet early embodiment convolutional network architecture included comparison purposes 
images sampled pixels centered input layer 
multiply add steps required evaluate lenet convolutional nature keeps number free parameters 
lenet architecture developed version usps postal service zip codes database size tuned match available data 
lenet achieved test error 
fact network small number parameters attain error rate indication architecture appropriate task 
lenet experiments lenet clear larger convolutional network needed optimal large size training set 
lenet lenet designed address problem 
lenet similar lenet details architecture 
contains level feature maps followed subsampling maps connected pairs layer feature maps feature maps followed subsampling maps followed fully connected layer units followed output layer units 
lenet contains connections free parameters 
test error 
series experiments replaced layer lenet euclidean nearest neighbor classifier local learning method bottou vapnik local linear classifier retrained time new test pattern shown 
methods improved raw error rate improve rejection performance 
boosted lenet theoretical schapire drucker developed boosting method combining multiple classifiers 
lenet combined trained usual way second trained patterns filtered net second machine sees mix patterns net got right got wrong third net trained new patterns second nets disagree 
testing outputs nets simply added 
error rate lenet low necessary artificially distorted images lenet order get samples train second third nets 
test error rate best classifiers 
glance boosting appears times expensive single net 
fact net produces high confidence answer nets called 
average computational cost times single net 
tangent distance classifier tangent distance classifier nearest neighbor method distance function insensitive small distortions translations input image 
consider image point high dimensional pixel space dimensionality equals number pixels evolving distortion character traces curve pixel space 
taken distortions define lowdimensional manifold pixel space 
small distortions vicinity original image manifold approximated plane known tangent plane 
excellent measure closeness character images distance tangent planes set distortions generate planes includes translations scaling skewing squeezing rotation line thickness variations 
test error rate achieved pixel images 
prefiltering techniques simple euclidean distance multiple resolutions allowed reduce number necessary tangent distance calculations 
svm polynomial classifiers studied methods generating complex decision surfaces 
unfortunately impractical high dimensional problems number product terms prohibitive 
support vector technique extremely economical way representing complex surfaces high dimensional spaces including polynomials types surfaces 
particularly interesting subset decision surfaces ones correspond hyperplanes maximum distance convex hulls classes high dimensional space product terms 
boser realized polynomial degree maximum margin set computed computing dot product input image subset training samples called support vectors elevating result th power linearly combining numbers obtained 
finding support vectors coefficients amounts solving high dimensional quadratic minimization problem linear inequality constraints 
sake comparison include results lecun gradient learning applied document recognition fig 

rejection performance percentage test patterns rejected achieve error systems 
fig 

number multiply accumulate operations recognition single character starting size normalized image 
obtained burges sch lkopf reported 
regular svm error rate regular test set 
cortes vapnik reported error rate svm data slightly different technique 
computational cost technique high multiply adds recognition 
sch lkopf svm technique error attained 
sch lkopf personal communication reached modified version svm 
unfortunately svm extremely expensive twice regular svm 
alleviate problem burges proposed rs svm technique attained regular test set computational cost multiply adds recognition expensive lenet 
discussion summary performance classifiers shown figs 

fig 
shows raw error rate classifiers example test set 
boosted lenet performed best achieving score closely followed lenet 
fig 
shows number patterns test set rejected attain error proceedings ieee vol 
november fig 

memory requirements measured number variables methods 
methods require byte variable adequate performance 
methods 
patterns rejected value corresponding output smaller predefined threshold 
applications rejection performance significant raw error rate 
score decide rejection pattern difference scores top classes 
boosted lenet best performance 
enhanced versions lenet better original lenet raw accuracies identical 
fig 
shows number multiply accumulate operations necessary recognition single image method 
nn demanding memory methods 
convolutional nn particularly suited hardware implementations regular structure low memory requirements weights 
single chip mixed analog digital implementations lenet predecessors shown operate speeds excess characters second 
rapid progress mainstream computer technology renders exotic technologies quickly obsolete 
cost effective implementations memory techniques elusive due enormous memory requirements computational requirements 
training time measured 
nn tangent distance classifier essentially zero training time 
single layer net pairwise net pca quadratic net trained hour multilayer net training times longer required passes training set 
amounts days cpu train lenet silicon graphics origin server single mhz processor 
important note training time somewhat relevant designer little interest final user system 
choice existing technique new technique brings marginal accuracy improvements price considerable training time final user choose 
fig 
shows memory requirements number free parameters various classifiers measured terms number variables need stored 
methods require byte variable adequate performance 
nearestneighbor methods may get bits pixel storing template images 
surprisingly nn require memory memory methods 
performance depends factors including accuracy running time memory requirements 
computer technology improves larger capacity recognizers feasible 
larger recognizers turn require larger training sets 
lenet appropriate available technology just lenet appropriate 
recognizer complex lenet required weeks training data available considered 
quite long time lenet considered state art 
local learning classifier optimal margin classifier tangent distance classifier developed improve lenet succeeded 
lecun gradient learning applied document recognition turn motivated search improved nn architectures 
search guided part estimates capacity various learning machines derived measurements training test error function number training examples 
discovered capacity needed 
series experiments architecture combined analysis characteristics recognition errors lenet lenet crafted 
find boosting gives substantial improvement accuracy relatively modest penalty memory computing expense 
distortion models increase effective size data set requiring collect data 
svm excellent accuracy remarkable high performance classifiers include priori knowledge problem 
fact classifier just image pixels permuted fixed mapping lost pictorial structure 
reaching levels performance comparable convolutional nn done considerable expense memory computational requirements 
rs svm requirements factor convolutional networks error rate close 
improvements results expected technique relatively new 
plenty data available methods attain respectable accuracy 
neural net methods run faster require space memorybased techniques 
nn advantage striking training databases continue increase size 
invariance noise resistance convolutional networks particularly suited recognizing rejecting shapes widely varying size position orientation ones typically produced heuristic real world string recognition systems 
experiment described importance noise resistance distortion invariance obvious 
situation real applications quite different 
characters generally segmented context prior recognition 
segmentation algorithms rarely perfect leave extraneous marks character images noise underlines neighboring characters cut characters produce incomplete characters 
images reliably size normalized centered 
normalizing incomplete characters dangerous 
example enlarged stray mark look genuine systems resorted normalizing images level fields words 
case upper lower profiles entire fields amounts check detected normalize image fixed height 
guarantees stray marks blown images creates wide variations size vertical position characters segmentation 
preferable recognizer robust variations 
fig 
shows examples distorted characters correctly recognized lenet 
estimated accurate recognition occurs scale variations factor vertical shift variations plus minus half height character rotations plus minus degrees 
fully invariant recognition complex shapes elusive goal convolutional networks offer partial answer problem invariance robustness respect geometrical distortions 
fig 
includes examples robustness lenet extremely noisy conditions 
processing images pose insurmountable problems segmentation feature extraction methods lenet able robustly extract salient features cluttered images 
training set network shown mnist training set salt pepper noise added 
pixel randomly inverted probability 
iv 
systems graph transformer networks classical back propagation algorithm described previous sections simple form gradient learning 
clear gradient back propagation algorithm describes general situation simple multilayer feedforward networks composed alternated linear transformations sigmoidal functions 
principle derivatives backpropagated arrangement functional modules long compute product jacobians modules vector 
want train systems composed multiple heterogeneous modules 
answer large complex trainable systems need built simple specialized modules 
simplest example lenet mixes convolutional layers subsampling layers fully connected layers rbf layers 
trivial example described sections iv iv system recognizing words trained simultaneously segment recognize words correct segmentation 
fig 
shows example trainable system 
system defined function implemented modules graph interconnection modules 
graph implicitly defines partial order modules updated forward pass 
example fig 
module updated modules updated possibly parallel followed module 
modules may may trainable parameters 
loss functions measure performance system implemented module 
simplest case loss function module receives external input carries desired output 
framework qualitative difference trainable parameters examples lenet action available www www research att com yann ocr 
proceedings ieee vol 
november fig 

examples unusual distorted noisy characters correctly recognized lenet 
grey level output label represents penalty lighter higher penalties 
fig 

trainable system composed heterogeneous modules 
external inputs outputs intermediate state variables 
object oriented approach object oriented programming offers particularly convenient way implementing systems 
module instance class 
module classes forward propagation method member function called fprop arguments inputs outputs module 
example computing output module fig 
done calling method fprop module arguments 
complex modules constructed simpler modules simply defining new class slots contain member modules intermediate state variables modules 
fprop method class simply calls fprop methods member modules appropriate intermediate state variables external input outputs arguments 
algorithms easily generalizable network modules including influence graph cycles limit discussion case directed acyclic graphs feed forward networks 
computing derivatives system just simple 
backward propagation method called module class defined purpose 
method module takes arguments method 
derivatives system computed calling method modules reverse order compared forward propagation phase 
state variables assumed contain slots storing gradients computed backward pass addition storage states computed forward pass 
backward pass effectively computes partial derivatives loss respect state variables parameters system 
interesting duality property forward backward functions certain modules 
example sum variables forward direction transformed simple fan replication backward lecun gradient learning applied document recognition direction 
conversely fan forward direction transformed sum backward direction 
software environment obtain results described called sn uses concepts 
home grown object oriented dialect lisp compiler fact derivatives computed propagation reverse graph easy understand intuitively 
best way justify theoretically lagrange functions 
formalism extend procedures networks recurrent connections 
special modules nn standard pattern recognition techniques formulated terms systems trained gradient learning 
commonly modules include matrix multiplications sigmoidal modules combination build conventional nn modules include convolutional layers subsampling layers rbf layers softmax layers 
loss functions represented modules single output produces value loss 
commonly modules methods 
general method function multiplication jacobian commonly examples 
method fanout connection sum vice versa 
method multiplication coefficient multiplication coefficient 
method multiplication matrix multiplication transpose matrix 
method addition constant identity 
interestingly certain nondifferentiable modules inserted system adverse effect 
interesting example multiplexer module 
regular inputs switching input output 
module selects inputs depending discrete value switching input copies output 
module differentiable respect switching input differentiable respect regular inputs 
function system includes modules differentiable respect parameters long switching input depend parameters 
example switching input external input 
interesting case module 
module inputs output 
output module minimum inputs 
function module differentiable switching surface set measure zero 
interestingly function continuous reasonably regular sufficient ensure convergence gradient learning algorithm 
object oriented implementation idea easily extended include method propagates gauss newton approximations second derivatives 
leads direct generalization modular systems second derivative back propagation appendix multiplexer module special case general situation described length section ix architecture system changes dynamically input data 
multiplexer modules dynamically reconfigure architecture system new input pattern 
gtn systems flexible tools building large trainable system 
descriptions previous sections implicitly assumed set parameters state information communicated modules fixed size vectors 
limited flexibility fixed size vectors data representation serious deficiency applications notably tasks deal variable length inputs continuous speech recognition handwritten word recognition tasks require encoding relationships objects features number nature vary invariant perception scene analysis recognition composite objects 
important special case recognition strings characters words 
generally fixed size vectors lack flexibility tasks state encode probability distributions sequences vectors symbols case linguistic processing 
distributions sequences best represented stochastic grammars general case directed graphs arc contains vector stochastic grammars special cases vector contains probabilities symbolic information 
path graph represents different sequence vectors 
distributions sequences represented interpreting elements data associated arc parameters probability distribution simply penalty 
distributions sequences particularly handy modeling linguistic knowledge speech handwriting recognition systems sequence path graph represents alternative interpretation input 
successive processing modules progressively refine interpretation 
example speech recognition system start single sequence acoustic vectors transform lattice phonemes distribution phoneme sequences lattice words distribution word sequences single sequence words representing best interpretation 
building large scale handwriting recognition systems systems developed designed easily quickly viewing system networks modules take graphs input produce graphs output 
modules called gt complete systems called gtn modules gtn communicate states gradients form directed graphs arcs carry numerical information scalars vectors 
statistical point view fixed size state vectors conventional networks seen representing proceedings ieee vol 
november fig 

traditional nn systems communicate fixed size vectors layers 
multilayer gtn composed trainable modules operate produce graphs arcs carry numerical information 
means distributions state space 
variable size networks space displacement nn described section vii states variable length sequences fixed size vectors 
seen representing mean probability distribution variable length sequences fixed size vectors 
gtn states represented graphs seen representing mixtures probability distributions structured collections possibly sequences vectors fig 

main points sections show gradient learning procedures limited networks simple modules communicate fixed size vectors generalized gtn gradient back propagation gt takes gradients respect numerical information output graph computes gradients respect numerical information attached input graphs respect module internal parameters 
gradient learning applied long differentiable functions produce numerical data output graph numerical data input graph functions parameters 
second point sections show functions implemented modules typical document processing systems image recognition systems commonly thought combinatorial nature differentiable respect internal parameters respect inputs usable part globally trainable system 
purposely avoid making probability theory 
quantities manipulated viewed penalties costs necessary transformed probabilities exponentials normalizing 
multiple object recognition hos difficult problems handwriting recognition recognize just isolated characters fig 

building segmentation graph hos 
strings characters zip codes check amounts words 
recognizers deal character time segment string individual character images 
impossible devise image analysis techniques segment naturally written sequences characters formed characters 
history automatic speech recognition remind training recognizer optimizing global criterion word sentence level preferable merely training hand segmented phonemes units 
works shown true handwriting recognition optimizing word level criterion preferable solely training recognizer characters recognizer learn recognize individual characters reject characters minimizing word error 
section section vi describe detail simple example gtn address problem reading strings characters words check amounts 
method avoids expensive unreliable task hand result segmentation required traditional systems trained individually labeled character images 
segmentation graph classical method segmentation recognition called hos 
main advantages approaches segmentation avoids making hard decisions segmentation large number different segmentations consideration 
idea heuristic image processing techniques find candidate cuts word string recognizer score alternative segmentations generated 
process depicted fig 

number candidate cuts generated 
candidate locations cuts locating minima vertical projection profile minima distance upper lower contours word 
better segmentation heuristics described section xi 
cut generation heuristic designed generate cuts necessary hope correct set cuts included 
cuts generated alternative segmentations best represented graph called segmentation graph 
segmentation graph directed acyclic graph start node node 
internal node associated candidate cut produced segmentation algorithm 
arc source node destination node lecun gradient learning applied document recognition fig 

recognizing character string gtn 
readability arcs low penalties shown 
associated image contains ink cut associated source node cut associated destination node 
arc created nodes decided ink corresponding cuts form candidate character 
typically individual piece ink associated arc pairs successive pieces ink included separated wide gap clear indication belong different characters 
complete path graph contains piece ink 
path corresponds different way associating pieces ink form characters 
recognition transformer viterbi transformer simple gtn recognize character strings shown fig 

composed gt called recognition transformer viterbi transformer goal recognition transformer generate graph called interpretation graph recognition graph contains possible interpretations possible segmentations input 
path represents possible interpretation particular segmentation fig 

recognition transformer refines arc segmentation arc set arcs interpretation graph character class attached penalties labels 
input 
role viterbi transformer extract best interpretation interpretation graph 
recognition transformer takes segmentation graph input applies recognizer single characters images associated arcs segmentation graph 
interpretation graph structure segmentation graph arc replaced set arcs node 
set arcs arc possible class image associated corresponding arc shown fig 
arc attached class label penalty image belongs class produced recognizer 
computed penalties candidate segments penalties combined penalties computed character recognizer obtain penalties arcs interpretation graph 
combining penalties different nature highly heuristic gtn training procedure tune penalties take advantage combination anyway 
path interpretation graph corresponds possible interpretation input word 
penalty particular interpretation particular segmentation sum arc penalties corresponding path interpretation graph 
computing penalty interpretation independently segmentation requires combine penalties paths interpretation 
appropriate rule combining penalties parallel paths section vi 
proceedings ieee vol 
november viterbi transformer produces graph single path 
path path cumulated penalty interpretation graph 
result recognition produced reading labels arcs graph extracted viterbi transformer 
viterbi transformer owes name famous viterbi algorithm application principle dynamic programming find shortest path graph efficiently 
penalty associated arc source node destination node note multiple arcs nodes 
interpretation graph arcs label viterbi algorithm proceeds follows 
node associated cumulated viterbi penalty cumulated penalties computed order satisfies partial order defined interpretation graph directed acyclic 
start node initialized cumulated penalty nodes cumulated penalties computed recursively values parent nodes upstream arcs destination furthermore value node minimizes right hand side noted minimizing entering arc node reached obtain total penalty path smallest total penalty 
call penalty viterbi penalty sequence arcs nodes viterbi path 
obtain viterbi path nodes arcs trace back nodes arcs follows starting node recursively minimizing entering arc start node reached 
label sequence read arcs viterbi path 
vi 
global training graph transformer networks section described process recognizing string hos assuming recognizer trained give low penalties correct class label correctly segmented characters high penalties erroneous categories correctly segmented characters high penalties categories poorly formed characters 
section explains train system string level requiring manual labeling character segments 
training performed gtn architecture slightly different recognition architecture described section applications priori knowledge expected modules order train separately 
example hos individually label single character images train character recognizer difficult obtain appropriate set images train model reject wrongly segmented candidates 
separate training simple requires additional supervision information lacking incomplete correct segmentation labels incorrect candidate segments 
furthermore shown separate training suboptimal 
section describes different methods training gtn handwriting recognizers string level viterbi training discriminative viterbi training forward training discriminative forward training 
generalization graphbased systems maximum posteriori criterion introduced section ii 
discriminative forward training somewhat similar called maximum mutual information criterion train hmm speech recognition 
rationale differs classical 
recourse probabilistic interpretation show gradient learning approach discriminative training simple instance pervasive principle error correcting learning 
training methods graph sequence recognition systems hmm extensively studied context speech recognition 
methods require system probabilistic generative models data provide normalized likelihoods space possible input sequences 
popular hmm learning methods baum welsh algorithm rely normalization 
normalization preserved models nn integrated system 
techniques discriminative training methods case 
authors proposed methods train nn hmm speech recognizers word sentence level 
globally trainable sequence recognition systems avoid difficulties statistical modeling resorting graph techniques 
best example recurrent nn rnn 
unfortunately despite early enthusiasm training rnn gradient techniques proven difficult practice 
gtn techniques simplify generalize global training methods developed speech recognition 
viterbi training recognition select path interpretation graph lowest penalty viterbi algorithm 
ideally path lowest penalty associated correct label sequence possible 
obvious loss function minimize average training set penalty path associated correct label sequence lowest penalty 
goal training find set recognizer parameters weights recognizer nn minimize average penalty correct lowest penalty path 
gradient loss function computed back propagation gtn architecture shown fig 

training architecture identical recognition architecture described previous section extra gt called lecun gradient learning applied document recognition fig 

viterbi training gtn architecture character string recognizer hos 
path selector inserted interpretation graph viterbi transformer 
transformer takes interpretation graph desired label sequence input 
extracts interpretation graph paths contain correct desired label sequence 
output graph called constrained interpretation graph known forced alignment hmm literature contains paths correspond correct label sequence 
constrained interpretation graph sent viterbi transformer produces graph single path 
path correct path lowest penalty 
path scorer transformer takes simply computes cumulated penalty adding penalties path 
output gtn loss function current pattern label information required system sequence desired character labels 
knowledge correct segmentation required part supervisor chooses segmentations interpretation graph yields lowest penalty 
process back propagating gradients viterbi training gtn described 
explained section iv gradients propagated backward modules gtn order compute gradients preceding modules tune parameters 
back propagating gradients path scorer quite straightforward 
partial derivatives loss function respect individual penalties constrained viterbi path equal loss function simply sum penalties 
back propagating viterbi transformer equally simple 
partial derivatives respect penalties arcs constrained graph arcs appear constrained viterbi path zero 
legitimate back propagate essentially discrete function viterbi transformer 
answer viterbi transformer collection min functions adders put 
shown section iv gradients back propagated min functions adverse effects 
back propagation path selector transformer similar back propagation viterbi transformer 
arcs appear gradient corresponding arc zero depending arc appear arcs alter ego contain right label gradient zero 
forward propagation recognition transformer instance recognizer single character created arc segmentation graph 
state recognizer instances stored 
arc penalty produced individual output recognizer instance gradient zero output instance recognizer 
recognizer outputs nonzero gradient part correct answer value pushed 
gradients recognizer outputs back propagated recognizer instance 
recognizer instance obtain vector partial derivatives loss function respect recognizer instance parameters 
recognizer instances share parameter vector merely clones full gradient loss function respect recognizer parameter vector simply sum gradient vectors produced recognizer instance 
viterbi training formulated differently hmm speech recognition systems 
similar algorithms applied speech recognition systems integrate nn time alignment hybrid hmm systems 
simple satisfying training architecture flaw potentially fatal 
problem mentioned section ii 
recognizer simple nn sigmoid output units minimum loss function attained recognizer gives right answer ignores input sets output constant vector small values components 
known collapse problem 
collapse occurs recognizer outputs simultaneously take minimum value 
hand recognizer output layer contains rbf units fixed parameters trivial solution 
due fact set rbf fixed distinct parameter vectors simultaneously take minimum value 
case complete collapse described occur 
totally prevent occurrence milder collapse loss function flat spot trivial solution constant recognizer output 
flat spot saddle point attractive directions difficult get gradient minimization procedures 
parameters rbf allowed proceedings ieee vol 
november adapt collapse problems reappear rbf centers converge single vector underlying nn learn produce vector ignore input 
different kind collapse occurs width rbf allowed adapt 
collapse occurs trainable module nn feeds rbf collapse occur hmm speech recognition systems generative systems produce normalized likelihoods input data 
way avoid collapse train system respect discriminative training criterion maximizing conditional probability correct interpretations correct sequence class labels input image 
problem viterbi training penalty answer reliably measure confidence take low penalty competing answers account 
discriminative viterbi training modification training criterion circumvent collapse problem described time produce reliable confidence values 
idea minimize cumulated penalty lowest penalty path correct interpretation increase penalty competing possibly incorrect paths dangerously low penalty 
type criterion called discriminative plays answers bad ones 
discriminative training procedures seen attempting build appropriate separating surfaces classes model individual classes independently 
example modeling conditional distribution classes input image discriminative focusing classification surface having separate generative model input data associated class class priors yields joint distribution classes inputs 
conditional approach need assume particular form distribution input data 
example discriminative criterion difference penalty viterbi path constrained graph penalty viterbi path unconstrained interpretation graph difference penalty best correct path penalty best path correct incorrect 
corresponding gtn training architecture shown fig 

left side diagram identical gtn viterbi training 
loss function reduces risk collapse forces recognizer increases penalty wrongly recognized objects 
discriminative training seen example error correction procedure tends minimize difference desired output computed left half gtn fig 
actual output computed right half fig 

discriminative viterbi loss function denoted call penalty viterbi path constrained graph penalty viterbi path unconstrained interpretation graph positive constrained graph subset paths interpretation graph viterbi algorithm selects path lowest total penalty 
ideal case paths coincide zero 
back propagating gradients discriminative viterbi gtn adds negative training previously described training 
fig 
shows gradients back propagated 
left half identical viterbi training gtn back propagation identical 
gradients back propagated right half gtn multiplied contributes loss negative sign 
process similar left half 
gradients arcs get positive contributions left half negative contributions right half 
contributions added penalties arcs sent halves connection forward pass 
arcs appear gradient zero 
contribute cost 
arcs appear zero gradient 
contribution right half cancels contribution left half 
words arc part answer gradient 
arc appears gradient 
arc lower penalty arc gradient arc low penalty higher penalty part desired answer 
variations technique speech recognition 
bottou version loss function saturated fixed value 
seen generalization learning vector quantization lvq loss function 
variations method viterbi path paths 
discriminative viterbi algorithm flaws version problems 
main problem criterion build margin classes 
gradient zero soon penalty constrained viterbi path equal viterbi path 
desirable push penalties wrong paths dangerously close 
section presents solution problem 
forward scoring forward training penalty viterbi path perfectly appropriate purpose recognition gives partial picture situation 
imagine lowest penalty paths corresponding different segmentations produced answer label sequence 
argued penalty interpretation lecun gradient learning applied document recognition fig 

discriminative viterbi training gtn architecture character string recognizer hos 
quantities square brackets penalties computed forward propagation 
quantities parentheses partial derivatives computed backward propagation 
smaller penalty obtained path produced interpretation multiple paths identical label sequences evidence label sequence correct 
rules compute penalty associated graph contains parallel paths 
combination rule borrowed probabilistic interpretation penalties negative log posteriors 
probabilistic framework posterior probability interpretation sum posteriors paths produce interpretation 
translated terms penalties penalty interpretation negative logarithm sum negative exponentials penalties individual paths 
penalty smaller penalties individual paths 
interpretation known method called forward algorithm computing quantity efficiently 
penalty computed procedure particular interpretation called forward penalty 
consider concept constrained proceedings ieee vol 
november graph subgraph interpretation graph contains paths consistent particular label sequence 
constrained graph possible label sequence may empty graphs infinite penalties 
interpretation running forward algorithm corresponding constrained graph gives forward penalty interpretation 
forward algorithm proceeds way similar viterbi algorithm operation node combine incoming cumulated penalties function operation seen soft version function set upstream arcs node penalty arc note numerical inaccuracies better factorize largest corresponding smallest penalty logarithm 
interesting analogy drawn consider graph apply forward algorithm equivalent nn run forward propagation multiplications replaced additions additions replaced log adds sigmoids 
way understand forward algorithm think multiplicative scores probabilities additive penalties arcs score case viterbi algorithm selects path largest cumulative score scores multiplied path forward score sum cumulative scores associated possible paths start node 
forward penalty lower cumulated penalty paths path dominates lower penalty penalty equal forward penalty 
forward algorithm gets name forward pass known baum welsh algorithm training hmm 
section viii gives details relation hmm advantage forward penalty respect viterbi penalty takes account different ways produce answer just lowest penalty 
important ambiguity segmentation combined forward penalty paths associated label sequence may penalty path associated label sequence penalty forward training gtn slight modification previously introduced viterbi training gtn 
suffices turn viterbi transformers fig 
forward take interpretation graph input produce forward penalty graph output 
penalties paths contain correct answer lowered just best 
back propagating forward penalty computation forward transformer quite different back propagating viterbi transformer 
penalties input graph influence forward penalty penalties belong low penalty paths stronger influence 
computing derivatives respect forward penalties computed node graph done back propagation graph source set downstream arcs node derivatives derivatives respect arc penalties obtained seen soft version back propagation viterbi scorer transformer 
arcs influence loss function 
arcs belong low penalty paths larger influence 
back propagation path selector 
derivative respect arcs alter ego simply copied corresponding arc derivatives respect arcs zero 
authors applied idea back propagating gradients forward scorer train speech recognition systems including bridle net model haffner tdnn model authors recommended discriminative training described section 
discriminative forward training information contained forward penalty discriminative training criterion call discriminative forward criterion 
criterion corresponds maximization posterior probability choosing paths associated correct interpretation 
posterior probability defined exponential minus constrained forward penalty normalized exponential minus unconstrained forward penalty 
note forward penalty constrained graph larger equal forward penalty unconstrained interpretation graph 
ideally forward penalty constrained graph equal forward penalty complete interpretation graph 
equality quantities achieved combined penalties paths correct label sequence negligibly small compared penalties paths posterior probability associated paths correct interpretation precisely want 
corresponding gtn training architecture shown fig 

difference denoted call forward penalty constrained graph lecun gradient learning applied document recognition fig 

discriminative forward training gtn architecture character string recognizer hos 
graph forward penalty complete interpretation positive constrained graph subset paths interpretation graph forward penalty graph larger forward penalty subgraph graph 
ideal case penalties incorrect paths infinitely large penalties coincide zero 
readers familiar boltzmann machine connectionist model recognize constrained unconstrained graphs analogous clamped constrained observed values output variable free unconstrained phases boltzmann machine algorithm 
back propagating derivatives discriminative forward gtn distributes gradients evenly viterbi case 
derivatives back propagated left half gtn fig 
interpretation graph 
derivatives negated back propagated right half result arc added contribution left half 
arc derivative 
arcs part correct path positive derivative 
derivative large incorrect path lower penalty correct paths 
similarly derivatives respect arcs part low penalty incorrect path large negative derivative 
hand penalty path associated correct interpretation smaller paths loss function close zero gradient back propagated 
training concentrates examples images yield classification error furthermore concentrates pieces image cause error 
discriminative forward training elegant efficient way solving infamous credit assignment problem learning machines manipulate dynamic data structures graphs 
generally idea situations learning machine choose discrete alternative interpretations 
previously derivatives interpretation graph penalties back propagated character recognizer instances 
back propagation character recognizer gives derivatives parameters 
gradient contributions different candidate segments added obtain total gradient associated pair input image correct label sequence example training set 
step stochastic gradient descent applied update parameters 
remarks discriminative training discussion global training criterion probabilistic interpretation individual penalties arcs graphs 
reasons 
example penalties associated different class labels sum class posteriors integrate input domain likelihoods 
discuss case class posteriors normalization 
local normalization penalties may eliminate information important locally rejecting classes piece image correspond valid character class segmentation candidates may wrong 
explicit garbage class introduced probabilistic framework address question problems remain difficult characterize class probabilistically train system way require density model unseen unlabeled samples 
probabilistic interpretation individual variables plays important role baum welsh algorithm combination expectation maximization em procedure 
unfortunately methods applied discriminative training criteria reduced gradient methods 
enforcing normalization probabilistic quantities performing learning complex inefficient time consuming creates ill conditioning loss function 
prefer postpone normalization far possible fact final decision stage system 
normalization quantities manipulated system direct probabilistic interpretation 
discuss second case generative model input 
generative models build boundary indirectly building independent density model class performing classification decisions basis models 
discriminative approach focus ultimate goal learning case learn classification decision surface 
theoretical arguments suggest estimating input densities real goal obtain discriminant function classification suboptimal strategy 
theory problem estimating densities high dimensional spaces ill posed finding decision boundaries 
proceedings ieee vol 
november fig 

explicit segmentation avoided sweeping recognizer possible location input field 
internal variables system direct probabilistic interpretation system viewed producing posterior probabilities classes 
fact assuming particular label sequence desired sequence gtn fig 
exponential minus interpreted estimate posterior probability label sequence input 
sum posteriors possible label sequences 
approach consists directly minimizing approximation number misclassifications 
prefer discriminative forward loss function causes numerical problems optimization 
see section way obtain scores base rejection strategy 
important point free choose parameterization deemed appropriate classification model 
fact particular parameterization uses internal variables clear probabilistic interpretation model legitimate models manipulate normalized quantities 
important advantage global discriminative training learning focuses important errors system learns integrate ambiguities segmentation algorithm ambiguities character recognizer 
section ix experimental results online handwriting recognition system confirm advantages global training versus separate training 
experiments speech recognition hybrids nn hmm showed marked improvements brought global training 
vii 
multiple object recognition space displacement neural network simple alternative explicitly segmenting images character strings heuristics 
idea sweep recognizer possible locations normalized image entire word string shown fig 

technique segmentation heuristics required system essentially examines possible segmentations input 
problems approach 
method general fig 

sdnn convolutional network replicated wide input field 
quite expensive 
recognizer applied possible location input large subset locations misalignments characters field view recognizers small effect error rate 
second recognizer centered character recognized neighbors center character field view recognizer possibly touching center character 
recognizer able correctly recognize character center input field neighboring characters close touching central character 
third word character string perfectly size normalized 
individual characters string may widely varying sizes baseline positions 
recognizer robust shifts size variations 
problems elegantly circumvented convolutional network replicated input field 
shown section iii convolutional nn robust shifts scale variations input image noise extraneous marks input 
properties take care problems mentioned previous paragraph 
second convolutional networks provide drastic saving computational requirement replicated large input fields 
replicated convolutional network called sdnn shown fig 

scanning recognizer prohibitively expensive general convolutional networks scanned replicated efficiently large variable size input fields 
consider instance convolutional net lecun gradient learning applied document recognition alter ego nearby location 
convolutional nature network units instances look identical locations input identical outputs states need computed twice 
thin slice new states shared network instances needs recomputed 
slices put result simply larger convolutional network structure identical original network feature maps larger horizontal dimension 
words replicating convolutional network done simply increasing size fields convolutions performed replicating output layer accordingly 
output layer effectively convolutional layer 
output receptive field centered elementary object produce class object output may indicate character contain 
outputs interpreted evidences presence objects possible positions input field 
sdnn architecture particularly attractive recognizing cursive handwriting reliable segmentation heuristic exists 
idea sdnn quite old attractive simplicity generated wide interest stated puts enormous demands recognizer 
speech recognition recognizer order magnitude smaller replicated convolutional networks easier implement instance haffner tdnn model 
interpreting output sdnn gtn output sdnn sequence vectors encode likelihoods penalties scores finding character particular class label corresponding location input 
postprocessor required pull best possible label sequence vector sequence 
example sdnn output shown fig 

individual characters spotted neighboring instances recognizer consequence robustness recognizer horizontal translations 
quite characters erroneously detected recognizer instances see piece character 
example recognizer instance sees right third output label 
eliminate extraneous characters output sequence pull best interpretation 
done new type gt input graphs shown fig 

sequence vectors produced sdnn coded linear graph multiple arcs pairs successive nodes 
arc particular pair nodes contains label possible categories penalty produced sdnn class label location 
graph called sdnn output graph 
second input graph transformer grammar transducer specifically finite state transducer encodes relationship input strings class labels corresponding output strings recognized characters 
transducer weighted fig 

gt pulls best interpretation output sdnn 
fig 

example multiple character recognition sdnn 
sdnn explicit segmentation performed 
finite state machine graph arc contains pair labels possibly penalty 
finite state machine transducer state follows arc new state observed input symbol matches symbol symbol pair attached arc point transducer emits second symbol pair penalty combines penalty input symbol penalty arc transducer transforms weighted symbol sequence weighted symbol sequence 
gt shown fig 
performs composition recognition graph grammar transducer 
operation takes possible sequence corresponding possible path recognition graph matches paths grammar transducer 
composition produces interpretation graph contains path corresponding output label sequence 
composition operation may combinatorially intractable turns exists efficient algorithm described details section viii 
experiments sdnn series experiments lenet trained goal replicated recognize multiple characters segmentations 
data generated proceedings ieee vol 
november fig 

sdnn applied noisy image digit string 
digits shown sdnn output represent winning class labels lighter grey level high penalty answers 
previously described mnist set follows 
training images composed central character side characters picked random training set 
separation bounding boxes characters chosen random pixels 
instances central character case desired output network blank space class 
addition training images degraded salt pepper noise random pixel inversions 
figs 
show examples successful recognitions multiple characters lenet sdnn 
standard techniques hos fail miserably examples 
seen examples network exhibits striking invariance noise resistance properties 
authors argued invariance requires sophisticated models feedforward nn lenet exhibits properties large extent 
similarly suggested accurate recognition multiple overlapping objects require explicit mechanisms solve called feature binding problem 
seen figs 
network able tell characters apart closely intertwined task impossible achieve classical hos technique 
sdnn able correctly group disconnected pieces ink form characters 
examples shown upper half fig 

top left example connected connected system correctly identifies separate objects 
top right example interesting reasons 
system correctly identifies individual ones 
second left half right half disconnected correctly grouped geometrical information decide associate left half vertical bar left right 
right half cause appearance erroneous sdnn output removed character model transducer prevents characters appearing contiguous outputs 
important advantage sdnn ease implemented parallel hardware 
specialized analog digital chips designed character recognition image preprocessing applications 
rapid progress conventional processor technology reduced precision vector arithmetic instructions intel mmx success specialized hardware hypothetical best 
global training sdnn experiments string images artificially generated individual character 
advantage know advance location label important character 
real training data correct sequence labels string generally available precise locations corresponding character input image unknown 
experiments described previous section best interpretation extracted sdnn output simple gt 
global training sdnn performed back propagating gradients gt arranged architectures similar ones described section vi 
somewhat equivalent modeling output sdnn hmm 
globally trained tdnn hmm hybrids speech recognition online handwriting recognition 
sdnn combination hmm elastic matching methods handwritten word recognition 
fig 
shows gt architecture training sdnn hmm hybrid discriminative forward criterion 
top part comparable top part fig 

right side composition recognition graph grammar gives interpretation graph possible legal interpretations 
left side composition performed grammar contains paths desired sequence labels 
somewhat similar function path selector previous section 
section vi loss function difference forward score obtained left half forward score obtained right half 
back propagate composition transformer need keep record arc recognition graph originated arcs interpretation graph 
derivative respect arc recognition graph equal sum derivatives respect arcs interpretation graph originated 
derivative computed penalties grammar graph allowing learn 
previous example discriminative criterion short video clips lenet sdnn available www www research att com yann ocr 
lecun gradient learning applied document recognition fig 

globally trainable sdnn hmm hybrid system expressed gtn 
criterion result collapse effect network output rbf adaptive 
training procedure equivalently formulated term hmm 
early experiments zip code recognition experiments online handwriting recognition demonstrated idea globally trained sdnn hmm hybrids 
sdnn extremely promising attractive technique ocr far yielded better results hos 
hope results improve experience gained models 
object detection spotting sdnn interesting application sdnn object detection spotting 
invariance properties convolutional networks combined efficiency replicated large fields suggests brute force object spotting detection large images 
main idea train single convolutional network distinguish images object interest images background 
utilization mode network replicated cover entire image analyzed forming sdnn 
output sdnn plane activated units indicate presence object interest corresponding receptive field 
sizes objects detected image unknown image network multiple resolutions results multiple resolutions combined 
idea applied face location address block location envelopes hand tracking video 
illustrate method consider case face detection images described 
images containing faces various scales collected 
images filtered zero mean laplacian filter remove variations global illumination low spatial frequency illumination gradients 
training samples faces manually extracted images 
face subimages size normalized height entire face approximately pixels keeping fairly large variations factor 
scale background subimages picked random 
single convolutional network trained samples classify face subimages nonface subimages 
scene image analyzed filtered laplacian filter subsampled powers oftwo resolutions 
network replicated multiple resolution images 
simple voting technique combine results multiple resolutions 
version global training method described previous section alleviate need manually locate faces building training sample 
possible location seen alternative interpretation parallel arcs simple graph contains start node node 
authors nn classifiers svm face detection great success 
systems similar described including idea presenting image network multiple scales 
systems convolutional networks take advantage speedup described rely techniques prefiltering real time tracking keep computational requirement reasonable limits 
addition classifiers invariant scale variations convolutional networks necessary multiply number scales images classifier 
viii 
graph transformer networks transducers section iv gtn introduced generalization multilayer networks state information represented graphs vectors 
section gtn framework generalized transduction proposes powerful graph composition algorithm 
previous numerous authors speech recognition gradient learning methods integrate graphbased statistical models notably hmm acoustic recognition modules mainly gaussian mixture models nn 
similar ideas applied handwriting recognition see review 
proposal systematic approach multilayer graph trainable systems 
idea transforming graphs graphs received considerable attention computer science proceedings ieee vol 
november concept weighted finite state transducers 
transducers applied speech recognition language translation proposals handwriting recognition 
line mainly focused efficient search algorithms algebraic aspects combining transducers graphs called acceptors context little effort devoted building globally trainable systems transducers 
proposed sections systematic approach automatic training graph manipulating systems 
different approach graph trainable systems called input output hmm proposed 
standard transduction established framework finite state transducers discrete symbols attached arcs graphs 
acceptor graphs single symbol attached arc transducer graphs symbols input symbol output symbol 
special null symbol absorbed symbol concatenating symbols build symbol sequence 
weighted transducers acceptors scalar quantity attached arc framework composition operation takes input acceptor graph transducer graph builds output acceptor graph 
path output graph symbol sequence corresponds path symbol sequence input acceptor graph path corresponding pair input output sequences transducer graph 
weights arcs output graph obtained adding weights matching arcs input acceptor transducer graphs 
rest call graph composition operation transducers standard transduction operation 
simple example transduction shown fig 

simple example input output symbols transducer arcs identical 
type transducer graph called grammar graph 
better understand transduction operation imagine tokens sitting start nodes input acceptor graph transducer graph 
tokens freely follow arc labeled null input symbol 
token follow arc labeled nonnull input symbol token follows arc labeled input symbol 
acceptable trajectory tokens reach nodes graphs tokens reached terminal configuration 
trajectory represents sequence input symbols complies acceptor transducer 
collect corresponding sequence output symbols trajectory transducer token 
procedure produces tree simple technique described section viii avoid generating multiple copies certain subgraphs detecting particular output state seen 
transduction operation performed efficiently presents complex bookkeeping prob fig 

example composition recognition graph grammar graph order build interpretation consistent 
forward propagation dark arrows 
gradients dashed arrows back propagated adaptation method group 
lems concerning handling combinations null nonnull symbols 
weights interpreted probabilities normalized appropriately acceptor graph represents probability distribution language defined set label sequences associated possible paths start node graph 
example application transduction operation incorporation linguistic constraints lexicon grammar recognizing words character strings 
recognition transformer produces recognition graph acceptor graph applying nn recognizer candidate segment 
acceptor graph composed transducer graph grammar 
grammar transducer contains path legal sequence symbol possibly augmented penalties indicate relative likelihoods possible sequences 
arcs contain identical input output symbols 
example transduction mentioned section path selector hos training gtn implementable composition 
transducer graph linear graph contains correct label sequence 
composition interpretation graph linear graph yields constrained graph 
generalized transduction data structures associated arc took finite number values composing input graph appropriate transducer sound solution 
applications data structures attached arcs graphs may vectors images lecun gradient learning applied document recognition high dimensional objects readily enumerated 
new composition operation solves problem 
handling graphs discrete symbols penalties arcs interested considering graphs arcs may carry complex data structures including continuous valued data structures vectors images 
composing graphs requires additional information 
examining pair arcs input graph need criterion decide create corresponding arc node output graph information attached input arcs 
decide build arc arcs entire subgraph nodes arcs 
criterion met build corresponding arc node output graph compute information attached newly created arc function information attached input arcs 
functions encapsulated object called composition transformer 
instance composition transformer implements methods check arc arc compares data structures pointed arcs arc graph arc second graph returns boolean indicating corresponding arc created output graph fprop arc arc called check arc arc returns true method creates new arcs nodes nodes output graph computes information attached newly created arcs function attached information input arcs arc arc arc arc called training order propagate gradient information output subgraph data structures respect parameters fprop call arguments method assumes function fprop compute values attached output arcs differentiable 
check method seen constructing dynamic architecture functional dependencies fprop method performs forward propagation architecture compute numerical information attached arcs 
method performs backward propagation architecture compute partial derivatives loss function respect information attached arcs 
illustrated fig 

fig 
shows simplified generalized graph composition algorithm 
simplified algorithm handle null transitions check tokens fig 

pseudocode simplified generalized composition algorithm 
simplifying presentation handle null transitions implement dead avoidance 
main components composition appear clearly recursive function enumerating token trajectories associative array map remembering nodes composed graph visited 
trajectory acceptable tokens simultaneously reach nodes graphs 
management null transitions straightforward modification token simulation function 
enumerating possible nonnull joint token transitions loop possible null transitions token recursively call token simulation function call method fprop 
safest way identifying acceptable trajectories consists running preliminary pass identifying token configurations reach terminal configuration tokens nodes 
proceedings ieee vol 
november easily achieved enumerating trajectories opposite direction 
start nodes follow arcs upstream 
main pass build nodes allow tokens reach terminal configuration 
graph composition transducers standard transduction easily efficiently implemented generalized transduction 
method check simply tests equality input symbols arcs method fprop creates single arc symbol output symbol transducer arc composition pairs graphs particularly useful incorporating linguistic constraints handwriting recognizer 
examples online handwriting recognition system described section ix check reading system described section 
rest term composition transformer denote gt generalized transductions multiple graphs 
concept generalized transduction general 
fact gt described earlier segmenter recognizer formulated terms generalized transduction 
case generalized transduction take input graphs single input graph 
method fprop transformer may create arcs complete subgraph arc initial graph 
fact pair check fprop seen procedurally defining transducer 
addition shown generalized transduction single graph theoretically equivalent standard composition graph particular transducer graph 
implementing operation way may inefficient transducer complicated 
practice graph produced generalized transduction represented procedurally order avoid building output graph may huge example interpretation graph composed grammar graph 
instantiate nodes visited search algorithm recognition viterbi 
strategy propagates benefits pruning algorithms beam search gtn notes graph structures section vi discussed idea global training backpropagating gradient simple gt method basis back propagation algorithm generic gt generalized composition transformer seen dynamically establishing functional relationships numerical quantities input output arcs 
check function decided relationship established function implements numerical relationship 
check function establishes structure ephemeral network inside composition transformer 
fprop assumed differentiable gradients back propagated structure 
parameters affect scores stored arcs successive graphs system 
threshold parameters may determine arc appears graph 
arcs equivalent arcs large penalties consider case parameters affecting penalties 
kind systems discussed application described section knowledge structure graph produced gt determined nature gt may depend value parameters input 
may interesting consider gt modules attempt learn structure output graph 
considered combinatorial problem amenable gradient learning solution problem generate large graph contains graph candidates subgraphs select appropriate subgraph 
gtn hmm gtn seen generalization extension hmm hand probabilistic interpretation kept penalties log probabilities pushed final decision stage difference constrained forward penalty unconstrained forward penalty interpreted negative log probabilities label sequences dropped altogether network just represents decision surface label sequences input space 
hand gtn extend hmm allowing combine principled framework multiple levels processing multiple models pereira transducer framework stacking hmm representing different levels processing automatic speech recognition 
unfolding hmm time yields graph similar interpretation graph final stage processing gtn viterbi recognition 
nodes associated time step state model 
penalty arc corresponds negative log probability emitting observed data position going state state time interval probabilistic interpretation forward penalty negative logarithm likelihood observed data sequence model 
section vi mentioned collapsing phenomenon occur loss functions train nn hmm hybrid systems 
classical hmm fixed preprocessing problem occur parameters emission transition probability models forced satisfy certain probabilistic constraints sum integral probabilities random variable possible values 
probability certain events increased probability events automatically decreased 
hand probabilistic assumptions hmm probabilistic model realistic discriminative training discussed section vi improve performance lecun gradient learning applied document recognition clearly shown speech recognition systems 
input output hmm iohmm strongly related gt viewed probabilistic model iohmm represents conditional distribution output sequences input sequences different length 
parameterized emission probability module transition probability module 
emission probability module computes conditional emission probability output variable input value value discrete state variable 
transition probability module computes conditional transition probabilities change value state variable input value 
viewed gt assigns output graph representing probability distribution sequences output variable path input graph 
output graphs structure penalties arcs simply added order obtain complete output graph 
input values emission transition modules read data structure input arcs iohmm gt 
practice output graph may large needs completely instantiated pruned low penalty paths created 
ix 
line handwriting recognition system natural handwriting mixture different styles lower case printed upper case cursive 
reliable recognizer handwriting greatly improve interaction pen devices implementation presents new technical challenges 
characters taken isolation ambiguous considerable information available context word 
built word recognition system pen devices main modules preprocessor normalizes word word group fitting geometrical model word structure module produces annotated image normalized pen trajectory replicated convolutional nn spots recognizes characters gtn interprets networks output word level constraints account 
network gtn jointly trained minimize error measure defined word level 
compared system sdnn described section vii system hos described section 
sequential nature information pen trajectory reveals information purely optical input image hos efficient proposing candidate character cuts especially script 
preprocessing input normalization reduces variability simplifying character recognition 
word normalization scheme fitting geometrical model word structure 
model flexible lines representing respectively line fig 

online handwriting recognition gtn hos 
core line base line line 
lines fitted local minima maxima pen trajectory 
parameters lines estimated modified version em algorithm maximize joint probability observed points parameter values prior parameters prevents lines collapsing 
recognition handwritten characters pen trajectory digitizing surface done time domain 
typically trajectories normalized local geometrical dynamical features extracted 
recognition may performed curve matching classification techniques tdnn 
representations advantages dependence stroke ordering individual writing styles difficult high accuracy writer independent systems integrate segmentation recognition 
intent writer produce legible image natural preserve pictorial nature signal possible time exploit sequential information trajectory 
purpose designed representation scheme called pen trajectories represented images picture element contains information local properties trajectory 
viewed annotated image pixel element feature vector features associated orientations pen trajectory proceedings ieee vol 
november fig 

online handwriting recognition gtn sdnn 
area pixel fifth associated local curvature area pixel 
particularly useful feature representation assumptions nature input trajectory 
depend stroke ordering writing speed types handwriting capital lower case cursive punctuation symbols 
representations global features computed complete words requiring segmentation 
network architecture best networks online offline character recognition layer convolutional network somewhat similar lenet fig 
multiple input planes different numbers units layers layer convolution kernels size layer subsampling layer convolution kernels size layer convolution kernels size layer subsampling classification layer rbf units class full printable ascii set 
distributed codes output lenet adaptive lenet 
hos system input network consisted planes rows columns 
determined resolution sufficient representing handwritten characters 
sdnn version number columns varied width input word 
number subsampling layers sizes kernels chosen sizes layers including input determined unambiguously 
architectural parameters remain selected number feature maps layer information feature map connected feature map 
case subsampling rates chosen small possible kernels small possible layer limit total number connections 
kernel sizes upper layers chosen small possible satisfying size constraints mentioned 
larger architectures necessarily perform better required considerably time trained 
small architecture half input field performed worse insufficient input resolution 
note input resolution ocr angle curvature provide information single grey level pixel 
network training training proceeded phases 
kept centers rbf fixed trained network weights minimize output distance rbf unit corresponding correct class 
equivalent minimizing mse previous layer center correct class rbf 
bootstrap phase performed isolated characters 
second phase parameters network weights rbf centers trained globally minimize discriminative criterion word level 
hos approach gtn composed main gt segmentation transformer performs hos outputs segmentation graph 
computed image attached arcs graph 
character recognition transformer applies convolutional network character recognizer candidate segment outputs recognition graph penalties classes arc composition transformer composes recognition graph grammar graph representing language model incorporating lexical constraints 
beam search transformer extracts interpretation interpretation graph 
task achieved usual viterbi transformer 
beam search algorithm implements pruning strategies appropriate large interpretation graphs 
sdnn approach main gt 
sdnn transformer replicates convolutional network word image outputs recognition graph linear graph class lecun gradient learning applied document recognition fig 

comparative results character error rates showing improvement brought global training sdnn hmm hybrid hos word dictionary 
penalties window centered regular intervals input image 
character level composition transformer composes recognition graph left right hmm character class fig 

word level composition transformer composes output previous transformer language model incorporating lexical constraints outputs interpretation graph 
beam search transformer extracts interpretation interpretation graph 
application language model simply constrains final output graph represent sequences character labels dictionary 
furthermore interpretation graph completely instantiated nodes created needed beam search module 
interpretation graph represented procedurally explicitly 
crucial contribution research joint training gt modules network respect single criterion explained sections vi vii 
discriminative forward loss function final output graph minimize forward penalty constrained interpretation correct paths maximizing forward penalty interpretation graph paths 
global training loss function optimized stochastic diagonal levenberg marquardt procedure described appendix uses second derivatives compute optimal learning rates 
optimization operates parameters system notably network weights rbf centers 
experimental results set experiments evaluated generalization ability nn classifier coupled word normalization preprocessing input representation 
results writer independent mode different writers training testing 
initial training isolated characters performed database approximately hand printed characters classes upper case lower case digits punctuation 
tests database isolated characters performed separately types characters upper case error patterns lower case error patterns digits error patterns punctuation error patterns 
experiments performed network architecture described 
enhance robustness recognizer variations position size orientation distortions additional training data generated applying local affine transformations original characters 
second third set experiments concerned recognition lower case words writer independent 
tests performed database words 
evaluated improvements brought word normalization system 
sdnn hmm system word level normalization network sees word time 
hos system doing word level training obtained normalization word character errors adding insertions deletions substitutions search constrained word dictionary 
word normalization preprocessing character level normalization error rates dropped word character errors respectively relative drop word character error respectively 
suggests normalizing word entirety better segmenting normalizing processing segments 
third set experiments measured improvements obtained joint training nn postprocessor word level criterion comparison training errors performed character level 
initial training individual characters global word level discriminative training performed database lower case words 
sdnn hmm system dictionary constraints error rates dropped word character error respectively word level training relative drop 
proceedings ieee vol 
november hos system slightly improved architecture dictionary constraints error rates dropped word character error respectively relative drop 
word dictionary errors dropped word character errors respectively word level training relative drop 
lower error rates obtained drastically reducing size dictionary words yielding word character errors 
results clearly demonstrate usefulness globally trained nn hmm hybrids handwriting recognition 
confirms similar results obtained earlier speech recognition 
check reading system section describes gtn check reading system intended immediate industrial deployment 
shows gradient learning gtn deployment fast cost effective yielding accurate reliable solution 
verification amount check task extremely time money consuming banks 
consequence high interest automating process possible see example 
partial automation result considerable cost reductions 
threshold economic viability automatic check readers set bank checks read error 
check rejected sent human operators 
case describe performance system correct reject error 
system cross threshold representative mixtures business personal checks 
checks contain versions amount 
courtesy amount written numerals legal amount written letters 
business checks generally machine printed amounts relatively easy read quite difficult find due lack standard business check layout 
hand amounts personal checks easy find harder read 
simplicity speed requirements initial task read courtesy amount 
task consists main steps 
system find fields lines text candidates contain courtesy amount 
obvious personal checks position amount standardized 
noted finding amount difficult business checks human eye 
strings digits check number date exceed amounts confused actual amount 
cases difficult decide candidate courtesy amount performing full recognition 
fig 

complete check amount reader implemented single cascade gt modules 
successive graph transformations progressively extract higher level information 
order read choose courtesy amount candidates system segment fields characters read score candidate characters find best interpretation amount contextual knowledge represented stochastic grammar check amounts 
gtn methodology build check amount reading system handles personal checks business checks 
gtn check amount recognition describe successive graph transformations allow network read check amount cf 
fig 

gt produces graph paths encode score current hypotheses considered stage system 
input system trivial graph single arc carries image check cf 
fig 

field location transformer performs classical image analysis including connected component analysis ink density histograms layout analysis heuristically extracts rectangular zones may contain check amount 
produces output graph called field graph cf 
fig 
candidate zone associated arc links start node node 
arc contains image zone penalty term computed simple features extracted zone absolute position size aspect ratio 
penalty term close zero features suggest field candidate large field deemed amount 
penalty function differentiable parameters globally tunable 
lecun gradient learning applied document recognition arc may represent separate dollar cent amounts sequence fields 
fact handwritten checks cent amount may written fractional bar aligned dollar amount 
worst case may find cent amount candidates fraction bar dollar amount 
segmentation transformer similar described section viii examines zone contained field graph cuts image pieces ink heuristic image processing techniques 
piece ink may character piece character 
arc field graph replaced corresponding segmentation graph represents possible groupings pieces ink 
field segmentation graph appended arc contains penalty field field graph 
arc carries segment image penalty provides evaluation likelihood segment contains character 
penalty obtained differentiable function combines simple features space pieces ink compliance segment image global baseline tunable parameters 
segmentation graph represents possible segmentations field images 
compute penalty segmented field adding arc penalties corresponding path 
differentiable function computing penalties ensure parameters optimized globally 
segmenter uses variety heuristics find candidate cut 
important ones called hit 
idea cast lines downward top field image 
line hits black pixel follow contour object 
line hits local minimum upper profile continue downward crossing black pixel just propagated vertically downward ink 
lines meet merged single cut 
procedure repeated bottom 
strategy allows separation touching characters double zeros 
recognition transformer iterates segment arcs segmentation graph runs character recognizer corresponding segment image 
case recognizer lenet convolutional nn described section ii weights constitute largest important subset tunable parameters 
recognizer classifies segment images classes fully printable ascii set plus class unknown symbols badly formed characters 
arc input graph replaced arcs output graph 
arcs contains label classes penalty sum penalty corresponding arc input segmentation graph penalty associated classifying image corresponding class computed recognizer 
words recognition graph represents weighted trellis scored character classes 
path graph represents possible character string corresponding field 
compute penalty interpretation adding penalties path 
sequence characters may may valid check amount 
composition transformer selects paths recognition graph represent valid character sequences check amounts 
transformer takes graphs input recognition graph grammar graph 
grammar graph contains possible sequences symbols constitute formed amount 
output composition transformer called interpretation graph contains paths recognition graph compatible grammar 
operation combines input graphs produce output generalized transduction see section ix 
differentiable function compute data attached output arc data attached input arcs 
case output arc receives class label arcs penalty computed simply summing penalties input arcs recognizer penalty arc penalty grammar graph 
path interpretation graph represents interpretation segmentation field check 
sum penalties path represents badness corresponding interpretation combines evidence modules process grammar 
viterbi transformer viterbi transformer selects path lowest accumulated penalty corresponding best grammatically correct interpretations 
gradient learning stage check reading system contains tunable parameters 
parameters manually adjusted parameters field locator segmenter vast majority learned particularly weights nn recognizer 
prior globally optimizing system module parameters initialized reasonable values 
parameters field locator segmenter initialized hand parameters nn character recognizer initialized training database labeled characters 
entire system trained globally check images labeled correct amount 
explicit segmentation amounts needed train system trained check level 
loss function minimized global training procedure discriminative forward criterion described section vi difference forward penalty constrained interpretation graph constrained correct label sequence forward penalty unconstrained interpretation graph 
derivatives back propagated entire structure practical segmenter 
rejecting low confidence checks order able reject checks carry erroneous viterbi answers rate confidence reject check proceedings ieee vol 
november fig 

additional processing required compute confidence 
confidence threshold 
compare unnormalized viterbi penalties different checks meaningless comes decide answer trust 
optimal measure confidence probability viterbi answer input image 
seen section vi target sequence case viterbi answer discriminative forward loss function estimate logarithm probability 
simple solution obtain estimate confidence reuse interpretation graph see fig 
compute discriminative forward loss described fig 
desired sequence viterbi answer 
summarized fig 
results version system fully implemented tested machine print business checks 
system basically generic gtn engine task specific heuristics encapsulated check fprop method 
consequence amount code write minimal adaptation earlier segmenter segmentation transformer 
system deals handwritten personal checks earlier implementations gtn concept restricted way 
nn classifier initially trained images character images various origins spanning entire printable ascii set 
contained handwritten machine printed characters previously size normalized string level 
additional images generated randomly distorting original images simple affine transformations images 
network trained character images automatically segmented check images manually 
network initially trained reject resulted segmentation errors 
recognizer inserted check reading system small subset parameters trained globally field level check images 
business checks automatically categorized machine printed performance correctly recognized checks errors rejects 
compared performance previous system test set correct errors rejects 
check categorized machine printed characters near standard position dollar sign detected machine printed standard position courtesy amount candidate 
improvement attributed main causes 
nn recognizer bigger trained data 
second gtn architecture new system take advantage grammatical constraints efficient way previous system 
third gtn architecture provided extreme flexibility testing heuristics adjusting parameters tuning system 
point important 
gtn framework separates algorithmic part system knowledgebased part system allowing easy adjustments 
importance global training minor task global training concerned small subset parameters 
independent test performed systems integrators showed superiority system commercial courtesy amount reading systems 
system integrated ncr line check reading systems 
fielded banks united states june reading millions checks day 
xi 
short history automatic pattern recognition increasing role learning invariably improved performance recognition systems 
systems described evidence fact 
convolutional nn shown eliminate need hand crafted feature extractors 
gtn shown reduce need hand crafted heuristics manual labeling manual parameter tuning document recognition systems 
training data plentiful computers get faster understanding learning algorithms improves recognition systems rely learning performance improve 
just back propagation algorithm elegantly solved credit assignment problem multilayer nn gradient learning procedure gtn introduced solves credit assignment problem systems functional architecture dynamically changes new input 
learning algorithms sense unusual forms gradient descent complex dynamic architectures efficient back propagation algorithms compute gradient 
results help establish usefulness relevance gradient minimization methods general organizing principle learning large systems 
shown steps document analysis system formulated gt lecun gradient learning applied document recognition ents back propagated 
parts system design philosophy terms graph transformation provides clear separation domain specific heuristics segmentation heuristics generic procedural knowledge generalized transduction algorithm worth pointing data generating models hmm maximum likelihood principle called justify architectures training criteria described 
gradient learning applied global discriminative loss functions guarantees optimal classification rejection hard justify principles put strong constraints system architecture expense performances 
specifically methods architectures offer generic solutions large number problems encountered pattern recognition systems 
feature extraction traditionally fixed transform generally derived expert prior knowledge task 
relies probably incorrect assumption human designer able capture relevant information input 
shown application gradient learning convolutional nn allows learn appropriate features examples 
success approach demonstrated extensive comparative digit recognition experiments nist database 
segmentation recognition objects images completely decoupled 
hard segmentation decisions early hos generate evaluate large number hypotheses parallel postponing decision criterion minimized 
hand images obtain segmented characters training character recognizer expensive take account way document sequence characters recognized particular fact segmentation candidates may wrong may look true characters 
train systems optimize global measure performance require time consuming detailed yields significantly better recognition performance allows train modules cooperate common goal 
ambiguities inherent segmentation character recognition linguistic model integrated optimally 
sequence heuristics combine sources information proposed unified framework generalized transduction methods applied graphs representing weighted set hypotheses input 
success approach demonstrated commercially deployed system reads millions business personal checks day generalized transduction engine resides lines code 
traditional recognition systems rely handcrafted heuristics isolate individually recognizable objects 
promising sdnn approach draws robustness efficiency convolutional nn avoid explicit segmentation altogether 
simultaneous automatic learning segmentation recognition achieved gradient learning methods 
presents small number examples gt modules clear concept applied situations domain knowledge state information represented graphs 
case audio signal recognition tasks visual scene analysis applications 
attempt apply gt networks problems hope allowing reliance automatic learning detailed engineering 
appendix preconditions faster convergence seen squashing function convolutional networks symmetric functions believed yield faster convergence learning extremely slow weights small 
cause problem weight space origin fixed point learning dynamics saddle point attractive directions 
simulations see 
choice parameters equalities satisfied 
rationale gain squashing transformation normal operating conditions interpretation state network simplified 
absolute value second derivative maximum improves convergence learning session 
particular choice parameters merely convenience affect result 
training weights initialized random values uniform distribution number inputs fan unit connection belongs 
connections share weight rule difficult apply case connections sharing weight belong units identical fan ins 
reason dividing fan initial standard deviation weighted sums range unit fall normal operating region sigmoid 
initial weights small gradients small learning slow 
large sigmoids saturated gradient small 
standard deviation weighted sum scales square root number inputs inputs independent scales linearly number inputs inputs highly correlated 
chose assume second hypothesis units receive highly correlated signals 
proceedings ieee vol 
november appendix stochastic gradient versus batch gradient gradient learning algorithms classes methods update parameters 
method dubbed batch gradient classical gradients accumulated entire training set parameters updated exact gradient computed 
second method called stochastic gradient partial noisy gradient evaluated basis single training sample small number samples parameters updated approximate gradient 
training samples selected randomly properly randomized sequence 
stochastic version gradient estimates noisy parameters updated batch version 
empirical result considerable practical importance tasks large redundant data sets stochastic version considerably faster batch version orders magnitude 
reasons totally understood theoretically intuitive explanation extreme example 
take example training database composed copies subset 
accumulating gradient set cause redundant computations performed 
hand running stochastic gradient training set amount performing complete learning iterations small subset 
idea generalized training sets exist precise repetition pattern redundancy 
fact stochastic update better redundancy certain level generalization expected 
authors claimed second order methods lieu gradient descent nn training 
literature abounds recommendations classical second order methods gauss newton levenberg marquardt algorithms quasi newton methods fletcher goldfarb shanno limited storage fletcher goldfarb shanno various versions conjugate gradients method 
unfortunately methods unsuitable training large nn large data sets 
gauss newton levenberg marquardt methods require operations update number parameters impractical moderate size networks 
quasi newton methods require operations update impractical large networks 
limited storage fletcher goldfarb shanno conjugate gradients require operations update appear appropriate 
unfortunately convergence speed relies accurate evaluation successive conjugate descent directions sense batch mode 
large data sets speed brought methods regular batch gradient descent match enormous speed brought stochastic gradient 
authors attempted conjugate gradient small batches batches increasing sizes attempts demonstrated surpass carefully tuned stochastic gradient 
experiments performed stochastic method scales parameter axes minimize eccentricity error surface 
appendix stochastic diagonal levenberg marquardt owing reasons appendix prefer update weights presentation single pattern accordance stochastic update methods 
patterns constant random order training set typically repeated times 
update algorithm dubbed stochastic diagonal levenberg marquardt method individual learning rate step size computed parameter weight pass training set 
learning rates computed diagonal terms estimate gauss newton approximation hessian second derivative matrix 
algorithm believed bring tremendous increase learning speed converges reliably requiring extensive adjustments learning parameters 
corrects major ill conditioning loss function due peculiarities network architecture training data 
additional cost procedure standard stochastic gradient descent negligible 
learning iteration particular parameter updated stochastic update rule instantaneous loss function pattern convolutional nn weight sharing partial derivative sum partial derivatives respect connections share parameter connection weight unit unit set unit index pairs connection share parameter stated previously step sizes constant function second derivative loss function axis hand picked constant estimate second derivative loss function respect larger smaller weight update 
parameter prevents step size lecun gradient learning applied document recognition large second derivative small model trust methods levenberg marquardt methods nonlinear optimization 
exact formula compute second derivatives respect connection weights approximations 
approximation drop diagonal terms hessian respect connection weights naturally terms average training set local second derivatives local second derivatives respect connection weights computed local second derivatives respect total input downstream unit state unit second derivative instantaneous loss function respect total input unit denoted interestingly efficient algorithm compute second derivatives similar back propagation procedure compute derivatives unfortunately derivatives leads known problems associated newton algorithm terms negative cause gradient algorithm move uphill downhill 
second approximation known trick called gauss newton approximation guarantees second derivative estimates nonnegative 
gauss newton approximation essentially ignores nonlinearity estimated function nn case loss function 
back propagation equation gauss newton approximations second derivatives similar formula back propagating derivatives sigmoid derivative weight values squared 
right hand side sum products nonnegative terms left hand side term nonnegative 
third approximation run average entire training set run small subset training set 
addition re estimation need done second order properties error surface change slowly 
experiments described reestimate patterns training pass training set 
size training set additional cost re estimating negligible 
estimates particularly sensitive particular subset training set averaging 
suggest second order properties error surface mainly determined structure network detailed statistics samples 
algorithm particularly useful networks weight sharing creates ill conditioning error surface 
sharing single parameter layers enormous influence output 
consequently second derivative error respect parameter may large quite small parameters network 
algorithm compensates phenomenon 
second order acceleration methods back propagation method works stochastic mode 
uses diagonal approximation hessian 
classical levenberg marquardt algorithm uses safety factor prevent step sizes getting large second derivative estimates small 
method called stochastic diagonal levenberg marquardt method 
acknowledgment systems described researchers lucent technologies 
particular burges bromley contributed check reading system 
experimental results described section iii include contributions burges cortes drucker jackel ller sch lkopf simard 
authors wish pereira vapnik denker guyon helpful discussions higgins providing applications motivated rabiner jackel support encouragement 
duda hart pattern classification scene analysis 
new york wiley 
lecun boser denker henderson howard hubbard jackel backpropagation applied handwritten zip code recognition neural computation vol 
pp 
winter 
seung sompolinsky tishby statistical mechanics learning examples phys 
rev vol 
pp 

vapnik levin lecun measuring learning machine neural computation vol 
pp 

cortes jackel solla vapnik denker learning curves asymptotic values rate convergence proceedings ieee vol 
november advances neural information processing systems cowan tesauro alspector eds 
san mateo ca morgan kaufmann pp 

vapnik nature statistical learning theory 
new york springer 
statistical learning theory 
new york wiley 
press flannery teukolsky vetterling numerical recipes art scientific computing 
cambridge uk cambridge univ 
amari theory adaptive pattern classifiers ieee trans 
electron 
comput vol 
ec pp 

adaptation learning automatic systems new york academic 
foundations theory learning systems 
new york academic 
minsky selfridge learning random nets proc 
th london symp 
information theory pp 

ackley hinton sejnowski learning algorithm boltzmann machines cognitive sci vol 
pp 

hinton sejnowski learning relearning boltzmann machines parallel distributed processing explorations microstructure cognition 
volume foundations rumelhart mcclelland eds 
cambridge ma mit 
rumelhart hinton williams learning internal representations error propagation parallel distributed processing explorations microstructure cognition vol 
cambridge ma bradford books pp 
bryson jr 
ho applied optimal control 
london uk 
lecun learning scheme asymmetric threshold networks proc 
paris france pp 

learning processes asymmetric threshold network disordered systems biological organization bienenstock fogelman eds 
les france springer verlag pp 

parker learning logic sloan school manage mit cambridge ma tech 
rep tr apr 
lecun mod les de apprentissage connectionist learning models ph dissertation universit curie paris june 
theoretical framework back propagation proc 
connectionist models summer school touretzky hinton sejnowski eds 
pittsburgh pa cmu morgan kaufmann pp 

bottou gallinari framework cooperation learning algorithms advances neural information processing systems vol 
touretzky lippmann eds 
denver morgan kaufmann 
suen nadal mai lam computer recognition unconstrained handwritten numerals proc 
ieee vol 
pp 
july 
srihari high performance reading machines proc 
ieee vol 
pp 
july 
lecun jackel boser denker graf guyon henderson howard hubbard handwritten digit recognition applications neural net chips automatic learning ieee trans 
commun vol 
pp 
nov 
keeler rumelhart integrated segmentation recognition hand printed numerals neural information processing systems lippmann moody touretzky eds 
san mateo ca morgan kaufmann vol 
pp 

burges lecun denker recognition space displacement neural network vol 
neural information processing systems moody hanson lippman eds 
san mateo ca morgan kaufmann 
rabiner tutorial hidden markov models selected applications speech recognition proc 
ieee vol 
pp 
feb 
morgan connectionist speech recognition hybrid approach 
boston kluwer 
hubel wiesel receptive fields binocular functional architecture cat visual cortex physiology london vol 
pp 

fukushima cognition self organizing multilayered neural network biological cybern vol 
pp 

fukushima miyake neocognitron new algorithm pattern recognition tolerant deformations shifts position pattern recognit vol 
pp 
nov 
mozer perception multiple objects connectionist approach 
cambridge ma mit bradford books 
lecun generalization network design strategies connectionism perspective pfeifer fogelman steels eds 
zurich switzerland elsevier 
lecun boser denker henderson howard hubbard jackel handwritten digit recognition back propagation network advances neural information processing systems nips david touretzky ed 
denver morgan kaufmann 
martin centered object integrated segmentation recognition overlapping hand printed characters neural computation vol 
pp 

wang jean multi resolution neural networks character recognition proc 
int 
conf 
neural networks vol 
iii pp 

bengio lecun burges nn hmm hybrid line handwriting recognition neural computation vol 

lawrence giles tsoi back face recognition convolutional neural network approach ieee trans 
neural networks vol 
pp 
jan 
lang hinton time delay neural network architecture speech recognition carnegie mellon univ pittsburgh pa tech 
rep cmu cs 
waibel hinton shikano lang phoneme recognition time delay neural networks ieee trans 
acoustics speech signal processing vol 
pp 
mar 
bottou fogelman blanchet speaker independent isolated digit recognition multilayer perceptron versus dynamic time warping neural networks vol 
pp 

haffner waibel time delay neural networks embedding time alignment performance analysis proc 
eurospeech nd europ 
conf 
speech communication technology genova italy 
guyon albrecht lecun denker hubbard design neural network character recognizer touch terminal pattern recognit vol 
pp 

bromley bottou guyon lecun moore shah signature verification time delay neural network int 
pattern recognit 
artificial intell vol 
pp 
aug 
lecun solla eigenvalues covariance matrices application neural network learning phys 
rev lett vol 
pp 
may 
dietterich bakiri solving multiclass learning problems error correcting output codes artificial intell 
res vol 
pp 

bahl brown de souza mercer maximum mutual information hidden markov model parameters speech recognition proc 
int 
conf 
acoustics speech signal processing pp 

speech recognition continuous parameter hidden markov models comput speech language vol 
pp 

juang discriminative learning minimum error classification ieee trans 
acoustics speech signal processing vol 
pp 
dec 
lecun jackel bottou cortes denker drucker guyon muller simard vapnik comparison learning algorithms handwritten digit recognition int 
conf 
artificial neural networks fogelman gallinari eds 
paris ec cie pp 

guyon personnaz dreyfus denker lecun comparing different neural net architectures lecun gradient learning applied document recognition classifying handwritten digits proc 
ieee ijcnn washington dc vol 
ii pp 

ott construction quadratic polynomial classifiers proc 
ieee int 
conf 
pattern recognition pp 

sch rmann word recognition system postal address reading ieee trans 
comput vol 
pp 
aug 
lee handwritten digit recognition nearest neighbor radial basis functions backpropagation neural networks neural computation vol 
pp 

saad solla dynamics line gradient descent learning multilayer neural networks advances neural information processing systems vol 
touretzky mozer hasselmo eds 
cambridge ma mit pp 

cybenko approximation superpositions sigmoidal functions math 
control signals syst vol 
pp 

bottou vapnik local learning algorithms neural computation vol 
pp 

schapire strength weak learnability machine learning vol 
pp 

drucker schapire simard improving performance networks boosting algorithm advances neural information processing systems hanson cowan giles eds 
san mateo ca morgan kaufmann pp 

simard lecun denker efficient pattern recognition new transformation distance advances neural information processing systems vol 
hanson cowan giles eds 
san mateo ca morgan kaufmann 
boser guyon vapnik training algorithm optimal margin classifiers proc 
th annu 
workshop computational learning theory vol 
pp 

burges improving accuracy speed support vector machines advances neural information processing systems jordan mozer petsche eds 
cambridge ma mit 
boser bromley lecun jackel application anna neural network chip high speed character recognition ieee trans 
neural networks vol 
pp 
mar 
bridle probabilistic interpretation feedforward classification networks outputs relationship statistical pattern recognition neurocomputing algorithms architectures applications fogelman herault eds 
les arcs france springer 
lecun bottou bengio reading checks graph transformer networks proc 
ieee int 
conf 
acoustics speech signal processing 
munich germany vol 
pp 

bengio neural networks speech sequence recognition 
london uk international thompson 
burges lecun denker jackel ben shortest path segmentation method training neural network recognize character strings proc 
int 
joint conf 
neural networks baltimore md vol 
pp 

breuel system line recognition handwritten text proc 
ieee icpr jerusalem pp 

viterbi error bounds convolutional codes asymptotically optimum decoding algorithm ieee trans 
inform 
theory vol 
pp 
apr 
lippmann gold neural net classifiers useful speech recognition proc 
ieee st int 
conf 
neural networks san diego ca june pp 

sakoe yoshida iso watanabe speaker independent word recognition dynamic programming neural networks proc 
int 
conf 
acoustics speech signal processing glasgow pp 

bridle recurrent neural network architecture hidden markov model interpretation speech commun vol 
pp 

lee waibel connectionist viterbi training new hybrid method continuous speech recognition proc 
int 
conf 
acoustics speech signal processing albuquerque nm pp 

silverman combining hidden markov models neural network classifiers proc 
int 
conf 
acoustics speech signal processing albuquerque nm pp 

bottou mlp lvq dp comparison cooperation proc 
int 
joint conf 
neural networks seattle wa vol 
pp 

bengio de mori global optimization neural network hidden markov model hybrid ieee trans 
neural networks vol 
pp 
march 
haffner waibel multi state time delay neural networks continuous speech recognition vol 
advances neural information processing systems 
san mateo ca morgan kaufmann pp 

bengio simard frasconi learning long term dependencies gradient descent difficult ieee trans 
neural networks vol 
pp 
mar 
kohonen statistical pattern recognition neural network benchmarking studies proc 
ieee nd int 
conf 
neural networks san diego ca vol 
pp 

haffner connectionist speech recognition global mmi algorithm proc 
eurospeech rd europ 
conf 
speech communication technology berlin pp 

denker burges image segmentation recognition mathematics induction 
reading ma addison wesley 
bottou une approche th de apprentissage applications la reconnaissance de la parole ph dissertation univ paris xi france 
bengio lecun feature model design automatic speech recognition proc 
eurospeech rhodes greece pp 

waibel connectionist architectural learning high performance character speech recognition proc 
int 
conf 
acoustics speech signal processing minneapolis mn vol 
pp 

pereira riley sproat weighted rational transductions application human language processing arpa natural language processing workshop 
lades buhmann von der malsburg distortion invariant object recognition dynamic link architecture ieee trans 
comput vol 
pp 
march 
boser bromley lecun jackel analog neural network processor programmable topology ieee solid state circuits vol 
pp 
dec 
weissman guyon henderson recognition segmentation line hand printed words advances neural information processing systems hanson cowan giles eds 
denver morgan kaufmann pp 

devillers aubert combining tdnn hmm hybrid system improved continuous speech recognition ieee trans 
speech audio processing vol 
pp 
jan 
baird bromley burges denker jackel lecun pednault thompson reading handwritten digits zip code recognition system ieee trans 
comput vol 
pp 
july 
bengio lecun word normalization line handwritten word recognition proc 
ieee int 
conf 
pattern recognition jerusalem 
lecun original approach localization objects images proc 
inst 
elect 
eng vol 
pp 
aug 
wolf platt postal address block location convolutional locator network advances neural information processing systems cowan tesauro alspector eds 
san mateo ca morgan kaufmann pp 

nowlan platt convolutional neural network hand tracker advances neural information processing systems tesauro touretzky leen eds 
san mateo ca morgan kaufmann pp 

rowley baluja kanade neural network proceedings ieee vol 
november face detection proc 
ieee cvpr pp 

osuna freund girosi training support vector machines application face detection proc 
ieee cvpr pp 

bourlard links markov models multilayer perceptrons advances neural information processing systems touretzky ed 
denver morgan kaufmann vol 
pp 

bengio de mori neural network gaussian mixture hybrid speech recognition density estimation advances neural information processing systems moody hanson lippmann eds 
denver morgan kaufmann pp 

pereira riley speech recognition composition weighted finite automata finite state devices natural processing 
cambridge ma mit 
mohri finite state transducers language speech processing computational linguistics vol 
pp 

guyon denker overview synthesis line cursive handwriting recognition techniques handbook optical character recognition document image analysis wang bunke eds 
new york world scientific 
mohri riley weighted determinization minimization large vocabulary recognition proc 
eurospeech rhodes greece pp 

bengio frasconi input output hmm architecture advances neural information processing systems vol 
tesauro touretzky leen eds 
cambridge ma mit pp 

input output hmm sequence processing ieee trans 
neural networks vol 
pp 

mohri pereira riley rational design weighted finite state transducer library lecture notes computer science 
new york springer verlag 
lee juang discriminative utterance verification connected digits recognition ieee trans 
speech audio processing vol 
pp 

bengio lecun discriminative feature model design automatic speech recognition proc 
eurospeech rhodes greece 
bengio bengio em algorithm asynchronous input output hidden markov models proc 
international conference neural information processing hong king pp 

suen state art line handwriting recognition ieee trans 
pattern anal 
machine intell vol 
pp 
dec 
connectionist recognizer line cursive handwriting recognition proc 
int 
conf 
acoustics speech signal processing adelaide vol 
pp 

recognition cursive script amounts postal checks proc 
europ 
conf 
postal technol nantes france june pp 

suen cursive script recognition applied processing bank checks proc 
int 
conf 
document analysis recognition montreal canada aug pp 

lam suen liu said automatic processing information checks int 
conf 
systems man cybernetics vancouver canada oct pp 

burges ben denker lecun line recognition handwritten postal words neural networks int 
pattern recognit 
artificial intell vol 

lecun bengio henderson weissman jackel line handwriting recognition neural networks spatial representation versus temporal representation proc 
int 
conf 
handwriting drawing 
ller hl fast neural net simulation dsp processor array ieee trans 
neural networks vol 
pp 
jan 
battiti second order methods learning steepest descent newton method neural computation vol 
pp 

kramer sangiovanni vincentelli efficient par learning algorithms neural networks advances neural information processing systems vol 
touretzky ed 
san mateo ca morgan kaufmann pp 

moller efficient training feed forward neural networks ph dissertation aarhus univ aarhus denmark 
becker lecun improving convergence back propagation learning second order methods univ toronto connectionist res 
group toronto ontario canada tech 
rep crg tr sept 
yann lecun member ieee received dipl ing degree ecole sup rieure ing en paris ph degree computer science universit pierre marie curie paris 
time universit pierre marie curie proposed early version back propagation learning algorithm neural networks 
joined department computer science university toronto toronto ont canada research associate 
joined adaptive systems research department bell laboratories holmdel nj worked neural networks machine learning handwriting recognition 
head image processing services research department labs research red bank nj 
published technical papers book chapters neural networks machine learning pattern recognition handwriting recognition document understanding image processing large scale integration vlsi design information theory 
addition topics current interests include video user interfaces image compression content indexing multimedia material 
dr lecun serves board machine learning journal served associate editor ieee transactions neural networks 
general chair machines learn workshop held year snowbird ut served program chair ijcnn nips 
member ieee neural network signal processing technical committee 
bottou received dip degree ecole polytechnique paris re en math matiques appliqu es degree ecole normale sup rieure paris ph degree computer science universit de paris sud 
time universit de paris sud worked speech recognition proposed framework stochastic gradient learning global training 
joined adaptive systems research department bell laboratories holmdel nj worked neural networks statistical learning theory local learning algorithms 
returned france research engineer 
chairman neural network simulators traffic forecasting software 
returned bell laboratories worked graph transformer networks optical character recognition 
member image processing services research department labs research red bank nj 
learning algorithms current interests include arithmetic coding image compression indexing 
lecun gradient learning applied document recognition bengio received eng 
degree electrical engineering sc 
ph degrees computer science mcgill university montreal canada respectively 
postdoctoral fellow massachusetts institute technology cambridge 
joined bell laboratories labs research red bank nj 
joined faculty computer science department universit de montr montr canada associate professor 
neural networks research interests centered learning algorithms especially data sequential spatial nature speech handwriting time series 
patrick haffner graduated ecole polytechnique paris ecole nationale sup rieure des communications enst paris 
received ph degree speech signal processing enst 
worked design tdnn ms tdnn architectures atr japan carnegie mellon university 
research scientist cnet france com france developed connectionist learning algorithms telephone speech recognition 
joined bell laboratories holmdel nj worked application optical character recognition transducers processing financial documents 
joined image processing services research department labs research red bank nj 
research interests include statistical connectionist models sequence recognition machine learning speech image recognition information theory 
proceedings ieee vol 
november 
