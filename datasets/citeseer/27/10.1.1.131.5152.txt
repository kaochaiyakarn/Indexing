data mining knowledge discovery springer science business media manufactured netherlands 
automatic subspace clustering high dimensional data rakesh agrawal almaden ibm com ibm almaden research center harry road san jose ca johannes gehrke johannes cs cornell edu computer science department cornell university ithaca ny dimitrios gunopulos dg cs ucr edu department computer science eng university california riverside riverside ca prabhakar raghavan verity com verity editor geoff webb 
data mining applications place special requirements clustering algorithms including ability find clusters embedded subspaces high dimensional data scalability user comprehensibility results non presumption canonical data distribution insensitivity order input records 
clique clustering algorithm satisfies requirements 
clique identifies dense clusters subspaces maximum dimensionality 
generates cluster descriptions form dnf expressions minimized ease comprehension 
produces identical results irrespective order input records presume specific mathematical form data distribution 
experiments show clique efficiently finds accurate clusters large high dimensional datasets 
keywords subspace clustering clustering dimensionality reduction 
clustering descriptive task seeks identify homogeneous groups objects values attributes dimensions jain dubes kaufman rousseeuw 
clustering techniques studied extensively statistics arabie hubert pattern recognition duda hart fukunaga machine learning cheeseman stutz michalski stepp 
database community includes clarans ng han focused clarans ester birch zhang dbscan ester cure guha :10.1.1.121.9220:10.1.1.13.4395
done author ibm almaden 
agrawal current clustering techniques broadly classified categories jain dubes kaufman rousseeuw partitional hierarchical objects clustering criterion sokal partitional clustering obtains partition objects clusters objects cluster similar objects different clusters 
popular means medoid methods determine cluster representatives assign object cluster representative closest object sum distances squared objects representatives minimized 
clarans ng han focused clarans ester birch zhang viewed extensions approach large databases :10.1.1.13.4395
mode seeking clustering methods identify clusters searching regions data space object density large 
dbscan ester finds dense regions separated low density regions clusters objects dense region :10.1.1.121.9220
hierarchical clustering nested sequence partitions 
agglomerative hierarchical clustering starts placing object cluster merges atomic clusters larger larger clusters objects single cluster 
divisive hierarchical clustering reverses process starting objects cluster subdividing smaller pieces jain dubes 
cure guha extension approach scales size dataset 

desiderata data mining perspective emerging data mining applications place special requirements clustering techniques motivating need developing new algorithms 
effective treatment high dimensionality 
object data record typically dozens attributes domain attribute large 
meaningful look clusters high dimensional space average density points data space quite low berchtold 
compounding problem dimensions combinations dimensions noise values uniformly distributed 
distance functions dimensions data may ineffective 
clusters may exist different subspaces comprised different combinations attributes 
interpretability results 
data mining applications typically require cluster descriptions easily assimilated user insight explanations critical importance fayyad 
particularly important simple representations visualization techniques high dimensional spaces 
description clusters accurate detail lost 
common failing means type techniques distance metric typically manhattan euclidian distance clusters near spherical shape guha 
automatic subspace clustering high dimensional data scalability usability 
clustering technique fast scale number dimensions size input 
insensitive order data records 
presume canonical form data distribution 
current clustering techniques address points adequately considerable done addressing point separately 
problem high dimensionality tackled requiring user specify subspace subset dimensions cluster analysis 
business machines 
user identification subspaces quite error prone 
way address high dimensionality apply dimensionality reduction method dataset 
methods principal component analysis karhunen lo transformation duda hart fukunaga optimally transform original data space lower dimensional space forming dimensions linear combinations attributes 
new space property distances points remain approximately 
techniques may succeed reducing dimensionality shortcomings 
new dimensions difficult interpret making hard understand clusters relation original data space 
second techniques effective identifying clusters may exist different subspaces original data space 
gives dimensional analogue shows general means algorithm variants may fail find meaningful clusters high dimensionality spaces 
algorithms distance metrics take dimensions account 
data include sets dimensions uniform distributions noise results clusters include points close clusters small 
points uniformly distributed dimension clustered dimension 
result form elongated clusters clusters example 
choose 
means minimizes cluster variation mi dist mean th record th cluster 
case uniform distribution dimension age clusters include points far apart clustering shown 
agrawal small example means clustering algorithm combines points different clusters clusters produces large example means clustering algorithm partitions data clusters small ones 
discuss points appendix section 
clustering algorithms developed database community birch clarans dbscan designed scalable emphasis earlier statistics machine learning literature ng han zhang :10.1.1.13.4395
techniques developed discover clusters full dimensional space 
surprising effective identifying clusters exist subspaces original data space 
section provide experimental results birch dbscan support observation 

contributions layout algorithm henceforth referred clique satisfies desiderata 
clique automatically finds subspaces high density clusters 
produces identical results irrespective order input records presume canonical distribution input data 
generates cluster descriptions form dnf expressions strives generate minimal descriptions ease comprehension 
empirical evaluation shows clique scales linearly number input records scalability number dimensions attributes data highest dimension clusters embedded increased 
formally defining problem subspace clustering section 
section heart clique 
section thorough performance evaluation clique conclude summary section 
subspace clustering giving formal description problem subspace clustering give intuitive explanation clustering model 
interested automatically identifying general subspaces high dimensional data space allow better clustering data points original space 
restricting search subspaces original space new dimensions example linear combinations original dimensions important restriction allows simpler comprehensible presentation results 
original dimensions typically real meaning user simple linear combination dimensions may hard interpret fayyad 
density approach clustering cluster region higher density points surrounding region 
problem automatically identify projections input data subset attributes property projections include regions high density 
approximate density data points partition data space find number points lie inside cell unit partitioning 
accomplished partitioning dimension number equal length intervals 
means automatic subspace clustering high dimensional data unit volume number points inside approximate density unit 
appropriate subspaces task find clusters corresponding projections 
data points separated valleys density function 
clusters unions connected high density units subspace 
simplify descriptions constrain clusters axis parallel hyper rectangles 
unit dimensional subspace described conjunction inequalities intersection axis parallel halfspaces defined dimensional intervals 
cluster union cells described dnf expression 
compact description obtained covering cluster minimal number maximal possibly overlapping rectangles describing cluster union rectangles 
notion subspace clustering tolerant missing values input data 
data point considered belong particular subspace attribute values subspace missing irrespective values rest attributes 
allows records missing values clustering accurate results replacing missing values values taken distribution 

problem statement ad set bounded totally ordered domains ad dimensional numerical space 
refer ad dimensions attributes input consists set dimensional points vm vi vi vi vid jth component vi drawn domain partition data space non overlapping rectangular units 
units obtained partitioning dimension intervals equal length input parameter 
unit intersection interval attribute 
unit form ud ui li hi open interval partitioning ai 
say vd contained unit ud vi hi ui 
selectivity unit defined fraction total data points contained unit 
call unit dense selectivity density threshold input parameter 
similarly define units subspaces original dimensional space 
consider projection data set atk ti unit subspace intersection interval attributes 
cluster maximal set connected dense units dimensions 
dimensional units connected common face exists dimensional unit connected connected 
units rt common face dimensions assume dimensions tk atk rt htk tk 
tk region dimensions axis parallel rectangular dimensional set 
interested regions expressed unions units henceforth agrawal 
illustration definitions 
region mean unions 
region expressed dnf expression intervals domains ai 
say region contained cluster contained cluster said maximal proper superset contained minimal description cluster non redundant covering cluster maximal regions 
minimal description cluster set maximal regions union equals union proper subset equal formally define problem subspace clustering subspace clustering 
set data points input parameters find clusters subspaces original data space minimal description cluster form dnf expression 
examples 
dimensional space age salary partitioned grid 
unit intersection intervals example unit age salary 
region rectangular union units 
regions age salary age salary 
assuming dense units shaded cluster 
note maximal region contained cluster maximal region 
minimal description cluster dnf expression age salary age salary 
assuming dimensional unit dense clusters original data space 
points projected salary dimension dimensional dense units 
connected clusters dimensional salary subspace salary salary 
cluster age subspace dense unit subspace 
remarks 
model generalized allow different values different dimensions 
require scaled account difference relative volumes checking density units different subspaces 
automatic subspace clustering high dimensional data 
identification clusters subspaces projections original data space 
model adapted handle categorical data 
arbitrary order introduced categorical domain 
partitioning scheme admits categorical value interval places empty interval different values 
consequently dimension chosen clustering clusters value dimension 
model clustering points subspace considered nonparametric 
parameters number intervals dimension density threshold approximate density space 
presume specific mathematical forms data distribution data points separated valleys density function 
related 
similar approach clustering high dimensional data proposed shoshani 
technique computes approximation density function user specified subspace grid uses function cluster data 
hand automatically discover interesting subspaces generate minimal descriptions clusters 
different technique find rectangular clusters high density projection data space proposed friedman 
algorithm works top fashion 
starting full space greedily chooses projection taken solution step order get closer optimal solution 
subspace identification problem related problem finding quantitative association rules identify interesting regions various attributes srikant agrawal miller yang :10.1.1.40.8600
techniques proposed quite different 
imagine adapting tree classifier designed data mining 
mehta shafer subspace clustering :10.1.1.104.152
tree growth phase splitting criterion changed clustering criterion average cluster diameter optimized 
tree pruning phase minimize total description length clusters obtained data description clusters 
aggarwal yu proposed different technique identifying clusters subspaces proclus 
technique similar iterative clustering agrawal techniques means medoids algorithms described 
technique iteratively groups objects clusters eliminates relevant dimensions clusters 
clustering model density proposed clustering results differ 
note proclus optimises criterion similar means algorithm find spherically shaped clusters 
number clusters average number dimensions cluster user defined parameters 
orclus aggarwal yu modifies proclus algorithm adding merging process clusters selecting cluster principal components attributes 
lac proposed algorithm discovers clusters subspaces spanned different combinations dimensions local weightings features 
approach mitigates risk loss information encountered feature selection global dimensionality reduction techniques 
procopiuc density projective clustering algorithm doc proposed 
approach requires maximum distance attribute values maximum width bounding hypercubes parameter input pursues optimality criterion defined terms density cluster corresponding subspace 
monte carlo procedure developed approximate high probability optimal projective cluster 
practice may difficult set parameters doc relevant attribute different local variance 

algorithms clustering technique clique consists steps 
identification subspaces contain clusters 

identification clusters 

generation minimal description clusters 
discuss algorithms steps section 

identification subspaces contain clusters difficulty identifying subspaces contain clusters lies finding dense units different subspaces 

bottom algorithm finding dense units 
simplest way identify dense units create histogram subspaces count points contained unit 
approach infeasible high dimensional data 
bottom algorithm exploits monotonicity clustering criterion respect dimensionality prune search space 
algorithm similar apriori algorithm mining association rules aggarwal 
somewhat similar bottom scheme automatic subspace clustering high dimensional data register determining modes high dimensional histograms 
lemma monotonicity 
collection points cluster dimensional space part cluster dimensional projections space 
proof dimensional cluster includes points fall inside union dense units 
units dense selectivity 
projections unit large selectivity include points inside dense 
units cluster connected projections connected 
follows projections points lie cluster dimensional projection 
algorithm algorithm proceeds level level 
determines dimensional dense units making pass data 
having determined dimensional dense units candidate dimensional units determined candidate generation procedure 
pass data find candidate units dense 
algorithm terminates candidates generated 
candidate generation procedure takes argument dk set dimensional dense units 
returns superset set dimensional dense units 
assume relation represents lexicographic ordering attributes 
self join dk join condition units share dimensions 
pseudo code join operation ai represents ith dimension unit li hi represents interval ith dimension 
insert ck select lk hk lk hk dk dk ak ak lk lk hk hk ak ak discard dense units ck projection dimensions included ck 
correctness procedure follows property dimensional dense unit projections dimensions dense lemma 
scalability 
phase clique database records accessed dense unit generation 
generation ck storage dense units dk candidate units ck 
making pass data need storage ck page buffer database records 
algorithm databases size 
memory needs managed carefully candidates agrawal 
candidate dimensional dense units computed dimensional dense units 
selectivity candidate computed database pass dense ones shown filled 
may swamp available buffer 
situation handled employing scheme aggarwal 

candidate units ck generated fit buffer database scanned determine selectivity candidates 
dense units resulting candidates written disk non dense candidates deleted 
procedure repeated ck examined 
time complexity 
dense unit exists dimensions projections subset dimensions different combinations dense 
running time algorithm exponential highest dimensionality dense unit 
aggarwal 
gunopulos 
shown candidate generation procedure produces minimal number candidates guarantee dense units 
highest dimensionality dense unit number input points 
algorithm passes database 
follows running time algorithm mk constant number database passes reduced adapting ideas toivonen brin 


making bottom algorithm faster 
procedure just described dramatically reduces number units tested dense may computationally infeasible task hand high dimensional data 
dimensionality subspaces considered increases explosion number dense units need prune pool candidates 
pruned set dense units form candidate units level dense unit generation algorithm 
objective dense units lie interesting subspaces 
automatic subspace clustering high dimensional data mdl pruning 
decide subspaces corresponding dense units interesting apply mdl minimal description length principle 
basic idea underlying mdl principle encode input data model select encoding minimizes code length rissanen 
assume subspaces sn 
pruning technique groups dense units lie subspace 
subspace computes fraction database covered dense units xs ui count ui count ui isthe number points fall inside ui 
number xs referred coverage subspace subspaces large coverage selected rest pruned 
rationale cluster exists dimensions subspace dimensions exist dense units subspace projections dense units cover cluster original dimensions cover points cluster 
sort subspaces descending order coverage 
want divide sorted list subspaces sets selected set pruned set model arrive cut point see illustration 
set compute mean cover fractions subspace set compute difference mean 
code length sum bit lengths numbers store 
decide prune subspaces si sn averages xs xs 
integers number bits required store log log respectively 
subspace store difference integer 
total length encoding cl log log log xs log xs code length minimized determine optimal cut point 
partitioning subspaces selected prune sets 
agrawal time complexity 
optimal cut positions sorted sequence sets pruned subspaces consider 
sorting optimal cut computed passes sorted sequence pass compute averages second pass compute cl 
pruning dense units subspaces low coverage algorithm faster tradeoff may clusters 
cluster exists dimensions projections subset dimensions clusters 
bottom approach considered want find cluster dimensions may pruned subspaces 

finding clusters input step clique set dense units dimensional subspace output partition units connected units connected 
partition cluster definition 
problem equivalent finding connected components graph defined follows graph vertices correspond dense units edge vertices corresponding dense units common face 
units corresponding vertices connected component graph connected path units common face cluster 
hand units corresponding vertices different components connected cluster 
depth search algorithm aho find connected components graph 
start unit assign cluster number find units connected 
units visited find repeat procedure 
algorithm input starting unit lk hk dfs num examine left neighbor dimension hl lk hk dense num dfs examine right neighbor dimension hr lk hk dense num dfs automatic subspace clustering high dimensional data time complexity 
number dense units subspace large dense unit selectivity dense units subsequent steps clique stored memory 
give asymptotic running times terms dense unit accesses dense units stored main memory data structure hash tree aggarwal allows efficient querying 
dense unit visited algorithm checks neighbors find connected units 
total number dense units subspace total number data structure accesses kn 

generating minimal cluster descriptions input step consists disjoint sets connected dimensional units subspace 
set cluster goal generate concise description 
generate minimal description cluster want cover units comprising cluster minimum number regions regions contain connected units 
cluster dimensional subspace aset regions subspace cover region contained unit contained regions optimal cover cover minimal number rectangles 
computing optimal cover known np hard dimensional case culberson 
best approximate algorithm known special case finding cover dimensional rectilinear polygon holes produces cover size bounded factor times optimal 
algorithm works dimensional case setting 
general set cover problem best known algorithm approximating smallest set cover gives approximation factor ln size universe covered feige lund yannakakis 
problem similar problem constructive solid geometry formulae zhang bowyer 
related problem covering marked boxes grid rectangles logic minimization 
hong 
clustering algorithms image analysis 
berger bigun wharton find rectangular dense regions 
domains datasets low dimensional spaces techniques computationally expensive large datasets high dimensionality 
solution problem consists steps 
greedily cover cluster number maximal rectangles regions discard redundant rectangles generate minimal cover 
consider maximal regions cover cluster cover regions find cover maximal regions simply extending non maximal regions 
covering maximal regions 
input step set connected dense units dimensional space output set maximal regions greedy growth algorithm task 
agrawal 
illustration greedy growth algorithm 
greedy growth 
arbitrary dense unit greedily grow described maximal region covers 
find unit covered maximal regions grow maximal region covers 
procedure units covered maximal region obtain maximal region covering dense unit grow dimension left right unit 
grow possible directions connected dense units contained result rectangular region 
grow region dimension left right region 
connected dense units obtaining possibly bigger rectangular region 
procedure repeated dimensions yielding maximal region covering order dimensions considered growing dense unit randomly determined 
illustrates algorithm works 
dense units appear shaded 
starting dense unit grow horizontal dimension finding rectangle consisting dense units 
extended vertical dimension 
extended maximal rectangle obtained case step find maximal region starting dense unit covered example time complexity 
show maximal region greedy growth algorithm perform dense unit accesses number dense units contained subspace lies number dimensions number dense units greedy growth algorithm access unit region covers ascertain part cluster 
addition access neighbor unit ascertain maximal 
number neighbor units bounded number dense units contained new maximal region covers far uncovered dense unit greedy growth algorithm find new regions 
new region requires dense unit accesses greedy growth algorithm performs total dense unit accesses 
automatic subspace clustering high dimensional data 
worst case greedy growth algorithm dimensional case assume dense units upper corners 
minimal cover include rectangle upper corner 
rectangle maximal reach lower staircase 
means circumference rectangle area sum sizes rectangles 
bound tight 
contain cluster dense units bounded parallel hyperplanes cylinder parallel dimension 
hyperplanes parallel dimensions boundary cluster touches hyperplanes consists convex vertices covered maximal region 
size region region reach hyperplane 
case greedy growth algorithm perform dense unit accesses 
shows dimensional analog case 
similarly show maximal regions pair corner hyperplane corner produce new maximal region pair 
greedy growth algorithm find ofthese 

minimal cover 
step clique takes input cover cluster finds minimal cover 
minimality defined terms number maximal regions rectangles required cover cluster 
computing optimal cover case known np hard dimensional case culberson 
best approximation algorithm known gives factor special case finding cover dimensional rectilinear polygon holes 
polygon holes algorithm approximates cover factor log size optimal cover 
similar bound derived approximation algorithm goodrich 
algorithm assumes space covering finite vc dimension case 
note asymptotically bound bad bound general set cover problem 
computational geometry literature contains algorithms covering points minimum number rectangles kleitman culberson dimensional datasets 
propose greedy heuristic removal heuristic 
remove cover smallest number units maximal region redundant unit contained maximal region 
break ties arbitrarily 
repeat procedure maximal region removed 
agrawal 
optimal covering consists regions size 
time complexity 
removal heuristic easy implement efficient execution 
needs simple scan sorted list regions 
cost sorting regions log number dense units upper bound number regions 
scan requires ri dense unit accesses region ri 
total number accesses regions ri 
unfortunately prove lemma worst case behavior removal heuristic lemma 
dimensions removal heuristic may construct cover bigger minimum factor 
consider instance cube opposite corner sub cubes removed 
cluster covered maximal regions solutions removal heuristic may regions example vertical size maximal regions removed see 
lower bound suggests approximation ratio worst case deteriorate number dimensions 
clear lower bound construction intricate structure arise practice 
stochastic analysis 
stochastic analysis suggesting removal heuristic unit independently dense probability model quite general number data points unit random variable drawn independently arbitrary distribution units specify threshold domain random variables unit independently dense probability depending underlying distribution 
application dense unit high selectivity small 
show provided small obtain approximation ratio 
prove theorem show lemma lemma 
constant depending probability cluster size automatic subspace clustering high dimensional data proof assume dense units subspace lexicographically ordered 
done ordering dimensions dimension ordering intervals significant 
cluster consists connected dense units 
units independently dense probability units dense pi bound number different shapes connected units take 
different trees vertices isomorphism lov sz 
fix tree wecan label nodes tree unit root grow set connected units tree labeling created 
traverse labeling 
units connected set corresponds node th unit added connected set th node encountered 
th unit connected unit corresponds parent th node 
different units connected unit follows step different ways grow tree 
consequently create different sets size contain unit tree nodes 
trees total number connected sets potential clusters 
assume cluster connected units includes unit connected exists depth search tree spans rooted assumption lexicographical ordering units tree unique 
process counts potential clusters size contain unit probability connected set units cluster include unit itfollows expected number clusters size include expected number clusters size expected number clusters 
follows probability cluster having size 
theorem 
fixed 
unit independently dense probability expected size cover obtain constant factor optimal cover 
proof number units cluster algorithm maximal rectangles cover 
complete proof bound expected number maximal rectangles cover cluster ia total number maximal rectangles ia clusters number clusters 
exists constant depending ia wehave ia clusters agrawal follows expected size cluster covers find constant factor total number clusters size optimal cover 

performance experiments empirically evaluate clique synthetic real datasets 
goals experiments assess efficiency accuracy clique efficiency determine running time scales dimensionality data space 
dimensionality clusters 
size database 
accuracy test clique recovers known clusters subspaces high dimensional data space 
experiments run mhz ibm rs model workstation 
data resided aix file system stored gb scsi drive sequential throughput mb second 

synthetic data generation synthetic data generator zait produce datasets clusters high density specific subspaces 
data generator allows control structure size datasets parameters number records number attributes range values attribute 
assume bounded data space dimensional cube data points live 
range values set attributes 
data space partitioned multidimensional grid generated dividing dimension partitions equal length 
box grid forms unit 
clusters hyper rectangles subset dimensions average density data points inside hyper rectangle larger average density subspace 
faces cluster parallel axis way describe cluster intersection set attribute ranges 
cluster descriptions provided user 
description specifies subspace hyper rectangle range attribute subspace 
attribute values data point assigned cluster generated follows 
attributes define subspace cluster embedded value drawn independently random uniform distribution range hyper rectangle 
remaining attributes value drawn independently random uniform distribution entire range attribute 
distributing specified number points equally specified clusters additional points added random noise 
values attributes points drawn independently random uniform distribution entire range attribute 
automatic subspace clustering high dimensional data 
synthetic data results scalability accuracy results observed synthetic data 
experiments run 
times seconds 
database size 
shows scalability size database increased records 
data space dimensions clusters different dimensional subspace set 
expected running time scales linearly size database number passes database change 
dimensionality data space 
shows scalability dimensionality data space increased 
database records clusters different dimensional subspace set 
curve exhibits quadratic behavior 
note problem searching interesting subspaces inherently scale dimensionality data space increases 
case searching clusters dimensions 
number dimensional 
scalability number data records 

scalability dimensionality data space 
agrawal 
number subspaces pruned 

scalability dimensionality hidden cluster 
subspaces dimensional space 
algorithm performs better worst case dimensions pruned dense unit generation phase 
shows percentage subspaces pruned mdl algorithm run 
input synthetic dataset dimensions hidden dimensional clusters set 
case dimensional subspaces dimensional subspaces pruned 
result pruning faster algorithm risk missing clusters 
dimensionality hidden clusters 
shows scalability highest dimensionality hidden clusters increased dimensional space 
case cluster embedded relevant subspace highest dimensionality 
database records set dimensional clusters dimensional dimensional clusters dimensional dimensional clusters 
selected lower highest dimensional clusters volume clusters increases cluster density decreases 
lower dimensions increase increase number dense units algorithm runs fast 
increase running time reflects time complexity algorithm automatic subspace clustering high dimensional data mk number records constant maximum dimensionality hidden clusters 
accuracy 
experiments original clusters recovered algorithm 
cases extra clusters reported typically comprising single dense unit low selectivity 
artifact byproduct data generation algorithm fact set low 
result units noise points dense 

comparisons birch dbscan svd ran clique birch dbscan synthetic datasets 
purpose experiments assess algorithms birch dbscan designed clustering full dimensional space subspace clustering 
task finding clusters full dimensional space design goal algorithms clique advantage 
clusters embedded dimensional subspaces varying dimensionality space 
clique able recover clusters case 
birch 
provided correct number clusters input postprocessing clustering algorithm built top birch 
output consists cluster centers full dimensional space 
input datasets points 
input clusters hyper rectangles dimensional subspaces values remaining attributes uniformly distributed 
equivalent hyper rectangle full data space remaining attributes include range 
birch successfully recovers cluster reports center approximately center equivalent hyper rectangle full data space number points reported cluster approximately correct 
results summarized table show birch discover dimensional clusters embedded dimensional data space fails dimensionality table 
birch experimental results 
dim 
dim 

clusters true clusters data clusters clusters identified agrawal table 
dbscan experimental results 
dim 
dim 

clusters true clusters data clusters clusters identified data space increases 
expected birch uses distance function takes dimensions account 
number dimensions uniform distribution increases distance function fails distinguish clusters 
dimensionality data space increases birch return clusters input parameter 
different randomly generated datasets returns clusters 
final column gives number correct embedded clusters birch identified 
dbscan 
dbscan discovers number clusters give number clusters input 
dbscan run data having dimensions 
input datasets points 
birch experiments clusters dimensional subspaces 
ran dbscan different input values best results table 
dbscan discover dimensional clusters dimensional data space dimensionality space reduced 
dimensional data space recover dimensional embedded clusters 
dbscan uses density cluster definition small number dimensions uniform distribution lower density space clusters 
svd 
singular value decomposition svd duda hart fukunaga synthetic datasets find dimensionality space reduced subspaces contain dense units deduced projections new space 
table rk gives ratio sum largest eigenvalues sum eigenvalues 
eigenvalues sorted decreasing order 
rk quantity rk indicates variance retained new space defined eigenvectors corresponding largest eigenvalues 
experiments variation original space smallest eigenvalue large largest achieve dimensionality reduction 
addition new projections linear combinations original vectors identify subspaces contain clusters 
automatic subspace clustering high dimensional data 
real data results table 
svd decomposition experimental results 
dim 
dim 

data clusters clusters rd table 
real data experimental results 
dim 
dim 

dataset data clusters clusters store bank ran clique datasets obtained insurance industry department store store bank bank 
table summarizes results experiment 
run selectivity threshold dimension divided intervals equal length 
show table dimensionality original data space highest dimensionality subspace clusters number clusters datasets 
cases discovered meaningful clusters embedded lower dimensional subspaces 

introduced problem automatic subspace clustering motivated needs emerging data mining applications 
solution propose clique designed find clusters embedded subspaces high dimensional data requiring user guess subspaces interesting clusters 
clique generates cluster descriptions form dnf expressions minimized ease comprehension 
insensitive order input records presume canonical data distribution 
designing clique combined developments fields including data mining stochastic complexity pattern recognition computational geometry 
agrawal empirical evaluation shows clique scales linearly size input scalability number dimensions data highest dimension clusters embedded increased 
clique able accurately discover clusters embedded lower dimensional subspaces clusters original data space 
having demonstrated computational feasibility automatic subspace clustering believe considered basic data mining operation operations associations sequential patterns discovery time series clustering classification business machines 
automatic subspace clustering useful applications data mining 
index olap data instance data space partitioned dense sparse regions 
data dense regions stored array tree structure store sparse regions 
currently users required specify dense sparse dimensions arbor software 
similarly precomputation techniques range queries olap data cubes ho require identification dense regions sparse data cubes 
clique purpose 
plan address problem evaluating quality clusterings different subspaces 
approach choose clusters maximize ratio cluster density expected density clusterings dimensionality 
plan investigate system support provided user selecting model parameters 
area try alternative approach finding dense units 
user interested clusters subspaces highest dimensionality techniques proposed algorithms discovering maximal itemsets bayardo lin kedem 
techniques allow clique find dense units high dimensionality having find projections 
appendix dimensionality reduction principal component analysis karhunen lo kl transformation optimal way dimensional points dimensional points error projections sum squared distances minimal duda hart fukunaga 
transformation gives new set orthogonal axes linear combination original ones sorted degree preserve distances points original space 
set points dimensions finding set axes kl transformation equivalent solving singular value decomposition problem matrix row represents data point 
svd matrix decomposition matrix diagonal matrix column orthonormal matrix 
matrix represents axes kl decomposition eigenvectors matrix ordered respective values matrix 
note new space potentially lower dimensionality 
addition small entry matrix corresponding vectors may eliminated lower dimensionality space obtained 
problem assume may clearly defined clusters original space try find dimensions clustering 
clearly points may automatic subspace clustering high dimensional data 
examples kl transformation helpful 
far apart dimensional space quite close specific projection dimensions 
effects projections trying capture 
addition interest comprehension want dimensions linear combinations original ones 
examples illustrate points 
data distributions shown 
original axes labeled 
cases data points uniformly distributed inside shaded area 
left case cluster right 
assuming number points cases density shaded regions different sets 
eigenvectors labeled corresponds eigenvalue largest magnitude tothe eigenvalue smallest magnitude 
eigenvalue larger indicating large variation axis 
eigenvectors essentially cases 
said kl transformation quite successful instances 
transformation find actual clusters distinguish cases argue clusters discovered projecting points examining distribution projections 
dimensional data uniformly distributed dimension contains clusters dimension 
despite clustering large variation 
examples kl transformation helpful 
agrawal axes 
results kl transformation eigenvectors shown 
variation eigenvalue corresponding eigenvector second largest eigenvalue quite large 
come space dimensionality 
furthermore projection new axes identify clusters 
right illustrates clusters may exist different subspaces 
data points uniformly distributed inside dimensional rectangles 
rectangles long skinny dense 
addition intersect 
reasonable selectivities clusters projections rectangles small faces cluster subspaces 
kl decomposition help large variation original axes 
resulting axes eigenvalues approximately equal 
means dimensional space approximates original space 
dimensional space kl transformation clustering 
density points dimensional space low obtain clustering 
acknowledgment code clique builds components ramakrishnan srikant wrote quantitative association rules 
srikant generously explained code modified places meet clique requirements 
roberto bayardo provided helpful feedback improve presentation 
christoph julio ortega provided datasets study 
thankful prof 
kriegel raymond ng raghu ramakrishnan providing access dbscan clarans birch respectively 
notes 
clustering quest data mining research project ibm almaden 
preliminary version results appeared aggarwal 


considered addition heuristic view cluster empty space 
add cover maximal region cover maximum number uncovered units cluster 
break ties arbitrarily 
repeat procedure cluster covered 
general set cover addition heuristic known give cover factor ln optimum number units covered lov sz 
appear addition heuristic quality approximation matches negative results feige lund yannakakis obvious choice 
implementation high dimensional geometric setting inefficient 
implementation requires complex computation number uncovered units candidate maximal region cover 
residual uncovered regions arise cover formed complicated efficient data structures known efficiently maintaining uncovered units 

run experiments clarans code required modification points high dimensions 
expect clarans show similar behavior birch identifying clusters embedded subspaces 
automatic subspace clustering high dimensional data aggarwal yu 
finding generalized projected clusters high dimensional spaces 
proc 
sigmod conference pp 

procopiuc wolf yu park 
fast algorithms projected clustering 
proc 
acm sigmod int 
conf 
management data philadelphia pa agrawal gehrke gunopulos raghavan 
automatic subspace clustering high dimensional data data mining applications 
proc 
acm sigmod int 
conf 
management data pp 

agrawal mannila srikant toivonen verkamo 
fast discovery association rules 
advances knowledge discovery data mining fayyad piatetsky shapiro smyth uthurusamy eds 
aaai mit press chap pp 

aho hopcroft ullman 
design analysis computer algorithms 
addison 
arabie hubert 
overview combinatorial data 
clustering classification 
arabie hubert eds 
new jersey world scientific pub pp 

arbor software 
application manager user guide version edition 
bayardo 
efficiently mining long patterns databases 
proc 
acm sigmod conference management data seattle washington 
berchtold bohm keim kriegel 

cost model nearest neighbor search highdimensional data space 
proceedings th symposium principles database systems pods pp 

berger 
algorithm point clustering grid generation 
ieee transactions systems man cybernetics 
brin motwani ullman tsur 
dynamic itemset counting implication rules market basket data 
proc 
acm sigmod conference management data 
goodrich 
optimal set covers finite vc dimension 
proc 
th acm symp 
computational geometry pp 

cheeseman stutz 
bayesian classification autoclass theory results 
advances knowledge discovery data mining 
fayyad piatetsky shapiro smyth uthurusamy eds 
chap 
aaai mit press pp 

register 
numerical classification method partitioning large multidimensional mixed data set 
technometrics 
papadopoulos gunopulos ma 
subspace clustering high dimensional data 
siam international conference data mining sdm 
duda hart 
pattern classification scene analysis 
john wiley sons 

method apparatus storing retrieving multi dimensional data computer memory 
patent 
ester kriegel sander xu 
density algorithm discovering clusters large spatial databases noise 
proc 
nd int conference knowledge discovery databases data mining portland oregon 
ester kriegel xu 
database interface clustering large spatial databases 
proc 
st int conference knowledge discovery databases data mining montreal canada 
fayyad piatetsky shapiro smyth uthurusamy 
eds 

advances knowledge discovery data mining 
aaai mit press 
feige 
threshold ln approximating set cover 
proceedings eighth annual acm symposium theory computing pp 


performance guarantees sweep line heuristic covering rectilinear polygons rectangles 
siam disc 
math 
kleitman 
algorithm constructing regions rectangles independence minimum generating sets collections intervals 
proc 
th annual symp 
theory computing washington pp 

agrawal friedman 
optimizing noisy function variables application data mining 
uw msr summer research institute data mining 
fukunaga 
statistical pattern recognition 
academic press 
guha rastogi shim 
cure efficient clustering algorithm large databases 
proceedings acm sigmod pp 

gunopulos khardon mannila 
data mining hypergraph transversals machine learning 
proc 
th acm symp 
principles database systems pp 

ho agrawal megiddo srikant 
range queries olap data cubes 
proc 
acm sigmod conference management data tucson arizona 
hong 
mini heuristic algorithm level logic minimization 
selected papers logic synthesis integrated circuit design newton eds 
ieee press 
business machines 

ibm intelligent miner user guide version release sh edition july 
jain dubes 
algorithms clustering data 
prentice hall 
kaufman rousseeuw 
finding groups data cluster analysis 
john wiley sons 
lin 
kedem 
pincer search new algorithm discovering maximum frequent sets 
proc 
th int conference extending database technology edbt valencia spain 
lov sz 
ratio optimal integral fractional covers 
discrete mathematics 
lund yannakakis 
hardness approximating minimization problems 
proceedings acm symposium theory computing pp 


np complete set covering problems 
thesis mit 
mehta agrawal rissanen 
sliq fast scalable classifier data mining 
proc 
fifth int conference extending database technology edbt avignon france 
michalski stepp 
learning observation conceptual clustering 
machine learning artificial intelligence approach michalski carbonell mitchell eds 
volume morgan kaufmann pp 

miller yang 
association rules interval data 
proc 
acm sigmod international conf 
management data pp 

ng han 
efficient effective clustering methods spatial data mining 
proc 
vldb conference santiago chile 
procopiuc jones agarwal murali 
monte carlo algorithm fast projective clustering 
sigmod 
culberson 
covering simple orthogonal polygon minimum number orthogonally convex polygons 
proc 
acm rd annual computational geometry conference pp 

rissanen 
stochastic complexity statistical inquiry 
world scientific publ 
bigun 
hierarchical image segmentation multi dimensional clustering boundary refinement 
pattern recognition 
shafer agrawal mehta 
sprint scalable parallel classifier data mining 
proc 
nd int conference large databases bombay india 
shoshani personal communication 
sokal 
numerical taxonomy 
freeman 

minimum dissection rectilinear polygon arbitrary holes rectangles 
proc 
acm th annual computational geometry conference berlin germany pp 

srikant agrawal 
mining quantitative association rules large relational tables 
proc 
acm sigmod conference management data montreal canada 
toivonen 
sampling large databases association rules 
proc 
nd int conference large databases mumbai bombay india pp 

wharton 
generalized histogram clustering multidimensional image data 
pattern recognition 
automatic subspace clustering high dimensional data zait 
comparative study clustering methods 
generation computer systems 
zhang bowyer 
csg set theoretic solid modelling nc machining blend surfaces 
proceedings second annual acm symposium computational geometry pp 

zhang ramakrishnan livny 
birch efficient data clustering method large databases 
proc 
acm sigmod conference management data montreal canada 
