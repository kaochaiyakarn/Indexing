machine learning kluwer academic publishers boston 
manufactured netherlands 
technical note model trees classi cation eibe frank eibe cs waikato ac nz yong wang cs waikato ac nz stuart cs waikato ac nz geoffrey holmes geo cs waikato ac nz ian witten cs waikato ac nz department computer science university waikato hamilton new zealand received sep accepted nov final manuscript nov editor raymond mooney 
model trees type decision tree linear regression functions leaves form basis successful technique predicting continuous numeric values 
applied classi cation problems employing standard method transforming classi cation problem problem function approximation 
surprisingly simple transformation model tree inducer quinlan generates accurate classi ers state art decision tree learner particularly attributes numeric 
keywords model trees classi cation algorithms decision trees 
applications machine learning practice involve predicting class takes continuous numeric value technique model tree induction proved successful addressing problems quinlan wang witten :10.1.1.34.885
structurally model tree takes form decision tree linear regression functions terminal class values leaves 
attributes play natural role regression functions discrete attributes handled natural way 
converse classical decision tree situation classi cation discrete attributes play natural role 
prompted symmetry situation wondered model trees classi cation 
turned classi ers surprisingly accurate 
order apply continuous prediction technique model trees discrete classi cation problems consider conditional class probability function seek model tree approximation 
classi cation class model tree generates greatest approximated chosen predicted class 
results show model tree inducer generate classi ers signi cantly accurate decision trees frank produced 
section explains method reviews features responsible performance 
experimental results standard datasets reported section 
section brie reviews related 
section summarizes results 

applying model trees classi cation model trees binary decision trees linear regression functions leaf nodes represent linear approximation unknown function 
model tree generated stages 
rst builds ordinary decision tree splitting criterion maximization intra subset variation target value 
second prunes tree back replacing subtrees linear regression functions appropriate 
model prediction smoothing process invoked compensate sharp discontinuities inevitably occur adjacent linear models leaves pruned tree 
original formulation model trees linear models internal nodes smoothing process incorporated leaf models manner described 
section rst describe salient aspects model tree algorithm 
describe procedure new trees classi cation 
justi cation procedure subsection example inferred class probabilities arti cial situation true probabilities known 

model tree algorithm construction model trees clearly described quinlan account scheme 
implementation called described wang witten implementation details 
freely available version di ers described wang witten improved handling missing values describe appendix 
changes tuning parameters 
necessary elaborate brie key aspects model trees surface discussion experimental results section 
rst central idea model trees linear regression step performed leaves pruned tree 
variables involved regression attributes participated decisions nodes subtree pruned away 
step omitted target taken average target value training examples reach leaf tree called regression tree 
second aspect smoothing procedure original formulation occurred model prediction 
idea rst leaf model compute predicted value lter value path back root smoothing value predicted linear model node 
quinlan calculation model trees classification np kq prediction passed higher node prediction passed node value predicted model node number training instances reach node constant 
quinlan default value experiments 
implementation achieves exactly ect slightly di erent representation 
nal stage model formation create new linear model leaf combines linear models path back tothe root leaf models automatically create smoothed predictions need adjustment predictions 
example suppose model leaf involved attributes linear coe cients model parent attributes ax cy dz combine single formula na nb kc kd continuing way root gives single smoothed linear model install leaf prediction 
smoothing substantially enhances performance model trees turns applies equally application classi cation 

procedure shows diagrammatic form model tree builder classi cation data taken known iris dataset 
upper part depicts training process lower part testing process 
training starts deriving new data sets original dataset possible value class 
case derived datasets setosa virginica varieties iris 
derived dataset contains number instances original class value set depending instance appropriate class 
step model tree inducer employed generate model tree new datasets 
speci instance output model trees constitutes approximation probability instance belongs associated class 
output values model trees approximations necessarily sum 
testing process instance unknown class processed model trees result approximation probability belongs class 
class model tree gives highest value chosen predicted class 
frank original dataset attributes 



class setosa setosa virginica 
setosa virginica derived datasets attributes target 




attributes 



target 
attributes target 




model trees new instance attributes target 



predicted class classi cation model trees classification class class example model trees classi cation class probabilities data generation training dataset inferred class probabilities 
justi cation learning procedure ectively divides instance space regions decision tree strives minimize expected mean squared error model tree output target values zero training instances particular region 
training instances lie particular region viewed samples underlying probability distribution assigns class values zero instances region 
standard procedure statistics estimate probability distribution minimizing mean square error samples taken devroye lugosi breiman friedman olshen stone 

example consider class problem true class probabilities linear functions attributes depicted summing point 
dataset instances generated randomly probabilities 
uniformly distributed values chosen probability value determine instance assigned rst second class 
data generated depicted frank classes represented lled hollow circles 
apparent density lled circles greatest lower left corner decreases upper right corner converse true hollow circles 
data submitted generates 
case structure trees generated trivial consist single node root 
shows linear functions represented trees 
discussion excellent approximations original class probabilities data generated 
class boundary point planes example illustrates classi ers model trees able represent oblique class boundaries 
reason model trees produced outperform univariate decision trees produced 
smooths regression functions adjacent leaves model tree 

experimental results experiments designed explore application model trees classi cation comparing results decision tree induction linear regression determining components essential performance 
speci cally address questions 
classi ers model trees compare state art decision trees classi ers simple linear regression 

important linear regression process leaves smoothing process 
answer rst question compare accuracy classi ers smoothed model trees generated pruned decision trees generated see performs better 
performance improvement conceivably due aspects procedure converts nominal attribute attribute values binary attributes procedure employed breiman generates model tree class 
test ran exactly encodings transforming nominal attribute binary ones procedure employed generating dataset class building decision tree dataset class probabilities provided arbitrate classes 
refer resulting algorithm report results linear regression lr input output encoding 
investigate second question rst compare accuracy classi ers model trees generated ones smoothed regression trees srt 
noted regression trees model trees constant functions leaf nodes represent oblique class boundaries 
apply smoothing operation routinely applies model trees 
compare accuracy classi ers smoothed model trees unsmoothed model trees umt 
model trees classification table 
datasets experiments dataset instances missing numeric binary nominal classes values attributes attributes attributes balance scale breast glass glass heart statlog hepatitis ionosphere iris letter pima indians segment sonar vehicle vote waveform noise zoo anneal audiology australian autos breast cancer heart heart horse colic hypothyroid german kr vs kp labor lymphography primary tumor sick soybean vowel smoothed regression tree special case smoothed model tree unsmoothed tree special case smoothed tree minor modi cations code needed generate srt umt models 

experiments standard datasets uci collection merz murphy experiments summarized table 
rst sixteen involve binary attributes seventeen involve non binary nominal attributes 
linear regression functions designed numerically valued domains binary attributes special case numeric frank attributes expect classi ers smoothed model trees particularly appropriate rst group 
table summarizes accuracy methods investigated 
results correct classi cations averaged fold non strati ed crossvalidation runs standard deviations shown 
folds scheme 
results starred show signi cant improvement corresponding result versa 
speak results signi cantly di erent di erence statistically signi cant level paired sided test points consisting estimates obtained fold cross validation run learning schemes compared 
table shows di erent methods compare 
entry indicates number datasets method associated column signi cantly accurate method associated row 

discussion results answer rst question observe outperforms fteen datasets outperforms 
numbers appear boldface table 
sixteen datasets having numeric binary attributes signi cantly accurate signi cantly accurate remaining datasets signi cantly accurate signi cantly accurate 
results show classi ers smoothed model trees generated signi cantly accurate pruned decision trees generated majority datasets particularly numeric attributes 
table shows signi cantly accurate twelve datasets rst column row signi cantly accurate rst row column 
signi cantly accurate seventeen datasets signi cantly accurate 
results show superior performance due change input output encoding 
complete discussion rst question comparing simple linear regression lr tom 
table shows lr performs signi cantly worse seventeen datasets signi cantly worse eighteen 
lr outperforms eleven datasets fourteen 
results linear regression surprisingly 
datasets application linear regression leads disastrous results recommend general technique 
answer second questions comparing accuracy classi ers ones smoothed regression trees srt assess importance linear regression process leaves incorporates 
table shows produces signi cantly accurate classi ers datasets signi cantly accurate ones 
compared pruned decision trees classi ers smoothed regression trees signi cantly accurate fteen datasets model trees classification table 
experimental results percentage correct classi cations standard deviation dataset lr srt umt balance scale breast glass glass heart statlog hepatitis ionosphere iris letter pima indians segment sonar vehicle vote waveform noise zoo anneal audiology australian autos breast cancer heart heart horse colic hypothyroid german kr vs kp labor lymphography primary tumor sick soybean vowel signi cantly accurate 
results show linear regression functions leaf nodes essential classi ers smoothed model trees outperform ordinary decision trees 
complete second question compare accuracy classi ers unsmoothed model trees umt 
table shows produces signi cantly accurate classi ers datasets signi cantly accurate classi ers 
comparison pruned decision trees leads smoothing process necessary ensure high accuracy model tree classi ers 
frank table 
results paired tests number indicates method column signi cantly outperforms method row lr srt umt lr srt umt 
related neural networks obvious alternative model trees classi cation tasks 
applying neural networks classi cation standard procedure approximate conditional class probability functions 
output node neural network approximates probability function class 
contrast neural networks probability functions classes approximated asingle network model trees necessary build separate tree class 
model trees er advantage neural networks user guesses structure size obtain accurate results 
built fully automatically ciently neural networks 
er opportunities structural analysis approximated class probability functions neural networks completely opaque 
idea treating multi class problem way classi cation problems possible value class applied standard decision trees dietterich bakiri 
quinlan predecessor generate way classi cation tree class 
accuracy obtained signi cantly inferior direct application original multi class problem able obtain better results error correcting output code simple class code 
smyth gray retro tted decision tree classi er kernel density estimators leaves order obtain better estimates class probability functions 
improve class probability estimates arti cial datasets classi cation accuracies signi cantly better 
resulting structure opaque includes function training instance 
investigated tting trees kernel estimators leaves time regression trees classi cation trees 
applied classi cation problems manner model trees advantage able represent non linear class boundaries linear oblique class boundaries model trees 
su er models employ kernel estimators 
important di erence smyth 
model tree algorithm smooths models model trees classification adjacent leaves model tree 
substantially improves performance model trees classi cation problems saw 
closely related method linear regression methods nding linear discriminants 
comparing experimental results obtained ordinary linear regression nd datasets linear regression performs cases gives disastrous results linear models simply appropriate 

shown classi cation problems transformed problems function approximation standard way successfully solved constructing model trees produce approximation conditional class probability function individual class 
classi ers derived outperform state art decision tree learner problems numeric binary attributes problems multivalued nominal attributes 
resulting classi ers comprehensible decision trees opaque produced statistical kernel density approximators 
expected time taken build model tree log linear number instances cubic number attributes 
model trees class built ciently dataset modest number attributes 
acknowledgments waikato machine learning group supported new zealand foundation research science technology provided stimulating environment research 
anonymous referees helpful constructive comments 
donated lymphography dataset 
appendix treatment missing values explain instances missing values treated version results 
testing decision tree calls test attribute value unknown instance propagated paths results combined linearly standard way quinlan 
problem deal missing values training 
tackle problem breiman 
describe surrogate split method split value attribute considered particular instance missing value di erent attribute surrogate split appropriately chosen value test vis frank replaced attribute value selected maximize probability test ect 
described procedure 
rst simpli cation 
breiman original procedure follows 
set training instances node values splitting attribute known 
subset split left branch corresponding subset right branch 
de ne way surrogate split number instances correctly assigned left subnode surrogate split jl andr jr corresponding number right subnode 
probability thats predicts estimated jsj 
chosen surrogate split maximizes estimate 
breiman chooses attribute value maximize estimate simpli cation choose surrogate attribute class continue select optimal value described 
stratagem reported wang witten 
second di erence blur sharp distinctions breiman procedure 
original procedure training instance value attribute missing assigned left right subnode 
produces sharp step function discontinuity inappropriate cases poor predictor modi cation employed soften decision making stochastic probability curve illustrated 
steepness transition determined likelihood test assigning instance incorrect subnode assessed considering training instances thevalue attribute known 
estimate probability assigns instance missing value rightmost subnode probability assigned left node just probability instance incorrectly assigned left subnode estimated pil jl likewise probability assigned right subnode pcr jr ml mean class value instances corresponding value estimate pr model form pr ax class value chosen curve pass points ml pil pcr shown 
desired ect approximating sharp step function predictor il cr decision unimportant ml prediction unreliable pil signi cantly greater pcr signi cantly decision particularly important ml di er appreciably 
training instance stochastically assigned right subnode probability pr 
testing surrogate splitting class model trees classification cr il class soft step function model tted training data value course unavailable 
instance propagated left right subnodes resulting outcomes combined linearly weighting scheme described quinlan left outcome weighted proportion training instances assigned left subnode right outcome proportion assigned right subnode 
notes 
successor quinlan 
commercial product test version available www com 

see www cs waikato ac nz ml 
realistic evaluation standard datasets imperative missing values accommodated 
removed instances missing values half datasets lower part table instances usable 

holte variant dataset classes combined classes deleted horse colic dataset attributes deleted attribute class 
deleted identi er attributes datasets 
breiman friedman olshen stone 

classi cation regression trees 
belmont ca wadsworth 
devroye lugosi 

theory pattern recognition 
new york springer verlag 
dietterich bakiri 

solving multiclass learning problems error correcting output codes journal ai research 
holte 

simple classi cation rules perform commonly datasets machine learning 
merz murphy 

uci repository machine learning data bases www 
ics uci edu mlearn mlrepository html 
irvine ca university california department information computer science 
quinlan 

learning continuous classes proceedings australian joint conference onarti cial intelligence pp 

world scienti singapore 
frank quinlan 

programs machine learning 
morgan kaufmann 
smyth gray fayyad 

retro tting decision tree classi ers kernel density estimation proceedings international conference machine learning pp 

san francisco ca morgan kaufmann 


kernel regression trees proceedings poster papers european conference machine learning 
university economics faculty informatics statistics prague 
wang witten 

induction model trees predicting continuous classes proceedings poster papers european conference machine learning university economics faculty informatics statistics prague 
