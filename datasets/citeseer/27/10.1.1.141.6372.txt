feature selection high dimensional genomic microarray data eric xing michael jordan richard karp division computer science university california berkeley ca department statistics university california berkeley ca cs berkeley edu jordan cs berkeley edu karp cs berkeley edu report successful application feature selection methods classification problem molecular biology involving data points dimensional space 
approach hybrid filter wrapper approaches feature selection 
sequence simple filters culminating koller sahami markov blanket filter decide particular feature subsets subset cardinality 
compare resulting subset cardinalities cross validation 
investigates regularization methods alternative feature selection showing feature selection methods preferable problem 

structural functional data analysis human genome increased fold years presenting enormous opportunities challenges machine learning 
particular gene expression microarrays rapidly maturing technology provide opportunity assay expression levels thousands tens thousands genes single experiment 
assays provide input wide variety statistical modeling efforts including classification clustering density estimation 
example measuring expression levels associated kinds tissue tumor non tumor obtains labeled data sets build diagnostic classifiers 
number replicates experiments severely limited data analyze cf 
golub observations expression levels genes 
extreme observations features natural essential investigate feature selection regularization methods 
feature selection methods received attention classification literature kohavi john langley kinds methods generally studied filter methods wrapper methods 
essential difference approaches wrapper method algorithm build final classifier filter method 
classifier set features wrapper method searches space subsets cross validation compare performance trained classifier tested subset 
filter method hand attempts find predictive subsets features making simple statistics computed empirical distribution 
example algorithm ranks features terms mutual information features class label 
wrapper algorithms perform better filter algorithms require orders magnitude computation time 
additional problem wrapper methods repeated cross validation single data set lead uncontrolled growth probability finding feature subset performs validation data chance 
essence hypothesis spaces extremely large cross validation overfit 
theoretical attempts calculate complexity measures feature selection setting generally lead pessimistic exponentially data points needed provide guarantees choosing feature subsets ng described generic feature selection methodology referred fs ordered leads optimistic ng 
ng approach cross validation compare feature subsets different cardinality 
ng proves approach yields generalization error upper bounded logarithm number irrelevant features 
problem features filtering methods key advantage significantly smaller computational complexity wrapper methods reason methods main focus 
earlier papers analyzed microarray data filtering methods golub chow press 
show possible exploit prediction error oriented wrapper methods context large feature space 
particular adopt spirit ng fs ordered approach specific algorithmic instantiation general approach filtering methods choose best subsets cardinality 
simple filtering methods carry major pruning hypothesis space cross validation final comparisons 
feature selection methods search combinatorial space feature subsets regularization shrinkage methods trim hypothesis space constraining magnitudes parameters bishop 
consider example linear regression problem parameters fit squares 
regularization adds penalty term squares cost function typically squared norm norm 
terms multiplied parameter regularization parameter 
choosing cross validation obtains fit parameters shrunk zero 
approach effectively restricts hypothesis space providing protection overfitting feature selection methods aim provide 
goal obtain small feature sets computational reasons feature selection obligatory step 
goal obtain best predictive classifier regularization methods may perform better feature selection methods 
papers machine learning literature compared approaches directly propose current 

feature selection section describe feature selection methodology adopted 
summarize briefly approach proceeds phases 
phase unconditional univariate mixture modeling provide initial assessment viability filtering approach provide discretization second phase 
second phase rank features information gain measure substantially reducing number features input third phase 
third phase computationally intense procedure markov blanket filtering choose candidate feature subsets passed classification algorithm 
unconditional mixture modeling useful empirical assumption activity genes expression generally assume distinct biological states 
combination binary patterns multiple genes determines sample phenotype 
assumption expect marginal probability expression level modeled univariate mixture components includes degenerate case single component 
representative samples empirical marginals shown 
feature feature 
representative histograms gene expression measurements 
axes represents normalized expression level 
underlying binary state gene vary classes gene discriminative classification problem discarded 
suggests heuristic procedure measure separability mixture components assay discriminability feature 
fi denote component gaussian mixture model feature fi denotes means standard deviations mixing proportions mixture model 
fit parameters em algorithm 
note feature fi fit independently 
suppose denote underlying state gene latent variable zi 
suppose define decision fi feature fi posterior probability zi greater mixture model fi equal 
define mixture overlap probability zi fi zi zi fi zi 
mixture model true representation probability gene expression mixture overlap probability represent bayes error classification model 
probability heuristic surrogate discriminating potential gene assessed unconditional marginal 
note mixture model quantizer allowing discretize measurements feature 
simply replace continuous measurement fi associated binary value fi 
fact main mixture models remainder 
particular section quantized features define information gain measure 
information gain ranking turn methods class labels 
goal methods find approximation conditional distribution feature vector class label 
information gain commonly surrogate approximating conditional distribution classification setting cover thomas 
class labels induce partition 
sc 
probability partition empirical proportions subset suppose test feature fi induces partition training set ek 
sc ek sc ek ek 
define information gain due feature respect partition 
sc ek ek 
sc ek entropy function 
information gain provides simple initial filter screen features 
example rank genes order increasing information gain select features conservatively statistical significance test ben dor 
calculate information gain need quantize values features 
achieved approach unconditional mixture model quantization discussed previous section 
markov blanket filtering features pass information gain filter input computationally intensive subset selection procedure known markov blanket filtering technique due koller sahami 
subset feature set fg denote projection variables markov blanket filtering aims minimize discrepancy conditional distributions fg measured conditional entropy fg log kullback leibler divergence 
goal find small feature set small 
intuitively feature fi conditionally independent class label small subset features able omit fi compromising accuracy class prediction 
koller sahami formalize idea notion markov blanket 
definition markov blanket feature set class label set mi fi mi markov blanket fi fi fi mi fi mi proposition due koller sahami establishes relevance markov blanket concept measure proposition complete feature set subset fi 
mi mi markov blanket fi proposition implies find markov blanket feature fi feature set safely remove fi increasing divergence desired distribution 
koller sahami prove sequential filtering process unnecessary features removed feature tagged unnecessary existence markov blanket mi remains unnecessary stages features removed 
cases features markov blanket limited size look features approximate markov blanket purpose define fi fm fi fm fi fi fm fi fi fm 
markov blanket fi fi 
exact zero occur relax condition seek set fi small 
note really markov blanket fi fi 
suggests easy heuristic way search feature approximate markov blanket 
goal find small non redundant feature subset features form approximate markov blanket feature di strongly correlated fi construct candidate markov blanket fi collecting features highest correlations defined pearson correlations original non quantized feature vectors fi small integer 
algorithm proposed koller sahami initialize iterate feature fi mi set features fj fi correlations fi fj highest 
compute fi mi choose minimizes fi mi define fi heuristic sequential method far efficient methods conduct extensive combinatorial search subsets feature set 
heuristic method requires computation quantities form fm fi fi fm easily computed discretization discussed section 
classification algorithms gaussian classifier logistic regression classifier nearest neighbor classifier study 
section provide brief description classifiers 
gaussian classifier gaussian classifier generative classification model 
model consists prior probability class gaussian density class maximum likelihood estimates parameters readily obtained 
restricting binary classification posterior probability associated gaussian classifier logistic function quadratic function feature vector denote exp xt note covariance matrix diagonal features independent class obtain continuous valued analog popular naive bayes classifier 
functions underlying covariances means class priors 
classes equal covariance equal zero quadratic function reduces linear function 
logistic regression logistic regression discriminative counterpart gaussian classifier 
assume posterior probability logistic linear function feature vector parameter estimated 
geometrically classifier corresponds smooth function increasing zero decision hyperplane feature space 
maximum likelihood estimates parameter vector iterative optimization algorithms 
high dimensional setting small number data points stochastic gradient ascent provided effective optimization procedure 
stochastic gradient algorithm takes simple form yn xn parameter vector tth iteration xn step size chosen empirically experiments 
nearest neighbor classification simple nearest neighbor classification algorithm setting equal 
distance metric pearson correlation coefficient 

regularization methods regularization methods provide popular strategy cope overfitting problems bishop 
represent log likelihood associated probabilistic model data set simply maximizing log likelihood consider penalized likelihood define regularized estimate parameters arg max appropriate norm typically norm free parameter known regularization parameter basic idea penalty term leads significant decrease variance estimate expense slight bias yielding decrease risk 
take bayesian point predictive power genes information gain genes respect partition removal gene mb bayes error information gain divergence gene gene gene 
feature selection stage procedure 
genes ranked eq 
genes ranked eq 
genes ranked fi eq 

view interpret penalty term log prior case regularization viewed maximum posteriori estimation method 
regularization parameter generally set form cross validation 
penalty rotation invariant penalty shrinks parameters ray origin 
penalty hand shrinks parameters ball rotation invariant 
parameters shrink quickly parameters set zero case tibshirani 
penalty flavor feature selection method 
parameter estimation straightforward regularization setting penalty term simply contributing additive term gradient 
example case logistic regression penalty obtain stochastic gradient yn xn shrinkage origin apparent 
case gaussian classifier regularized ml estimate easily solved closed form 

experiments results section report results analysis data microarray classification problem 
data collection samples leukemia patients sample giving expression levels genes golub 
pathological criteria samples include type called type ii called aml 
samples split sets provider aml serving training set remaining test set 
goal learn binary classifier cancer subtypes gene expression patterns 
filtering results shows mixture overlap probability defined eq 
single gene ascending order 
seen small percentage genes overlap probability significantly smaller constitute random guessing gaussian model underlying mixture components construed class labels 
information gain provided individual gene respect partition leukemia class labels compared partition obtained mixture models 
small fraction genes induce significant information gain 
take top genes list proceed approximate markov blanket filtering 
displays values fi mi cf 
eq 
fi assessment extent approximate markov blanket mi subsumes information carried fi renders fi redundant 
genes ordered removal sequence right left 
note redundancy measure fi mi increases fewer genes remaining 
point fi mi decreases presumably compositional change approximate markov genes compared original contents genes removed 
increasing trend fi mi resumes 
fact real biological regulatory network fan fan generally small provides justification enforcing small markov 
case keep markov small avoid fragmenting small data set 
classification results shows training set test set errors different classifiers 
feature subset cardinality abscissa graphs chose feature subset markov blanket filtering 
classifier independent method feature subsets figures 
figures show classifiers initial coevolving trend training testing curves low dimensional feature spaces dimensionality differs different classifiers knn training error testing error gaussian generative model training error testing error logistic regression training error testing error error rate error rate error rate number features number features number features 
classification sequence different feature spaces increasing dimensionality due inclusion gradually qualified features 
classification knn classifier classification quadratic bayesian classifier gaussian generative model linear classifier obtained logistic regression 
classifiers genes selected stages feature selection 
classifiers quickly overfit training data 
logistic linear classifier knn test error tops approximately percent entire feature set genes 
generative gaussian quadratic classifier overfits severely full feature space 
classifiers best performance achieved significantly lower dimensional feature space 
classifiers knn requires features achieve best performance 
shows optimal choice number features possible achieve error rates gaussian classifier logistic regression classifier knn respectively 
course actual diagnostic practice test set available numbers optimistic 
choose number features automatic way leave cross validation training data 
cardinality feature subset feature subset chosen filtering method choose cardinalities cross validation 
essence hybrid filter method wrapper method filter method choose feature subsets wrapper method compare best subsets different cardinalities 
results leave cross validation shown 
note minima cross validation curves 
breaking ties choosing minima having smallest cardinality running resulting classifier test set obtain error rates gaussian classifier logistic regression classifier knn respectively 
compared prediction performance unconditional mixture modeling mm filter information gain ig filter case discretization provided table 
performance classification randomly selected features trials training error test error classifier max min average max min average knn gaussian logistic phase mixture modeling 
results logistic regression classifier shown 
seen number features determined cross validation mm filter compared full markov blanket filtering resulting classifier higher test set error versus 
ig filter selected number features test set error rate significantly higher 
result particular suggests sufficient simply performance relevance check select features redundancy reduction method markov blanket filter appears required 
note mm filter results better performance ig filter 
approach performs markov blanket filtering mm filter advantage require class labels 
opens possibility doing feature selection data set context unsupervised clustering see xing karp 
high dimensional problems may possible bypass feature selection algorithms obtain reasonable classification performance choosing random subsets features 
case leukemia data set shown results table 
experiments reported table chose randomly selected features classifier 
performance poorer case explicit feature selection 
knn cross validation error gaussian generative model cross validation error logistic regression cross validation error error rate error rate error rate number features number features number features 
plots leave cross validation error classifiers 
error rate lr mm filter cross validation error number features error rate lr ig filter cross validation error number features errors higher obtained explicit feature selection 
comparison figures show range test set performance achievable feature selection regularization approaches respectively show feature selection curves generally associated smaller error 
regularization approach worst case leave features feel feature selection provides better alternative problem 
error rate 
plots leave cross validation error logistic regression classifier unconditional mixture modeling filter information gain filter 
regularization versus feature selection gaussian generative model training error testing error lambda error rate gaussian generative model lambda training error testing error 
training set test set error function regularization parameter 
results gaussian classifier penalties 
shows results regularized gaussian classifier penalties 
similar results logistic regression classifier 
choosing optimal value optimistically test set obtain test set errors gaussian classifier norm respectively 
logistic regression classifier obtain test set errors 

discussion shown feature selection methods applied successfully classification problem involving training data points dimensional space 
problem exemplifies situation increasingly common applications machine learning molecular biology 
microarray technology possible put probes genes entire genome chip data point provided experimenter lies high dimensional space defined size genome investigation 
high dimensional problems feature selection methods essential investigator sense data particularly goal study identify genes expression patterns meaningful biological relationships classification problem 
computational reasons impose important constraints 
demonstrated smaller problems extant literature feature selection kohavi john langley seen high dimensional problem studied feature selection lead improved classification 
classifiers studied generative gaussian classifier discriminative logistic regression classifier nn classifier performed significantly better reduced feature space full feature space 
attempted discuss biological sig specific features algorithm identified worth noting fifteen best features identified algorithm included set informative features golub similar degree overlap study data set chow press 
fact overlap perfect due redundancy features data set 
note particular algorithm works explicitly eliminate redundant features golub chow methods 
compared feature selection regularization methods leave feature set intact shrink numerical values parameters zero 
results show explicit feature selection yields classifiers perform better regularization methods 
advantages associated feature selection including computational feel feature selection provides preferred alternative data 
worth noting approaches mutually exclusive may worthwhile consider combinations 
andrew ng helpful comments 
partially supported onr muri nsf iis 
ben dor friedman yakhini 

scoring genes relevance 
agilent technologies technical report 
cancer class discovery class prediction gene expression monitoring 
science 
kohavi john 

wrapper feature subset selection 
artificial intelligence 
koller sahami 

optimal feature selection 
proceedings thirteenth international conference machine learning 
langley 

selection relevant features machine learning 
proceedings aaai fall symposium relevance 
aaai press 
ng 

feature selection learning exponentially irrelevant features training examples 
proceedings fifteenth international conference machine learning 
smith brown 

dna microarray system analyzing complex dna samples color fluorescent probe hybridization 
genome research 
tibshirani 

regression selection shrinkage lasso 
journal royal statistical society 
xing karp 

cliff clustering high dimensional microarray data iterative feature filtering normalized cuts 
proceedings nineteenth international conference intelligent systems molecular biology 
bishop 

neural networks pattern recognition 
oxford oxford university press 
chow moler mian 
press 
identification marker genes transcription profiling data mixture feature relevance experts 
physiological genomics 
cover thomas 

elements information theory 
new york wiley 
speed 

comparison discrimination methods classification tumors gene expression data 
technical report department statistics university california berkeley 
golub slonim tamayo mesirov loh downing bloomfield lander 

molecular classification 
