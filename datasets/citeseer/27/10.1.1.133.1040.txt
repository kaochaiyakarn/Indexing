machine learning proceedings thirteenth international conference 
experiments new boosting algorithm 
earlier introduced new boosting algorithm called adaboost theoretically significantly reduce error learning algorithm consistently generates classifiers performance little better random guessing 
introduced related notion pseudo loss method forcing learning algorithm multi label concentrate labels hardest discriminate 
describe experiments carried assess adaboost pseudo loss performs real learning problems 
performed sets experiments 
set compared boosting breiman bagging method aggregate various classifiers including decision trees single attributevalue tests 
compared performance methods collection machine learning benchmarks 
second set experiments studied detail performance boosting nearest neighbor classifier ocr problem 
boosting general method improving performance learning algorithm 
theory error weak learning algorithm consistently generates classifiers need little bit better random guessing 
despite potential benefits boosting promised theoretical results true practical value boosting assessed testing method real machine learning problems 
experimental assessment new boosting algorithm called adaboost 
boosting works repeatedly running weak learning algorithm various distributions training data combining classifiers produced weak learner single composite classifier 
provably effective boosting algorithms schapire freund :10.1.1.153.7626
described analyzed adaboost argued new boosting algorithm certain properties practical easier implement predecessors 
algorithm experiments described detail section 
home page www research att com orgs ssr people uid 
expected change www research att com uid near uid 
term weak learning algorithm practice boosting combined quite strong learning algorithm 
yoav freund robert schapire laboratories mountain avenue murray hill nj research att com describes distinct sets experiments 
set experiments described section compared boosting bagging method described breiman works general fashion repeatedly rerunning weak learning algorithm combining computed classifiers constructs simpler manner :10.1.1.32.9399:10.1.1.32.9399
details 
compared boosting bagging methods combining classifiers 
comparison allows separate effect modifying distribution round done differently algorithm effect voting done 
experiments compared boosting bagging number different weak learning algorithms varying levels sophistication 
include algorithm searches simple prediction rules test single attribute similar holte simple classification rules algorithm searches single decision rule tests conjunction attribute tests similar flavor rule formation part cohen ripper algorithm rnkranz widmer irep algorithm quinlan decision tree algorithm 
tested algorithms collection benchmark learning problems taken uci repository 
main experiments boosting performs significantly uniformly better bagging weak learning algorithm generates fairly simple classifiers algorithms 
combined boosting outperform bagging slightly results compelling 
boosting simple rules algorithm construct classifiers quite relative say 
kearns mansour argue viewed kind boosting algorithm comparison adaboost seen comparison competing 
see dietterich kearns mansour detail point 
second set experiments test performance boosting nearest neighbor classifier handwritten digit recognition 
case weak learning algorithm simple lets gain insight interaction boosting algorithm nearest neighbor classifier 
show boosting algorithm effective way finding small subset prototypes performs complete set 
show compares favorably standard method condensed nearest neighbor terms test error 
separate reasons improvement performance achieved boosting 
better understood effect boosting generates hypothesis error training set small combining hypotheses error may large better random guessing 
helpful learning problems having properties 
property holds real world problems observed examples tend varying degrees hardness 
problems boosting algorithm tends generate distributions concentrate harder examples challenging weak learning algorithm perform harder parts sample space 
second property learning algorithm sensitive changes training examples significantly different hypotheses generated different training sets 
sense boosting similar breiman bagging performs best weak learner exhibits unstable behavior :10.1.1.32.9399:10.1.1.32.9399
bagging boosting tries actively force weak learning algorithm change hypotheses changing distribution training examples function errors previously generated hypotheses 
second effect boosting variance reduction 
intuitively weighted majority hypotheses trained different samples taken training set effect reducing random variability combined hypothesis 
bagging boosting may effect producing combined hypothesis variance significantly lower produced weak learner 
bagging boosting may reduce bias learning algorithm discussed 
see kong dietterich discussion bias variance reducing effects voting multiple hypotheses breiman comparing boosting bagging terms effects bias variance :10.1.1.57.5909
set experiments compare boosting bagging try comparison separate bias variance reducing effects boosting 
previous 
drucker schapire simard performed experiments boosting algorithm 
schapire original boosting algorithm combined neural net ocr problem :10.1.1.153.7626
followup comparisons ensemble methods done drucker 
drucker cortes adaboost decision tree algorithm ocr task 
jackson craven adaboost learn classifiers represented sparse perceptrons tested algorithm set benchmarks 
quinlan conducted independent comparison boosting bagging combined collection uci benchmarks :10.1.1.49.2457
algorithm adaboost input sequence examples xm ym labels yi kg weak learning algorithm weaklearn integer specifying number iterations initialize 
call weaklearn providing distribution dt 

get back hypothesis ht 
calculate error ht dt 
ht set abort loop 

set 

update distribution dt dt dt zt ht xi yi zt normalization constant chosen dt distribution 
output final hypothesis arg max ht log algorithm adaboost 
boosting algorithm section describe boosting algorithm called adaboost 
see earlier details algorithm theoretical properties 
describe versions algorithm denote adaboost adaboost 
versions equivalent binary classification problems differ handling problems classes 
adaboost simpler version adaboost 
boosting algorithm takes input training set examples xm ym xi instance drawn space represented manner typically vector attribute values yi class label associated xi 
assume set possible labels finite cardinality addition boosting access unspecified learning algorithm called weak learning algorithm denoted generically weaklearn 
boosting algorithm calls weaklearn repeatedly series rounds 
round booster provides weaklearn distribution dt training set response weaklearn computes classifier hypothesis ht correctly classify fraction training set large probability respect dt 
weak learner goal find hypothesis ht minimizes training error pri dt ht xi yi note error measured respect distribution dt provided weak learner 
process continues rounds booster combines weak hypotheses ht single final hypothesis algorithm adaboost input sequence examples xm ym labels yi kg weak learning algorithm weaklearn integer specifying number iterations mg initialize jbj 
call weaklearn providing distribution dt 

get back hypothesis ht 

pseudo loss ht dt ht xi yi ht xi 
set 

update dt dt dt zt ht ht zt normalization constant chosen dt distribution 
output final hypothesis arg max tx log algorithm adaboost 
ht unspecified manner dt computed round computed 
different boosting schemes answer questions different ways 
adaboost uses simple rule shown 
initial distribution uniform compute distribution dt dt weak hypothesis ht multiply weight example number ht classifies xi correctly weight left unchanged 
weights renormalized dividing normalization constant zt 
effectively easy examples correctly classified previous weak hypotheses get lower weight hard examples tend misclassified get higher weight 
adaboost focuses weight examples hardest weaklearn 
number computed shown function final hypothesis weighted vote weighted linear threshold weak hypotheses 
instance outputs label maximizes sum weights weak hypotheses predicting label 
weight hypothesis ht defined log greater weight hypotheses lower error 
adaboost stated theorem 
theorem shows weak hypotheses consistently error slightly better training error final hypothesis drops zero exponentially fast 
binary classification problems means weak hypotheses need slightly better random 
theorem suppose weak learning algorithm weaklearn called adaboost generates hypotheses errors defined 
assume upper bound holds error final hypothesis jfi xi ty exp tx theorem implies training error final hypothesis generated adaboost small 
necessarily imply test error small 
weak hypotheses simple large difference training test errors theoretically bounded see earlier subject 
experiments indicate theoretical bound training error weak generally correct qualitatively 
test error tends better theory suggest indicating clear defect theoretical understanding 
main disadvantage adaboost unable handle weak hypotheses error greater 
expected error hypothesis randomly guesses label number possible labels 
weak hypotheses need just slightly better random guessing requirement error quite strong may hard meet 
adaboost second version adaboost attempts overcome difficulty extending communication boosting algorithm weak learner 
allow weak learner generate expressive hypotheses identifying single label choose set plausible labels 
may easier choosing just label 
instance ocr setting may hard tell particular image easy eliminate possibilities 
case choosing hypothesis may output set indicating labels plausible 
allow weak learner indicate degree plausibility weak hypothesis outputs vector components values close correspond labels considered plausible implausible respectively 
note vector values probability vector components need sum 
give weak learning algorithm expressive power place complex requirement performance weak hypotheses 
usual prediction error ask weak hypotheses respect sophisticated error measure call pseudo loss 
ordinary error computed respect examples pseudo loss computed respect distribution deliberately term plausible probable emphasize fact numbers interpreted probability label 
set pairs examples incorrect labels 
manipulating distribution boosting algorithm focus weak learner hard classify examples specifically incorrect labels hardest discriminate 
see boosting algorithm adaboost ideas achieves boosting weak hypothesis pseudo loss slightly better random guessing 
formally pair index training example incorrect label associated example set mg distribution distribution defined set 
round boosting adaboost supplies weak learner distribution dt 
response weak learner computes hypothesis ht form ht 
restriction ht 
particular prediction vector define probability distribution 
intuitively interpret representing binary question form predict label associated example xi yi correct label incorrect labels interpretation weight dt assigned represents importance distinguishing incorrect label example xi 
weak hypothesis ht interpreted manner 
ht xi yi ht xi ht correctly predicted xi label yi ht deems yi plausible implausible 
similarly ht xi yi ht xi ht incorrectly opposite prediction 
ht xi yi ht xi ht prediction taken random guess 
values ht interpreted probabilistically 
interpretation leads define pseudo loss hypothesis ht respect distribution dt formula dt ht xi yi ht xi space limitations prevent giving complete derivation formula explained detail earlier 
verified pseudo loss minimized correct labels yi assigned value incorrect labels yi assigned value 
note pseudo loss trivially achieved constant valued hypothesis ht 
weak learner goal find weak hypothesis ht small pseudo loss 
standard shelf learning algorithms may need modification manner modification straightforward 
receiving ht distribution updated rule similar adaboost 
final hypothesis outputs instance label maximizes weighted average weak hypothesis values ht 
theorem gives bound training error final hypothesis 
note theorem requires weak hypotheses pseudo loss slightly better trivial constant valued hypothesis regardless number classes 
weak hypotheses ht evaluated respect pseudo loss course evaluate final hypothesis ordinary error measure 
theorem suppose weak learning algorithm weaklearn called adaboost generates hypotheses pseudo losses defined 
upper bound holds error final hypothesis jfi xi number classes 
ty exp boosting bagging tx section describe experiments comparing boosting bagging uci benchmarks 
mention briefly small implementation issue learning algorithms modified handle examples weighted distribution created boosting algorithm 
possible booster distribution dt supplied directly weak learning algorithm method call boosting reweighting 
learning algorithms require unweighted set examples 
weak learning algorithm choose set examples independently random distribution replacement 
number examples chosen round matter discretion experiments chose examples round size original training set refer method boosting resampling 
boosting resampling possible pseudo loss 
case set chosen set replacement distribution dt 
procedure consistent interpretation discussed section 
experiments chose sample size jbj round resampling method 
weak learning algorithms mentioned weak learning algorithms experiments 
cases examples described vector values corresponds fixed set features attributes 
values may discrete continuous 
examples may missing values 
weak learners build hypotheses classify examples repeatedly testing values chosen attributes 
simplest weak learner call searches single attribute value test examples attributes missing name train test classes disc 
cont 
values soybean small labor promoters iris hepatitis sonar glass audiology stand cleve soybean large ionosphere house votes votes crx breast cancer pima indians di vehicle vowel german segmentation hypothyroid sick euthyroid splice kr vs kp satimage letter recognit table benchmark machine learning problems experiments 
minimum error pseudo loss training set 
precisely computes classifier defined attribute value predictions 
classifier classifies new example follows value attribute missing predict attribute discrete value example equal attribute continuous value predict predict 
ordinary error adaboost predictions 
simple classifications pseudo loss predictions vectors number classes 
algorithm searches exhaustively classifier form minimum error pseudo loss respect distribution provided booster 
words possible values 
considered 
preprocessing search carried error implementation nm time number attributes number examples 
typical pseudo loss implementation adds factor number class labels 
algorithm boosting reweighting 
second weak learner somewhat sophisticated search decision rule tests conjunction attribute value tests 
sketch main ideas algorithm call omit finer details lack space 
details provided full 
algorithm requires unweighted training set resampling version boosting 
training set randomly divided growing set data pruning set remaining 
error boosting pseudo loss boosting bagging bagging comparison pseudo loss versus ordinary error multi class problems boosting bagging 
phase growing set grow list attribute value tests 
test compares attribute value similar tests 
entropy potential function guide growth list tests 
list initially empty test added time time choosing test cause greatest drop potential 
test chosen branch expanded branch highest remaining potential 
list continues grown fashion test remains reduce potential 
second phase list pruned selecting prefix list minimum error pseudo loss pruning set 
third weak learner quinlan decision tree algorithm 
default options pruning turned 
expects unweighted training sample resampling 
attempt adaboost designed minimize error pseudo loss 
furthermore expect pseudoloss helpful weak learning algorithm strong algorithm usually able find hypothesis error 
bagging compared boosting breiman bootstrap aggregating bagging method training combining multiple copies learning algorithm :10.1.1.32.9399
briefly method works training copy algorithm bootstrap sample sample size chosen uniformly random replacement original training set size 
multiple hypotheses computed combined simple voting final composite hypothesis classifies example class assigned underlying weak hypotheses 
see details 
method quite effective especially breiman unstable learning algorithms small change data effects large change computed hypothesis 
order compare adaboost uses pseudoloss bagging extended bagging natural way weak learning algorithm minimizes pseudo loss ordinary error 
described section weak learning algorithm expects provided distribution set 
round bagging construct distribution bootstrap method select jbj chosen uniformly random replacement bagging boosting comparison boosting bagging weak learners 
assign weight jbj times number times chosen 
hypotheses ht computed manner combined voting natural manner combined hypothesis outputs label maximizes ht 
error pseudo loss differences bagging boosting summarized follows bagging uses resampling reweighting bagging modify distribution examples uses uniform distribution forming final hypothesis bagging gives equal weight weak hypotheses 
experiments conducted experiments collection machine learning datasets available repository university california irvine 
summary properties datasets table 
datasets provided test set 
reran algorithm times algorithms randomized averaged results 
datasets provided test set fold cross validation averaged results runs total runs algorithm dataset 
experiments set number rounds boosting bagging 
results discussion results experiments shown table 
figures indicate test error rate averaged multiple runs algorithm 
columns indicate weak learning algorithm pseudoloss adaboost error adaboost 
note pseudo loss class problems resulting algorithm identical corresponding error algorithm 
columns labeled indicate weak learning algorithm boosting bagging 
columns boosting bagging marked boost bag respectively 
goals carrying experiments determine boosting pseudo loss error worthwhile 
shows different algorithms performed class problems pseudo loss versus error 
point scatter plot represents error achieved competing algorithms benchmark point url www ics uci edu mlearn mlrepository html boosting boosting boosting bagging comparison versus various boosting bagging methods 
benchmark 
experiments indicate boosting pseudo loss clearly outperforms boosting error 
pseudo loss dramatically better error non binary problem slightly worse iris classes 
adaboost better adaboost discuss adaboost henceforth 
shows pseudo loss bagging gave mixed results comparison ordinary error 
pseudo loss gave better results occasionally pseudo loss hurt considerably 
shows similar scatterplots comparing performance boosting bagging benchmarks weak learner 
boosting plotted error rate achieved pseudo loss 
bagging best possible light error rate achieved error pseudo loss whichever gave better result particular benchmark 
binary problems experiments error 
simpler weak learning algorithms test boosting significantly uniformly better bagging 
boosting error rate worse bagging error rate pseudo loss error small number benchmark problems difference performance quite small 
average boosting improved error rate compared bagging gave improvement pseudo loss error 
boosting improved error rate bagging pseudo loss error 
weak learning algorithm boosting bagging evenly matched boosting slight advantage 
average boosting improved error rate bagging 
boosting beat bagging benchmarks bagging beat amount benchmark 
remaining benchmarks difference performance 
shows similar manner performed compared bagging compared boosting weak learners pseudo loss non binary problems 
shows boosting quite learning algorithm right comparison 
algorithm beat benchmarks tied lost 
mentioned average performance relative 
comparison improvement performance error pseudo loss error pseudo loss error name boost bag boost bag boost bag boost bag boost bag soybean small labor promoters iris hepatitis sonar glass audiology stand cleve soybean large ionosphere house votes votes crx breast cancer pima indians di vehicle vowel german segmentation hypothyroid sick euthyroid splice kr vs kp satimage letter recognit table test error rates various algorithms benchmark problems 

boosting somewhat better 
win tie lose numbers algorithm compared average improvement 
boosting nearest neighbor classifier section study performance learning algorithm combines adaboost variant nearest neighbor classifier 
test combined algorithm problem recognizing handwritten digits 
goal improve accuracy nearest neighbor classifier speed 
speed achieved reducing number prototypes hypothesis required number distance calculations increasing error rate 
similar approach nearest neighbor editing tries find minimal set prototypes sufficient label training set correctly 
dataset comes postal service usps consists training examples test examples 
training test examples evidently drawn different distributions significant improvement performance partition data training testing done random partition 
report results original partitioning training set test set sizes generated randomly partitioning union original training test sets 
image represented matrix bit pixels 
metric identifying nearest neighbor classifying instance standard euclidean distance images viewed vectors 
naive metric gives reasonably performance 
nearest neighbor classifier uses training examples prototypes achieves test error randomly partitioned data 
sophisticated tangent distance plans 
weak hypothesis defined subset training examples mapping new test point weak hypothesis predicts vector closest point round boosting weak hypothesis generated adding prototype time set set reaches prespecified size 
set choose mapping minimizes pseudo loss resulting weak hypothesis respect distribution 
initially set prototypes empty 
candidate prototypes selected random current marginal distribution training examples 
candidates causes largest decrease pseudo loss added set process repeated 
boosting process influences weak learning algorithm ways changing way random examples selected second changing calculation pseudo loss 
happens round boosting set pseudo loss significantly respect new distribution possibly different mapping 
case choosing new set prototypes reuse set additional boosting steps advantage gained partition exhausted details sample examples largest weight boosting iterations 
line iteration second iteration third iteration 
underneath image line form label example labels get highest second highest vote combined hypothesis point run algorithm corresponding normalized votes 
omitted 
ran iterations boosting algorithm number prototypes weak hypothesis second third remaining weak hypotheses 
sizes chosen errors weak hypotheses approximately equal 
compared performance algorithm strawman algorithm uses single set prototypes 
similar algorithm prototype set generated incrementally comparing prototype candidates step choosing minimizes empirical error 
compared performance boosting algorithm strawman hypothesis uses number prototypes 
compared performance condensed nearest neighbor rule cnn greedy method finding small set prototypes correctly classify entire training set 
results discussion results experiments summarized table 
table describes results experiments adaboost experiment repeated times different random seeds strawman algorithm repeated times cnn times 
compare results random partition data training testing partition defined usps 
see cases examples training error adaboost better strawman algorithm 
performance test set similar slight advantage adaboost hypotheses include examples slight advantage strawman fewer rounds 
examples error adaboost random partition average error achieved training set 
random partition final error error training set 
err num prototypes graphs performance boosting algorithm randomly partitioned usps dataset 
horizontal axis indicates total number prototypes added combined hypothesis vertical axis indicates error 
topmost jagged line indicates error weak hypothesis trained point weighted training set 
bold curve bound training error calculated theorem 
lowest thin curve medium bold curve show performance combined hypothesis training set test set respectively 
comparing cnn see strawman algorithm adaboost perform better cnn examples hypotheses 
larger hypotheses generated adaboost strawman better generated cnn 
main problem cnn tendency overfit training data 
adaboost strawman algorithm suffer overfitting 
shows typical run adaboost 
uppermost jagged line concatenation errors weak hypotheses respect distribution 
peak followed valley corresponds errors weak hypothesis constructed prototype time 
weighted error starts boosting iteration drops 
heaviest line describes upper bound training error guaranteed theorem bottom lines describe training test error final combined hypothesis 
interesting performance boosting algorithm test set improved significantly error training set zero 
surprising occam razor argument predict increasing complexity hypothesis error reduced zero degrade performance test set 
shows sample examples large weights boosting algorithm typical run 
types hard examples 
examples atypical wrongly labeled example line examples second line 
second type tends dominate iterations consists examples similar different labels examples versus third line 
algorithm point correct training examples clear votes assigned different labels example pairs trying improve discrimination random partition usps partition adaboost strawman cnn adaboost strawman cnn rnd size theory train test train test test size theory train test train test test size table average error rates training test sets percent 
columns labeled random partition random partition union training test sets usps partition means usps provided partition training test sets 
columns labeled theory give theoretical upper bounds training error calculated theorem 
size indicates number prototypes defining final hypothesis 
similar examples 
agrees intuition pseudo loss mechanism causes boosting algorithm concentrate hard discriminate labels hard examples 
demonstrated adaboost settings improve performance learning algorithm 
starting relatively simple classifiers improvement especially dramatic lead composite classifier outperforms complex learning algorithms 
improvement far greater achieved bagging 
note non binary classification problems boosting simple classifiers done effectively sophisticated pseudo loss 
starting complex algorithm boosting improve performance compelling advantage bagging 
boosting combined complex algorithm may give greatest improvement performance reasonably large amount data available note instance boosting performance letter recognition problem training examples 
naturally needs consider improvement error worth additional computation time 
rounds boosting quinlan got results rounds :10.1.1.49.2457
boosting may applications reducing error classifier 
instance saw section boosting find small set prototypes nearest neighbor classifier 
described boosting combines effects 
reduces bias weak learner forcing weak learner concentrate different parts instance space reduces variance weak learner averaging hypotheses generated different subsamples training set 
theory explain bias reducing effects need better theory variance reduction 

jason catlett william cohen extensive advice design experiments 
ross quinlan suggesting comparison boosting bagging 
leo breiman corinna cortes har ris drucker jeff jackson michael kearns ofer niyogi warren smith david wolpert anonymous icml reviewers helpful comments suggestions criticisms 
contributed datasets 
leo breiman :10.1.1.32.9399
bagging predictors 
technical report department statistics university california berkeley 
leo breiman 
bias variance arcing classifiers 
unpublished manuscript 
william cohen 
fast effective rule induction 
proceedings twelfth international conference machine learning pages 
tom dietterich michael kearns mansour 
applying weak learning framework understand improve 
machine learning proceedings thirteenth international conference 
harris drucker corinna cortes 
boosting decision trees 
advances neural information processing systems 
harris drucker corinna cortes jackel yann lecun vladimir vapnik 
boosting ensemble methods 
neural computation 
harris drucker robert schapire patrice simard 
boosting performance neural networks 
international journal pattern recognition artificial intelligence 
harris drucker robert schapire patrice simard 
improving performance neural networks boosting algorithm 
advances neural information processing systems pages 
yoav freund 
boosting weak learning algorithm majority 
information computation 
yoav freund robert schapire 
decision online learning application boosting 
available electronically web pages email request 
extended appeared computational learning theory second european conference eurocolt pages springer verlag 
johannes rnkranz gerhard widmer 
incremental reduced error pruning 
machine learning proceedings eleventh international conference pages 
geoffrey gates 
reduced nearest neighbor rule 
ieee transactions information theory pages 
peter hart 
condensed nearest neighbor rule 
ieee transactions information theory may 
robert holte 
simple classification rules perform commonly datasets 
machine learning 
jeffrey jackson mark craven 
learning sparse perceptrons 
advances neural information processing systems 
michael kearns mansour 
boosting ability top decision tree learning algorithms 
proceedings eighth annual acm symposium theory computing 
eun bae kong thomas dietterich :10.1.1.57.5909
error correcting output coding corrects bias variance 
proceedings twelfth machine learning pages 
ross quinlan 
programs machine learning 
morgan kaufmann 
ross quinlan :10.1.1.49.2457
bagging boosting 
proceedings fourteenth national conference artificial intelligence 
robert schapire :10.1.1.153.7626
strength weak learnability 
machine learning 
patrice simard yann le cun john denker 
efficient pattern recognition new transformation distance 
advances neural information processing systems volume pages 
