machine learning kluwer academic publishers boston 
manufactured netherlands 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization thomas dietterich tgd cs orst edu department computer science oregon state university corvallis editor doug fisher 
bagging boosting methods generate diverse ensemble classifiers manipulating training data base learning algorithm 
breiman pointed rely effectiveness instability base learning algorithm 
alternative approach generating ensemble randomize internal decisions base algorithm 
general approach studied previously ali pazzani dietterich kong 
compares effectiveness randomization bagging boosting improving performance decision tree algorithm 
experiments show situations little classification noise randomization competitive slightly superior bagging accurate boosting 
situations substantial classification noise bagging better boosting better randomization 
keywords decision trees ensemble learning bagging boosting monte carlo methods 
goal ensemble learning methods construct collection ensemble individual classifiers diverse accurate 
achieved highly accurate classification decisions obtained voting decisions individual classifiers ensemble 
authors demonstrated significant performance improvements ensemble methods breiman kohavi kunz bauer kohavi maclin opitz :10.1.1.105.6964:10.1.1.33.353:10.1.1.30.8572:10.1.1.32.9399
popular techniques constructing ensembles aggregation bagging breiman adaboost family algorithms boosting freund schapire :10.1.1.30.8572:10.1.1.133.1040:10.1.1.32.9399
methods operate base learning algorithm invoking times different training sets 
bagging training set constructed forming bootstrap replicate original training set 
words training set examples new training set constructed drawing examples uniformly replacement adaboost algorithm maintains set weights original training set adjusts weights classifier learned base learning algorithm 
adjustments increase weight examples misclassified base learning algorithm decrease weight examples correctly classified 
ways adaboost weights construct new training set give base learning algorithm 
boosting sampling examples drawn replacement probability proportional weights 
second method boosting weighting base learning algorithms accept weighted training set directly 
algorithms entire training set associated weights base learning algorithm 
methods shown effective quinlan 
bagging generates diverse classifiers base learning algorithm unstable small changes training set cause large changes learned classifier 
breiman explores causes instability learning algorithms discusses ways reducing eliminating 
bagging lesser extent boosting viewed ways exploiting instability improve classification accuracy 
adaboost requires instability bagging adaboost larger changes training set placing large weights examples 
explores alternative method constructing ensembles rely instability 
idea simple randomize internal decisions learning algorithm 
specifically implemented modified version release learning algorithm decision split introduce internal node tree randomized 
implementation computes best splits non negative information gain ratio chooses uniformly randomly 
continuous attributes possible threshold considered distinct split best splits may involve splitting attribute 
crude randomization technique 
imagine sophisticated methods preferred select splits higher information gain 
goal explore simple method works 
previous dietterich kong reported promising results technique tasks 
performed thorough experiment learning tasks 
compare randomized bagging adaboost boosting weighting 
explore effect random classification noise performance techniques 

methods started release modified support randomization bagging boosting weighting 
implement boosting weighting imported bug fixes release concern proper handling continuous splits weighted training examples 
employed domains drawn uci repository merz murphy 
domains shuttle satimage phoneme performed stratified fold cross validation evaluate ensemble methods running 
remaining domains large designated test sets employed standard train test methodology 
domains selected regard results current study domains tested part study 
randomization bagging constructed ensembles containing classifiers 
boosting constructed ensembles classifiers 
adaboost algorithm terminated early classifier weighted error greater unweighted error equal zero smaller ensemble necessarily 
cases evaluated ensembles pruned unpruned decision trees 
pruning confidence level 
check ensembles sufficiently large evaluated performance different ensemble sizes determine ensemble size matched exceeded performance final ensemble 
randomized bagged required ensemble sizes similar nearly runs converged reached accuracy ensemble size iterations 
king rook vs king krk domain required largest number iterations folds converged iterations 
letter recognition task required large number iterations ranging randomized bagging 
adaboost iterations sufficient domains cases iterations probably yield improvements 
include folds king rook vs king krk letter recognition splice phoneme segmentation waveform 
pruned trees generally required somewhat smaller ensembles unpruned trees effect minor 
domain algorithm configuration randomized bagged boosted test data determine pruning needed 
previous research shown pruning substantial difference algorithm performance quinlan want pruning decision confound algorithm comparison 
real applications choice prune internal cross validation training set 
test data decision making optimistic assumption cross validated decisions correctly 
table summarizes domains results experiments pruning employed 
find particular pattern pruning employed note adaboost pruning significant difference domains 
randomized pruning difference domains bagged pruning significant difference domains 
general lack significant differences probably result relatively low pruning confidence level employed 
performed statistical tests compare algorithm configurations 
domains cross validation performed applied fold test construct confidence interval difference error rates algorithms 
confidence interval include zero test concludes significant difference performance algorithms 
test known dietterich elevated type error incorrectly find significant difference time indicated confidence level 
test unable conclude difference algorithms table 
domains employed study 
column labeled asterisk indicates pruned trees employed 
error rate column gives error rate confidence limit 
error rates estimated fold cross validation phoneme satimage shuttle randomized bagged adaboosted error error error error index name rate rate rate rate sonar letter splice segment glass soybean autos satimage annealing krk heart heart breast phoneme voting vehicle lymph breast credit primary shuttle heart iris sick hepatitis credit waveform horse colic heart labor audiology hypo table 
pairwise combinations ensemble methods 
cell contains number wins losses ties algorithm row algorithm column 
adaboost bagged random bagged adaboost trusted finds difference regarded suspicion 
domains single test set employed constructed confidence interval normal approximation binomial distribution correction pairing algorithms 
test safe somewhat conservative 

results table summarizes results statistical tests 
ensemble methods randomized better domains bagged better adaboosted better 
able better ensemble methods 
summarizes observed differences randomized bagged 
randomized versus boosted 
plots called kohavi plots introduced ronny kohavi mlc system kohavi sommerfield dougherty 
point plots difference performance algorithms scaled performance 
example sonar task unpruned gives error rate randomized error rate bagged error rate 
means randomized give reduction error rate bagged give reduction 
difference percentage reduction error rate plotted 
upper lower bounds confidence interval similarly scaled 
vertical axis indicates importance observed difference terms improvement error bars indicate statistical significance observed difference 
plot domains sorted ascending order differences 
numerical indexes assigned domains ordering 
consider 
left shows domains indexes corresponding letter recognition splice segmentation satimage king rook vs king krk randomized clearly superior bagged 
conversely domains indexes corresponding waveform audiology hypo bagged superior randomized 
ali pazzani noticed domains bagging poorly tend domains large number training examples 
random bagged ratio domain index 
difference performance randomized bagged 
difference scaled error rate 
error bars give confidence interval cross validated test tends give intervals narrow 
domains numbered correspond entries table random adaboost ratio domain index 
difference performance randomized adaboosted 
difference scaled error rate 
error bars give confidence interval cross validated test tends give intervals narrow 
domains numbered correspond entries table random bagged ratio audiology hypo splice segment waveform satimage king rook vs king letter recognition total number examples non majority class es 
difference performance randomized bagged function number training examples non majority class 
difference scaled error rate 
error bars give confidence interval cross validated test tends give intervals narrow 
understand imagining bagging randomization produce particular decision tree leaves 
bootstrap sample contains examples corresponding leaves bagged tend grow decision tree 
limit infinite sample size grow tree bagging effect error rate 
conclude effectiveness bagging reduced training set large corresponding decision trees large 
effectiveness randomization hand depend size training set 
infinitely large training set randomized produce diverse set decision trees 
course ensemble probably accurate single tree grown randomness 
explore point plots difference accuracy randomized bagged function total number training examples non majority classes problem 
see domains randomization outperforms bagging domains non majority class examples 
domains bagging outperforms randomization cases confidence interval just barely avoids zero waveform training sets small 
cases suspicious cross validated test precisely situations test tends give incorrect results 
analysis conclude randomized certainly competitive probably superior bagged applications relatively little noise data 
disappointing aspect results shown table randomization reach zero error letter recognition domain 
previous study dietterich kong reported experiment fold randomized attained perfect performance letter recognition task training examples testing remaining 
attempted replicate result success able determine source discrepancy 
compare randomized adaboosted 
shows adaboost superior randomized domains indexes corresponding king rook vs king krk vehicle sick king rook pawn audiology hypo randomized superior 
domain randomized better splice index 
able identify particular characteristic domains explains adaboosted 
main adaboosted generally doing better randomized 
important issue explored previous researchers question ensemble methods perform situations large amount classification noise training test examples incorrect class labels 
aaai talk quinlan reported experiments showing adaboosted perform situations 
ali pazzani observed randomization noisy domains bagging 
experiments considered ensembles size 
conjectured larger ensembles able overcome effects noise 
explore effect classification noise added random class noise domains audiology hypo king rook vs king pawn satimage sick splice segment vehicle waveform 
data sets chosen pair ensemble methods gave statistically significantly different performance domains 
perform noise experiments letter recognition king rook vs king krk huge size data sets 
add classification noise rate chose fraction data points randomly replacement changed class labels incorrect label example chosen uniformly randomly incorrect labels 
data split subsets stratified fold cross validation stratification performed new labels 
table shows win lose tie counts pairs learning methods noise levels 
table reveals patterns confirm observations ali pazzani observations quinlan 
add table 
pairwise combinations methods levels noise domains 
cell contains number wins losses ties algorithm row algorithm column 
noise adaboost bagged random bagged adaboost noise adaboost bagged random bagged adaboost noise adaboost bagged random bagged adaboost noise adaboost bagged random bagged adaboost noise problems randomized adaboosted lose advantage bagged gains advantage 
example noise adaboosted beats domains ties noise adaboosted wins domains loses 
contrast bagged noise beats domains ties noise bagged wins domains ties 
compare bagging randomizing adaboosted see noise adaboost superior bagged randomized 
noise adaboost inferior bagged randomized 
classification noise destroys effectiveness adaboost compared ensemble methods compared domains 
compare bagged randomized see noise evenly matched 
noise bagging slight advantage wins losses ties 
high number ties indicates bagging randomizing behaving similarly amount noise increases 
analysis conclude best method applications large amounts classification noise bagged randomized behaving 
contrast adaboost choice applications 
way gain insight behavior ensemble methods construct error diagrams introduced dietterich 
diagrams help visualize accuracy diversity individual classifiers constructed ensemble methods 
pair classifiers measure accuracy average error rates test data measure diversity computing degree agreement statistic known 
construct scatter plot point corresponds pair classifiers 
coordinate diversity value coordinate mean accuracy classifiers 
statistic defined follows 
suppose classes square array cij contains number test examples assigned class classifier class second classifier 
define cii total number test examples 
estimate probability classifiers agree 
measure agreement 
difficulty problems class common reasonable classifiers tend agree simply chance pairs classifiers obtain high values 
statistic corrects computing cij estimates probability classifiers agree chance observed counts table 
specifically cij fraction examples classifier assigns class fraction examples second classifier assigns class classifier chooses examples assign class completely randomly probability simultaneously assign particular test example class product fractions 
cases classifiers lower measure agreement classifiers agree examples assign class definitions statistic computed agreement classifiers equals expected chance classifiers agree example 
negative values occur agreement expected chance systematic disagreement classifiers 
shows error diagrams randomized bagged adaboosted sick dataset 
illustrative diagrams domains 
see bagged gives compact cloud points 
point low error rate high value indicates classifiers accurate diverse 
randomized slightly worse error rate diverse collection hypotheses 
adaboosted hypotheses wide range accuracies degrees agreement 
clearly mean error mean error mean error kappa kappa kappa 
error diagrams sick data set bagged top randomized middle adaboosted bottom 
accuracy diversity increase points come near origin 
shows tradeoff accuracy diversity 
classifiers accurate diverse 
conversely diverse accurate 
shows dramatically adaboost strategy constructing ensembles produces diverse ensembles bagging randomizing 
pattern accuracy diversity observed data sets effect pattern relative performance 
seen adaboosted typically better bagged randomized explained greater diversity adaboosted 
relative performance bagging randomizing apparent error diagrams 
sick dataset example adaboost better bagging randomizing 
bagging randomizing statistically indistinguishable randomizing higher diversity 
shows error diagrams splice task shows pattern relative diversity 
adaboosted diverse randomized diverse bagged 
domain randomized outperforms boosting bagging 
explained part high error hypotheses adaboost creates near top error diagram 
error diagrams help understand effect noise ensemble methods 
shows error diagrams sick added classification noise 
compare see noise affects methods 
cloud points randomized basically shifted upward expect classification noise added 
note randomized diverse 
contrast bagged shifted upward left substantially diverse result noise 
cloud points adaboosted moves close error rate small values 
observe classifiers making nearly random guesses 
net result adaboost shifts best method worst randomizing bagging remain statistically indistinguishable 
general pattern observed data sets 
noise improves diversity bagging damages accuracy adaboost severely leaves randomized unaffected aside expected shift error rates 
figures show segment data set behaves noise added 
noise randomized slightly diverse bagged result randomized adaboosted tied accurate bagged 
noise added diversity randomized hardly changed diversity bagged substantially increased 
accuracy adaboosted severely degraded classifiers error rates greater 
net result bagging randomization equal performance noise better adaboost 
effects explained 
plausible explanation poor response adaboost noise mislabeled training examples tend receive mean error mean error mean error kappa kappa kappa 
error diagrams splice data set bagged top randomized middle adaboosted bottom 
mean error mean error mean error kappa kappa kappa 
error diagrams sick data set random classification noise 
bagged top randomized middle adaboosted bottom 
mean error mean error mean error kappa kappa kappa 
error diagrams segment data set 
bagged top randomized middle adaboosted bottom 
mean error mean error mean error kappa kappa kappa 
error diagrams segment data set random classification noise 
bagged top randomized middle adaboosted bottom 
high weights adaboost algorithm 
iterations training examples big weights mislabeled examples 
classifier learned mislabeled examples low values compared classifier learned equally weighted training examples 
fact expect see negative values observed 
improved diversity bagging explained observation 
suppose mislabeled example substantial effect learning algorithm classifier produces way outliers big effect linear regression 
example mislabeled example cause split examples side result training data fragmented decision tree inaccurate 
bootstrap replicate training set fraction training examples appear general 
average training examples omitted 
omitted examples mislabeled training examples omission lead large changes learned decision tree 
contrast randomized omits training examples 
splitting decision randomized continues making splits produces pure nearly pure leaf nodes 
randomized ignore mislabeled training examples 
diversity affected addition noise 
settings low noise randomized produces diverse classifiers bagged permits better 
furthermore ability randomized grow tree training data tend individual tree accurate 
fact randomized deliberately suboptimal splitting decisions may limit advantage reducing accuracy trees 
settings moderate amounts noise advantage data disadvantage bagging diverse occasionally gives better results 
test hypothesis adaboost placing weight noisy examples consider 
see weight placed average noisy data points uncorrupted data points 
consider fraction total weight placed corrupted data points rapidly converges adaboost placing half weight corrupted data points training set 
difficult verify hypothesis concerning effect noisy examples bagging 
research needed explore test hypothesis 

compared methods constructing ensemble classifiers randomizing bagging boosting 
experiments show set tasks boosting gives best results cases long little noise data 
randomizing bagging give quite similar results evidence randomizing slightly better bagging low noise settings 
mean weight training example corrupted examples uncorrupted examples iteration 
mean weight training example corrupted training examples remaining uncorrupted training examples sick data set 
added classification noise bagging clearly best method 
appears able exploit classification noise produce diverse classifiers 
performance adaboost destroyed classification noise error rates individual classifiers high 
surprisingly performance randomized classification noise bagging 
experiments showed randomization able increase diversity classifiers noise rate increased 
randomization method studied simple best candidate splits computed chosen uniformly random 
obvious step probability chosing split proportional information gain split 
refinement perform limited discrepancy randomization splits randomized tree specified value 
value set cross validation 
algorithm explicitly consider making 
random splits 
ensure best tree tree produced included ensemble 
randomization produce trees different accuracies worthwhile consider weighted vote adaboost weight determined accuracy tree training data 
improvements randomized competitive adaboost low noise settings 
form outlier identification removal randomized bagging high noise settings 
acknowledgments author gratefully acknowledges support national science foundation nsf iri cda 
notes 
annealing treated unmeasured values separate attribute values missing values 
auto class variable automobile 
breast cancer domains features treated continuous 
heart disease data sets recoded discrete values appropriate 
attributes treated continuous vs king krk data set 
lymphography lymph nodes lymph nodes nodes attributes treated continuous 
segment features rounded significant digits avoid roundoff errors 
shuttle attributes treated continuous 
voting records physician fee freeze attribute removed task challenging 
ali 

comparison methods learning combining evidence multiple models 
tech 
rep department information computer science university california irvine 
ali pazzani 

error reduction learning multiple descriptions 
machine learning 
bauer kohavi 

empirical comparison voting classification algorithms bagging boosting variants 
machine learning 
breiman 

heuristics instability stabilization model selection 
tech 
rep department statistics university california berkeley ca 
breiman 

bagging predictors 
machine learning 
breiman 

bias variance arcing classifiers 
tech 
rep department statistics university california berkeley ca 
dietterich 

approximate statistical tests comparing supervised classification learning algorithms 
neural computation 
dietterich kong 

machine learning bias statistical bias statistical variance decision tree algorithms 
tech 
rep department computer science oregon state university corvallis oregon 
available ftp ftp cs orst edu pub tgd papers tr bias ps gz 
freund schapire 

experiments new boosting algorithm 
proc 
th international conference machine learning pp 

morgan kaufmann 
kohavi kunz 

option decision trees majority votes 
proc 
th international conference machine learning pp 

morgan kaufmann 
kohavi sommerfield dougherty 

data mining mlc machine learning library 
international journal artificial intelligence tools 
maclin opitz 

empirical evaluation bagging boosting 
proceedings fourteenth national conference artificial intelligence pp 
cambridge ma 
aaai press mit press 
dietterich 

pruning adaptive boosting 
proceedings fourteenth international conference machine learning pp 
san francisco 
morgan kaufmann 
merz murphy 

uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
quinlan 

programs empirical learning 
morgan kaufmann san francisco ca 
quinlan 

bagging boosting 
proceedings thirteenth national conference artificial intelligence pp 
cambridge ma 
aaai press mit press 
