evaluating replicability significance tests comparing learning algorithms remco bouckaert eibe frank mountain information technology oaks drive dairy flat auckland new zealand xm nz computer science department university waikato private bag hamilton new zealand remco eibe cs waikato ac nz 
empirical research learning algorithms classification tasks generally requires significance tests 
quality test typically judged type error test indicates difference type ii error indicates difference 
argue replicability test importance 
say test low replicability outcome strongly depends particular random partitioning data perform 
empirical measures replicability compare performance popular tests realistic setting involving standard learning algorithms benchmark datasets 
results give recommendations test 
significance tests applied compare performance estimates obtained resampling methods cross validation randomly partition data 
consider problem test may sensitive particular random partitioning process 
case possible data learning algorithms significance test researcher finds method preferable finds evidence 
lack replicability cause problems tuning algorithm test may judge favorably latest modification purely due sensitivity particular random number seed partition data 
extend previous replicability studying replicability popular tests realistic setting standard benchmark datasets taken uci repository machine learning problems 
structure follows 
section review significance tests comparing learning algorithms introduce notion replicability 
section discusses popular tests detail 
section contains empirical results tests highlights lack replicability 
section summarizes results recommendations empirical findings 
evaluating significance tests consider scenario certain application domain interested mean difference accuracy classification algorithms domain algorithms trained dataset instances 
know joint distribution underlying domain consequently compute difference exactly 
need estimate check estimated difference true difference perform significance test 
need estimate variance differences different training sets 
obtaining unbiased estimate mean variance difference easy sufficient supply data 
case sample number training sets size run learning algorithms estimate difference accuracy pair classifiers large test set 
average differences estimate expected difference generalization error possible training sets size variance estimate variance 
perform paired test check null hypothesis mean difference zero 
type error test probability rejects null hypothesis incorrectly finds significant difference 
type ii error probability null hypothesis rejected difference 
test type error close chosen significance level 
practice dataset size estimates obtained dataset 
different training sets obtained subsampling instances sampled training testing 
training set si get matching pair accuracy estimates difference xi 
mean variance differences xi estimate mean variance difference generalization error different training sets 
unfortunately violates independence assumption necessary proper significance testing re data obtain different xi consequence type error exceeds significance level 
problematic important researcher able control type error know probability incorrectly rejecting null hypothesis 
heuristic versions test developed alleviate problem :10.1.1.37.3325
study replicability significance tests 
consider test accuracy estimates generated cross validation 
cross validation performed data randomized resulting training test sets exhibits distribution 
ideally test outcome independent particular partitioning resulting randomization process easier replicate experimental results published literature 
practice certain sensitivity partitioning 
measure replicability need repeat test times data different random partitionings repetitions count outcome 
note test greater replicability test type type ii error consistent outcomes individual dataset 
measures replicability 
measure call consistency raw counts 
outcome repetition test data call test consistent difference call consistent 
procedure repeated multiple datasets fraction outcomes test consistent consistent indication replicable test second measure call replicability probability runs test data set produce outcome 
probability worse 
estimate need consider pairs 
performed test different particular dataset pairs 
assume tests rejects null hypothesis 
rejecting pairs accepting ones 
probability estimated probability form measure replicability different datasets 
assume datasets ik number datasets test agrees times ik 
define replicability ik 
larger value measure test produce outcome different dataset 
significance tests section review tests comparing learning algorithms 
testing essential empirical research surprisingly little written topic 
cv paired test dietterich evaluates significance tests measuring type type ii error artificial real world data 
finds paired test applied random subsampling exceedingly large type error 
random subsampling training set drawn random replacement remainder data testing 
repeated number times 
contrast cross validation random subsampling ensure test sets overlap 
fold cross validation viewed special case random subsampling repeated times data training guaranteed test sets overlap 
paired test fold cross validation fares better experiments exhibits inflated type error 
real world datasets type error approximately twice significance level 
alternative proposes heuristic test runs fold crossvalidation called cv paired test 
times fold cross validations runs folds 
run data randomly split subsets equal size 
call subsets folds run consider learning schemes measure course cases may possible split data subsets exactly size 
respective accuracies aij bij fold run obtain aij bij corresponding learning scheme trained data excluding fold run tested remainder 
note exactly pair training test sets obtain aij bij 
means paired significance test appropriate consider individual differences accuracy xij aij bij input test 
denote mean difference single run fold cross validation 
variance cv paired test uses test statistic statistic plugged student distribution degrees freedom 
note numerator uses term differences xij 
consequently outcome test strongly dependent particular partitioning data test performed 
expected replicability test high 
empirical evaluation demonstrates case 
empirical results show cv paired test type error significance level 
show higher type ii error standard test applied fold cross validation 
consequently test recommended low type error essential test 
tests evaluated mcnemar test test difference proportions 
tests single train test split consequently take variance due choice training test set account 
tests mcnemar test performs better acceptable type error type ii error slightly lower cv paired test 
tests inferior cv test consider experiments 
tests random subsampling mentioned dietterich standard test high type error conjunction random subsampling 
nadeau bengio observe due underestimation variance samples independent different training test sets overlap 
consequently propose correct variance estimate dependency account 
aj bj accuracy algorithms respectively measured run 
assume run instances training remaining instances testing 
xj difference xj aj bj estimates mean variance differences 
statistic corrected resampled test xj statistic conjunction student distribution degrees freedom 
difference standard test factor denominator replaced factor nadeau bengio suggest normal usage call times larger 
empirical results show test dramatically improves standard resampled test type error close significance level mcnemar test cv test suffer high type ii error 
tests repeated fold cross validation consider tests times fold cross validation value 
section observe differences xij aij bij fold run simply xij estimate mean xij estimate variance 
assuming various values xij independent test statistic distributed distribution df degrees freedom 
unfortunately independence assumption highly flawed tests assumption show high type error similar plain subsampling 
variance correction previous subsection performed cross validation special case random subsampling ensure test sets run overlap 
course test sets different runs overlap 
results statistic xij number instances training number instances testing 
call test corrected repeated fold cv test 
empirical evaluation evaluate replicability affects various tests performed experiments selection datasets uci repository 
naive bayes nearest neighbor classifier default settings implemented weka version 
tests involve multiple folds folds chosen stratification ensures class distribution dataset reflected folds 
tests run times pair learning schemes significance level tests stated 
results cv paired test table shows datasets properties results cross validation test 
right columns show number times test reject weka freely available source www cs waikato ac nz ml 
dataset inst 
atts 
cl nb vs nb vs nn vs nn anneal arrhythmia audiology autos balance scale breast cancer credit rating ecoli german credit glass heart statlog hepatitis horse colic hungarian heart disease ionosphere iris labor lymphography pima diabetes primary tumor sonar soybean vehicle vote vowel wisconsin breast cancer zoo consistent consistent replicability table 
number cases inst attributes atts classes cl dataset number draws pair classifiers cross validation test nb naive bayes nn nearest neighbor 
null hypothesis number times cross validation test indicates difference corresponding pair classifiers 
example anneal dataset test indicates difference naive bayes times times indicate difference 
note dataset algorithm settings significance test experiments 
difference way dataset split folds runs 
clearly test sensitive particular partitioning anneal data 
looking column naive bayes vs test justify claim perform datasets vehicle dataset just choosing appropriate random number seeds 
just support claim algorithms perform differently cases 
rows test consistently indicates difference schemes particular iris hungarian heart disease datasets 
rows contain cell outcomes test consistent 
row labeled consistent bottom table lists number datasets outcomes test 
calculated number column 
compared schemes results turn consistent 
note possible comparing algorithms preferred null hypothesis rejected 
closer inspection data reveals happens null hypothesis accepted time runs 
consequently cases contribute value consistency measure 
accept outcome runs agree rest get number labeled consistent table number column 
cross validation test consistent fewer cases low rate 
row shows value replicability measure pairs learning schemes considered 
results reflect behaviour consistency measures 
replicability values pretty low considering smaller 
results corrected resampled test resampling experiments data randomized training remaining measure accuracy 
repeated different random number seed run 
table shows results corrected resampled test 
number runs resampling varied see effect replicability 
replicability increases number runs 
exception row consistent value decreases increasing runs 
explained random fluctuations due random partitioning datasets 
replicability reasonably acceptable number runs 
case results consistent value replicability measure approximately 
results tests repeated cross validation standard test single run fold cross validation observed consistent results datasets comparing nb nb nn nn respectively 
contrasting corrected resampling runs takes computational effort see fold cross validation runs nb vs consistent consistent replicability nb vs nn consistent consistent replicability vs nn consistent consistent replicability table 
replicability corrected resampled test 
consistent 
substantially consistent corrected resampling runs 
note test inflated type error 
performing experiment conjunction standard test differences obtained times fold cross validation produced consistent results datasets nb nb nn nn respectively 
looks impressive compared tests evaluated far 
type error test high overlapping training test sets practice 
reduce type error necessary correct variance 
table shows results corrected paired test paired outcomes times fold cross validation 
comparing table corrected resampling consistency better assuming computational effort cases column run table compared runs column table column runs table column runs table said replicability measure indicates repeated cross validation helps improve replicability compared just performing random subsampling 
ensure improved replicability cross validation due stratification performed case random subsampling performed experiment resampling done stratification 
replicability scores differed slightly ones shown table suggesting improved replicability due stratification 
corrected paired test times fold cross validation exhibits best replicability scores performed experiment see sensitive replicability significance level 
results shown table demonstrate significance level major impact consistency replicability measure note greater single case indicating replicability test 
runs nb vs consistent consistent replicability nb vs nn consistent consistent replicability vs nn consistent consistent replicability table 
replicability corrected rx fold cross validation test 
simulation experiment significance level nb vs consistent consistent replicability nb vs nn consistent consistent replicability vs nn consistent consistent replicability table 
replicability corrected fold cross validation test various significance levels 
study effect observed difference accuracy replicability performed simulation study 
data sources selected randomly generating bayesian networks binary variables class variable probability zero 
probability class variable known cause largest variability due selection training data 
network arrows variables class variables independently selected various different probabilities 
guarantees learning scheme expected accuracy test data 
data sources ban structure generated starting naive bayes model adding arrows guaranteeing acyclicity 
stochastic simulation collection training sets instances created 
naive bayes trained accuracy measured test set cases generated data sources 
average difference accuracy shown table row marked accuracy ranges 
tests run times training sets 
table shows tests data source percentage training sets test consistent indicates outcome times 
column shows minimum consistency data sources 
cross validation times resampling fold cross validation show low consistency 
replicability increases dramatically times resampling increases performing times repeated fold cross validation 
consistent results observed uci datasets 
table shows tests fewer problems data sources apart cv test easy decide schemes differ 
test problems data source conservative test low type error high type ii error tends err side cautious deciding schemes differ 
source accuracy min 
cv resampling resampling fold cv corrected fold cv table 
results data sources difference accuracy naive bayes percent consistency tests percent 
considered tests choosing learning algorithms classification tasks 
argued test appropriate type error low type ii error high replicability 
high replicability facilitates reproducing published results reduces likelihood oversearching 
experiments replicability obtained runs random subsampling conjunction nadeau bengio corrected resampled test replicability improved times fold cross validation random subsampling 
methods acceptable best replicability recommend 
acknowledgments eibe frank supported marsden 

witten frank 
data mining practical machine learning tools techniques java implementations 
morgan kaufmann 

bouckaert 
choosing learning algorithms calibrated tests 
proc th int conf machine learning 
morgan kaufmann 

bouckaert 
choosing learning algorithms sign tests high replicability 
proc th australian joint conference artificial intelligence 
springer verlag 

blake merz 
uci repository machine learning databases 
irvine ca 
www ics uci edu mlearn mlrepository html 
dietterich 
approximate statistical tests comparing supervised classification learning algorithms 
neural computation 
nadeau bengio 
inference generalization error 
machine learning 
quinlan 
programs machine learning morgan kaufmann 

friedman geiger goldszmidt 
bayesian network classifiers 
machine learning 
pearl probabilistic reasoning intelligent systems 
morgan kaufmann 
