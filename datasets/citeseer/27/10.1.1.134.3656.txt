text classification bootstrapping keywords em shrinkage andrew mccallum mccallum com just research henry street pittsburgh pa applying text classification complex tasks tedious expensive hand label large amounts training data necessary performance 
presents alternative approach text classification requires labeled documents uses small set keywords class class hierarchy large quantity unlabeled documents 
keywords assign approximate labels unlabeled documents 
preliminary labels starting point bootstrapping process learns naive bayes classifier expectation maximization hierarchical shrinkage 
classifying complex data set computer science research papers leaf topic hierarchy keywords provide accuracy 
classifier learned bootstrapping reaches accuracy level close human agreement 
provided labeled training examples variety text classification algorithms learn reasonably accurate classifiers lewis joachims yang cohen singer :10.1.1.11.8264
applied complex domains classes algorithms require extremely large training sets provide useful classification accuracy 
creating sets labeled data tedious expensive typically labeled person 
leads consider learning algorithms require large amounts labeled data 
kamal nigam cs cmu edu school computer science carnegie mellon university pittsburgh pa labeled data difficult obtain unlabeled data readily available plentiful 
castelli cover show theoretical framework unlabeled data improve classification exponentially valuable labeled data 
fortunately unlabeled data obtained completely automated methods 
consider problem classifying news articles short perl script night automated internet downloads fill hard disk unlabeled examples news articles 
contrast take days human effort tedium label 
previous nigam shown just small number labeled documents text classification error reduced labeled documents augmented large collection unlabeled documents 
considers task learning text classifiers labeled documents 
knowledge classes interest provided form keywords class class hierarchy 
keywords typically generated quickly easily small number labeled documents 
classification problems naturally come hierarchically organized classes 
algorithm proceeds keywords generate preliminary labels documents term matching 
labels hierarchy unlabeled documents input bootstrapping algorithm produces naive bayes classifier 
bootstrapping algorithm combines hierarchical shrinkage expectation maximization em unlabeled data 
em iterative algorithm maximum likelihood estimation parametric estimation problems missing data 
scenario class labels documents treated missing data 
em works training classifier documents software programming engineering programming software language design logic engineering tools environments university programs semantics semantics denotational language construction types garbage collection garbage collection memory optimization region compiler design compiler code parallel data language 
os distributed system systems network time 
nlp language natural processing information text computer science computer university science system artificial intelligence learning university computer intelligence 
machine planning learning planning learning temporal algorithm reasoning algorithms plan university networks problems hardware architecture circuits design computer university performance 
knowledge representation knowledge representation language system natural 
interface design interface design user sketch interfaces hci computer system multimedia university cooperative collaborative cscw provide group information retrieval information text documents classification retrieval 
multimedia multimedia real time data media subset cora topic hierarchy 
node contains title probable words calculated naive bayes shrinkage vertical word redistribution hofmann puzicha 
words initial keywords class indicated plain font italics 
labeled keywords uses classifier re assign probabilistically weighted class labels documents calculating expectation missing class labels 
trains new classifier documents iterates 
improve classification incorporating shrinkage statistical technique improving parameter estimation face sparse data 
classes provided hierarchical relationship shrinkage estimate new parameters weighted average specific unreliable local class estimates general reliable ancestors class hierarchy 
optimal weights average calculated em process runs simultaneously em re estimating class labels 
experimental evaluation bootstrapping approach performed data set computer science research papers 
leaf hierarchy computer science keywords class provided input 
keyword matching provides accuracy 
bootstrapping algorithm uses input outputs naive bayes text classifier achieves accuracy 
interestingly accuracy approaches estimated human agreement levels 
experimental domain originates part ra research project effort build domain specific search engines web machine learning techniques 
demonstration system cora search engine computer science research papers mccallum 
bootstrapping classification algorithm described cora place research papers yahoo hierarchy specific computer science 
search engine including hierarchy publicly available www cora com 
generating preliminary labels keywords step bootstrapping process keywords generate preliminary labels unlabeled documents possible 
class just keywords 
shows examples number type keywords experimental domain human provided keywords shown nodes non italic font 
generate preliminary labels keywords term matching rule list fashion document step keywords place document category keyword matches 
finding keywords obtain broad coverage simultaneously finding sufficiently specific keywords obtain high accuracy difficult requires intimate knowledge data lot trial error 
result classification keyword matching inaccurate incomplete 
keywords tend provide high precision low recall brittleness leave documents unlabeled 
documents match keywords wrong class 
general expect low recall keywords dominating factor error 
experimental domain example unlabeled documents contain keywords 
method priming bootstrapping keywords take set keywords labeled mini document containing just words 
input standard learning algorithm 
testing keyword labeling approaches area ongoing 
bootstrapping algorithm goal bootstrapping step generate naive bayes classifier inputs inaccurate incomplete preliminary labels unlabeled data class hierarchy 
straightforward method simply take unlabeled documents preliminary labels treat labeled data standard supervised setting 
approach provides minimal benefit reasons labels noisy sample labeled documents skewed regular document distribution includes documents containing keywords data sparse comparison size feature space 
adding remaining unlabeled data running em helps counter second reasons 
adding hierarchical shrinkage naive bayes helps counter third reasons 
detailed description bootstrapping algorithm short overview standard naive bayes text classification proceed adding em incorporate unlabeled data conclude explaining hierarchical shrinkage 
outline entire algorithm table 
naive bayes framework build framework multinomial naive bayes text classification lewis mccallum nigam :10.1.1.11.8264
useful think naive bayes estimating parameters probabilistic generative model text documents 
model class document selected 
words document generated parameters class specific multinomial unigram model 
classifier parameters consist class prior probabilities class conditioned word probabilities 
formally class cj document frequency relative classes written cj 
word wt vocabulary wt cj indicates frequency classifier expects word wt occur documents class cj 
standard supervised setting learning parameters accomplished set labeled training documents estimate word probability parameters wt cj count frequency word wt occurs word occurrences documents class cj 
supplement inputs collection unlabeled documents class hierarchy keywords class 
generate preliminary labels unlabeled documents possible term matching keywords rule list fashion 
initialize uniform path leaf class root class hierarchy 
iterate em algorithm step build maximum likelihood multinomial node hierarchy class probability estimates document equations 
normalize path leaf class root class hierarchy sum 
step calculate expectation class labels document classifier created step equation 
increment new attributing word held data probabilistically ancestors class 
output naive bayes classifier takes unlabeled document predicts class label 
table outline bootstrapping algorithm 
laplace smoothing primes estimate count avoid probabilities zero 
wt di count number times word wt occurs document di define cj di document class label 
estimate probability word wt class cj wt cj di wt di cj di di ws di cj di class prior probability parameters set way indicates number classes cj di cj di 
unlabeled document classifier determine probability document belongs class cj bayes rule naive bayes assumption words document occur independently class 
denote kth word document di classification cj di cj di cj cj di cj 
empirically large number training documents naive bayes job classifying text documents lewis :10.1.1.11.8264
complete presentations naive bayes text classification provided mitchell mccallum nigam 
adding unlabeled data em standard supervised setting document comes label 
bootstrapping scenario preliminary keyword labels incomplete inaccurate keyword matching leaves documents unlabeled labels incorrectly 
order entire data set naive bayes classifier expectation maximization em algorithm generate probabilistically weighted class labels documents 
results classifier parameters data 
em class iterative algorithms maximum likelihood maximum posteriori parameter estimation problems incomplete data dempster 
model data generation data missing values em iteratively uses current model estimate missing values uses missing value estimates improve model 
available data em locally maximize likelihood parameters give estimates missing values 
scenario class labels unlabeled data missing values 
implementation em iterative step process 
initially parameter estimates set standard naive bayes way just labeled documents 
iterate steps 
step calculates probabilistically weighted class labels cj di document classifier equation 
step estimates new classifier parameters documents equations cj di continuous step 
iterate steps classifier converges 
initialization step preliminary labels identifies mixture component class seeds em local maxima finds correspond class definitions 
previous nigam shown technique significantly increases text classification accuracy limited amounts labeled data large amounts unlabeled data 
expectation em correct complete labels entire data set 
improving sparse data estimates shrinkage provided large pool documents naive bayes parameter estimation bootstrapping suffer sparse data naive bayes parameters estimate 
provided class hierarchy integrate statistical technique shrinkage bootstrapping algorithm help alleviate sparse data problem 
consider trying estimate probability word intelligence class nlp 
clearly non negligible probability limited training data may unlucky observed frequency intelligence nlp may far true expected value 
level hierarchy artificial intelligence class contains documents union children 
probability word intelligence reliably estimated 
shrinkage calculates new word probability estimates leaf class weighted average estimates path leaf root 
technique balances trade specificity reliability 
estimates leaf specific unreliable hierarchy estimates reliable unspecific 
calculate mixture weights averaging guaranteed maximize likelihood held data em algorithm 
think hierarchical shrinkage generative model slightly augmented described section 
class leaf selected 
word position document ancestor class including selected shrinkage weights 
word chosen multinomial word distribution ancestor 
word training data labeled ancestor responsible generating estimating mixture weights simple matter maximum likelihood estimation ancestor emission counts 
ancestor labels provided training data em fill missing values 
term vertical em refer process calculates ancestor mixture weights term horizontal em refer process filling missing class leaf labels unlabeled documents 
vertical horizontal em run concurrently interleaved steps 
formally wt cj wt cj word probability estimates wt cj isthe maximum likelihood estimate training data just leaf wt cj maximum likelihood estimate parent training data union parent children wt cj estimate root training data wt cj uniform estimate wt cj 
interpolation weights cj define include cj written kj aj 
new word probability estimate shrinkage denoted wt cj wt cj wt cj pk wt cj 
vectors calculated iterations em 
step calculate class cj word unlabeled held data probability word generated ith ancestor 
step normalize sum expectations obtain new mixture weights held data mixture weight concentrate leaves 
specifically initializing mixture weights leaf uniform distribution 
di denote probability ath ancestor cj generate word occurrence di step consists estimating di jpa cj pm 
cj step derive new guaranteed improved weights summing normalizing di di cj di di cj di 
steps iterate converge 
weights calculate new shrinkage word probability estimates equation 
classification new test documents performed just equation laplace estimates word probability estimates replaced shrinkage estimates 
complete description hierarchical shrinkage text classification mccallum 

related research efforts text learning bootstrapping approaches 
training algorithm blum mitchell classification works cases feature space separable naturally redundant independent parts 
example web pages thought text web page collection text hyperlink anchors page 
riloff jones bootstraps dictionary locations just small set known locations 
mutual bootstrap algorithm works iteratively identifying syntactic constructs indicative known locations identifying new locations indicative constructs 
preliminary labeling keyword matching similar seed collocations yarowsky 
word sense disambiguation task bootstrapping algorithm seeded examples common collocations particular sense word seed life biological sense plant 
experimental results section provide empirical evidence bootstrapping text classifier unlabeled data produce high accuracy text classifier 
test domain computer science research papers 
created leaf hierarchy computer science topics part shown 
creating hierarchy took minutes examined conference proceedings explored computer science sites web 
selecting keywords associated node took minutes 
test set created expert hand labeling random sample research papers papers cora archive time began experiments 
third fit category discarded resulting document test set 
labeling documents took hours 
papers outside area computer science astrophysics papers papers complete hierarchy considered computer science papers 
class frequencies data skewed test set populous class accounted documents 
research represented words title author institution 
detailed description segments automatically extracted provided mccallum seymore 
method lab lab acc keyword nb nb nb em nb nb human table classification results different techniques keyword matching human agreement naive bayes nb naive bayes combined hierarchical shrinkage em 
classification accuracy acc number labeled lab keyword matched labeled lab unlabeled documents method shown 
words occurring fewer documents words standard stoplist discarded 
stemming 
bootstrapping performed algorithm outlined table 
table shows classification results different classification techniques 
rule list classifier keywords provides 
documents test set containing keywords assigned class rule list classifier counted incorrect 
interesting time comparison documents labeled time took generate keyword lists 
naive bayes accuracy labeled documents 
labeled documents test set leave naive bayes reaches 
running bootstrapping algorithm documents preliminary labels keyword matching 
em shrinkage incorporate remaining documents fix preliminary labels leverage hierarchy resulting accuracy 
interesting comparison agreement test set human experts 
experiments reveal inner workings bootstrapping 
build naive bayes classifier standard supervised way labeled documents classifier gets accuracy 
corresponds performance iteration bootstrapping 
note matches accuracy traditional naive bayes labeled training documents requires quarter human labeling effort 
run bootstrapping documents left unlabeled keyword matching accuracy reaches 
indicates shrinkage em labeled documents providing substantially benefit remaining unlabeled documents 
explanation small impact documents left unlabeled keyword matching fall naturally hierarchy 
remember third documents fall outside hierarchy 
preliminary labels keyword matching 
presence outlier documents skews em parameter estimation 
inclusive computer science hierarchy allow unlabeled documents benefit classification 
complete hierarchy documents identify outliers 
techniques robust estimation em discussed mclachlan basford 
specific technique text hierarchies add extra leaf nodes containing uniform word distributions interior node hierarchy order capture documents belonging predefined topic leaves 
allow em perform large percentage documents fall classification hierarchy 
similar approach planned research topic detection tracking tdt baker 
experimentation techniques area ongoing research 
considered building text classifier labeled training documents 
place bootstrapping algorithm uses large pool unlabeled documents class specific knowledge form keywords class class hierarchy 
bootstrapping algorithm combines expectation maximization hierarchical shrinkage correct complete preliminary labeling provided keyword matching 
experimental results show accuracies close human agreement obtained bootstrapping algorithm 
plan refine probabilistic model allow documents placed interior hierarchy nodes documents multiple class assignments classes modeled multiple mixture components 
investigating principled methods re weighting word features semi supervised clustering provide better discriminative training unlabeled data 
kamal nigam supported part darpa hpkb program contract 
baker hofmann mccallum yang 

hierarchical probabilistic model novelty detection text 
technical report just research 
www cs cmu edu mccallum 
blum mitchell 

combining labeled unlabeled data training 
colt 
castelli cover 

relative value labeled unlabeled samples pattern recognition unknown mixing parameter 
ieee transactions information theory 
cohen singer 

context sensitive learning methods text categorization 
si gir 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
hofmann puzicha 

statistical models occurrence data 
technical report ai memo ai lab mit 
joachims 

text categorization support vector machines learning relevant features 
ecml 
lewis 

naive bayes independence assumption information retrieval 
ecml 
mccallum nigam 

comparison event models naive bayes text classification 
aaai workshop learning text categorization 
tech 
rep ws aaai press 
www cs cmu edu mccallum 
mccallum rosenfeld mitchell ng 

improving text shrinkage hierarchy classes 
icml 
andrew mccallum kamal nigam jason rennie seymore 

machine learning techniques build domain specific search engines 
ijcai 

mclachlan basford 

mixture models 
marcel dekker new york 
mitchell 

machine learning 
mcgraw hill new york 
nigam mccallum thrun mitchell 

text classification labeled unlabeled documents em 
machine learning 
appear 
riloff jones 

learning dictionaries information extraction multi level bootstrapping 
aaai 

seymore mccallum rosenfeld 

learning hidden markov model structure information extraction 
aaai workshop machine learning information extraction 
appear 
yang 

evaluation statistical approaches text categorization 
journal information retrieval 

yarowsky 

unsupervised word sense disambiguation rivaling supervised methods 
acl 
