generative model clustering directional data ece university tx usa ece utexas edu cs university tx usa cs utexas edu joydeep ece university tx usa ghosh ece utexas edu cs university tx usa cs utexas edu high dimensional directional data increasingly important contemporary applications analysis text gene expression data 
natural model multivariate directional data provided von mises fisher vmf distribution unit hypersphere analogous multi variate gaussian distribution rd propose modeling complex directional data mixture vmf distributions 
derive analyze variants expectation maximization em framework estimating parameters mixture 
propose clustering algorithms corresponding variants 
interesting aspect methodology spherical kmeans algorithm kmeans cosine similarity shown special case algorithms 
modeling text data vmf distributions lends theoretical validity cosine similarity widely information retrieval community 
part experimental validation results modeling high dimensional text gene expression data mixture vmf distributions 
results indicate approach yields superior clusterings especially difficult clustering tasks high dimensional spaces 
categories subject descriptors information storage retrieval information search retrieval pattern recognition clustering keywords clustering directional data mixtures von mises fisher em 
clustering segmentation data fundamental data analysis step widely studied various disciplines 
large datasets acquired scientific domains world wide web variety complex characteristics severely challenge traditional methods clustering 
article concerned clustering high dimensional permission digital hard copies part classroom granted fee provided copies distributed profit commercial advantage notice full citation page 
copy republish post servers redistribute lists requires prior fee 
sigkdd august washington dc usa 
copyright acm 
directional data increasingly common application domains 
broadly categorize clustering approaches generative parametric discriminative non parametric ones :10.1.1.44.7709
performance approach specific method approach quite data dependent clustering method works best types data 
generative models provide better insight nature clusters 
application point view lot domain knowledge incorporated generative models clustering data uncovers specific desirable patterns looking 
clustering algorithms generative model framework involve appropriate application expectation maximization em algorithm properly chosen statistical generative model data consideration 
vector data studied clustering algorithms popular generative models mixture gaussians effect analogous euclidean mahalanobis type distances discriminative perspective 
motivation related application domains clustering minimizing euclidean distortions yields poor results 
example empirical studies information retrieval applications show cosine similarity far effective measure similarity analyzing clustering text documents 
domains require directional data data deals directions unit vectors 
need generative models appropriate analysis clustering directional data 
article propose generative mixture model directional data unit hypersphere derive clustering algorithms mixture model 
show connection proposed algorithms class existing algorithms clustering high dimensional directional data detailed experimental comparisons 
important domain directional data encountered text analysis text clustering particular 
experimentally shown order remove bias arising due length document helps normalize data vectors 
spkmeans algorithm performs kmeans cosine similarity euclidean distortion empirically outperform schemes 
quite domains bioinformatics collaborative filtering directional data encountered :10.1.1.167.7612
similarity measure useful domains pearson correlation coefficient 
rd pearson product moment correlation ae di xi gamma yi gamma ppd xi gamma theta ppd yi gamma pd xi pd yi 
considering mapping 
xi gamma di xi gamma ae xk 
pearson correlation exactly cosine similarity analysis clustering data pearson correlations essentially clustering problem directional data 
application domains described share characteristic objects clustered reside high dimensional space thousands 
clustering high dimensional data great interest lately proposed methods density heuristic discriminatory approach 
noteworthy generative approach clustering high dimensional data mixtures gaussians 
certain works generative model exponential family explicitly developed modeling text 
spkmeans algorithm clustering normalized data proposed connection generative model involving vmf distributions spkmeans algorithm observed 
online competitive learning scheme vmf distributions minimizing kl divergence distortion proposed 
approaches cluster directional data explicit probabilistic generative models 
directional nature data explicitly resulting significantly better clustering performances certain standard techniques kmeans cosine similarity especially difficult clustering tasks clusters overlap cluster sizes skewed cluster sizes small relative dimensionality data 
interestingly shall see proposed approaches implicit simulated annealing type behavior alleviate problems associated high dimensionality 
proceeding give brief outline 
vmf distribution sphere section 
section introduce generative model mixture vmf distributions analyze maximum likelihood parameter estimate mixture model dataset em framework 
analysis section clustering algorithms soft hard assignments respectively proposed section 
detailed experimental results proposed algorithms section 
interesting aspects proposed algorithms discussed section 
section presents concluding remarks directions 
word notation bold faced variables represent vectors delta denotes norm sets represented calligraphic upper case letters denotes set reals sd gamma denotes gamma dimensional sphere embedded rd probability density functions denoted lower case alphabets delta delta probability set events denoted random variable distributed delta expectations functions denoted ep delta 

von mises fisher vmf dis tribution dimensional unit random vector kxk said variate von mises fisher vmf distribution probability density function xj cd 
sd gamma 
normalizing constant cd cd gamma ss id gamma ir delta represents modified bessel function kind order see details vmf distributions 
density xj parameterized mean direction concentration parameter called characterizes strongly unit vectors drawn xj concentrated mean direction larger values imply stronger concentration mean direction 
particular xj reduces uniform density sd gamma xj tends point density 
von mises fisher distribution natural directional data properties analogous multivariate gaussian distribution multi variate data rd example maximum entropy density sd gamma subject constraint fixed vmf density see details 

em mixture section introduce mixture vmf movmf distributions generative model directional data derive mixture density parameter estimation update equations data set expectation maximization em framework 
probability density function movmf generative model xj theta kx xj theta fff delta delta delta ffk delta delta delta kg pk ffh ffh fh xj single vmf distribution parameters 
order sample point generative model perspective th vmf chosen random probability ffh point sampled sd gamma fh xj 
fx delta delta delta xng data set generated sampling independently generative model 
fz delta delta delta corresponding set called hidden random variables zi xi generated fh xj 
knowledge values hidden variables log likelihood observed data ln zj theta nx ln fzi xij zi maximum likelihood parameter estimates obtained 
values hidden variables known really random variable dependent distribution called complete data log likelihood 
theta possible estimate conditional distribution zj theta forms step em framework 
exact details estimation done deferred moment 
fact discuss ways estimating hidden variable distributions lead significantly different algorithms 
assume distribution theta zi hjx xi theta known data points step parameter estimation suppose posterior distribution theta hidden variables zj theta known 
specified henceforth expectations taken distribution set random variable zj theta 
expectation complete data log likelihood distribution ep ln zj theta kx nx ln ffh theta kx nx ln fh xij theta parameter estimation step theta re estimated expression maximized 
note order maximize expression maximize term containing ffh term containing separately functional dependencies note theta fixed 
find expression ffh introduce lagrangian multiplier constraint pk ffh 
partial derivatives lagrangian objective function ffh solving get ffh nx theta concentrate terms containing set constraints th 
lagrange multipliers delta delta delta corresponding constraints lagrangian kx ln cd theta nx th xi theta gamma th partial derivatives respect setting zero get pn theta pn theta ad pn theta kp theta analysis kkt conditions skipped brevity 
accounted positive square roots 
final estimates come 
ad id id gamma closed form solution possible numerical techniques solve reasonable approximation solution obtained setting gamma gamma rh ad 
step distribution estimation standard setting em algorithm give update equations parameters involved 
set parameters section outline schemes updating distributions zj theta likelihood data maximized 
update scheme exactly follows soft assignment scheme learning mixture models em 
standard em framework distribution hidden variables theta ffh fh xij theta pk ffl fl xij theta second update scheme widely heuristic unsupervised learning :10.1.1.33.2557:10.1.1.33.2557
case distribution hidden variables theta argmax jxi theta soft assignments theoretically motivated hard assignments received theoretical attention discussion :10.1.1.48.3989:10.1.1.33.2557
rest section formally study connection soft hard assignments em framework 
note analysis applicable general mixture model learning em framework 
arguments introduce function theta theta ln zj theta distribution zj theta :10.1.1.33.2557:10.1.1.33.2557
maximumlikelihood perspective primary objective function want maximize get correct mixture model incomplete data log likelihood ln theta 
interesting property function lower bounds ln theta choices sense try maximize respect theta maximizes delta theta :10.1.1.33.2557
corresponding optimal value function theta ep ln zj theta ep ln zj theta gamma ep ln zj theta ep ln zj theta zj theta ep ln theta ln theta incomplete data log likelihood 
easily see theta optimizes delta optimizes expectation complete data log likelihood 
step equivalent optimizing respect theta follows incomplete data log likelihood ln theta non decreasing iteration alternate maximization function terms theta equivalently parameter distribution updates :10.1.1.33.2557
iteration set updates forms basis algorithm soft movmf discussed section 
hard assignment case effectively hidden random variables distribution probability mixture components 
shall denote class distributions important question way optimally pick distribution perform regular step guarantee incomplete log likelihood data non decreasing iteration update 
unfortunately possible general 
show possible reasonably lower bound incomplete log likelihood data expectations 
distribution optimal sense gives tightest lower bound incomplete log likelihood distributions lower bound reasonable sense expectation lower bounded expectation complete log likelihood distribution 
iterative update scheme analogous regular em guarantees tight lower bound incomplete log likelihood non decreasing iteration update 
parameter estimation step remains practically unchanged replaced update equations 
optimizes theta functional value delta theta smaller choice particular theta theta ln theta distributions property entropy 
particular 
definition function eq ln zj theta ln theta expectation lower bounds likelihood data 
definitions easily prove ep ln zj theta eq ln zj theta 
adding incomplete data log likelihood ln theta sides inequality ep ln zj theta eq ln zj theta combining get ep ln zj theta eq ln zj theta ln theta expectation lies incomplete data likelihood expectation complete data likelihood reasonable lower bound incomplete data likelihood value 
show choice distribution optimal sense expectation gives tightest lower bound distributions distribution subset theta 
lambda argmax theta 
lambda jxi theta theta 
ln zj theta nx kx theta ln theta nx ln theta nx ln lambda jxi theta nx kx theta ln theta eq ln zj theta choice optimal 
analysis forms basis algorithm hard movmf discussed section 
algorithms section propose algorithms clustering directional data development previous section 
algorithms soft schemes respectively called soft movmf hard movmf 
soft movmf algorithm algorithm estimates parameters mixture model exactly derivations section em 
assigns soft probabilistic labels point posterior probabilities components mixture conditioned point 
termination algorithm gives parameters theta vmf distributions model data set soft clustering posterior probabilities theta appropriate convergence criteria determine algorithm terminate 
algorithm soft movmf input set data points sd gamma output soft clustering mixture vmf distributions initialize ffh delta delta delta repeat fthe expectation step emg fh xij cd th xi theta xij pk xij fthe maximization step emg ffh nx theta pn theta pn theta gamma pn theta kpn theta convergence hard movmf algorithm estimates parameters mixture model hard assignment derived posterior distribution 
algorithm obtained algorithm replacing posteriors hardened posteriors 
hard assignments iteration point belongs single cluster 
updates component parameters done posteriors components points 
difference case posterior probabilities take values 
termination algorithm gives parameters theta model data set hard assignment setup hard clustering disjoint conditional posteriors component vmf distributions 
revisiting spherical kmeans briefly revisit spkmeans algorithm shown perform real life text clustering tasks light developments sections 
spkmeans procedure algorithm 
main observation spkmeans algorithm looked special case soft movmf hard movmf algorithms certain restrictive assumptions generative model 
precise say assume generative model mixture priors components ffh 
order get spkmeans reduction soft movmf assume components equal infinite concentration parameters 
assumptions step reduces assigning point nearest cluster nearness computed cosine similarity point cluster representative 
point xi assigned cluster lambda argmax lambda jxi theta lim ti lambda pk theta lambda show spkmeans seen special case hard movmf algorithm addition assuming priors components equal assume concentration parameters components equal 
hard movmf reduces spkmeans 
assumptions model estimation common concentration parameter hard assignment depend value cosine similarity 
set values define xh fx xt easy see forms disjoint partitioning set values rewrite hard movmf algorithm similar notation set partitions xh fx xt addition algorithms report experimental results algorithm belongs class sense spkmeans derived mixture vmf models restrictive assumptions 
centroids mixture components estimated hard movmf 
concentration component set inversely proportional number points cluster corresponding component order simulate frequency sensitive algorithm spkmeans input set data points sd gamma output disjoint partitioning initialize delta delta delta repeat fthe expectation step emg set xh oe delta delta delta xh xh argmax fthe maximization step emg xh xh xk convergence competitive learning implicitly prevents formation null clusters known problem regular kmeans 

experimental results section briefly describe datasets experimental methodology 
discuss performance algorithms consideration various datasets 
datasets results standard text datasets newsgroups yahoo news dataset yeast levels 
newsgroups dataset collection documents different usenet newsgroups approximately equal number documents group 
results subsets dataset 
full dataset called news 
smaller version small news created randomly chosen documents newsgroups 
various subsets datasets studied understand impact dataset properties relative performance algorithms 
news diff subset consisting different newsgroups alt atheism rec sport baseball sci space documents cluster small news diff consists newsgroups documents cluster 
news sim subset consisting similar newsgroups comp graphics comp os ms windows comp windows documents cluster small news sim consists newsgroups documents cluster 
yahoo news series dataset collection yahoo news articles different categories 
full yahoo dataset experimentation 
underlying clusters dataset highly skewed terms number documents clusters sizes ranging 
rosetta yeast gene expression dataset collection gene expression vectors constructed www ai mit edu people newsgroups ftp ftp cs umn edu users boley dna microarray experiments yeast genes 
original dataset consists experiments measuring expression yeast genes 
subset genes known phylogenetic profiles experiments 
methodology performance algorithms discussed section text datasets analyzed mutual information mi cluster class labels 
mi gives amount statistical similarity cluster class labels 
random variable cluster assignments random variable preexisting labels data mi ln expectation computed joint distribution estimated particular clustering dataset consideration 
results reported averaged runs algorithms started random initialization 
standard deviations mi reasonably small algorithms reduce clutter error bars shown plots 
note gene expression datasets class labels unavailable average cosine similarity data point cluster representative evaluate performance algorithms 
number clusters mutual information value mi values small news hard movmf clustering results small news results newsgroups section briefly discuss performance algorithms newsgroups datasets 
algorithms performed similarly news dataset achieving mi approximately correct number clusters 
experiments artificial directional data revealed empirical fact clusters equal priors sufficiently large number data points cluster reasonably separated algorithms perform similarly clustering problem simple 
hand conditions violated give quite different clusterings respective biases 
lesion experiments various subsets news performed study biases detail 
example small news dataset just sampled version news dataset significant performance differences observed shown 
number documents cluster small spkmeans perform true number clusters small number points susceptible getting trapped bad local minima respective objective functions 
hand soft movmf performs significantly better algorithms entire range hard movmf gives satisfactory mi values till true number clusters falls sharply 
number clusters mutual information value mi values news diff hard movmf clustering results news diff spkmeans soft movmf ii iii ii iii class class ii class iii table confusion matrix news diff effect small number points cluster high dimensions studied closely variants news diff dataset 
basic news diff dataset easy dataset cluster equal priors reasonably separate clusters sufficient number documents cluster 
shall shortly see smaller version small news diff simple dataset performance algorithms change significant amounts 
results news diff shown 
performance algorithms comparable vmf algorithms give higher values mi consistently entire range number clusters experimented 
correct number clusters vmf algorithms perform significantly better 
demonstrate clearly presenting confusion matrices generated spkmeans soft movmf table 
vmf algorithms soft movmf performs consistently better hard movmf entire range 
number clusters mutual information value mi values small news diff hard movmf clustering results small news diff spkmeans soft movmf ii iii ii iii class class ii class iii table confusion matrix small news diff results small news diff dataset 
note vmf algorithms perform significantly better spkmeans show poor performance number data points cluster relatively small compared dimensionality data 
fact high dimensional data sparse representation text documents points chosen random high probability orthogonal kmeans iterative update scheme may get stuck local minimum point initialized see 
result algorithms suffer high dimensions especially number data points available small 
explains performance spkmeans 
vmf algorithms soft movmf performs significantly better hard movmf entire range 
attributed interesting implicit simulated annealing type behavior soft movmf demonstrates 
behavior discussed detail section 
typical confusion matrices generated spkmeans soft movmf correct number clusters table 
spkmeans soft movmf ii iii ii iii class class ii class iii table confusion matrix news sim datasets help study effect high number clusters mutual information value mi values news sim hard movmf clustering results news sim dimensional overlapping clusters performance algorithms 
results news sim data 
clusters sufficient number points difficult dataset cluster newsgroups similar topics 
points spkmeans attain reasonable performance suffer little due cluster overlaps 
vmf algorithms consistently perform better algorithms 
vmf algorithms soft movmf achieve higher values mi differences significant 
representative confusion matrices generated spkmeans soft movmf table 
number clusters mutual information value mi values small news sim hard movmf clustering results small news sim small news sim dataset difficult dataset cluster news subsets spkmeans soft movmf ii iii ii iii class class ii class iii table confusion matrix small news sim consider 
overlapping clusters relatively small number points cluster 
expected spkmeans poorly dataset 
vmf algorithms perform significantly better 
soft movmf achieves higher values mi entire range cluster values experiments performed 
typical confusion matrices table 
number clusters mutual information value mi values yahoo hard movmf clustering results yahoo results yahoo news yahoo news dataset difficult dataset clustering sense addition having amount overlap clusters insufficient points cluster clusters highly skewed terms number points cluster 
results different algorithms 
entire range soft movmf consistently performs better algorithms 
correct number clusters performs significantly better algorithms 
spkmeans similar behavior till moderate number clusters 
higher numbers clusters spkmeans generates empty clusters mi increase fast explicit mechanism preventing null clusters 
performance hard movmf interesting mi values comparable times worse spkmeans 
perform marginally better correct number clusters improvement statistically significant 
seen earlier hard movmf performed significantly better algorithms datasets discussed 
fact similar behavior observed datasets experimented 
skewness yahoo terms cluster sizes possible reason bad performance number clusters cosine similarity values average cosine yeast gene expression hard movmf clustering results yeast data dataset experiments skewed datasets explain behavior 
results yeast gene expression results yeast gene expression dataset 
clearly seen different clustering domain different performance measure vmf algorithms perform significantly better kmeans algorithms 
vmf algorithms performance hard movmf marginally better soft movmf differences significant 
interesting note vmf algorithms trying maximize different objective functions perform significantly better spkmeans terms average cosine similarity objective function spkmeans explicitly tries maximize 

discussion mixture vmf distributions gives parametric modelbased generalization widely cosine similarity measure 
discussed section spherical kmeans algorithm uses cosine similarity arises special case em mixture things concentration distributions held constant 
different perspective argue particular dataset sampled vmf distribution say natural similarity pair data points set cosine similarity 
precisely data points represented orthonormal basis fisher information matrix identity fisher kernel similarity xi xj ln xij ln xjj see xi xj xj exactly cosine similarity :10.1.1.44.7709
words cosine similarity particular application necessarily clustering implicit assumption data drawn vmf distribution 
terms performance magnitude improvement shown soft movmf algorithm difficult clustering tasks surprising especially low dimensional non directional data improvements soft assignment kmeans standard hard assignment versions oftentimes quite minimal 
particular curious regarding couple issues soft movmf performing substantially better hard movmf final probability values obtained soft movmf close ii soft movmf needs estimate parameters doing better insufficient number points relative dimensionality space small datasets 
turns issues understood closer look soft movmf converges 
experiments initialized initial centroids small random perturbations global centroid 
soft movmf initial posterior membership distributions data points uniform entropy hidden random variables high 
change entropy iterations news subsets 
behavior similar datasets 
kmeans algorithms relocation happens iterations minor adjustments soft movmf data points iterations entropy remains high maximum possible entropy log 
cluster patterns discovered iterations entropy drops drastically small number iterations 
algorithm converges entropy practically zero points effectively hard assigned respective clusters 
note behavior strikingly similar simulated annealing approaches considered inverse temperature parameter 
drastic drop entropy iterations typical critical temperature behavior observed annealing 
text data non negative features values data points lie orthant dimensional hypersphere naturally concentrated 
gene expression data spread hypersphere high concentration regions 
case final values convergence high reflecting concentration data implying low final temperature annealing perspective 
initializing low value equivalently high temperature idea case soft movmf running values keep increasing successive iterations get final large values giving effect decreasing temperature process explicit simulated annealing strategy 
explains soft movmf algorithm performs difficult clustering tasks high dimensions 
hard movmf algorithm general vmf model suffers hard assignments 
spkmeans difficult datasets due hard assignment scheme significantly modeling capabilities 
number iterations entropy hidden variables entropy iterations soft movmf news small news news diff small news diff variation entropy hidden variables number iterations 
proposed algorithms clustering directional data 
algorithms essentially expectation maximization schemes applied appropriate generative model mixture von mises fisher distributions 
show spkmeans algorithm shown text clustering special case proposed schemes 
high dimensional clustering algorithm special case proposed algorithms 
algorithms comprehensively evaluated real life high dimensional datasets varying degrees complexity 
experimental results certain high dimensional datasets text gene expression normalization properties match modeling assumptions vmf mixture model 
proposed algorithms form powerful clustering techniques datasets 
vmf distribution considered proposed techniques simplest parametric distributions directional data 
general models distribution added expressive power may useful certain circumstances 
parameter estimation problem significantly difficult models 
needs substantially data get reliable estimates parameters 
complex models may viable problems 
tradeoff model complexity terms number parameters sample complexity needs studied detail context directional data 
adapt local search strategy incremental em yield better local minima hard soft assignments 
acknowledgments research supported part nsf ecs nsf career award 
aci texas advanced research program 

aggarwal 
re designing distance functions distance applications high dimensional data 
sigmod record march 
banerjee dhillon ghosh sra 
clustering hyperspheres expectation maximization 
technical report tr department computer sciences university texas february 
banerjee ghosh 
frequency sensitive competitive learning clustering high dimensional hyperspheres 
proceedings international joint conference neural networks pages may 
bradley bennett demiriz 
constrained means clustering 
technical report microsoft research may 
collins 
em algorithm 
fulfillment written preliminary exam ii requirement september 
cover thomas 
elements information theory 
wiley interscience 
dasgupta 
learning mixtures gaussians 
ieee symposium foundations computer science 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
dhillon kogan editors 
proc 
workshop clustering high dimensional data applications 
siam 
dhillon guan kogan 
iterative clustering high dimensional text data augmented local search 
proceedings ieee international conference data mining 
dhillon modha 
concept decompositions large sparse text data clustering 
machine learning 
dhillon sra 
modeling data directional distributions 
technical report tr university texas dept computer sciences february 
eisen spellman brown botstein 
cluster analysis display genome wide expression patterns 
proc 
natl 
acad 
sci 
functional discovery compendium expression profiles 
cell 
ghosh 
scalable clustering methods data mining 
ye editor handbook data mining pages 
lawrence erlbaum 
gupta ghosh 
detecting seasonal divergent trends visualization high dimensional transactional data 
proc 
st siam intl 
conf 
data mining april 
indyk 
sublinear time approximation scheme clustering metric spaces 
th symposium foundations computer science 
jaakkola haussler :10.1.1.44.7709
exploiting generative models discriminative classifiers 
kearns solla cohn editors advances neural information processing systems volume pages 
mit press 
jain dubes 
algorithms clustering data 
prentice hall new jersey 
kearns mansour ng :10.1.1.48.3989
information theoretic analysis hard soft assignment methods clustering 
th annual conf 
uncertainty artificial intelligence uai 
mardia 
statistical distributions volume chapter characteristics directional distributions pages 
reidel dordrecht 
mardia 
directional statistics 
john wiley sons nd edition 
neal hinton :10.1.1.33.2557
view em algorithm justifies incremental sparse variants 
jordan editor learning graphical models pages 
mit press 
nigam mccallum thrun mitchell 
text classification labeled unlabeled documents em 
machine learning 
rao 
linear statistical inference applications 
wiley new york nd edition 
sarwar karypis konstan :10.1.1.167.7612
item collaborative filtering recommendation algorithms 
world wide web pages 
sch olkopf smola 
learning kernels 
mit press 
sinkkonen kaski 
clustering conditional distributions auxiliary space 
neural computation 
smyth 
clustering sequences hidden markov models 
mozer jordan petsche editors advances neural information processing volume pages 
mit press 
strehl ghosh mooney 
impact similarity measures web page clustering 
proc th natl conf artificial intelligence workshop ai web search aaai pages 
aaai july 
