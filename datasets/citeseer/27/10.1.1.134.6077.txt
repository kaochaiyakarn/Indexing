pergamon pii vision res vol 
pp 
elsevier science rights reserved printed great britain sparse coding overcomplete basis set strategy employed bruno olshausen david received july revised form december spatial receptive fields simple cells mammalian striate cortex reasonably described physiologically characterized localized oriented ban ass comparable basis functions wavelet transforms 
previously shown receptive field properties may accounted terms strategy producing sparse distribution output activity response natural images 
addition describing expansive fashion examine neurobiological implications sparse coding 
particular interest case code overcomplete number code elements greater effective dimensionality input space 
basis functions non orthogonal linearly independent code recruit basis functions necessary representing input input output function deviate purely linear 
deviations linearity provide potential explanation weak forms non linearity observed response properties cortical simple cells predictions expected interactions units response naturalistic stimuli 
elsevier science coding gabor wavelet natural images mammalian visual cortex evolved millions years effectively cope images natural environment 
importance resources efficiently competition survival reasonable think cortex discovered efficient coding strategies representing natural images 
explore extent theories efficient coding provide insights cortical image representa tion 
notion efficiency adopt barlow principle redundancy reduction barlow states useful goal sensory coding transform input manner reduces redundancy due complex statistical dependencies confusion arises term redundancy reduction contradict conventional wisdom brain contains redundant circuitry deal noise physical damage 
important distinguish form redundancy raw input stream reflects structure external world redundancy introduced system schemes population coding motor system 
notion redundancy refer 
psychology uris hall university ithaca ny correspondence addressed department psychology center neuroscience uc davis newton ct davis ca fax email redwood ucdavis edu 
elements input stream 
usefulness redundancy reduction understood considering process image formation occurs light reflecting independent entities objects world focussed array photoreceptors retina 
activities photoreceptors form particularly useful signal organism structure world explicit embedded form complex statistical dependencies redundancies activities 
reasonable goal visual system extract statistical dependencies images may explained terms collection independent events 
hope strategy recover explicit representation underlying independent entities gave rise image useful survival organism 
atick colleagues atick redlich atick dong atick dan atick reid achieved considerable success showing principle redundancy reduction may applied understanding response properties retinal ganglion cells terms strategy whitening decorrelating set outputs response amplitude spectrum natural images 
limitation approach considered redundancy due linear pairwise correlations image pixels 
natural images olshausen field bo sparse representation ai ai ai 
sparse coding 
image represented small number active code elements ai large set 
elements active varies image 
element sparse code time inactive probability distribution activity highly peaked zero heavy tails 
contrast code probability distribution activity spread evenly range values gaussian 
oriented lines edges especially curved fractal edges give rise statistical dependencies higher order linear pairwise correlations point correlations field olshausen field important consider forms structure developing efficient code 
goal find linear coding strategy capable reducing higher order forms redundancy 
strategy reducing higher order redundancy shall explore probabilistic model capture image structure 
scheme images described terms linear superposition basis functions basis functions adapted best account image structure terms collection statistically independent events 
conjecture appropriate form probability distribution events sparse meaning image may usually described terms small number basis functions chosen larger set field illustrated fig 

shown previously code sought natural images basis functions emerge qualitatively similar form simple cell receptive fields basis functions certain wavelet transforms olshausen field 
shall examine closely consequences utilizing complete code number basis functions greater effective dimensionality input 
shall see sparse coding overcomplete basis set leads interesting interactions code elements sparsification weeds basis functions needed describe image structure 
interactions lead deviations strictly linear input output relationship observed responses cortical simple cells tested empirically 
introducing representational frame overcomplete linear generative model images 
section describes probabilistic framework modeling images terms sparse statistically independent components question sparseness addressed detail 
derive algorithm learning overcomplete sparse codes simulation results obtained applying algorithm natural images described 
discuss experimental predictions arise model relation algorithm efficient coding methods proposed 
image model describing image model revisit standard notions linear coding commonly adopted image processing community 
typical form coding strategy apply linear transform image inner product image set spatial weighting functions bi xi denotes discrete spatial position dimensional image 
output transformation represented values bi 
alternatively may write operation vector notation wi vector components ii matrix components wij 
generally goal coding strategy find invertible weight matrix transforms input criterion optimality output activities met decorrelation sparseness 
basis coding strategies discrete cosine transform jpeg image compression orthonormal wavelet transforms mallat 
terms physical implementation coding scheme realized strictly feed forward neural network case functions correspond spatial receptive field output bi 
usually thinks standard model cortical simple cell physiological data show sparse coding overcomplete basis set strategy employed vi 
interesting forms non linearity cells captured straightforward linear model 
alternative way coding images linear framework explore terms generative model illustrated fig 

image modeled terms linear superposition basis functions bi mixed amplitudes ai ai 
choice basis functions bi determines image code 
ai computed image satisfy equality quantities constitute output code 
terminology adopted wavelet community bi analysis functions synthesis functions 
cases may possible directly relate analysis functions synthesis functions 
example linearly independent inputs bi equal rows inverse transpose weight matrix formed pi di form orthonormal basis code self inverting bi 
general conditions may hold 
particular code overcomplete number basis functions exceeds effective dimensionality input number non zero eigenvalues input covariance matrix multiple solutions ai explaining image 
shall exclusively concerned case basis set overcomplete 
obvious reason desiring overcomplete code possesses greater robustness face noise forms degradation 
reason pertinent purposes overcomplete code allow greater flexibility matching generative model input structure 
especially important images little reason believe priori images composed discrete independent causes dimensionality input 
features compose images occur continuum positions scales overcomplete code allow smooth interpolation continuum 
point emphasized simoncelli freeman adelson heeger show overcomplete codes allow small translations scaling local image features result smooth graceful change distribution activity coefficients 
contrast critically sampled code number basis functions exactly equals number effective input dimensions local image changes typically result fairly global drastic undulations cient values 
instabilities undesirable doing pattern analysis terms maintaining sparse image representation 
noted point order recover truly independent components images objects need utilize image model 
external 
world ai 
generative image model attempts capture underlying causes images 
case images assumed composed linear superposition basis functions mixed amplitudes ai 
goal efficient coding learn basis functions best account structure images terms statistically independent events 
goes simple linear superposition incorporates notions translation scale appearance objects retina changes depending viewing configuration non linear aspects imaging occlusion 
shall revisit concerns see section entitled challenges choose deal extra complications restrict admittedly impoverished class overcomplete linear image models order ask set bases best capture independent structure images 
useful question ask simple cells fairly linear early processing stages may limited complexity image model achieved 
probabilistic framework problem find set basis functions qs best account structure images terms linear superposition sparse statistically indepen dent events 
language probability theory wish match closely possible distribution images arising linear image model actual distribution images observed nature 
words generate images stochastically drawing ai equation independently distribution depicted fig 
probability distribution generated images look adapt resemble distribution images generated nature 
order calculate probability images arising model need specify prior probability distribution coefficients probability image arising certain state coefficients model lla 
specified probabilistic aspects imaging olshausen field lla il 
dimensional iso probability plots gaussian likelihood cauchy prior product 
axes plot 
model probability image arising model ii da 
shall specify form distributions discuss problem assessing closely model distribution images matches observed nature 
probability image arising particular choice coefficients essentially expresses model level noise uncertainty imaging process 
assume gaussian white additive image noise imaging model ai probability image arising particular choice coefficients ila zan ii bl denotes sum ai cr nis variance noise normalizing constant 
basis set overcomplete infinite variety explaining image lla take form gaussian ridge line plane 
illustrated dimensions fig 

prior probability distribution coefficients incorporate notion sparse statistically independent components image model 
statistical independence incorporated specifying factorial distribution ai oi 
probability state coefficients simply product individual probabilities component ai 
notion sparseness incorporated shaping probability distribution ai uni modal peaked zero heavy tails implying units inactive fig 

choose parameterize distribution ai function determines shape distribution fl parameter controls steepness normalizing constant 
example choosing fi logo corresponds specifying cauchy distribution prior desired sparse shape fig 

assess probability distribution images generated model ii matches actual probability distribution images sampled nature take kullback leibler divergence distributions kl log di 
measures average information gain image drawn judging favor image drawn opposed lq kullback 
greater difference distributions greater kl kl distributions equal 
fixed minimizing kl amounts maximizing log kb log jp log di 
goal learning find set sparse coding overcomplete basis set strategy employed vi 
maximize average log likelihood images sparse statistically independent prior 
sparseness 
reason sparseness appropriate prior ai intuition natural images may generally described terms small number structural primitives example edges lines elementary features field 
addition see evidence sparse structure images filtering set log gabor filters collecting histograms resulting output distributions distributions typically show high kurtosis field indicative sparse structure 
form reasoning leads believe sparse coding appropriate natural images consider implied seeking alternative form probability distribution code elements multi modally distributed 
case event image feature take values frequently spend little time 
difficult conceive examples natural images 
tends typically case event occurs rarely spends time zero occur continuum giving rise distribution depicted fig 

note reasons desiring sparseness separate written increasing capacity associative memory baum moody minimizing wiring length ease forming associations metabolic efficiency baddeley 
obvious advantages sparse code independent criteria considering 
data composed causes multimodal distributions heavy peaks non zero values seeking sparse code constitute inappropriate strategy 
words sparse coding general principle finding statistically indepen dent components data applies data sparse structure 
learning sparse codes turn problem learning set basis functions image model best accounts images terms sparse statistically independent components 
described probabilistic framework goal find set bases qs arg na log 
unfortunately easier said done evaluation requires integrating possible states equation general intractable 
assume function inside integral lal lla fairly tightly peaked maximum space fig 
may approximate volume surface integral evaluating maximum 
goal find arg max cp 
price pay approximation trivial solution bi greater norm smaller ai increasing due peak zero 
problem may alleviated adding appropriate constraint length basis functions described 
order see optimization problem equation involves helpful re cast objective energy function framework defining logp la case equation may restated arg alo ai step obtained expressions ila equations setting 
function minimized sum terms term computes reconstruction error forces span input space second term incurs penalty coefficient activities encourages sparse representations 
minimized separate phases nested inside 
inner phase minimized respect ai image holding bi fixed 
outer phase long timescale image presentations minimized respect inner loop minimization ai may performed iterating appropriate procedure derivative respect ai zero 
image ai determined equilibrium solution differential equation ti bi st ai residual image ai 
equation ai driven sum terms 
term takes spatially weighted sum current residual image function bi weights 
second term applies non linear self inhibition ai derivative differentially pushes activity zero shown fig 

neural network implementation computation shown fig 

outer loop minimization may olshausen field bo ai ai ai 
effect sparseness cost function input output relationship unit 
sparseness cost function log 
derivative sparseness cost function utilized gradient descent 
effect differentially suppress values near zero 
accomplished simple gradient descent 
yields learning rule air learning rate 
terms network implementation shown fig 
updated simple hebbian learning outputs computed image ai resulting residual image 
mentioned doing result growing bound prevent happening norm basis function separately adapted output variance ai held appropriate level oal desired variance coefficients 
intuitive interpretation algorithm image presentation gradient distribution activity ai differentially reducing value low activity coefficients high activity coefficients 
learn error induced sparsification process resulting set bases tolerate sparsification minimum mean square reconstruction error 
basis set overcomplete non orthogonal effect cation choose case overlaps bases effective describing image structure 
interaction bases cause outputs somewhat non linear function inputs 
note closed form solution ai terms input 
ai determined result recurrent computation 
form computation similar analysis synthesis loop proposed mumford way cortical feedback perform inference images 
case system trying infer bases appropriate explaining image 
simulation methods order confirm algorithm capable recovering sparse independent structure tested number artificial data sets containing known forms sparse structure 
method results tests described olshausen field 
focus applying algorithm natural images 
data training taken pixel images natural surroundings trees rocks mountain scenes 
data raw form pose potential problems vast variance different directions input space corrupted artifactual data highest image spatial frequencies 
large variance due power spectrum natural images 
image statistics roughly stationary eigenvectors covariance matrix essentially equivalent fourier bases 
variance low frequency eigenvectors larger variance high frequency eigenvectors 
produces huge differences variance different directions troublesome gradient descent techniques searching structure space 
standard technique ameliorate effects sphere data equalizing variance directions friedman schematically illustrated fig 

amplitude spectrum falls roughly orientations frequency plane sphering may accomplished filtering circularly symmetric whitening filter frequency response wq attenuating low frequencies boosting high frequencies yield roughly flat amplitude spectrum spatial frequencies 
wise boost high frequencies indiscriminately reasons highest spatial frequencies digitized images typically corrupted noise effects aliasing 
order avoid aliasing image sufficiently blurred sampling power spectrum reduced nearly zero nyquist frequency corresponding largest sample spacing grid 
order resulting sampled image looking blurred integrity data highest spatial frequencies sacrificed order image look sharp 
sparse coding overcomplete basis set strategy employed vi 

network implementation computing ai 
output unit represents value single coefficient ai 
output activities fed back basis functions qbi form reconstruction image 
reconstructed image subtracted input image residual image fed forward drive output ai self inhibited 
process analogous analysis synthesis loop proposed mumford performing inference images 
learning accomplished doing hebbian update average joint activity outputs ai residual image computed negative feedback connections 
second energy corners frequency domain artifact working rectangular sampling lattice effectively higher sampling density diagonals factor vertical horizontal directions fig 

reasons appropriate cut energy highest spatial frequencies corners fourier plane filtering circularly symmetric low pass filter 
chose purpose exponential filter frequency response fo cutoff frequency cycles picture steepness parameter 
chosen produce fairly sharp cutoff avoid eliminating data sharp introduce substantial ringing space domain 
combined whitening low pass filter preprocess data frequency response fe ff 
phase filter set zero frequencies 
profile resulting filter frequency space domain shown fig 

filter roughly resembles spatial frequency response characteristic retinal ganglion cells 
training data obtained extracting image patches random preprocessed images skipping patch pixels border image 
speed training image patch average image variance discarded patches low variance contribute little establishing gradient consume equal amount computation time 
ai computed initializing iterating equation conjugate gradient method halting iterations change whichever came 
stopping point chosen observing iterations slight changes occurred ai 
set basis functions initialized random values updated equations averages computed image presentations 
learning rate parameter gradually lowered learning initial setting iterations second iterations remainder 
rate parameter gain adjustment set target level output variance goal set variance image pixels cr 
value parameter set relative ai ai 
form sparseness cost function log 
results stable solution usually arrived approximately updates image tions 
result shown fig 

vast majority basis functions localized array exception low frequency functions expected occupy larger spatial extent 
functions oriented broken different spatial frequency bands 
result come olshausen field yl 

iy rectangular sampling artefacts region valid data 
illustration steps taken preprocessing 
hypothetical distribution sphered variance directions equal 
energy fourier plane artifact rectangular sampling 
low pass filtering circle frequency space eliminates artifacts 
profile combined low pass whitening filter fe spatial frequency space inverse fourier transform ift assumes zero phase 
surprise simply reflects fact natural images contain localized oriented structures limited phase alignment spatial frequency field 
result intuitive sense common image structures lines edges may captured handful oriented basis functions having separate descriptor pixel line edge 
observed fig 
learned bases code structures natural images sparsely pixels set bases chosen random 
general form solution localized oriented bandpass functions robust observed values ranging different forms prior laplacian 
noted preprocessing steps mentioned previously affect qualitative appearance basis functions localized oriented bandpass functions 
main effect whitening vastly decreases time required learning better gradients pointing true minimum obtained minimizations respect ai fewer iterations required variables 
devised modification algorithm speeds learning requiring whitening results look similar shown 
main effect low pass filtering removes artifacts sampling 
low pass filtering visible anisotropy orientation tuning diagonally oriented functions somewhat elongated horizontal vertically oriented functions 
addition functions appear localized checker boards expected order tile far corners fourier plane 
entire set basis functions forms complete image code spans joint space spatial position orientation scale manner similar wavelet codes previously shown form sparse representations natural images field field sparse coding overcomplete basis set strategy employed vi 
ii ii ii ii ii 
set basis functions learned sparse coding algorithm 
basis functions totally overlapping entire set codes image patch 
normalized fill grey scale zero represented grey level 
daugman 
shown fig 
distribution basis functions spatial frequency orientation 
vast majority lie high spatial frequency bands expected wavelet code order form complete tiling space spatial frequency 
note basis functions deviate somewhat strict self similarity high spatial frequency functions narrowly tuned log frequency low spatial frequency functions 
characterizing band width vs spatial frequency relationship adequately require simulations larger window sizes order span larger range spatial frequencies 
number basis functions equals number input pixels representation effectively times overcomplete discern observing eigenvalues input covariance matrix singular values matrix drop sharply dimensions 
effect sparsification overcomplete representa tion demonstrated fig 

compare distribution activity obtained purely ward computation bi coefficient values ai 
readily see case sparseness cost function shifts responsibility coding structure units best match structure units 
input output relationship unit somewhat non linear units selective aspects image respond 
non linear response property closed form solution response ai image receptive field olshausen field fx cycles picture 
distribution basis functions spatial frequency 
basis function fit gabor function spatial frequency underlying gabor function plotted upper half fx fy plane 
unit may discerned mapping various spatial functions similar methods employed physiological experiments 
previously ascertained receptive field unit spot mapping showed basically similar form basis functions somewhat tighter spatial localization olshausen field fig 

addition mapping spots mapped response gratings spatial frequency orientation 
result assays unit lth row column shown fig 

space frequency domains unit selective stimulus properties unit better take 
effect seen inverse fourier transform spatial frequency response shows undulations obtained spot mapping due sharper cutoff spatial frequency 
similar effect observed cortical cells 
discussion model predictions important prediction arises overcomplete sparse coding model expect observe interesting forms interaction response simple cells coding images 
example scenario illustrated fig 

units overlapping basis functions strictly feedforward computation took inner product basis function image result units responding aligned edge having somewhat higher activity 
code unit aligned edge take responsibility coding unit suppressed needed 
potential advantage coding scheme forming associations easier having consider relationships units truly necessary representing structure 
possible tage loss population style code susceptible noise small changes input small translation image feature result distinctly different patterns neural activity new basis function code translated structure 
case question coding schemes employed resolved multi unit record ing methods 
isolating overlapping simple cells ascertained spot mapping methods observing joint activity response naturalistic stimuli containing edges contours see trade responses depicted fig 

outcome sparse coding learning algorithm pertains cortical image representation basis functions high spatial frequency bands substantially fewer lower spatial frequency bands 
tiling space spatial frequency expected wavelet code exact proportions depend bandwidth spacing factor decrease number expected octave decrease spatial frequency 
currently available physiological assays relative numbers cells different spatial frequency bands disagree ment general picture 
studies macaque put vast majority simple cells mid low spatial frequency range cyc deg region highest spatial frequency band range cyc deg de valois albrecht parker 
reason believe number sparse coding overcomplete basis set strategy employed vi 
ii iii ii ili iiii ii il ii 
iiii ii iiii iii il il il 

sy olshausen field ni receptive field frequency response receptive field predicted frequency response 
results mapping response profile unit bars top gratings bottom 
right shown hilbert transform pair spatial profiles recovered inverse fourier transform frequency response assuming zero phase 
note show ringing due sharper cutoff frequency response incurred sparsification 
high frequency cells may substantially underestimated units generally smaller receptive fields difficult isolate low frequency unit exhibits prolonged response bars olshausen anderson 
important issue resolve experiments wavelet codes taken seriously models complete early visual representation 
sparse coding vs coarse coding notion sparse coding relatively small number units recruited represent image odds notion coarse coding population codes large numbers units participate coding single parameter attribute color stimulus velocity 
noted code utilized sparse distributed code occupies middle ground dense population codes local representa tions grandmother cells hinton ghahramani 
note example learned basis functions broadly tuned stimulus dimensions spatial frequency expected coarse code narrowly tuned position local code 
sparse distributed code units share representation different images minimize total number active image 
dense population codes appropriate situations single attributes need encoded intended position actuator motor system 
important represent attributes simultaneously various spatial features image introducing population codes effectively blur spatial position nearby features indistinguishable single feature positioned mean 
sparse coding coarse coding schemes appropriate different circumstances 
resolving nervous system different coding schemes played important goal experi ments 
relation prager developed algorithm concurrently independently virtually identical 
applied algorithm number test problems showing learn sparse structure data 
tested natural images obtaining similar results 
algorithms quite similar idea finding independent components data called independent components analysis ica 
algorithms comon amari cichocki yang bell sejnowski 
closely related bell sejnowski see article issue 
formal relationship described appendix see olshausen shows algorithms solving maximum likelihood problem making different simplifying assumptions 
bell sejnowski assume weight matrix square full rank unique solution exists ai terms feedforward model equation 
advantage approach algorithm runs considerably faster 
disadvantage code overcomplete limited critically sampled representation 
trained natural images bell sejnowski algorithm develops receptive fields basis functions qualitatively similar sparse coding overcomplete basis set strategy employed vi 
feedforward response bi response ai 
example scenario basis function overlap 
respond strictly linear feedforward network sparse coding network function best describes stimulus responds 
described major difference units grouped high frequency broadband functions spanning range spatial frequencies 
addition algorithm develops checkerboard receptive fields arise artifact working rectangular sampling lattice training images low pass filtered remove energy corners fourier domain 
algorithm trained images low pass filtered reduced dimensionality basis functions simply drop take zero norm algorithm able utilize extra dimensions 
methods learning sparse codes described zemel 
principal difference unit values considered binary models conceivably extended analog domain 
algorithms formed inspiration develop ment algorithm 
class efficient coding methods projection pursuit methods friedman intrator law cooper fyfe baddeley press lee lu 
trained natural images exception press lee lu 
show full family receptive fields forming complete image code 
realm generative models dayan hinton neal zemel rao ballard described methods learning causal structure data hierarchical fashion 
rao ballard network reduced single layer system similar uses quadratic penalty term corresponding gaussian prior 
trained natural images develop localized receptive fields artificially localized gaussian spatial window presumably prior gaussian sparse 
challenges major limitation relies entirely linear image model necessarily limited forms independent structure extract images 
real causes images objects mix linearly occlude undergo shifts position changes size rotations types interaction need included generative image model order hope recovering real causes images independence principle 
example deal translation may modify equation form ai case need determine shift parameters addition ai image 
case occlusion saund described soft function dealing feature overlaps binary image domain 
analog domain appear necessary introduce dynamic variable represent depth ordering scene order properly render overlapping objects 
shortcoming current image model utilizes single stage 
surely statistical dependencies elements single stage model desirable modeled second third stage hierarchical fashion dayan lewicki sejnowski 
order nonlinearities mentioned need dealt 
simply adding linear image model top current number units results merely identity transform discovered unpublished observations 
surprising happened beg question combined transformation discovered linear stage 
adding units second stage may enable discovery complex structure unacceptable solution simply result combinatorial explosion complex features replicated position scale 
olshausen field considered purely empirical point view response properties cortical neurons bewildering array data little sense theory interpretation 
form theory attempted offer notion visual cortex trying produce efficient representation terms extracting statistically independent hopefully meaningful structure images 
drew prior notions structure natural images order propose sparse coding viable option reducing statistical dependencies elements representation 
receptive fields emerge algorithm strongly resemble primary visual cortex previously deduced engineers form efficient image representations 
solution robust long notion sparseness enforced provides compelling functional account response properties cortical simple cells terms sparse code natural images 
code overcomplete interesting forms non linearity arise input output relationship forms interaction may tested experimentally 
current theory merely sheds light response properties cortical simple cells hope general approach extended hierarchical fashion may lend insights aspects cortical processing response properties neurons higher stages cortical processing role feedback 
amari cichocki yang 

new learning algorithm blind signal separation 
advances neural information processing systems vol 

cambridge ma mit press 
atick 

information theory provide ecological theory sensory processing 
network 
atick redlich 

theory early visual processing 
neural computation 
atick redlich 

retina know natural scenes 
neural computation 
baddeley 

efficient code vi 
nature 
barlow 

possible principles underlying transforma tions sensory messages 

ed sensory communication pp 

cambridge ma mit press 
barlow 

unsupervised learning 
neural computation 
baum moody 

internal representations associative memory 
biological cybernetics 
bell sejnowski 

information maximization approach blind separation blind deconvolution 
neural computation 
comon 

independent component analysis new concept 
signal processing 
dan atick reid 

efficient coding natural scenes lateral geniculate nucleus experimental test computational theory 
journal neuroscience 
daugman 

entropy reduction decorrelation visual coding oriented neural receptive fields 
eee transactions biomedical engineering 
dayan hinton neal zemel 

helmholtz machine 
neural computation 
de valois albrecht 

spatial frequency selectivity cells macaque visual cortex 
vision research 
dong atick 

temporal decorrelation theory lagged non lagged responses lateral geniculate nucleus 
network computation neural systems 
field 

relations statistics natural images response properties cortical cells 
journal optical society america 
field 

scale invariance self similar wavelet transforms analysis natural scenes mammalian visual systems 
hunt 
eds wavelets fractals fourier transforms pp 

oxford oxford university press 
field 

goal sensory coding 
neural computation 


forming sparse representations local anti learning 
biological cybernetics 


sparse coding primate cortex 
arbib 
ed handbook brain theory neural networks pp 

cambridge ma mit press 
friedman 

exploratory projection pursuit 
journal american statistical association 
fyfe baddeley 

finding compact sparse distributed representations visual images 
network 


low entropy coding unsupervised neural networks 
ph thesis dept electrical engineering cambridge university 
prager 

development low entropy coding recurrent network 
network 
hinton ghahramani 
generative models discovering sparse distributed representations 
philosophical trans actions royal society press 
intrator 

feature extraction unsupervised neural network 
neural computation 
kullback 

information theory statistics 
new york john wiley sons 
law cooper 

formation receptive fields realistic visual environments bienenstock cooper munro bcm theory 
proceedings national academy sciences usa 
lewicki sejnowski 

bayesian unsupervised learning higher order structure 
advances neural information processing systems vol 

cambridge ma mit press 
lu 

independence rejection unsupervised learning algorithm extracting latent source structures arbitrary image populations 
technical report mbs 
institute mathematical behavioral sciences uc irvine 
mallat 

theory multiresolution signal tion wavelet representation 
ieee transactions pattern analysis machine intelligence 
mumford 

neuronal architectures pattern theoretic problems 
koch davis 
eds large scale neuronal theories brain pp 

cambridge ma mit press 
olshausen 

learning linear sparse factorial codes 
ai memo massachusetts institute technology 
olshausen anderson 

model spatial frequency organization primate striate cortex 
bower 
ed computation pp 

kluwer 
field 
emergence simple cell receptive field properties learning sparse code natural images 
nature 
olshausen field 
natural image statistics efficient coding 
network 
parker 

dimensional spatial structure receptive fields monkey striate cortex 
journal optical society america 
sparse coding overcomplete basis set strategy employed vi 
pearlmutter parra 

context sensitive generalization ica 
international conf 
neural information processing 
september hong kong 
press lee 

projection pursuit analysis statistical structure natural scenes 
cns cambridge ma 
rao ballard 

dynamic model visual recognition predicts neural response properties visual cortex 
neural computation 
saund 

multiple cause mixture model unsupervised learning 
neural computation 
simoncelli freeman adelson heeger 

shiftable multiscale transforms 
ieee transactions information theory 


effect threshold relationship receptive field profile spatial frequency tuning curve simple cells cat striate cortex 
visual neuroscience 
zemel 

minimum description length framework unsupervised 
ph thesis university toronto department computer science 
mike lewicki chris lee tony bell george peter dayan federico girosi useful conversations development 
authors supported nimh bao mh 
part carried center biological computational learning mit 
appendix relation ca algorithm bell sejnowski bell sejnowski describe algorithm independent components analysis ica maximizing mutual information inputs outputs neural network see article issue 
show algorithm may understood solving maximum likelihood problem algorithm making different simplifying assumption 
connection shown parra 
bell sejnowski examine case number basis functions equal number inputs linearly independent 
case unique set ai ii aq equals zero image terms previous discussion lla gaussian hump single maximum gaussian ridge fig 

go zero equation lla delta function integral equation ao da lo det arg log log det making definitions convention bell sejnowski ui wi gradient descent learning rule ui ij precisely bell sejnowski learning rule output non linearity network equal cdf cumulative density function prior ai yi ui ui le dx 
independent component analysis algorithm bell formally equivalent maximum likelihood case noise square system dimensionality put dimensionality input 
easy generalize case number outputs number inputs way 
number outputs greater effective dimensionality input non zero eigenvalues input covariance matrix extra dimensions output simply drop 
pose problem blind separation problems number independent sources dimensionality equal number mixed signals dimensionality concern representation images desirable feature 
