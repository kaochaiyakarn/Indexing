decades statistical language modeling go 
statistical language models estimate distribution various natural language phenomena purpose speech recognition language technologies 
significant model proposed attempts improve state art 
review point promising directions argue bayesian approach integration linguistic theories data 

outline statistical language modeling slm attempt capture regularities natural language purpose improving performance various natural language applications 
large statistical language modeling amounts estimating probability distribution various linguistic units words sentences documents 
statistical language modeling crucial large variety language technology applications 
include speech recognition slm got start machine translation document classification routing optical character recognition information retrieval handwriting recognition spelling correction 
machine translation example purely statistical approaches introduced 
researchers rule approaches beneficial introduce elements slm statistical estimation 
information retrieval language modeling approach proposed statistical information theoretical approach developed 
slm employs statistical estimation techniques language training data text 
categorical nature language large vocabularies people naturally statistical techniques estimate large number parameters consequently depend critically availability large amounts training data 
ronald rosenfeld school computer science carnegie mellon university pittsburgh pa usa roni cs cmu edu past years successively larger amounts text various types available online 
result domains data available quality language models increased dramatically 
improvement asymptote 
online text continues accumulate exponential rate doubt growth rate web quality currently statistical language models improve significant factor 
informal estimate ibm shows bigram models effectively saturate words trigram models saturate words 
domains data 
ironically successful slm techniques little knowledge language really popular language models grams take advantage fact modeled language may sequence arbitrary symbols deep structure intention thought 
possible reason situation knowledge impoverished data optimal techniques grams succeeded knowledge approaches 
go far knowledge 
words premier proponent statistical approach language modeling fred jelinek put language back language modeling 
unfortunately handful attempts date incorporate linguistic structure theories knowledge statistical language models attempts modestly successful 
follows section introduces statistical language modeling detail discusses potential improvement area 
section overviews major established slm techniques 
section lists promising current research directions 
section suggests interactive approach bayesian approach tion linguistic knowledge model points encoding knowledge main challenge facing field 

statistical language modeling 
definition statistical language model simply probability distribution possible sentences instructive compare statistical language modeling computational linguistics 
admittedly fields communities fuzzy boundaries great deal overlap 
way characterize difference follows 
word sequence sentence surface form hidden structure associated parse tree word senses 
statistical language modeling estimating pr computational linguistics estimating pr course estimate joint pr pr pr derived 
practice usually feasible 
statistical language models usually context bayes classifier play role prior likelihood function 
example automatic speech recognition acoustic signal goal find sentence spoken 
bayesian framework solution language model plays role prior 
contrast document classification document goal find class belongs 
typi cally examples documents say classes different language models constructed 
bayes classi solution language model plays role 
similar fashion derive role language models bayesian classifiers language technologies listed 

measures progress assess quality language modeling technique likelihood new data commonly 
average log likelihood new random sample spoken utterances documents linguistic unit 
average log likelihood new data sample language model 
quantity viewed empirical estimate cross entropy true unknown data distribution regard model distribution cross entropy actual performance language models reported terms perplexity perplexity interpreted geometric average branching factor language model 
function language model 
considered function model measures model better model lower perplexity 
considered function language estimates entropy complexity language 
ultimately quality language model measured effect specific application designed effect error rate application 
error rates typically non linear poorly understood functions language model 
lower perplexity usually result lower error rates plenty counterexamples literature 
rough rule thumb reduction perplexity usually practically significant reduction noteworthy usually translates improvement application performance perplexity improvement baseline quite significant rare 
attempts devise metrics better correlated application error rate perplexity easier optimize error rate 
attempts met limited success 
perplexity continues preferred metric practical language model construction 
details see :10.1.1.28.9434
cross entropy 
known weaknesses current models simplest language model drastic effect application observed say removing language model speech recognition system 
current language modeling techniques far optimal 
evidence comes sources brittleness domains current language models extremely sensitive changes style topic genre text trained 
example model casual phone conversations better words transcripts conversations words transcripts tv radio news broadcasts 
effect quite strong changes trivial human language model trained dow jones newswire text see perplexity doubled applied similar associated press newswire text time period 
false independence assumption order remain tractable virtually existing language modeling techniques assume form independence different portions document 
example commonly model gram assumes probability word sentence depends identity words 
cursory look natural text proves assumption patently false 
false independence assumptions statistical models usually lead overly sharp distributions 
precisely happening language modeling seen example document classification posterior computed equation usually extremely sharp reaching virtually classes virtually zero 
course true posterior average classification error rate typically greater zero 
shannon style experiments claude shannon pioneered technique eliciting human knowledge language asking human subjects predict element text 
shannon technique bound entropy english 
formulated gambling setup derive estimate entropy english 
speech language research group ibm performed shannon style experiments potential sources language modeling improvement identified observing analyzing performance human subjects predicting correcting text 
experiments performed researchers 
example performed experiments aimed establishing potential language modeling improvements specific linguistic areas 
common observation experiments people improve performance language model easily routinely substantially 
apparently reasoning linguistic common sense domain levels 

survey major slm techniques section briefly reviews major established slm techniques 
detailed technical treatment see 
language models date decompose probability sentence product conditional probabil ities pr def pr pr th word sentence called history 

grams def grams current speech recognition technology 
virtually commercial speech recognition products form gram 
gram reduces dimensionality estimation problem modeling language markov source order value trades stability estimate variance appropriateness bias 
trigram common choice large training corpora millions words bigram smaller ones 
deriving trigram bigram probabilities sparse estimation problem large corpora 
example observing trigrams consecutive word triplets words worth newspaper articles full third trigrams new articles source novel 
furthermore observed trigrams vast majority occurred majority rest similarly low counts 
straightforward maximum likelihood ml estimation gram probabilities counts advisable 
various smoothing techniques developed 
include discounting ml estimates recursively backing lower order grams linearly interpolating grams different order 
approaches include variable length gram lattice approach 
done compare perfect smoothing techniques various conditions 
analysis :10.1.1.131.5458
addition toolkits implementing various techniques disseminated :10.1.1.33.3380
way battle sparseness vocabulary clustering 
class word assigned 
model structures 
example trigram pr pr pr pr pr pr pr pr pr pr pr quality resulting model depends course clustering narrow discourse domains atis results achieved manual clustering semantic categories 
constrained domains manual clustering linguistic categories parts speech usually improve model 
automatic iterative clustering information theoretic criteria applied large corpora reduce perplexity model interpolated word counterpart :10.1.1.13.9919

decision tree models decision trees cart style algorithms applied language modeling 
decision tree arbitrarily partition space histories asking arbitrary binary questions history internal nodes 
training data leaf con struct probability distribution pr reduce variance estimate leaf distribution interpolated internal node distributions path root 
usual trees grown greedily selecting node informative question judged reduction entropy 
pruning cross validation 
applying cart technology language modeling word 
quite challenge space histories large word sequence word vocabulary space possible questions larger 
questions restricted individual words history questions 
strong bias introduced restricting class questions considered greedy search algorithms 
support optimal single word questions node algorithms developed rapid optimal binary partitioning vocabulary 
attempt cart style lm history window words restricted questions individual words allowed complicated questions consisting composites simple questions 
took months train result fell short expectations reduction perplexity baseline trigram reduction interpolated 
second attempt stronger bias introduced vocabulary clustered binary hierarchy word assigned bit string representing path leading root :10.1.1.13.9919
tree questions restricted identity significant bit word history 
reduced candidate set handful questions node 
unfortunately results disappointing approach largely abandoned 
theoretically decision trees represent ultimate partition models 
trees exist significantly outperform ngrams 
finding difficult computational data sparseness reasons 

linguistically motivated models get inspiration intuitive view language models actual linguistic content quite negligible 
slm techniques directly derived grammars commonly uses linguists 
context free grammar cfg crude understood model natural language 
cfg defined vocabulary set non terminal symbols set production transition rules 
sentences generated starting initial non terminal repeated application transition rules transforming non terminal sequence terminals words non terminals terminals sequence achieved 
specific cfgs created parsed annotated corpora incomplete coverage new data 
probabilistic stochastic context free grammar puts probability distribution transitions emanating non terminal inducing distribution set sentences 
transition probabilities estimated annotated corpora inside outside algorithm estimation maximization em algorithm see 
likelihood surfaces models tend contain local maxima locally maximal likelihood points algorithm usually fall short global maximum 
furthermore global ml estimation feasible generally believed context sensitive transition probabilities needed adequately account actual behavior language 
unfortunately efficient training algorithm known situation 
spite successfully incorporated cfg knowledge sources slm achieve reduction speech recognition error rate atis domain 
parsing utterances cfg produce sequence grammatical fragments various types constructing trigram fragment types supplant standard ngram 
link grammar lexicalized grammar proposed 
word associated ordered sets typed links link connected similarly typed link word sentence 
legal parse consists satisfying links sentence planar graph 
link grammar expressive power cfg arguably conforms better human linguistic intuition 
link grammar english constructed manually coverage 
probabilistic forms link grammar attempted 
link grammar related dependency grammar discussed section 

exponential models models discussed far suffer data fragmentation detailed modeling necessarily results new parameter estimated data 
apparent decision trees tree grows leaves contain fewer fewer data points 
fragmentation avoided exponential model form parameters normalizing term features arbitrary functions pair 
training corpus ml estimate shown satisfy constraints empirical distribution training corpus 
ml estimate shown coincide maximum entropy distribution highest entropy distributions satisfying equation 
unique ml solution iterative procedure :10.1.1.43.7345
paradigm general mdi framework suggested language modeling seen considerable success :10.1.1.103.7637:10.1.1.103.7637:10.1.1.40.180:10.1.1.40.180
strength lies incorporating arbitrary knowledge sources avoiding fragmentation 
example conventional ngrams distance ngrams long distance word pairs triggers encoded features resulted perplexity reduction speech recognition word error rate reduction trigram baseline 
modeling elegant general weaknesses 
training model computationally challenging altogether infeasible 
model cpu intensive need explicit normalization 
unnormalized modeling attempted 
smoothing analyzed 
relative success modeling focused attention remaining problem feature induction selection useful features included model 
automatic iterative procedure selecting features candidate set described 
interactive procedure eliciting candidate sets described 
language modeling remains subject intensive research see example 

adaptive models far treated language homogeneous source 
fact natural language highly heterogeneous varying topics genres styles 
cross domain adaptation test data comes source language model exposed training 
useful adaptation information current document 
common quite effective technique exploiting information cache continuously developing history create turn runtime dynamic gram interpolated static model weight optimized held data 
cache lms introduced 
report reduction perplexity reports reduction recognition error rate 
introduced adaptation scheme 
domain adaptation test data comes source training data heterogeneous consisting subsets varying topics styles 
adaptation proceeds steps 
clustering training corpus dimension variability say topic 

runtime identifying topic set topics test data 

locating appropriate subsets training corpus build specific model 

combining specific model corpus wide model statistical terminology shrinking specific model general trade variance bias 
usually done linear interpolation word probability level sentence probability level 
special common case small amounts data target domain large amounts domains 
case relevant step combining models domains 
outcome disappointing training data outside domain surprisingly little benefit 
example modeling switchboard domain conversational speech words wsj corpus newspaper articles words bn corpus broadcast news transcriptions improve percentage points application performance domain model trained words 
significant improvement difficult corpus disappointing considering amount data involved 
estimates words switchboard data help model words outof domain data 
suggest adaptation techniques crude 

promising current directions section discusses current research directions author subjective opinion show significant promise 

dependency models dependency grammars dg describe sentences terms asymmetric pairwise relationships words 
single exception word sentence dependent word called head parent 
single exception root serves head entire sentence 
dgs see 
probabilistic dgs developed algorithms learning corpora 
probabilistic dependency grammars particularly suited gram style modeling word predicted small number words 
main difference conventional gram structure model predetermined word predicted words immediately preceded 
dg words serve predictors depends dependency graph hidden variable 
typical implementation parse sentence generate dependency graphs attendant probabilities compute generation probability gram style model estimate complete sentence probability approximate selves derived sentence approximated single best scoring parse 
example model uses parser generate candidate parses trains parameters maximum entropy 
probabilistic link grammar mentioned section falls roughly category 
employed parser probabilistic parameterization pushdown automata em type algorithm training encouraging results recognition word error rate reduction notoriously difficult switchboard corpus 
method combining hidden linguistic structure chain rule parameterization yield linguistically grounded computationally tractable model 

dimensionality reduction reasons language hard model statistically ostensibly categorical extremely large number categories dimensions 
prime example vocabulary 
language models vocabulary large set unrelated entries 
bank closer loan banks say brazil 
results large number parameters 
linguistic intuition great deal structure relationship words 
feel true dimension vocabulary quite lower 
similarly phenomena language underlying space may moderate low dimensionality 
consider topic adaptation 
topic changes probabilities words vocabulary change 
documents exactly thing straightforward approach require inordinate number parameters 
underlying topic space reasonably modeled fewer dimensions 
motivation uses technique latent semantic analysis simultaneously reduce dimensionality vocabulary topic space 
occurrence vocabulary word document tabulated 
large matrix reduced singular value decomposition lower dimension typically 
new smaller matrix captures salient correlations specific combinations words hand clusters documents 
decomposition yields matrices project document space word space new combined space 
consequently new document projected combined space effectively classified combination fundamental underlying topics adapted accordingly 
type adaptation combined gram perplexity reduction trigram baseline reported 
technique developed reduce recognition errors trigram baseline 

sentence models language models described far chain rule decompose probability sentence product conditional probabilities type pr cally done facilitate estimation relative counts 
decomposition ostensibly harmless approximation exact equality 
result language modeling large reduced modeling distribution single word 
turn may significant hindrance modeling linguistic structure linguistic phenomena impossible best awkward think encode conditional framework 
include sentence level features person number agreement semantic coherence length 
furthermore external influences sentence previous sentences topic factored prediction word cause small biases compound 
address issues proposed sentence exponential model compared conditional exponential model equation true constant eliminates serious burden normalization 
importantly features capture arbitrary properties entire sentence 
training model requires sampling exponential distribution non trivial task 
monte carlo markov chain sampling methods language studied 
sampling efficiency crucial 
consequently bottleneck model number features amount data rare features accurately need modeled 
interestingly shown benefit come common features 
parse features tried semantic features discussed 
interactive methodology feature induction proposed 
methodology leads formulation training problem logistic regression significant practical benefits ml training 

challenges frustrating aspect statistical language modeling contrast intuition speakers natural language simplistic nature successful models 
native speakers feel strongly language deep structure 
sure articulate structure encode probabilistic framework 
established linguistic theories surprisingly little help probably goal draw line properly language isn slm goals quite different 
example consider problem clustering vocabulary words discussed section 
mentioned automatic iterative methods proposed :10.1.1.13.9919
table lists example word classes derived method 
words placement appear satisfactory words place 
surprisingly words count corpus insufficient reliable assignment 
ironically exactly words stood benefit clustering 
general reliably table data driven word classes 
committee commission panel subcommittee 
attorney surgeon 
action activity intervention warfare 
center association faceted institute 
particular year night morning word assigned class benefit assignment 
vocabulary clustering effective 
believe solution problem inject human knowledge language process 
take forms interactive modeling 
data driven optimization human knowledge decision making play complementary roles intertwined iterative process 
vocabulary clustering problem means human put loop arbitrate borderline decisions override 
example human decide tuesday belongs cluster monday day thursday friday occur times placed automatically occur 
example approach interactive feature induction methodology described 
encoding knowledge priors 
human knowledge wrong 
better solution encode knowledge prior bayesian updating scheme 
training phenomena sufficiently represented training corpus continue captured prior 
data exist override prior 
vocabulary clustering problem experts beliefs relationships vocabulary entries suitably encoded clustering paradigm changed optimize appropriate posterior measure 
example data may exist separate friday phrases god friday 
encoding linguistic knowledge prior exciting challenge seriously attempted 
include defining distance metric words phrases stochastic version structured word ontologies wordnet 
syntactic level include bayesian versions manually created lexicalized grammars 
practice bayesian framework interactive process may combined advantage superior theoretical foundation computational advantages 
am grateful chen sanjeev khudanpur john lafferty bob moore helpful comments 

peter brown john cocke stephen della pietra vincent della pietra frederick jelinek john lafferty robert mercer paul 
statistical approach machine translation 
computational linguistics june 
ralf brown robert 
applying statistical english language modeling symbolic machine translation 
proceedings th international conference theoretical methodological issues machine translation tmi pages july 
ponte bruce 
croft 
language modeling approach information retrieval 
proceedings st international conference research development information retrieval sigir pages 
adam berger john lafferty 
information retrieval statistical translation 
proceedings nd annual conference research development information retrieval sigir pages 
fred jelinek 
language modeling summer workshop johns hopkins university 
closing remarks 
bahl jim baker frederick jelinek robert mercer 
perplexity measure difficulty speech recognition tasks 
program th meeting acoustical society america acoust 
soc 
am 
suppl 

stanley chen douglas beeferman ronald rosenfeld :10.1.1.28.9434
evaluation metrics language models 
proceedings darpa broadcast news transcription understanding workshop pages 
ronald rosenfeld 
maximum entropy approach adaptive statistical language modeling 
computer speech language 
longer version published adaptive statistical language modeling maximum entropy approach ph thesis computer science department carnegie mellon university tr cmu cs april 
shannon 
mathematical theory communication 
bell systems technical journal 
shannon 
prediction entropy printed english 
bell systems technical journal january 
cover king 
convergent gambling estimate entropy english 
ieee transactions information theory 
brill florian henderson 
grams linguistic sophistication improve language modeling 
proceedings th annual meeting acl 
frederick jelinek 
statistical methods speech recognition 
mit press cambridge massachusetts 

population frequencies species estimation population parameters 
biometrika 
ian witten timothy bell 
problem estimating probabilities novel events adaptive text compression 
ieee transactions information theory july 
slava katz 
estimation probabilities sparse data language model component speech recognizer 
ieee transactions acoustics speech signal processing march 
hermann ney ute essen reinhard kneser 
structuring probabilistic dependences stochastic language modeling 
computer speech language 
reinhard kneser hermann ney 
improved backing gram language modeling 
proceedings ieee international conference acoustics speech signal processing volume pages detroit michigan may 
frederick jelinek robert mercer 
interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice pages amsterdam netherlands north holland may 
ron singer tishby 
power amnesia 
cowan tesauro alspector editors advances neural information processing systems pages 
kaufmann san mateo ca 
guyon pereira 
design linguistic postprocessor variable memory length markov models 
proceedings rd icdar pages 
reinhard kneser 
statistical language modeling variable context length 
proceedings ic slp volume pages philadelphia october 
thomas niesler philip woodland 
variable length category gram language models 
computer speech language 
man hung siu mari ostendorf 
variable gram extensions conversational speech language modeling 
ieee transactions speech audio processing 
pierre dupont ronald rosenfeld 
lattice language models 
technical report cmu cs carnegie mellon university department computer science september 
stanley chen joshua goodman :10.1.1.131.5458
empirical study smoothing techniques language modeling 
proceedings th annual meeting acl pages santa cruz california june 
ronald rosenfeld 
cmu statistical language modeling toolkit arpa csr evaluation 
proceedings spoken language systems technology workshop pages austin texas january 
philip clarkson ronald rosenfeld 
statistical language modeling cmu cambridge toolkit 
proceedings european conference speech communication technology eurospeech 
andreas stolcke 
srilm sri language modeling toolkit 
www speech sri com projects srilm 
stanley chen 
language model tools user guide 
www cs cmu edu sfc manuals ps december 
patti price 
evaluation spoken language systems atis domain 
proceedings darpa speech natural language workshop june 
wayne ward 
cmu air travel information service understanding spontaneous speech 
proceedings darpa speech natural language workshop pages june 
peter brown vincent della pietra peter desouza jennifer lai robert mercer :10.1.1.13.9919
classbased gram models natural language 
computational linguistics december 
reinhard kneser hermann ney 
improved clustering techniques class statistical language modeling 
proceedings european conference speech communication technology eurospeech 
leo breiman jerome friedman richard olshen charles stone 
classification regression trees 
wadsworth brooks cole advanced books software monterey california 
bahl peter brown peter de souza robert mercer 
tree statistical language model natural language speech recognition 
ieee transactions acoustics speech signal processing july 
arthur das david nahamoo michael jeffrey powell 
iterative flip flop approximation informative split construction decision trees 
proceedings ieee international conference acoustics speech signal processing toronto canada may 
peter brown steven della pietra vincent della pietra robert mercer philip resnik 
language modeling decision trees 
research report research yorktown heights ny 
marcus santorini marcinkiewicz 
building large annotated corpus english penn 
computational linguistics 
james baker 
trainable grammars speech recognition 
proceedings spring conference acoustical society america pages boston ma june 
frederick jelinek john lafferty robert mercer 
basic methods probabilistic contextfree grammars 
de mori editors speech recognition understanding advances trends applications volume computer systems sciences pages 
springer verlag 
moore appelt dowding moran 
combining linguistic statistical knowledge sources natural language processing atis 
spoken language systems technology workshop pages austin texas february 
morgan kaufmann publishers danny sleator davy temperley 
parsing english link grammar 
technical report cmu cs computer science department carnegie mellon university pittsburgh pa october 
john lafferty danny sleator davy temperley 
grammatical trigrams probabilistic model link grammar 
proceedings aaai fall symposium probabilistic approaches natural language cambridge ma october 
jaynes 
information theory statistical mechanics 
physics reviews 
darroch ratcliff 
generalized iterative scaling log linear models 
annals mathematical statistics 
della pietra della pietra lafferty 
inducing features random fields 
ieee transactions pattern analysis machine intelligence april 
della pietra della pietra mercer roukos 
adaptive language modeling minimum discriminant estimation 
proceedings speech natural language darpa workshop february 
raymond lau ronald rosenfeld salim roukos 
trigger language models maximum entropy approach 
proceedings icassp pages ii ii april 
adam berger stephen della pietra vincent della pietra 
maximum entropy approach natural language processing 
computational linguistics 
stanley chen seymore ronald rosenfeld 
topic adaptation language modeling unnormalized exponential models 
icassp seattle washington 
stan chen ronald rosenfeld 
survey smoothing techniques models 
ieee transactions speech audio processing 
ronald rosenfeld larry wasserman cai zhu 
interactive feature induction logistic regression sentence exponential language models 
proceedings ieee workshop automatic speech recognition understanding keystone december 
doug beeferman adam berger john lafferty 
model lexical attraction repulsion 
proceedings th annual meeting association computational linguistics pages madrid spain 
john lafferty bernard suhm 
cluster expansions iterative scaling maximum entropy language models 
hanson silver editors maximum entropy bayesian methods pages 
kluwer academic publishers 
jochen peters dietrich 
compact maximum entropy language models 
proceedings ieee workshop automatic speech recognition understanding keystone december 
sanjeev khudanpur jun wu 
maximum entropy language model integrating grams topic dependencies conversational speech recognition 
proceedings ieee international conference acoustics speech signal processing phoenix az 
jun wu sanjeev khudanpur 
combining nonlocal syntactic gram dependencies language modeling 
proceedings european conference speech communication technology eurospeech budapest hungary 
roland kuhn 
speech recognition frequency words modified markov model natural language 
th international conference computational linguistics pages budapest august 
julian kupiec 
probabilistic models short long distance word dependencies running text 
proceedings darpa workshop speech natural language pages february 
roland kuhn renato de mori 
cache natural language model speech reproduction 
ieee transactions pattern analysis machine intelligence pami 
roland kuhn renato de mori 
correction cache natural language model speech reproduction 
ieee transactions pattern analysis machine intelligence pami june 
fred jelinek salim roukos bernard merialdo strauss 
dynamic language model speech recognition 
proceedings darpa workshop speech natural language pages february 
reinhard kneser volker steinbiss 
dynamic adaptation stochastic language models 
proceedings ieee conference acoustics speech signal processing pages minneapolis mn 
volume ii 
iyer mari ostendorf 
modeling long distance dependence language topic mixture vs dynamic cache models 
ieee transactions speech audio processing ieee sap 
seymore ronald rosenfeld 
story topics language model adaptation 
proceedings european conference speech communication technology eurospeech 
seymore stanley chen ronald rosenfeld 
nonlinear interpolation topic models language model adaptation 
proceedings icslp 
godfrey mcdaniel 
switchboard telephone speech corpus research development 
proceedings ieee international conference acoustics speech signal processing volume pages march 
douglas paul janet baker 
design wall street journal csr corpus 
proceedings darpa speech natural language workshop pages february 
david graff 
broadcast news speech language model corpus 
proceedings darpa workshop spoken language technology pages 
ronald rosenfeld rajeev agarwal bill byrne iyer mark liberman elizabeth shriberg jack vidal 
error analysis disfluency modeling domain 
proceedings international conference speech language processing 
mff cz dg bib html 
glenn eugene charniak 
experiments learning probabilistic dependency grammars corpora 
technical report tr computer science department brown university 
chelba david engle frederick jelinek victor sanjeev khudanpur harry eric ronald rosenfeld andreas stolcke wu 
structure performance dependency language model 
proceedings european conference speech communication technology eurospeech pages 
volume 
michael collins 
new statistical parser bigram lexical dependencies 
proceedings th annual meeting association computational linguistics pages may 
chelba fred jelinek 
recognition performance structured language model 
proceedings european conference speech communication technology eurospeech pages 
volume 
jerome bellegarda 
multi span language modeling framework large vocabulary speech recognition 
ieee transactions speech audio processing 
deerwester dumais furnas landauer harshman 
indexing latent semantic analysis 
am 
soc 
inform 
science 
jerome bellegarda 
large vocabulary speech recognition multi span statistical language models 
ieee transactions speech audio processing 
stanley chen ronald rosenfeld 
efficient sampling feature selection sentence maximum entropy language models 
icassp phoenix arizona 
zhu stanley chen ronald rosenfeld 
linguistic features sentence maximum entropy language models 
proceedings european conference speech communication technology eurospeech budapest hungary 
stanley chen 
unpublished 

christiane fellbaum editor 
wordnet electronic lexical database 
language speech communication 
mit press 
