machine learning kluwer academic publishers boston manufactured netherlands learning predict methods temporal di erences richard sutton rich gte com gte laboratories incorporated road waltham ma 
received april revised february keywords incremental learning prediction connectionism credit assignment evaluation functions 
article introduces class incremental learning procedures specialized prediction past experience incompletely known system predict behavior 
conventional prediction learning methods assign credit means di erence predicted actual outcomes new methods assign credit means di erence temporally successive predictions 
temporal di erence methods samuel checker player holland bucket brigade author adaptive heuristic critic remained poorly understood 
prove convergence optimality special cases relate supervised learning methods 
real world prediction problems temporal di erence methods require memory peak computation conventional methods produce accurate predictions 
argue problems supervised learning currently applied really prediction problems sort temporal di erence methods applied advantage 

article concerns problem learning predict past experience incompletely known system predict behavior 
example experience learn predict particular chess positions lead win particular cloud formations rain particular economic conditions rise fall 
learning predict basic prevalent kinds learning 
pattern recognition problems example treated prediction problems classi er predict correct classi cations 
learning predict problems arise heuristic search learning evaluation function predicts utility searching particular parts search space learning underlying model problem domain 
important advantage prediction learning training examples taken directly temporal sequence ordinary sensory input special supervisor teacher required 
sutton article introduce provide rst formal results theory temporal di erence td methods class incremental learning procedures specialized prediction problems 
conventional prediction learning methods driven error predicted actual outcomes td methods similarly driven error di erence temporally successive predictions learning occurs change prediction time 
example suppose attempts predict week rain saturday 
conventional approach compare prediction actual outcome rain saturday 
td approach hand compare day prediction day 
chance rain predicted monday chance tuesday td method increases predictions days similar monday conventional method increase decrease depending saturday actual outcome 
show td methods kinds advantages conventional prediction learning methods 
incremental easier compute 
example td method predicting saturday weather update day prediction day conventional method wait saturday days week 
conventional method computing time td method require storage week 
second advantage td methods tend cient experience converge faster produce better predictions 
argue predictions td methods better easier compute conventional methods 
earliest known td method samuel celebrated checker playing program 
pair successive positions game samuel program di erence evaluations assigned positions modify evaluation earlier 
similar methods holland bucket brigade author adaptive heuristic critic sutton barto sutton anderson learning systems studied witten booker 
td methods proposed models classical conditioning sutton barto hop eld tank moore klopf 
td methods remained poorly understood 
performed theoretical understanding worked 
reason studied independently parts larger complex systems 
systems td methods functions better predicting goal related events penalties checker game outcomes 
advocate viewing td methods simpler way methods ciently learning predict arbitrary events just goal related ones 
simpli cation allows evaluate isolation enabled obtain formal results 
prove convergence optimality td methods important special cases formally relate conventional supervised learning procedures 
temporal difference learning simpli cation focus numerical prediction processes rule symbolic prediction dietterich michalski 
approach taken connectionism samuel original predictions numerical features combined adjustable parameters weights 
representational assumptions detailed section 
current interest learning procedures multi layer connectionist networks rumelhart hinton williams ackley hinton sejnowski barto anderson williams note concerned di erent set issues 
multi layer networks concerned learning input output mappings complex functional forms 
remains paradigm interested extending going 
consider mappings simple functional forms di erences supervised learning methods td methods clearest cases 
td methods directly extensible case multi layer networks see section 
section introduces speci class td procedures contrasting conventional supervised learning approaches focusing computational issues 
section develops extended example illustrates potential performance advantages td methods 
section contains convergence optimality theorems discusses td methods gradient descent 
section discusses extend td procedures section relates research 

temporal di erence supervised learning approaches prediction learning historically important learning paradigm supervised learning 
paradigm learner asked associate pairs items 
just rst item pair learner supposed recall second 
paradigm pattern classi cation concept acquisition learning examples system identi cation associative memory 
example pattern classi cation concept acquisition rst item instance pattern concept second item indication 
system identi cation learner reproduce input output behavior unknown system 
rst item pair input second corresponding output 
prediction problem cast supervised learning paradigm rst item data prediction second item actual outcome prediction 
example predict saturday weather form pair measurements taken monday actual observed weather saturday pair measurements taken tuesday saturday weather 
pairwise approach ignores sequential structure problem easy understand analyze widely 
refer supervised learning approach prediction learning sutton refer learning methods take approach learning methods 
argue methods td methods far preferable 
single step step prediction clarify claim distinguish kinds prediction learning problems 
single step prediction problems information correctness prediction revealed multi step prediction problems correctness revealed step prediction partial information relevant correctness revealed step 
example weather prediction problem mentioned step prediction problem inconclusive evidence relevant correctness monday prediction available form new observations tuesday wednesday thursday friday 
hand day weather predicted basis previous day observations monday predict tuesday weather tuesday predict wednesday weather single step prediction problem assuming observations time day prediction con rmation refutation day 
concerned multi step prediction problems 
single step problems data naturally comes observation outcome pairs problems ideally suited pairwise supervised learning approach 
td methods distinct supervised learning methods case 
td methods improve conventional methods multi step problems argue predominate real world applications 
example predictions year economic performance con rmed bit bit economic situation observed year 
outcome elections updated new poll outcome chess game updated move 
baseball predicts pitch strike updates prediction continuously ball ight 
fact problems classically cast single step prediction problems naturally viewed multi step problems 
perceptual learning problems vision speech recognition classically treated supervised learning training set isolated correctly classi ed input patterns 
people hear see things hand receive stream input time constantly update hypotheses seeing hearing 
people faced single step problem unrelated pattern class pairs series related patterns providing information classi cation 
disregard structure 
computational issues subsection introduce particular td procedure formally relating classical supervised learning procedure widrow ho rule 
show temporal difference learning procedures produce exactly weight changes td procedure implemented incrementally requires far computer power 
subsection td procedure conceptual bridge larger family td procedures produce weight changes di erent supervised learning method 
detail representational assumptions 
consider multi step prediction problems experience comes observation outcome sequences form xm xt observations available time sequence outcome sequence 
sequences normally experienced 
components xt assumed real valued measurements features assumed real valued scalar 
observation outcome sequence learner produces corresponding sequence predictions pm estimate general pt function preceding observation vectors time simplicity assume function xt predictions vector modi able parameters weights pt functional dependence xt denoted explicitly writing xt 
learning procedures expressed rules updating moment assume updated complete sequence change sequence 
observation increment tow denoted wt determined complete sequence processed changed sum sequence increments mx wt consider incremental cases updated observation incremental cases updated accumulating wt training set consisting sequences 
supervised learning approach treats sequence observations outcome sequence observation outcome pairs pairs xm increment due time depends error pt changing ect pt prototypical supervised learning update procedure wt pt positive parameter ecting rate learning gradient vector partial derivatives pt respect component example consider special case pt linear function xt pt xt xt xt cases reduced reorganizing observations away xt includes earlier observations 
cases predictions depend reduced including component xt sutton th components xt respectively 
case wehave xt reduces known widrow ho rule widrow ho wt xt xt linear learning method know delta rule lms lter 
widely connectionism pattern recognition signal processing adaptive control 
basic idea di erence xt represents scalar error prediction xt multiplied observation vector xt determine weight changes xt indicates changing weight ect error 
example error positive xt positive wi increased increasing xt reducing error 
widrow ho rule simple ective robust 
theory better developed learning method see widrow stearns 
instance prototypical supervised learning procedure generalized delta rule backpropagation procedure rumelhart hinton williams 
case pt computed layer connectionist network nonlinear function xt update rule exactly just widrow ho rule di erence complicated process compute gradient case note wt depend critically determined sequence known 
observations predictions sequence remembered wt computed 
words computed incrementally 
td procedure produces exactly result computed incrementally 
key represent error pt sum changes predictions pt mx pk pk pm equations combined mx pt mx mx mx mx kx def pk pk pk pk pt pt tx transpose column vector noted vectors column vectors 
temporal difference learning words converting back rule wt pt pt tx equation computed incrementally wt depends pair successive predictions sum past values substantially memory longer necessary individually remember past values equation milder demands computational speed device implements requires slightly arithmetic operations additional ones needed accumulate pt distributed time evenly 
computes increment tow time step wait sequence completed compute increments due sequence 
maximum possible length sequence circumstances require computer th memory speed required run 
reasons clear shortly refer procedure td procedure 
addition refer procedure linear predictions pt linear function observation vectors xt vector memory parameters thatis pt xt wehave just proven theorem multi step prediction problems linear td procedure produces sequence weight changes widrow ho procedure 
introduce family td procedures produce weight changes different supervised learning procedure 
td family learning procedures hallmark temporal di erence methods sensitivity successive predictions error predictions nal outcome 
response increase decrease prediction pt pt increment wt determined increases decreases predictions preceding observation vectors xt procedure special case predictions altered equal extent 
article consider class td procedures greater alterations predictions 
particular consider exponential weighting recency predictions observation vectors occurring steps past weighted wt pt pt tx strictly speaking incremental procedures implementing combination td rule appropriate updating observation basis 
sutton note equivalent td implementation prototypical supervised learning method 
accordingly call new procedure td procedure td 
alterations past predictions exponential form may appropriate particular applications exponential form computed incrementally 
value sum incrementally compute current information xt tx td produces weight changes di erent supervised learning method 
di erence greatest case td weight increment determined ect prediction associated observation wt pt pt note procedure formally similar prototypical procedure 
equations identical actual outcome replaced prediction pt equation 
methods learning mechanism di erent errors 
relationships td simplicity focus 

examples faster learning td methods section address claim td methods cient experience supervised learning methods converge rapidly accurate predictions way 
td methods data sequences certain statistical structure ubiquitous prediction problems 
structure naturally arises data sequences generated dynamical system system state evolves partially revealed time 
real system dynamical system including weather national economies chess games 
section develop illustrative examples game playing example help develop intuitions random walk example simple demonstration experimental results 
game playing example counter intuitive learn ciently supervised learning methods 
learning predict outcome temporal difference learning novel bad loss 
game playing example showing ine ciency supervised learning methods 
circle represents position class positions person board game 
bad position known long experience lead time loss time win 
rst game novel position occurs evolves shown dashed arrows 
evaluation novel position receive result experience 
td methods correctly conclude considered bad state supervised learning methods associate fully winning outcome followed 
better knowing true actual outcome performance standard 
biased potentially inaccurate subsequent prediction possibly better experience 
example meant provide intuitive understanding possible 
suppose game position learned bad resulted time loss rarely win side 
example position backgammon race disadvantageous con guration cards 
represents simple case position single bad state led time loss time win 
suppose play game reaches position seen progresses reach bad state nally ends victory 
moves follows path shown dashed lines 
result experience opinion bad state presumably improve novel state 
value associate result experience 
supervised learning method form pair novel state win followed conclude novel state win 
td method hand form pair novel state bad state immediately followed conclude novel state bad lead loss 
assuming win sutton 
generator bounded random walks 
markov process generated data sequences example 
walks state states right left 
edge state walk terminates 
properly classi ed bad state td method correct novel state led position know usually leads defeat happened irrelevant 
methods converge evaluations nite experience td method learns better evaluation limited experience 
td method prediction better game lost reaching bad state 
case supervised learning method tend associate novel position fully losing td method tend associate bad position chance losing presumably accurate assessment 
case adjusting evaluation novel state bad state evaluation actual outcome td method better experience 
bad state evaluation better performance standard uncorrupted random factors subsequently uence nal outcome 
eliminating source noise td methods outperform supervised learning procedures 
example ignored possibility bad state previously learned evaluation error 
errors inevitably exist ect ciency td methods ways easily evaluated example 
example prove td methods better balance demonstrate subsequent prediction easily better performance standard actual outcome 
game playing example show td methods fail 
suppose bad state usually followed defeats preceded novel state leads victory 
odd case td methods perform better perform worse methods 
techniques eliminating minimizing sort problem remains greater di culty td methods supervised learning methods 
td methods try take advantage information provided temporal sequence states methods ignore 
possible information misleading helpful 
temporal difference learning note example involved learning evaluation function speci evaluation functions 
methods equally predict outcomes unrelated player goals number pieces left game 
td methods cient supervised learning methods learning evaluation functions cient general prediction learning problems 
random walk example game playing example complex analyze great detail 
previous experiments td methods complex domains samuel sutton barto sutton anderson anderson 
aspects domains simpli ed eliminated aspects essential order td methods ective 
propose required characteristic system predicted dynamical system state observed evolving time 
true td methods learn ciently supervised learning methods simple prediction problems illustrate subsection 
example simplest dynamical systems generates bounded random walks 
bounded random walk state sequence generated random steps right left boundary reached 
shows system generates state sequences 
walk begins center state step walk moves neighboring state right left equal probability 
edge state walk terminates 
typical walk suppose wish estimate probabilities walk rightmost state states 
applied linear supervised learning td methods problem straightforward way 
outcome de ned left right learning methods estimated expected value choice expected value equal probability termination 
non terminal state corresponding observation vector xi state time xt xi walk occurred learning procedure sequence xd xc xd xe xf 
vectors unit basis vectors length components fth xd appearing di erent component state 
state walk time th component observation vector prediction pt xt simply value th component particularly simple case example clear possible 
theorems prove general class dynamical systems require set observation vectors linearly independent 
computational experiments performed observation outcome sequences generated described 
order obtain statistically reliable sutton error widrow hoff 
average error random walk problem repeated presentations 
data td di erent values error measure rms error ideal predictions learning procedure repeatedly training set convergence weight vector 
measure averaged training sets produce data shown 
data point performance level attained widrow ho procedure 
data point standard error approximately di erences widrow ho procedure procedures highly signi cant 
results training sets consisting sequences constructed learning procedures 
procedures weight increments computed td 
di erent values resulting widrow ho supervised learning procedure resulting linear td resulting range intermediate td procedures 
rst experiment weight vector updated sequence indicated 
accumulated sequences update weight vector complete presentation training set 
training set repeatedly learning procedure procedure longer produced signi cant changes weight vector 
small vector nal value independent initial value 
presentations training paradigm 
true probabilities termination ideal predictions nonterminal states computed described section 
states respectively 
measure temporal difference learning error widrow hoff 
average error random walk problem experiencing sequences 
data td di erent values error measure rms error ideal predictions learning procedure single presentation training set 
measure averaged training sets 
data points represent performances widrow ho supervised learning procedure 
performance learning procedure training set mean squared rms error procedure asymptotic predictions training set ideal predictions 
averaging training sets performance improved rapidly reduced method best extreme td method shown 
result contradicts conventional wisdom 
known repeated presentations widrow ho procedure minimizes rms error predictions actual outcomes training set widrow stearns 
optimal method performed worse td methods 
answer widrow ho procedure minimizes error training set necessarily minimize error experience 
section prove fact linear td converges considered optimal estimates matching experience consistent likelihood estimate underlying markov process 
second experiment concerns question learning rate training set just repeatedly convergence 
di cult prove theorem concerning learning rate easy perform relevant computational experiment 
data learning procedures values pro sutton error best widrow hoff 
average error best value random walk problem 
data point represents average training sets error estimates td particular values single presentation training set 
value horizontal coordinate 
value selected shown yield lowest error value 
changes 
training set procedure 
second weight updates performed sequence complete training set 
third learning procedure applied range values learning rate parameter fourth bias terminations components weight vector initially set 
results representative values shown 
surprisingly thevalue signi cant ect performance best results obtained intermediate values 
values widrow ho td procedure produced worst estimates 
td methods performed better absolute terms wider range values supervised learning method 
plots best error level achieved value value best value 
repeated presentation experiment values superior case 
experiment best value near 
reason optimal problem td relatively slow propagating prediction levels back sequence 
example suppose states start prediction value sequence xd xe xf experienced 
td change prediction procedures change decreasing extents 
temporal difference learning sequence repeatedly handicap change works back additional step presentation single presentation means slower learning 
handicap avoided working backwards sequences 
example sequence xd xe xf rst prediction updated light prediction updated new level 
way ect propagated back tothe sequence single presentation 
drawback technique loses implementation advantages td methods 
changes prediction sequence rst incremental implementation 
issue learning done ine existing database working backward way produce best predictions 

theory temporal di erence methods section provide theoretical foundation temporal di erence methods 
foundation particularly needed methods learning done basis previously learned quantities 
bootstrapping way may td methods cient di cult analyze con dence 
fact hitherto td method proved stable convergent correct predictions 
theory developed concerns linear td procedure class tasks typi ed random walk example discussed preceding section 
major results asymptotic convergence theorem linear td new data sequences theorem linear td converges repeated presentations optimal maximum likelihood estimates 
discuss td methods viewed gradient descent procedures 
convergence linear td theory data sequences generated absorbing markov processes random walk process discussed preceding section 
processes state depends current state formally simplest dynamical systems 
de ned set terminal states set nonterminal states set transition probabilities pij pij probability transition state state process state absorbing property means inde nite cycles nonterminal states possible sequences set zero probability eventually terminate 
initial state absorbing markov process provides way generating state sequence qm qm assume initial state chosen probabilistically nonterminal states witten sketch proof td procedure predicted discounted costs markov decision problem steps left appears theorem proposed true 
sutton probability random walk example give learning algorithms direct knowledge state sequence related observation outcome sequence xm observation vector xt chosen dependent corresponding nonterminal state qt andthe scalar outcome chosen dependent terminal state qm follows assume speci observation vector xi corresponding nonterminal state qt xt xi nonterminal state assume outcomes selected arbitrary probability distribution expected value zj rst step formal understanding learning procedure prove converges asymptotically correct behavior experience 
desired behavior case map nonterminal state observation vector xi true expected value outcome state sequence starting want predictions xi equal fz ig call ideal predictions 
complete knowledge markov process computed follows fz ig pij matrix ij denote ij th component vector denote th component 
denote matrix entries ij pij denote vector components write equation fz ig pij pjk second equality existence limit inverse assured theorem 
theorem applied elements probabilities going nonterminal state steps absorbing markov process probabilities converge 
set observation vectors xi linearly independent chosen small known predictions widrow ho rule converge expected value ideal predictions see widrow stearns 
prove result linear td theorem absorbing markov chain distribution starting probabilities outcome distributions nite expected values zj linearly independent set observation vectors xi positive initial weight vector predictions linear td weight updates sequence converge simplify presentation proofs straightforward potentially distracting steps placed appendix separate theorems 
referred text theorems 
temporal difference learning expected value ideal predictions 
wn denotes weight vector sequences experienced limn wn fz ig proof linear td updates wn sequence follows denotes number observation vectors sequence wn wn mx wn wn pt pt pm pt pt pm def observation vector corresponding state qt entered time sequence 
equation groups weight increments time occurrence sequence 
increment corresponds particular state transition alternatively group source destination states transitions wn wn ij wn xj xi xi ij xi xi ij denotes number times transition occurs sequence 
ij 
random processes generating state transitions outcomes independent take expected value yielding wn wn xj xi xi zj xi xi di expected number times markov chain state sequence expected value ij absorbing markov chain see kemeny snell di di strictly positive state di probability visited discarded 
wn denote expected value wn dependence wn linear write wn wn wn xj xi xi zj xi xi sutton iterative update formula wn depends initial conditions 
rearrange terms convert matrix vector notation letting denote diagonal matrix diagonal entries ii di denote matrix columns xi wn wn wn wn xd qx wn wn xj xi xj xi wn wn xd qx wn wn xd wn xd xd wn xd xd assuming moment limn xd theorem sequence fx converges lim xt wn xd lim xt wn desired result 
note exist diagonal positive diagonal entries exist theorem 
remains show limn xd 
rst showing positive de nite xd full set eigenvalues real parts positive 
enable show chosen eigenvalues xd modulus assures powers converge 
pij temporal difference learning positive de nite applying lemma see varga proof lemma real symmetric strictly diagonally dominant matrix positive diagonal entries positive de nite 
apply lemma directly symmetric 
theorem matrix positive de nite exactly symmetric matrix positive de nite prove positive de nite applying lemma clearly real symmetric remains show positive diagonal entries strictly diagonally dominant 
note ij ik kj ii ij di ij fact times 
diagonal entries positive ii ii ii ii di ii di pii furthermore diagonal entries non positive ij ij ij di ij dj ji 
strictly diagonally dominant iij ijj forall strict inequality holding ii ij need show ii ij words ij directly shown ij di ij ij di ij dj ji di di ij pij pij furthermore strict inequality hold strictly positive strictly diagonally dominant lemma applies proving positive de nite 
matrix positive de nite ay real vectors 
sutton xd full set eigenvalues real parts positive 
set eigenvalues clearly full matrix nonsingular product matrices established nonsingular 
pair 
bi xz 
xd xz xz xz denotes conjugate transpose 
implies re re xz xz xz xz re left side xz xz strictly positive real part furthermore eigenvector xt xd xt xd eigenvectors xt xd form positive real part 
bi chosen modulus criterial value di erent di erent choose smallest value 
positive eigenvalues xd xt modulus 
immediately implies see varga limn xd xt completing proof 
just shown expected values predictions linear td converge ideal predictions data sequences generated absorbing markov processes 
course just widrow ho procedure predictions converge continue vary expected values experience 
case widrow ho procedure known asymptotic variance predictions nite arbitrarily small learning rate parameter fur thermore reduced appropriate schedule variance converges zero 
conjecture stronger forms modulus complex number bi temporal difference learning convergence hold linear td remains open question 
open question convergence linear td 
wenow know td td widrow ho rule converge mean ideal predictions conjecture intermediate td procedures 

optimality learning rate result obtained previous subsection assures td methods supervised learning methods converge asymptotically ideal estimates data sequences generated absorbing markov processes 
kinds procedure converge result gets faster 
words kind procedure better predictions nite nite amount experience 
despite previously noted empirical results showing faster learning td methods proved general case 
subsection related formal result helps explain empirical result faster learning td methods 
show predictions linear td optimal important sense repeatedly nite training sets 
rst de ne optimal predictions nite training sets 
optimal predictions extremely expensive td supervised learning methods compute directly 
td methods special relationship 
common training process nite amount data learning process converges see ackley hinton sejnowski rumelhart hinton williams 
prove linear td converges repeated presentations training paradigm optimal predictions supervised learning procedures converge suboptimal predictions 
result helps explain td methods empirically faster learning rates 
stepping better nal result sense better rst step 
word optimal misleading suggests agreed criterion best way doing 
fact kinds optimality choosing critical decision 
suppose observes training set consisting nite number observation outcome sequences knows sequences generated absorbing markov process described previous section 
mean best predictions training set 
priori distribution possible markov processes known predictions optimal mean square sense calculated rule 
unfortunately di cult justify priori assumptions possible markov processes 
order avoid making assumptions mathematicians developed kind optimal estimate known maximum likelihood estimate 
kind optimality concerned 
example suppose ips coin times gets heads 
best estimate probability getting head toss 
sense best estimate depends entirely priori assumptions run fair biased coins sutton uniquely determined 
hand best answer maximumlikelihood sense requires assumptions simply general maximum likelihood estimate process produced set data process probability producing data largest 
maximum likelihood estimate prediction problem 
observation vectors xi nonterminal state distinct enumerate nonterminal states appearing training set ectively know state process time 
terminal states produce observation vectors outcomes possible tell sequences terminal state assume sequences terminate di erent states 
denote sets terminal nonterminal states respectively training set 
ij pij fraction times state entered transition occurred state outcome sequence termination occurred state maximum likelihood estimates true process parameters estimate expected value outcome process state choose estimate ideal fact maximum likelihood estimate underlying process exactly correct 
call estimates optimal predictions 
note estimated quantity corresponds absorbing markov chain 
limn theorem applies assuring existence limit inverse equation 
procedure outlined serves de nition optimal performance note impractical implement 
relies heavily observation vectors xi distinct assumption map states 
secondly procedure involves keeping statistics pair states pij state component observation vector 
number states procedure requires memory learning procedures require memory 
addition right side re computed time additional data available new estimates needed 
procedure may require computation time step compared supervised learning td methods 
consider case observation vectors linearly independent training set repeatedly weights updated complete presentation training set 
case widrow ho procedure alternatively wemay assume terminal state distribution sequence outcome depends penultimate state 
change analysis 
temporal difference learning converges minimize rms error predictions actual outcomes training set widrow stearns 
illustrated earlier random walk example linear td converges di erent set predictions 
show predictions fact optimal predictions maximum likelihood sense discussed 
prove theorem theorem training set observation vectors xi linearly independent positive initial weight vector predictions linear td converge repeated presentations training set weight updates complete presentation optimal predictions 
wn value weight vector training set times limn wn proof proof theorem theorem highlight di erences 
linear td updates wn presentation training set wn wn ms rwp ms number observation vectors sth sequence training set tth prediction sth sequence ms de ned outcome sth sequence 
ij number times transition appears training set sums wn wn ij wn xj xi xi ij zj xi xi wn di pij wn xj xi xi di pij zj xi xi di number times state appears training set 
rest proof theorem starting carries estimates substituting actual values 
step proof requires additional support show holds number sequences training set state note ij di pij number times state appears training set destination transition 
occurrences state destination transition state sequence dj di pij converting matrix notation yields desired algebraic manipulations 
just shown linear td repeatedly nite training set converges optimal estimates 
widrow ho rule hand converges estimates minimize error training sutton set saw random walk example general di erent optimal estimates 
td converges better set estimates repeated presentations helps explain learn better estimates single presentation prove 
needed characterization learning rate td methods compared available supervised learning methods 
temporal di erence methods gradient descent statistical learning methods td methods viewed gradient descent hillclimbing space modi able parameters weights 
goal viewed minimizing error measure space weights repeatedly incrementing weight vector approximation direction decreases steeply 
denoting approximation direction steepest descent gradient methods typically written wt wt positive constant determining step size 
step prediction problem pt xt fz natural error measure expected value square di erence quantities ex fz xg denotes expectation operator observation vectors measures error weight vector averaged observation vectors time step usually obtains additional information single observation vector 
usual step de ne observation error measure property 
multi step prediction problem fz xg time step weight increments determined xt relying fact ect equation wt approximated steps small wt xt fz xt rwp xt quantity fz directly known estimated 
done gets supervised learning method td method 
fz approximated outcome occurs temporal difference learning xt get classical supervised learning procedure 
alternatively fz approximated xt immediately prediction get extreme td method td 
key analysis recognition de nition real goal prediction match expected value subsequent outcome actual outcome occurring training set 
td methods perform better methods actual outcome sequence best estimate expected value 

generalizations td article chosen analyze particularly simple cases erence methods 
clari ed operation possible prove theorems 
realistic problems may require complex td methods 
section brie explore ways simple methods extended 
explicitly noted theorems earlier strictly apply extensions 
predicting cumulative outcomes temporal di erence methods limited predicting nal outcome sequence predict quantity accumulates sequence 
step sequence may incur cost wish predict expected total cost sequence 
common way arise costs elapsed time 
example bounded random walk want predict steps taken termination pole balancing problem may want predict time failure balancing packet switched telecommunications network may want predict total delay sending packet 
game playing points may game predicting expected net gain loss 
examples quantity predicted cumulative sum anumber parts parts known sequence evolves 
convenience continue refer parts costs minimization goal applications 
problems natural observation vector received step predict total cumulative cost step total cost sequence 
pt predict remaining cumulative cost th observation cost sequence 
cost preceding portion sequence known total sequence cost estimated sum known cost far estimated cost remaining cf 
algorithm dynamic programming 
procedures earlier easily generalized include case predicting cumulative outcomes 
ct denote actual cost incurred times cij denote expected value cost incurred transition state state wewould pt equal expected value zt pm ck number observation vectors sutton sequence 
prediction error represented terms temporal di erences zt pt pm ck pt pm ck pk pk ne pm steps derive td family procedures de ned derive td family de ned wt ct pt pt tx theorems earlier article carry cumulative outcome case obvious modi cations 
example ideal prediction statei expected value cumulative sum costs xt pij pjk pij vector components holds case 
steps similar proof theorem show linear cumulative td expected value weight vector sequences experienced wn wn cij xj xi xi cij xi xi wn wn xj xi xj xi rest proof theorem follows unchanged 
intra sequence weight updating far concentrated td procedures vector updated presentation complete sequence training set 
observation sequence generates increment vector respects simpler update weight vector immediately observation 
fact previously studied td methods operated fully incremental way 
pij temporal difference learning extending td intra sequence updating requires bit care 
obvious extension wt wt pt pt tx pt def xt wt changed sequence temporal changes prediction sequence de ned procedure due changes changes probably undesirable feature extreme cases may lead instability 
update rule ensures changes prediction due ective causing weight alterations wt wt xt wt xt wt tx rwp xk wt re nement samuel checker player adaptive heuristic critic sutton holland bucket brigade system described barto sutton anderson 
prediction consider problem making prediction particular xed amount time 
example suppose interested predicting rain monday predict rain monday predict rain tuesday day week 
problem involves sequence predictions td methods directly applied prediction di erent event clear desired relationship 
order apply td methods problem embedded larger family prediction problems 
wemust form estimate probability rain days estimate probability rain days 
provide overlapping sequences inter related predictions event case rain day 
ifthe predictions accurate de ned actual outcome time rains doesn rain 
update rule weight vector compute tx rwp illustrated key steps constructing td method particular problem 
embed problem interest larger class sutton problems necessary order produce appropriate sequence predictions 
second write recursive equations expressing desired relationship predictions di erent times sequence 
simplest cases article concerned just pt pt cumulative outcome case pt pt ct third construct update rule uses mismatch recursive equations drive weight changes better match 
steps similar taken formulating dynamic programming problem 

related research temporal di erence methods previously identi ed studied view previous machine learning research having 
section brie review previous light ideas developed 
samuel checker playing program earliest known td method samuel celebrated checker playing program 
learning generalization procedure modi ed parameters function evaluate board positions 
evaluation position thought estimate prediction game eventually turn starting position 
sequence positions actual game anticipated continuation naturally gave rise sequence predictions game nal outcome 
samuel learning procedure di erence evaluations pair successive positions occurring game error alter prediction associated rst position pair prediction associated second 
predictions positions computed di erent ways 
versions program prediction rst position simply result applying current evaluation function position 
prediction second position backed minimax score lookahead search started position current evaluation function 
samuel referred di erence predictions delta 
updating procedure complicated td intent delta pt pt linear td 
samuel learning procedure signi cantly di ered td methods discussed treatment nal step sequence 
considered sequence de nite externally supplied outcome victory defeat 
prediction position sequence altered match nal outcome 
samuel procedure hand position de nite priori evaluation evaluation position sequence explicitly altered 
procedures constrained evaluations predictions non terminal positions match follow samuel provided additional constraint temporal difference learning terminal positions 
pointed useless evaluation functions satisfy just rst constraint function constant positions 
discourage learning procedure nding useless evaluation functions samuel included evaluation function non modi able term measuring pieces program opponent 
modi cation may decreased likelihood nding useless evaluation functions prohibit 
example constant function attained setting modi able terms cancel ect non modi able 
samuel learning procedure constrained nd useful evaluation functions possible worse experience 
fact samuel reported observing extensive self play training sessions 
way get program improving set weight largest absolute value back zero 
drastic intervention program local optima possibility program evaluation functions changed little little winning losing game 
samuel learning procedure successful played important role signi cantly improving play playing program human checker masters 
christensen korf investigated simpli cation samuel procedure constrain evaluations terminal positions obtained promising preliminary results christensen christensen korf 
terminal constraint may critical temporal di erence theory apparently strictly necessary obtain performance 
backpropagation connectionist networks backpropagation technique rumelhart hinton williams exciting developments incremental learning methods 
technique extends widrow ho rule applied interior hidden units multi layer connectionist networks 
backpropagation network input output functions units deterministic di erentiable 
result partial derivatives error measure respect connection weight de ned apply gradient descent approach original widrow ho rule 
term backpropagation refers way partial derivatives ciently computed propagating sweep network 
rumelhart backpropagation explicitly supervised learning procedure 
purpose backpropagation td methods accurate 
backpropagation decides part network change uence network output reduce error td methods decide output temporal sequence outputs changed 
backpropagation addresses structural credit assignment issue td methods address temporal credit assignment issue 
sutton currently backpropagation td methods address di erent parts credit assignment problem important note perfectly compatible easily combined 
article emphasized linear case td methods equally applicable predictions formed nonlinear functions style networks 
key requirement gradient computable 
linear system just xt di erentiable nonlinear elements computed backpropagation process 
example anderson implemented combination backpropagation temporal di erence method adaptive heuristic critic see successfully applying nonlinear balancing task towers hanoi problem 
holland bucket brigade holland bucket brigade technique learning sequences rule invocations kind adaptive production system called classi er system 
production rules classi er system compete active right hand sides called messages memory data structure called message list 
con ict resolution carried competitive auction 
rule matches current contents message list bid depends product speci city strength modi able numerical parameter 
highest bidders active post messages new message list round auction 
bucket brigade process adjusts strengths rules determines rules active times 
rule active loses strength amount bid gains strength message posts triggers rules active round auction 
strength gained exactly bids rules 
rules post message bids responders pooled divided equally posting rules 
principle long learned way strength passed back rule rule name bucket brigade 
chain stable nal rule ect environment achieve goal receive new strength form payo external environment 
temporal di erence methods bucket brigade borrow key idea samuel steps sequence evaluated adjusted immediate near immediate successors nal outcome 
similarity td methods bucket brigade seen detailed level considering ect bucket brigade isolated linear chain rule invocations 
rule strength thought prediction payo ultimately obtained environment 
assuming equal speci cities strength rule experiences net change dependent di erence strength strength succeeding rule 
td bucket brigade updates strength prediction sequence strengths predictions immediately temporal di erence strength prediction 
temporal difference learning numerous di erences bucket brigade td methods 
important bucket brigade assigns credit rules caused rules active td methods assign credit solely temporal succession 
bucket brigade performs temporal structural credit assignment single mechanism 
contrasts td backpropagation combination discussed preceding subsection uses separate mechanisms kind credit assignment 
relative advantages approaches determined 
nite discounted predictions adaptive heuristic critic prediction problems considered far de nite outcomes 
point time actual outcome corresponding prediction known 
supervised learning methods require property learning changes actual outcome known problems completely known 
example suppose wish predict total return investing stock companies goes business total return fully determined 
problem de nition goes business earns income year total return nite 
reasons sort nite horizon prediction problems usually include form discounting 
example process generates costs ct transition pt predict discounted sum zt ct discount rate parameter determines extent concerned short range long range prediction 
pt equal zt recursive equations de ning desired relationship temporally successive predictions 
predictions accurate write pt ct ct ct pt ct mismatch td error di erence sides equation ct pt pt sutton adaptive heuristic critic uses error witten rst proposed updating predictions discounted sum discrepancy sort 
sutton learning rule identical td wt ct pt pt tx pt linear form xt xt adaptive heuristic critic probably best understood linear td method predicting discounted cumulative outcomes 

analyses experiments suggest td methods may learning methods choice real world learning problems 
argued problems involve temporal sequences observations predictions 
conventional supervised learning approaches disregard temporal structure td methods specially tailored 
result computed incrementally require signi cantly memory peak computation 
td method exactly predictions learning changes supervised learning method retaining computational advantages 
td method di erent learning changes proved converge asymptotically correct predictions 
empirically td methods appear learn faster supervised learning methods td method proved optimal predictions nite training sets repeatedly 
td methods appear computationally cheaper learn faster conventional approaches prediction learning 
progress due primarily treating td methods general methods learning predict specialized methods learning evaluation functions previous 
simpli cation theory easier greatly broadens range applicability 
clear td methods pattern recognition problem data gathered time example speech recognition process monitoring target identi cation market trend prediction 
potentially bene vis vis supervised learning methods 
speech recognition example current learning methods applied correct classi cation word known 
means critical information waveform itwas processed stored credit assignment 
learning proceeded simultaneously processing td methods storage avoided making practical consider far features combinations features 
general prediction learning methods td methods applied classic problem learning internal model world 
mean having model ability predict current actions observations 
prediction problem multi step external world modeled causal dynamical system td methods applicable advantageous 
sutton sutton barto temporal difference learning begun pursue approach lines td methods recurrent connectionist networks 
animals face problem learning internal models world 
learning process perform function animals called classical conditioning 
example dog repeatedly sound bell fed learn predict meal just bell evidenced bell 
detailed features learning process suggest animals may td method graham sutton barto 
author acknowledges especially assistance andy barto martha steenstrup chuck anderson john moore harry klopf 
oliver selfridge pat langley ron rivest mike john aspinall gene bud frawley jonathan bachrach mike seymour steve epstein jim les ron williams marie 
early stages research supported afosr contract 
ackley hinton sejnowski 

learning algorithm boltzmann machines 
cognitive science 
anderson 

learning problem solving multilayer connectionist systems 
doctoral dissertation department computer information science university massachusetts amherst 
anderson 

strategy learning multilayer connectionist representations 
proceedings fourth international workshop machine learning pp 

irvine ca morgan kaufmann 
barto 

learning statistical cooperation self interested neuronlike computing elements 
human neurobiology 
barto sutton anderson 

neuronlike elements solve di cult learning control problems 
ieee transactions systems man cybernetics 
booker 

intelligent behavior adaptation task environment 
doctoral dissertation department computer communication sciences university ann arbor 
christensen 

learning static evaluation functions linear regression 
mitchell carbonell michalski eds machine learning guide current research 
boston kluwer academic 


dynamic programming models applications 
englewood cli nj prentice hall 
sutton dietterich michalski 

learning predict sequences 
michalski carbonell mitchell eds machine learning arti cial intelligence approach volume ii los altos ca morgan kaufmann 
hop eld tank 

logic learning 
ed model neural networks behavior 
newyork plenum press 


neural model adaptive behavior 
doctoral dissertation department information computer science university california irvine 


disjunctive models boolean category learning 
biological cybernetics 
holland 

escaping brittleness possibilities general purpose learning algorithms applied parallel rule systems 
michalski carbonell mitchell eds machine learning arti cial intelligence approach volume ii los altos ca morgan kaufmann 
graham 

temporal primacy overrides prior training serial compound conditioning rabbit membrane response 
animal learning behavior 
kemeny snell 

finite markov chains 
new york springer verlag 
klopf 

neuronal model classical conditioning air force wright aeronautical laboratories technical report 
wright patterson afb oh 
moore berthier sutton barto 

simulation classically conditioned membrane response neuron adaptive element response topography neuronal ring intervals 
behavioral brain research 
rumelhart hinton williams 

learning internal representations error propagation institute cognitive science technical report 
la jolla ca university california san diego 
mcclelland eds parallel distributed processing explorations microstructure cognition volume foundations 
cambridge ma mit press 
samuel 

studies machine learning game checkers 
ibm journal research development 
reprinted feigenbaum feldman eds computers thought 
new york mcgraw hill 
sutton 

temporal credit assignment reinforcement learning 
doctoral dissertation department computer information science university massachusetts amherst 
sutton barto 

modern theory adaptive networks expectation prediction 
psychological review 
temporal difference learning sutton barto 

adaptive network constructs uses internal model environment 
cognition brain theory quarterly 
sutton barto 

temporal di erence model classical conditioning 
proceedings ninth annual conference cognitive science society pp 

seattle wa lawrence erlbaum 
sutton 

learning world models connectionist networks 
proceedings seventh annual conference cognitive science society pp 

irvine ca lawrence erlbaum 
varga 

matrix iterative analysis 
englewood cli nj prentice hall 
widrow ho 

adaptive switching circuits 
convention record part iv pp 

widrow stearns 

adaptive signal processing 
englewood cli nj prentice hall 
williams 

reinforcement learning connectionist networks mathematical analysis institute cognitive science technical report 
la jolla ca university california san diego 
witten 

adaptive optimal controller discrete time markov environments 
information control 
sutton appendix theorems theorem limn inverse proof see kemeny snell 
theorem matrix linearly independent columns nonsingular 
proof singular exist vector ay ay ay ay imply ay contradicting assumptions linearly independent columns 
theorem square matrix positive de nite positive de nite 
proof ay yt yt yt second term number equals inverse 
ay yt implying ay sign positive de nite positive de nite 
document digitally version original article 
numerous small di erences typesetting line breaks small changes wording 
gures 
includes left original published errata volume 
original printed twice place 
