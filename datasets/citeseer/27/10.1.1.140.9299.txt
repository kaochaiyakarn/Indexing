advanced hybrid mpi openmp parallelization paradigms nested loop algorithms clusters smps national technical university athens school electrical computer engineering computing systems laboratory campus athens greece mail cslab ece ntua gr 
parallelization process nested loop algorithms popular multi level parallel architectures clusters smps trivial issue existence data dependencies algorithm impose severe restrictions task decomposition applied 
propose techniques parallelization algorithms pure mpi parallelization fine grain hybrid mpi openmp parallelization coarse grain mpi openmp parallelization 
apply advanced hyperplane scheduling scheme enables pipelined execution overlapping communication useful computation leading full cpu utilization 
implement variations perform number micro kernel benchmarks verify intuition hybrid programming model potentially exploit characteristics smp cluster efficiently pure messagepassing programming model 
conclude performance model application hardware dependent propose directions efficiency improvement hybrid model 
clusters smps emerged dominant high performance computing platform 
platforms active research concern traditional parallelization programming paradigms may deliver optimal performance pure messagepassing parallel applications fail take consideration level smp cluster architecture 
intuitively parallel paradigm uses memory access intra node communication message passing inter node communication match better characteristics smp cluster 
mpi de facto message passing api openmp grown popular multi threading library important scientific addresses hybrid mpi openmp programming model 
hybrid model applied real scientific applications 
lot important scientific complexity aspects affect performance hybrid programs 
need multi threading mpi implementation efficiently support hybrid model spotted research community 
hybrid openmp mpi programming paradigm addresses fine grain parallelization usually incremental parallelization computationally intensive code parts openmp sharing constructs 
hand programming paradigms allow parallelization distribution entire smp cluster beneficial case nested loop algorithms 
pure mpi code generic apply case aforementioned take account particular architectural features smp cluster 
propose hybrid mpi openmp programming paradigms efficient parallelization perfectly nested loop algorithms fine grain model coarse grain 
apply advanced pipelined hyperplane scheduling allows minimal completion time 
conduct experiments order verify actual performance model 
rest organized follows section briefly presents algorithmic model target architecture 
section refers pure mpi parallelization paradigm section describes variations hybrid parallelization paradigm adopted pipelined hyperplane schedule 
section analyzes experimental results obtained adi micro kernel benchmark section summarizes proposes 
algorithmic model target architecture model concerns dimensional perfectly nested loops uniform data dependencies form 
computation endfor 
endfor endfor loop computation calculation involving dimensional matrix indexed 

ease analysis follow assume deal rectangular iteration spaces loop bounds constant 
assume loop computation imposes arbitrary constant flow dependencies calculation loop instance may require values certain elements matrix computed previous iterations 
target architecture concerns smp cluster pcs 
adopt generic approach assume cluster nodes threads execution node 
obviously smp cluster architecture probably select number execution threads match number available cpus node approach considers sake generality number nodes number execution threads node user defined parameters 
pure mpi paradigm pure mpi parallelization tiling transformation 
tiling popular loop transformation achieve coarse grain parallelism multi processors enhance data locality uni processors 
tiling partitions original iteration space atomic units execution called tiles 
mpi node assumes execution sequence tiles successive longest dimension original iteration space 
complete methodology described extensively 
noted prime objective experimentally verify performance benefits different parallelization models sake simplicity resorted hand parallelization opposed automatic parallelization 
parallelization models automatically generated minimal compilation time overhead reflects automatic parallelization method pure mpi model easily applied hybrid model 
furthermore advanced pipelined scheduling scheme adopted follows time step mpi node concurrently computes tile receives data required computation tile sends data computed previous tile 
true overlapping computation communication theoretically implied scheme non blocking mpi communication primitives dma support assumed 
unfortunately mpich implementation ch adi device support advanced dma driven non blocking communication limitations hold hybrid model affect performance comparison 
identify tile identify mpi node cartesian coordinates denote tile size 
control index original loop holds 
core pure mpi code resembles 
buf pack snd buf mpi snd buf dest mpi recv buf src compute mpi unpack recv hybrid mpi openmp paradigm hybrid mpi openmp programming model intuitively matches characteristics smp cluster allows level communication pattern distinguishes intra inter node communication 
specifically intra node communication implemented common access node memory appropriate synchronization ensure data calculated execution order initial algorithm preserved thread level synchronization 
inversely inter node communication achieved message passing implicitly enforces node level synchronization ensure valid execution order far different nodes concerned 
main variations hybrid model fine grain hybrid model coarse grain 
fine grain model computationally intensive parts pure mpi code usually incrementally parallelized openmp sharing directives 
coarse grain model threads spawned close creation mpi processes thread ids enforce spmd structure hybrid program similar structure pure mpi code 
hybrid models implement advanced hyperplane scheduling allows minimal completion time 
hyperplane scheduling variations hybrid model subject subsections 
hyperplane scheduling proposed hyperplane scheduling distributes tiles assigned threads specific node groups concurrently executed 
group contains tiles safely executed parallel equal number threads violating data dependencies initial algorithm 
way group thought distinct time step node execution sequence determines threads node executing tile time step ones remain idle 
scheduling aims minimizing total number execution steps required completion hybrid algorithms 
hybrid model group identified dimensional vector vector coordinates identify particular mpi node group refers coordinate thought current time step implicitly determine thread computing time step tile 
formally group denoted dimensional corresponding node determined coordinates tile executed thread obtained holds holds number threads th dimension 
value establish thread compute group calculated tile valid execute tile time step 
opposite case remain idle wait time step 
hyperplane scheduling implemented openmp pseudo code scheme pragma omp parallel num threads compute pragma omp barrier hyperplane scheduling extensively analyzed 
fine grain parallelization pseudo code fine grain hybrid parallelization depicted table 
fine grain hybrid implementation applies openmp parallel sharing construct tile computation pure mpi code 
hyperplane scheduling described subsection time step corresponding group instance required threads needed tile computations spawned 
inter node communication occurs outside openmp parallel region 
note hyperplane scheduling ensures calculations concurrently executed different threads violate execution order original algorithm 
required barrier thread synchronization implicitly enforced exiting construct 
note fine grain approach overhead re spawning threads time step pipelined schedule 
coarse grain parallelization pseudo code coarse grain hybrid parallelization depicted table 
threads spawned ids determine flow execution spmd code 
inter node communication occurs openmp parallel region completely assumed master thread means openmp master directive 
reason mpich implementation provides best mpi thread level thread safety allowing master thread call mpi routines 
intra node synchronization threads achieved aid directive 
noted coarse grain model compared fine grain compensates relatively higher programming complexity fact threads created respective overhead fine grain model diminished 
furthermore communication entirely assumed master thread threads able perform computation time spawned fine grain model 
naturally thread safe mpi implementation allow efficient communication scheme threads able call mpi routines 
alternatively non thread safe environment sophisticated load balancing scheme compensate master communication appropriately balanced computation distribution considered 
experimental results order evaluate actual performance different programming paradigms parallelized alternating direction implicit adi integration micro kernel run experiments different iteration spaces tile grains 
platform node dual smp cluster 
node pentium iii cpus mhz mb ram kb cache runs linux kernel 
intel icc compiler version linux optimization flags static 
mpi implementation mpich configured options device ch shared 
performed series experiments order evaluate relative performance parallelization methods 
specifically case mpi processes mpi model process smp node mpi processes openmp threads hybrid models 
second case started pure mpi program mpi processes smp node mpi processes openmp threads hybrid models 
cases run adi micro kernel various iteration spaces variable tile heights order obtain minimum completion time 
naturally experimental results depend largely micro kernel benchmark communication pattern shared memory parallelism achieved openmp hardware characteristics platform cpu memory network 
experimental results graphically displayed 
easily drawn pure mpi model cases fastest hand coarse grain hybrid model better finegrain 
vs series experiments performance coarse grain hybrid model comparable pure mpi fact delivers best results iteration space 
order explain differences performance models conducted thorough profiling computation communication times fig 
computation times clearly indicate pure mpi model achieves efficient parallelization computational part fine grain model worse coarse grain regards time spent computational part parallel code 
communication times follow irregular pattern average indicate superior performance pure mpi model 
time sec computation time iteration space mpi nodes fine grain hybrid nodes threads coarse grain hybrid nodes threads tile height time sec communication time iteration space mpi nodes fine grain hybrid nodes threads coarse grain hybrid nodes threads fig 

computation vs communication profiling iteration space tile height advantage coarse grain model compared fine grain lies model threads spawned second threads need re spawned time step 
additional overhead accounts higher computation times fine grain model experimentally verified 
coarse grain model suffers serious disadvantage inefficient communication pattern master thread node assumes communication mpi node pure mpi model 
disadvantage diminished efficient load balancing computation distribution scheme thread safe mpi implementation allow threads call mpi routines 
alternative parallel programming paradigms nested loop algorithms clusters smps 
implemented variations tested performance adi integration micro kernel benchmark 
turns performance hybrid coarse grain openmp mpi model looks quite promising class algorithms performance paradigm clearly depends number factors application 
intend investigate behavior paradigms closely extensive communication vs computation profiling apply efficient computation distribution order mitigate communication restrictions imposed non thread safe mpi implementation 


pipelined scheduling tiled nested loops clusters smps memory mapped network interfaces 
proceedings acm ieee conference supercomputing baltimore maryland usa 
ieee computer society press 


mpi versus mpi openmp ibm sp nas benchmarks 
proceedings acm ieee conference supercomputing dallas texas usa 
ieee computer society press 

dong em 

dual level parallelism deterministic stochastic cfd problems 
proceedings acm ieee conference supercomputing baltimore maryland usa 
ieee computer society press 


automatic code generation executing tiled nested loops parallel architectures 
proceedings acm symposium applied computing sac madrid mar 


compiling tiled iteration spaces clusters 
proceedings ieee international conference cluster computing chicago sep 

ding 
mpi openmp paradigms cluster smp architectures tracking algorithm multi dimensional array transposition 
proceedings acm ieee conference supercomputing baltimore maryland usa 
ieee computer society press 

george em 
robert kirby 
parallel scientific computing mpi seamless approach parallel algorithms implementation 
cambridge university press 


performance comparison mpi openmp programming styles shared memory multiprocessors 
acm spaa san diego usa jun 

skjellum 
multi threaded message passing interface mpi architecture performance program issues 



communication optimization aspects parallel programming models hybrid architectures 
international journal high performance computing applications 

tang yang 
optimizing threaded mpi execution smp clusters 
proceedings th international conference supercomputing pages sorrento italy 
acm press 
pragma omp parallel 
time steps current node 
pack previously computed data pack snd buf 
send communication data mpi snd buf dest time steps current node receive data tile mpi recv buf src calculate candidate tile execution pragma omp parallel pragma omp master pack 
previously computed data calculate candidate tile execution current thread buf pack snd buf send communication data mpi snd buf dest execute valid tile receive data tile mpi recv buf src compute current tile compute current thread execute valid tile wait communication completion compute current tile mpi compute unpack communication data pragma omp master unpack recv buf wait communication completion mpi unpack communication data unpack recv synchronize threads time step pragma omp barrier table table 
fine grain hybrid parallelization 
coarse grain hybrid parallelization time sec time sec time sec total execution time iteration space mpi nodes fine grain hybrid nodes threads coarse grain hybrid nodes threads tile height total execution time iteration space mpi nodes fine grain hybrid nodes threads coarse grain hybrid nodes threads tile height total execution time iteration space mpi nodes fine grain hybrid nodes threads coarse grain hybrid nodes threads tile height time sec time sec total execution time iteration space mpi nodes fine grain hybrid nodes threads coarse grain hybrid nodes threads fig 

experimental results adi integration time sec tile height total execution time iteration space mpi nodes fine grain hybrid nodes threads coarse grain hybrid nodes threads tile height total execution time iteration space mpi nodes fine grain hybrid nodes threads coarse grain hybrid nodes threads tile height 
