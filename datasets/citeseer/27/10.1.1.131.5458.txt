arxiv cmp lg jun appear proceedings th annual meeting acl june empirical study smoothing techniques language modeling stanley chen harvard university aiken computation laboratory oxford st cambridge ma sfc eecs harvard edu extensive empirical comparison smoothing techniques domain language modeling including described jelinek mercer katz church gale 
investigate time factors training data size corpus brown versus wall street journal gram order bigram versus trigram affect relative performance methods measure cross entropy test data 
addition introduce novel smoothing techniques variation jelinek mercer smoothing simple linear interpolation technique outperform existing methods 
smoothing technique essential construction gram language models speech recognition bahl jelinek mercer domains church brown kernighan church gale 
language model probability distribution strings attempts reflect frequency string occurs sentence natural text 
language models speech recognition resolve acoustically ambiguous utterances 
example takes takes know ceteris paribus prefer transcription 
smoothing central issue language modeling literature lacks definitive comparison existing techniques 
previous studies nadas katz church gale mackay peto compare joshua goodman harvard university aiken computation laboratory oxford st cambridge ma goodman eecs harvard edu small number methods typically single corpus single training data size 
result currently difficult researcher intelligently choose smoothing schemes 
carry extensive empirical comparison widely smoothing techniques including described jelinek mercer katz church gale 
carry experiments training data sizes varied corpora bigram trigram models 
demonstrate relative performance techniques depends greatly training data size gram order 
example bigram models produced large training sets church gale smoothing superior performance katz smoothing performs best bigram models produced smaller data 
methods parameters tuned improve performance perform automated search optimal values show sub optimal parameter selection significantly decrease performance 
knowledge smoothing systematically investigates issues 
addition introduce novel smoothing techniques belonging class smoothing models described jelinek mercer second simple linear interpolation method 
relatively simple implement show methods yield performance bigram models superior performance trigram models 
take performance method cross entropy test data nt lt log pm ti pm ti denotes language model produced method test data composed sentences contains total nt words 
entropy inversely related average probability model assigns sentences test data generally assumed lower entropy correlates better performance applications 
smoothing gram models gram language modeling probability string expressed product probabilities words compose string word probability conditional identity words wl wi wi denotes words wi wj 
typically taken corresponding bigram trigram model respectively 
consider case 
estimate probabilities wi wi equation acquire large corpus text refer training data take pml wi wi wi wi wi wi wi ns wi ns wi wi wi denotes number times string occurs text ns denotes total number words 
called maximum likelihood ml estimate wi wi 
intuitive maximum likelihood estimate poor amount training data small compared size model built generally case language modeling 
example consider situation pair words bigram say doesn occur training data 
pml clearly inaccurate probability larger zero 
zero bigram probability lead errors speech recognition disallows bigram regardless informative acoustic signal term smoothing describes techniques adjusting maximum likelihood estimate hopefully produce accurate probabilities 
term wi wi meaningful pad string distinguished token 
assume distinguished tokens preceding sentence 
example simple smoothing technique pretend bigram occurs johnson jeffreys yielding wi wi wi wi wi vocabulary set words considered 
desirable quality preventing zero bigram probabilities 
scheme flaw assigning probability say thou assuming occurred training data intuitively word common thou 
address smoothing technique interpolate bigram model unigram model pml wi wi ns model reflects word occurs training data 
example take wi wi pml wi wi pml wi getting behavior bigrams involving common words assigned higher probabilities jelinek mercer 
previous simplest type smoothing practice additive smoothing johnson jeffreys take wi wi jeffreys advocate 
gale church argued method generally performs poorly 
turing estimate central smoothing techniques 
directly gram smoothing additive smoothing perform interpolation lower higher order models essential performance 
turing states gram occurs times treated occurred times nr nr number grams occur exactly times training data 
katz smoothing extends intuitions turing adding interpolation higherorder models lower order models 
nr lambda old bucketing number counts distribution lambda new bucketing average non zero count distribution minus values old new bucketing schemes jelinek mercer smoothing point represents single bucket widely smoothing technique speech recognition 
church gale describe smoothing method combines turing estimate bucketing technique partitioning set grams disjoint groups group characterized independently set parameters 
katz models defined recursively terms lower order models 
gram assigned buckets frequency predicted lower order models 
bucket treated separate distribution turing estimation performed giving corrected counts normalized yield probabilities 
smoothing technique katz smoothing widely speech recognition due jelinek mercer 
class smoothing models involve linear interpolation brown 
take wi pml wi wi maximum likelihood estimate interpolated smoothed lower order distribution defined analogously 
training distinct generally felicitous bahl jelinek mercer suggest partition ing buckets bucket constrained value 
yield meaningful results data esti mate need disjoint data calculate pml 
held interpolation reserves section training data purpose 
alternatively jelinek mercer describe technique called deleted interpolation differ ent parts training data rotate training results averaged 
pml smoothing techniques motivated bayesian framework including nadas mackay peto 
novel smoothing techniques great novel methods tried techniques performed especially 
method average count scheme instance jelinek mercer smoothing 
referring equation recall bahl suggest bucketing 
partitioning average number counts wi non zero element wi wi yields better results 
intuitively sparse data 
ing pml wi larger larger generally correspond sparse distributions quantity ignores allocation counts words 
example consider distribution counts distributed evenly words data estimate setting yields optimal result 
sparse distribution counts single word 
average number counts word directly express concept sparseness 
graph value assigned bucket original new bucketing schemes identical data 
notice new bucketing scheme results tighter plot indicating better grouping distributions similar behavior 
method count technique combines intuitions 
mackay peto argue reasonable form smoothed distribution wi wi wi wi parameter thought number counts added distribution new counts distributed distribution 
secondly turing estimate interpreted stating number extra counts proportional number words exactly count distribution 
works wi number words count constants 
experimental methodology data penn treebank tipster corpora distributed linguistic data consortium 
treebank extracted text tagged brown corpus yielding words 
tipster associated press ap wall street journal wsj san jose mercury news sjm data yielding words respectively 
created distinct vocabularies brown corpus tipster data 
vocabulary contains words occurring brown vocabulary consists words occurring times tipster 
experiment selected segments held data segment training data 
held segment test data performance evaluation development test data optimizing parameters smoothing method 
piece held data chosen roughly words 
decision reflect practice training data size words realistic development test data available 
decision prevent having optimize training versus held data tradeoff data size 
addition development test data optimize typically parameters practice small held sets generally adequate avoided altogether techniques deleted estimation 
smoothing implementations section discuss details implementations various smoothing techniques 
due space limitations descriptions comprehensive complete discussion chen 
titles sections include mnemonic refer implementations sections 
specified smoothing models defined recursively terms lower order models recursion distribution uniform distribution wi 
method highlight parameters tuned optimize performance 
parameter values determined training held data 
baseline smoothing interp baseline baseline smoothing method instance jelinek mercer smoothing constrain equal single value wi pml wi wi additive smoothing plus plus delta consider versions additive smoothing 
referring equation fix plus smoothing 
plus delta consider 
katz smoothing katz original katz uses single parameter different kn 
smooth unigram distribution additive smoothing parameter 
church gale smoothing church gale smooth counts nr needed turing estimate technique described gale sampson 
smooth unigram distribution turing bucketing 
bucketing scheme described original scheme analogous described bahl jelinek mercer 
assumption bucket large accurate turing estimation depends grams non zero counts occur 
partitioning space wi wi values uniform way done church gale partition space cmin non zero grams fall bucket 
original describes bigram smoothing detail extending method trigram smoothing ambiguous 
particular unclear bucket trigrams wi wi wi 
chose may yield better performance belief difficult implement requires great deal computation 
jelinek mercer smoothing interp held interp del int implemented versions jelinek mercer smoothing differing data train bucket suggested bahl similar church gale implementation choose buckets ensure cmin words data train fall bucket 
interp held trained held interpolation development test sets 
interp del int trained relaxed deleted interpolation technique described jelinek mercer word deleted time 
interp del int bucket gram count deletion turned significantly improve performance 
novel smoothing methods new avg count new count implementation new avg count corresponding smoothing method average count identical interp held novel bucketing scheme described section 
implementation new count different parameters equation results display performance interp baseline method bigram trigram models tipster brown wsj subset tipster 
figures display relative performance various smoothing techniques respect baseline method corpora measured difference entropy 
graphs left figures point represents average runs error bars represent empirical standard deviation runs 
due resource limitations performed multiple runs data sets sentences 
point graphs right represents single run consider sizes amount data available 
graphs bottom figures close ups graphs focusing algorithms perform better baseline 
give idea cross entropy differences translate perplexity bits correspond roughly change perplexity 
run noted optimal values parameters technique searched powell search algorithm realized numerical recipes press pp 

parameters chosen optimize cross entropy development test sets associated training set 
constrain search searched parameters affect performance significantly verified preliminary experiments data sizes 
katz church gale perform parameter search training sets sentences due resource constraints manually extrapolated parameter values optimal values smaller data sizes 
ran interp del int sizes sentences due time constraints 
graphs see additive smoothing performs poorly methods katz interp held consistently perform 
implementation church gale performs poorly large bigram training sets performs best 
novel methods new avg count new count perform uniformly training data sizes superior trigram models 
notice performance relatively consistent corpora varies widely respect training set size gram order 
method interp del int performs significantly worse interp held differ data train delete word time del int cross entropy test data bits token average runs size sentences sentences training data words sentence tipster bigram wsj bigram tipster trigram wsj trigram cross entropy test data bits token single run size brown bigram brown trigram wsj trigram tipster bigram wsj bigram tipster trigram sentences training data words sentence baseline cross entropy test data graph left displays averages runs training sets sentences graph right displays single runs training sets sentences difference test cross entropy baseline bits token difference test cross entropy baseline bits token average runs size sentences plus church gale plus delta katz interp held interp del int new avg count new count see sentences training data words sentence katz average runs size sentences interp del int interp held new count new avg count sentences training data words sentence difference test cross entropy baseline bits token difference test cross entropy baseline bits token single run size sentences plus plus delta church gale katz interp held interp del int new avg count new count see sentences training data words sentence single run size sentences interp held interp del int new count new avg count sentences training data words sentence trigram model tipster data relative performance various methods respect baseline graphs left display averages runs training sets sentences graphs right display single runs training sets sentences top graphs show algorithms bottom graphs zoom methods perform better baseline method katz difference test cross entropy baseline bits token difference test cross entropy baseline bits token average runs size sentences church gale plus delta plus katz interp held interp del int new avg count new count see sentences training data words sentence interp del int new avg count average runs size sentences interp held church gale new count katz sentences training data words sentence difference test cross entropy baseline bits token difference test cross entropy baseline bits token church gale single run size sentences plus plus delta katz interp held interp del int new avg count new count see sentences training data words sentence interp del int new count new avg count single run size sentences katz church gale interp held church gale sentences training data words sentence bigram model tipster data relative performance various methods respect baseline graphs left display averages runs training sets sentences graphs right display single runs training sets sentences top graphs show algorithms bottom graphs zoom methods perform better baseline method difference test cross entropy baseline bits token interp del int katz interp held bigram model new avg count new count church gale sentences training data words sentence difference test cross entropy baseline bits token trigram model interp del int new avg count sentences training data words sentence katz interp held new count bigram trigram models brown corpus relative performance various methods respect baseline difference test cross entropy baseline bits token interp del int church gale bigram model new count new avg count interp held sentences training data words sentence katz church gale difference test cross entropy baseline bits token interp del int katz trigram model interp held new count new avg count sentences training data words sentence bigram trigram models wall street journal corpus relative performance various methods respect baseline difference test cross entropy baseline bits token performance katz respect delta sent sent sent sent delta difference test cross entropy baseline bits token sent performance new avg count respect min sent sent sent minimum number counts bucket performance katz new avg count respect parameters cmin respectively hypothesize deleting larger chunks lead similar performance 
show values parameters cmin affect performance avg count respectively training data sizes 
notice poor parameter setting lead significant losses performance optimal parameter settings depend training set size 
give informal estimate difficulty implementation method table display number lines code implementation excluding core code common techniques 
implement baseline method just interp held code special case 
written anew probably lines 
method lines interp baseline plus plus delta katz church gale interp held interp del int new avg count new count table implementation difficulty various methods terms lines code katz discussion knowledge empirical comparison smoothing techniques language modeling scope study multiple training data sizes corpora performed parameter optimization 
show order completely characterize relative performance techniques necessary consider multiple training set sizes try bigram trigram models 
multiple runs performed possible discover calculated differences statistically significant 
furthermore show sub optimal parameter selection significantly affect relative performance 
find widely techniques katz smoothing jelinek mercer smoothing perform consistently training set sizes bigram trigram models katz smoothing performing better trigram models produced large training sets bigram models general 
results question generality previous result concerning katz smoothing katz reported method slightly outperforms unspecified version jelinek mercer smoothing single training set words 
furthermore show church gale smoothing previously compared common smoothing techniques outperforms existing methods bigram models produced large training sets 
find novel methods average count count superior existing methods trigram models perform bigram models method count yields marginally worse performance extremely easy implement 
study measure performance solely cross entropy test data interesting see cross entropy differences correlate performance applications speech recognition 
addition interesting see results extend fields language modeling smoothing prepositional phrase attachment collins brooks part speech tagging church stochastic parsing magerman 
authors stuart shieber anonymous reviewers comments previous versions 
william gale geoffrey sampson supplying code turing frequency esti mation tears research supported national science foundation 
iri 
cda 
second author supported national science foundation graduate student fellowship 
bahl jelinek mercer bahl frederick jelinek robert mercer 

maximum likelihood approach continuous speech recognition 
ieee transactions pattern analysis machine intelligence pami march 
brown brown peter john cocke stephen dellapietra vincent dellapietra frederick jelinek john lafferty robert mercer paul 

statistical approach machine translation 
computational linguistics june 
brown brown peter stephen dellapietra vincent dellapietra jennifer lai robert mercer 

estimate upper bound entropy english 
computational linguistics march 
chen chen stanley 
building probabilistic models natural language 
ph thesis harvard university 
preparation 
church church kenneth 

stochastic parts program noun phrase parser unrestricted text 
proceedings second conference applied natural language processing pages 
church gale church kenneth william gale 

comparison enhanced turing deleted estimation methods estimating probabilities english bigrams 
computer speech language 
collins brooks collins michael james brooks 

prepositional phrase attachment backed model 
david yarowsky kenneth church editors proceedings third workshop large corpora pages cambridge ma june 
gale church gale william kenneth church 

estimation procedures language context poor estimates worse 
proceedings computational statistics th symposium pages yugoslavia september 
gale church gale william kenneth church 

wrong adding 
de haan editors corpus research language 
amsterdam 
gale sampson gale william geoffrey sampson 

turing frequency estimation tears 
journal quantitative linguistics 
appear 

population frequencies species estimation population parameters 
biometrika 
jeffreys jeffreys 
theory probability 
clarendon press oxford second edition 
jelinek mercer jelinek frederick robert mercer 

interpolated estimation markov source parameters sparse data 
proceedings workshop pattern recognition practice amsterdam netherlands north holland may johnson johnson 
probability deductive inductive problems 
mind 
katz katz slava 
estimation probabilities sparse data language model component speech recognizer 
ieee transactions acoustics speech signal processing assp march 
kernighan church gale kernighan church gale 

spelling correction program noisy channel model 
proceedings thirteenth international conference computational linguistics pages 

note general case bayes laplace formula inductive posteriori probabilities 
transactions faculty 
mackay peto mackay david linda peto 

hierarchical dirichlet language model 
natural language engineering 
magerman magerman david 
natural language parsing statistical pattern recognition 
ph thesis stanford university february 
nadas nadas arthur 

estimation probabilities language model ibm speech recognition system 
ieee transactions acoustics speech signal processing assp august 
press press flannery teukolsky vetterling 

numerical recipes cambridge university press cambridge 
