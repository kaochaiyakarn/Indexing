survey reinforcement learning keerthi ravindran department computer science automation indian institute science bangalore mail csa ernet 
gives compact self contained tutorial survey reinforcement learning tool increasingly nding application development dynamic systems 
research reinforcement learning past decade led development variety useful algorithms 
surveys literature presents algorithms cohesive framework 
keywords 
reinforcement learning dynamic programming optimal control neural networks reinforcement learning rl term borrowed animal learning literature minsky refers class learning tasks algorithms learning system learns associative mapping maximizing scalar evaluation reinforcement performance environment user 
compared supervised learning shown environment provides learning system value rl di cult feedback environment 
time learning system tries environment immediately returns scalar reinforcement evaluation pair indicates faced immediate rl task 
di cult rl task delayed rl environment single scalar reinforcement evaluation collectively xt sequence pairs occuring time system operation 
delayed rl tasks commonly arise optimal control dynamic systems planning problems ai 
main interest solution delayed rl problems 
study immediate rl problems methods solving play useful role solution delayed rl problems 
delayed rl encompasses diverse collection ideas having roots animal learning barto sutton barto control theory bertsekas kumar ai dean wellman 
delayed rl algorithms rst employed samuel celebrated checkers 
keerthi ravindran 
navigating grid world 
publication barto sutton anderson barto delayed rl algorithm called adaptive heuristic critic application control problem pole balancing research ying start 
watkins learning algorithm watkins impact research 
number signi cant ideas rapidly emerged past years eld reached certain level maturity 
provide comprehensive tutorial survey various ideas methods delayed rl 
avoid distractions unnecessary clutter notations ideas intuitive rigorous fashion 
preparing tutorial obtained lot guidance works watkins barto sutton watkins barto bradtke singh bradtke barto 
illustrate key features delayed rl task consider simple example 
example navigating robot illustrates grid world robot navigates 
blank cell grid called state 
shaded cells represent barriers states 
state space set states 
cell marked goal state 
aim reach state 
navigation done actions fn wg actions denoting possible movements coordinate directions 
rules transition de ned follows 
suppose robot state action chosen 
resulting state state directly north state instance choosing shown gure lead system staying goal state special case 
de nition action taken goal state results transition back goal state 
general problems rules transition stochastic 
tutorial survey reinforcement learning robot moves discrete integer time points starting 
time step robot state xt de ne immediate reward xt xt ect robot penalized time step spent non goal states 
simple verify maximizing total reward time xt equivalent minimum time navigation starting state 
denote maximum achievable optimal value 
interested nding feedback policy start starting state select actions reach goal minimum number time steps 
usefulness immediate rl methods delayed rl roughly explained follows 
typical delayed rl methods maintain approximation optimal function 
action performed state state results taken approximate immediate evaluation pair 
solving immediate rl problem uses evaluation function sub optimal policy delayed rl problem 
immediate rl algorithms 
delayed rl problems harder solve immediate rl problems reason 
suppose example performance sequence actions selected policy leads robot goal 
improve policy experience goodness action performed 
total reward obtained gives cumulative ect actions performed 
scheme reasonably cumulative evaluation individual actions 
referred temporal credit assignment problem 
previous paragraph hint rl methods temporal credit assignment 
dynamic programming dp bertsekas ross known tool solving problems example 
line method requires availability complete model environment 
concerns delayed rl di erent 
see th clearly return example impose requirement robot knowledge environment way learning line experience trying various actions visiting states 
delayed rl algorithms particularly situations general format 
delayed rl algorithm initialize learning system 
repeat referred primary reinforcement 
general situations function xt action time step optimal action gives maximum value 

learning usually achieved stochastic exploration policy choosing actions 
typically exploration policy chosen totally random learning approach optimal policy learning nears completion 
keerthi ravindran 
system state choose action exploration policy apply system 

environment returns reward yields state 
experience update learning system 

set model environment isavailable advantageous avoid line method dp delayed rl algorithm 
problems state space large dp algorithm operates entire state space delayed rl algorithm operates parts state space relevant system operation 
model available delayed rl algorithms employ simulation mode operation line operation speed learning avoid doing experiments hardware 
term real time operation mean line operation simulation mode operation 
applications representing functions asv exactly infeasible 
better alternative parametric function approximators connectionist networks 
approximators suitably chosen delayed rl algorithm 
clarify take instance consider function approximator 
denotes real line denotes vector parameters approximator learnt sothat approximates 
usually step delayed rl algorithm learning system uses experience come direction changed improving performance 
step size function approximator alter new value new new example multilayer perceptrons hertz denotes set weights thresholds network updating carried backpropagation achieve 
rest denote updating process refer learning rule 
organized follows 
section discusses immediate rl 
formulate delayed rl problems mention basic results 
methods estimating total reward discussed 
methods play important role delayed rl algorithms 
dp techniques delayed rl algorithms 
section addresses various practical issues 
concluding remarks 
immediate reinforcement learning immediate rl refers learning associative mapping reinforcement evaluator 
learn learning system interacts closed loop tutorial survey reinforcement learning environment 
time step environment chooses learning system uses function approximator select action 
environment returns evaluation reinforcement ideally learning system adjust produce maximum possible value words solve parametric global optimization problem 
def maxr supervised learning popular paradigm learning associative mappings hertz 
supervised learning shown supervisor provides learning system value 
immediate rl supervised learning di er important ways 
supervised learning shown supervisor provides learning system forms directed information uses learning rule positive step size 
immediate rl directed information available employ strategy obtain information 
supervised learning learning system simply check decide correct map value formed immediate rl correctness exploring values immediate rl problems di cult solve supervised learning problems 
anumber immediate rl algorithms described literature 
stochastic learning automata algorithms narendra deal special case singleton nite set 
associative reward penalty ar algorithm barto anandan barto barto jordan extends learning automata ideas case nite set 
williams proposed class immediate rl methods interesting theoretical results 
gullapalli developed algorithms general case nite dimensional real spaces real valued 
discuss algorithms relevant useful delayed rl 
simple way solving take time global optimization algorithm complete enumeration explore space obtain correct function approximator learn pair 
idea reason 
situations immediate rl tool approximate policy delayed rl learning system little control choice learning system chooses particular sends environment evaluation environment sends reinforcement evaluation alters value 
immediate rl seeks approaches appropriate situations 
rst consider case nite set fa denote dimensional real space 
function approximator usually formed composition functions function approximator keerthi ravindran xed function rm idea set follows 
rm merits various ai values 
zk denote th component ofz 
merit vector formed max selector zk max zi come issue learning choosing 
stage input merit vector returned anda action having largest merit value 
environment returns reinforcement 
order learn need evaluate goodness goodness 
obviously existing information 
need estimator call provides estimate 

di erence isa measure goodness simple learning rule gk gk small positive step size 
learning requires members evaluated clearly max selector suitable exploration 
instance stage learning assigns largest merit wrong action say gives mistake value smaller action going generated learning system replace controlled stochastic action selector generates actions randomly learning begins approaches learning completed 
popular stochastic action selector boltzmann distribution pi def exp zi exp zj nonnegative real parameter temperature controls stochasticity action selector 
expected reinforcement action selector def jx pi stochastic action selector approaches max selector 

train approximate 
easy xed value estimated performance stochastic action selector time 
simple learning rule achieves small positive step size 
important comments regarding convergence learning rules come learning rules designed estimate expectation averaging time 
non zero large size 
instance generated distribution 
avoid comes close step size controlled properly 
value may learning begins slowly decreased learning 
tutorial survey reinforcement learning learning take place sequence values carried covers parts space possible 
course learning system control choice achieve exploration 
explore usually done 
learning done number trials 
trial consists random choice operating system time steps 
time step system learning system chooses action learns 
depending rules environment new results time step begins 
usually learning repeated multiple trials space thoroughly explored 
consider case continuous say nite dimensional real space 
idea merit values suitable 
better directly deal function approximator order exploration controlled random perturbation added form 
simple choice take gaussian zero mean having standard deviation satis es ast 
setting training reinforcement estimator case discrete 
function approximator adopt learning rule small positive step size 
problems bound available bound suitably employed guide exploration choose gullapalli 
jordan rumelhart suggested method forward models continuous action spaces 
known di erentiable function simple deterministic learning law gradient ascent update known jordan rumelhart suggest learnt line data learnt function local maxima obtained learning rule may converge 
typically serious problem 
stochastic approach discussed earlier su er local maxima problems 
add deterministic method explores systematic directions stochastic method explores random directions expected faster 
comparison similar comparison deterministic stochastic techniques continuous optimization 
delayed reinforcement learning delayed rl concerns solution stochastic optimal control problems 
section discuss basics problems 
solution methods delayed rl 
sections mainly consider problems state control spaces nite sets 
main issues solution methods delayed rl easily explained problems 
deal continuous state action spaces brie 
keerthi ravindran consider discrete time stochastic dynamic system nite set states system operation 
time agent controller observes state xt selects performs action nite set xt possible actions 
assume system markovian stationary xt ag ag def pxy policy method adopted agent actions 
objective decision task nd policy optimal de ned sense described 
general action speci ed agent policy time depend entire past history system 
restrict attention policies specify actions current state system 
deterministic policy de nes action 
stochastic policy de nes probability distribution set feasible actions gives values probf ag 
sake notations simple consider deterministic policies section 
ideas easily extended stochastic policies appropriate detailed notations 
precisely de ne optimality criterion 
state agent performs action receives immediate payo reward 
policy de ne value function follows ef xt xt jx xg rewards discounted factor 
case avoided leads di culties associated existence summation 
course di culties handled putting appropriate assumptions problem solved 
avoid unnecessary distraction go details see bradtke bertsekas tsitsiklis 
expectation understood lim ef xt xt jx xg probability particular state sequence occurs taken obvious way repeatedly employing wish maximize value function 
maxv referred optimal value function 
bounded 
number nite 
exists 
state completely observable method uses observable states retains past information see chrisman mozer whitehead ballard 
tutorial survey reinforcement learning de ne optimal policy 
denote policy achieves maximum 
collection policies 
xg 
de ned picking rst action policies 

turns achieves maximum words 

result easy see looks bellman optimality equation important equation satis es 
max pxy 
fact satis es explained follows 
term square brackets right hand side total reward get action chosen rst time step system performs optimally time steps 
clearly term exceed 
violate de nition 

term equal 

holds 
turns unique function satis es fact requires non trivial proof details ross bertsekas bertsekas tsitsiklis 
discussion yields mechanism computing known 
arg max pxy 
di culty computation system model function pxy known 
di culty function employ function called function 
set feasible state action pairs 
policy de ne pxy denotes total reward obtained rst action time steps 

bellman optimality equation get 
max 
useful rewrite bellman optimality equation 
pxy max 
compute arg max 
keerthi ravindran known computed system model 
advantage function function play crucial role deriving model free delayed rl algorithm called learning watkins 
consider examples give useful hints problem formulation 
examples commonly mentioned rl literature 
example navigating robot dynamics example robot moved cell pieces moved chess board 
true robot motions involve dynamics ects velocity acceleration need considered 
example include dynamics crude way appropriate grid world 
ht vt denote horizontal vertical coordinates cell occupied robot time ht vt denote velocities 
vector ht vt ht vt denotes system state time components integer 
goal state coordinate vector goal cell words robot come rest hmax vmax limits velocity magnitudes 
state space fx blank cell hj hmax vj include extra state called failure state denote situations barrier shaded cell entered velocity limit exceeded 
ffg accelerations horizontal vertical directions respectively actions 
keep integers assume accelerations takes integer values 
amax positive integer denotes limit magnitude accelerations 
isan admissible action ofa integer lying amax amax 
example state transitions deterministic 
de ned follows 
barrier cells velocity limits application action xt ht vt ht vt lead state ht ht vt vt ht vt denote curve grids world resulting transition ht vt time time solution di erential equations ht dh ht vt dv vt 
cuts barrier cell velocity vector say failure occured transition 
state transitions de ned xt negative acceleration mean deceleration 
xt failure occurs transition tutorial survey reinforcement learning primary aim avoid failure 
failure avoiding trajectories trajectory reaches goal state time steps possible 
aims met de ne easily checked 

exist trajectory starting avoids failure 

starting exists failure avoiding trajectory exist trajectory reaches 
starting exists failure avoiding trajectory reaches optimal policy leads generation trajectory reaches fewest number steps avoiding failure 
example playing backgammon consider game backgammon players look game perspective assuming follows xed policy 
decision move current board pattern dice roll known 
state consists board pattern dice roll pair 
action consists set marker movements 
state transition de ned follows 
markers accordance chosen action 
step deterministic results new board pattern 
rolls dice 
step stochastic 
markers policy 
step deterministic stochastic depending type policy 
rolls dice 
step stochastic 
set states correspond win set goal states reached 
de ne reward ifx goal state 
policy say value function denote probability win state 
example pole balancing deviate problem formulation example involves continuous state action spaces 
standard problem learning controllers balancing inverted pendulum pivoted trolley problem similar balancing stick hand barto 
system comprises straight horizontal track railway track carriage free move 
carriage axis perpendicular track pointing side pendulum free turn 
controller task keep keerthi ravindran 
pole balancing 
pendulum upright alternately pulling pushing carriage track 
shown gure 
say balancing failed inequalities violated hmax hmax max max hmax max speci ed bounds magnitudes aim balance failure long time possible 
state system tuple time derivatives respectively 
action force applied carriage 
takes real values interval fmax fmax simplify problem solution action space taken fmax michie chambers barto anderson 
discrete time formulation problem obtained cutting continuous time non negative real line uniform time intervals duration applied force constant interval 
state system continuous time instant xt discrete time state th time step 
mechanical dynamics system de nes state transition change failure occurs assume sake consistent problem formulation system stays failure 
example take state space ffg fx hmax hmax max maxg failure state collectively represents states constant action time step corresponding interval 
tutorial survey reinforcement learning aim avoid failure choose methods estimating delayed rl methods knowledge crucial ways optimality checked seeing satis es bellman optimality equation optimal improve elaborate details section 
section discuss detail methods estimating policy 
methods estimating similar deal brie section 
aim nd function approximator estimates material section taken works watkins sutton jaakkola 
avoid employ simplifying notations 
xed omit superscript call refer xt xt simply rt 
random variable denote random variable instance random variable 
simple approximation step truncated return understood section denote number time steps elapsed system passed state stressing point situation time actual system time time relative occurence obvious context 
rmax bound size easy verify max rmax converges uniformly su ers important drawback 
computation expectation requires complete enumeration probability tree possible states reachable time steps 
breadth tree large computations burdensome 
way problem set obtained monte carlo simulation experiments real system choice way systems model unavailable 
approximation su ers di erent drawback 
breadth probability tree grows grows approximation keerthi ravindran obtained average trials 
averaging achieved learning rule similar small step size 
learning random choice eventually number trials expect resulting satisfy 
approach approximation available 
estimate appropriate step truncated return xn xn state occurs time steps system passed state analysis justify statement 
consider ideal learning rule suppose gets modi ed vnew process satisfying 
similar easily derive max vnew max go number learning steps achieve note convergence achieved xed small value say 
hand xed learning rule guaranteed achieve bound 
system model available best choose small employ 
suppose model unavailable avoided expensive 
case suitable learning rule employs uses real time data better 
reasons 
suppose estimate small ideal mean close small variance 
small variance means lead fast averaging fast convergence hand chosen large tohave close large variance lead slow averaging 
estimate require large means 
large di erence negligible yield similar performance 
mentioned trial consists starting system random state running system number time steps 
tutorial survey reinforcement learning discussion implies better employ clear suitable value chosen dynamically goodness aid manipulation sutton suggested new estimate constructed geometrically averaging fv normalizing term 
sutton referred learning algorithm uses td 
td stands temporal di erence 
name justi ed 
expanding get fact expression may rewritten recursively state occuring time step putting gives putting gives range values obtained varying approximately achieved varying 
simple idea learning process zero progresses better estimate properly chosen signi cant computational ciency usually achieved compared simply sutton 
sutton singh developed automatic schemes doing assuming cycles state trajectories 
de nition involves appears compute 
computations involving nicely rearranged suitably approximated yield practical algorithm suited doing learning concurrently real time system operation 
consider learning rule de ne temporal di erence operator di erence predictions consecutive time steps prediction information prediction information 
name temporal di erence 
note easily computed experience time step 
simple rearrangement terms second line yields keerthi ravindran form suitable involves terms extending nite time 
way handle problem choose large accumulate xn memory truncate right hand side include rst terms apply time steps occured 
simpler approximate way achieving include ects temporal di erences occur time 
say system state time systems transits state time compute update 
system transits state time compute update 
reason altered process uses existing time small adapted slowly approximate updating method expected close 
way implementing eligibility trace state visited klopf klopf klopf barto watkins learning rule time xt xt system state time eligibility traces adapted visited xt xt important remarks regarding implementation scheme 
previous learning rules update time step updates states positive eligibility trace time step 
rule suitable connectionist implementation implementations sequential computers 
cient way keep track states visited update 
value depend small small 

rule updating eligibility traces assumes learning takes place single trial 
learning done multiple trials eligibility traces reset zero just new trial begun 
equation applies learning rules 
dayan sejnowski jaakkola shown real time td learning rule appropriate assumptions variation time converges probability 
practically learning achieved trials decreasing zero learning progresses 
far section assumed policy deterministic 
stochastic policy ideas section hold appropriate interpretations expectations include stochasticity taken instances generated stochastic policy 
tutorial survey reinforcement learning come estimation recall denotes total reward obtained choosing rst action time steps 
details concerning extension clearly described report rummery niranjan 
estimator learnt concurrently real time system operation 
lines argument value function obtain learning rule similar eq xt xt respectively system state action chosen time eq visited eq xt eq xt reset zero new trial begun random starting state 
stochastic policy better replace probf bg rummery niranjan suggest stochastic case corresponds instance generated policy 
feel estimate better term ts better de nition 
size small computations expensive 
delayed reinforcement learning methods dynamic programming dp methods ross bertsekas known classical tools solving stochastic optimal control problem formulated 
delayed rl methods solve problem di er dp methods 
main di erences 
dp methods simply aim obtain optimal value function optimal policy line iterative methods delayed rl methods aim learn concurrently real time system operation improve performance time 
connection rl rst established werbos watkins 
keerthi ravindran dp methods deal complete state space computations delayed rl methods operate set states occur real time system operation 
applications large small manageable subset applications dp methods su er curse dimensionality delayed rl methods problem 
typically delayed rl methods employ function approximators value function policy generalize learning provide near optimal performance unseen parts state space 
dp methods fundamentally require system model 
hand main delayed rl methods model free particularly suited line learning control complicated systems di cult derive 
delayed rl methods continuously learn time better suited dp methods adapting situations system goals non stationary 
said delayed rl methods enjoy certain key advantages add dp fore runner delayed rl methods obtained clues 
fact correct say delayed rl methods basically rearrangements computational steps dp methods applied real time system operation 
delayed rl methods grouped categories model methods model free methods 
model methods direct links dp 
model free methods viewed appropriate modi cations model methods avoid model requirement 
methods described detail 
model methods subsection discuss dp methods possible modi cation yield delayed rl methods 
popular dp methods value iteration policy iteration 
value iteration easily extends give delayed rl method called real time dp 
policy iteration directly yield delayed method forms basis important model free delayed rl method called actor critic 
value iteration basic idea value iteration compute 

lim optimal value function nite horizon length maximum expected return decision task terminated steps starting state maximum expected return just maximum expected immediate payo max tutorial survey reinforcement learning recursion max pxy compute 
iterations terminated large number iterations taken approximation 
value iteration policy involved 
easy attach suitable policy value function follows 
associated value function policy greedy respect arg max pxy state space large size size kd number components number values component take value iteration prohibitively expensive 
di culty usually referred curse dimensionality 
correct 
prove convergence 
turns convergence established general algorithm value iteration special case 
call algorithm generalized value iteration 
generalized value iteration set function states 
repeat 
choose subset states bn set 
maxa 
reset 
pxy bn choose take bn algorithm reduces value iteration 
useful cases generalized value iteration 
rst concern issue convergence 
bn value state backed th iteration 
proof convergence result bertsekas tsitsiklis watkins barto 
local value improvement theorem mn maxx jv 
maxx bn jv 
mn 
proof take bn 
leta 
anda policy greedy respect pxy pxy 

mn view recursion doing xed point iteration solve bellman optimality equation 
keerthi ravindran similarly pxy pxy 

mn theorem proved 
theorem implies mn mn mn maxx jv 
little thought shows true 
iteration iterations done away state backed iterations bn get mk mk 
value state backed nitely holds 
case value iteration value state backed iteration holds 
generalized value iteration proposed bertsekas developed bertsekas tsitsiklis suitable method solving stochastic optimal control problems multi processor systems communication common clock 
processors available state space partitioned sets processor 
times processor backs values states di erent processor 
back values states processor uses values states communicated processors 
barto bradtke singh suggested generalized value iteration way learning real time system operation 
called algorithm real time dynamic programming rtdp 
generalized value iteration specialized rtdp denotes system time 
time step say system resides state xn 
available chosen action greedy respect xn 
bn set states values backed chosen include xn states 
order improve performance immediate lookahead search xed search depth exhaustively policy include probable states bn 
value xn going undergo change time step idea include bn predecessors xn moore atkeson 
may ask model system available simply value iteration generalized value iteration bertsekas tsitsiklis suggest 
words motivation rtdp 
answer simple 
problems playing games backgammon state space extremely large small subset occurs usage 
rtdp works concurrently actual system operation focusses regions state space relevant system behaviour 
instance successful learning accomplished checkers program samuel backgammon program tesauro variations rtdp 
barto barto bradtke singh rtdp interesting connections useful extensions learning real time search algorithms arti cial intelligence korf 
convergence result mentioned earlier says values states convergence holds certain assumptions 
analysis required sophisticated 
see bertsekas tsitsiklis bradtke details 
tutorial survey reinforcement learning backed nitely order ensure convergence 
important suitably explore state space order improve performance 
barto bradtke singh suggested ways doing exploration adding stochasticity policy doing learning cumulatively multiple trials 
inaccurate system model available updated real time system identi cation technique likelihood estimation method barto 
current system model perform computations 
convergence adaptive methods proved gullapalli barto 
policy iteration policy iteration operates maintaining representation policy value function forming improved policy 
suppose policy known 
improve answer obvious rst answer simpler question 
policy uniformly better simple theorem watkins gives answer 
policy improvement theorem policy uniformly better policy proof toavoid clumsy details give rigorous proof watkins 
starting better follow step follow follow right 
argument better follow step state just reached 
repeating argument get better follow see bellman dreyfus ross detailed proof 
return original question policy value function policy ifwe de ne arg max holds 
policy improvement theorem uniformly better main idea policy iteration 
policy iteration set arbitrary initial policy compute repeat 
compute 

find compute practical performance su cient states relevant system behaviour backed repeatedly 
thrun discussed importance exploration suggested variety keerthi ravindran 
set occurs step 
nice features algorithm terminates nite number iterations nite number policies termination occurs get maxq satis es bellman optimality equation optimal policy 
algorithm su ers serious drawback expensive entire value function associated policy recalculated iteration step 
may close unfortunately simple short cut compute 
known model free method called actor critic method gives inexpensive approximate way implementing policy iteration 
model free methods model free delayed rl methods derived making suitable approximations computations value iteration policy iteration eliminate need system model 
important methods result approximations barto sutton anderson actor critic barto watkins learning watkins 
methods milestone contributions optimal feedback control dynamic systems 
actor critic method actor critic method proposed barto sutton anderson popular balancing pole moving cart way step step basis process forming value function process forming new policy 
method viewed practical approximate way doing policy iteration perform step line procedure estimating value function policy time perform step online procedure improving policy 
actor critic method best derived combining ideas andx rl estimating value function respectively 
details follows 
actor denote total maintain approximator merits various feasible actions state order exploration choose actions stochastic action selector 
critic maintain approximator estimates value function expected total reward corresponding stochastic policy mentioned 
ideas update consider process learning actor 
immediate rl learning complicated reason 
immediate mathematical analysis method done williams baird 
original pole balancing barto sutton anderson suggested di erent way including stochasticity 
tutorial survey reinforcement learning rl environment immediately provides evaluation action delayed rl ect action total reward immediately available estimated appropriately 
suppose time step system state action selector chooses action ak learning rule parallels gk gk expected total reward obtained applied system state policy followed step onwards 
approximation pxy estimate unavailable model 
approximation state occuring real time operation action applied state gives gk gk de ned 
algorithm results 
actor critic trial set random starting state 
repeat number time steps 
system state choose action apply system 
state 

compute 
update 
update gk algorithm uses td estimate speed learning td rule employed 
barto sutton anderson gullapalli gullapalli idea eligibility traces updating 
give intuitive explanation usage 
lin suggested accumulation data trial update states visited trial update state action pairs experienced trial 
learning just actor critic method model free line way approximately implementing policy iteration watkins learning algorithm model free line way approximately implementing generalized value iteration 
rtdp algorithm generalized value iteration concurrently real time system operation requires system model doing crucial operation determination maximum right hand side 
learning overcomes problem keerthi ravindran elegantly operating function value function 
recall de nition function comment advantage value function 
aim learning nd function approximator approximates solution bellman optimality equation line mode employing model 
sake ideas systematically assuming system model available consider modi cation ideas function value function 
think terms function approximator value function basic update rule max pxy function corresponding rule pxy max rule ideas easily modi ed employ function 
main concern derive algorithm avoids system model 
model replace summation term wherex instance state resulting application action state achieve ect update rule averaging learning rule max carried value backed 
line mode system operation learning algorithm 
learning trial set starting state 
repeat number time steps 
choose action apply system 
state 

update 

reset equation isvery appropriate learning rule 
watkins showed value admissible pair backed nitely step size decreased suitable way converges probability 
practically learning achieved step appropriate exploration policy tries revised proof watkins dayan 
tsitsiklis jaakkola proofs 
tutorial survey reinforcement learning actions doing multiple trials ensure states frequently visited decreasing zero learning progresses 
discuss way speeding learning td estimate function derived 
td employed learning trial fundamental requirement policy step learning trial policy update rule match note 
td employ greedy policy arg max step 
leads problem greedy policy allow exploration action space poor learning occur 
rummery niranjan give nice comparitive account attempts described literature dealing con ict 
details approach rummery niranjan promising 
consider stochastic policy boltzmann distribution probf exp exp 
actions equal probabilities stochastic policy tends greedy policy 
learn started suitable large value depending initial size values decreased zero annealing rate generated multiple learning trials performed 
way exploration takes place initial large values 
td learning rule estimates expected returns policy converge 
ideas somewhat similar simulated annealing 
extension continuous spaces optimal control dynamic systems typically involves solution delayed rl problems having continuous state action spaces 
state space continuous action space discrete delayed rl algorithms discussed earlier easily extended provided appropriate function approximators generalize real time experience state topologically nearby states see discussion approximators 
hand action space continuous extension algorithms di cult 
main cause di culty easily seen try extending rtdp continuous action spaces max operation non trivial di cult 
methods value iteration need maintain function approximator actions 
rest subsection give brief review note step put restriction choosing feasible action 
stochastic exploration policy generates feasible action positive probability canbe 
learning complete greedy policy argmax optimal system performance 
action attains maximum convenience take stochastic policy maximizing actions equally probable 
keerthi ravindran various methods handling continuous action spaces 
just presentation easy assumptions 
system controlled deterministic 
describe transition 
xt xt action constraints dimensional real space functions involved continuously di erentiable 
rst consider model methods 
werbos proposed variety algorithms 
describe important algorithm werbos refers backpropagated adaptive critic 
algorithm actor critic type somewhat di erent actor critic method 
function approximators action critic 
critic meant approximate time step updated td learning rule 
actor tries improve policy time step hint provided policy improvement theorem 
speci de ne def time system state xt action xt leading state xt 
assume sothat xt xt holds 
hint aim adjust xt give new value new simple learning rule achieves requirement xt new xt xt xt xt ja small positive step size 
partial derivative evaluated xt xt xt jy xt come model free methods 
simple idea adapt function approximator system model function werbos algorithm 
line experience combination xt xt learn method proposed serious de ciency associated ill formed model free method suggested jordan jacobs 
key di culty associated method learning system adapts system performance improve werbos describes ways treating stochastic systems 
de ciency pointed gullapalli 
tutorial survey reinforcement learning fact early stages learning method perform confused way 
problem suggests learnt train actor critic 
direct model free method derived ideas employing learning rule similar adapting 
method proposed successfully demonstrated gullapalli gullapalli gullapalli 
gullapalli method learns observing ect randomly chosen perturbation policy systematic policy change method 
propose new model free method systematically changes policy similar method avoids need adapting system model 
achieved function approximator approximating function associated actor 
td learning rule updating policy improvement attempted learning rule similar xt xt xt ja currently performing simulations study performance new method relative model free methods mentioned 
werbos algorithm learning algorithm deterministic gullapalli algorithm stochastic 
deterministic methods expected faster stochastic method better assurance convergence true solution 
arguments similar mentioned 
issues section discuss number issues relevant practical implementation rl algorithms 
nice discussion issues barto 
function approximation avariety function approximators employed researchers practically solve rl problems 
input space function approximator nite straight forward method look table singh moore atkeson 
theoretical results convergence rl algorithms assume representation 
disadvantage look table input space large memory requirement prohibitive 
continuous input spaces discretized look table 
discretization done nely obtain accuracy face curse dimensionality 
way problem dependent discretization see example boxes representation barto sutton anderson michie chambers gullapalli rosen solve pole balancing problem 
lawrence proposed new delayed rl method called transition point signi cantly reduce memory requirement problems optimal actions change infrequently time 
keerthi ravindran non look table approaches parametric function approximation methods 
methods advantage able generalize training data give reasonable performance unvisited parts input space 
connectionist methods popular 
connectionist methods employed rl classi ed groups multi layer perceptrons methods clustering cmac recurrent networks 
multi layer perceptrons successfully anderson pole balancing lin complex test problem tesauro backgammon thrun robot navigation boyen gullapalli 
hand watkins chapman kaelbling reported bad results 
modi ed form platt resource allocation network platt method radial basis functions anderson pole balancing 
researchers cmac albus solving rl problems watkins test problem singh tham prager navigation problem lin kim pole balancing sutton dyna architecture 
recurrent networks context information feedback mozer dealing rl problems incomplete state information 
non connectionist methods rl 
mahadevan connell statistical clustering association learning automatic programming mobile robot 
novel feature approach number clusters dynamically varied 
chapman kaelbling tree clustering approach modi ed learning algorithm di cult test problem huge input space 
function approximator exercise care ensure learning input point seriously disturb function values advantageous choose function approximator away function values states near modi ed similarly values states far left unchanged 
usually leads generalization performance learnt function approximator states visited learning 
respect cmac methods clustering rbf statistical clustering suitable multi layer perceptrons 
ect errors introduced function approximators optimal performance controller understood 
pointed watkins bradtke barto function approximation done careful way poor learning result 
context learning thrun schwartz shown errors function approximation lead systematic estimation function 
linden points problems value function discontinuous continuous function approximators inappropriate 
suggest clear remedies problem 
mentioned needs done function approximators rl 
bertsekas singh yee derived theoretical bounds errors value function terms function 
tutorial survey reinforcement learning modular hierarchical architectures applied problems large task space sparse rewards rl methods terribly slow learn 
dividing problem simpler subproblems hierarchical control structure ways overcoming 
sequential task decomposition method 
method useful number complex tasks performed making nite number elemental tasks skills say tn 
original objective controller achieved temporally concatenating number elemental tasks form called composite task 
example cj ft tng composite task elemental tasks performed order listed 
reward functions de ned tasks making abundant original problem de nition 
singh proposed algorithm modular connectionist network jacobs making ideas 
controller unaware decomposition task learn elemental tasks decomposition composite tasks simultaneously 
tham prager lin proposed similar solutions 
mahadevan connell developed method subsumption architecture brooks decomposition task speci ed user hand controller learns elemental tasks maes brooks shown controller learn decomposition similar framework 
methods require external agency specify problem decomposition 
controller learn problem decomposed 
singh preliminary results needs done 
approach problem form hierarchical control watkins 
di erent levels controllers learning perform task level directing lower level controllers achieve objective 
example ship navigator decides direction sail reach port steers ship direction indicated navigator 
navigator higher level controller lower level controller 
higher level controllers smaller task space lower level controllers set simpler tasks improved performance results 
examples hierarchical architectures feudal rl dayan hinton hierarchical planning singh 
methods require external agency specify hierarchy 
done usually making structure problem 
training controllers simpler tasks rst training perform progressively complex tasks simpler tasks lead better performance 
stage controller faced simple learning task 
technique called shaping animal behaviour literature 
gullapalli singh reported success idea 
singh shows controller discover decomposition task technique 
controllers di may operate di erent temporal resolutions 
keerthi ravindran speeding learning apart ideas mentioned various techniques suggested speeding rl 
novel ideas suggested lin experience playback teaching 
rst discuss experience playback 
experience consists quadruple occuring real time system operation state action applied state resulting state 
past experiences stored nite memory bu er appropriate strategy maintain point time current stochastic policy 
ag chosen tolerance 
learning update rule applied current experience chosen subset experience playback especially useful learning rare experiences 
teaching user provides learning system experiences expedite learning 
incorporating domain speci knowledge helps speeding learning 
example problem nominal controller gives reasonable performance may easily available 
case rl methods controller improve performance singh 
domain speci information greatly help choosing state representation setting function approximators barto 
applications inaccurate system model available 
turns ine cient discard model simply employ model free method 
cient approach anumber planning steps line learning steps 
planning step may time step model method time step model free method experience generated available system model 
approach appropriate adapt system model line experience 
ideas form basis sutton dyna architectures sutton related methods moore atkeson peng williams 
tried give cohesive overview existing rl algorithms 
research reached mature level rl successfully demonstrated practical applications gullapalli tesauro mahadevan connell thrun clear guidelines general applicability exist 
connection dp rl nicely bridged control theorists ai researchers 
contributions groups pipeline interesting results forthcoming expected rl strong impact intelligent control dynamic systems 
albus new approach manipulator control cerebellar model articulation controller cmac 
trans 
asme dynamic sys meas contr 
tutorial survey reinforcement learning anderson learning problem solving connectionist systems 
ph thesis university massachusetts amherst ma usa 
anderson strategy learning multilayer connectionist representations 
technical report tr gte laboratories waltham ma usa 
anderson april learning control inverted pendulum neural networks 
ieee control systems magazine pages 
anderson learning hidden unit restarting 
advances neural information processing systems hanson cowan giles editors pages morgan kaufmann san mateo ca usa 
connectionist learning control architecture navigation 
advances neural information processing systems lippman moody touretzky editors pp 
morgan kaufmann san mateo ca usa 
connectionist modeling control nite state environments 
ph thesis university massachusetts amherst ma usa 
barto learning statistical cooperation self interested neuron computing elements 
human neurobiology 
barto game cooperativity self interested units 
neural networks computing denker editor pages american institute physics new york usa 
barto learning adaptive critic methods 
handbook intelligent control neural fuzzy adaptive approaches white editors pages van nostrand reinhold new york usa 
barto anandan pattern recognizing learning automata 
ieee transactions systems man 
barto anandan anderson cooperativity pattern recognizing stochastic learning automata 
proceedings fourth yale workshop applications systems theory ct usa 
barto bradtke singh real time learning control asynchronous dynamic programming 
technical report coins university massachusetts amherst ma usa 
barto jordan gradient back propagation layered networks 
proceedings ieee annual conference neural networks butler editors pages ii ii san diego ca usa 
barto singh computational economics reinforcement learning 
connectionist models proceedings summer school touretzky elman sejnowski hinton editors pages morgan kaufmann san mateo ca usa 
barto sutton landmark learning illustration associative search 
biological cybernetics 
barto sutton simulation anticipatory responses classical conditioning neuron adaptive element 
behavioural brain research 
barto sutton anderson neuronlike elements solve di cult learning control problems 
ieee transactions systems man cybernetics 
barto sutton brouwer associative reinforcement learning associative memory 
ieee transactions systems man cybernetics 
barto sutton watkins learning sequential decision making 
learning computational neuroscience foundations adaptive networks gabriel moore editors pages mit press cambridge ma usa 
bellman dreyfus applied dynamic programming 
rand 
keerthi ravindran bertsekas distributed dynamic programming 
ieee transactions automatic control 
bertsekas dynamic programming deterministic stochastic models 
prentice hall englewood cli nj usa 
bertsekas tsitsiklis parallel distributed computation numerical methods 
prentice hall englewood cli nj usa 
boyen modular neural networks learning context dependent game strategies 
masters thesis computer speech language processing university cambridge cambridge england 
bradtke reinforcement learning applied linear quadratic regulation 
advances neural information processing systems hanson cowan giles editors pages morgan kaufmann san mateo ca usa 
bradtke incremental dynamic programming line adaptive optimal control 
technical report 
fast learning predictive forward models 
advances neural information processing systems moody hanson lippmann editors pages morgan kaufmann san mateo ca usa 
brooks achieving arti cial intelligence building robots 
technical report memo massachusetts technology cial intelligence laboratory cambridge ma usa 
lawrence transition point dynamic programming 
advances neural information processing systems cowan tesauro alspector editors pages morgan kaufmann san fransisco ca usa 
chapman vision instruction action 
mit press 
chapman kaelbling input generalization delayed reinforcement learning algorithm performance 
proceedings international joint conference onarti cial intelligence 
chrisman planning closed loop execution partially observable markovian decision processes 
proceedings aaai 
dayan navigating temporal di erence 
advances neural information processing systems lippmann moody touretzky editors pages morgan kaufmann san mateo ca usa 
dayan reinforcing connectionism learning statistical way 
ph thesis university edinburgh edinburgh scotland 
dayan hinton feudal reinforcement learning 
advances neural information processing systems hanson cowan giles editors pages morgan kaufmann san mateo ca usa 
dayan sejnowski td converges probability technical report cnl salk institute san diego ca usa 
dean wellman planning control 
morgan kaufmann san mateo ca usa 
gullapalli stochastic reinforcement algorithm learning real valued functions 
neural networks 
gullapalli reinforcement learning application control 
technical report coins ph 
thesis university massachusetts amherst ma usa 
gullapalli comparison supervised reinforcement learning methods learning task 
proceedings ieee symposium intelligent control va usa 
gullapalli barto convergence indirect adaptive asynchronous value iteration algorithms 
advances neural information processing systems cowan tesauro alspector editors pages morgan kaufmann san fransisco ca usa 
tutorial survey reinforcement learning gullapalli franklin february acquiring robot skills reinforcement learning 
ieee control systems magazine pages 
hertz krogh palmer theory neural computation 
addison wesley ca usa 
jaakkola jordan singh convergence stochastic iterative dynamic programming algorithms 
advances neural information processing systems cowan tesauro alspector editors pp 
morgan kaufmann san fransisco ca usa 
jacobs jordan nowlan hinton adaptive mixtures local experts 
neural computation 
jordan jacobs learning control unstable system forward modeling 
advances neural information processing systems touretzky editor morgan kaufmann san mateo ca usa 
jordan rumelhart forward models supervised learning distal teacher 
center cognitive science occasional massachusetts institute technology cambridge ma usa 
kaelbling learning embedded systems 
technical report tr ph thesis department computer science stanford university stanford ca usa 
kaelbling learning embedded systems 
mit press cambridge ma usa 
klopf brain adaptive sytems theory 
report air force cambridge research laboratories bedford ma usa 
klopf neuron theory memory learning intelligence 
washington usa 
klopf neuronal model classical conditioning 

korf real time heuristic search 
cial 
kumar survey results stochastic adaptive control 
siam journal control optimization 
lin programming robots reinforcement learning teaching 
proceedings ninth national conference onarti cial intelligence pages mit press cambridge ma usa 
lin self improvement reinforcement learning planning teaching 
machine learning proceedings eighth international workshop birnbaum collins editors pages morgan kaufmann san mateo ca usa 
lin self improving reactive agents case studies reinforcement learning frameworks 
animals animats proceedings international conference simulation adaptive behaviour pages mit press cambridge ma usa 
lin self improving reactive agents reinforcement learning planning teaching 
machine learning 
lin hierarchical learning robot skills reinforcement 
proceedings international conference neural networks pages 
lin kim cmac adaptive critic self learning control 
ieee trans 
neural networks 
linden discontinuous functions reinforcement learning 
available anonymous ftp archive cis ohio state edu directory pub 
maes brooks learning coordinate behaviour 
proceedings eighth national onarti cial intelligence pages morgan kaufmann san mateo ca usa 
backgammon 
times books new york usa 
mahadevan connell scaling reinforcement learning robotics exploiting subsumption architecture 
machine learning proceedings eighth international workshop birnbaum collins editors pages morgan kaufmann san mateo ca usa 
keerthi ravindran andersen jordan ar learning applied network model cortical area 
proceedings international joint conference neural networks 
michie chambers boxes experiment 
machine intelligence dale michie editors pages oliver boyd 
reinforcement connectionist approach robot path nding non maze environments 
machine learning 
minsky theory neural analog reinforcement systems application brain model problem 
ph thesis princeton university princeton nj usa 
minsky steps arti cial intelligence 
proceedings institute radio engineers 
reprinted computers thought feigenbaum feldman editors mcgraw hill new york 
moore cient memory learning robot control 
ph thesis university cambridge cambridge moore variable resolution dynamic ciently learning action maps multivariate real state spaces 
machine learning proceedings eighth international workshop birnbaum collins editors pages morgan kaufmann san mateo ca usa 
moore atkeson memory reinforcement learning cient computation prioritized sweeping 
advances neural information processing systems hanson cowan giles editors pages morgan kaufmann san mateo ca usa 
mozer discovering structure reactive environment exploration 
advances neural information processing touretzky editor pages morgan kaufmann san mateo ca usa 
mozer discovering structure reactive environment exploration 
neural computation 
narendra learning automata 
prentice hall englewood cli nj usa 
peng williams cient learning planning dyna framework 
proceedings international joint conference neural networks pages 
platt learning combining memorization gradient descent 
advances neural information processing systems lippmann moody andd touretzky editors pages morgan kaufmann san mateo ca usa 
rosen goodwin vidal adaptive range coding 
advances neural information processing systems lippmann moody andd touretzky editors pages morgan kaufmann san mateo ca usa 
ross stochastic dynamic programming 
academic press new york usa 
rummery niranjan line learning connectionist systems 
technical report cued infeng tr university cambridge cambridge england 
samuel studies machine learning game checkers 
ibm journal research development pages 
reprinted computers thought feigenbaum feldman editors mcgraw hill new york 
samuel studies machine learning game checkers ii progress 
ibm journal research development pages 
selfridge sutton barto training tracking robotics 
proceedings ninth international joint conference cial intelligence joshi editor pages morgan kaufmann san mateo ca usa 
teaching arti cial neural systems drive manual tutorial survey reinforcement learning training techniques autonomous systems 
proceedings annual international conference neural networks san diego ca usa 
singh transfer learning composition sequential tasks 
machine learning proceedings eighth workshop birnbaum collins editors pages morgan kaufmann san mateo ca usa 
singh reinforcement learning hierarchy models 
proceedings tenth national conference onarti cial intelligence san jose ca usa 
singh cient learning multiple sequential tasks 
advances neural information processing systems moody hanson lippmann editors pages morgan kaufmann san mateo ca usa 
singh scaling reinforcement learning algorithms learning variable temporal resolution models 
proceedings ninth international machine learning conference 
singh transfer learning composing solutions elemental sequential tasks 
machine learning 
singh barto grupen connelly robust reinforcement learning motion planning 
advances neural information processing systems cowan tesauro alspector editors pages morgan kaufmann san fransisco ca usa 
singh yee upper bound loss approximate optimal value functions 
technical report university massachusetts amherst ma usa 
sutton temporal credit assignment reinforcement learning 
ph thesis massachusetts amherst ma usa 
sutton learning predict method temporal di erences 
machine learning 

sutton integrated architecture learning planning reacting approximating programming 
proceedings seventh international conference machine learning pages morgan kaufmann san mateo ca usa 
sutton planning incremental dynamic programming 
machine learning proceedings eighth international workshop birnbaum collins editors pages morgan kaufmann san mateo ca usa 
sutton integrated modeling control reinforcement learning dynamic programming 
advances neural information processing systems lippmann moody touretzky editors pages morgan kaufmann san mateo ca usa 
sutton barto modern theory adaptive networks expectation prediction 
psychological review 
sutton barto temporal di erence model classical conditioning 
proceedings ninth annual conference cognitive science society erlbaum hillsdale nj usa 
sutton barto time derivative reinforcement 
learning computational neuroscience foundations adaptive networks gabriel moore editors pages mit press cambridge ma usa 
sutton barto williams reinforcement learning direct adaptive optimal control 
proceedings th american control conference pages boston ma usa 
sutton singh step size bias td learning 
proceedings eighth yale workshop adaptive learning systems pages yale university usa 
tan cost sensitive internal representation reinforcement learning 
machine learning proceedings eighth international workshop birnbaum collins editors pages morgan kaufmann san mateo ca usa 
keerthi ravindran tesauro practical issues temporal di erence learning 
machine learning 
tham prager modular learning architecture manipulator task decomposition 
machine learning proceedings eleventh conference cohen hirsh editors nj morgan kaufmann usa 
available gopher dept engg university cambridge cambridge england 
thrun cient exploration reinforcement learning 
technical report cmu cs school computer science carnegie mellon university pittsburgh pa usa 
thrun exploration model building mobile robot domains 
proceedings international conference neural networks 
thrun muller active exploration dynamic environments 
advances neural information processing systems moody hanson lippmann editors morgan kaufmann san mateo ca usa 
thrun schwartz issues function approximation reinforcement learning 
proceedings fourth connectionist models summer school lawrence erlbaum hillsdale nj usa 
tsitsiklis asynchronous stochastic approximation learning 
technical report lids laboratory information decision systems mit cambridge ma usa 
clouse kinds training information evaluation function learning 
proceedings ninth annual conference onarti cial intelligence pages morgan kaufmann san mateo ca usa 
watkins learning rewards 
ph thesis cambridge university cambridge england 
watkins dayan technical note learning 
machine learning 
werbos building understanding adaptive systems statistical numerical approach factory automation brain research 
ieee transactions systems man 
werbos generalization back propagation application recurrent gas market model 
neural networks 
werbos neural network control system identi cation 
proceedings th conference control pages tampa fl usa 
werbos consistency hdp applied simple reinforcement learning problems 
neural networks 
werbos menu designs reinforcement learning time neural networks control miller sutton werbos editors pages mit press ma usa 
werbos approximate dynamic programming real time control neural modeling 
handbook intelligent control neural fuzzy adaptive approaches white editors pages van nostrand reinhold ny usa 
whitehead complexity analysis cooperative reinforcement learning 
proceedings ninth conference onarti cial intelligence pages mit press cambridge ma usa 
whitehead complexity cooperation learning 
machine learning proceedings eighth international workshop birnbaum collins editors pages morgan kaufmann san mateo ca usa 
whitehead ballard active perception reinforcement learning 
neural computation 
williams reinforcement learning connectionist networks mathematical analysis 
technical report ics institute cognitive science university california san diego la jolla ca usa 
tutorial survey reinforcement learning williams reinforcement learning connectionist systems 
technical report nu ccs college computer science northeastern university boston ma usa 
williams baird iii mathematical analysis actor critic architectures learning optimal controls incremental dynamic programming 
proceedings sixth yale workshop adaptive learning systems pages new haven ct usa 
