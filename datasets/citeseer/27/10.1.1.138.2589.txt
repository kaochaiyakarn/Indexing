multiagent reinforcement learning theoretical framework algorithm adopt general sum stochastic games framework multiagent reinforcement learning 
extends previous littman zero sum stochastic games broader framework 
design multiagent learning method framework prove converges nash equilibrium speci ed conditions 
algorithm useful nding optimal strategy exists unique nash equilibrium game 
exist multiple nash equilibria game algorithm combined learning techniques nd optimal strategies 
reinforcement learning gained attention extensive study years 
learning method need model environment online reinforcement learning wellsuited multiagent systems agents know little agents environment changes learning 
applications reinforcement learning multiagent systems include soccer pursuit games coordination games 
systems single agent reinforcement learning methods applied modi cation 
approach treats agents system part environment ignoring di erence responsive agents passive environment 
propose multiagent reinforcement learning method explicitly take agents account 
propose new framework needed multiagent reinforcement learning 
hu michael wellman arti cial intelligence laboratory university ann arbor mi usa umich edu ai eecs umich edu people framework adopt stochastic games called markov games generalization markov decision processes case 
stochastic games de ned non cooperative games agents pursue self interests choose actions independently 
littman introduced player zero sum stochastic games multiagent reinforcement learning 
zero sum games agent gain agent loss agents strictly opposite interests 
adopt framework general sum stochastic games agents need longer opposite interests 
general sum games include zero sum games special cases 
games notions optimality loses meaning agent payo depends agents choices 
solution concept nash equilibrium adopted 
nash equilibrium agent choice best response agents choices 
agent gain unilateral deviation 
nash equilibrium solution wewant design learning agent noncooperative multiagent systems 
systems agent pursues goal communication agents 
nash equilibrium plausible self enforcing solution concept systems 
payo structure state transition probabilities known agents solve nash equilibrium strategy nonlinear programming method proposed 
interested situations agents incomplete information agents payo functions state transition probabilities 
show multiagent learning algorithm designed converges nash equilibrium certain restrictions game 
algorithm designed player general sum stochastic games extended player general sum games 
learning algorithm guarantees agent learn nash equilibrium 
say agent learn nash equilibrium 
exist nash equilibrium game learning algorithm works ectively 
game nash equilibria 
case learning algorithm needs combined empirical estimation action choices agent 
preliminaries state basic game theory concepts section 
concepts refer single state static games 
sections see concepts connected multi state stochastic games 
zero sum games payo matrices players described player payo negative 
su cient simplify game player zero sum games called matrix games 
player general sum games agents payo matrices unrelated 
solutions game depend games called bimatrix games 
de nition pair matrices constitutes bimatrix game size 
payo player corresponding entry matrix 
rows correspond actions player columns correspond actions player sets discrete actions players respectively 
state solution concepts bimatrix games 
main concept nash equilibrium 
nash equilibrium agent action best response agents choices 
de nition pure strategy nash equilibrium bimatrix game action pro le example bimatrix game seen strategy pair constitutes pure strategy nash equilibrium 
bimatrix game example de nition mixed strategy nash equilibrium bimatrix game set probability distributions action space 
expected payo agent situation player player adopt mixed strategies respectively 
reason interested mixed strategies arbitrary bimatrix game may pure strategy nash equilibrium mixed strategy nash equilibrium 
theorem nash exists mixed strategy nash equilibrium nite bimatrix game 
mixed strategy nash equilibrium bimatrix game mangasarian stone algorithm quadratic programming algorithm 
markov decision process reinforcement learning comparison purpose state framework markov decision process 
see stochastic game framework related markov decision process 
de nition markov decision process tuple discrete state space discrete action space reward function agent transition function set probability distributions state space abuse notation little 
transposed multiplied matrix markov decision process objective agent nd strategy policy maximize expected sum discounted rewards initial state rt reward time discount factor 
rewrite equation js action determined policy proved exists optimal policy bellman equation holds max js called optimal value state agent knows reward function state transition function solve iterative searching methods 
learning problem arises agent know reward function state transition probabilities 
agent needs interact environment nd optimal policy 
agent learn reward function state transition function solve optimal policy equation 
approach called model reinforcement learning 
agent directly learn optimal policy knowing reward function state transition function 
approach called model free reinforcement learning 
model free reinforcement learning methods qlearning 
basic idea learning de ne right hand side equation js de nition total discounted reward attained action state optimal policy 
equa tion maxq know optimal policy action maximize states 
learning agent starts arbitrary initial values time agent choose action observes reward rt 
agent updates values equation qt qt rt max qt learning rate 
learning rate needs decay time order learning algorithm converge 
watkins dayan proved sequence converges optimal 
stochastic game framework markov decision process mdp single agent decision problem 
natural extension mdp multiagent systems stochastic games essentially agent markov decision processes 
focus player stochastic games studied 
de nition stochastic games de nition player stochastic game tuple discrete state space discrete action space player payo function player transition probability map set probability distributions state space closer look stochastic game consider process observable discrete time points 
time point state process denoted st assume st takes values set process controlled decision makers referred player player respectively 
state player independently chooses actions receives rewards respectively 
game called zero sum 
sum restricted constant game called general sum game 
assumed transition players take actions independent time 
exist stationary transition probabilities js satisfying constraint mx js objective player maximize discounted sum rewards 
discount factor strategies players respectively 
initial state players receive values game strategy de ned course game 
called decision rule time strategy called stationary strategy decision rule xed time 
called behavior strategy ht ht history time ht st stationary strategy special case behavior strategy ht decision rule assigns mixed strategies di erent states 
decision rule stationary strategy form maximal number states 
mixed strategy state nash equilibrium stochastic games de ned assuming players complete information payo functions players 
de nition stochastic game nash equilibrium point pair strategies de nition nash equilibrium requires agent strategy best response strategy 
de nition nash equilibrium similar games 
strategies constitute nash equilibrium behavior strategies markov strategies stationary strategies 
sm sm sm stochastic games bimatrix games interested stationary strategies simple strategies 
theorem shows exist nash equilibrium stationary strategies stochastic game 
theorem theorem general sum discounted stochastic game possesses equilibrium point stationary strategies 
stochastic games bimatrix games view stage stochastic game bimatrix game 
time period stochastic game state agent choose actions independently receive payo bimatrix game 
repeated games seen degenerate case stochastic games state 
example index state repeated game bimatrix game time period 
multiagent reinforcement learning want extend traditional reinforcement learning method markov decision process stochastic games 
assume games incomplete perfect information meaning agents know agents payo functions observe agents immediate payo actions taken previously 
issues designing multiagent learning algorithm target learning optimal values de ne nx nx js js optimal value state action pair total discounted reward received agent agents execute actions state follow nash equilibrium strategies 
learn values agent needs maintain tables values total number states 
agent table rows corresponding columns corresponding entry 
total number entries agent needs learn ja ja ja ja sizes action spaces assuming ja ja jaj space requirement ism jaj agents space requirement jaj exponential number agents 
large number agents need nd compact representation action space 
single agent learning learning agent multiagent systems updates tables state observes state actions taken agents rewards received agents 
difference updating rule 
single agent qlearning values updated qt qt rt max qt multiagent learning just maximize values values depend action agent 
zero sum game minimize agent actions choose maximal 
minimax learning algorithm rt update st st rt st st time time line actions littman 
general sum games mini max algorithm agent payo opposite 
propose agent adopt nash strategy update values best agent general sum game 
learning algorithm learning agent say agent updates rule mixed strategy nash equilibrium bimatrix game 
der nd agent needs learn game 
learning learning agent maintains tables state values agent 
possible assume agent observe agent immediate rewards previous actions learning 
detail learning algorithm stated table 
game zero sum 
agent needs learn table state 
learning algorithm qt qt rt max min qt di erent littman minimax learning algorithm value updated qt qt rt max min qt table multiagent learning algorithm agent initialize anda initialize loop choose action st isa mixed strategy nash equilibrium solution bimatrix game st st 
observe update andq st st st st st st st st mixed strategy nash solutions bimatrix game st st littman learning algorithm assumed agent choose pure nash equilibrium strategy mixed strategy 
thing note learning algorithm action timet important convergence learning 
action choices important short term performance 
studied issue action choice explore 
convergence algorithm section prove convergence qlearning algorithm certain assumptions 
rst assumptions standard ones learning assumption state action visited nitely 
assumption learning rate satis es conditions 

st 
assumptions regarding structure game assumption bimatrix game satis es properties 
nash equilibrium global optimal 
andk 
nash equilibrium global optimal agent receives higher payo agent deviates nash equilibrium strategy 

convergence proof lemmas proved littman 
lemma conditional average lemma assumptions process qt qt converges ht history time lemma assumptions process dened ut ut ptv converges pt satis es ptv ptv converges iteration de ned vt vt converges order prove convergence point learning algorithm nash equilibrium point need theorem proved 
theorem assertions equivalent 
pair constitutes equilibrium point static bimatrix game equilibrium pay entry equals nx js 
equilibrium point discounted stochastic game equilibrium payo 
theorem showed nash solution bimatrix game de ned theorem part nash solution game 
sequence learning algorithm converges values de ned theorem pair stationary nash equilibrium strategies derived 
state part nash equilibrium solution bimatrix game 
lemma qk pair mixed nash equilibrium strategies bimatrix game 
pt contraction mapping 
proof 
case qk xx xx max jq inequality derives de nition nash equilibrium 
inequality property assumption 
cases satisfying property assumption proof simpler omit 
similar proof 
property assumption xx property assumption xx case qk 
similar proof case 
property assumption xx wehave jp qk holds state qk proceed prove main theorem states multiagent learning methods converges optimal nash equilibrium values 
theorem stochastic game assumptions coupled sequences fq updated converge nash equilibrium values de ned js pair mixed nash equilibrium strategies bimatrix game function de ned solution stochastic game 
proof 
lemma qk qk lemma sequence converges js de ne js sequence fq converges easy show contraction mapping 
see true rewrite js 
pt contraction mapping js contraction mapping proceed show de ned xed point oft de nition wehave js js theorem sequence converges lemma sequence converges discussions want point convergence result depend sequence actions taken agent 
convergence result requires action tried state visited 
require agent agent agree nash equilibrium bimatrix game learning 
fact agent learn optimal value behavior assumption agent long agent observe agent immediate rewards 
second convergence depends certain restrictions bimatrix games learning 
required nash equilibrium operator usually contraction operator 
probably relax restriction proving nash equilibrium operator non expansion operator 
theorem littman convergence guaranteed 
issues addressed 
rst equilibrium selection problem 
exist multiple nash equilibria learning nash equilibrium strategy guarantee agent choose nash equilibrium 
combine empirical estimation agent strategy reinforcement learning nash equilibrium strategy 
issue related action choice learning 
multiagent reinforcement learning method converges requires nite trials 
learning agent choose myopic action kinds actions 
agent chooses action maximize current value approach called greedy approach 
drawback greedy approach agent may trapped local optimal 
avoid problem agent explore possible actions 
cost associated exploration 
conducting exploration agent gives better current reward 
intend design algorithm handle exploration exploitation tradeo stochastic games 
tucker balch 
learning roles behavioral diversity robot teams 
sen 
caroline claus craig boutilier 
dynamics reinforcement learning cooperative multiagent systems 
sen 
appear aaai 
edwin de jong 
non random exploration online reinforcement learning 
sen 
jerzy 
competitive markov decision process 
springer verlag 
leslie kaelbling michael littman andrew moore 
reinforcement learning survey 
journal arti cial intelligence research may 
michael littman 
markov games framework multi agent reinforcement learning 
proceedings eleventh international conference machine learning pages 
new brunswick 
mangasarian stone 
person nonzero sum games quadratic programming 
journal mathematical analysis applications 
john nash 
non cooperative games 
annals mathematics 
martin osborne ariel rubinstein 
course game theory 
mit press 
martin puterman 
markov decision processes discrete stochastic dynamic programming 
new york john wiley sons 
sandip sen editor 
collected papers aaai workshop multiagent learning 
aaai press 
richard sutton andrew barto 
reinforcement learning 
mit press bradford books march 
michael littman 
analysis value function reinforcement learning algorithms 
submitted review december 
ming tan 
multi agent reinforcement learning independent vs cooperative agents 
proceedings tenth international conference machine learning pages amherst ma june 
morgan kaufmann 
frank 
optimality equilibria stochastic games 
amsterdam netherlands centrum voor wiskunde en informatica 
christopher watkins peter dayan 
qlearning 
machine learning 
