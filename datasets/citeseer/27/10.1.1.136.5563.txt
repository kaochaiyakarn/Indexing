solving multiple instance problem axis parallel rectangles thomas dietterich richard lathrop tom lozano erez oregon state university corvallis department information computer science university california irvine ca pharmaceutical oyster pt 
blvd south san francisco ca mit artificial intelligence laboratory technology square cambridge ma multiple instance problem arises tasks training examples ambiguous single example object may alternative feature vectors instances describe feature vectors may responsible observed classification object 
describes compares kinds algorithms learn axis parallel rectangles solve problem 
algorithms ignore multiple instance problem perform poorly 
algorithm directly confronts multiple instance problem attempting identify feature vectors responsible observed classifications performs best giving correct predictions musk odor prediction task 
illustrates artificial data debug compare algorithms 
consider learning problem 
suppose keyed lock door supply room office 
staff member key chain containing keys 
key key chain open supply room door 
staff members supply room key opens supply room door staff members supply room key may open doors office door mail room door conference room door 
preprint submitted elsevier science november suppose lock smith attempting infer general required shape key order open supply room door 
knew required shape predict examining key key unlock door 
lock smith job difficult staff members uncooperative 
showing key key chains opens supply room door just hand entire key chain ask 
furthermore access supply room door try individual keys 
examine shapes keys key rings infer answer 
call kind learning problem multiple instance problem 
arises complex applications machine learning learning system partial incomplete knowledge training example 
traditional supervised learning problems learning system training examples form 
situation depicted 
object typically represented fixed length vector attribute values usually called feature vector 
machine learning applications complex situation shown arise 
learner incomplete information training example 
knowing training example represented feature vector learner knows example represented set potential feature vectors 
lock smith problem knowing key key chain opens supply room learning system knows keys key chain opens door 
early example learning situation arose meta dendral project 
meta dendral goal learn rules predict behavior molecules inside mass spectrometer 
mass spectrometer molecule high energy particles causes molecule break fragments 
fragments analyzed produce histogram mass charge ratio called mass spectrum 
main problem meta dendral predict bonds break 
molecule analogous key chain bond analogous individual key 
observing molecules resulting fragments meta dendral able formulate small number bond breakage rules accounted observed fragments 
similar situation arises explanation learning promiscuous domain theory 
input example domain theory construct multiple explanations account observed result 
learning task examine training examples find explanation account observed results 
case example process unknown result 
object instance instance instance fig 

supervised learning usual situation multiple instance situation 
key chain alternative explanation individual key 
multiple instance problem arises application domain drug activity prediction central importance 
domain input object molecule observed result measurement degree molecule binds target binding site 
binding site cavity pocket part larger molecule input molecule binds 
drug molecule bind tightly desired binding site poor drug molecule bind 
variant instances alternative conformations molecule alternative shapes molecule adopt rotating bonds 
shapes bind binding site produce observed result 
conformations typically effect binding 
learning task infer requirements observed drug activity 
directly analogous lock smith problem 
molecule key chain 
different shapes adopt conformations individual keys 
goal infer general shape required binding binding site opening lock 
drug activity measured ways 
settings laboratory assay measure kinetics binding reaction determine binding strength directly 
settings activity measured observed biological effect 
example musk odor prediction task described activity measured human subjects characterize chemical compound active inactive 
availability inactive molecules aspect drug activity problem extends lock smith problem 
inactive molecule know possible conformations shapes bind binding site 
lock smith problem analogous having key chains people businesses know keys key chains open supply room door 
set possible objects mi 
object mi observed result mi 
musk data labeled active inactive treat mi binary quantity mi active molecules mi inactive molecules 
function represents unknown process 
goal learning find approximation analyzing set training examples drawn labeled ordinary supervised learning usually represent object mi vector real valued features mi rn 
machine learning papers distinction objects feature vectors correspondence 
labeled training example form hv mi mi setting object mi may variant instances denoted mi mi mi variants represented usually distinct feature vector mi 
complete training example written mi mi mi mi words representation training example ambiguous machine learning algorithm overcome ambiguity 
assume complete set variants finite known learning algorithm 
property domains described observed result positive active drug design mass spectrometry variant instances produced positive result 
furthermore observed result negative variant instances produced positive result 
model introducing second function mi takes single variant instance produces result 
externally observed result mi defined follows mi 
short object mi predicted positive example exists feature vector mi variant instance predicted positive definition allows possibility variant predicted positive refer variants positive examples positive instances may produced positive result 
similarly refer variants negative example negative instances 
remainder goal machine learning algorithm construct approximation internal function hypothesis consistent set training examples classifies feature vector negative example negative classifies feature vector positive example positive 
call learning problem multiple instance problem training example represented multiple instances feature vectors 
goal demonstrate multiple instance problem important problem compare effectiveness general approaches solving problem case axis parallel rectangle bias appropriate 
begins describing application domain drug activity prediction multiple instance problem arises 
describe feature representation application bias appear axis parallel hyper rectangles 
consider general designs apr learning algorithms noise tolerant standard algorithm 
naive apr algorithm just forms smallest apr bounds positive examples 
explore version algorithm ignores multiple instance problem 
outside algorithm 
algorithm variation standard algorithm 
constructs smallest apr bounds positive examples shrinks apr exclude false positives 
shrinking process addresses multiple instance problem 
inside algorithm 
algorithm starts seed point feature space grows rectangle goal finding smallest rectangle covers instance positive example instances negative example 
necessary expand resulting apr statistical technique order get performance 
results show inside algorithm performs better 
evidence algorithm identify relevant features better outside algorithm 
results demonstrate standard algorithm performs worse argue multiple instance problem ignored considered design learning algorithms 
conduct research obtain results extremely valuable develop artificial data set mimics behavior real data 
valuable reasons real data sets study contained examples 
great danger overfitting data sets 
overfitting occur single run 
greater risk entire algorithm development process overfit data attempted improve cross validated accuracy 
artificial data set allowed develop debug data right answer known 
substantially improved efficiency research 
construct artificial data set forced carefully analyze real data sets order understand generated 
provided ideas new algorithms 
remainder organized follows 
describing application domain analysis data sets study 
analysis describe artificial data set 
algorithm designs compare performance artificial data 
runs artificial data help determine parameter values learning algorithms 
run learning algorithms real world data sets summarize results 
drug activity prediction algorithms described motivated task drug activity prediction 
background drugs small molecules binding larger protein molecules enzymes cell surface receptors 
potency drug determined degree binds larger target molecule 
drug molecules typically bind target molecules 
exploit variety weak interactions including hydrogen bonds van der waals attractions electrostatic charge interactions hydrophobic interactions 
right molecular shape conforms closely shape binding site enables van der waals attractions hydrophobic interactions large surface areas presents electronically active surface atoms near complementary binding site atoms enables electrostatic hydrogen bond interactions 
analogy drawn lock key key operate lock shape complementary shape lock 
goal drug activity prediction predict activity new synthesized molecules analyzing collection training examples consisting previously synthesized molecules observed activities binding binding site medical interest 
focusing expensive time consuming efforts chemists synthesizing promising candidate molecules accurate drug activity predictions yield large savings time money pharmaceutical companies 
greater benefit applying machine learning drug activity prediction guide process drug design 
chemists obtain dimensional model requirements drug activity able design better drugs 
shape binding site inferred ray crystallography guide drug design 
practical cases shape binding site unknown machine learning methods able provide dimensional shape hypothesis support drug design 
rational drug design cut years time required discover new drugs 
result drugs higher potency fewer side effects 
multiple instance problem multiple instance problem arises drug activity prediction choice representation drug molecules 
describe selected representation creates multiple instance problem 
binding strength largely determined shape drug molecules representation capture shape molecule 
unfortunately rotated bond fig 

shape molecule changes rotates internal bonds 
thin lines indicate molecular surface 
molecules adopt multiple shapes rotating internal bonds shown 
representation captures shape molecule produce multiple feature vectors shape changes 
combination angles rotatable bonds molecule defines conformation 
conformation potential energy determined interactions atoms making molecule 
analogous gravitational potential energy objects separated specified distance 
ordinary temperatures conformation molecule solution rapidly changing 
probability molecule adopt particular conformation depends exponentially potential energy conformation boltzmann distribution low energy conformations probable high energy conformations 
practice conformations molecule bind binding site conformations low energy lowest possible energy conformation molecule 
standard assumption conformations correspond local energy minima possible candidates binding 
molecule rotatable bonds usually expect find local minimum conformations 
fortunately fraction sufficiently low energy 
low energy conformations computed methods including monte carlo search bond angle space systematic bond angle search molecular dynamics simulates motions atoms newtonian mechanics 
remainder restrict attention low energy conformations 
low energy local minimum conformation create distinct feature vector input learning algorithm 
approaches drug activity prediction avoid problem 
approach employ representation invariant changes bond angles 
previous research drug activity prediction attempted representations notably methods derived success reported simple molecules families molecules having large amounts shared structure 
diverse molecules type studied methods successful 
approach employ shape oriented representation attempt guess advance conformation molecule 
method shown promise shape representation difficulty picking right conformations 
methods employ form shape representation include koehler schaefer richards snyder 
method confronts multiple instance problem directly elegant distance geometry approach 
unfortunately combinatorial explosions search space approach limit complexity binding site hypotheses constraints positions key atoms 
approach describe learn detailed constraints position entire molecular surface 
ray representation molecular shape shows representation employed capture shape molecules 
constructed set rays emanating origin sample space approximately uniformly 
extract features molecule molecule placed standard position orientation origin lies inside 
rays feature values measured 
feature value distance origin molecule surface 
computed molecular surface connolly method probe radius addition shape features computed domain specific features represented position designated atom oxygen atom molecular surface see 
determine standard position orientation molecule molecules aligned translation rotation 
alignments carried ad hoc algorithm superimposed atoms benzene rings molecules attempted place designated oxygen atom positions support formation hydrogen bond 
benzene ring planar member ring carbon atoms strong de localized bonding structure 
denoted hexagon alternating single double bonds 
molecules contains benzene ring 
fig 

schematic diagram ray representation rays shown 
thick curve indicates molecular surface 
ray representation sufficient molecules compact shape 
similar representations easily constructed molecules shapes long columns curved segments loops 
representation sufficient molecules studied 
note shape molecule changes conformation distances measured rays change 
different conformations represented distinct feature vectors 
note measured feature values locally correlated values measured adjacent rays quite similar 
suggests actual number features required characterize active molecules number relevant features probably substantially 
ray representation immediately suggests representation hypotheses 
suppose binding site requires surface molecule certain locations 
placing upper lower bound ray describe allowed positions molecular surface ray 
separated rays allowed positions ray independent surface patch molecule interacts different surface patch binding site 
bounds rays correspond axis parallel hyper rectangle dimensional feature space 
shows bounds sided sided require molecule stick certain distance lower bound ray require molecule stick far upper bound ray 
conditions reflect important domain interactions 
lower bound may require molecule extend far critical van der waals fig 

schematic diagram binding hypothesis represented bounds rays 
molecular surface satisfies bounds molecule predicted active 
rays sided bounds upper bound lower bound rays unconstrained irrelevant binding 
hydrophobic interactions binding site 
upper bound may prohibit molecule colliding wall binding site 
predicting musk strength develop learning methods drug activity prediction chose study problem predicting strength synthetic musk molecules 
problem attractive aspects non proprietary large number musk compounds similar non musk compounds published open literature identity shape binding site sites unknown molecules similar size composition active drug molecules 
aspect musk problem substantially different typical pharmaceutical problems musk strength measured qualitatively expert human judges drug activity usually measured quantitatively biochemical assays 
musk problem somewhat difficult 
surveyed literature musk compounds selected overlapping data sets musk molecules 
subjective nature test musk strength quite bit variation 
considered compounds appeared publications published musk judgements agreed 
data set contains musk molecules similar non musk molecules 
data set table musk data sets data set non total low energy conformations fig 

musk molecules contains non 
molecules shared data sets 
molecules identified bond graphs entered computer necessary search space possible conformations molecule find low energy minima 
data sets employed monte carlo search algorithm implemented program minimize amber force field 
data set subset resulting low energy conformations molecule chosen maximize pairwise root mean square distances atom positions pair conformations 
goal obtain small set diverse low energy conformations molecule 
data set selected molecules carefully 
certain molecules properly classified conformational searching thorough 
conformations produced retained 
despite larger number molecules second data set difficult learning algorithms contains conformations 
indirect evidence difficulties created multiple instance problem 
table summarizes data sets 
shows molecules training data set 
molecules entirely carbon hydrogen atoms exception oxygen 
previous authors concluded oxygen critical musk strength 
added oxygen features shape features 
features measure displacements oxygen atom designated point space 
fourth feature measures euclidean distance oxygen atom designated point 
features chosen axis parallel rectangle method representing binding requirements employed 
related research reported grew initial efforts pharmaceutical apply machine learning drug activity prediction 
subsequent produced apr neural network algorithm called compass improves algorithms reported 
advantage compass robust errors initial alignment molecules learning process compass automatically optimizes relative alignment molecules 
advantage compass handle activity prediction tasks activities continuous quantities reported active inactive classifications 
primary contribution reported demonstrates critical importance solving multiple instance problem shows solve problem hypotheses represented axis parallel rectangles 
axis parallel rectangles generally easier interpret neural networks expect new applications algorithms described 
data analysis musk data set application problem important analyze data assess biases appropriate gain insights help guide choice learning algorithms 
performed fairly detailed analysis musk data set section 
additional motive data analysis help design artificial data set properties similar musk data specify right answer 
extremely helpful algorithm debugging sensitivity testing 
critical helping reduce overfitting real musk data set 
artificial data sets proven useful development dna sequence assembly algorithms comparison learning algorithms 
began constructing hyper rectangle tightly contains data 
computed width bounds ray plotted histogram shown 
feature values measured 
note substantial variation typically nearly ray 
rays tight bounds correspond regions musk molecule benzene ring essentially count width ca apr bounds fig 

histogram width bounds apr tightly contains feature vectors musk data set variation data set 
shows actual lower upper bounds apr enclosing data apr enclosing positive feature vectors 
variant instances molecule included 
features sorted ascending order width positive apr bounds 
note small separation positive apr data apr suggests bounds single ray eliminate negative molecules 
shows enlarged view left bounds tight 
notice positive upper bounds give better separation data upper bounds region 
suggests features narrow bounds better discriminating positive negative examples features wide bounds 
shows kernel density estimate positive negative feature vectors feature typical features 
kernel density estimate constructed placing small gaussian observed feature value summing gaussians construct probability distribution 
gaussians standard deviation see distribution positive negative examples similar 
appears separate negatives positives placing bound 
note large central feature value ca feature number fig 

lower upper bounds apr enclosing positive feature vectors thin vertical lines apr enclosing feature vectors thick vertical lines indicate extension positive bounds 
features sorted width positive apr feature value ca feature number fig 

expanded view tightest bounds 
peak 
features central peaks peaks positive negative densities nearly coincide 
see difficult learning problem 
get crude idea hard exclude negative instances compute number bounds negative instance lies outside positive apr shows histogram number bounds exclude negative 
note negative probability density feature value positive density negative density fig 

kernel density estimate positive negative feature vectors feature musk data set count number feature values fall outside positive apr fig 

histogram number bounds negative instance excluded apr covers positive instances 
instances excluded positive apr excluded number excluding bounds distributed roughly exponentially mean 
molecule index positive molecules negative molecules feature value fig 

display multiple instances feature musk data set 
horizontal axis shows value feature instance 
vertical axis arbitrary numbering molecules positive molecules appear negative molecules 
molecule represented horizontal line vertical ticks 
ticks mark feature values multiple feature vectors representing single molecule 
horizontal line joins ticks 
horizontal dashed line separates negatives positives 
analysis far ignored multiple instance problem 
display helps visualize multiple instances molecule ray 
display see molecules exhibit wide range feature values molecules 
furthermore see bounds neighborhood cover instance positive molecule exclude instances negative molecules 
precisely guarantee instance positive molecule included bounds dimension set lower bound upper bound mini positive molecules maxj vd mi maxi positive molecules vd mi ranges variant instances molecule mi vd mi value feature variant instance mi call bounds minimax bounds dimension construct minimax bounds dimensions musk data set resulting minimax apr include positive molecules unfortunately 
different instances molecule chosen different dimensions 
prove lemma lemma apr covers instance positive molecules contain minimax apr proof 
suppose apr covered instance positive molecules upper bound minimax bound feature impossible definition exists positive molecule minimax upper bound equal smallest value feature instance molecule 
smaller value cover instances molecule argument applies lower bounds 
summarize data analysis follows 
distributions positive negative feature values similar approximately gaussian long tails 
explicitly consider multiple instance problem construct fairly tight bounds exclude negative instances 
take positive apr crude approximation true apr conclude negative instances excluded relatively dimensions mean 
artificial data set data analysis preceding section constructed artificial data set follows 
chose artificial correct apr forcing features sided bounds forcing remaining features irrelevant 
applied apr generate random feature values replace feature values musk data set 
number molecules feature vectors artificial data set musk data set 
precisely artificial apr denoted constructed follows 
data apr exactly includes feature vectors musk data set 
set bounds feature features chose width bounds random fraction uniformly width feature positioned interval width uniformly randomly bounds feature process constructing artificial feature vectors involved constructing gaussian probability distributions feature positive feature vectors negative feature vectors 
distributions identical means standard deviation negative gaussian larger standard deviation positive gaussian 
standard deviation positive gaussian chosen times width features times width remaining features 
mean chosen uniformly randomly lie standard deviation inside features remaining features 
note gaussian distribution non zero probability possible positive feature vector generated random procedure lie outside negative feature vector lie inside additional steps taken ensure artificial data consistent generating feature vectors positive molecule mi repeatedly generated features feature vector mi generated feature values lay bounds ensured instance positive molecule satisfied generating features vectors negative molecule mi determined bounds violated sampling random exponential distribution fitted data 
ensured repeated random generation feature values generated violated bounds fact lie outside shows gaussian kernel density estimate positive negative probability densities feature artificial data set 
note small amount negative density positive density estimated distributions nearly identical means 
surprising data quite difficult discriminate positive molecules negative molecules 
probability density feature value positive distribution negative distribution fig 

artificial data set 
positive negative probability densities feature constructing artificial data set guide development testing large variety learning algorithms described 
artificial data set developed musk data set risk fitted musk data sets artificial data set 
answer determined examining ways artificial data set accurate replication musk data sets 
certainly number features range feature values number conformations molecule faithfully reproduced artificial data set 
opportunities fitting musk data set 
musk data set range feature values number conformations molecule different little chance overfitting data set 
feature values artificial data set entirely different musk data set 
particular correlations features captured artificial data set 
real data set features highly redundant molecular surface change adjacent rays 
artificial data set feature value chosen independently correlation 
furthermore musk data sets features describe important oxygen atom artificial data set special features relating oxygen atom 
artificial data set know exists low dimensional axis parallel rectangle consistent data real musk data sets apr gfs elim count apr apr algorithms outside algorithms inside algorithms positive apr gfs positive apr select features exclude false positives gfs kde apr multiple instance cost function iterated discrimination gfs elim count apr standard apr fig 

relationships various axis parallel rectangle algorithms described 
boxes heavier lines mark best algorithm type 
believe little risk overfitting particularly musk data set doing experiments artificial data set 
learning algorithms learning algorithms compare performance artificial data set 
algorithm illustrates general approach constructing presence multiple instance problem 
help reader keep track various algorithms section gives derivation tree shows algorithms related 
schematic diagram multiple instance problem dimensions 
coordinate axes represent measured values features measured point ray intersects molecular surface 
unfilled shapes represent feature vectors active molecules 
filled shapes represent feature vectors inactive molecules 
points shape diamonds denote feature vectors variant instances molecule 
goal learning algorithms described section find rectangle includes unfilled point shape feature vector positive molecule include filled points feature vectors negative molecule 
noted addition algorithms described experimented large number variations methods 
method fig 

multiple instance learning problem 
unfilled shapes represent feature vectors active molecules filled shapes represent feature vectors inactive molecules 
points shape denote feature vectors molecule 
shown best representative chosen algorithms having fundamental approach 
standard apr algorithms axis parallel hyper rectangle viewed conjunction tests feature values 
simple algorithm designed analogy standard algorithm learning boolean conjunctions 
simply construct apr exactly covers positive feature vectors positive apr 
maximally specific conjunctive generalization positive instances 
call algorithm positive apr algorithm 
shows resulting positive apr solid line bounding box unfilled points 
apr treats feature relevant 
hypothesis domain feature values nearby rays highly correlated parts molecular surface involved binding 
obvious step choose subset bounds apr sufficient exclude negative instances 
analogous method described haussler 
process removing bounds apr best organized process fig 

elim count procedure identifying negative instance easiest exclude 
solid rectangle smallest rectangle includes unfilled points positive apr 
line included filled point indicates side solid box tightened exclude point 
number indicates unfilled points excluded box tightened 
dashed box indicates final apr produced elim count 
adding bounds apr covers entire feature space 
greedy algorithm considers bound positive apr chooses bound eliminates negative instances 
bound added solution negative instances eliminated process repeated negatives remain eliminated 
call gfs apr algorithm performs greedy feature selection 
difficulty algorithm positive apr may contain negative examples 
shows positive apr solid line bounding box unfilled points 
note filled shapes negative instances included apr greedy feature selection choose subset bounds eliminate negative examples covered positive apr eliminate negative instances apply greedy algorithm prior selecting relevant features 
negative instance count minimum number positive instances excluded apr order exclude negative instance 
counts shown small lines indicate side apr tightened order exclude negative instance 
example eliminate black circle top apr need eliminate white diamond upper right corner apr greedy algorithm iteratively chooses eliminate negative instance easiest eliminate requires eliminating fewest covered positive instances negatives eliminated 
resulting apr inner dashed box 
note example resulting apr covers instance positive example general need case 
instances positive example excluded greedy algorithm 
constructing apr apply greedy feature selection algorithm described obtain apr guaranteed false positives 
call gfs elim count apr algorithm eliminates negative instances counting number positive instances need eliminated performs greedy feature selection 
outside multiple instance apr algorithm modify gfs elim count algorithm address multiple instance problem 
merely counting number positive instances excluded order exclude negative instance consider excluding positive instances sense positive molecule positive instance covered apr implement defining cost excluding positive instance choosing exclude cheap positive instances necessary order exclude covered negative instance 
cost excluding positive instance mi molecule mi depend excluded positive instances molecule mi 
surviving positive instances cost excluding mi small subsequent decisions excluding instances mi 
furthermore surviving positive instances feature values frequently observed positive molecules survivors relevant instances instance mi eliminated 
variant instance mi isolated positive instances feature values similar probably instance included apr develop cost function incorporates properties employed estimate probability density dd surviving positive instances feature applied gaussian kernel density estimation estimate density 
small gaussian kernel centered value vd mi feature surviving instance mi positive molecule gaussians summed obtain probability density function dd 
notation dd mi indicates probability dd observing value vd mi feature mi mi surviving variant instances positive molecule mi 
cost eliminating instance mi feature defined gamma ix dd mi mi term gives low cost instances indexed molecule high probability density estimate dd 
final term mi measures degree instance isolated positive instances 
dd mi small instance isolated 
experiments ff 
instance remaining instance molecule receives cost high 
experimented cost functions worked slightly better 
apply algorithm elim count point choose negative instance cheapest eliminate eliminates inexpensive positive instances 
algorithm avoid eliminating covered instance positive molecule alternative expensive 
call algorithm gfs elim kde eliminates negative instances kernel density estimate kde positive instances 
drawback gfs elim kde quite expensive compute required kernel density estimates 
number features number instances positive molecule gamma total number positive negative instances respectively number times compute cost equation 
computational cost bounded compute kernel density estimates estimate process positive instance 
quite large negative instance may exclude positive instances dimensions 
average musk data set positive instances excluded dimension musk data set positive instances excluded dimension numbers reflect application heuristic avoid wasting effort bad features 
case means gamma computational cost approximately gamma immense 
musk data set approximately theta 
musk data set theta 
inside multiple instance apr algorithm alternative outside approach construct apr starting single positive instance growing apr expanding cover additional positive instances 
constructed somewhat complex algorithm approach 
call iterated discrimination algorithm basic procedures grow algorithm growing apr tight bounds specified set features 
discrim algorithm choosing set discriminating features analyzing apr expand algorithm expanding bounds apr improve generalization ability 
algorithm works phases 
phase grow discrim procedures iteratively applied simultaneously choose set discriminating features construct apr tight bounds features 
second phase expand procedure applied expand tight bounds 
describe procedures turn 
algorithm growing tight apr goal algorithm find smallest apr covers instance positive molecule 
define size apr sum widths bounds size ap gamma different optimization methods applied problem tested simulated annealing greedy algorithm backfitting algorithm 
describe greedy backfitting procedures 
optimization choosing initial seed positive instance 
greedy procedure grows apr series greedy steps 
greedy step identifies positive instance covered positive molecule added apr increase size 
apr expanded include positive instance 
procedure continued instance positive molecule covered 
surprisingly experiments resulting apr consistent covered instances negative molecules required algorithms 
backfitting algorithm extension greedy procedure employed projection pursuit method 
begins greedy algorithm choosing seed positive instance series greedy steps 
greedy step revisits previous decisions consider differently 
suppose algorithm just taken greedy step th decision 
id positive instance chosen step backfitting procedure revisits previous decisions previous decision revisited algorithm constructs apr covers fi gamma 
call apr gamma considers instances molecule identifies instance increase size gamma chosen instance replaces list choices algorithm 
backfitting algorithm choice 
repeated passes choices progress ceases decisions changed 
greedy step choose st positive instance 
choose initial positive instance select positive instance closest euclidean distance minimax apr defined earlier 
experiments musk data set artificial data set showed backfitting procedure smaller tighter greedy procedure simulated annealing method 
applied backfitting procedure experiments reported 
algorithm selecting discriminating features tight apr constructed applied select discriminating features 
employed greedy algorithm 
say feature tight apr strongly discriminates negative instance instance lies outside bounds apr feature instance lies bounds apr lies outside bounds feature feature 
definition strongly discriminate choose features iteratively selecting step feature strongly discriminates largest number negative instances 
negative instances removed consideration process repeated bounds selected negative instances excluded 
rationale margin apr experiments artificial data set irrelevant features discriminate training set negative instances small margins 
small values margin robustly identify relevant features 
various margins considered results relatively insensitive parameter long larger iterative selection positive instances discriminating features iterated discrimination algorithm alternates application algorithms follows 
backfitting algorithm applied construct tight apr bounds features 
subset features selected discriminating features 
backfitting algorithm applied construct tight apr time measures size apr discriminating features 
cause choose different positive instances different bounds 
feature selection procedure invoked narrow set discriminating features 
back forth loop continues converges typically requires iterations 
interesting compare behavior backfitting algorithm iteration behavior subsequent iterations 
iteration half features irrelevant backfitting algorithm trying minimize total size apr apr bounds tend wider irrelevant features relevant ones surface positive molecules variable rays 
irrelevant features discarded backfitting algorithm alter choices quite substantially 
subsequent iterations choice relevant features hardly changes 
result choice positive instances change algorithm converges rapidly 
expect generally true kind algorithm details section 
expanding apr improve generalization experiments artificial data set showed tight apr resulting previous methods generalize 
typically tight excludes positive instances test set negative instances 
problem apr constructed exactly big cover positive instance molecule training set bigger 
overcome problem turned kernel density estimation 
relevant feature tight apr apply kernel density estimation estimate probability positive instance satisfy bounds feature 
goal expand bounds high probability new positive instances fall inside apr algorithm controlled user specified parameters ffl ffl parameter specifies amount probability lie outside expanded bounds apr feature dimension 
parameter determines width gaussian kernel 
width kernel set positive instances centered apr bounds kernel density estimator conclude probability lay bounds 
positive instances normally distributed ffl 
relevant dimension apr algorithm computes width kernel expands lower upper bounds apr ffl probability lies lower bound kernel density estimate ffl probability lies upper bound 
tail kernel density estimate contains ffl probability bound changed 
illustrated 
tall vertical lines show initial tight bounds apr medium height vertical lines show expanded bounds moved outward probability tail density estimate exactly ffl 
note lower bound expanded considerably positive instances close tight lower bound consequently larger tail 
upper bound hardly expanded positive instances quite sparse near tight bound 
experimental results results artificial data set table shows results running apr algorithms artificial data set 
addition show results decision tree algorithm backpropagation neural network algorithm ignored multiple instance problem treated positive instances positive examples 
evaluate generalization performance constructed additional molecules 
new molecule generated randomly choosing replacement molecule musk data set replacing feature vectors probability feature value fig 

expanding apr bounds kernel density estimation musk data set 
small vertical lines mark value feature positive feature vector 
curve shows estimate probability density values 
tall vertical lines show bounds initial tight apr thicker medium vertical lines show expanded bounds 
artificial feature vectors 
proportion positive negative molecules number feature vectors molecule artificial test set artificial training set 
algorithms require user specify various parameters chose parameters gave best test set performance 
iterated discrimination best parameter values ffl 
backpropagation conducted systematic search parameters adaptive solutions computer processors bit mode 
best parameters employed single hidden layer sigmoid units learning rate momentum weight updates applied patterns trained epochs 
results clearly show iterated discrimination algorithm far superior algorithms tested 
performance iterated discrimination statistically significantly different algorithms binomial test difference proportions 
algorithms distinguished statistically 
surprising positive apr gfs elim count apr algorithms poorly addresses multiple table artificial data set performance 
training set molecules test set molecules 
true false false true algorithm positives negatives positives negatives errors correct iterated discrim gfs elim count apr pruned gfs elim kde apr positive apr gfs positive apr ffl hidden units learning rate momentum epochs table artificial data set 
success identifying correct relevant features 
true false false true algorithm errors correct iterated discrim apr gfs positive apr gfs elim kde apr gfs elim count apr positive apr stance problem 
somewhat surprising gfs elim kde apr algorithm badly explicitly consider multiple instance problem deciding positive instances remove apr experimented variations gfs elim kde apr algorithm 
example variation required negative instance discriminated positive instances margin safety 
hurt performance false negatives increased decreasing false positives 
variation explored employ various methods expanding shrinking apr feature selection 
worked gfs elim kde algorithm described 
advantage artificial data measure generalization performance 
table shows algorithm choosing correct features 
see iterated discrimination algorithm relative width positive apr bounds feature index relevant features irrelevant features fig 

bound widths positives apr artificial data set 
vertical bar separates relevant features irrelevant features 
successful identifying relevant features 
algorithms positive apr correctly identify half relevant features iterated discrimination features 
data conclude superior performance iterated discrimination apr primarily explained ability find relevant features 
turn appears result approach finding tight apr covers positive instance positive molecule 
relevant features tight apr tighter irrelevant dimensions 
contrast bounding box positive examples starting point algorithms wide bounds relevant irrelevant dimensions 
shows widths bounds positive apr percentage widths bounds feature vectors 
features true relevant features 
clearly tighter bounds irrelevant features wide bounds indistinguishable irrelevant features 
contrast shows widths bounds tight apr constructed backfitting algorithm iteration iterated discrimination 
relevant features clearly identified 
noted real musk data sets analogous plot separate features nearly clearly 
relative width apr bounds feature index relevant features irrelevant features fig 

bound widths apr artificial data set 
vertical bar separates relevant features irrelevant features 
drawback iterated discrimination algorithm need choose values ffl 
shows performance artificial data set varies function parameters 
general increases best choice ffl decreases 
expect large values mean gaussian kernel smaller probability tails distribution decreases 
sufficiently widen apr bounds desired probability tails ffl decrease 
peak performance iterated discrimination artificial data obtained ffl 
unfortunately parameter values confidently extrapolated musk data set artificial data generated gaussian distributions match gaussian kernel density estimator 
need look musk data set choose best parameters 
summarize analysis artificial data set follows artificial data pose difficult learning problem 
backpropagation perform data set 
algorithms ignore multiple instance problem perform 
inside approach constructing apr performs best 
result superior ability identify discriminating features 
correct epsilon tau tau tau fig 

performance iterated discrimination artificial data set function parameters ffl results musk data set turn musk data sets 
table summarizes performance algorithms data measured fold crossvalidation 
confidence interval shown column 
see top methods statistically indistinguishable 
probably reflects difference artificial data set features relevant irrelevant musk data sets features probably measuring relevant parts molecular surface measurements redundant 
artificial data set chosen parameter values iterated discrimination backpropagation optimize cross validated accuracy 
backpropagation exceedingly difficult train network 
parameters worked artificial data set musk data set 
believe primarily reflects fact bias standard multilayer sigmoid units appropriate learning axis parallel rectangles 
note apr methods performed better non apr methods 
real musk data table demonstrate clearly artificial data table importance ignoring multiple instance problem 
top algorithms table methods explicitly attempt solve multiple instance problem 
artificial table fold cross validation performance musk data set molecules true false true false algorithm positives negatives positives negatives errors correct iterated discrim gfs elim kde apr gfs elim count apr gfs positive apr positive apr pruned ffl ffl 
hidden units learning rate momentum epochs 
data set best algorithm iterated discrimination errors test molecules 
note non multiple instance algorithms positive apr high false positive rates 
expected mistakenly classify feature vector molecule positive molecule classified positive 
partly explains particularly poor performance backpropagation 
additionally algorithms advantage knowing hypotheses take form axis parallel rectangles 
backpropagation represent apr choose appropriate hypotheses domain 
table gives insight behavior iterated discrimination algorithm 
fold fold cross validation table shows number relevant dimensions set selected positive instances changes 
note iterations performed algorithm fold 
choice relevant dimensions essentially unchanged iteration 
shows critical importance heuristic selecting relevant dimensions 
choice relevant dimensions big influence positive instances chosen backfitting algorithm active variants 
second iteration algorithm changes choice positive molecules 
shows sensitivity iterated discrimination choice parameter values 
things note 
larger values give better performance 
second values ffl giving peak performance quite wide especially 
compare table musk data set 
fold cross validation 
fold left half table indicates change number relevant dimensions starting iteration iterated discrimination algorithm 
right half table indicates instances selected backfitting changed iteration 
values iteration show number positive molecules training set total number selected instances 
change relevant dimensions change selected instances iteration iteration fold gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma see unfortunately artificial data set accurately predict point peak generalization suggests smaller values ffl correspond wider apr bounds 
similarly see musk data set accurately predict point peak generalization musk data set 
values ffl shall choose musk data set 
wide plateau obvious choice 
wide plateau isn clear value ffl choose 
thing consider musk data set positive molecules positive molecules musk data set 
mean tight apr produced backfitting narrower bonds musk data set musk data set 
suggests choose smaller values ffl produce wider apr bounds 
choose ffl smallest value giving peak performance musk data set 
note method choosing parameters musk data set experiments musk data set risks overfitting data sets share molecules 
musk data set correct epsilon tau tau tau fig 

performance iterated discrimination musk dat set function ffl 
conformations shall see best parameters turn worked best musk data set believe degree contamination small 
results musk data set table shows final results running algorithms large musk data set 
shown confidence interval percentage correct classification algorithm 
encountered extreme difficulty training multi layer sigmoid net data set 
parameter values worked musk data set artificial data set resulted effective training data set squared error decrease behaved 
parameters employed hidden units epochs learning rate momentum chosen performing cross validation training set fold fold cross validation 
strictly fair parameter choice repeated training set fold cross validation 
case performance poor 
relative ranking algorithms musk data set gfs positive apr dropped backpropagation 
gap iterated discrimination algorithms increased statistically significant 
table fold cross validation performance musk data set molecules true false true false algorithm positives negatives positives negatives errors correct iterated discrim gfs elim kde apr gfs elim count apr positive apr gfs positive apr frequent class pruned ffl hidden units learning rate momentum epochs apr algorithms gfs positive apr outperform algorithms backpropagation 
performed worse percent correct basis trivial strategy guessing frequent class difference statistically significant 
shows cross validated performance iterated discrimination various values ffl parameters 
note values ffl chosen basis musk data set give peak performance 
peak performance attained parameter values ffl ffl ffl 
matches best performance reported apr neural network algorithm data set parameter values chosen cross validation 
performance robust wide range parameter values 
shows visualization binding hypothesis learned entire musk data set applied classify conformation molecule named true musk molecule names drawn 
stereo pairs provide dimensional picture viewed stereo viewer converging eyes point page images fuse come focus 
bounds satisfied correctly classified musk molecule 
shows apr applied classify conformation molecule correct epsilon tau tau tau fig 

performance iterated discrimination musk dat set function ffl 
named non musk 
bounds satisfied 
violated bounds identify region molecule stick far 
conformations molecule classified non musk correctly classified non musk 
shows apr surrounding bond graph structure diagram conformation molecule 
helps visualize regions molecule important musk activity 
see preponderance bounds testing shape molecule opposite oxygen atom 
region hydrophobic bulk previous chemical studies emphasized importance region musk activity 
notable absence apr bounds middle left side molecule suggests molecules shape freedom area 
note tight bounds needed test position rings axis orthogonal plane 
probably reflects fact conformations aligned aromatic rings superimposed little variation musk non musk molecules axis 
displays employed chemists suggest changes molecules improve binding 
example molecule satisfy apr adding group molecule region bounds 
change non musk musk 
fact molecule 
fig 

musk data set apr binding hypothesis applied classify conformation molecule true musk 
dashed lines depict connolly surface molecule 
heavily shaded area bottom surface oxygen atom 
line segments depict apr bounds selected rays index number ray 
upper frames give stereo pair showing front view lower frames show back view molecule 
apr bounds satisfied 
discussion standard problem learning axis parallel rectangles learning algorithm solve problems select relevant features set bounds features 
multiple instance problem adds third difficulty algorithm choose positive instances treat genuine positives 
fig 

musk data set apr binding hypothesis applied classify conformation molecule non musk 
upper frames give front view lower frames give back view molecule 
note bounds front view back view violated 
experiments see setting bounds features easiest problems 
iterated discrimination postponed setting exact bound values determined relevant features genuine positive instances 
hand essential coordinate choice relevant features positive instances 
example data shown stopped iterated discrimination iteration resulting performance substantially worse 
fig 

musk data set apr structure diagram bond graph conformation molecule 
apr bounds shown single stereo pair 
oxygen atom bottom image 
artificial data set critical helping debug understand algorithms 
particular note performance gfs apr algorithm musk data set indistinguishable iterated discrimination artificial data revealed worse selecting relevant features 
borne musk data set experiments gfs elim kde performed worse 
similar behavior observed gfs elim count apr algorithm 
strongly recommend artificial data set approach algorithm development evaluation 
significance results drug design limited factors 
algorithms address class qualitative data 
effect drugs measured qualitative response usually quantitative measures drug efficacy human subjects laboratory assays 
chemists primarily interested algorithms predicting real valued 
mentioned jain dietterich lathrop jain bauer chapman describe apr neural network method called compass quantitative activity predictions 
second algorithms assume conjunction conditions satisfied binding 
case 
example drugs medical importance antagonist drugs job prevent natural compound binding blocking access binding site 
different antagonist drugs may operate fitting different binding sites binding different modes general binding site 
easy conceive extensions algorithms reported handle multiple binding modes broader applicability drug design 
important direction research 
third limitation algorithms discussed placing molecule standard position orientation respect rays 
classes molecules difficult choose standard position orientation 
highly flexible molecules diverse sets molecules difficult 
dietterich jain lathrop lozano erez jain dietterich lathrop describe method called dynamic permits relative orientations molecules change slightly learning 
comparisons state art methods jain bauer chapman show dynamic permits accurate robust activity predictions 
conducted initial experiments dynamic apr gives response provide quantitative signal needed control 
attempts define signal succeeded 
need dynamic raises interesting direction research 
considered called discrete problem input object represented finite set possible instances 
applications exhibit problem applications space possible instances continuous infinite 
alternative positions orientations molecules provide example continuous multiple instance problem 
related problems arise optical character recognition 
multiple instance problem important problem arises realworld tasks training examples ambiguous single example object may alternative feature vectors describe feature vectors may responsible observed classification object 
particular problem arises drug activity prediction training example molecule observed binding strength feature vector describes possible shape conformation molecule 
binding strength result single shape fitting binding site usually feature vectors properly represents active molecular shape 
representation molecular shape representation binding hypotheses 
feature space hypothesis corresponds rectangle apr 
general approaches designing apr algorithms ignore multiple instance problem start bounding apr positive examples shrink attending multiple instances outside approach start single point apr grow considering multiple instances inside approach 
experiments clearly show inside approach best 
ignoring multiple instance problem apr algorithms neural networks decision trees gives quite poor performance 
outside approach great difficulty identifying relevant features apr multiple instance problem ignored apr algorithms generally perform neural networks decision trees task principle networks trees represent 
illustration importance choosing appropriate bias inductive learning algorithms 
drug activity prediction multiple instance problem important subjects research 
particularly interesting issue design multiple instance modifications decision trees neural networks popular machine learning algorithms 
authors wish george providing initial impetus consider shape representation musk domain 
early involvement conducted 
research initiated pharmaceutical 
people participated development project including barr bauer david chapman roger jain brad katz john park teresa webster 
authors teammates mike ross chief technical officer help bringing project successful 
data sets described deposited irvine machine learning database repository 
aha generalizing case studies case study proceedings ninth international conference machine learning aberdeen scotland 
structure activity relationships human applied science publishers london 
yu 
new journal chemistry 
review methodology ed quantitative structure activity relationships drugs academic press new york 
buchanan mitchell model directed learning production rules waterman hayes roth eds pattern directed inference systems 
academic press new york 
chang internal coordinate monte carlo method searching conformational space journal american chemical society 
connolly solvent accessible surfaces proteins nucleic acids science 
cramer iii patterson comparative molecular field analysis 

effect shape binding carrier proteins 
journal american chemical society 
distance geometry molecular conformation john wiley sons new york 
dietterich flann inductive approach solving imperfect theory problem technical report department computer science oregon state university corvallis oregon 
dietterich jain lathrop lozano erez comparison dynamic tangent distance drug activity prediction advances neural information processing systems morgan kaufman san mateo ca 
dietterich michalski inductive learning structural descriptions evaluation criteria comparative review selected methods artificial intelligence 
engle burks artificially generated data sets testing dna sequence assembly algorithms genomics 
fehr new aromatic musk design synthesis helvetica acta 
friedman stuetzle projection pursuit regression journal american statistical association 
ham structure activity studies musk pattern recognition chemical senses 
fujita ae oe ss analysis 
method correlation biological activity chemical structure journal american chemical society 
leo constants correlation analysis chemistry biology wiley interscience new york 
haussler quantifying inductive bias ai learning algorithms valiant learning framework artificial intelligence 
haussler learning conjunctive concepts structural domains machine learning 
jain dietterich lathrop chapman bauer webster lozano erez compass shape machine learning tool drug design journal computer aided molecular design 
jain bauer chapman compass predicting biological activities molecular surface properties 
performance comparisons benchmark chemistry 
jain harris park quantitative binding site model generation compass applied multiple targeting ht receptor chemistry 
koehler schaefer molecular shape analysis structure activity relationship investigation inhibitors archives biochemistry biophysics 
kuntz structure strategies drug design discovery science 
lindsay buchanan feigenbaum applications artificial intelligence organic chemistry dendral project mcgraw hill new york 
murphy aha uci repository machine learning databases machine readable data repository department information computer science university california irvine ca 
structure activity studies musk pattern recognition chemical senses 

richards structure activity relationships molecular similarity matrices med 
chem 

silverman density estimation statistics data analysis chapman hall london 
simard le cun denker efficient pattern recognition new transformation distance hanson cowan giles eds advances neural information processing systems morgan kaufmann san mateo ca 
burks gram simulating solving single digest partial restriction map problem comp 
applic 
biosci 

program distributed columbia university new york 
st wold multivariate data analysis experimental design biomedical research ellis west eds progress chemistry elsevier science publishers amsterdam 
theimer davies musk odor molecular properties agr 
food chem 

snyder pseudo receptor modeling new concept dimensional construction receptor binding sites receptor research 
weiner case singh jr weiner new force field molecular mechanical simulation nucleic acids proteins journal american chemical society 
weiner nguyen case atom force field simulations proteins nucleic acids journal computational chemistry 

