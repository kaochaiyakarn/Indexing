mit massachusetts institute technology artificial intelligence laboratory learning deictic representation sarah finney leslie pack kaelbling tim oates ai memo april massachusetts institute technology cambridge ma usa www ai mit edu reinforcement learning methods operate propositional representations world state 
representations intractably large generalize poorly 
deictic representation believed viable alternative promise generalization allowing existing reinforcement learning methods 
experiments learning deictic representations reported literature 
explore effectiveness forms deictic representation na propositional representation simple blocks world domain 
find empirically deictic representations worsen performance 
conclude discussion possible causes results strategies effective learning domains objects 
report describes done artificial intelligence lab massachusetts institute technology 
research sponsored office naval research contract telegraph telephone part ntt mit collaboration agreement national science foundation graduate research fellowship 
humans speak probably think world objects chairs pencils doors 
individual objects features sets objects stand various relations 
artificial agents successfully interact world need represent reason objects features relations hold 
interested agents embedded inherently relational environments physically embodied robots learn act 
learning algorithms appropriate 
representations appropriate 
answers questions interact 
order logic fol immediately comes mind possible representation due compactness ability generalize objects quantification 
develop learning algorithms truly relational representations done generally inductive logic programming specifically dzeroski relational reinforcement learning 
moving complex mechanisms important establish existing techniques break domains 
document attempts apply relatively standard reinforcement learning techniques apparently relational domain 
strategy ai planning example relational domains 
specify hopefully small finite domain objects describe domain large set propositions representing possible instantiation properties relations domain 
method may inappropriate learning give representational support generalization objects 
starting agre chapman gaining momentum debate problem late see winter issue ai magazine culmination debate arguments viability deictic representations relational domains 
representations point objects world name role ongoing activity arbitrary identifiers typically case order representations 
despite fact deictic representations lack quantification allow generalization fol affords attendant computational complexity 
importantly existing reinforcement learning algorithms 
deictic representations appear attractive systematic investigation circumstances result better performance compared naive propositional representations underlying domain relational 
presents results set experiments forms deictic representations compared propositional representations respect learning problem blocks world domain 
results reinforcement learning algorithms described neuro dynamic programming chapman kaelbling algorithm 
remainder organized follows 
section briefly reviews reinforcement learning impact choice representation nature learning problem 
section reviews deictic representations impact 
section describes set experiments aimed understanding deictic representations advantageous section reviews related concludes 
reinforcement learning representations term reinforcement learning rl denote set problems set algorithms 
reinforcement learning problems framed terms agent environment rewards 
agent observe environment affect environment actions 
time agent takes action receives scalar reward signal indicates taken particular action current state environment 
task facing agent learn mapping observations histories observations actions maximizes reward receives time 
mapping learned agent called policy denoted observation action value probability action observation observations called states symbol may may correspond underlying states environment 
agent perceptions reliable correspondence states environment environment said fully observable 
observations correspondence states multiple distinct states appear agent 
environments said partially observable 
yield difficult learning problems optimal action value conflated states differs 
different ways represent states advantages disadvantages 
discussion concrete consider blocks world block different colors block table 
way representing states atomically atom unique configuration blocks 
possible learn example action state yields reward leads state 
representation simple sense single number represent entire state learning implemented tables indexed states 
price paid simplicity high 
atomic representations intractably large small numbers objects 
blocks possible assignments colors blocks ways stack blocks 
second way determine states similar way generalize learn example action best take state probably best take state 
factored representations problems atomic representations 
example state represented set propositions currently true block red block block 
distance states measured terms number bits different encodings 
generalization possible learning algorithms example neural networks decision trees 
term propositional representation denote naive relational domain 
example blocks propositions specify colors blocks propositions specify blocks stacked 
propositional representations afford opportunities generalization refer objects name block block unable perform important type generalization 
domains objects type identity object unimportant long particular property 
example goal blocks world agent pick green blocks want learn rule expressed order logic green pickup 
block green top block picked 
rule applies regardless number blocks environment size propositional equivalent rule polynomial function number blocks 
propositional version contains disjunction blocks cover existential quantification disjunct contains conjunction blocks cover universal quantification 
rl learning domains states represented expressions fol 
partly dealing uncertainty fol problematic 
existing largely assumed atomic propositional state representations combining rl fol required novel data structures learning algorithms 
unfortunate wealth theory algorithms apply standard representations 
deictic representations potential bridge gap allowing generalization afforded fol representations amenable solution face uncertainty existing algorithms 
deictic representations subject section 
deictic representations word deictic derives greek means able show 
linguistics introduced artificial intelligence agre chapman building ullman visual routines 
deictic expression points meaning relative agent uses context 
book am holding door front examples deictic expressions natural language 
important classes deictic representations derived directly perception action relations agent objects world 
agent directed perception sensibly speak think object am fixated 
agent pick things name am holding 
primitive deictic names just suggested compound deictic expressions directly perceptible relations 
example speak object top object am fixated color object left object am fixated 
benson describes interesting system synthesizing complex deictic expressions demand 
central deictic representations notion marker attentional resource control agent 
agent mark objects attend move markers shift foci attention 
perceptual information features relations available objects marked 
objects effectively invisible agent 
markers influence activity marked objects manipulated agent 
ways perceptions actions agent structured general framework deictic representations 
extreme agent marker object environment effectively complete propositional representation 
extreme agent markers limited perceptual window world 
environment may fully observable markers partially observable standpoint agent 
points spectrum characterized focused deixis fd wide deixis wd 
fd special marker called focus number additional markers 
perceptual information available object marked focus 
perceptual actions moving focus moving markers object marked focus moving focus object marked markers 
objects marked focus manipulated agent 
wd just fd perceptual information available objects marked regardless marked focus markers 
number practical differences focused deixis wide deixis 
information state environment available fd wd 
means space required fd representations wd representations mean agent learn long sequences perceptual actions gather sufficient information state environment behave effectively 
may require access short state histories learner effect remember seen done past 
starting hypothesis deictic representations ameliorate problems full propositional representations 
advantages include focused partial observability agent gets see objects marked objects typically include ones play role current activity relevant reward ultimately received 
irrelevant aspects world easily observed learning easier need generalize 
passive generalization despite lack quantification deictic representations able generalize objects 
example know cup am holding doesn matter cup cup cup 
biological plausibility objects named role play agent current activity 
conjectured deixis plays role way humans represent world 
deictic representations especially agent significant control perceives substantial degree partial observability exchange focusing things lose ability see rest 
mccallum observed thesis partial observability edged sword may help learning obscuring irrelevant distinctions hinder obscuring relevant ones 
thing missing literature systematic evaluation impact switching propositional deictic representations respect learning performance 
sections reports set experiments exploration 
experimental domain blocks world learning agent exists simulated blocks world learn hand remove red blue blocks green block block may lifted 
choice problem domain arbitrary 
whitehead ballard introduced pioneering deixis relational domains 
developed lion algorithm deal perceptual aliasing partial observability resulted deictic representation avoiding aliased states 
mccallum domain demonstrate number markers required solve problem reduced keeping short history observations 
martin domain motivate evaluate algorithm learning policies generalize initial configurations 
whitehead ballard blocks world whitehead ballard blocks world differs ways 
whitehead representation markers perceptual information action 
representation marker agent move arbitrarily marker gaining perceptual information performing actions 
whitehead algorithm required separate perception action markers allow agent avoid ambiguous states restriction apply algorithms 
need separate markers action perception combined give agent advantage naturally focusing areas world relevant current activity see section 
second whitehead action set includes primitives looking top bottom stack 
gave agent basic primitive action set moving focus attention hopes learn put meaningful search actions needs task 
hope general action set arrive learner able solve diverse set tasks domain 
lastly dealing ambiguous states trying avoid whitehead ballard added history perceptual space 
assumes agent able differentiate ambiguous states looking back history tasks interested property true 
mccallum blocks world mccallum blocks world tested utile suffix memory algorithm whitehead ballard differed addition history differentiating ambiguous state having marker 
world contains additional memory markers allow agent solve complex tasks mccallum moved arbitrarily 
mccallum world includes whitehead learning watching technique agent acts teacher policy probability 
direct exploration influence agent decisions 
believed agent able arrive optimal policy advise learning 
action set mccallum slightly general specific whitehead pick green block task 
blocks world experimental setup described differs previous empirical deictic representations important ways 
goal understand conditions representation deictic propositional better 
want systematically compare utility propositional deictic representations evaluate learning algorithm designed operate representation 
second tuned perceptual features training paradigm task 
despite apparent simplicity reinforcement learning domain perceptual features finely tuned specific task faced learning agent frequent examples correct actions provided teacher 
tried develop set perceptual features reasonable agent arbitrary task blocks world provide information agent observations rewards 
deictic representations deictic name object conceived long string holding idea implemented set markers 
example agent focusing particular block block block looking agent fixes marker block fixes attention block block looking 
experiments developed flavors deictic representation 
case focused deixis focus marker additional marker 
agent receives perceptual information relating focused block color red blue green table block agent hand 
addition agent identify additional marker bound block left right focus 
focused deixis scenario imposes narrow focus attention requires deliberate care deciding attend 
second case wide deixis focused deixis additional information 
perceptual information color spatially proximal markers available marked blocks just focused block 
addition information available spatially proximal blocks pairs marked objects identity markers right marker returned 
information blocks left marker redundant case 
action set deictic agents move focus direction focus moved top stack table 
focus moved side block height focus falls top stack side 
focus color block specified color focus land randomly 
pick action succeeds focused block non table block top stack 
put put block top stack focused 
marker focus marker move specified marker coincide focus 
focus marker marker move focus coincide specified marker 
full propositional representation fully observable propositional case arbitrary names assigned block including table blocks 
agent perceive block color red blue green table location block indicated index horizontal position table name block block question 
addition single bit indicates hand holding block 
propositional agent action set pick block action succeeds block non table block top stack 
put put block top stack hand 
move hand left right action fails agent attempts move hand edge table 
choice algorithms experiments took approach model free value reinforcement learning algorithms goal understand strengths weaknesses domain 
discuss alternative methods 
longer observe state deictic representation include history order problem markovian 
additional information requirement renders observation space large explicit representation value function look table 
required learning algorithms approximate value function 
include history order problem markovian 
chose learning neural network function approximator known neuro dynamic programming ndp baseline common successful method reinforcement learning large domains feature representation 
hoped improve performance function approximators history selectively algorithm mccallum tree algorithm 
initial experiments tree settled modified version simpler algorithm 
neuro dynamic programming neural networks approximate value state action pair 
look reward distributions determine observation bits relevant predicting reward divide state space accordingly 
believed advantage neuro dynamic programming respects 
hypothesized able learn faster virtue ability discern parts observation vector irrelevant task ignore decreasing amount distracting information 
initial experiments tree settled modified version simpler algorithm 
description neuro dynamic programming baseline comparison neuro dynamic programming ndp algorithm described bertsekas tsitsiklis 
ndp setup layer neural network action action set 
number input nodes network equal length current percept vector plus past pairs actions percept vectors specified history parameter number hidden nodes number input nodes divided 
single output unit value indicates approximate value corresponding action 
learning rate discount factor 
sarsa update values 
observed sarsa led stable results learning partial observability domain 
description algorithm algorithm tree structure determine elements state space important predicting reward 
tree initialized just root node state distinctions whatsoever fringe nodes beneath distinction 
statistics kept root node fringe nodes immediate discounted reward received agent lifetime statistical test kolmogorov smirnov test performed determine distinctions fringe worth adding permanently tree 
distinction useful fringe deleted distinction nodes added leaves new fringe created beneath new leaf nodes 
values stored updated leaf nodes tree 
domains fully observable modify original algorithm 
allow fringe include historical distinctions 
trial agent history experiences match leaf node substantially complicates bookkeeping 
solve started episode observations past history window allowed agent consider making splits 
added bit observation vector indicated agent alive time observation appended buffer necessary number non alive experiences trial 
rest observation vectors filled arbitrarily 
partial observability domains sarsa arriving values generating sampled values determining split important 
lastly statistic original algorithm 
kept vectors immediate step discounted reward state represented leaf fringe node state divided outgoing action 
vectors compared vectors parent node examining possible split children percept vectors include features possible values 
experiments learning rate discount factor 
issues algorithm main reasons decided algorithm 
kolmogorov smirnov ks test executing fringe test 
objective fringe test determine set nodes representing additional distinction values differ significantly parent leaf node 
basically fringe test compares utility making additional distinction vs making distinction 
fringe test ks test looks values instance originally stored parent compares values instances tentatively stored particular fringe node 
values instance calculated qit rit uit qit value instance recorded time rit immediate reward received time discount factor uit utility tree node corresponding instance recorded time step 
sampling instances values directly value calculated tree node point summary instances values statistical test attempt find significant differences account uncertainty learning problem 
really kinds uncertainty learning incremental state representation algorithms 
uncertainty values transition probabilities states true underlying mdp 
uncertainty values transition probabilities states approximated state representation encoded tree 
statistical test described tries get uncertainty problem types uncertainty 
feel ks test way may answering right question 
second reason values calculated fringe test 
fringe dynamically created time fringe test carried 
understood text mccallum thesis steps fringe test 
particular leaf node expand fringe fixed depth combination actions observations 

deposit leaf instances corresponding fringe 
update fringe node values accordingly 

compare value distribution instances fringe node parent keep new distinction distributions differ threshold try new combination 
difficulty sampled value instance fringe depends utility state represented node tree 
possible utility node depends turn node newly created fringe 
problem statistics nodes tree updated fringe test 
may result incorrect values value distributions compared 
problem especially acute early stages learning tree relatively small likelihood outside node depending fringe significant 
correct approach recompute values entire tree fringe test compare original values parent new values fringe 
unfortunately extremely computationally expensive 
resulting performance hit severe render impractical purposes 
modifications algorithm described essentially tree purposes selective perception 
major distinction remains tree requires experience world price greater computational complexity remembers historical data uses estimate model environment transition dynamics uses model choose state split 
experiments propositional representations yield large observation spaces full observability 
deictic representations yield small observation spaces partial observability 
learning easier 
question section explores empirically experiments different blocks world starting configurations 
comparing sizes state action spaces representation compute size underlying state space full propositional deictic representations 
size state space computed state space configurations ways name blocks configurations am awake hand full ways arrange blocks ways arrange blocks color am awake true false hand full true false 
furthermore full propositional case deictic case ways name blocks blocks ways name blocks blocks markers blocks blocks colors 
number configurations 
blocks blocks configurations 
underlying state space full propositional ground states blocks blocks 
underlying state space deictic case markers ground states blocks blocks 
compute size observation spaces full propositional deictic representations 
full propositional case agent observes boolean valued am awake hand full features 
furthermore block observes block color possible values red blue green table 
name block underneath values blocks position stack values 
size observation space number ground states dramatically roughly blocks roughly blocks 
deictic cases agents observe boolean valued am awake hand full features 
furthermore wide case agent observes marker marked object color values red blue green table 
identity marker underneath values markers marker identity marker right values markers marker note observation space corresponds size needed look table includes combinations percepts possible 
marked object agent hand values true false 
focused case agents observes focused object color values red blue green table 
identity marker underneath values non focus markers marker identity marker right values non focus markers marker identity marker left values non focus markers marker identity marker values non focus markers marker focused object agent hand values true false 
size observation spaces constant domains size focused deictic observation space wide deictic observation space 
action set deictic representations see section change additional blocks constant actions 
full propositional action set see section requires pickup action block possible actions blocks blocks 
experimental setup learning task pick green block covered red block 
domain blocks consists block long table green block red block 
second domain blocks additional blue block distractor 
blocks world configurations blocks blocks 
table blocks positioned blocks 
agent receives reward picks green block reward takes action fails attempting move hand left right edge world attempting pick table reward 
performance learning algorithms measured follows 
configuration blocks assignment names blocks randomized fully observable case assignment markers blocks randomized deictic cases 
agent allowed take actions learning 
time agent picked green block original configuration restored names markers reassigned 
epoch actions state learning algorithm frozen agent took additional actions total accumulated reward measured 
results plotted follows 
different lengths optimal action sequences deictic propositional agents scale result reflect optimal performance respect agent 
data point agent computed accumulated reward testing steps adding maximum penalty obtained consistently executing worst possible action worst possible score dividing maximum possible reward obtained optimal policy steps 
curve figures consistently optimal agent score trial consistently terrible agent score trial 
total reward trial scaled initial results learning curves algorithm representation 
figures show ndp propositional non deictic learner performed best 
algorithm focused deictic wide deictic full propositional number training iterations blocks domain millions total reward trial scaled full propositional focused deictic wide deictic ndp number training iterations blocks domain millions learning curves algorithms blocks domain 
original hypothesis additional block distract propositional learner represent block deictic learner 
case ndp performance non deictic learner better blocks hypothesis gap certainly get smaller blocks 
fact point addition sufficiently blocks learning curves cross deictic learner reduced observation space gave upper hand 
figures tell story 
task configuration size observation space additional block learning performance ndp second configuration degraded deictic representation 
section explores question 
total reward trial scaled algorithm focused deictic wide deictic full propositional number training iterations blocks domain millions total reward trial scaled full propositional ndp wide deictic focused deictic number training iterations blocks domain millions learning curves algorithms blocks domain 
furthermore despite hypothesis perform better ndp discovered opposite true 
clearly agent algorithm learns slowly agent ndp distractor block added 
importantly domains agent able reach solution optimal cases tree allowed get large trees plotted figures capped nodes including fringe nodes 
section investigates trees need large 
discussion sections dissect counter intuitive results explanations 
comparing deictic propositional representations ndp initially hypothesized deictic representation compact observation space ability focus interesting parts state space advantage propositional representation 
furthermore conjectured advantage grow pronounced amount state information increased 
figures show clearly hypothesis true 
really understand trade deictic non deictic representations need understand problem gets harder addition distractor blocks 
starting hypothesis blocks added exploration problem difficult deictic learner 
space observables stays configurations true space consists block configurations locations markers grows quickly 
furthermore deictic actions different non deictic actions conditional blocks markers bound 
deictic learner direct access focus location increase complexity important inaccessible part state space reduce likelihood arriving solution exploration 
set quantify representation long take random agent stumble solution 
metric quantity called mean time goal tries quantify difficulty exploration problem counting number actions taken random agent arrive goal 
measured mean time goal way representation non deictic wide deictic focused deictic set agent take actions randomly 
increased number distractor blocks time placing additional blue block side original stack green red 
various block configurations time goal measured number steps random agents took reach goal 
results averaged trials agent configuration 
left graph shows number steps taken randomly average number steps reach goal random walk focused deictic non deictic number additional blue blocks average number steps reach goal random walk focused deictic non deictic non deictic longer action sequence number additional blue blocks mean time goal various representations number distractor blocks increased 
agents start blocks configuration blue blocks successively added side original stack 
arrive solution deictic case non deictic case dramatically 
exploration task gets harder extra block little wonder learning performance worsens 
set experiments involve trying pin choice action sets affects exploration difficulty 
experiment investigated length optimal action sequence affects exploration difficulty 
original action set deictic agent required steps best case reach goal 
non deictic agent hand required steps reach goal 
number actions required reach goal tested non deictic agent task required steps reach goal 
task pick green block initially covered red blocks 
expected see longer optimal action sequence mean time goal non deictic case grow deictic case 
right graph shows time goal grew somewhat faster certainly order magnitude deictic case 
concluded merely length optimal action sequence caused difficulty problem increase deictic case 
average number steps reach goal random walk focused deictic non deictic deictic reduced action set number additional blue blocks average number steps reach goal random walk deictic full propositional modified deictic number additional distractor blocks mean time goal representations number distractor blocks increased different action sets 
second experiment investigated size action set number possible actions affects exploration difficulty 
removed actions deictic action set believed distracting deictic agent focus marker marker focus 
compared random deictic agent new action set called reduced action set consisting actions random agent original non deictic action set consisting actions 
left side shows result experiment 
rate growth slightly slower go order magnitude 
experiment changed deictic non deictic action sets order zero characteristic deictic action set making exploration difficult 
created new action sets called modified action sets 
idea modify non deictic action set suffered thought difficult characteristic deictic action set division focusing block picking original non deictic action set hand pick block essentially tasks 
actions modified action set 
deictic interestingly modified action set similar set mccallum blocks world experiments 
focus direction move focus left right 
version action stricter original set 
block specified direction focus land focus stay put 
look color focus green red blue table colored block 
pick top pick block top stack marked focus exact block focused 
put put block held top stack marked focus 
non deictic move hand block move hand stack containing specified block 
move hand direction move hand left right 
pick top pick block top stack underneath hand 
put put block held top stack underneath hand 
right graph shows result 
surprisingly agent time goal growing deictic opposite happened modified action sets time goal grew roughly rate original non deictic set 
changed 
main difference deictic action sets follows modified action set pick top action reduces great deal dependence actions focus location 
focus location crucial agent number distractor blocks increases part state space grows exponentially 
removing strict dependence focus location increases likelihood agent exploration progress 
modified action set probability choosing successful actions increases total reward trial scaled probability choosing harmful actions decreases 
please see appendix informal analysis relative various action sets 
follow learning experiments see modified action sets show deictic agents learn comparably full propositional agent blocks slightly faster full propositional agent blocks 
ndp full propositional wide deictic focused deictic number training iterations blocks domain total reward trial scaled ndp focused deictic wide deictic full propositional number training iterations blocks domain learning curves representations blocks deictic agents modified action set 
compare right hand sides figures 
wants deictic representation set actions chosen carefully reasonable action set bias provided 
unsatisfying note order render deictic action set tractable remove dependence deictic representations initially appealing focus attention 
option providing necessary structure hierarchical temporally extended actions 
clever action hierarchy away partial observability exists level atomic actions 
hierarchical temporally extended actions provide appropriate level decisions virtue abstracting away exploratory see section providing nearly deterministic action space 
comparing neuro dynamic programming order agent learn policy optimal cases initial results showed grow tree unreasonably large nodes 
trees keep growing reaching natural limiting state 
reason avoid running memory add arbitrary cap size trees 
observing results developed theory trees need large 
tree needs nodes distinguish states different respect reward received 
states different value represented distinct leaf tree 
policy followed changes learning process tree grow large include leaves states distinct values policies followed learning 
simple example problem clear 
section considers simple maze shown 
start goal example 
state start state state goal state 
unnecessary distinctions simple maze agent observe wall cardinal direction environment partially observable states look identical require different actions reach goal 
time step agent receives new percept vector may move directions 
agent rewarded reaching goal penalized step lead goal penalized slightly attempting move wall 
trial algorithm distinctions way 
split distinguishes states south clear blocked 
separates states states state observed agent trial restarted 
subgroups split east blocked separating states states 
large reward received going east state leaf states contains policy going east 
shows tree point learning process 
clearly policy optimal learned tell states apart 
east clear states policy east south clear states distinctions east blocked states policy south south blocked states policy west example tree splits 
distinction previous action node states shown 
glance looks meaningless distinction additional information yield previous state 
know state knowing action merely gives information preceding state went east state states previously south action means states previously utility knowledge subsequent policy decisions depend current state 
east clear states policy east south clear states east blocked states policy south distinctions action north possible states probably policy west south blocked states action east possible states probably policy west action south possible states probably policy north action west possible states probably policy north example tree splits 
examining policy effect agent split distinction begins sense 
agent state policy says go east 
explores go state 
state oscillate west east exploration leads agent south state 
agent visits state generally just performed east action 
hand agent state performed south west action 
splitting previous action current policy disambiguates high probability states yields reasonable policy goes north state shown 
distinction history policy may lead algorithm distinctions attempt fully represent value function new policy 
example executing policy shown fact need different values state depending visiting second time 
trees larger need required store value function optimal policy 
experiment confirm explanation large trees fix policy agent allow tree grow 
expected obtain trees contain unnecessary splits representing value function fixed policy 
avoid making occasional bad split due statistical nature problem underlying cause large trees 
note problem certainly exhibited tree 
sample policy may complicate trees 
problem growing unreasonably large trees pomdps difficult address 
fundamentally kind arms race complex tree required adequately explain values current policy 
new complex tree allows complex policy represented requires complex tree represent values 
believe reason wasn encountered serious problem previous problems require addition history order differentiate states enormous explosion names state see add history 
furthermore mccallum tree new york driving task required history trees terribly large 
reason terminated training arbitrarily states trees grown larger training continued 
case mccallum remarks trees larger anticipated fact trees larger better performing hand written trees comparison 
possible way solve tree size problem adopting action critic approach alternating developing policy learning value function policy trying tack value functions tree 
explored 
redundant leaves notable reason trees growing large pomdps 
ability characterize current state terms historic actions observations learning algorithm frequently comes multiple perceptual characterizations correspond underlying world state 
instance set states described focus green block looked described focus green block looked course clear multiple action observation sequences indicate single underlying state algorithm 
data gets states doomed build sub tree underneath 
leads conclude actual identification underlying world dynamics probably prerequisite effective value learning pomdps 
approaches converting inherently relational problem propositional successful long run 
na grows size square number objects environment worse severely redundant due arbitrariness assignment names objects 
deictic approach fatal flaws relatively generic action set leads hopelessly long early trials 
intermediate rewards ameliorate assigning intermediate values attentional states particularly difficult 
additionally inherent dramatic partial observability poses problems model free value reinforcement learning algorithms 
saw best performance ndp fixed window history necessary amount history increases ndp able select relevant aspects swamped huge input space 
saw section tree growing algorithms susceptible problems induced interactions memory partial observability estimates values 
change approach higher level 
strategies consider deictic propositional representation forgo direct value reinforcement learning 
alternative value learning direct policy search affected problems partial observability inherits problems come local search 
applied learning policies expressed stochastic finite state controllers blocks world domain 
methods appropriate parametric form policy reasonably known priori probably scale large open ended environments 
strategy apply pomdp framework directly learn model world dynamics includes evolution hidden state 
solving model analytically optimal policy certainly intractable 
online state estimation module endow agent mental state encapsulating important information action observation histories 
reinforcement learning algorithms successfully learn map mental state actions 
drastic approach give propositional representations want deictic expressions naming individual objects real relational representations 
important early done relational reinforcement learning showing relational representations get appropriate generalization complex completely observable environments 
states represented conjunctions relational facts tilde rt system induce logical regression tree tests order expressions internal nodes predicts values leaves 
internal nodes contain variables generalize object identities 
applying inductive logic programming techniques tilde rt example stochastic domains unclear tilde rt cases 
driessens adapted algorithm tilde rt system allowing relational reinforcement learner incremental 
promising 
show problems encountered tree systems committing optimal split early forced duplicate useful subtrees splits effect despite relational representation 
furthermore relational domains complexity learning problem shifts query generation mechanism problem propositional domains proposed splits generally just attribute line 
ultimately deal generalization objects relational representations deal partial observability learning models world dynamics 
plan continue pursuing program indirect reinforcement learning learning model doing state estimation relational representations deictic names objects world 
acknowledgments funded office naval research contract telegraph telephone part ntt mit collaboration agreement national science foundation graduate research fellowship 
informal analysis different action sets figures informally analyze difficulty action sets original non deictic original deictic modified deictic move space state configurations goal 
column shows configurations blocks seen agent way optimal policy goal 
column lists available actions actions available action set see sections description action sets move agent strictly forward trajectory 
list forward moving actions approximate probability randomly selecting actions possible actions action set 
full column lists available actions move agent strictly backward goal probability randomly selecting actions 
note probabilities listed approximate intended mainly illustrate differences action sets 
take home message original deictic action set likelihood randomly choosing right actions advance low 
compare likelihoods full propositional action set modified deictic action set 
reason exploration difficult original deictic action set 


world configuration likelihood advancing likelihood regressing start probability advancing actions likelihood regressing actions likelihood pickup red hand left hand right put hand left hand right pickup green pickup red put trajectory non deictic agent original action set 
advancing actions likelihood regressing actions likelihood likelihood advancing likelihood regressing world configuration advancing actions likelihood regressing actions likelihood likelihood advancing likelihood regressing world configuration 
start probability 
look green look table look red put focus marker look red focus focus marker focus left focus right 
start probability 
focus marker look green pickup look green look table focus focus left focus right 
focus right focus left focus look table focus left put focus right look table trajectory deictic agent original action set 
pickup look red focus marker advancing actions likelihood regressing actions likelihood likelihood advancing likelihood regressing world configuration advancing actions likelihood regressing actions likelihood likelihood advancing likelihood regressing world configuration 
start probability 
look green put look red focus left focus right look green look table look red focus left focus right 
start probability 
pickup top look table look green pickup top look table focus left focus right 

focus left focus right look red pickup top put focus look table 
put look green focus focus left focus right look table trajectory deictic agent modified action set look table philip agre david chapman 
pengi implementation theory activity 
proceedings sixth national conference artificial intelligence 
baird 
residual algorithms reinforcement learning function approximation 
th international conference machine learning 
dana ballard mary polly rajesh rao 
deictic codes embodiment cognition 
behavioral brain sciences 
scott benson 
learning action models reactive autonomous agents 
phd thesis stanford university 
dimitri bertsekas 
dynamic programming optimal control 
athena scientific belmont massachusetts 
volumes 
dimitri bertsekas john tsitsiklis 
neuro dynamic programming 
athena scientific belmont massachusetts 
david chapman leslie pack kaelbling 
input generalization delayed reinforcement learning algorithm performance comparisons 
proceedings international joint conference artificial intelligence sydney australia 
kurt driessens jan ramon hendrik blockeel 
speeding relational reinforcement learning incremental order decision tree learner 
european conference machine learning 
dzeroski de raedt blockeel 
relational reinforcement learning 
proceedings fifteenth international conference machine learning pages 
morgan kaufmann 
saso dzeroski luc de raedt kurt driessens 
relational reinforcement learning 
machine learning 
sridhar mahadevan 
hierarchical memory reinforcement learning 
th advances neural information processing systems 
tommi jaakkola michael jordan satinder singh 
convergence stochastic iterative dynamic programming algorithms 
neural computation november 
henry kautz bart selman 
planning satisfiability 
th european conference artificial intelligence 
lane leslie pack kaelbling 
nearly deterministic abstractions markov decision processes 
th national conference artificial intelligence 
appear 
mario martin 
reinforcement learning embedded agents facing complex tasks 
phd thesis universitat de catalunya barcelona spain 
andrew mccallum 
reinforcement learning selective perception hidden state 
phd thesis university rochester rochester new york 
andrew mccallum 
instance utile distinctions reinforcement learning hidden state 
proceedings twelfth international conference machine learning pages san francisco ca 
morgan kaufmann 
nicolas meuleau kee kim leslie pack kaelbling anthony cassandra 
solving pomdps searching space finite policies 
manuscript submitted uai 
stephen muggleton luc de raedt 
inductive logic programming theory methods 
journal logic programming 
richard sutton 
open theoretical questions reinforcement learning 
th european conference computational learning theory 
john tsitsiklis benjamin van roy 
analysis learning function approximation 
ieee transactions automatic control 
shimon ullman 
visual routines 
cognition 
steven whitehead dana ballard 
learning perceive act trial error 
machine learning 
williams 
simple statistical gradient algorithms connectionist reinforcement learning 
machine learning 

