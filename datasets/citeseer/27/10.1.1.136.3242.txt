rainforest framework fast decision tree construction large datasets johannes gehrke raghu ramakrishnan venkatesh ganti department computer sciences university wisconsin madison raghu cs wisc edu classification large datasets important data mining problem 
classification algorithms proposed literature studies shown far algorithm uniformly outperforms algorithms terms quality 
unifying framework decision tree classifiers separates scalability aspects algorithms constructing decision tree central features determine quality tree 
generic algorithm easy instantiate specific algorithms literature including cart fact id extensions sliq sprint quest 
addition generality yields scalable versions wide range classification algorithms approach offers performance improvements factor sprint algorithm fastest scalable classification algorithm proposed previously 
contrast sprint generic algorithm requires certain minimum amount main memory proportional set distinct values column input relation 
current main memory costs requirement readily met workloads 
supported ibm corporate fellowship research supported ibm 
permission copy fee part material granted provided copies distributed direct commercial advantage vldb copyright notice title publication date appear notice copying permission large data base endowment 
copy republish requires fee special permission endowment 
proceedings th vldb conference new york usa classification important data mining problem ais :10.1.1.41.6931
input database training records record attributes 
attributes underlying domain totally ordered called ordered attributes attributes underlying domain ordered called categorical attributes 
distinguished attribute called class label categorical attribute small domain 
denote elements domain class label attribute class labels term class label clear context 
remaining attributes called predictor attributes ordered categorical nature 
goal classification build concise model distribution class label terms predictor attributes 
resulting model assign class labels database testing records values predictor attributes known value class label unknown 
classification wide range applications including scientific experiments medical diagnosis fraud detection credit approval target marketing 
classification models proposed literature 
overviews classification methods see wk mst 
decision trees especially attractive data mining environment reasons 
due intuitive representation easy assimilate humans 
second constructed relatively fast compared methods mar sam 
accuracy decision tree classifiers comparable superior models lls han 
restrict attention decision tree classifiers 
area decision tree classification exist large number algorithms construct decision trees called classification trees terms interchangeably 
algorithms machine learning statistics community main memory algorithms today databases general larger main memory ais :10.1.1.41.6931:10.1.1.41.6931
approaches dealing large databases 
approach discretize ordered attribute run algorithm discretized data 
discretization methods classification take class label account discretizing assume database fits main memory qui fi maa dks 
catlett cat proposed sampling node classification tree considers studies datasets fit main memory 
methods partitioning dataset subset fits main memory considered chan stolfo cs cs method enables classification large datasets studies show quality resulting decision tree worse classifier constructed complete database account 
framework scaling existing decision tree construction algorithms 
general framework call rainforest reasons closes gap limitations main memory datasets algorithms machine learning statistics literature scalability requirements data mining environment 
main insight careful analysis algorithms literature knowledge algorithms including qui cart mag fact lv id extensions qui qui qui fay sliq sprint mar mra sam quest ls access data common pattern described 
data access algorithms scale size database adapt gracefully amount main memory available restricted specific classification algorithm 
aspect decision tree classification addressed extensively statistics machine learning 
framework applied algorithms literature results scalable version algorithm modifying result algorithm 
evaluate quality resulting decision tree affected framework concentrate scalability issues 
rest organized follows 
section formally introduce problem decision tree classification describe previous database literature 
section introduce framework discuss encompasses previous 
section scalable algorithms construct decision trees section results detailed performance evaluation 
conclude section 
decision tree classifiers problem definition decision tree model data encodes distribution class label terms predictor attributes 
directed acyclic graph form tree 
root tree incoming edges 
node exactly incoming edge zero outgoing edges 
node outgoing edges call leaf node call internal node 
lots trees choose grow fast rain forest 
happen 
leaf node labeled class label internal node labeled predictor attribute called splitting attribute 
edge originating internal node predicate associated involves splitting attribute set predicates outgoing edges internal node non overlapping exhaustive 
set predicates non overlapping conjunction predicates evaluates false 
set predicates exhaustive disjunction predicates evaluates 
call set predicates outgoing edges internal node splitting predicates combined information splitting attribute splitting predicates called splitting criteria denoted 
internal node lete fe set outgoing edges fq set predicates edge ei associated predicate qi 
define notion family tuples node respect database family root node decision tree set tuples non root node parent qp predicate edge ep family node set tuples tuple qp evaluates true 
informally family node set tuples database follows path root classified tree 
path root leaf node corresponds classification rule wherep conjunction predicates edges class label node ways control size classification tree 
bottom pruning algorithm mra phases phase growth phase deep tree constructed 
phase pruning phase cut back avoid overfitting training data 
topdown pruning algorithm rs phases interleaved stopping criteria calculated tree growth inhibit construction parts tree appropriate 
concentrate tree growth phase due data intensive nature time consuming part decision tree construction mar sam 
tree pruned topdown bottom orthogonal issue 
previous database literature agrawal introduce agi interval classifier database indices efficiently retrieve portions classified dataset sql queries 
method scale large training sets sam :10.1.1.104.152:10.1.1.104.152
fukuda fmm construct decision trees twodimensional splitting criteria 
algorithm produce rules high classification accuracy scalability design goals 
addition decision tree longer intuitive representation tree dimensional splits node 
decision tree classifier mar called sliq designed large databases uses memory data structure grows linearly number tuples training database 
limiting data structure eliminated sam introduced sprint scalable classifier :10.1.1.104.152
sprint works large datasets removes relationships main memory size dataset 
sprint builds classification trees binary splits gini index decide splitting criterion controls final quality decision tree application mdl principle ris mra 
decide splitting attribute node algorithm requires access ordered attribute sorted order 
conceptually node decision tree sort ordered attribute required 
sprint avoids sorting node creation attribute lists 
attribute list la attribute vertical partition training database tuple entry la consists projection class label record identifier 
attribute lists created algorithm sorted preprocessing step 
tree growth phase internal node splits distributed children 
tuple vertically partitioned attribute lists attribute list needs distributed separately 
distribution attribute list performed hash join attribute list splitting attribute record identifier duplicated attribute list establishes connection parts tuple 
hash join attribute list read distributed sequentially initial sort order attribute list preserved 
morimoto developed algorithms decision tree construction categorical predictor variables large domains emphasis improve quality resulting tree 
rastogi shim developed public scalable decision tree classifier top pruning rs 
pruning orthogonal dimension tree growth techniques easily incorporated schema 
discussion think sprint prix eat meal world class restaurant 
sprint runs minimal amount main memory scales large training databases 
comes drawbacks 
materializes attribute lists node possibly size database possible create attribute list categorical attributes optimization 
second large cost keep attribute lists sorted node tree connection vertically separated parts tuple record identifier costly needs performed 
size hash table proportional number tuples large 
sprint pays significant price scalability 
show section obser nature decision tree algorithms enable speed sprint significantly cases 
return restaurant analogy techniques section allow sample rainforest crunch tm ice cream restaurant paying just ordered 
emphasis research machine learning statistics community improving accuracy classifiers 
studies performed determine algorithm highest prediction accuracy smt bu dbp cm mst 
studies indicate algorithm uniformly accurate datasets studied 
mehta show quality studies mra mar indicate accuracy decision tree built sprint uniformly superior 
concentrated developing unifying framework applied decision tree algorithms results scalable version algorithm modifying result 
scalable versions algorithms produce exactly decision tree sufficient main memory available run original algorithm complete database main memory 
carry restaurant analogy 
step techniques section allow pick different restaurant day eat little want pay order 
rainforest framework introduce known greedy top decision tree induction schema 
show schema refined generic rainforest tree induction schema detail separation scalability issues quality concerns achieved 
concluding section overview resulting design space algorithms section 
decision tree algorithms build tree top way root node database examined best splitting computed 
recursively non root node examined crit computed 
known schema top decision tree induction example specific instance schema binary splits shown mar 
schema shown 
thorough examination algorithms literature shows greedy schema refined generic rainforest tree induction schema shown 
decision tree algorithms including cart fact id extensions sliq sprint quest proceed generic schema know algorithm literature adhere 
remainder denote cl representative classification algorithm 
note node utility predictor attribute possible splitting attribute examined independent predictor attributes information class label distribution distinct attribute value sufficient 
define avc set predictor attribute node projection input node partition classification algorithm cl output decision tree rooted top decision tree induction schema node algorithm cl apply cl number children create children ck best split partition dk ci di endfor endif rainforest refinement predictor attribute call cl find best partitioning avc set endfor cl decide splitting criterion tree induction schema refinement class label counts individual class labels aggregated 
define avc group node set avc sets node 
acronym avc stands attribute value 
note size avc set predictor attribute node depends number distinct attribute values number class labels 
main difference greedy top schema subtly refined rainforest schema isolates important component 
avc set allows separation scalability issues classification tree construction algorithms decide splitting criterion consider main memory requirements step rainforest schema shown 
lines avc sets predictor attribute needed main memory time argument procedure cl find best partitioning 
total main memory required lines maximum size single avc set 
addition algorithm cl stores predictor attribute result procedure cl find best partitioning input procedure cl decide splitting criterion size statistics negligible 
line statistics collected lines evaluated procedure cl decide splitting criterion main memory requirements step minimal 
lines distribute tuples partition page open file needed 
preceding analysis insights rainforest schema trivial observation long find efficient way construct avc group node scale classification algorithm cl adheres generic rainforest schema 
consider size sa ofthe avc set predictor attribute node note sa proportional number distinct attribute values attribute size family real life datasets expect avc group root node fit entirely main memory current memory sizes highly avc set individual predictor attribute fits main memory 
assumption avc group root node fits memory imply input database fits memory 
avc group compressed representation reconstructed avc group avc group contains aggregated information sufficient decision tree construction 
section calculate example numbers avc group root node generated synthetic data generator introduced agrawal ais designed model real life data :10.1.1.41.6931:10.1.1.41.6931:10.1.1.41.6931
maximum memory size avc group generated datasets megabytes 
current memory sizes megabytes home computers believe corporate data mining environment avc group root node fit main memory single avc set root node fit memory 
depending amount main memory available cases distinguished 
avc group root node fits main memory 
describe algorithms case sections 
individual avc set root node fits main memory avc group root node fit main memory 
describe algorithms case section 
individual avc sets root fit main memory 
case discussed full 
understanding rainforest family algorithms useful keep mind steps carried tree node generic schema 
avc group construction avc group exist node considered read order construct avc group 
involves scan input database materialized partition superset 
need construct avc group 

choose splitting attribute predicate step uses decision tree algorithm cl scaled rainforest framework knowledge decision tree algorithms choices examining avc sets node 

partition children nodes read entire dataset write records partitioning child buckets splitting criterion chosen previous step 
sufficient memory build avc groups children time optimization 
state precondition processing behavior tuple send crit computed children nodes allocated root parent state send sent child fill root node parent avc group updated write root parent appended partition root node parent avc group updated appended partition undecided processing takes place dead crit computed split children processing takes place algorithms section differ primarily utilize additional memory third step deal insufficient memory hold avc group step 
comparing size avc group node attribute lists created sprint sam typically smaller single attribute list avc set size proportional number distinct values columns number records outperform sprint factor primary design goal rain forest framework outperform sprint provide general framework scale broad range decision tree classification algorithms literature :10.1.1.104.152:10.1.1.104.152
reason techniques sprint straightforwardly extend broader range algorithms data management sprint designed enable efficient sequential access ordered attributes sorted order 
decision tree algorithms exhibit access pattern cart implemented data management sprint 
decision tree algorithms id qui gid exhibit sequential access pattern scaled approach 
algorithms section algorithms cases listed 
algorithms rf write rf read rf hybrid require avc group root node avc group individual node tree fits main memory assume common case discussed section 
remaining algorithm algorithm rf vertical works case single avc set fits memory complete avc group fit 
scalability splitting criterion selection orthogonal rainforest schema dwell issues dealing quality resulting decision tree 
order describe algorithms precisely introduce notion state node possible states send fill write undecided 
state node determines tuple processed list states preconditions processing behavior shown states processing behavior 
node created state set undecided mentioned call node new node 
node state dead called dead node 
algorithm rf write algorithm rf write assume avc group root node fits main memory 
algorithm rf write works follows scan database construct avc group algorithm cl applied children created 
additional scan database tuple written partitions 
algorithm recurses turn partition 
remainder paragraph describe algorithm rf write detail 
state set scan database 
avc group constructed scan 
algorithm cl called avc group argument 
assume cl splits attribute partitions 
algorithm rf write allocates children nodes sets state state child write additional pass tuple read processed tree crit applied sent child ct node ct state write appended ct partition 
scan partition child node ct consists ct 
algorithm applied partition recursively 
level tree algorithm rf write reads entire database twice writes entire database 
algorithm rf read basic idea algorithm rf read read original database writing children nodes 
point avc groups new nodes fit main memory read original database times time constructing simple analysis assumes tree balanced 
precisely level tuples belong families nodes level read twice written 
dead nodes tree set tuples processed level necessarily constitute input database 
avc groups unexamined subset new nodes tree 
precisely step algorithm rf read state root node set scan database crit computed 
children nodes fc created 
suppose point main memory hold avc groups children nodes fc inmemory 
address problem size estimation avc groups section case need write partitions ci algorithm rf write 
scan construct avc groups children simultaneously set state change state newly allocated child ci build second scan avc groups nodes ck simultaneously main memory 
scan algorithm cl applied memory avc group child node ci ci 
ci splits children nodes allocated state set send ci state set dead 
note far scans original database construct levels tree 
proceed way level tree long sufficient main memory available hold avc groups new nodes level 
suppose arrive level sufficient memory hold avc groups new nodes memory 
case divide set new nodes groups gi gi gj avc groups nodes group gi fit memory 
group processed individually states nodes gi changed undecided fill scan database construct avc groups scan splitting criteria computed 
gl groups level processed proceed level tree 
note level gl scans database necessary 
increasing usually number nodes level tree usually main memory requirements collective avc groups nodes level grow 
algorithm rf read increasing number scans database level tree 
efficient splitting algorithms apply bottom pruning case families pure leaf nodes large usually known advance 
splitting algorithms prune tree top fay rs approach viable solution 
included algorithm rf read completeness marks design spectrum rainforest framework parents algorithm rf hybrid described section 
think important practice due restrictions usability 
algorithm rf hybrid combining algorithm rf write algorithm rf read gives rise algorithm rf hybrid 
describe simple form rf hybrid paragraph refine version 
rf hybrid proceeds exactly rf read tree level reached avc groups new nodes fit main memory 
point rf hybrid switches rf write algorithm rf hybrid creates partitions scan database partition partitions 
algorithm recurses node complete subtree rooted version rf hybrid uses available memory efficiently rf write require increasing number scans database lower levels tree rf read 
improve simple version algorithm rf hybrid observation assume arrive tree level avc groups new nodes fit main memory 
algorithm rf hybrid switches rf read rf write partitioning pass available main memory 
tuple read processed tree written partition new information concerning structure tree gained pass 
exploit observation follows select set construct avc groups main memory writing partitions nodes partitioning pass algorithm cl applied memory avc groups nodes splitting criteria computed 
concurrent construction avc groups nodes advantage 
node avc group constructed consider recursion algorithm rf hybrid crit known saved scan partition immediately proceed second scan construct avc groups children due concurrent construction avc groups nodes save node scan partition 
choose 
save node scan partition maximize sum sizes families nodes restricting factor size main memory node maintain avc group main memory 
formulate problem follows node associated benefit size associated cost size avc group maintained main memory 
assume estimates sizes avc groups nodes 
address problem size estimation avc groups section formulation preceding paragraph choice instance knapsack problem gj 
instance knapsack problem consists knapsack capacity set items item state precondition processing behavior tuple fill root node parent avc group updated projection class label written file root node parent avc group updated appended partition projection class label written temporary file description states modified processing behavior algorithm rf vertical associated cost benefit 
goal find subset items total cost subset exceed capacity knapsack maximizing sum benefits items knapsack 
knapsack problem known np complete gj 
decided modified greedy approximation finds packing half benefit optimal packing works practice 
call greedy algorithm modified considers item largest benefit separately special case necessary get stated bound respect optimal solution 
output greedy algorithm subset new nodes afford construct avc groups memory ii benefit number saved maximized 
note greedy algorithm addresses problem selecting optimal set new nodes construct avc sets memory 
extension algorithm rf hybrid consider writing partitions nodes consider extension research 
algorithm rf vertical algorithm rf vertical designed case avc group root node fit main memory individual avc set fits memory 
presentation rf vertical assume loss generality predictor attributes fa avg large avc sets individual avc set fits main memory avc sets attributes fit memory 
denote remaining predictor attributes fav amg class label attribute limited presentation special scenario ease explanation discussion easily extended general case 
rf vertical proceeds similar rf hybrid process predictor attributes special way node write temporary file reconstruct avc sets attributes 
normal processing attributes completed read times avc set constructed turn 
node tree tuple database algorithm rf vertical processing tuple node slightly changed states assume state fill 
afford construct complete avc group main memory construct attribute lists predictor attributes memory 
predictor tributes write temporary file zn insert projection class label 
zn schema ha av ci 
scan completed algorithm cl applied memory avc groups attribute 
algorithm cl compute final splitting criterion procedure cl decide splitting criterion called avc sets attributes examined 
predictor attribute scan zn construct avc set call procedure cl find best partitioning avc set 
attributes examined call procedure cl decide splitting criterion compute final splitting criterion node slightly modified processing behavior node states fill summarized 
description concentrated possibility construct avc set predictor attributes 
general possibilities preparing construction avc sets predictor attributes node complete set options full 
avc group size estimation estimate size avc group new node note assume avc group smaller avc group parent considerably larger 
estimate size avc group new node conservative way estimate size parent avc set splitting attribute 
parent node splits know size avc set node exactly 
approach usually overestimates sizes avc groups worked practice 
algorithms estimation number distinct values attribute intend explore research 
experimental results machine learning statistics literature main performance measures classification tree algorithms quality rules resulting tree ii decision tree construction time lls 
generic schema described section allows instantiation knowledge classification tree algorithms literature modifying result algorithm 
quality orthogonal issue framework concentrate solely decision tree predictor attribute distribution maximum number entries salary commission salary commission age education level car zipcode uniformly chosen house value depends zipcode home years loan size avc group root sizes avc sets ais construction time :10.1.1.41.6931:10.1.1.41.6931
remainder section study performance techniques enable classification algorithms scalable 
datasets methodology gap scalability requirements real life data mining applications sizes datasets considered literature especially visible looking possible benchmark datasets evaluate scalability results 
largest dataset statlog collection training databases mst contains records largest training dataset considered lls tuples 
synthetic data generator introduced agrawal ais henceforth referred :10.1.1.41.6931:10.1.1.41.6931
synthetic data predictor attributes shown table 
included generator classification functions assign labels records produced 
selected functions function function ais performance study :10.1.1.41.6931
function generates relatively small decision tree trees generated function large 
note adheres methodology sprint performance study sam :10.1.1.104.152
feasibility framework relies size initial avc group examined sizes avc group training data sets generated generator 
maximum number entries avc group root node requiring maximum memory size mb 
partition predictor attribute house value vertically main memory requirements hold avc groups root node main memory reduced mb entries 
maximal avc set sizes predictor attribute displayed table 
function denotes integer uniform distribution values change memory available rainforest algorithms experiments call number avc set entries fit memory buffer size 
order run rf write datasets generated generator size entries rf vertical run buffer size entries 
experiments performed pentium pro mhz processor running solaris version mb main memory 
algorithms written compiled version compilation option 
interested behavior rainforest algorithms datasets larger main memory uniformly stopped tree construction leaf nodes family smaller tuples clever implementation switch main memory algorithm node fits main memory 
scalability results examined performance algorithms rf write rf hybrid rf vertical size input database increases 
algorithms rf write rf hybrid fixed size avc group buffer entries algorithm rf vertical fixed size avc group buffer entries 
figures show running time algorithms number tuples input database increases 
function constructs large decision trees tree growth takes longer function 
running time algorithms grows nearly linearly number tuples 
algorithm rf hybrid outperforms algorithms rf write rf vertical terms running time difference pronounced function 
figures show number page accesses tree construction assuming pagesize kb 
experiments investigated internal properties avc groups training database influence performance 
expected size input database buffer size matter experiments confirm 
fixed size input database tuples classification function function 
shows effect increase absolute size avc group input database holding available buffer sizes constant entries rf write rf hybrid entries rf vertical varied size avc group manipulation data generator entries original size entries original size 
small avc group sizes times rf vertical rf hybrid identical 
larger buffer size shows effect larger sizes rf hybrid writes partitions frequently rf vertical 
running time rf write affected change avc group size rf write writes partitions regardless amount memory available 
shows effect increase absolute size avc group input database varying buffer sizes 
buffer size rf write rf hybrid set exactly avc group root node fits memory buffer size rf vertical set exactly largest avc set root node fits memory 
avc group size buffer size increased simultaneously keeping ratio constant running times stay constant 
shows effect skew attributes avc group affects performance 
number tuples remained constant set buffer sizes rf write rf hybrid buffer size rf vertical 
duplicated attribute increasing number attributes skewed distribution distinct attributes values loan attributes 
reduced number attribute values remaining attributes loan attributes dominant contributors avc group size 
skew held number distinct attribute values loan attributes combined size entries 
example skew value indicates loan attribute distinct attribute values second loan attribute distinct values 
expected running time influenced skew avc group size remained constant 
experiment shown added extra attributes random values records input database holding number entries constant rf hybrid rf write entries rf vertical 
adding attributes increases tree construction time additional attributes need processed change final decision tree 
splitting algorithm choose noisy attribute splitting criterion 
seen rainforest family algorithms exhibits roughly linear scaleup number attributes 
performance comparison sprint section performance comparison sprint sam :10.1.1.104.152
tried implementation sprint efficient possible resulting implementation improvements algorithm described sam :10.1.1.104.152
create attribute list categorical attributes 
second node splits children nodes create histograms categorical attributes distribution categorical attribute list sav ing additional scan 
memory hash table large perform hash join pass attribute list 
figures show comparison sprint rainforest algorithms functions 
algorithms rf hybrid rf write set avc buffer size entries avc group root fits inmemory rf vertical set buffer size largest avc set single attribute root node fits memory 
figures show function rf hybrid rf vertical outperform sprint factor 
function generates larger trees function speedup factor 
speed come 
compared cost repeated memory sorting rainforest algorithms cost creation attribute lists sprint repeated sorts avoided 
numbers show repeated memory sorting avc groups times faster initial attribute list creation time 
second compared cost arrive splitting criterion node plus distribution children 
sprint splitting criterion computed scan attribute lists distribution performed hash join attribute lists attribute list splitting attribute 
rainforest family algorithms read twice written rf vertical needs write vertical partitions necessary 
set buffer size rf write avc group root fits memory buffer size rf vertical largest avc set fits memory 
shows cost determining splitting criterion plus partitioning original database factor faster scanning hash joining attribute lists 
cost dominant cost tree construction explains rainforest family algorithms outperforms sprint factor 
developed comprehensive approach scaling decision tree algorithms applicable decision tree algorithms aware 
key insight observation decision trees literature base splitting criteria tree node avc group node relatively compact 
best splitting criteria developed statistics machine learning exploited classification scalable manner 
addition depending available memory algorithms offer significant performance improvements sprint classification algorithm fastest scalable classifier literature 
memory hold individual avc sets obtain speed sprint memory hold avc sets node speedup better 

wish wei yin loh insightful discussions 
time seconds page accesses thousands time seconds function rf hybrid rf write rf vertical number tuples millions time seconds scalability function rf hybrid rf write rf vertical number tuples millions scalability time scalability time number page accesses function rf hybrid rf write rf vertical number tuples millions page accesses thousands number page accesses function rf hybrid rf write rf vertical number tuples millions scalability page accesses scalability page accesses increase avc group constant buffer size rf hybrid rf write rf vertical fraction original size time seconds increase avc group size varying buffer size rf hybrid rf write rf vertical fraction original size changing avc group size changing avc group size time seconds time seconds time seconds skew avc sets rf hybrid rf write rf vertical skew time seconds increase number attributes rf hybrid rf write rf vertical additional number attributes changing avc group skew changing number attributes comparison sprint function rf hybrid rf write rf vertical sprint number tuples millions time seconds comparison sprint function rf hybrid rf write rf vertical sprint number tuples millions comparison sprint comparison sprint sorting cost sprint attribute list creation rf cumulative memory sort rf cumulative memory sort number tuples millions time seconds partitioning cost rf write rf vertical sprint number tuples millions sorting cost comparison partitioning cost comparison agi agrawal ghosh imielinski iyer swami 
interval classifier database mining applications 
proc 
vldb 
ais agrawal imielinski swami :10.1.1.41.6931
database mining performance perspective 
ieee tkde december 
astrahan 
whang 
approximating number unique values attribute sorting 

information systems breiman friedman olshen stone 
classification regression trees 
bu brodley utgoff 
multivariate versus univariate decision trees 
tr department computer science university massachussetts 
cat catlett 
machine learning large databases 
sydney 
phd thesis university 
cheng fayyad irani qian 
improved decision trees generalized version id 
proc 
machine learning 
cm mingers 
neural networks decision tree induction discriminant analysis empirical comparison 
journal operational cs research society 
chan stolfo 
experiments multistrategy learning meta learning 
cikm 
proc 
cs chan stolfo 
meta learning multistrategy parallel learning 
proc 
second intl 
workshop multistrategy learning 
dbp brown 
comparison decision classifiers backpropagation neural networks multimodal classification problems 
pattern recognition 
dks dougherty sahami 
super fay unsupervised discretization continous features 
proc 
machine learning 
fayyad 
induction decision trees multiple concept learning 
phd thesis eecs de fi university michigan 
fayyad irani 
multi interval discretization continous valued attributes classification learning 
proc 
international joint conference artificial intelligence 
fmm fukuda morimoto morishita 
constructing efficient decision trees optimized numeric association rules 
proc 
vldb 
gj garey johnson 
tractability 
computer han hand 
construction assessment classification rules 
haas naughton seshadri stokes 
sampling estimation number distinct values attribute 
proc 
vldb 
lls 
lim 
loh 
shih 
empirical comparison decision trees classification methods 
tr department statistics uw madison june 
ls 
loh 
shih 
split selection methods classification trees 
statistica sinica october 
lv 
loh 
tree structured maa classification generalized analysis discussion 
journal american statistical association 
maass 
efficient agnostic pac learning simple hypothesis 
proc 
conference computational learning theory 
mag 
approach mar tion modeling 
handbook marketing research 
mehta agrawal rissanen 
sliq fast scalable classifier data mining 
proc 
edbt 
mra mehta rissanen agrawal 
mdl decision tree pruning 
proc 
kdd 
mst michie spiegelhalter taylor editors 
machine learning neural statistical classification 
qui quinlan 
discovering rules induction large collections examples 
expert systems micro electronic age 
qui quinlan 
learning efficient classification procedures 
machine learning artificial intelli qui gence approach 
quinlan 
induction decision trees 
machine learning 
qui quinlan 
programs machine learning 
rs rastogi shim 
public decision tree classifier integrates pruning building 
proc 
vldb 
ris rissanen 
stochastic complexity statistical inquiry 
sam shafer agrawal mehta :10.1.1.104.152
sprint scalable parallel classifier data mining 
proc 
vldb 
smt shavlik mooney towell 
symbolic neural learning algorithms empirical comparison 
machine learning 
wk weiss kulikowski 
computer systems learn classification prediction methods statistics neural nets machine learning expert systems 
morimoto fukuda 
algorithms mining association rules binary segmentations huge categorical databases 
proc 
vldb 
