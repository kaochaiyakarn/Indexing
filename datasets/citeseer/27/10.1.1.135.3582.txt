feature subset selection order identi cation unsupervised learning jennifer dy dy ecn purdue edu carla brodley brodley ecn purdue edu school electrical computer engineering purdue university west lafayette usa explores problem feature subset selection unsupervised learning wrapper framework 
particular examine feature subset selection wrapped expectation maximization em clustering order identi cation identifying number clusters data 
investigate di erent performance criteria evaluating candidate feature subsets scatter separability maximum likelihood 
true number clusters unknown experiments simulated gaussian data real data sets show incorporating search feature selection procedure obtains better class accuracy xing number classes 
reasons true number gaussian components necessarily equal number classes clustering di erent feature subsets result di erent numbers true clusters 
empirical evaluation shows feature selection reduces number features improves clustering performance respect chosen performance criteria 

feature selection problems human de nes features potentially useful subset chosen original pool features automated feature selection algorithm 
significant body research exists methods selecting features supervised learning fukunaga kohavi john little attention paid automatic feature selection unsupervised learning 
feature selection useful limit redundancy features promote comprehensibility nd clusters structures hidden high dimensional data 
unsupervised learning goal nd smallest feature subset best uncovers natural groupings clusters data criterion 
di cult task nd feature subset maximizes performance criterion need clusters de ned 
unsupervised clustering needs features variables span space trying cluster 
problem di cult know number clusters approach feature selection unsupervised learning inspired wrapper approach supervised learning kohavi john wrap search best feature subset supervised induction algorithm wrap search clustering algorithm 
introduce methods fssem feature subset selection wrapped em clustering fssem fssem order identi cation 
term em clustering represents expectation maximization em algorithm dempster applied estimating maximum likelihood parameters nite gaussian mixture mclachlan basford 
apply wrapper approach em clustering applied clustering method 

unsupervised feature selection literature maintain wrapper lter model distinction characterize feature subset selection supervised learning de ne wrapper approach unsupervised learning applying unsupervised learning algorithm feature subset search space evaluating feature subset criterion function utilizes clustering result 
filter methods hand intrinsic property data select features utilizing clustering algorithm ultimately applied 
classic feature selection algorithm absence class labels apply karhunen loeve transform klt fukunaga 
klt pure feature subset selection algorithm involves transformation original feature space selection 
feature selection unsupervised learning relatively new 
approaches customized particular clustering algorithm 
devaney ram applied sequential forward backward search 
evaluate candidate subset measured category utility clusters applying cobweb hierarchical clustering algorithm conjunction feature subset 
applied blind similar lter feedback analogous wrapper approaches cobweb feature dependence measure select features 
vaithyanathan dom formulated objective function choosing feature subset nding optimal number clusters document clustering problem bayesian statistical estimation framework 
agrawal 
introduced clustering algorithm clique proceeds level level starting feature upto highest dimension feature subspaces clusters regions high density points generated 

feature subset selection em clustering fssem fssem wraps feature subset selection clustering algorithm 
basic idea search feature subset space evaluating subset ft rst clustering space ft em clustering evaluating resulting clusters feature subset chosen feature selection criterion 
feature selection criteria investigated discussed section 
exhaustive search possible feature subsets number available features subset maximizes selection criterion computationally intractable 
greedy search sequential forward backward elimination fukunaga kohavi john typically 
experiments reported applied sequential forward search 
plan explore ect search methods fssem 
note em initialized new feature subset 
assume data comes mixture model multivariate gaussians mclachlan basford 
apply em algorithm estimate maximum likelihood mixture model parameters cluster probabilities data point 
em clustering results soft clusters data point belongs cluster probability 
note framework introduced easily extended mixture probability distributions mclachlan basford clustering methods including graph theoretic approaches jain dubes 
em algorithm trapped local maximum initialization values important 
sub sampling initialization algorithm proposed fayyad 
sub sampling sub sampling iterations 
initializing parameters em iterates convergence likelihood change default iterations whichever comes rst 
limit number iterations em converges asymptotically convergence slow near maximum 
em estimation constrained away singular solutions parameter space limiting diagonal elements component covariance matrices greater average variances unclustered data 
adding scalar multiplied identity matrix positive semi de nite matrix nal matrix positive de nite eigenvalues greater zero nonsingular 

order identi cation fssem unsupervised clustering di cult know number clusters single perceptual class may modeled better multiple gaussian mixture single gaussian cluster 
furthermore searching best subset features run new problem value depends feature subset 
illustrates point 
dimensions shown left clusters dimension shown right clusters 
di culty better 
ultimately way judge criterion tied nal clustering 
fssem just fss wrapped em em clustering order identi cation 
feature subset search clusters 
currently applies method bouman 
adds minimum description length rissanen penalty term log likelihood criterion 
new objective function log xj log nd number data points dimension number real numbers needed specify parameters xxxx xxxx xxxx xxx xxx xx xx xx xx xx xx xx xx xx xx xxx xx project axis xxxxxxxxxxx xxxxxx xxxxx xxx xx 
number cluster components varies dimension 
log xj log likelihood observed data parameters note vary penalty term needed maximum likelihood estimate increases clusters 
penalty likelihood maximum data point considered individual cluster 
search large number clusters kmax sequentially decrement number cluster remains 
choose value optimizes criterion function 
myriad ways nd optimal number clusters em clustering see smyth overview 

feature selection criteria factors characterizes feature selection algorithm performance feature evaluation criterion 
investigate known measures scatter separability maximum likelihood 
describing measure discuss bias respect dimension feature space 
conclude approach ameliorate biases dimension normalization procedure 
scatter separability criterion investigate scatter matrices separability criteria discriminant analysis fukunaga feature selection criterion 
criteria discriminant analysis assume features interested features separate data clusters unimodal separated scatter means 
possible separability criteria choose trace sb criterion invariant nonsingular linear transformation fukunaga 
sw class scatter matrix sb class scatter matrix de ned follows sw pk jef jg pk sb pk mo mo mo pk probability instance belongs cluster dimensional random feature vector representing data number clusters sample mean vector cluster mo total sample mean data points instances data set sample covariance matrix cluster ef expected value operator 
sw measures scattered samples cluster means average covariance cluster 
sb measures scattered cluster means total mean 
distance pair samples particular cluster small possible cluster means far apart possible respect chosen similarity metric euclidean case 
sb sb normalized average cluster covariance 
larger value trace sb larger normalized distance clusters results better cluster discrimination 
maximum likelihood ml criterion scatter separability criterion utilize additional information provided em clustering probability distribution estimate data 
separability di erent underlying assumption em clustering 
separability biased clusters cluster means far apart biased clusters equal means different covariances 
separability isa general criterion clustering algorithm appropriate distance clustering algorithms means 
maximum likelihood ml hand measures data parameters model 
tells model ts data 
addition criterion clustering employ ml nd feature subset models data best em clustering 
choose subset maximizes criterion 
bias criterion values dimension criteria biases respect dimension separability criterion increases number features dimension increases ml decreases dimension increases 
biases occur clustering assignments remain 
separability measure biased way cause trace sb basically adding number dimension terms 
fukunaga proved criterion form xd monotonically creases dimension assuming cluster ing assignment 
dy relates trace sb proof 
ml biased way value table 
results synthetic data sets cv error clusters method class class class class class class fssem tr xed xed xed fssem tr fssem ml xed xed xed fssem ml em xed xed xed em marginal density greater equal yjx yjx conditional density yjx yjx 
feature subsets di erent dimensions clustering data subset leads clustering leads clustering 
want compare exist spaces di erent dimension 
provide fair comparison propose heuristic normalization scheme 
crit fi cj criterion value feature subset fi represent data cj clustering assignment 
crit represents arbitrary criterion function 
example crit trace sb criterion 
compute sw sb feature subset fi clustering assignments cj fi 
sw sb matrices number features fi 
normalize criterion value alue crit crit criterion value alue crit crit maximize criterion values ml trace discussed previous subsections 
alue ci alue cj choose clustering ci feature subset fi 
normalized criterion values equal ci cj favor clustering lower dimensional feature subset 
normalization removes bias dimension product crit fi ci crit fj ci projects ci dimensions 
normalized value focuses quality clusters obtained 
clustering assignments di erent feature subsets alue equal alue want 
experiments reported applied normalization criterion detailed analysis dy showed fssem dimensionality bias correction results features necessary trace criterion selection feature ml criterion 

experimental evaluation experiments designed evaluate fssem fssem dimensions ability select relevant features ability correctly identify order ability nd structure data 
rst experiments synthetic data detailed analysis fssem variants realworld data sets 
synthetic gaussian mixture data synthetic data sets contains relevant irrelevant features relevant means created component mixture model features 
irrelevant features generated gaussian normal random variables 
design rst synthetic data sets similar simulated gaussian structures reported smyth 
rst data set consists gaussian clusters covariance matrix means 
considerable overlap clusters added noise features increase di culty problem 
data set feature considered relevant feature needed identify clusters 
second data set consists gaussian clusters 
clusters means covariance matrices orthogonal 
remaining cluster overlaps tails right side clusters 
third data set clusters means covariances equal data sets gaussian normal random noise features added 
generated data points generated clusters equal proportions 
data sets set kmax respectively 
standard measures evaluating clusters clustering literature 
single clustering assignment class label explains appli cation jain dubes 
working synthetic data know true mixture model number clusters true cluster assignments correct features 
measure performance class error de ne number instances misclassi ed divided total number instances assuming instance cluster classi ed majority class determined training data 
compute class error assign data point cluster 
comparing clusterings di erent number clusters training error 
class error training decreases increase number clusters trivial result error data cluster 
ameliorate problem cross validation error particular tenfold cross validation 
aside class error compute average number clusters average number features selected feature precision recall 
recall number relevant features selected subset divided total number relevant features 
precision number relevant features selected subset divided total number features selected 
due space limitations report fold cross validated class error cv error number clusters table 
complete results dy 
tr refers trace scatter separability criterion ml refers maximum likelihood criterion 
looking rst fssem tr compared fssem tr see including order identi cation fssem tr feature selection results lower cv error trace criterion 
data sets fssem tr signi cantly lower cv error fssem tr 
adding search feature subset selection search allows algorithm nd relevant features average feature recall fssem tr versus fssem tr 
best number clusters depends chosen feature subset 
example closer examination noted class problem xed clusters formed feature better separated clusters formed features 
consequence fssem tr select feature 
variable feature search fssem tr nds clusters feature 
feature considered feature clusters resulting higher separability 
em lower cv error em due knowing correct number clusters 
em em poorer performance fssem tr retained noisy features 
fssem tr performed better fssem ml terms cv error 
average feature recall precision data sets fssem tr respectively 
ml perform respect cv error favors features unimodal distributions noise features 
fssem tr fssem tr biased separable clusters identi ed de ned relevant features 
fssem ml fssem ml hand biased data fewer clusters data gaussian distribution de ned noise features 
fixing ml criterion improved cv error performance increased chances arriving local minima noise features cluster di erent centroid allowing relevant features equal true number clusters selected 
xing counter ml bias noisy unimodal features re ected recall 
real world data sets illustrate fssem real data results data sets ionosphere blake merz high resolution computed tomography images lungs hrct lung data set dy 
see dy experiments additional data sets 
data set class information known remove class labels training 
synthetic data know true number gaussian clusters real world data sets 
class may composed gaussian clusters 
clusters may gaussian distribution 
see clustering algorithms clusters correspond classes class multi modal compute class error way synthetic gaussian data 
real data sets know relevant features 
compute precision recall report average number features selected average number clusters 
table reports fold cross validation error number clusters di erent algorithms 
data sets set kmax equal 
ionosphere data hrct lung data labeled classes respectively 
note class error measure cluster performance interpretation 
cluster quality di cult measure depends particular appli table 
results ionosphere hrct data sets 
cv error clusters method ionosphere hrct lung ionosphere hrct lung fssem tr xed xed fssem tr fssem ml xed xed fssem ml em xed xed em cation 
major distinction unsupervised clustering supervised learning 
class error just interpretation data 
measure cluster performance terms trace criterion ml criterion 
naturally fssem tr fssem tr better rest terms trace fssem ml fssem ml better rest terms maximum likelihood 
choosing tr ml depends application goals 
interested nding features best separate data fssem tr 
interested nding features model gaussian clusters best fssem ml 
fssem ml performed best terms cv error ionosphere data 
presents scatter plot ionosphere data best features chosen fssem ml means covariances ellipses discovered 
shows original data projected features class labeled means covariances 
shows scatter plot best features chosen fssem tr corresponding scatter plot original data 
observe trace favored cluster clusters separated 
hand fssem ml favored clusters gaussian allow overlap 
ml clustering matches ionosphere class labels closely fssem ml performed better respect cv error 
fssem ml obtained better cv error em em 
em em features fssem ml fssem tr features average 
fewer features possible visualize scatter plots dimensions 
order identi cation improved performance criteria true number clusters appears classes ionosphere data set 
features discarded values constant discrete data 
constant features discrete features discrete levels equal number clusters produce nite likelihood nite gaussian mixture model 
hrct lung data fssem tr performed better fssem ml terms cv error 
presents scatter plot hrct lung data best features chosen fssem tr 
shows original data projected features class labeled means covariances 
observe clusters tr separated match class labels 
fssem ml selected single feature resulted highly overlapping clusters 
hrct lung di cult data set due skewed class distribution approximately data disease 
em discovered cluster class error equal error majority classi cation rule close values obtained methods 
high dimensions obscure hrct lung classes result em nding cluster 
em set performed better fssem tr terms cv error know true number clusters 
addition em uses features fssem tr uses average features 
examined initial cluster result noticed features selected fssem tr row column centroid locations pathology region identi ed radiologist image 
features clustered data terms location regions upper left lower left upper right lower right side image 
features result separated clusters discriminate disease classes 
subsequently removed resulting current data set features ran algorithm 
fssem tr picked gray level histogram features texture features relevant discriminating diseases 
discarded row column features learned data separated terms location 
feature selection unsupervised data useful discovering relevant features discovering unwanted features produce systematic structures 
feature feature feature feature feature feature feature feature 
scatter plots ionosphere data best features chosen fssem ml best features chosen fssem tr 
represent di erent class assignments 
cluster class means ellipses covariances 
clusters discovered fssem ml fssem tr respectively 
means covariances labeled classes 
feature feature feature feature 
scatter plots hrct lung data best features chosen fssem tr 
squares represent di erent class assignments 
cluster class means ellipses covariances 
shows clusters fssem tr 
shows means covariances labeled classes 

introduced wrapper framework performing feature selection clustering order identi cation concurrently 
compared di erent feature selection criteria scatter separability maximum likelihood ml 
criteria di erent assumptions biases limitations 
sepa measure prefers feature subsets cluster centroids far apart 
ml prefers feature subsets lead clusters gaussian model best 
cluster separation required maximize ml criterion lead overlapping clusters 
experiments synthetic data trace separability criterion performed better ml criterion 
result sense data designed separated relevant feature subsets 
results hrct ionosphere data sets point utility feature subset selection learn data importance picking criterion tied clustering objectives 
results showed incorporating order identi cation feature subset selection process led better results xing true number classes 
reasons number classes necessarily equal number gaussian clusters di erent feature subsets di erent number clusters 
investigate di erent clustering algorithms di erent search strategies wrapper framework 
normalization scheme introduced ective general ad hoc 
investigate methods normalizing biases di erent criteria 
investigation needs done ects feature search 
interesting investigate feature selection criterion criterion clustering 
initial experiments dy 
acknowledgments dr craig ml lunch group constructive comments discussions 
research supported nsf 
iri nih lm 
agrawal gehrke gunopulos raghavan 

automatic subspace clustering high dimensional data data mining applications 
proceedings acm sigmod international conference management data pp 

seattle wa acm press 
blake merz 

uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
bouman shapiro cook atkins cheng 

cluster unsupervised algorithm modeling gaussian mixtures 
dynamo ecn purdue edu bouman software cluster 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
devaney ram 

cient feature selection conceptual clustering 
proceedings fourteenth international conference machine learning pp 

nashville tn morgan kaufmann 
dy 

preliminary report feature selection unsupervised learning 
unpublished manuscript school electrical computer engineering purdue university west lafayette 
dy brodley kak 

customized queries approach cbir em 
proceedings ieee conference computer vision pattern recognition pp 

fort collins ieee computer society press 
fayyad reina bradley 

initialization iterative re nement clustering algorithms 
proceedings fourth international conference knowledge discovery data mining pp 

new york aaai press 
fukunaga 

statistical pattern recognition second edition 
san diego ca academic press 
jain dubes 

algorithms clustering data 
englewood cli nj prentice hall 
kohavi john 

wrappers feature subset selection 
arti cial intelligence 
mclachlan basford 

mixture models inference applications clustering 
new york marcel dekker 
rissanen 

universal prior integers estimation minimum description length 
annals statistics 
smyth 

clustering monte carlo crossvalidation 
proceedings second international conference knowledge discovery data mining pp 

portland aaai press 


feature selection preprocessing step hierarchical clustering 
proceedings sixteenth international conference machine learning pp 

bled slovenia morgan kaufmann 
vaithyanathan dom 

model selection unsupervised learning applications document clustering 
proceedings sixteenth international conference machine learning pp 

bled slovenia morgan kaufmann 
