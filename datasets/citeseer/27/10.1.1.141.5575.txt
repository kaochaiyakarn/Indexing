ieee transactions multimedia vol 
june grounded spoken language acquisition experiments word learning deb roy member ieee language grounded sensory motor experience 
grounding connects concepts physical world enabling humans acquire words sentences context 
currently machines process language grounded 
semantic representations pre specified meaning interpreted humans 
interested developing computational systems represent words utterances underlying concepts terms sensory motor experiences leading richer levels machine understanding 
key element development effective architectures processing multisensory data 
inspired theories infant cognition computational model learns words acoustic video input 
channels input derived different sensors integrated information theoretic framework 
acquired words represented terms associations acoustic visual sensory experience 
model implemented real time robotic system performs interactive language learning understanding 
successful learning demonstrated infant directed speech images 
index terms cross modal language learning multimodal semantic grounding 
language grounded experience 
dictionary definitions words defined terms words humans understand basic concepts terms associations sensory motor experiences cf 

grasp concepts underlying words red heavy requires interaction physical world 
link body environment fundamental aspect language enables humans acquire words sentences context 
aspects human cognition language processing clearly understood draw lessons human processing guide design intelligent machines 
infants learn words associating speech patterns objects actions people 
primitive meanings words utterances inferred observing world multiple senses 
multisensory grounding early words forms foundation complex concepts corresponding linguistic capacities 
syntax emerges children combine words refer relations concepts 
language learner linguistic abilities mature manuscript received january revised october 
supported part associate editor coordinating review approving publication dr thomas 
author media laboratory massachusetts institute technology cambridge ma usa mail media mit edu 
digital object identifier tmm speech refers increasingly notions 
words utterances fundamentally meaning humans grounding multimodal embodied experience 
sensory motor basis semantics provides common ground people understand 
contrast currently automatic spoken language processing systems grounded 
machine training recordings spoken utterances paired manually generated transcriptions semantic labels 
depending task transcriptions may vary level abstraction ranging low level phonetic labels high level semantic labels 
various statistical methods including hidden markov models hmms neural networks employed model acoustic label mappings 
refer general approach modeling mappings speech signals human specified labels ungrounded speech understanding semantics speech signal represented abstractly machine 
labels isolates machine physical world 
ungrounded approach lead practical applications transcription telephony 
exist fundamental limits ungrounded approach 
anticipate limitations ungrounded speech understanding comparison human counterparts 
interrelated advantages identified grounded approach 
learning problem may solved labeled data function labels may replaced contextual cues available learner environment 
language occur vacuum 
infants observe spoken language rich physical social contexts 
furthermore infant directed speech usually refers immediate context caregivers rarely refer events occurring time place 
connection speech immediate surroundings presumably helps infant glean meaning salient words phrases observing contexts speech occurs 
advantage approach learner acquires knowledge observations world reliance labeled data 
similar advantages anticipated machines 
second advantage grounded approach speech understanding leverage context disambiguate words utterances multiple levels ranging acoustic semantic ambiguity 
tight binding language world enables people integrate nonlinguistic information language understanding process 
acoustically semantically ambiguous utterances disambiguated context heard 
extra linguistic information naturally easy forget vital role language processing 
similar advantages expected machines able effectively context ieee ieee transactions multimedia vol 
june fig 

levels conceptual abstraction grounded sensory motor experience 
language acquired forming concepts learning associations words utterances conceptual structures 
processing language 
advantages motivate investigate grounded speech acquisition 
illuminating examine differences learning procedures speech systems infants 
traditionally speech understanding systems trained providing speech corresponding transcriptions may include semantic labels addition phonetic word labels 
constitutes drastically impoverished input compared infants 
handicap infants acquire language 
training labeled data advantages 
recognition task defined mature techniques supervised machine learning may employed parameter estimation classifiers 
propose new methods explore human learning multiple channels unlabeled data 
learning problem challenging potential payoffs great 
goal build multimodal understanding systems leverage cross channel information leading intelligent robust systems trained data 
presents model grounded language learning called cell cross channel early lexical learning 
cell leverages cross modal structure segment discover words continuous speech learn visual associations words 
rely transcriptions labels speech provides noisy ambiguous labels video vice versa 
describe new algorithms developed implement model real time audio visual processing system 
system embedded robotic embodiment enabling language learning understanding face face interactions 
experimental evaluations infant directed speech occurring video word learning achieved face highly spontaneous speech 
ii 
grounding connecting meaning world grounding concrete form achieved giving machines capacity sense act physical world 
humans sense act world shared physical context provides common ground mediates communication humans machines 
fig 
illustrates concepts emerge sensory motor experience layers analysis 
left side interactions physical world give rise sensory motor action categories 
structures represent relations categories inferred increasing levels abstraction right 
ultimately causal logical relations may inferred appropriate types structured learning employed 
current restricted levels shown framework leads naturally higher levels conceptual linguistic learning 
philosophy built communication systems ground input physical sensors 
humans endowed similar sensory motor capacities 
shared endowment results similar semantic representations lowest levels abstraction 
person able perceive infrared ultraviolet rays young child naturally acquire words grounded referents 
young children nouns label small objects probably objects able manipulate hands build sufficiently accurate models 
names larger objects acquired stages development 
design sensors manipulators regulates type concepts machine acquire 
argue machines functional level share abilities limits physiology acquire human semantics 
emphasis placed grounding learning sensors avoiding reliance human generated labels transcriptions 
ensures machine develop representations capture richness inherent continuous variations physical world 
engineering perspective sensory grounding forces adopt statistical approaches robust various types noise encountered sensory signals 
focuses grounding language physical world situations may useful ground semantics virtual worlds 
example created video game synthetic character see objects virtual world synthetic vision 
semantics spoken words grounded attributes virtual objects enabling speech human machine interaction course playing video game 
situations level semantic abstraction required communication task render direct physical grounding impractical 
cases virtual representation task may serve useful proxy ground human machine communication 
common denominator virtual physical grounding humans machines perceptual access shared nonlinguistic referents 
roy grounded spoken language acquisition iv 
cell cross channel early lexical learning fig 

framework learning sensory data 
feature detectors extract channels input sensors 
input channels divided sets 
carries symbolic information words signed gestures 
second set carries representations referents may associated symbols 
example visual channels may represent shape color objects associated shape color symbolic terms 
iii 
learning cross channel structure world provide infants transcribed data 
environment provides rich streams continuously varying information multiple modes input 
infants learn combining information multiple modalities 
promising path research build machines similarly integrate evidence modalities learn naturally occurring data supervision 
key advantage approach potentially unlimited new sources training data may utilized develop robust recognition technologies 
ultimately envision machines actively explore world acquire knowledge sensory motor interactions 
fig 
shows framework learning multisensory input 
set sensors provides input 
feature detectors extract channels input sensors 
general number input channels greater number sensors 
example shape color texture motion channels extracted camera 
phonemes speaker identity prosody pitch loudness examples channels extracted acoustic input 
subset input channels assumed represent symbolic information words phrases 
remaining channels represent referents symbols 
goal learning appropriately segment cluster incoming data input channels order identify build associations symbols referents 
models language acquisition include models speech segmentation minimum description length encoding acoustic representations cross situational learning text coupled line drawings representing simple visual semantics 
algorithms acquiring syntactic structure semantic associations acoustic words semantic transcriptions demonstrated 
lead tabula rasa learning acoustic vocabularies higher level language structures speech recordings transcribed semantic level 
physical grounding concepts explored context robotics alternative symbol processing view artificial intelligence 
model departs previous language learning words semantics acquired sensor input human assisted transcription labeling data 
explore issues grounded language created system learns spoken words visual semantics integrating visual acoustic input 
system learns segment continuous speech priori lexicon forms associations acoustic words visual semantics 
effort represents step introducing grounded semantics machines 
system represent words symbols 
words represented terms audio visual associations 
allows machine represent relations words physical referents 
important feature word learning system trained solely microphone camera input 
similar human learning presence multiple channels sensory input obviates need manual annotations training process 
remainder model word learning describe experiments testing model interactive robotics infant directed speech 
developed model cell summarized fig 

model discovers words searching segments speech reliably predict presence occurring visual categories 
input consists spoken utterances paired images objects 
experiments results spoken utterances recorded mothers played infants natural settings 
play centered everyday objects shoes balls toy cars 
images objects paired spontaneous speech recordings provide multisensory input system 
goal approximate input infant receive listening caregiver simultaneously attending objects environment 
output cell consists lexicon audio visual items 
lexical item includes statistical model hmms acquired spoken word statistical visual model shape color category 
acquire lexical items system segment continuous speech word boundaries form visual categories form appropriate correspondences word visual models 
correspondence speech visual streams extremely noisy 
experiments infant directed speech described section ix majority spoken utterances corpus contained direct occurring visual context 
learning problem cell faces extremely challenging system fish salient cross channel associations noisy input 
camera images objects converted statistical representations shapes 
spoken utterances captured microphone mapped sequences phoneme probabilities 
short term memory stm buffers phonetic representations spoken utterances paired representations occurring visual input 
short term recurrence filter searches stm repeated subsequences speech occur matching visual contexts 
resulting pairs speech segment shape representations placed long term memory ltm 
filter mutual information searches ltm speech shape speech color pairs usually occur rarely occur apart ltm 
ieee transactions multimedia vol 
june fig 

cell model 
layered memory architecture combined recurrence mutual information filters see text acquire audio visual lexicon unlabeled input 
pairings retained ltm rejected pairings periodically discarded garbage collection process 
representing comparing spoken utterances motivated fact infants age months possess language specific phonemic discrimination capabilities system endowed english phoneme feature extraction 
spoken utterances represented arrays phoneme probabilities 
recurrent neural network similar processes rasta plp coefficients estimate phoneme speech silence probabilities 
rnn input units hidden units output units 
hidden units connected time delay concatenated rasta input coefficients 
rnn trained line back propagation time timit database phonetically transcribed speech recordings 
rnn recognizes phonemes accuracy standard timit training test datasets 
session recordings segmented utterances detecting contiguous segments speech probability silence estimated rnn low 
spoken utterances segmented time phoneme boundaries providing hypotheses word boundaries 
locate phoneme boundaries rnn outputs treated state emission probabilities hmm framework 
viterbi dynamic programming search obtain learning system certain structures innate support data driven learning 
goal word learning chose start month old stage point infants able discern phonemic speech sound differences begun word learning 
model different stages language acquisition phonological syntactic learning different choices innate 
note transcribed data strictly purpose training rnn serve feature detector generating phoneme probabilities 
word learning performed cell new experimental database transcriptions 
phoneme sequence phoneme probability array 
viterbi decoding utterance system obtains phoneme sequence sequence phonemes utterance location phoneme boundary sequence information recovered viterbi search 
phoneme boundary serve speech segment start point 
subsequence utterance terminated phoneme boundaries form word hypothesis 
define distance metric measures similarity speech segments 
possibility treat phoneme sequence speech segment string string comparison techniques 
method applied problem finding recurrent speech segments continuous speech 
limitation method relies single phoneme sequence 
sequence rnn output equivalent unpruned phoneme lattice multiple phoneme sequences may derived 
additional information developed distance metric 
best path sequence phonemes observed speech segment 
sequence may generate hmm model assigning hmm state phoneme connecting state strict left right configuration 
state transition probabilities states phoneme inherited context independent set phoneme models trained timit training set 
consider speech segments decoded phoneme sequences sequences generate hmms wish test hypothesis generated vice versa 
forward algorithm compute probability hmm derived speech segment generated speech segment vice versa 
probabilities effective measure roy grounded spoken language acquisition fig 

extraction object shape color channels ccd camera 
purposes represent joint probability phoneme sequence speech segment 
improvement likelihood ratio test generate confidence metric 
method likelihood estimate scaled likelihood default alternate hypothesis alternative hypothesis hmm derived speech sequence symmetric distance speech segments defined terms logarithms scaled likelihoods practice metric robustly detect phonetically similar speech segments embedded spontaneous speech 
basis determining acoustic matches segments recurrence filter stm mutual information filter build lexical items ltm see section vii 
vi 
visual processing motivated visual abilities infants system endowed color shape feature extractors 
dimensional objects represented view approach multiple dimensional images object captured multiple viewpoints collectively form model object 
representations designed invariant transformations position scale plane rotation 
representation color invariant changes illumination 
fig 
shows stages visual processing extract representations object shapes colors 
ground segmentation accomplished assuming background uniform color 
gaussian model illumination normalized background estimated set images 
new image gaussian model evaluated pixel thresholded empirically determined threshold value classify pixels background foreground 
large connected regions pixels classified foreground indicate presence object 
shape object represented set histograms represents silhouette object different viewpoint 
assume sufficient stored viewpoints novel viewpoint object may matched interpolation 
pixels image correspond object ground segmentation steps build representation object silhouette 
locate outer edge points object finding foreground pixels adjacent background pixels 
edge points interior object ignored 
pair edge points compute values euclidean distance points normalized largest distance edge points silhouette angle tangents edge object edge points 
accumulate histogram distance angle measurements 
resulting histogram representation object silhouette invariant rotation angles relative object size distances normalized 
multidimensional histograms represent object shapes enables schiele crowley shown histograms local images features powerful representation object recognition 
ieee transactions multimedia vol 
june information theoretical statistical divergence functions comparison silhouettes 
experimentation divergence effective histograms indexed values histogram cell 
representation dimensional shapes collection shape histograms corresponding particular view object 
results reported dimensional object represented histograms 
viewpoints chosen random 
simple objects views sufficient capture basic shape characteristics 
refer set histograms view set 
view sets compared summing divergences best matches individual histograms 
color objects represented histograms 
compensate lighting changes red green blue components pixel divided sum components resulting set illumination normalized values 
triplets illumination normalized values add free parameters pixel 
reason normalized blue value pixels stored colors dropped 
image color histogram generated accumulating illumination normalized red green values foreground pixel object 
normalized red green values divided bins leading histogram 
similar representation shape color histograms recorded image capture color difference different viewpoints 
similar shape comparisons sum divergences best matching views compare color object 
vii 
audio visual lexical acquisition heart cell model cross channel learning algorithm simultaneously solves problems speech segmentation visual categorization speech vision association 
key problem clustering different representations question combine distance metrics operate distinct representations 
cell mutual information quantify cross channel structure 
section describes cell cross channel lexical learning architecture sections provide results algorithm learning robot directed infant directed speech images objects 
input cell consists series spoken utterances paired view sets 
refer utterance view set pair audio visual event av event 
av events generated object view spoken utterance detected 
lexical acquisition comprised steps 
step av events passed short term memory stm buffer 
buffer capacity av events 
new event inserted buffer recurrence filter searches approximately repeating audio visual patterns buffer 
speaker repeats word phrase twice contiguous utterances playing similar shaped objects recurrence filter select recurrent sound shape pair potential lexical item 
recurrence filter uses audio visual distance metrics earlier determine matches 
distance metrics applied independently visual acoustic components av events 
matches simultaneously metrics recurrence detected 
recurrence filter performs exhaustive search possible image sets speech segments phoneme boundaries av events 
summarize output recurrence filters consists reduced set speech segments hypothesized visual referents 
second step hypotheses generated recurrence filter clustered information theoretic measure reliable clusters generate lexicon 
assume sound shape hypotheses ltm 
simplicity ignore color channel example process repeated input channels 
clustering process proceed considering hypothesis point turn 
assume hypotheses chosen point 
remaining hypotheses may compared assume thresholds defined show values determined 
indicator variables defined respect hypothesis setting thresholds variables indicate hypothesis matches acoustically visually respectively 
mutual information defined probabilities required calculate estimated frequency counts 
avoid noisy estimates events occur times disregarded 
note function thresholds determine system searches settings thresholds maximizes mutual information smoothing frequencies avoids collapse thresholds zero 
hypothesis taken point point maximum mutual information mmi 
hypotheses size stm determined experimentally represents balance learning performance speed 
smaller stms lead poor learning performance larger stms significantly improve learning dramatically increased learning speed 
roy grounded spoken language acquisition result highest mmi selected output system 
selected hypothesis hypotheses match visually acoustically removed processing 
effect strategy leads greedy algorithm hypotheses best mmi scores extracted 
process described effectively combines acoustic visual similarity metrics mmi search procedure 
mutual information metric determine goodness hypothesis 
knowledge presence cluster acoustic visual greatly reduces uncertainty presence cluster visual acoustic hypothesis high goodness rating selected output system 
interesting aspect mmi combine similarity metrics invariance scale factors similarity metric 
metric organizes sound shape hypotheses independently 
mmi search finds structural correlations modalities directly combining similarity scores 
result clusters identified method locally dynamically adjust allowable variances modality 
locally adjusted variances achieved fixed scheme combining similarity metrics 
final step threshold mmi score hypothesis select exceed threshold 
automatic determination mmi threshold addressed 
current experiments set manually optimize performance 
viii 
interactive robotic implementation support human machine interactions cell incorporated real time speech vision interface embodied robotic system 
input consists continuous multiword spoken utterances images objects acquired video camera mounted robot 
visual system extracts color shape representations objects ground visual semantics acquired words 
teach system person places objects front robot describes 
lexicon acquired robot engaged object labeling task speech generation object selection task speech understanding 
robotic embodiment degree freedom robotic constructed enable active control orientation small video camera mounted device fig 

animated face designed give robot appearance synthetic character 
facial features including eyelids mouth feathers convey information state system user natural manner 
direction gaze miniature camera embedded right eyeball robot 
direction camera focus apparent physical orientation robot provides mechanism establishing joint attention 
facial expressions servo controlled facial features convey information internal state cell 
eyes kept open vision fig 

robot degrees freedom capture images objects 
small ccd camera mounted right eyeball 
turntable provides fifth degree freedom viewing objects various perspectives 
turntable collecting images infant directed speech experiments described section ix 
system 
feathers mounted head extended attentive pose audio processing system detects start utterance 
robot mouth beak moves synch output speech 
spoken output phoneme speech synthesizer convey internal representations speech segments 
viterbi decoder extract phoneme sequence segment speech 
phoneme sequence resynthesized phoneme synthesizer 
naturalness output improved controlling duration individual phonemes observed durations viterbi decoding 
acquiring lexicon robot modes operation acquisition generation understanding 
mode toggled manually software switch 
acquisition mode robot searches presence objects viewing surface 
object detected system gathers multiple images build view set object 
spoken utterance detected view set gathered av event generated processed cell 
teach system user example place cup front robot say coffee cup verify system received contextualized spoken input back user speech recognized phoneme sequence 
provides natural feedback mechanism user understand nature internal representations created system 
acquiring lexical order step syntax learn word order language learner method clustering words syntactic categories 
syntax specify rules ordering word classes 
cell acquired lexicons divided natural classes words grounded shape words grounded color 
distributional analysis track ordering word classes utterances contain color shape words adjacent position spoken intervening words 
pilot experiment single user provided robot spoken utterances describing objects varying speech synthesizer entropic research laboratory pennsylvania ave se suite washington dc 
ieee transactions multimedia vol 
june fig 

objects play infant caregiver interactions 
shapes colors 
approximately equal numbers utterances produced describe object 
speech gathered spontaneous face face setting robot running acquisition mode 
small data set system learned color terms precede shape terms english 
information encoded single statistic higher probability shape color compared color shape word pairs 
statistic determine sequence words speech generation build simple language model speech understanding 
experiment word order learning represents step semantically grounded syntax acquisition 
method linking early lexical learning syntax acquisition closely related semantic bootstrapping hypothesis posits language learners semantic categories seed syntactic categories 
theory perceptually accessible categories objects actions seed syntactic classes nouns verbs 
seed categories established input utterances deduce phrase structure combination constraints innate biases structures 
turn phrase structure interpret input utterances novel words 
distributional analysis expand syntactic classes initial semantically bootstrapped categories 
plan expand cell enable complex aspects grounded syntax learning 
speech generation lexical items acquired system generate spoken descriptions objects 
mode robot searches objects viewing surface 
object detected system builds view set object compares lexical item ltm 
acoustic prototype best matching item generate spoken response 
spoken output may describe shape color depending best match 
word order statistics second generation mode finds best matching ltm item color shape object 
system generates speech describe features object 
order concatenation determined acquired word order statistics 
apple robot say red ball opposed ball red assuming learned words red ball seen apple heard specific word sequence 
speech understanding speech understanding mode input utterances matched existing speech models ltm 
simple grammar allows single words word pairs recognized 
transition probabilities word pairs determined acquired word order statistics 
response speech system finds objects viewing surface compares visual models recognized lexical item 
forced choice selects best match returns robot gaze object 
effect person speak phrase brown dog brown dog robot find object best matching visual semantics spoken word phrase 
provide additional feedback selected object index ltm generate spoken description 
feedback leads revealing behaviors incorrect incomplete lexicon acquired 
nature errors provides user guidance subsequent training interactions 
ix 
experiments infant directed spontaneous speech evaluate cell natural spontaneous spoken input experiments conducted corpus audio visual data infant directed interactions 
caregivers months infants asked play objects recorded 
selected classes objects commonly named young infants balls shoes keys toy cars trucks dog horses 
total objects objects class obtained see fig 

objects class vary color size texture shape 
caregiver infant pair participated sessions course days 
session played objects time 
caregiver speech recorded roy grounded spoken language acquisition fig 

mutual information function acoustic visual thresholds lexical candidates 
wireless headset microphone dat 
total collected approximately utterances comprising words speakers 
utterances contained multiple words mean utterance length words 
robot described section viii gather images object various randomly determined viewpoints 
images simple approximation person perspective views object infants play 
total images captured object resulting database images 
view sets objects generated images described 
infant directed speech evaluations shape channel extracted images color terms 
prepare corpus processing performed steps 
segment audio utterance boundaries 
done automatically finding contiguous frames speech detected recurrent neural network 
utterance generate view set object play randomly chosen images available images object 
video recordings caregiver infant interactions determine correct object utterance 
utterance image set constituted av event 
input learning system consists sequence av events system order utterances observed infant interactions 
audio visual data corresponding speakers processed separately 
top items resulting mmi maximization step evaluated speaker 
noted earlier learning problem posed data set extremely challenging spoken utterances contain words directly refer object play 
example caregiver said phrases look go playing car ball 
cell identify reliable lexical items ball car despite poor correspondences 
described section vii lexical hypotheses analyzed searching maximum mutual information channels 
fig 
presents examples mutual information surfaces actual lexical hypotheses generated speakers experiment 
plot height surface shows mutual information function thresholds left speech segment corresponding word paired images shoe 
resulting surface relatively low values thresholds 
lexical candidate right paired speech segment word dog images dog 
result strongly peaked surface form 
thresholds selected point surface height mutual information maximized 
results results experiments evaluated measures 
acoustic visual prototype generate lexical item pointer source speech recording view set maintained 
interface built allow evaluator listen original speech recording prototype extracted 
interface displayed images corresponding view set 
tool assess results 
lexical item evaluated different measures measure segmentation accuracy start speech prototype correspond word boundaries english 
measure word discovery speech segment correspond single english word 
accepted words attached articles inflections allowed initial final consonant errors 
example words dag dog ag dog initial missing dag dog accepted positive instances measure 
dog counted error 
measure semantic accuracy lexical item passes second measure visual prototype associated correspond word meaning 
lexical item fails measure automatically fails measure 
possible apply measure acoustic model visual prototype carried input output 
effect model assumes speech segment selected prototype lexical candidate best choice visual association occurred 
comparison ran system acoustic input 
case meaningful mmi method ieee transactions multimedia vol 
june table contents ltm cell process participant data system searched globally recurrent speech patterns speech segments repeated entire set recordings speaker 
acoustic model may thought rough approximation minimum description length approach finding highly repeated speech patterns words language 
table lists contents lexicon generated cell participants 
phonetic text transcript speech prototype manually generated 
text transcripts asterisks placed start entry indicate presence segmentation error 
example dog indicates cutoff additional phonemes word erroneously concatenated target word 
lexical item list associated object visual information 
letters distinguish different objects object class 
phoneme transcripts indicator ono indicate sounds ruf ruf sound dog car 
corresponding text transcript shows type sound parentheses 
extremely difficult establish accurate boundaries words instances 
reason lexical items disregarded measures performance 
interesting note cell link objects appropriate sounds 
considered meaningful cell terms object shapes 
finding consistent infant learning young children commonly observed sounds refer common objects 
reason items processed due stated difficulties assessing segmentation accuracy 
final columns show item passed criterion accuracy measure 
cases word fire associated fire truck shoe 
accepted valid measure clearly grounded specific objects 
bottom table measures accumulated calculate accuracy measure 
comparison lexical items acquired acoustic model shown table ii 
results derived participant data table cases discernible words heard text transcript left blank 
cell performed acoustic model measures 
similar results subjects 
table iii summarizes performance cell roy grounded spoken language acquisition table ii contents ltm acoustic model process data participant table acoustic model speakers study 
cross modal learning achieved higher scores exception 
measure segmentation accuracy poses extremely difficult challenge dealing raw acoustic data 
acoustic model produced lexical items corresponded perfectly english words lexical items 
contrast lexical items produced cell correctly segmented single words 
half accepted items correctly grounded visual channel fail measure 
example words choose crawl successfully extracted cell associated car ball respectively 
words directly refer objects failed measure 
structural consistency word shape aided system producing segmentations 
measure word discovery approximately lexical items produced cell single words exception segmentation accuracy participant cl acoustic compared cell 
optional articles inflections 
contrast acoustic model performance dropped 
results demonstrate benefit incorporating cross channel information word learning process 
cross channel structure lead fold increase accuracy compared analyzing structure acoustic channel 
measure large difference performance cell acoustic system surprising visual input lexical formation 
cell performance promising hypothesized lexical candidates valid english words linked semantically relevant visual categories 
measures cross channel structure leveraged improve learning performance 
looking agreement different channels input cell able find lexical candidates effectively unsupervised learning 
acoustic model performed considering input received consisted unsegmented speech 
fact learned words acquired cell including go baby 
result suggests ieee transactions multimedia vol 
june table iii summary results measures performance 
percentage accuracy cell caregiver shown 
performance acoustic model shown parentheses speech occurs 
side channels information may serve ground semantics speech leading reduced ambiguity various levels spoken language understanding problem 
ideas exploring grounded speech learning understanding create systems able resolve ambiguities speech signal 
learning cell driven bottom process discovering structure observed sensor data 
plan experiment learning architectures integrate top purpose driven categorization bottom methods 
doing cross channel clusters associations acquired optimized achieve high level goals 
acknowledgment authors acknowledge pentland gorin patel schiele pinker feedback anonymous reviewers 
addition cross channel structure channel structure useful leveraged learning words 
processes learner may attempt determine associations words 
xi 
directions successfully implemented evaluated cell computational model sensor grounded word learning 
implemented system learns words natural video acoustic input signals 
achieve learning difficult problems simultaneously solved segmentation continuous spontaneous speech pre existing lexicon unsupervised clustering shapes colors association spoken words semantically appropriate visual categories 
mutual information metric cross channel comparisons clustering 
system demonstrates utility mutual information combine modes input multisensory learning 
results cell show possible learn segment continuous speech acquire statistical models spoken words providing learning system speech occurring visual input 
visual input serves extremely noisy labels speech 
converse true 
system learns visual categories accompanying speech labels 
resulting statistical models may speech visual recognition words objects 
manually trained data replaced streams sensor data serve labels 
idea may applied variety domains multimodal data available human annotation expensive 
exploring applications robust adaptive human computer interfaces 
current spoken language interfaces process respond speech signals 
contrast humans pay attention context johnson body mind 
chicago il univ chicago press 
lakoff women fire dangerous things 
chicago il univ chicago press 
harnad symbol grounding problem phys 
vol 
pp 

barsalou perceptual symbol systems beh 
brain sci vol 
pp 

pinker language instinct 
new york 
snow mothers speech research input interaction talking children language input acquisition snow ferguson eds 
cambridge cambridge univ press 
huttenlocher smiley early word meanings case object names language acquisition core readings bloom ed 
cambridge ma mit press pp 

siskind naive physics event perception lexical semantics language acquisition ph dissertation mass inst 
technol cambridge 
sankar gorin adaptive language acquisition multi sensory device 
london chapman hall pp 

regier human semantic potential 
cambridge ma mit press 
roy jebara cassell pentland synthetic character guided perception emotion story visual proceedings siggraph 
los angeles ca acm siggraph aug 
becker hinton self organizing neural network discovers surfaces random dot stereograms nature vol 
pp 

de sa ballard category learning multi modality sensing neural comput vol 

brent efficient probabilistically sound algorithm segmentation word discovery mach 
learn vol 
pp 

de marcken unsupervised language acquisition ph dissertation mass inst 
technol cambridge 
gorin automated language acquisition 
soc 
amer vol 
pp 

gorin wright riccardi acoustic morphemes lattices spoken language understanding proc 
int 
conf 
spoken language processing 
brooks elephants don play chess robot 
auton 
syst vol 
pp 

steels vogt grounding adaptive language games robotic agents proc 
th eur 
conf 
artificial life 
roy learning words sights sounds computational model ph dissertation mass inst 
technol cambridge 
integration speech vision mutual information proc 
icassp istanbul turkey 
roy grounded spoken language acquisition williams stevens linguistic experience alters phonetic perception infants months age science vol 
pp 

lalonde development speech perception initial capabilities emergence phonemic categories develop 
psychol vol 
pp 

robinson application recurrent nets phone probability estimation ieee trans 
neural networks vol 
pp 
mar 
hermansky morgan rasta processing speech ieee trans 
speech audio processing vol 
pp 
oct 
werbos backpropagation time proc 
ieee vol 
pp 

seneff zue transcription alignment timit database getting started darpa timit cd rom acoustic phonetic continuous speech database garofolo ed 
gaithersburg md nist 
rabiner tutorial hidden markov models selected applications speech recognition proc 
ieee vol 
pp 
feb 
wright carey statistical models topic identification phoneme substrings proc 
icassp pp 

rose word spotting continuous speech utterances automatic speech speaker recognition lee paliwal eds 
norwell ma kluwer ch 
pp 

color vision hue categorization young human infants exper 
psychol human percept 
perf vol 
pp 

infant discrimination internal external pattern elements exper 
child psychol vol 
pp 

schiele crowley probabilistic object recognition multidimensional receptive field histograms proc 
th int 
conf 
pattern recognition icpr vol 
august pp 

cover thomas elements information theory 
new york wiley interscience 
pinker language learnability language development 
cambridge ma harvard univ press 
grimshaw form function language acquisition device logical problem language acquisition baker mccarthy eds 
cambridge ma mit press pp 

deb roy received degree computer engineering university waterloo waterloo canada ph degrees massachusetts institute technology mit cambridge 
assistant professor media arts sciences mit media laboratory directs cognitive machines group 
