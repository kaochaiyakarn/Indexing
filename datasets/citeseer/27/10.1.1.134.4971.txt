comparison optimization heuristics data mapping problem summary nikos computer science engineering university notre dame notre dame mansour computer science department american university lebanon geo rey fox northeast parallel architectures center syracuse university syracuse ny technical report tr december compare performance heuristics suboptimal solutions data distribution dimensional meshes numerical solution partial di erential equations pdes multicomputers 
data mapping heuristics evaluated respect criteria covering load balancing interprocessor communication exibility ease class single phase iterative pde solvers 
evaluation suggests simple fast block distribution heuristic ective complex computational expensive algorithms 
supported alex nason prize award part cornell theory center 

single phase iterative pde solvers considered mapping discrete pde operator linear system algebraic equations ax associated computations processors multicomputer 
commonly single program multiple data programming model processors execute program independently process parts linear system assigned 
processor pi computes unknowns subsystem communicates processors nonlocal global data needed 
execution time data parallel solver max communicate assuming computation communication overlap 
equation particularly relevant loosely synchronous class iterative solvers considered 
loosely synchronous model computations carried phases 
phase consists computations local subproblem followed communication nonlocal data 
parallel iterative pde solvers data mapping problem formulated di erent levels geometric level discrete geometrical data structures element meshes tensor grids associated pde domain ii algebraic level linear system algebraic equations associated discretization continuous pde problem 
evaluate data mapping strategies geometrical data structures 
minimization execution time data parallel iterative solvers requires equal distribution processors workload calculation communication minimization overheads due communication nonlocal unknowns global parameters convergence tests 
problem nding data distributions minimize intractable optimization problem 
heuristics proposed nding suboptimal solutions 
heuristics greedy schemes divide conquer method 
examples nearest neighbor mapping block partitioning recursive coordinate bisection recursive graph bisection recursive spectral bisection cm clustering scattered decomposition 
algorithms deterministic optimization local search techniques minimize cost functions approximate execution time examples kernighan lin algorithm geometry graph partitioning 
class mapping algorithms physical optimization employs techniques natural sciences examples neural networks simulated annealing genetic algorithms 
deal published data mapping attempts comparing algorithms aggregate limited number performance measures 
measures evaluate compare performances data mapping heuristics irregular iterative pde computations 
heuristics considered block partitioning recursive spectral bisection geometry graph partitioning neural network algorithm simulated annealing algorithm genetic algorithm 
heuristics popular frequently data mapping algorithms literature 
report performance data machinedependent machine independent data obtained parallel 
ndings evaluation summarized table compares algorithms respect measures load balance submesh connectivity splitting submeshes message size exibility number machine parameters execution time 
evaluation suggests simple fast mapping algorithm ective complex computationally expensive algorithms 
organized follows 
section describe communication requirements class parallel iterative pde solver approaches mathematical formulation data mapping problem 
section brief description data mapping algorithms evaluate 
section comparisons criteria actual performance data parallel pde solver ndings summarized table 
section 

data mapping parallel iterative pde solvers anumber iterative pde solvers solution discrete linear system algebraic equations reduced matrix vector multiplication see 
multiplication distributed memory mimd machines implemented steps local communication local computation see 
high level description class parallel iterative solvers pertinent data mapping steps local communication ii local computation iii global synchronization 
local communication consists exchange messages processors parallel machine messages transfer local data inner outer interface unknowns see required neighbor subdomains 
local computation mainly consists matrix vector vector vector operations 
global synchronization consist reduction operations required acceleration convergence checking stopping criteria 
objective function re ects computation communication cost parallel iterative pde solver mapping data mesh processors distributed memory multicomputer typically written max dh dh dh fd function maps submeshes processors pi computational load processor iteration proportional number nodes cost notice interested single phase iterative pde solvers consider solvers multiple phases di erent communication requirements phase preconditioning multigrid hp re nement methods 
interior point interface points submesh local unknowns non local unknowns interior points inner interface points correspond local unknowns stored local memory outer interface points correspond non local unknowns stored remote memory 
local matrix vector multiply right need values non local unknowns form processors geometrically adjacent subdomains 
communication required iteration processors dh dh nally dh set submeshes adjacent cardinality henceforth referred submesh connectivity 
formulation assumes computation communication overlap 
approaches minimum computation load pi near evenly distributed processors communication cost processors minimum 
clearly conditions necessary minimizing equation 
synchronization term explicitly re ected di cult expressed quantitatively 
synchronization cost nonlinear function communication computation communication computation overlapping 
term explicitly re ected network bus contention term appears implicitly communication term dh dh dh network contention factors depends minimized ort minimize communication processor dh considered literature reasonable approximation execution time 
approaches identi ed literature minimization 
rst approach expansion components explicit machine dependent algorithm dependent parameters 
approach adopted physical optimization methods guided objective function 
smooth function minimization gives rise minimax criterion computationally expensive 
avoid shortcomings approximate objective function px jd px dh dh scaling factor expressing relative importance communication term respect computation term dependent solver equal number oating point operations mesh node iteration 
equivalent represents approximation 
rst term quadratic deviation computation loads average computation load minimal deviations zero 
second term minimized sum interprocessor communication costs minimized 
enjoys smoothness computational locality change due remapping node pi pj determined information pi pj 
note shares ability tradeo computation workload communication cost purpose minimizing total sum 
cost interprocessor communication di cult expressed accurately 
depends hardware software components multicomputer impossible quantify 
expressions 
expression proposed multicomputers interconnection network cp dh message start time latency machine time communicating word communication time unit distance dh number interface nodes submeshes determines message size dh physical hamming distance 
note inclusion equation accounts cost submesh connectivity 
second expression communication cost processors di dj physical distance processors message size cd dh cd appeared literature mid relevant early multicomputer machines 
advantage computing incremental change cd faster computing cp 
physical optimization algorithms considered employ incremental changes basic step cient computation change important ciency physical algorithms 
second mapping approach uses qualitative criteria derived mapping requirements outline paragraph 
approach address data mapping problem distinct phases partitioning phase allocation phase 
partitioning phase mesh decomposed submeshes criteria approximately satis ed maximum di erence number nodes submeshes minimum ii ratio number interface nodes number interior nodes submesh minimum iii number submeshes adjacent submesh minimum allocation phase submeshes distributed processors criterion satis ed iv communication requirements underlying computation processors architecture minimum 
mesh nodes merit partition non overlapping submeshes fd characterized terms set geometrically adjacent submeshes submesh ii terms number interface mesh nodes dh shared submeshes optimal partitioning de ned criteria iii viewed simultaneously minimizes max jd jj max dh dh dh max 
data mapping algorithms section brie review algorithms solution data mapping problem algorithm recursive spectral bisection algorithm geometry graph partitioning algorithm neural network algorithm simulated annealing algorithm genetic algorithm 
algorithms rst formulation data mapping problem rst algorithms adopt second approach 
partitioning algorithm simple attractive mapping method considered researchers see called data strip block partitioning heuristic 
heuristic referred di erent names dimensional strip partitioning dimensional strip partitioning multilevel load balanced method median splitting sector splitting block partitioning algorithm 
referring block partitioning algorithm algorithm number submeshes blocks strips axis number sub meshes blocks strips axis total number submeshes domains 
allocation phase partitioning algorithm hypercube mesh architectures utilizes identity mapping order distribute submeshes processors 
indexing submeshes results form gray code guarantees geometrically neighbor submeshes allocated physically neighbor processors 
algorithm partitioning meshes nonconvex geometry produces submeshes connected component 
avoid splitting submeshes components star shaped nonconvex domains boundary conforming algorithm 
generalization algorithm handles shaped domains appears 
description algorithm mesh points 
algorithm sort mesh points coordinate axis assign ith set points gray code submesh endfor sort mesh points coordinate axis assign jth set points gray code submesh gray code submesh endfor recursive spectral bisection recursive spectral bisection rsb utilizes spectral properties laplacian matrix associated mesh 
rsb recursively bisects mesh log times 
submeshes distributed processors allocation methods results report identity distribution function submesh id assigned processor rsb implementation enumerates submeshes adjacent submeshes assigned possible physically neighbor processors 
laplacian matrix de ned li vertex joined edge degree vertex 
bisection step eigenvector corresponding second largest eigenvalue laplacian matrix computed components vector provide distance information nodes mesh 
nodes sorted values eigenvector components 
sorted list nodes split form equal size submeshes 
outline rsb 
recursive spectral bisection 
compute second fiedler eigenvector laplacian matrix 
sort nodes values fiedler components 
assign half nodes submesh 
repeat recursively steps submeshes geometry graph partitioning algorithm local optimization algorithms search nite perturbations initial solution perturbation lower cost function 
examples perturbations graph partitioning problem appear literature see 
feasible solutions called neighbors result nite number consecutive perturbations set neighbors called neighborhood structure 
simplest neighborhood structure partitioning graph initial way partitioning set ns partitionings obtained partitioning single swap operation swap operation perturbation forming de ned fbg fag local optimization algorithm initial solution neighborhood structure performs local search neighborhood replaces current solution neighbor solution optimizes minimizes maximizes cost function process repeated better solution exists 
locally optimal solution identi ed 
outline local optimization algorithm described 
local optimization algorithm improve initial solution improve null improve return endwhile cost cost null 
geometry graph partitioning ggp heuristic local optimization algorithm 
ggp heuristic uses geometrical properties mesh graph euclidean graph order deliver quasi uniform partitionings minimal diameter 
cost function ggp algorithm minimizes px ei dk ej ei ej ei ej adjacent di erent subdomains ei ej 
algorithms longer history discussed elementary text computer science john rice analyzed mathematically early stanley reiter 
criteria ii iii see section imposed implicitly minimization cost function seeking solutions optimize certain function known pro function 
pro function ai bi ai bi ca ai bi dai ca ra 
ai bi 
ai bi ai dbi ca ra cb dai cb bi ai bi rb dbi cb rb ce denotes set adjacent nodes node number nodes adjacent node ai bi ca cb mass center subdomains see 
dai ca dbi cb distances nodes ai bi mass centers ca cb subdomains respectively 
ra rb ideal radius subdomains positive weights 
cb ra left illustration mass centers ca cb distances dai ca dbi cb radii ra rb quadrilateral sub meshes 
right plot values cost function distance mass centers ca cb axis subdomains way partition 
axis represents swaps interface points take place execution ggp algorithm pro function ggp algorithm weighted combination pro function kl algorithm function selecting pairs mesh nodes swapping reduces diameter subdomains 
ggp algorithm climbs local minima cost function swapping points increase temporarily value cost function decrease diameter subdomains bringing mass centers far apart 
easy see simple convex geometries way partitioning decrease diameter subdomains implies decrease interface points 
genetic algorithms genetic algorithms gas population candidate solutions called individuals evolve successive generations starting random solutions 
generation individuals selected reproduction tness genetic operators applied selected mates spring replace parents 
process tness gradually increased optimal solutions evolve propagation combination high performance building blocks 
details data distribution heuristics gas see outline genetic algorithm 
genetic algorithm random generation initial population repeat evaluate tness individuals allocate reproduction trials size step select parents list trials apply crossover mutation hill climbing spring endfor convergence solution fittest 
genetic algorithm data mapping encodes individual string integers integer refers processor position string represents mapped mesh node 
tness individual reciprocal value objective function maximizing tness correspond minimizing objective function 
objective function involving communication cost function cp equation 
reproduction scheme determines individuals survive selects pairs surviving individuals reproduction 
scheme involves sorting individuals ascending order 
individuals assigned survival probabilities uniform scale values 
number reproduction trials copies individual determined probabilities 
obviously zero trials means death trials allow 
match making individuals done random choice list reproduction trials 
reproduction scheme illustrated simple example shown 
springs generated applying genetic operators selected parents 
genetic operators employed ga point crossover mutation 
crossover accomplished randomly selecting equal length substrings parents swapping reproduction scheme genetic algorithm 

mutation refers randomly remapping randomly chosen mesh node 
crossover applied individuals population rate mutation 
step creating new generation greedy hill climbing procedure applied spring solutions improving structure 
procedure considers interface mesh nodes candidate solution allows remapping interface nodes overloaded underloaded processors 
remapping invoked negative 
simulated annealing simulated annealing sa starts initial random mapping solution corresponds system high energy temperature state energy objective function 
sa algorithm reduces temperature system gradually freezing point cooling schedule 
temperature regions solution space searched metropolis algorithm 
iteration metropolis algorithm starts proposing random perturbation evaluating resultant change 
perturbation move accomplished random remapping randomly chosen mesh node 
remapping leads lower objective function value corresponds downhill move energy landscape accepted 
increase objective function uphill move may accepted probability perturbations repeated temperature thermal equilibrium 
equilibrium reached number attempted accepted perturbations equal predetermined maximum numbers 
maximum number attempts allowed maximum number accepted moves 
initial temperature determined probability accepting uphill moves initially 
freezing point isthe temperature probability isvery small 
cooling schedule determines temperature fraction 
perturbations followed computation occur inner iteration sa algorithm 
important compute ciently possible 
chosen cd pi pj equation communication component computing cd faster computing cp 
choice improves sa execution time ects quality mapping solutions discussed 
details data distribution graph partitioning heuristics sa see 
outline simulated annealing algorithm data mapping 
simulated annealing algorithm initial con guration random data mapping determine initial temperature determine freezing temperature converged repeat perturb mapping solution update mapping soln accept perturbation random update mapping solution reject perturbation equilibrium cooling schedule endwhile neural network algorithm hop eld type neural network data mapping described aims quickly nding low minima objective function 
network represented matrix neurons 
row corresponds mesh node number neurons row equal log neuron associated neural variable refers column network 
nn starts initial random neural values converges xed point number sweeps 
xed point network associated minimum energy function 
nn repeats procedure log times time determining bits column network submeshes mesh nodes mapped 
iteration mesh partitioned submeshes mapped processors 
derive network equation neural variables replaced magnetic spin variables energy expression 
associate spin mesh node mean eld approximation technique physics derive spin update equation appropriate scaling factors spin coupling matrix mesh graph refers current submesh bisected belongs 
second term interpreted interaction aligns neighboring spins 
third term interpreted long range force responsible global spin balance 
rst term nn equation noise term tries ip current helps system avoid local minima 
note message latency information missing equation equation derived assuming communication cost function cd 
outline neural network algorithm nn 
neural network algorithm log generate random spins repeat spins pick spin randomly compute equation endfor convergence determine bit neurons endfor graph contraction previous shown physical optimization algorithms slow mapping large problems 
execution time unacceptable compared typical time solving problems mapped 
physical optimization algorithms practical suggested graph mesh contraction reducing size problem parameter size contracted mesh approximately contracted mesh mapped mapping solution interpolated 
simple cient graph contraction heuristic algorithm developed description 
allocation phase allocation phase submeshes generated algorithms assigned processors heuristics described evaluation geometry allocation gba algorithm 
gba algorithm rst projects partitioning graph graph vertices submeshes edges de ned connectivity submeshes processors interconnection graph euclidean space problems 
allocation problem reduced easier np complete problem planar assignment problem 
optimal solutions planar assignment problem obtained global techniques spectral methods see local search techniques 
gba uses local search algorithm optimize objective functions rectilinear manhattan distance communicating processors ii distance centers subdomains projection points represent processors euclidean space 
minimization objective functions results allocation neighbor subdomains neighbor processors 

performance evaluation section discus machine independent machine dependent performance analysis data mapping approaches algorithms block partitioning direction section rsb recursive spectral bisection section ggp geometry graph partitioning section ga genetic algorithm section sa simulated annealing section nn neural network section performance algorithms strongly depends test cases geometry mesh 
reason compare algorithms model problem de ned general non convex domain holes 
domain includes geometric characteristics appear real applications 
provides fair test case comparison algorithms possesses properties convexity simply connectivity allow algorithms rsb perform better general local search optimization algorithms 
choice data mapping methods independent pde operator choose poisson pde operator dirichlet boundary conditions external boundary mesh domain consists elements nodes 
pde operator discretized bilinear nite element method resulting linear system equations equations solved jacobi semi iterative jacobi si method 
model problem 
experimental results described physical optimization algorithms problem dependent parameter values normalized respect machine time oating point operation 
communication parameters relevant target machine ncube ii 
suitable values contraction parameter chosen mapping instance 
machine independent analysis machine independent measures consider submesh connectivity ii number interface nodes iii splitting submeshes iv interior interface nodes submeshes 
analysis distributions mesh ncube ii con gurations processors 
distributions average maximum values di erent measures computed plotted 
shows average maximum number total submesh interface nodes message size term communication cost equation proportional number submesh interface nodes 
shows ggp rsb yield smallest number interface nodes 
rsb tries minimizes node separator mesh ggp algorithm tries minimizes node separator submesh diameter tries maximize inter center distance submeshes 
ga sa yield number interfaces nn yields largest number interfaces 
number interfaces appears weighted term communication cost component objective function ga sa implicitly incorporated nn update equation 
graph contraction pre mapping step speeding physical optimization algorithms increase length submesh interfaces due contracted super nodes produces 
shows average maximum submesh connectivity total message latency proportional submesh connectivity processor send messages processors sequential manner 
indicates ga ggp rsb yield connectivities 
expected ga objective function explicitly includes signi cant message latency cost equation 
minimum node separator requirement sought ggp rsb help minimizing submesh connectivity meshes 
shows connectivity values nn worse nn account connectivity update equation 
sa yield connectivity values 
gives standard deviation number nodes submesh partitioned submeshes 
deviation values illustrate balanced computational load 
clearly rsb ggp produce near equal distributions points processors algorithms rst optimize criterion 
physical algorithms insist perfect load balance 
aim minimize total sum computational load communication cost 
produce mapping large imbalances er tradeo computation load communication cost individual processors aim minimizing total workload slower processors 
shows bar charts ga rsb components total workload processors mesh mapped 
clearly average number interfaces average connectivity number processors ga nn sa rsb ggp maximum interfaces ga nn sa rsb ggp number processors average maximum number interface nodes mapping solution 
ga nn sa rsb ggp number processors maximum connectivity ga nn sa rsb ggp number processors average maximum connectivity submeshes mapping solution 
processors processors ga nn sa rsb ggp standard deviation number nodes submesh connectivity avr distance interfaces mesh points subdomains connectivity avr distance interfaces mesh points subdomains bar charts depict tradeo load balance communication ga left rsb right 
chart consists groups axis bars depict connectivity average distance number interfaces divided total number nodes divided submesh 
axis metric 
shows rsb produces perfect load balance regardless communication cost individual processors true ggp 
ga reduces computational load communication cost large 
processors increases communication cost small 
processors 
sa nn algorithms involve tradeo table summary results algo 
load bal 
connect 
interf comp comm 
params map 
code tradeo submeshes speed perfect fast secs rsb perfect slow mins ggp perfect slow mins nn acceptable acceptable limited fast mins sa extr 
slow mins ga slow mins table summarizes comparison data mapping algorithms respect criteria 
note table re ects quality contracted graphs nn sa ga execution times algorithms reduced farther optimizations data structures implementation 
shows data partitions generated data mapping algorithms 
solutions show disconnected subdomains nn sa ga ggp rsb 
submeshes produced top left rsb top right ggp algorithm center left ga center right nn algorithm bottom left sa bottom right mesh 
machine dependent analysis machine dependent measures total elapsed execution time pde solver interprocessor communication time reported times seconds 
solver executed ncube ii processors 
table elapsed communication time seconds model problem ncube ii processors 
elapsed rsb ggp nn sa ga maximum mean val dev 
comm 
time rsb ggp nn sa ga maximum mean val dev 
elapsed rsb ggp nn sa ga maximum mean val dev 
comm 
time rsb ggp nn sa ga maximum average dev 
table presents maximum average standard deviation values 
tables show ids processors parenthesis maximum 
tables summarized observations di erence maximum best worst values nn value processors ii processor maximum processor maximum algorithms perfect load balance 
model see equation usually adopted literature assuming processors computational loads processor maximum slowest processor 
experiments see deviation logic overheads due imperfect balance processors workload due computation communication course synchronization 
observation indicates model see equation usually adopted literature complete 
model ignores runtime ects network bus contention processors idle time due synchronization 
machine independent analysis fact fast ective data distribution algorithms suggest block distribution method sequential solution data mapping problem dimensional static unstructured meshes 

performance evaluation results mapping algorithms pde computations irregular dimensional meshes 
experimental results concerned performance data distribution algorithms respect measures 
evaluation algorithms summarized table selecting mapping algorithm suits di erent application requirements 
example applications mesh times mapping algorithms slower execution time better solution quality chosen 
machine dependent performances algorithms di er great amount 
table shows algorithms satisfy mapping criteria better degree slow 
sa ga ggp rsb involve intricate parameter dependence 
ga sa 
ndings sequential ab initio mapping dimensional meshes comparisons block partitioning greedy algorithms ones cuthill mckee ordering schemes indicate simplest fast data mapping method ective data distribution method solution data mapping problem single phase iterative pde solvers 

acknowledgments authors editors detailed comments improved presentation 
horst simon making available rsb code purdue university providing processor ncube ii execution parallel 
berger 
partitioning strategy nonuniform problems multiprocessors 
ieee trans 
computers may pp 


mapping problem 
ieee transactions computers 

communication overhead intel ipsc hypercube 
technical report nas nasa 
houstis houstis rice 
automatic load balanced partitioning strategies pde computations 
houstis gannon editors proceedings international conference supercomputing pages 
acm press 
houstis houstis 
geometry mapping strategies pde computation 
houstis gannon editors proceedings international conference supercomputing pages 
acm press 
houstis houstis rice 
domain software tool mapping pde computations parallel architectures 
editors domain decomposition methods partial di erential equations iv pages siam publications 
houstis kim rice 
parallel iterative methods 
advances computer methods partial di erential equations vii 
knight richter eds imacs new brunswick nj pages 
houstis houstis 
parallelization level blas operations distributed memory machines 
advances computer methods partial di erential equations vii 
knight richter eds imacs new brunswick nj pages 
rice 
partitioning heuristics pde computations parallel hardware geometry characteristics 
advances computer methods partial di erential equations vii 
knight richter eds imacs new brunswick nj pages 

mapping pde computations distributed memory mimd machines 
csd tr computer science department purdue university lafayette 
elias houstis john rice 
mapping algorithms software environment data parallel pde iterative solvers special issue journal parallel distributed computing data parallel algorithms programming vol pp april 
de 
software tool load balanced adaptive multiple grids distributed memory computers 
sixth distributed memory computing conference april pp 

dragon gustafson 
low cost hypercube load balance algorithm 
th conf 
hypercube concurrent computers applications 

heuristic approaches task allocation parallel computing 
ph thesis ohio state university 

simple cient automatic fem domain 
computers structures 
flower otto 
optimal mapping irregular nite element domains parallel processors 
parallel computers impact mechanics 
fox 
load balancing loosely synchronous problems neural network 
rd conf 
hypercube concurrent computers applications 
fox 
physical computation 
concurrency practice experience dec 

fox johnson otto salmon walker solving problems concurrent processors 
prentice hall new jersey 
fox 
graphical approach load balancing sparse matrix vector multiplication hypercube 
proceedings ima institute schultz editor pages 
springer verlag 
fox 
review automatic load balancing decomposition methods hypercube 
proceedings ima institute schultz editor pages 
springer verlag 
fox 
architecture problems portable parallel software systems 
technical report sccs npac syracuse university 
fukunaga yamada 
assignment job modules array processors 
ieee transactions circuits systems 
michael gary david johnson 
computers intractability guide theory np completeness 
freeman san francisco 
goldberg 
genetic algorithms search optimization machine learning 
addison wesley 

satoshi goto 
cient algorithm dimensional placement problem electrical circuit layout 
ieee transactions circuits systems cas 
hammond mapping unstructured grid computations massively parallel computers 
phd thesis computer science department rensselaer polytechnic institute troy ny 
hey 
concurrent supercomputing europe 
th distributed memory computing conf 

houstis rice ko yang wang weerawarana 
numerical simulation programming environment parallel mimd machines 
proceedings supercomputing editor pages 
acm press 
houstis byun 
workload partitioning strategy pdes generalized neural network 
technical report csd tr department computer sciences purdue university 
hop eld 
neural networks physical systems emergent collective computational abilities 
proceedings national academy sciences young 
applied iterative methods 
new york 
johnson aragon mcgeoch schevon 
optimization simulated annealing experimental evaluation part graph partitioning 
kirkpatrick gelatt vecchi 
optimization annealing 
science 
kernighan lin 
cient heuristic procedure partitioning graphs 
bell system technical journal feb 
kincaid young grimes 
fortran package solving large sparse linear systems adaptive accelerated iterative methods 
acm transactions mathematical software 
lee aggarwal 
mapping strategy parallel processing 
ieee trans 
computers vol 
april 

mansour geo rey fox 
hybrid genetic algorithm task allocation multicomputers 
international conference genetic algorithms pp july morgan kaufmann publishers 
mansour geo rey fox 
allocating data multicomputer nodes physical optimization algorithms loosely synchronous computations 
concurrency practice experience vol 
number pp october 
mansour choudhary fox 
graph contraction physical optimization methods quality cost tradeo mapping data parallel computers 
international supercomputing conference japan july acm press 
miller 
teng thurston vavasis 
automatic mesh partitioning alan george john gilbert joseph li editors graph theory sparse matrix computation springer verlag 
morrison otto 
scattered decomposition nite elements 
journal scienti computing 
muhlenbein parallel genetic algorithms population genetics combinatorial optimization 
scha er editor proceedings third international conference genetic algorithms pages san mateo 
ncube ncube supercomputers 

set new mapping coloring heuristics distributed memory parallel processors 
proceedings copper mountain conference iterative methods el editor volume pages 
pothen simon liou 
partitioning sparse matrices eigenvectors graphs 
siam matrix anal 
appl july 


lee 
park kim 
cient algorithm graph partitioning problem problem transformation method 
computer aided design 
christos papadimitriou kenneth steiglitz 
combinatorial optimization algorithms complexity 
prentice hall englewood cli nj 
sadayappan 
cluster partitioning approaches mapping parallel programs hypercube 
proceedings supercomputing houstis polychronopoulos editors pages 
springer verlag 
sadayappan 
nearest neighbor mapping nite element graphs processor meshes 
ieee trans 
computers vol 
dec 

saltz wu 
multiprocessors run time compilation 
concurrency practice experience 

horst simon 
partitioning unstructured problems parallel processing 
technical report rnr nasa ames research center mo field ca 
saad schultz 
data communication parallel architectures 
parallel computing 
stout bruce 
intensive hypercube communication 
journal parallel distributed computing 
talbi 

parallel genetic algorithm graph partitioning problem 
houstis gannon editors proceedings international conference supercomputing pages 
acm press 
thompson joe wayne 
numerical grid generation 
north holland new york 
walker 
characterizing parallel performance large scale particle cell plasma simulation code 
concurrency practice experience dec 

williams 
performance dynamic load balancing algorithms unstructured mesh calculations 
concurrency practice experience 


