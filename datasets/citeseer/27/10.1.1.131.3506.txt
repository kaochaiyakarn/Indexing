institute research cognitive science simple maximum entropy models natural language processing adwait ratnaparkhi university pennsylvania walnut street suite philadelphia pa may site nsf science technology center research cognitive science ircs report simple maximum entropy models natural language processing adwait ratnaparkhi dept computer information science university pennsylvania adwait cis upenn edu may problems natural language processing viewed linguistic classi cation problems linguistic contexts predict linguistic classes 
maximum entropy models er clean way diverse pieces contextual evidence order estimate probability certain linguistic class occurring certain linguistic context 
report demonstrates particular maximum entropy model example problem proves relevant mathematical facts model simple accessible manner 
report describes existing procedure called generalized iterative scaling estimates parameters particular model 
goal report provide detail re implement maximum entropy models described ratnaparkhi reynar ratnaparkhi ratnaparkhi provide simple explanation maximum entropy formalism 
problems natural language processing nlp re formulated statistical classi cation problems task estimate probability class occurring context orp 
contexts nlp tasks usually include words exact context depends nature task tasks context may consist just single word may consist words associated syntactic labels 
large text corpora usually contain information cooccurrence completely specify possible pairs words typically sparse 
problem nd method sparse evidence reliably estimate probability model 
consider principle maximum entropy jaynes states correct distribution maximizes entropy uncertainty subject constraints represent evidence facts known experimenter 
jaynes discusses advantages making inferences basis partial information probability distribution maximum entropy known 
unbiased assignment amount arbitrary assumption information 
explicitly ifa denotes set possible classes denotes set possible contexts maximize entropy logp remain consistent evidence partial information 
representation evidence discussed determines form representing evidence way represent evidence encode useful facts features impose constraints values feature expectations 
feature function events fj 
features constraints form pfj model expectation fj fj constrained match observed expectation pfj pfj fj observed probability training sample model consistent observed evidence meets constraints speci ed 
principle maximum entropy recommends fp pfj arg max 

total table task nd probability distribution constraints total table way satisfy constraints maximizes entropy set consistent models section shows ky fj normalization constant model parameters 
parameter corresponds exactly feature fj viewed weight feature 
simple example example illustrates maximum entropy simple problem 
suppose task estimate probability distribution fx yg 
furthermore suppose fact known 
constraint implicit probability distribution 
table represents cells labelled values consistent constraints 
clearly nitely consistent ways ll cells table way shown table 
principle maximum entropy recommends assignment table non committal assignment probabilities meets constraints formally maximum entropy framework fact implemented constraint model expectation feature epf total table uncertain way satisfy constraints epf de ned follows fx yg observed expectation ore pf 
objective maximize fx yg logp subject constraint 
assuming features map event constraint feature expectation simply constraint sum certain cells table represents event space 
constrained maximum entropy problem solved trivially inspection iterative procedure usually required larger problems multiple constraints may overlap ways prohibit closed form solution 
features typically express cooccurrence relation linguistic context particular prediction 
example ratnaparkhi estimates model wherea possible part speech tag contains word tagged things 
useful feature fj determiner observed expectation pfj feature number times expect see word tag determiner training sample normalized number training samples 
advantage maximum entropy framework experimenters need focus orts deciding features 
extent feature fj contributes weight automatically determined iterative scaling algorithm 
furthermore kind contextual feature model model ratnaparkhi uses features look tag bigrams word pre xes single words 
section discusses preliminary de nitions section discusses maximum entropy property model form section discusses relation maximum likelihood estimation section describes generalized iterative scaling algorithm 
preliminaries de nitions introduce relative entropy notation 
lemmas describe properties relative entropy measure 
de nition relative entropy liebler distance 
relative entropy probability distributions de nition 
log set possible classes set possible contexts nite training sample events observed probability model probability fj function type fj pfj fj fp pfj ky fp logp logp fj event space denotes probability distribution de ned set probability distributions consistent constraints set probability distributions form entropy proportional log likelihood sample distribution lemma 
probability distributions proof see cover thomas 
lemma pythagorean property 
de nition fact discussed csiszar della pietra 
term pythagorean re ects fact property equivalent tothe pythagorean theorem geometry vertices right triangle squared distance function 
proof 
note andt log fj log log log fj log log fj log fj log substitution andp log logp logp logq log logp logp logq logp logq maximum entropy lemmas derive maximum entropy property models form satisfy constraints theorem 
arg maxp 
furthermore unique 
proof 
suppose uniform distribution jej show lemma lemma show unique log jej log jej maximum likelihood secondly models form satisfy alternate explanation maximum likelihood framework theorem 
arg maxq 
furthermore unique 
proof 
observed distribution sample clearly suppose show lemma lemma show unique theorems state arg maxp arg maxq unique 
viewed maximum entropy framework maximum likelihood framework 
duality appealing maximum likelihood model data closely possible maximum entropy model assume facts constraints 
parameter estimation generalized iterative scaling darroch ratcli gis procedure nds parameters kg unique distribution gis procedure requires constraint kx fj constant 
case choose max kx fj add correction feature fl fl kx fj note existing features fl ranges wherec greater 
furthermore gis procedure assumes events atleast feature active fj fj theorem 
procedure fj fj fj ly fj see darroch ratcli proof convergence 
darroch ratcli show likelihood non decreasing implies 
see della pietra proof improved iterative scaling nds parameters correction feature 
see csiszar geometric interpretation gis 
computation iteration gis procedure requires quantities pfj 
computation pfj straightforward training sample bn merely normalized count pfj nx ai bi fj ai bi nx fj ai bi number event tokens opposed types 
computation model feature expectation fj fj model overlapping features intractable consist distinguishable events 
approximation originally described lau fj nx bi fj bi sums contexts computation tractable 
procedure terminate xed number iterations change log likelihood negligible 
running time iteration dominated computation npa training set size number predictions average number features active event 
report presents relevant mathematical properties maximum entropy model simple way contains information reimplement models described ratnaparkhi reynar ratnaparkhi ratnaparkhi 
model convenient natural language processing allows unrestricted contextual features combines principled way 
furthermore generality allows experimenters re di erent problems eliminating need develop highly customized problem speci estimation methods 
cover thomas cover thomas 

elements information theory 
wiley new york 
csiszar csiszar 

divergence geometry probability distributions minimization problems 
annals probability 
csiszar csiszar 

geometric interpretation darroch ratcli generalized iterative scaling 
annals statistics 
darroch ratcli darroch ratcli 

generalized iterative scaling log linear models 
annals mathematical statistics 
della pietra della pietra della pietra la erty 

inducing features random fields 
technical report cmu cs school computer science carnegie mellon university 


maximum entropy hypothesis formulation especially multidimensional contingency tables 
annals mathematical statistics 
jaynes jaynes 

information theory statistical mechanics 
physical review 
lau lau rosenfeld roukos 

adaptive language modeling maximum entropy principle 
proceedings human language technology workshop pages 
arpa 
ratnaparkhi ratnaparkhi 

maximum entropy part speech tagger 
conference empirical methods natural language processing university 
ratnaparkhi ratnaparkhi 

statistical parser maximum entropy models 
appear second conference empirical methods natural language processing 
reynar ratnaparkhi reynar ratnaparkhi 

maximum entropy approach identifying sentence boundaries 
fifth conference applied natural language processing pages washington 
