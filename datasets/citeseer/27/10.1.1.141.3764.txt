document created framemaker proc 
th acm symposium operating systems principles copper mountain colorado december net user level network interface parallel distributed computing thorsten von eicken anindya basu vineet werner vogels department computer science cornell university ithaca ny net communication architecture provides processes virtual view network interface enable userlevel access high speed communication devices 
architecture implemented standard workstations offthe shelf atm communication hardware removes kernel communication path providing full protection 
model net allows construction protocols user level performance limited capabilities network 
architecture extremely flexible sense traditional protocols tcp udp novel abstractions active messages implemented efficiently 
net prototype node atm cluster standard workstations offers microseconds round trip latency mbytes sec bandwidth 
achieves tcp performance maximum network bandwidth demonstrates performance equivalent meiko cs tmc cm supercomputers set split benchmarks 
increased availability high speed local area networks shifted bottleneck local area communication limited bandwidth network fabrics software path traversed messages sending receiving ends 
particular traditional unix networking architecture path taken messages kernel involves copies crosses multiple levels abstraction device driver user application 
resulting processing overheads limit peak communication bandwidth cause high message latencies 
effect users upgrade ethernet faster network fail observe application speed commensurate improvement raw network performance 
solution situation elude vendors large degree fail recognize importance message overhead concentrate peak bandwidths long data streams 
may justifiable applications video playback applications relatively small messages rely heavily quick round trip requests replies 
increased techniques distributed shared memory remote procedure calls remote object oriented method invocations distributed cooperative file caches increase importance low round trip latencies high bandwidth low latency point 
authors email tve basu vogels cs cornell edu 
software www cs cornell edu info projects net net project supported air force material contract onr contract 
copyright association computing machinery permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage new copies bear notice full citation page 
copyrights components owned acm honored 
abstracting credit permitted 
new application domains benefit higher network performance flexible interface network 
placing protocol processing kernel traditional networking architecture easily support new protocols new message send receive interfaces 
integrating application specific information protocol processing allows higher efficiency greater flexibility protocol cost management 
example transmission mpeg compressed video streams greatly benefit customized retransmission protocols embody knowledge real time demands interdependencies video frames 
applications avoid copying message data sending straight data structures 
able accommodate application specific knowledge communication protocols important order able efficiently utilize network couple communication computation effectively 
promising techniques improve performance flexibility networking layer performance workstation class machines move parts protocol processing user space 
argues fact entire protocol stack placed user level operating system hardware allow protected user level access directly network 
goal remove kernel completely critical path allow communication layers process tailored demands 
key issues arise multiplexing network processes providing protection processes network interfere managing limited communication resources aid kernel path designing efficient versatile programming interface network 
issues solved parallel machines thinking machines cm meiko cs ibm sp allow user level access network 
machines custom network net interface usually restrict degree form multiprogramming permitted node 
implies techniques developed designs applied workstation clusters directly 
describes net architecture user level communication shelf hardware platform fore systems atm interfaces running standard operating system sunos 
communication architecture virtualizes network device process illusion owning interface network 
protection assured kernel control channel set tear 
net architecture able support legacy protocols novel networking abstractions tcp udp active messages implemented exhibit performance limited processing capabilities network interface 
split state art parallel language performance benchmark programs atm cluster standard workstations rivals current parallel machines 
cases net able expose full potential atm network saturating mbits sec fiber traditional networking protocols advanced parallel computing communication layers 
major contributions propose simple user level communication architecture sections independent network interface hardware allows hardware implementations describe high performance implementations standard workstations section evaluate performance characteristics communication parallel programs sections traditional protocols ip suite section 
researchers proposed user level network interfaces independently presentation full system require custom hardware os modification supports traditional networking protocols state art parallel language implementations 
exclusively uses shelf components system establishes baseline radical proposals include custom hardware new os architectures compared 
motivation related net architecture focuses reducing processing overhead required send receive messages providing flexible access lowest layer network 
intent fold provide low latency communication local area settings exploit full network bandwidth small messages facilitate novel communication protocols 
importance low communication latencies latency communication mainly composed processing overhead network latency time flight 
term processing overhead refer time spent processor handling messages sending receiving ends 
may include buffer management message copies checksumming flow control handling interrupt overhead controlling network interface 
separating overhead network latency distinguishes costs stemming network fabric technology due networking software layers 
advances network fabric technology dramatically improved network bandwidth processing overheads affected nearly 
effect large messages latency time source application executing send time destination application receiving message dominated transmission time new networks offer net improvement 
small messages local area communication processing overheads dominate improvement transmission time significant comparison 
wide area networks speed light eventually dominant latency component reducing overhead significantly affect latency may improve throughput 
net places strong emphasis achieving low communication overheads small messages increasingly important applications 
example distributed systems object oriented technology finding wide spread adoption naturally extended network allowing transfer objects remote execution methods corba extensions 
objects generally small relative message sizes required high bandwidth bytes vs kbytes communication performance suffers message overhead low 
electronic workplace relies heavily sets complex distributed services intended transparent user 
majority service invocations requests simple database servers implement mechanisms object naming object location authentication protection message size seen systems range bytes requests responses generally range bytes 
limit network traversal larger distributed objects caching techniques fundamental part modern distributed systems 
keeping copies consistent introduces large number small coherence messages 
round trip times important requestor usually blocked synchronization achieved 
software fault tolerance algorithms group communication tools require multi round protocols performance latency limited 
high processing overheads resulting high communication latencies prevent protocols today process control applications financial trading systems multimedia groupware applications 
projecting existing general systems benefit substantially numerous client server architectures rpc style interaction 
drastically improving communication latency requests responses acknowledgments large number systems may see significant performance improvements 
remote file systems categorized bulk transfer systems depend heavily performance small messages 
week long trace nfs traffic departmental cs fileserver uc berkeley shown vast majority messages bytes size messages account roughly half bits sent 
researchers propose networks workstations provide resources compute intensive parallel applications 
order feasible communication costs lans reduce order magnitude comparable modern parallel machines 
importance small message bandwidth communication bandwidth system measured sending virtually infinite stream node 
may representative applications demand high bandwidths sending small messages bytes increasing due trends demand low latencies 
net specifically targets segment net traffic attempts provide full network bandwidth small messages possible mainly reducing message overheads 
reducing minimal message size full bandwidth achieved may benefit reliable data stream protocols tcp buffer requirements directly proportional round trip latency 
example tcp window size product network bandwidth round trip time 
achieving low latency local area networks keep buffer consumption reason feasible achieve maximal bandwidth low cost 
communication protocol interface flexibility traditional unix networking architectures protocol stacks implemented part kernel 
difficult experiment new protocols efficiently support dedicated protocols deal application specific requirements 
easily design protocols datagram primitive offered kernel udp raw ip doing efficiently adding new protocol kernel stack possible 
lack support integration kernel application buffer management introduces high processing overheads especially affect reliable protocols need keep data retransmission 
particular shared buffer management count mechanisms lower copy application kernel transfer overheads 
example kernel implementation reliable transport protocol tcp counts prevent network device driver releasing network buffers remain available possible retransmission 
optimization available application specific reliable protocol implemented user space udp transport mechanism 
removing communication subsystem boundary application specific protocols new protocol design techniques application level framing integrated layer processing applied efficient protocols produced 
compiler assisted protocol development achieve maximum optimization protocols compiled small subset application specific protocols 
specialized settings tight coupling communication protocol application yield higher savings 
example high level language supporting form blocking rpc copy need case retransmission required high level semantics rpc guarantee source data remains unmodified rpc completes successfully 
address large rpc argument may passed directly network interface dma engine 
example moment process requests data remote node may pre allocate memory reply 
response arrives data transferred directly final position allocation intermediate buffers intermediate copies 
advantage techniques key element reducing overhead communication done applications direct access network interface new networking architecture new abstraction high performance communication required deliver promise low latency high bandwidth communication applications standard workstations shelf networks 
central idea net simply remove kernel critical path sending receiving messages 
eliminates system call overhead importantly offers opportunity streamline buffer management performed user level 
research projects pointed eliminating kernel send receive paths requires form message multiplexing demultiplexing device hardware software introduced purpose enforcing protection boundaries 
approach proposed incorporate mux demux directly network interface ni depicted move buffer management protocol processing user level 
essence virtualizes ni provides process illusion owning interface network 
approach raises issues selecting virtual ni abstraction processes providing support legacy protocols side side generation parallel languages enforcing protection kernel intervention message 
related number issues surrounding user level network interface access studied past 
mach operating system combination powerful message demultiplexer microkernel user level implementation tcp ip protocol suite solved network performance problems arose unix single os server responsible network communication 
performance achieved roughly monolithic bsd system application device channel abstraction developed university arizona provides application programs direct access experimental osiris atm board aurora gigabit testbed 
techniques developed osiris board reduce processing overhead pathfinder multiplexor implemented hardware fbufs cross domain buffer management :10.1.1.52.9688
hp bristol mechanism developed lan applications reserve buffer pools board 
data arrives vci associated application data transferred directly correct pool 
application access buffers directly forced go kernel copy operation retrieve data provide data sending 
kernel protocols aware buffer pools exploit fully 
parallel computing community machines thinking machines cm meiko cs ibm sp cray node node ni ni node ni node ni legend user application operating system kernel ni network interface traditional networking architecture places kernel path communication 
net architecture uses simple multiplexing demultiplexing agent implemented hardware data communication path uses kernel set 
ni ni message multiplex provide user level access network solutions rely custom hardware somewhat constrained controlled environment multiprocessor 
hand parallel machines resemble clusters workstations closely reasonable expect concepts developed designs transferred workstations 
successive simplifications generalizations shared memory leading slightly different type solution network accessed indirectly memory accesses 
shrimp uses custom nis allow processes establish channels connecting virtual memory pages nodes data written page side gets propagated automatically side 
thekkath proposes memory network access model separates flow control data flow 
remote memory operations implemented emulating unused opcodes mips instruction set 
shared memory abstraction allows reduction communication overheads clear efficiently support legacy protocols long data streams remote procedure call 
net design goals experience network interfaces parallel machines clear providing user level access network net best avenue offering communication latencies bandwidths mainly limited network fabric time offer full flexibility protocol design integration protocol buffering appropriate higher communication layers 
efforts developing fast implementations tcp internetworking protocols clearly affirm relevance protocols high performance networking new network interface proposal able support protocols effectively typically case parallel machines example 
aspects set net apart proposals discussed focus low latency high bandwidth small messages emphasis protocol design integration flexibility desire meet goals widely available standard workstations shelf communication hardware 
user level network interface architecture net user level network interface architecture virtualizes interface way combination operating system hardware mechanisms provide process illusion owning interface network 
depending sophistication actual hardware net components manipulated process may correspond real hardware ni memory locations interpreted os combination 
role net limited multiplexing actual ni processes accessing network enforcing protection boundaries resource consumption limits 
particular process control contents message management send receive resources buffers 
sending receiving messages net architecture composed main building blocks shown endpoints serve application handle network contain communication segments regions memory hold message data message queues hold descriptors messages sent 
terms process application interchangeably refer arbitrary unprivileged unix processes 
received 
process wishes access network creates endpoints associates communication segment set send receive free message queues endpoint 
send message user process composes data communication segment pushes descriptor message send queue 
point network interface expected pick message insert network 
network backed network interface simply leave descriptor queue eventually exert back pressure user process queue full 
ni provides mechanism indicate message queue injected network typically setting flag descriptor indicates associated send buffer reused 
incoming messages net destination data transferred appropriate communication segment message descriptor pushed corresponding receive queue 
receive model supported net polling event driven process periodically check status receive queue block waiting message arrive unix select call register upcall net 
upcall net signal recv queue free queue net endpoint communication segment net ni send queue application application application endpoint emulated endpoint emulated endpoint kernel endpoint net building blocks 
endpoints serve application handle network communication segments regions memory hold message data message queues send recv free queues hold descriptors messages sent received 
regular endpoints serviced net network interface directly 
emulated endpoints serviced kernel consume additional network interface resources offer level performance 

term upcall general sense refer mechanism allows net signal asynchronous event application 
trap os trap state receive queue satisfies specific condition 
conditions currently supported receive queue non empty receive queue full 
allows event driven reception second allows processes notified receive queue overflows 
net specify nature upcall unix signal handler thread user level interrupt handler 
order amortize cost upcall reception messages important net implementation allows messages pending receive queue consumed single upcall 
furthermore process able disable upcalls cheaply order form critical sections code atomic relative message reception 
multiplexing demultiplexing messages net uses tag incoming message determine destination endpoint appropriate communication segment data message queue descriptor 
exact form message tag depends network substrate example atm network atm virtual channel identifiers may 
case process registers tags net creating communication channels outgoing messages channel identifier place correct tag message possibly destination address route incoming messages tag mapped channel identifier signal origin message application 
net notion message tag similar idea parallel machines including parallel process id header messages 
message tag net general allows communication arbitrary processes parallel process id tag serves communication parallel program running closed environment 
operating system service needs assist application determining correct tag specification destination process route nodes 
operating system service assist route discovery switch path setup signalling tasks specific network technology 
service perform necessary authentication authorization checks ensure application allowed access specific network resources conflicts applications 
path peer determined request passed security constraints resulting tag registered net perform message multiplexing demultiplexing function 
channel identifier returned requesting application identify communication channel destination 
endpoints communication channels allow net enforce protection boundaries multiple processes accessing network depending routes allocated may allow extend boundaries network 
achieved mechanisms endpoints communication segments message queues accessible owning process outgoing messages tagged originating endpoint address incoming messages net delivered correct destination endpoint 
application interfere communication channels application host 
addition set routes carefully controlled collection operating systems cluster hosts protection extended network application directly interfere communication streams parties 
zero copy vs true zero copy net attempts support true zero copy architecture data sent directly application data structures intermediate buffering ni transfer arriving data directly user level data structures 
consideration current limitations bus addressing ni functionality net architecture specifies levels sophistication base level requires intermediate copy networking buffer corresponds generally referred zero copy direct access net supports true zero copy intermediate buffering 
base level net architecture matches operation existing network adapters providing reception model queue free buffers filled net messages arrive 
regards communication segments limited resource places upper bound size feasible regard communication segments memory regions general data structures placed 
means sending message constructed buffer communication segment reception data deposited similar buffer 
corresponds generally called zero copy truth represents copy application data structures buffer communication segment 
direct access net supports true zero copy protocols allowing communication segments span entire process address space letting sender specify offset destination communication segment message data deposited directly ni 
net implementations described support baselevel architecture hardware available support memory mapping required direct access architecture 
addition bandwidth atm network warrant enhancement copy overhead dominant cost 
base level net architecture base level net architecture supports queue interface network stages messages limited size communication segment way application data structures network 
communication segments allocated buffer message data typically pinned physical memory 
base level net architecture send receive queues hold descriptors information destination respectively origin endpoint addresses messages length offsets communication segment data 
free queues hold descriptors free buffers available network interface storing arriving messages 
management send buffers entirely process net architecture place constraints size number buffers allocation policy 
main restriction buffers lie communication segment properly aligned requirements network interface allow dma transfers 
process provides receive buffers explicitly ni free queue control order buffers filled incoming data 

true zero copy achieved base level net need application copy information received data structure 
case data accessed buffers application take action information need copy operation 
simple example reception acknowledgment messages update counters need copied longer term storage 
optimization small messages heavily control messages protocol implementation send receive queues may hold entire small messages descriptors pointers data 
avoids buffer management overheads improve round trip latency substantially 
size small messages implementation dependent typically reflects properties underlying network 
kernel emulation net communication segments message queues generally scarce resources impractical provide process net endpoints 
furthermore applications telnet really benefit level performance 
software engineering reasons may desirable single interface network applications 
solution dilemma provide applications kernel emulated net endpoints 
application emulated endpoints look just regular ones performance characteristics quite different kernel multiplexes single real endpoint 
direct access net architecture direct access net strict superset base level architecture 
allows communication segments span entire address space process allows senders specify offset destination communication segment message data deposited 
capability allows message data transferred directly application data structures intermediate copy buffer 
form communication requires quite synchronization communicating processes parallel language implementations split take advantage facility 
main problem direct access net architecture difficult implement current workstation hardware ni essentially contain mmu kept consistent main processor ni able handle incoming messages destined unmapped virtual memory page 
essence requires ni include form memory mapping hardware ii local physical memory accessible ni iii page faults message arrival handled appropriately 
basic hardware level limited number address lines buses impossible ni access physical memory board mmu difficult support arbitrary sized communication segments 
net implementations net architecture implemented running sunos generations fore systems atm interfaces 
implementation uses fore sba interface similar active messages implementation hardware described 
second implementation uses newer fore sba interface board processor implement net directly 
implementations transport messages aal packets take advantage atm virtual channel identifiers communication endpoints associated transmit receive vci pair net sba fore systems sba interface operates programmed store cells cell deep output fifo retrieve incoming cells cell deep input fifo 

atm connection oriented network uses virtual channel identifiers name way connections 
operation time way send rcv switch trap level send overhead aal receive overhead aal total way table cost breakup single cell round trip aal function performed hardware serializing cells fiber atm header crc calculation 
particular dma payload crc calculation segmentation reassembly multi cell packets supported interface 
simplicity hardware requires net architecture implemented kernel providing emulated net endpoints applications described 
implementation consists loadable device driver user level library implementing aal segmentation reassembly sar layer 
fast traps kernel send receive individual atm cells carefully crafted assembly language quite small instructions send receive traps respectively 
implementation evaluated mhz sparcstation running sunos equipped sba interfaces 
atm network consists mbit taxi fibers leading fore systems asx switch 
round trip time single cell message consequence lack hardware compute aal crc send overhead receive overhead aal processing due crc computation 
cost breakup shown table 
send receive overheads bandwidth limited mbytes packets kbytes 
net sba second generation atm network interfaces produced fore systems sba substantially sophisticated sba includes board processor accelerate segmentation reassembly packets transfer data host memory dma 
processor controlled firmware downloaded board ram host 
net implementation described uses custom firmware implement base level architecture directly sba 
sba consists mhz intel processor kbytes memory dma capable bus interface simple fifo interface atm fiber similar sba aal crc generator 
host processor map sba memory address space order communicate operation 
experimental set consists mhz sparcstation mhz sparcstation workstations connected fore systems asx atm switch mbit taxi fiber links 
fore firmware operation performance complete redesign sba firmware net implementation motivated analysis fore original firmware showed poor performance 
apparent rationale underlying design fore firmware load specifics 
card calculates aal checksum payload aal crc required 
atm adaptation layer processing host processor possible 
kernel firmware interface patterned data structures managing bsd mbufs system streams 
allows traverse data structures dma order determine location message data move network autonomously 
performance potential fore firmware evaluated test program maps kernel firmware interface data structures user space manipulates directly send raw aal pdus network 
measured round trip time approximately maximum bandwidth achieved kbyte packets mbytes sec 
performance discouraging round trip time times larger simpler cheaper sba interface bandwidth reasonable sized packets falls short mbytes sec peak fiber bandwidth 
detailed analysis showed poor performance mainly attributed complexity kernel firmware interface 
message data structures complex necessary having follow linked data structures host dma incurs high latencies 
host processor faster loading easily 
net firmware base level net implementation sba modifies firmware add new net compatible interface main design considerations new firmware virtualize host interface multiple user processes communicate concurrently minimize number host accesses bus 
new host interface reflects base level net architecture directly 
maintains data structure holding protection information open endpoints 
communication segments pinned physical memory mapped dma space receive queues similarly allocated host poll crossing bus send free queues placed sba memory mapped user space poll queues dma transfers 
control interface net consists single resident command queue accessible kernel 
processes system call interface device driver implements kernel resident part net 
driver assists providing protection validating requests creation communication segments related endpoints providing secure interface operating system service manages multiplexing tags net channel registration 
tags atm network consist vci pair implements full duplex communication atm connection oriented network requires explicit connection set net connection oriented 
communication segments message queues distinct endpoints disjoint address space process creates endpoint 
order send pdu host uses double word store resident transmit queue provide pointer transmit buffer length packet channel identifier 
single cell packet sends optimized firmware small messages cell size 
larger sized messages host dma uses byte burst trans 
software engineering reasons new firmware functionality strict superset fore traditional networking layers function new applications faster net 
uam xfer uam raw net bytes net round trip times function message size 
raw net graph shows round trip times simple ping pong benchmark net interface directly 
inset graph highlights performance small messages 
uam line measures performance net active messages reliable single cell requests replies uam xfer uses reliable block transfers arbitrary size 
fetch cells time computes aal crc special sba hardware 
receive cells network periodically polls network input fifo 
receiving single cell messages improve round trip latency small messages 
single cell messages directly transferred receive queue entry large hold entire message avoids buffer allocation extra dma buffer pointers 
longer messages transferred fixed size receive buffers offsets communication segment pulled resident free queue 
cell packet received message descriptor containing pointers buffers dma ed receive queue entry 
performance shows round trip times messages bytes time message go host switch back 
round trip time cell message due optimization low quite par parallel machines cm custom network interfaces placed memory bus allow unix signal indicate message arrival polling adds approximately 
longer messages start bytes cost roughly extra additional cell bytes 
shows bandwidth raw base level net interface mbytes sec message sizes varying bytes kbytes 
clear graph packet sizes low bytes fiber saturated 
memory requirements current implementation pins pages communication segments physical memory maps sba dma space 
addition endpoint set uam xfer uam raw net mbits aal limit raw net uam store get mbytes bytes net bandwidth function message size 
aal limit curve represents theoretical peak bandwidth fiber caused quantization byte cells 
raw net measurement shows bandwidth achievable net interface directly uam store get demonstrate performance reliable net active messages block transfers 
send receive free buffer queues reside mapped user space 
number distinct applications run concurrently limited amount memory pinned host size dma address space memory size 
memory resource management important issue access network interface scalable 
reasonable approach provide mechanism conjunction kernel provide elementary memory management functions allow dynamic allocation dma address space communication segments active user processes 
exact mechanism achieve objective compromising efficiency simplicity interface remains challenging problem 
net active messages implementation performance net active messages uam layer prototype conforms generic active messages gam specification 
active messages mechanism allows efficient overlapping communication computation multiprocessors 
communication active messages form requests matching replies 
active message contains address handler gets called receipt message followed upto words arguments 
function handler pull message network integrate ongoing computation 
request message handler may may send reply message 
order prevent live lock reply message handler send reply 
generic active messages consists set primitives higher level layers initialize gam interface send request reply messages perform block gets stores 
gam provides reliable message delivery implies message sent delivered recipient barring network partitions node crashes catastrophic failures 
active messages implementation uam implementation consists user level library exports gam interface uses net interface 
library simple mainly performs flow control retransmissions necessary implement reliable delivery active messages specific handler dispatch 
flow control issues order ensure reliable message delivery uam uses window flow control protocol fixed window size 
endpoint total transmit receive buffers endpoint communicates 
storage allows requests replies kept case retransmission needed allows request reply messages arrive buffer overflow 
request messages generate reply explicitly acknowledged standard go back retransmission mechanism deal lost requests replies 
flow control implemented flow control mechanism attempt minimize message losses due congestion network 
sending receiving send request message uam processes outstanding messages receive queue drops copy message sent pre allocated transmit buffer pushes descriptor send queue 
send window full sender polls incoming messages space send window time occurs unacknowledged messages retransmitted 
sending reply messages explicit acknowledgments similar flow control window check necessary 
uam layer receives messages explicit polling 
message arrival uam loops receive queue pulls messages receive buffers dispatches handlers sends explicit acknowledgments necessary frees buffers receive queue entries 
active messages micro benchmarks different micro benchmarks run determine round trip times block transfer bandwidths single cell messages round trip time bulk transfers bandwidth bulk store bandwidth bulk get operations 

single cell round trip time estimated repeatedly sending single cell request message bytes data remote host specifying handler replies identical message 
measured round trip times shown start suggests uam overhead raw net includes costs send request message receive reply receive reply 

block transfer round trip time measured similarly sending messages varying sizes back forth hosts 
shows time byte transfer roughly byte cost higher raw net way uam transfer involves copies source data structure send buffer receive buffer destination data structure 

block store bandwidth measured repeatedly storing block specified size remote node loop measuring total time taken 
shows bandwidth reaches aal limit blocks kbytes 
dip performance bytes caused fact uam uses buffers holding bytes data additional processing time required 
peak bandwidth kbytes mbytes 
block get bandwidth measured sending series requests remote node fetch block specified size waiting blocks arrive 
block get performance nearly identical block stores 
summary performance active messages shows net interface suited building higher level communication paradigms parallel languages run times 
main performance penalty uam raw net due cost implementing reliability removing restrictions communication segment size uam send acknowledgment messages copies data buffers communication segment 
large transfers virtually bandwidth loss due extra copies small messages extra overhead copies acknowledgments noticeable 
performance uam close raw net raw interface worthwhile control byte aal packets required compatibility significant benefits achieved customized retransmission protocols 
split application benchmarks split simple parallel extension programming distributed memory machines global address space abstraction 
implemented top net active messages demonstrate impact net applications written parallel language 
split program comprises thread control processor single code image threads interact reads writes shared data 
type system distinguishes local global pointers compiler issue appropriate calls active messages global pointer dereferenced 
dereferencing global pointer scalar variable turns request reply active messages sequence exchange processor holding data value 
split provides bulk transfers map active message bulk gets stores amortize overhead large data transfer 
split implemented cm paragon sp meiko cs ibm sp cray supercomputers net active messages 
small set application benchmarks compare net version split cm meiko cs versions :10.1.1.23.716
comparison particularly interesting cm meiko machines easily characterized respect net atm cluster shown table cm processors slower meiko machine cm meiko cs net atm cpu speed mhz sparc mhz mhz message overhead round trip latency network bandwidth mb mb mb table comparison cm meiko cs net atm cluster computation communication performance characteristics matrix multiply blocks blocks sample sort sml msg bulk msg netw 
cpu cm atm meiko cm atm meiko cm atm meiko cm atm meiko cm atm meiko radix sort small msg cm atm meiko cm atm meiko radix sort bulk msg cm atm meiko cm atm meiko connected components conjugate gradient comparison split benchmarks cm net atm cluster meiko cs 
execution times normalized cm computation communication breakdown shown applications 
atm cluster network lower overheads latencies 
cs atm cluster similar characteristics slight cpu edge cluster faster network cs 
split benchmark set comprised programs blocked matrix multiply sample sort optimized small messages sort optimized bulk transfers radix sorts similarly optimized small bulk transfers connected components algorithm conjugate gradient solver 
matrix multiply sample sorts instrumented account time spent local computation phases communication phases separately time spent related processor network performance machines 
execution times runs processors shown times normalized total execution time cm ease comparison 
matrix multiply uses matrices blocks double floats 
main loop multiplies blocks prefetches blocks needed iteration 
results show clearly cpu network bandwidth disadvantages cm 
sample sort sorts array bit integers arbitrary distribution 
algorithm samples keys permutes keys sorts local keys processor 
version optimized small messages packs values message permutation phase optimized bulk transfers local values processor sends exactly message processor 
perfor mance shows cpu disadvantage cm small message version machine message overhead advantage 
atm cluster meiko come roughly equal slight cpu edge atm cluster slight network bandwidth edge meiko 
bulk message version improves meiko atm cluster performance dramatically respect cm lower bulk transfer bandwidth 
performance radix sort connected components benchmarks demonstrate net atm cluster workstations roughly equivalent meiko cs performs worse cm applications small messages small message radix sort connected components better ones optimized bulk transfers 
tcp ip udp ip protocols 
success new abstractions depends level able support legacy systems 
modern distributed systems ip protocol suite plays central role availability platforms provides portable base large classes applications 
benchmarks available test various tcp ip udp ip implementations focus bandwidth latency function application message size 
unfortunately performance kernelized udp tcp protocols sunos combined vendor supplied atm driver software disappointing maximum bandwidth udp achieved large transfer sizes larger kbytes tcp offer maximum achievable bandwidth 
observed round trip latency worse small messages latency udp tcp messages larger atm going ethernet simply reflect increased network performance 
shows latency fore atm protocols compared ethernet 
tcp udp modules implemented net base level net functionality 
low overhead net protocol processing resulting low latency form basis ethernet tcp ethernet udp fore atm tcp fore atm udp bytes tcp udp round trip latencies atm ethernet 
function message size 
tcp udp performance close raw net performance limits 
proof concept implementation 
tcp udp net implementation effort goals show architecture able support implementation traditional protocols second create test environment traditional benchmarks put net kernelized protocol processing perspective 
basing net tcp udp implementation existing software full protocol functionality interoperability maintained 
number modules critical performance path ported net arp icmp modules 
point secure net multiplexor support sharing single vci multiple channels making impossible implement standard ip atm transport mechanism requires single vci carry ip traffic applications 
ip net single channel carry ip traffic applications matches standard processing closely currently possible 
test setup exclusive net channel tcp connection simple implement 
progress resolve ip atm incompatibility implement proper icmp handling targeting ipv net 
additional level demultiplexing foreseen ipv flow id source address tag packets arrive dedicated ip atm vci 
packets tag resolve local net destination transferred kernel communication endpoint generalized processing possibly triggering icmp handling 
yield implementation fully interoperable ip atm implementations cover local wide area communication 
protocol execution environment 
tcp ip suite protocols frequently considered high speed networks atm 
experience shown core problems tcp ip performance usually lie particular implementations integration operating system protocols 
case fore driver software tries deal generic low performance buffer strategies bsd kernel 
net protocol developer experience restrictive environment kernel generalized buffer timer mechanisms mandatory properties network application incorporated protocol operation 
net gives developer freedom design protocols protocol support software timer buffer mechanisms optimized particular application network technology 
yields toolbox approach protocol application construction designers select variety protocol implementations 
result net tcp udp deliver low latency high bandwidth communication expected atm networks resorting excessive buffering schemes large network transfer units maintaining interoperability non net implementations 
message handling staging 
limiting factors performance kernel protocols bounded kernel resources available need shared potential network active processes 
implementing protocols user level efficient solutions avail mbits net udp fore udp sender fore udp receiver mbytes bytes udp bandwidth function message size 
able problems find origin operating system kernel single protocol processing unit 
net remove copy operations protocol path allows buffering staging strategies depend resources application scarce kernel network buffers 
example restricted size socket receive buffer max 
kbytes sunos common problem bsd kernel communication path ethernet speeds buffer overrun cause message loss case high bandwidth udp data streams 
removing restriction resources actual recipient intermediate processing unit main control factor tuned meet application needs efficiently incorporated flow control mechanisms 
deficiencies bsd kernel buffer mbuf mechanism identified long ago high performance networks amplify impact mechanism especially combination fore driver buffering scheme 
shows udp throughput saw tooth behavior caused buffer allocation scheme large kbyte buffers filled data remainder bytes copied small mbufs bytes 
allocation method strong degrading effect performance protocols smaller mbufs count mechanism large cluster buffers 
alternative kernel buffering mechanism significantly improve message handling kernel certainly remove saw tooth behavior seen questionable contribute significantly latency reduction example removing kernel application copies entirely 
base level net provides scatter gather message mechanism support efficient construction network buffers 
data blocks allocated receive transmit communication segments simple count mechanism added tcp udp support software allows shared messages need copy operations 
application controlled flow control feedback 
major advantages integrating communication subsystem application application aware state communication system take application specific actions adapt changing circumstances 
kernel communication systems facility block deny service application able communicate additional information 
sending side example feedback provided application state transmission queues simple establish back pressure mechanism queues reach high water mark 
things overcomes problems current sunos implementation drop random packets device transmit queue overload notifying sending application 
protocol specific information retransmission counters round trip timers buffer allocation statistics readily available application adapt communication strategies status network 
receive window net tcp example direct reflection buffer space application intermediate processing unit allowing close match application level flow control receive window updates 
ip main functionality ip protocol handle communication path adapt messages specifics underlying network 
receiving side ip net liberal messages accepts implements ip functionality forwarding messages interfacing icmp 
transport protocol selected net demultiplex information passed transport module possibly assist destination selection 
sending side functionality ip protocol reduced mapping messages net communication channels 
reduced functionality side protocol collapsed transport protocols efficient processing 
ip net exports mtu kbytes support fragmentation sending side known potential source wasting bandwidth triggering packet retransmissions 
tcp provides fragmentation mechanism tight coupling application protocol module relatively simple application assist udp achieving functionality 
udp core functionality udp twofold additional layer demultiplexing ip port identifiers protection corruption adding bit checksum data header parts message 
net implementation demultiplexing simplified source endpoint information passed net 
simple pcb caching scheme incoming channel allows significant processing speedups described 
checksum adds processing overhead bytes sparcstation combined copy operation retrieves data communication segment 
switched applications data protection higher level satisfied bit crc net aal level 
performance net udp compared kernel udp figures 
shows achieved bandwidth plots round trip latency function message size 
kernel udp bandwidth measured perceived sender received losses attributed kernel buffering problems sending receiving hosts 
experimental set net udp experience losses receive bandwidth shown 
mbits net tcp fore tcp mbytes bytes tcp bandwidth function data generation application 
fore tcp fore udp net tcp net udp bytes tcp tcp adds properties attractive protocol number settings reliability flow control 
reliability achieved simple acknowledgment scheme flow control advertised receive windows 
performance tcp depend rate data pushed network product bandwidth round trip time indicates amount buffer space needed maintain steady reliable high speed flow 
window size indicates bytes module send wait acknowledgments window updates receiver 
updates returned sender timely manner relatively small window needed achieve maximum bandwidth 
shows cases net tcp achieves mbytes sec bandwidth kbyte window window kernel tcp atm combination achieve mbytes sec 
round trip latency performance kernel net tcp implementations shown highlights fast net tcp round trip permits small window 
tcp tuning 
tcp high speed networks studied extensively especially wide area networks number changes extensions proposed tcp function correctly settings relatively high delay expected 
changes need incorporated net tcp implementation function wide area links high latencies longer permit small windows 
argued lately changes needed local area case order address deficiencies occur high latency atm kernel software 
net tcp shows acceptable performance achieved lan man settings modifications general algorithms large sequence numbers extensive buffer reservations 
tuning number tcp transmission control variables risk running atm done extreme caution 
low latency net allows udp tcp round trip latencies function message size 
conservative settings minimizing risk achieving maximum performance 
important tuning factor size segments transmitted larger segments maximum bandwidth achieved cases low latency available 
floyd shown tcp perform poorly atm segment size large due fact underlying cell reassembly mechanism causes entire segment discarded single atm cell dropped 
number solutions available provide mandate large segment sizes 
standard configuration net tcp uses byte segments sufficient achieve bandwidth shown 
popular approach compensate high latencies grow window size 
allows large amount data outstanding acknowledgments expected back hope keep communication pipe filled 
unfortunately increasing window number drawbacks 
large amount data buffered available retransmission 
furthermore risk triggering standard tcp congestion control mechanism segments dropped single window 
tuning window size large value increase chance situation occurring resulting drain communication pipe subsequent slow start 
unavoidable run risks lan setting protocol execution environment able guarantee low latency communication 
final tuning issue needed addressed net tcp bad ratio granularity protocol timers round trip time estimates 
retransmission timer tcp set function estimated round trip time range microseconds bsd kernel protocol timer pr slow timeout granularity milliseconds 
tcp packet discarded cell loss dropped due congestion retransmit timer set relatively large value compared actual round trip time 
ensure timely reaction possible packet loss net tcp uses millisecond protocol round trip latency bandwidth packets raw aal mbits active msgs mbits udp mbits tcp mbits split store mbits table net latency bandwidth summary 
timer granularity constrained reliability userlevel timing mechanisms 
bsd implementation uses timer pr fast timeout transmission delayed acknowledgment case send data available piggybacking potential transmission deadlock needs resolved 
timer delay acknowledgment second packet ms 
net tcp possible disable delay mechanism achieve reliable performance 
disabling bandwidth conserving strategy justified low cost active acknowledgment consists byte tcp ip header handled efficiently single cell net reception 
result available send window updated timely manner possible laying foundation maximal bandwidth exploitation 
wide area settings bandwidth conservation may play important role delayed acknowledgment scheme may enabled situations 
summary main objectives net provide efficient communication offer high degree flexibility accomplished 
processing overhead messages minimized latency experienced application dominated actual message transmission time 
table summarizes various net latency bandwidth measurements 
net presents simple network interface architecture simultaneously supports traditional inter networking protocols novel communication abstractions active messages 
net round trip latency messages smaller bytes sec 
compares favorably research results application device channels arizona achieve sec latency single byte messages byte messages hp environment latencies starting sec 
research efforts dedicated hardware capable mbits sec compared mbits sec standard hardware net 
main goal net architecture remove processing overhead achieve low latency secondary goal delivery maximum network bandwidth small messages achieved 
message sizes small bytes network saturated smaller sizes dominant bottleneck processor network interface 
net demonstrates removing kernel communication path offer new flexibility addition high performance 
tcp udp protocols implemented net achieve latencies throughput close raw maximum active messages round trip times microseconds absolute minimum 
final comparison node atm cluster meiko cs tmc cm supercomputers small set split benchmarks demonstrates right communication substrate networks workstations rival machines 
encouraging result obscure fact significant additional system resources parallel process schedulers parallel file systems need developed cluster workstations viewed unified resource 
acknowledgments net materialized numerous discussions email exchanges taste competition friends uc berkeley project particular david culler alan mainwaring rich martin tin liu 
split section possible generous help klaus eric schauser uc santa barbara shared split programs provided quick access meiko cs funded nsf infrastructure cda 
cm results obtained ucb machine funded nsf infrastructure cda 
ucb split group benchmarks particular arvind krishnamurthy 
atm workstation cluster purchased contract rome laboratory air force material command 
werner vogels supported onr contract 
fore systems making source sba firmware available 
final phase reviewers deborah estrin sosp shepherd provided helpful comments particular tcp ip 
abbot peterson 
increasing network throughput integrating protocol layers 
ieee acm transactions networking 
vol 
pages oct 
anderson culler patterson case networks workstations 
ieee micro feb pages 
bailey gopal peterson sarkar 
pathfinder pattern packet classifier 
usenix symposium operating systems design implementation pages nov 
felten li 
virtual memory mapped network interfaces 
ieee micro feb pages 
brakmo malley peterson 
tcp vegas new techniques fro congestion detection 
proc 
sigcomm pages aug 
braun diot 
protocol implementation integrated layer processing 
proc 
sigcomm sept 
culler dusseau goldstein krishnamurthy von eicken yelick 
split 
proc 
supercomputing 
culler dusseau martin schauser 
fast parallel sorting logp split 
proc 
july 
culler generic active message interface specification version 
cs berkeley edu papers papers gam spec ps clark tennenhouse 
architectural considerations new generation protocols 
proc 
pages aug 
clark jacobson 
analysis tcp processing overhead 
ieee communications pages june 
dalton watson banks edwards 

ieee network magazine vol pages july 
druschel peterson :10.1.1.52.9688
fbufs high bandwidth cross domain transfer facility 
proc 
th sosp 
pages 
december 
druschel peterson davie 
experiences high speed network adaptor software perspective 
proc 
sigcomm pages aug 
edwards watson banks dalton 
user space protocols deliver high performance applications low cost gb lan 
proc 
sigcomm pages aug floyd jacobson mccanne 
reliable multicast framework light weight sessions application level framing 
proc 
sigcomm sept 
jacobson braden borman 
tcp extensions high performance 
ietf request comments may 
kay pasquale 
importance non data touching processing overheads 
proc 
sigcomm pages aug kent mogul 
fragmentation considered harmful proc 
sigcomm 
pages 
aug 
krishnamurthy culler yelick 
connected components distributed memory machines 
dimacs series discrete mathematics theoretical computer science laubach 
classical ip arp atm 
ietf request comments january 
maeda bershad 
protocol service decomposition high performance networking 
proc 
th sosp pages 
dec 
partridge pink 
faster udp 
ieee acm transactions networking vol 
pages aug 
romanow floyd 
dynamics tcp traffic atm networks 
proc 
sigcomm 
pages aug 
schauser 
experience active messages meiko cs 
proc 
th international parallel processing symposium ipps santa barbara ca april 
brian smith 
cyclic udp priority driven best effort protocol 
www cs cornell edu info faculty nossdav ps 
thekkath levy lazowska 
separating data control transfer distributed operating systems 
proc 
th int conf 
asplos oct 
von eicken anindya basu vineet 
low latency communication atm networks active messages 
ieee micro feb pages 
von eicken culler goldstein schauser 
active messages mechanism integrated communication computation 
proc 
th isca pages may 

