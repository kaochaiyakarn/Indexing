efficient structure learning markov networks regularization su lee ganapathi daphne koller department computer science stanford university stanford ca koller cs stanford edu markov networks widely wide variety applications problems ranging computer vision natural language computational biology 
current applications rely heavily learned models structure markov network constructed hand due lack effective algorithms learning markov network structure data 
provide computationally effective method learning markov network structure data 
method regularization weights log linear model effect biasing model solutions parameters zero 
formulation converts markov network learning problem convex optimization problem continuous space solved efficient gradient methods 
key issue setting unavoidable approximate inference lead errors gradient computation network structure dense 
explore different feature schemes compare performance 
provide results method synthetic data real world data sets modeling joint distribution pixel values mnist data modeling joint distribution genetic sequence variations human data 
show method achieves considerably higher generalization performance standard method gaussian parameter prior pure maximum likelihood learning 
show learn mrf network structure computational cost greater learning parameters demonstrating existence feasible method important problem 
undirected graphical models markov networks log linear models growing variety applications including computer vision natural language computational biology 
modeling framework increasingly complex understood domains problem selecting exponentially large space possible network structures great importance 
including possibly relevant interactions model generally leads overfitting lead difficulties running inference network 
learning structure important task right provide insight underlying structure domain 
unfortunately problem learning markov networks remains challenge 
key difficulty maximum likelihood ml parameters networks analytic closed form finding parameters requires iterative procedure conjugate gradient bfgs iteration runs inference current model 
type procedure computationally expensive models inference tractable 
problem structure learning considerably harder 
dominant type solution problem uses greedy local heuristic search incrementally modifies model adding possibly deleting features 
approach adds features greedily improve model likelihood feature added removed :10.1.1.43.7345
feature addition step heuristic greedy lead inclusion unnecessary features overly complex structures overfitting 
alternative approach explicitly searches space low treewidth models utility models practice unclear hand designed models real world problems generally low tree width 
greedy heuristic search methods learned network best local optimum penalized likelihood score 
propose different approach learning structure log linear graphical model markov network 
viewing combinatorial search problem embed structure selection step problem parameter estimation features weight zero log space degenerate induce dependencies variables involve 
appropriately bias model sparsity technique increasingly popular context supervised learning problems involve large number features may irrelevant 
long known regularization model parameters optimizing joint objective trades fit data sum absolute values parameters tends lead sparse models weights value :10.1.1.35.7574

showed density estimation log linear models regularized likelihood sample complexity grows logarithmically number features log linear model ng 
shows similar result regularized logistic regression 
results show approach useful selecting relevant features large number irrelevant ones 
proposes effective algorithms regularized generalized linear models support vector machines feature selection log linear models encoding natural language grammars :10.1.1.13.4021:10.1.1.20.127:10.1.1.15.6182
surprisingly regularization proposed purpose structure learning general markov networks 
explore approach discuss issues important effective application large problems 
key point model score candidate feature set fixed convex optimization problem admits unique optimal solution 
due properties score solution features weight generally leading sparse network structure 
generally impractical simply initialize model include possible features exact inference model invariably intractable approximate inference methods loopy belief propagation give highly inaccurate estimates gradient leading poorly learned models 
propose algorithm schema gradually introduces features model lets regularization scheme eliminate optimization process 
explore different approaches feature gain method della pietra della pietra lafferty grafting method perkins theiler :10.1.1.13.4021:10.1.1.43.7345
provide sound termination condition algorithm criterion proposed perkins correct estimates gradient algorithm guaranteed terminate unique global optimum reasonable feature method :10.1.1.13.4021
test method synthetic data generated known mrfs real world tasks modeling joint distribution pixel values mnist data modeling joint distribution genetic sequence variations single nucleotide polymorphisms snps human data :10.1.1.138.1115
results show regularization performs approaches provides effective method learning mrf structure large complex domains 
preliminaries focus presentation framework log linear models forms convenient basis discussion learning 

xn set discrete valued random variables 
log linear model compact representation probability distribution assignments log linear model defined terms set feature functions fk xk function defines numerical value assignment xk subset xk set feature functions fk parameters log linear model weights fk 
distribution defined exp fk kfk xk xk assignment xk partition function ensures distribution normalized entries sum 
note definition features encompasses standard features relate unobserved network variables part speech word sentence observed elements data word structural features encode interaction hidden variables model 
log linear model induces markov network edge pair variables xi xj appear feature fk xk xi xj xk 
clique potentials constructed log linear features obvious way 
conversely markov network encoded log linear model defining feature indicator function assignment variables xc clique xc 
mapping markov networks useful inference algorithms belief propagation operate graph structure markov network :10.1.1.32.5538
standard learning problem mrfs formulated follows 
set iid training instances 
consisting full assignment variables goal output log linear model consists set features parameter vector specifies weight fk log likelihood function log form kfk log log fk fk fk xk sum feature values entire data set vector aggregate features arranged order parameter vector vector dot product operation 
closed form solution parameters maximize eq 
objective concave optimized numerical optimization procedures conjugate gradient bfgs 
gradient log likelihood fk fk 
expression particularly intuitive form gradient attempts feature counts empirical data equal expected counts relative learned model 
note compute expected feature counts perform inference relative current model 
inference step performed step gradient 
regularized structure learning formulate structure learning problem follows 
assume possibly large set features wish select subset inclusion model 
problem generally solved heuristic search combinatorial space possible feature subsets 
approach addresses search possible parameter vectors ir specifically optimizing log likelihood introduce laplacian parameter prior feature fk takes form exp 
define 
optimize posterior probability likelihood 
logarithm eliminating constant terms obtain objective max log 
cases prior features objective convex optimized efficiently methods conjugate gradient bfgs care needs taken discontinuity derivative 
principle simply optimize objective obtain globally optimal parameter assignment 
objective eq 
contrasted obtained standard parameter prior log linear models mean zero gaussian prior exp 
gaussian prior induces regularization term quadratic penalizes large features smaller ones 
conversely regularization penalizes small terms strongly forcing parameters 
known empirically optimizing regularized objective leads sparse representation relative small number non zero parameters 
aside intuitive argument theoretical results provide formal justification regularization approaches analysis 
ng 
suggests form regularization effective identifying relevant features relatively small number samples 
building directly results 
show result corollary 
xn set variables domain size distribution 
set indicator features subsets variables cardinality 
parameterization optimizes arg max data set assignment optimizes eq 
regularization parameter ln nd probability data set iid instances size cb ln nd 
words regularized log likelihood objective learn markov network maximal clique size expected log likelihood relative true underlying distribution worse log likelihood optimal markov network class norm number samples required grows logarithmically number nodes network polynomially dependence quite natural indicating samples required learn networks containing strong interactions 
note bound magnitude potential markov network nd polynomial number data instances suffices 
incremental feature discussion implicitly assumed find global optimum eq 
simple convex optimization 
simply include features model advance parameter optimization prune away irrelevant ones 
recall computing gradient requires performing inference resulting model 
features model may densely connected allow effective inference 
approximate inference belief propagation may fail converge give rise unstable estimates gradient suboptimal solutions 
approach contains feature component gradually selects features add model allowing optimization process search optimal values parameters 
precisely algorithm maintains set active features inactive feature fk parameter set parameters active features free changed optimizing objective eq 

addition various simple baseline methods explore feature methods greedy myopic compute heuristic estimate benefit gained introducing single feature active set 
grafting procedure perkins developed feature selection standard classification tasks features selected gradient optimize objective relative current active weights convergence gradient relative features zero :10.1.1.13.4021
inactive feature compute partial derivative objective eq 
relative select gradient largest 
conservative estimate obtained gain method della pietra :10.1.1.43.7345
method designed log likelihood objective 
begins optimizing parameters relative current active set inactive feature computes log likelihood gain adding feature assuming optimize feature weight arbitrarily weights features held constant 
introduces feature greatest gain 
della pietra show gain concave objective computed efficiently dimensional line search 
restricted case binary valued features provide closed form solution gain 
task compute optimal gain log likelihood optimal gain eq 

difficult see gain objective differs log likelihood linear term concave function optimized line search 
case binary valued features provide closed form solution gain omit lack space 
experiments slightly different form feature takes values 
feature change objective function introducing feature kfk log exp fk exp fk number training instances 
take derivative function set zero get closed form solution log fk sign fk 
fk sign fk methods heuristic consider potential gain adding single feature isolation assuming weights held constant 
grafting method optimistic estimates value adding single feature slope adding gain approach considers intuitively far go direction gain peaks 
gain heuristic fact lower bound actual gain obtained adding feature allowing features adapt 
gain heuristic provides better estimate value adding feature albeit slightly greater computational cost limited cases closed form solution 
observed perkins 
regularized objective provides sound stopping criterion incremental feature induction algorithm :10.1.1.13.4021
inactive fk gradient log likelihood gradient objective direction non positive objective stationary point 
importantly objective concave function unique global maximum 
termination condition achieved guaranteed local maximum regardless feature method 
words method contains heuristic component introducing features impact heuristic final outcome complexity 
approximate inference steps algorithm rely inference computing key quantities gradient needed parameter optimization grafting method termination condition expression gradient requires computation marginal probabilities relative current model 
similarly computation gain requires inference 
discussed networks useful models real applications exact inference intractable 
resort approximate inference results errors gradient 
approximate inference methods proposed commonly general class loopy belief propagation bp algorithms :10.1.1.32.5538
approximate inference algorithm bp raises important points 
important question issue relates computation gradient gain features currently inactive 
belief propagation algorithm executed particular network set active features creates cluster subset variables xk appear scope feature fk xk 
output bp inference process set marginal probabilities clusters returns necessary information computing expected sufficient statistics gradient objective see eq 

features fk xk currently inactive corresponding cluster induced markov network cases necessary marginal probabilities xk computed bp 
approximate marginal probability extracting subtree calibrated loopy graph contains variables xk 
convergence bp algorithm subtree loopy graph calibrated belief potentials agree 
view subtree calibrated clique tree standard dynamic programming methods tree see 
extract approximate joint distribution xk 
second key issue performance bp algorithms generally degrades significantly density network increases converge answers return typically accurate 
non convergence inference common network parameters allowed take larger extreme values see example theoretical results supporting empirical phenomenon 
important keep model amenable approximate inference continue improve long possible 
observation important consequences 
different feature achieve results exact inference outcomes vary greatly approximate inference due differences structure networks arising learning process 
shall see feature methods introduce relevant features better practice 
second order keep inference feasible long possible utilize annealing schedule regularization parameter large values leading greater sparsification structure gradually reducing allowing additional weaker features introduced 
method allows greater part learning executed robust model 
results experiments focus binary pairwise markov networks feature function indicator function certain assignment pair nodes single node 
conjugate gradient method solving optimization problem eq 

conditional marginal log likelihood evaluation metric learned network 
calculate divide variables groups 
test instance compute xh log xh 
practice divided variables groups calculated average observing group hiding rest 
note defined respect marginal probabilities global partition function empirically thought accurate 
considered feature induction schemes gain estimated change gain grad grafting simple simple pairwise similarity metric 
simple scheme score pairwise feature xi xj mutual information xi xj 
scheme varied regularization method regularization regularization regularization 
experiments synthetically generated data 
generated synthetic data gibbs sampling synthetic network 
network structure nodes generated treating possible edge bernoulli random variable sampling edges 
chose parameter bernoulli distribution node neighbors average 
order analyze dependence performance size connectivity network varied results experiments synthetic data see text details 
compare algorithm regularization regularization regularization different ways 
summarizes results data sets includes information networks experiment 
measure performance reconstruction error number training examples increases 
expected produces biggest improvement number training instances small prone overfitting 
effect pronounced measuring hamming distance number disagreeing edges learned structure true structure 
shows learn spurious edges 
surprisingly shows sparser distribution weights smaller number edges non negligible weights structures tend edges small values 
plot performance function density synthetic network 
synthetic network gets denser increasingly outperforms algorithms 
may graph gets dense introduce spurious edge may remove 
measure wall clock time function size synthetic network 
method labeled true simply learns parameter true model 
shows computational cost learning structure network gain ignore certain combinations efficiency 
calculating approximate gain regularization involves line search feature induction step 
learning parameter 
increasingly outperforms regularization methods number nodes increases 
experiments mnist digit dataset 
applied algorithm handwritten digits 
mnist training set consists binary images handwritten digits 
order speed inference learning resized image 
trained model digit separately training set consisting images digit 
digit training instances remainder test instances 
compares different methods 
save space show digits relative difference performance compared best competitor lowest digit highest digit average performance 
results experiments mnist dataset mentioned earlier performance regularized algorithm insensitive feature induction method assuming inference exact 
practice inference approximate induction algorithm introduces spurious features affect quality inference performance algorithms 
effect substantiated poor performance simple simple methods introduce features mutual information gradient grad approximate gain gain 
outperforms regardless feature induction algorithm paired 
illustrates induction algorithm introduces features increasingly outperforms 
shows visualization mrf learned modeling digits 
course expect short range interactions associativity neighboring pixels algorithm capture relationships 
shown graph simplify analysis relationships 
interestingly algorithm picks long range interactions presumably allow algorithm model variations size shape hand written digits 
experiments human genetic variation data 
human data represent binary genetic variation individuals 
data sets experiment contains binary genotype values genomic spots individuals 
table compares methods data sets 
data sets shows better performance 
table results experiments human dataset data enm enm number nodes gain grad gain discussion simple effective method learning structure markov networks 
viewed structure learning problem regularized parameter estimation task allowing solved convex optimization techniques 
showed computational cost method considerably greater pure parameter estimation fixed structure suggesting mrf structure learning feasible option applications 
important directions extended 
currently method handles feature log linear model independently attempt bias learning human data available www org 
sparsity structure induced markov network 
extend approach introduce bias variant regularization penalizes blocks parameters block norm 
key limiting factor mrf learning approach fact requires inference model 
cost mrf learning prohibitive complex domains involving large numbers variables dense interaction structure 
experiments show results obtained approximate algorithms belief propagation imprecise lead improvement objective 
learned network structure dense performance algorithm degrade 
exacerbated fact approximate gradient move parameters diminishing effect regularization rendering inference precise 
interesting explore inference methods goal correctly estimating direction magnitude gradient 
interesting explore viability learned network structures realworld applications density estimation knowledge discovery 
example data provides opportunities learned network predict important snps smaller subset measured snps 
structure learned network may reveal interesting insights viability different combinations snps 
acknowledgments give warm ben taskar useful discussions 
supported national science foundation dbi darpa transfer learning program 
bach jordan 
thin junction trees 
nips 
bach lanckriet jordan 
multiple kernel learning conic duality smo algorithm 
international consortium 
international project 
nature 
robert cowell david spiegelhalter 
probabilistic networks expert systems 
springer verlag new york nj usa 
iii 
notes cg lm bfgs optimization logistic regression 
august 
della pietra della pietra lafferty :10.1.1.43.7345
inducing features random fields 
ieee transactions pattern analysis machine intelligence 
deshpande garofalakis jordan 
efficient stepwise selection decomposable models 
proc 
uai pages 
lewis madigan 
large scale bayesian logistic regression text categorization 

goodman 
exponential priors maximum entropy models 
north american acl 
alexander john fischer iii alan willsky 
loopy belief propagation convergence effects message errors 
mach 
learn 
res 
lecun bottou bengio haffner :10.1.1.138.1115
gradient learning applied document recognition 
proceedings ieee november 
martijn hilbert kappen 
general lower bounds computer generated higher order expansions 
uai pages 
mccallum 
efficiently inducing features conditional random fields 
proc 
uai 
thomas minka 
algorithms maximum likelihood logistic regression 

murphy weiss jordan 
loopy belief propagation approximate inference empirical study 
pages 
pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann 
perkins theiler :10.1.1.13.4021
grafting fast incremental feature selection gradient descent function space 

stefan riezler alexander 
incremental feature selection regularization relaxed maximum entropy modeling 
proceedings emnlp 
michael jordan :10.1.1.13.4021
loopy belief propogation gibbs measures 
uai pages 
tibshirani :10.1.1.35.7574
regression shrinkage selection lasso 
royal 
statist 
soc 
martin wainwright erik sudderth alan willsky 
tree modeling estimation gaussian processes graphs cycles 
todd leen thomas dietterich volker tresp editors advances neural information processing systems pages 
mit press 
jonathan yedidia william freeman yair weiss :10.1.1.35.7574
generalized belief propagation 
advances neural information processing systems 
mit press 
zhu rosset hastie tibshirani :10.1.1.20.127:10.1.1.15.6182
norm support vector machines 
proc 
nips 
