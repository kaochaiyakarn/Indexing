journal applied intelligence kluwer academic publishers boston 
manufactured netherlands 
unsupervised neural network learning procedures feature extraction classi cation becker department psychology mcmaster university hamilton ont canada mark plumbley department computer science king college london strand london wc ls uk received revised editors pineda 
article review unsupervised neural network learning procedures applied task preprocessing raw data extract useful features subsequent classi cation 
learning algorithms reviewed grouped sections information preserving methods density estimation methods feature extraction methods 
major sections concludes discussion successful applications methods real world problems 
keywords unsupervised learning self organization information theory feature extraction signal processing 
di cult important parts classi cation process preprocessing raw data extract useful appropriate features 
raw large complex directly input classi er leading curse dimensionality generalization problems insu cient training examples available 
case simply reducing number variables representing data learning easier classifying stage 
cases prior knowledge problem determine heuristic features edges image classi cation problem 
possible rst author supported research james mcdonnell foundation natural sciences engineering research council canada 
part second author supported temporary academic initiative university london gr science engineering research council serc uk 
cases may complete solution cases features identi able 
article review unsupervised neural network learning procedures applied preprocessing task 
contrast supervised reinforcement learning procedures unsupervised algorithms knowledge eventual targets errors classi cation process 
learn eventual operation purely observing raw input data 
learning procedures reviewed grouped main categories 
section consider information preserving methods 
cover principal component analysis pca principal subspace methods temporal prediction information independent component analysis inca direct minimization information loss 
section consider density estimation methods related maximum likelihood parameter estimation 
include called encodings may learned various forms competitive learning advanced combinatorial representations 
section consider becker plumbley methods aim extract higher order features data pre processing stages system 
include extraction maximally selective projections gmax detect statistical dependencies data extraction invariant features extraction spatially invariant features imax 
sections concludes pointers examples applications techniques discussed section 

information preserving algorithms section consider learning systems attempt preserve information input data possible performing simpli cation data dimension reduction 
unsupervised learning systems access eventual classi cation performance di cult attempt preserve information input 
major part section considers principal component analysis pca methods single component algorithms variants extract components 

principal component analysis subspace methods principal component analysis pca popular dimension reduction feature extraction method 
appears literature various guises including factor analysis discrete karhunen loeve transform klt hotelling transform 
variety names largely re ects di erent applications 
fig 

input output pca network input weight matrix output wx 
suppose zero mean random vector xn distributed joint density covariance matrix xx normalized eigenvectors denoted en corresponding eigenvalues form transformed random vector ux matrix set successive eigenvectors ei new random vector yn components yj decorrelated order decreasing variance yi rst output component called principal component input rst output components ym called principal components input 
transform performed neural network similar process refer input vector output vector 
practical application random vector typically see sequence sample vectors distributed 
nite amount see nite sequence samples 
large average reasonable estimate underlying covariance matrix form estimate principal components observations sample sequence 
remainder section shall assume number samples large approximately equal 
input output linear neural network fig 
pca realized simply setting weight matrix ith row wi equal ith eigenvector ei input covariance matrix 
outputs yi network principal components input data 
reason pca useful preprocessing information preserving properties 
number outputs linear transform minimizes mean squared reconstruction error input sequence output sequence 
linear transform preserves maximum amount shannon information output assumptions uncorrelated additive gaussian noise input signal 
pca optimal linear dimension reduction method 
classical techniques pca entire data sample sequence fort available required weight vectors pca calculated standard techniques singular value decomposition svd 
version available allowing svd updated data arrives 
svd exact method data input data sequence seen far complex neural network algorithm 
addition nds principal components input small number principal components needed 
rst principal component estimated repeated multiplication initially random vector input covariance matrix 
start random column vector repeatedly apply multiplication direction principal component soon come dominate goes quickly larger corresponding factor eigenvectors 
course vector periodically need re scaled prevent ow possibly renormalizing length jw set step 
rst principal component second extracted subtracting ect rst input data re applying modi ed data new random vector 
unfortunately method simple neural network algorithm input data available procedure starts qx calculated 
neural network algorithms proposed nd princi unsupervised learning procedures pal components relatively simple online algorithms operating data vectors arrive 
finding rst principal component input vector xn single output nx simple hebbian algorithm wi xi wi wi xi write vector notation tend vector direction principal component length tend unbounded 
factor small update factor learning rate term 
theoretical convergence algorithms considered update factor decrease slowly time typically practical application lead slow convergence small values set small constant 
case algorithm fully converge stable point small random movements depending size oja modi ed algorithm renormalized step leading algorithm ensures unit length time 
weight vector algorithm asymptotically converges principal component 
update factor su ciently small oja showed step algorithm approximated single step algorithm becker plumbley referred oja rule 
weight vector algorithm converges unit length principal component explicitly normalizing step 
wy term right hand side tends decrease length gets large allowing increase gets small 
note passing oja rule local algorithm seen clearly write form wi wi xi wi term local means change weight wi determined purely activations xi weight weight value wi 
neural network algorithms extract principal component local algorithms 
somewhat important ifwe looking algorithm biologically plausible great consideration practical applications 
re estimation algorithms algorithms suggested extend oja rule output units ym yj wj nx wji xi hierarchical manner arranging weights rst output unit updated oja rule ect earlier outputs subtracted decorrelated away units forcing units extract di erent components 
example sanger generalized hebbian algorithm gha incorporates form gram schmidt orthogonalization give algorithm wj wj yj xj xj xj jx wk yk alternative algorithm derived oja karhunen stochastic gradient algorithm sga 
form gha replaced xj wj yj wk yk gha algorithm sga algorithm force successive outputs learn principal components subtracting estimates earlier components input data reaches learning algorithm outputs calculated original input data 
reduce oja rule network single output 
abbas fahmy suggested related approach 
successive output unit learns principal component ect component removed subtracting original data 
oja rule modi ed input data nd principal component 
possible disadvantage algorithms force particular roles particular units 
learning proceeds parallel sense rst unit nd rst principal component second extracted chain output units 
algorithms suggested attempt nd principal subspace space spanned principal components principal components 
su cient minimize mean square reconstruction error optimally preserve information linear rotation output components ect properties 
principal components needed particular application may su cient set vectors spans principal subspace 
principal subspace needed achieved symmetrical version williams symmetric error correction algorithm equivalent oja subspace network 
system re estimator xj replaced xj mx wk yk algorithm network converge set outputs span principal subspace principal components weight vectors wj orthonormal convergence gha sga algorithms 
oja ogawa modi ed algorithm add additional factor give wj wj yj 
favours lower numbered units higher numbered units resulting true principal components extracted just principal subspace 
decorrelating algorithms alternative approach re estimator xj ect principal components decorrelate successive outputs earlier outputs 
forces output learn respond di erent principal component outputs 
rubner example suggest hierarchical decorrelating model adaptive lateral inhibition lower numbered fig 

pca network hierarchical decorrelation output units 
unsupervised learning procedures output units higher numbered output units fig 

outputs system give yj wj yk means outputs yj calculated order ym forward weight vector wj updated oja rule lateral connections output yj yk updated simple anti hebbian algorithm yk algorithm lateral connections anti hebbian weight value decreases inhibition increases proportion product activations units 
output rst unit nds rst principal component calculated oja rule decorrelating terms 
inhibition unit increases second output decorrelated forcing nd second principal component 
third output decorrelated previous desired principal components extracted order 
kung suggested adaptive principal component extractor apex related rubner approach 
fig 

network symmetrical decorrelation output units 
becker plumbley suggested units obeying oja rule decorrelated symmetric decorrelating stage fig 

matrix notation lateral inhibition matrix forced diagonal 
outputs network allowed settle satis ed get identity matrix provided eigenvalues unity 
practice may besu cient rst terms expansion arti cial system 
settled oja rule update forward weight vector lateral connections updated anti hebbian algorithm rubner model time symmetric connections network lateral connection matrix vij forced subdiagonal 
note update symmetrical initially symmetrical remain leen analyzed stability network suggested algorithm lateral connections modi ed add term proportional lateral connection weight yk activity dependent term yj yk yk yj yk term means variance output units accumulated measure activity drive algorithm 
modi cation weight vectors converge principal components just principal subspace provided 
system favours output principal components appear particular order 
principal subspace previous algorithms generalizations oja rule consequently tend preserve input components output 
necessary information theory minimum reconstruction error viewpoints alternatives possible 
example plumbley suggested networks extract principal subspace produce uncorrelated equal variance outputs having outputs equal variance desirable systems limited channel capacity may facilitate subsequent classi cation normalizing scale outputs 
rst fig 
uses lateral inhibition output similar network self inhibitory connections vii allowed 
network leading settling diagonal entries allowed 
algorithm wj wj yj wj yj yk jk jk kronecker delta outputs converge uncorrelated equal variance set spans principal subspace 
unsupervised learning procedures second network fig 
uses set inhibitory interneurons speci ed leading fig 

pca networks uncorrelated equal variance outputs 
settling 
algorithm forward weights algorithm yj zk vk lateral connections algorithm converges outputs uncorrelated equal variance span principal subspace provided selected principal components input variance greater 
auto encoders principal subspace algorithms mentioned far error back propagation backprop algorithm nd principal subspace 
consider linear network fig 

auto encoder network andm 
fig 
yj zk hy hhj hj nx mx wji xi yj backprop minimize mean squared error jx system units hidden layer set vectors spans principal subspace 
note force weight vectors wi orthogonal set weights spans principal subspace su cient 
common networks backprop units auto encoder nonlinear activation functions sigmoid function exp 
case yj zk nx mx wji xi yj theory capable complex behaviour purely linear networks able extract nonlinear input subspace 
opinions divided achieved practice argued method svd reliable susceptible local minima 
problem nonlinear hidden layer codes usually di cult interpret 
saund zemel hinton becker plumbley explored ways constraining hidden unit codes adopted autoencoder networks obtain biologically plausible interpretable representations codes topographic maps 

temporal prediction techniques described far deal static patterns 
important characteristic real world data visual speech signals presence temporal structure 
discovering ways structure predictable time possible form compressed representation data capture underlying temporal constraints 
way pose problem unsupervised learning framework train network predict input time step previous inputs 
generalization auto encoder network error signal jx learned mapping information preserving extent input pattern accurately predicted inputs previous time steps 
major techniques providing temporally varying information tapped delay lines making inputs time steps available simultaneously time domain recurrent feedback connections number variations architectures training methods networks reviewed mozer 
delay line neural networks recurrent networks sensitive information potentially innite time span practice enormous di culty learning maintain state information long time intervals 
schmidhuber mozer proposed di erent ways addressing problem recurrent networks 

independent components analysis input composed combination independent signals linear methods components analysis general incapable separating independent sources 
jutten herault developed learning proce dure inca extracting statistically independent components input vector input vector modelled additive mixture unknown independent signal components az matrix consists unknown real scalars 
recursive network symmetrical net fig 
produce outputs yi xi vik yk come approximate independent signal components 
fact higher order moments signals uncorrelated signals independent modi ed hebb rule proposed yj yk rule higher order moments inputs 
choice functions jutten herault simulations claim worked test problems tan 

direct minimization information loss methods discussed ensure minimal information lost network mapping minimizing reconstruction error 
related general approach concepts information theory 
anumber learning procedures proposed minimize information loss network mapping subject processing constraints 
common feature methods preservation mutual information input vector output vector ix entropy joint distribution logp 
measure due shannon tells amount information amount remaining known uncertainty accounted vice versa 
unconstrained noise free case information preserved simply copying input 
linsker proposes maximizing information rate presence processing noise input output layer infomax principle 
information rate collection linear units gaussian input distribution independent equal variance gaussian noise added outputs log jq jq determinant covariance matrix output vector signal plus noise noise variance 
results tradeo maximizing variances outputs decorrelating depending noise level 
analyses apply linear networks 
adding usual form sigmoid nonlinearity information theoretic analysis di cult 
linsker shows adding weakly nonlinear cubic input output relation infomax principle certain input distributions results set units tuned di erent spatial frequencies spatial locations representation 
alternative optimality criterion proposed barlow nd minimally redundant encoding facilitate subsequent learning 
encoding sensory input vector element feature vector property elements statistically independent required form new associations event assuming features approximately independent conditioned knowledge conditional probabilities feature yi complete knowledge probabilities events conditional possible sensory inputs 
representation useful preprocessing stage variety problems 
barlow proposes way toachieve featural independence nd minimum entropy encoding invertible code information loss minimizes sum feature entropies 
general case problem intractable 
atick redlich proposed cost function barlow principle linear systems minimizes power redundancy outputs subject minimal information loss constraint 
closely related plumbley objective function minimizes infor unsupervised learning procedures mation loss subject xed power constraint hebbian learning scheme derived 
schmidhuber proposed ways approximating barlow minimum redundancy principle general case nonlinear networks 
gaussian assumptions implies stronger result statistically independent just decorrelated outputs 
schmidhuber learning scheme complex appears subject oscillations local minima 

applications applications pca subspace methods related data compression preprocessing speech images 
sanger example gha network image compression 
extended nonlinear units recti cation nonlinearity discover stereo disparity random dot stereograms 
leen rudnick pca networks signal pre processing improved classi cation performance 
karhunen sga principal subspace algorithm frequency estimation 
suggest factor jx gives initial convergence decreasing rst cycles 
alternative application subspace methods direct classi cation 
oja allows network learn subspace class modi cations learning algorithms sure subspaces di erentiate classes 
classifying class closest subspace input vector calculated projection subspace winner 
approach classify brodatz image textures 
temporal sequence predictors applied range prediction problems 
recurrent networks predominantly symbolic tasks goal represent discrete hidden variables extracting temporal structure 
example elman simple recurrent networks infer grammatical structure sentences 
mozer applied recurrent networks multiple time decay constants problem extracting musical structure becker plumbley multiple time scales predicting notes musical score 
tapped delay line method successfully real valued noisy signal prediction problems time series prediction 
key selecting appropriate architecture temporal problem appropriate characterization problem advance 
example problem requires explicit knowledge previous real valued inputs short time window time delay architecture appropriate 
problem requires bits history information arbitrarily long time periods small number hidden units recurrent connections learn hidden state variables 
accurate real valued state information di cult maintain time recurrent networks large number hidden units 
jutten herault applied independent components analysis algorithm images handwriting 
random samples points input neuron network able remove dependence input coordinates resulting output images consisting text 
companion jutten herault demonstrated success algorithm anumber synthetic nonlinear source separation problems 

density estimation techniques trying retain information contained input try develop representation characterizing underlying probability distribution 
approach lead useful features classi cation subsequent learning tasks 
standard statistical methods fall category density estimation techniques unsupervised learning procedures viewed way 
fact techniques discussed category density estimation speci cally characterized maximum likelihood parameter estimation methods 
general approach assume prior model constrains general form probability density function search particu lar model parameters de ning density function generated observed data 
approach mapped unsupervised learning problem treat network weights model parameters function computed network directly related density function 

encodings mixture models competitive learning possible choice prior model mixture gaussians 
underlying assumption case generated gaussians having di erent means variances prior probabilities mixing proportions fixing model parameters compute probability data point follows ig ig ig ipi pi probability ith gaussian 
applying bayes rule compute probability gaussians generated data point ijx ig ig ig ipi pj probabilities model parameters adapted performing gradient ascent log likelihood data model log log ig ig ig em algorithm alternately applies equation expectation step adapts model parameters maximization step converge maximum likelihood mixture model data 
competitive learning procedures primarily clustering 
general idea underlying competitive learning induce competition units responses winner take activation function lateral interactions unit competitive cluster tends active time 
typically winning unit learns case moving weight vector closer current input pattern 
example rumelhart zipser version competitive learning sets activity winning unit greatest total input xi rest zero uses learning rule wji wji wji unit wins pattern redistributing proportion unit weights weights active input lines rule maintains constraint wji 
unit performing gradient descent squared distance weight vector patterns nearest weight vector subject unit length constraint batch version equivalent standard means clustering algorithm 
nowlan pointed standard version competitive learning closely related tting mixture gaussians model equal priors equal variances em algorithm unit just winner adjust mean distance current input vector proportion probability gaussian model accounts current input equation 
competitive learning approximates step making hard inn ary decision unit accounts input 
learning rule applies proportional weighting replaced decision 
hard competitive learning viewed mixture gaussians estimation procedure small variances 
nowlan proposed soft competitive learning model neural networks 
allowing winner winning neighborhood adapt unit adapt weights input case proportion strongly responds case pi pi pi unsupervised learning procedures variances adapted similarly 
online version em algorithm gaussian densities equal priors adaptive means variances 
neal hinton shown incremental variants em perform gradient descent global energy function treat nowlan algorithm special case show performs approximate gradient descent 
appealing aspect soft competitive learning algorithm generalized incorporate arbitrary prior information supervisory signals competing experts model jacobs 
jordan jacobs show generalize competing experts framework discover hierarchical decomposition structure input 
useful variation competitive learning de ne neighborhood relation units example arranging lattice unit learns proportion distance winning unit 
forces competing units form topographic mapping nearby points input space mapped nearby points neighborhood space 
kohonen model unsupervised topological map formation set ni de ned unit determining units neighborhood 
winning unit euclidean distance weight vector current input vector minimal 
unit winner neighborhood nc adapts weights learning rule wji wji wji unit nc learning rate neighborhood size shrink gradually course learning units responses tend distributed evenly input probability distribution 
neighborhood set nc replaced continuous function distance units lattice leads smoother learning 
neighborhoods size standard competitive learning kohonen algorithm equivalent means clustering 
larger neighborhoods algorithm gen becker plumbley means adapts weight centre cluster patterns neighbors clusters resulting ordered mapping tends preserve topological structure input distribution 
luttrell shown kohonen algorithm viewed variant minimum distortion vector quantization 
interpret weight vector winning unit predicted input wc distortion reconstruction error just jx xj suppose additive noise process distorts output units making impossible determine complete certainty output winner 
unit contribute distortion error function distance winner yi yc jx xj noisy version equivalent algorithm neighborhood function framework luttrell goes generalize kohonen algorithm perform hierarchical 
summary hard competitive learning model kohonen algorithm viewed version mixture gaussians model small variances addition noise process distorting estimates probabilities gaussian capturing data 
fukushima neocognitron generalizes standard hard competitive learning form multi resolution hierarchy translation invariant feature detectors 
network produced impressive results translation scale invariant character recognition 
architecture network follows layer organized dimensions planes units having di erent receptive elds weights constrained identical columns units having overlapping receptive elds di erent weights 
units higher layers receive input spatially localized region units planes layer hierarchy receptive elds di erent spatial scales formed 
details unit activation functions complex essential features model units column compete respond lateral inhibition units spatially localized region plane transmit inclusive responses layer 
feature encoded particular plane detected input retina gets transmitted layer loss spatial localization 
competitive learning winning unit competing cluster column moves weights current input additionally units plane perform identical weight updates 
note possible multiple planes adapt simultaneously case 
neocognitron variation density estimation equivalent hard competitive learning addition equality constraints weights corresponding units di erent competing clusters having identical receptive elds 

combinatorial representations major limitation mixture models standard competitive learning schemes employ encoding single unit model assumed generated data 
multiple causes model appropriate compact data description consists independent parameters color shape size object visual scene 
mozer proposed distributed version competitive learning binary units discovers multiple classi cations data 
layer competing units receives input reconstruction error previous layer performs clustering error 
mozer shows scheme discover useful distributed binary features image compression 
general way arbitrary probability distributions data stochastic boltzmann machine sbm computationally cient cousin deterministic boltzmann machine dbm 
dbm considerably faster train mean eld approximation eliminates stochasticity annealing process sampling equilibrium correlation statistics 
theoretically train unsupervised dbm exactly sbm 
unfortunately mean eld approximation dbms particularly useful case dbm form adequate representation distribution states units represented single mean state 
peterson hartman suggest way problem 
negative phase random subset units left 
method showed network able learn set random patterns perform pattern completion partially speci ed noisy patterns 
freund haussler describe cient way unsupervised 
goal learn hidden causes collection patterns hidden unit representing hidden cause 
restricted architecture allowing layer connections visible units clamped settling required 
case ciently computing absolute probabilities input state summed possible state hidden units 
leads single phase learning procedure maximizes absolute probability training patterns 
network learns adopt hidden unit states generators input pattern set 
model single hidden layer take exponentially hidden units model arbitrary probability distribution radford neal personal communication 
neal presents general way modeling probability distribution pattern set multilayer stochastic connectionist belief network falls class pearl belief networks 
restricted single layer networks able model complex distributions fewer parameters high price learning time 

applications variations soft competitive learning referred radial basis function rbf networks widely preprocess speech data 
common technique unsupervised learning adapt gaussian means unsupervised learning procedures mixing proportions subsequently thrown add second layer linear units trained supervised learning discover optimal linear combinations gaussian unit outputs perform classi cation 
example nowlan showed method superior traditional hard competitive learning models classi cation tasks hand written digit vowel recognition 
jordan jacobs applied hierarchical version competing experts model set competing auto encoder networks learn hierarchical classi cation leaf morphology data 
kohonen applied algorithm preprocessed speech data clusters units usually correspond phonemes 
sequences quasi phonemes produced processing sequence time slices speech signal viewed ordered trajectory phonological map indicating network learned represent similar sounds nearby locations map 
applications clustering appropriate preprocessing stage kohonen algorithm easy implement tends sensitive initial conditions compared standard competitive learning prone greedy behaviour subset competing units capture patterns initially competitors prevented learning 
neocognitron applied simple character recognition problems 
equality constraints implicit learning procedure model able learn classes somewhat scale shift invariant 
algorithm proven able discriminate nonlinearly separable pattern classes building translation invariance feature detectors 
methods proposed freund haussler neal theoretical interest potential applications unsupervised higher order feature discovery classi cation 
cient methods real world applications remains demonstrated 

feature extraction methods methods discussed previous sections goal characterizing ac becker plumbley possible input patterns underlying distribution generated 
reasonable rst step extracting useful structure data assuming minimal prior knowledge 
real world data redundant noisy general methods clustering principal components useful improving signal noise ratio achieving data compression 
unsupervised learning applied preprocessing stages extract higher order features build representations 
approach build sophisticated prior models seen examples approach freund haussler neal methods 
approach restrict search particular kinds structure 
constraining assumptions kind structure looking build constraints network architecture objective function develop cient highly specialized learning procedures 
section consider examples learning procedures idea learning particular features data 

maximally selective projections bienenstock cooper munro proposed learning rule commonly referred bcm rule results form temporal selectivity single unit respect particular environment 
proposed measure selectivity depends ratio unit mean response inputs maximal response sel yj yj max yj ideal unit measure gives maximal response particular pattern low responses patterns 
achieve authors proposed family hebb learning rules satisfy wji yj yj yi wji denotes rate change time yj mean output satisfy sign yj yj sign yj yj yj yj yj positive constants units activities positive 
learning rule led development tight orientation tuning curves units simple oriented line patterns inputs 
intrator proposed related objective function maximizing selectivity causes unit discover projections data having bimodal multi modal distributions 
intrator objective function units sigmoidal nonlinearity yj threshold unit set applied group units inhibit leads discovery multiple features data 
intrator discusses relation method exploratory projection pursuit 
method tends discover projections having non gaussian skewed distribution may useful features certain classi cation problems 

statistical dependencies inputs gmax algorithm goal redundancy detection 
causes unit discover statistical dependencies input lines maximizing di erence output distribution unit response structured input distribution expected input lines independent 
probabilistic binary units asymmetric divergence distributions maximized log log general feasible calculate explicitly requires sampling possible states input units 
pearlmutter hinton approximate expected value output distribution simulating negative learning phase somewhat analogous boltzmann machine input patterns independent components generated 
unit trained manner images oriented bars learns centre surround receptive elds learned linsker hebbian network 
unfortunately straightforward generalization gmax principle multiple output units 
pearlmutter hinton propose possible approximations adding extra term objective function minimizes correlation coe cient outputs units mechanism mutual inhibition 
encourage units learn statistically independent features encourage discovery mutually exclusive features 
gmax principle mayhave limited applicability general problems binary case sampling problem mentioned possible apply principle continuous gaussian case described subsection results interesting algorithm applicable multi layer nonlinear networks 

invariant features suggested computation invariant features world plays fundamental role human pattern recognition 
suggests reasonable goal unsupervised learning develop invariance detectors units discover features input distribution exhibit form invariance unit nds nontrivial linear combination inputs zero 
attractive aspect view actual output invariance detector represent extent current input violates network model regularities world 
cient way transmitting information current input 
unsupervised learning procedures algorithms learning invariant features input proposed 
kohonen oja proposed learning algorithm single unit acts novelty detector responding best patterns orthogonal principal subspace input distribution 
fallside proposed learning procedure implements linear prediction lter unit receives inputs representing values signal time frames tries output zero computing sum signal current time slice linear combination signal values previous time slices 
atick redlich proposed equivalent learning procedure spatial predicting unit model development retinal ganglion cell kernels 
methods applied nonlinear case methods closely related imax described subsection 
becker shown continuous generalization gmax gaussian input distributions results invariance detector minimizes ratio output variance divided variance expected input lines independent sum variances inputs yi yj yi 
analysis assumes output yi linear function inputs yj inputs outputs nonlinear hidden units resulting multi layer learning procedure discovering higher order invariants 
algorithm generalized apply group units form mixture model di erent invariant properties input patterns 
schraudolph sejnowski propose closely related learning scheme combining variance minimizing anti hebbian term term prevents weights converging zero 
show set competing units discover population codes stereo disparity random dot stereograms 
bell proposed energy minimizing algorithm single axon leads anti hebbian learning rule predicts evolution ion channel densities allows axon redistribute chan becker plumbley nels ciently process recurring spatiotemporal patterns 

spatially coherent features becker hinton proposed objective function unsupervised learning discover properties sensory input exhibit coherence space time 
imax learning procedure maximizing mutual information outputs ya yb network modules receive input different parts sensory input di erent modalities di erent spatial temporal samples shown 
entropies yb ya yb ya yb constraining assumptions distributions ya yb 
discrete binary case expected outputs units approximated joint probabilities sampling input ensemble 
entropies computed analytically yi hlog pii pi log pi log pi yi expected value ith unit output averaged uctuations training case ensemble cases yi treated stochastic binary variable yi 
learning rule obtained di erentiating maximize image patch image patch fig 

units separate inputs maximize mutual information 
iyi yj iyi yj log pi log pi log pij pi expected output ith unit training case probability training case pij learning rule applied multi layer modules learn features linearly separable shift random binary shift patterns 
modules receive input random binary patterns left half shifted version right half 
inputs modules unrelated apart having shift 
unfortunately particular problem binary version imax tendency trapped local maxima objective function encourages units strongly binary develop large weights 
way extend imax handle multi valued spatially coherent features maximize mutual information discrete valued variables binary variables 
set units forced represent probability distribution states discrete random variable fa ang adopting states probabilities sum 
done example softmax activation function suggested bridle ai xi xi total weighted summed input ith unit 
mutual information valued variables computed straightforward manner probabilities value pairwise probabilities ia ai logp ai bj logp bj ai bj ij log ai bj straightforward generalization binary case similar learning rules derived 
gaussian assumptions simpler objective function imax learning derived 
assume modules receive input caused common underlying signal corrupted independent gaussian noise input patch modules transform input outputs ya yb noisy versions signal ya na yb nb log ya yb ya yb stands variance 
measure tells information average ya yb conveys common underlying signal feature input samples 
multi dimensional features continuous version imax generalized extract multiple parameters represent multi dimensional gaussian signals independent additive gaussian noise 
case objective function maximized ya yb signal log jq ya yb jq ya yb parameter vectors extracted neighboring patches covariance matrix jqj determinant 
zemel hinton explored applications method visual object recognition problems 

applications intrator demonstrated nonlinear version bcm rule speech dimensionality large compared supervised backprop algorithm able nd richer linguistically meaningful structure unsupervised learning procedures containing burst locations di erent stops allowed better generalization speakers voiced stops 
intrator pp 
multi valued discrete version imax successfully applied single layer networks toy problems extracting combinations spatial frequency phase sinusoidal intensity patterns 
applied limited success lapedes di cult problem extracting mutually predictable features protein sequences dimensional descriptions evan personal communication method prone getting trapped local optima 
continuous version imax mainly applied toy problems su cient complexity solved single layer linear network 
example binary shift problem performs extremely getting stuck local minima problem 
applied successfully continuous higher order feature extraction problems learning represent stereo disparity surface curvature random dot stereograms 
mixture model underlying coherent feature algorithm extended develop population codes spatially coherent features stereo disparity model locations discontinuities depth 
applied temporally varying patterns classify temporally coherent objects time 
zemel hinton applied multidimensional version imax problem learning represent viewing parameters simple synthetic dimensional objects 
learning procedure tries extract multiple features image patch uncorrelated predictors feature vector extracted neighboring patch 
method potentially powerful linear methods components analysis network compute arbitrary nonlinear transformations order extract features 
di culty method practical limitations computing determinants ill conditioned matrices 
becker plumbley drawback representation learned algorithm viewing parameters problem simple 
features network learns represent nonlinear combinations viewing parameters scale location size easily interpreted 
source separation promising real world application imax related methods signal separation contrast enhancement oftwo sources 
problem complementary coherence detection discovering common underlying signal want factor separate components sources 
way try extract features having minimal mutual information rule trivial solutions features convey information 
haykin proposed minimizing equation adding penalty term jq prevent degenerate solutions ya yb 
haykin applied idea successfully problem detecting distinctive features pair orthogonally polarized radar detectors goal detect re ector target dual polarized radar images 
learning appropriate nonlinear mapping method able outperform standard linear preprocessing pca 
note alternative algorithm source separation jutten herault discussed section 
unsupervised learning procedures applied real world problems reduce noise compress data extract useful features subsequent classi cation 
attempted provide survey widely successful methods highlight methods show promise 
major challenge research area learning procedures extract nonlinear features applicable variety signal classi cation problems 
authors wish chris williams rich zemel peter dayan anonymous reviewers helpful comments earlier drafts 

abbas fahmy 
neural model adaptive karhunen loeve transform klt 
proceedings international joint conference neural networks ijcnn baltimore pages ii 

atick redlich 
predicting ganglion simple cell receptive eld organizations information theory 
technical report hep institute advanced study princeton 

atick redlich 
theory early visual processing 
neural computation 

baldi hornik 
neural networks principal component analysis learning examples local minima 
neural networks 

barlow 
unsupervised learning 
neural computation 

becker 
information theoretic unsupervised learning algorithm neural networks 
phd thesis university 

becker 
learning categorize objects temporal coherence 
advances neural information processing systems pages 
morgan kaufmann 

becker hinton 
self organizing neural network discovers surfaces random dot stereograms 
nature 

becker hinton 
learning mixture models spatial coherence 
neural computation 

bell 
self organisation real neurons channel space 
advances neural information processing systems pages 
morgan kaufmann 

bienenstock cooper munro 
theory development neuron selectivity orientation speci city binocular interaction visual cortex 
journal neuroscience 

bourlard kamp 
auto association multilayer perceptrons singular value decomposition 
biological cybernetics 

bridle 
probabilistic interpretation feedforward classi cation network outputs relationships statistical pattern recognition 
soulie herault editors nato asi series systems computer science 
springer verlag 

carpenter grossberg 
massively parallel architecture self organizing neural pattern recognition machine 
computer vision 

cottrell munro zipser 
image compression back propagation demonstration extensional programming 
sharkey editor advances cognitive science volume 
norwood nj 

dempster laird rubin 
maximum likelihood incomplete data em algorithm 
proceedings royal statistical society 

elman 
finding structure time 
cognitive science 

elman zipser 
learning hidden structure speech 
ics report institute cognitive science university california san diego 

fallside 
analysis multi dimensional linear autoregressive class single layer connectionist models 
iee conference arti cial neural networks pages 


adaptive network optimal linear feature extraction 
proceedings international joint conference neural networks ijcnn pages washington dc 

freund haussler 
unsupervised learning distributions binary vectors layer networks 
advances neural information processing systems pages 
morgan kaufmann publishers 

fukushima 
self organizing multilayered neural network 
biological cybernetics 

fukushima 
neocognitron self organizing neural network model mechanism pattern recognition una ected shift position 
biological cybernetics 

fukushima 
hierarchical neural network model associative memory 
biological cybernetics 


learning deterministic boltzmann machine networks 
phd thesis university toronto 


relationships svd klt pca 
pattern 

golub van loan 
matrix computations 
north oxford academic oxford 

gonzalez 
digital image processing 
addison wesley reading ma second edition 

hinton sejnowski 
learning relearning boltzmann machines 
rumelhart mcclelland pdp research group editors parallel distributed processing explorations microstructure volume pages 
cambridge ma mit press 

hornik 

convergence analysis local feature extraction algorithms 
neural networks 

intrator 
feature extraction unsupervised neural network 
neural computation 

jacobs jordan nowlan hinton 
adaptive mixtures local experts 
neural computation 
unsupervised learning procedures 
jordan jacobs 
hierarchies adaptive experts 
advances neural information processing systems pages 
morgan kaufmann 

jutten herault 
blind separation sources part adaptive algorithm architecture 
signal processing 

jutten herault 
blind separation sources part ii problems statement 
signal processing 

karhunen 
tracking sinusoidal frequencies neural network learning algorithms 
proceedings ieee international conference speech signal processing icassp toronto canada 

kohonen 
clustering taxonomy topological maps patterns 
lang editor proceedings sixth international conference pattern recognition silver spring md 
ieee computer society press 

kohonen 
neural phonetic typewriter 
ieee computer 

kohonen oja 
fast adaptive formation lters associative memory recurrent networks neuron elements 
biological cybernetics 

kung 
neural network learning algorithm adaptive principal component extraction apex 
proceedings ieee international conference speech signal processing icassp pages ii 

lapedes farber 
nonlinear signal processing neural networks prediction system modelling 
technical report la ur los alamos national laboratory 

leen 
dynamics learning linear networks 
network 

leen rudnick 
hebbian feature discovery improves classi er ciency 
proceedings international joint conference neural networks ijcnn pages washington dc 

linsker 
self organization perceptual network 
ieee computer march 

linsker 
deriving receptive elds optimal encoding criterion 
advances neural information processing systems pages 
morgan kaufmann 

luttrell 
hierarchical vector quantisation 
proceedings inst 
elec 
eng volume pages 

mozer 
discovering discrete distributed representations iterative competitive learning 
advances neural information processing systems pages 
morgan kaufmann 

mozer 
induction temporal structure 
advances neural information processing systems pages 
morgan kaufmann 

mozer 
neural net architectures temporal sequence 
weigend gershenfeld editors predicting past 
redwood city ca addison wesley publishing 
becker plumbley 
neal 
connectionist learning belief networks 
arti cial intelligence 

neal hinton 
new view em algorithm justi es incremental variants 
submitted publication 

nowlan 
maximum likelihood competitive learning 
touretzky editor neural information processing systems vol 
pages san mateo ca 
morgan kaufmann 

nowlan 
soft competitive adaptation neural network learning algorithms fitting statistical mixtures 
phd thesis carnegie mellon university pittsburgh pa 
published cmu technical report cmu cs 

oja 
simpli ed neuron model principal component analyser 
journal mathematical biology 

oja 
neural networks principal components subspaces 
international journal neural systems 

oja 
principal components minor components linear neural networks 
neural networks 

oja karhunen 
stochastic approximation eigenvectors eigenvalues expectation random matrix 
journal mathematical analysis applications 

oja ogawa 
pca fully parallel neural networks 
aleksander taylor editors arti cial neural networks pages amsterdam 
north holland 

pearl 
probabilistic reasoning intelligent systems networks plausible inference 
san mateo california morgan kaufmann 

pearlmutter hinton 
maximization unsupervised learning procedure discovering regularities 
denker editor neural networks computing american institute physics conference proceedings pages 

peterson anderson 
mean eld theory learning algorithm neural networks 
complex systems 

peterson hartman 
explorations mean eld theory learning algorithm 
neural networks 

plumbley 
cient information transfer anti hebbian neural networks 
neural networks 

plumbley 
hebbian anti hebbian network optimizes information capacity principal subspace 
proceedings iee arti cial neural networks conference ann pages brighton uk may 

plumbley fallside 
informationtheoretic approach unsupervised connectionist models 
david touretzky geo rey hinton terrence sejnowski editors proceedings connectionist models summer school pages 
morgan kaufmann san mateo ca 

rubner 
self organizing network principal component analysis 
letters 

rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland editors parallel distributed processing explorations microstructure volume pages 
mit press cambridge ma 

rumelhart zipser 
competitive learning 
cognitive science 

sanger 
optimal unsupervised learning single layer feedforward neural network 
neural networks 

saund 
dimensionality reduction connectionist networks 
ieee transactions pattern analysis machine intelligence 

schmidhuber 
learning factorial codes predictability minimization 
neural computation 

schmidhuber 
learning unambiguous reduced sequence 
advances neural information processing systems pages 
morgan kaufmann 

schraudolph sejnowski 
competitive anti hebbian learning invariants 
advances neural information processing systems pages 
morgan kaufmann 

shannon 
mathematical theory communication 
bell system technical journal 

haykin 
application unsupervised neural networks enhancement polarization targets dual polarized radar images 
ieee canadian computer engineering 

von der malsburg 
self organization orientation sensitive cells striate cortex 
kybernetik 

watanabe 
pattern recognition human mechanical 
john wiley sons new york 

weigend huberman rumelhart 
predicting connectionist approach 
international journal neural systems 

williams 
feature discovery learning 
ics report institute cognitive science university california san diego 

zemel hinton 
developing topographic representations minimizing description length 
cowan tesauro alspector editors advances neural information processing systems pages 
morgan kaufmann 

zemel hinton 
discovering relationships characterize objects 
lippmann moody touretzky editors advances neural information processing systems pages 
morgan kaufmann publishers 
becker assistant professor psychology programme ordinator degree neural computation mcmaster university hamilton ontario 
research interests include information theoretic unsupervised learning algorithms perceptual development models human learning memory 
dr becker received ba degree queen university psychology msc degree queen university computer science phd degree university toronto computer science 
unsupervised learning procedures mark plumbley lecturer department electronic electrical engineering king college london 
deputy director ec funded network excellence neural networks european neural network society 
research interests include information theory neural networks unsupervised learning perceptual systems genetic algorithms 
dr plumbley received ba degree university cambridge churchill college electrical science tripos phd university cambridge department engineering neural networks 
