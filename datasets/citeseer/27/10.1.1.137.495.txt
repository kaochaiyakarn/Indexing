non linear dimensionality reduction david demers garrison cottrell dept computer science engr institute neural computation university california san diego gilman dr la jolla ca method creating non linear encoder decoder multidimensional data compact representations 
commonly technique autoassociation extended allow non linear representations objective function penalizes activations individual hidden units shown result minimum dimensional encodings respect allowable error reconstruction 
reducing dimensionality data minimal information loss important feature extraction compact coding computational efficiency 
data representations processing constraints feature variables may identified redundancy eliminated 
algorithms exponential input reduction single dimension may provide valuable computational savings 
feedforward networks hidden layer shown extract principal components data baldi hornik 
networks extract features develop compact encodings data cottrell munro zipser 
principal components analysis projects data linear subspace email demers cs ucsd edu email gary cs ucsd edu non linear principal components net auto associator output decoding layer hidden layer bottleneck encoding layer network capable non linear lower dimensional representations data 
minimum information loss multiplying data eigenvectors sample covariance matrix 
examining magnitude corresponding eigenvalues estimate minimum dimensionality space data may projected estimate loss 
data lie non linear submanifold feature space principal components overestimate dimensionality 
example covariance matrix data sampled helix full rank principal components 
helix dimensional manifold smoothly parameterized single number 
addition hidden layers inputs representation layer representation layer outputs provides network capable learning non linear representations kramer oja nakano 
networks perform non linear analogue principal components analysis extract principal manifolds 
shows basic structure network 
dimensionality representation layer problematic 
ideally dimensionality encoding number representation units needed determined data 
propose pruning method determining dimensionality representation 
greedy algorithm successively eliminates representation units penalizing variances results encodings minimal dimensionality respect allowable reconstruction error 
algorithm performs non linear dimensionality reduction nldr 
input dimensionality estimation regularization priori assignment number units representation layer problematic 
order achieve maximum data compression number small possible wants preserve data encode data minimum error 
intrinsic dimensionality known ahead time typical method estimate dimensionality desired 
minimization variance representation unit essentially squeeze variance data hidden units 
repeated minimization results increasingly lower dimensional representation 
formally dimensionality raw data wish find approximate inverse rn rn denote dimensional vector elements functions fi component functions fi constant autoassociation eliminated yielding function components 
constant value fi means variance fi data zero 
add regularization term objective function penalizing variance representation units 
variance driven near zero simultaneously achieving target error primary task autoassociation unit penalized pruned 
hp hp net hp net net net input unit jth training pattern hp net activation pth hidden unit representation layer penalized expectation operator 
notational clarity superscripts suppressed 
hi xj estimated hp mean activation hp patterns training data 
hp hp hp hp ol derivative activation function unit respect input ol output lth unit preceding layer 
ph hp hp 
simply add delta hp due backpropagation output layer 
train multi layer network learn identity map 
error user specified threshold increased unit lowest variance 
network weights variance reduced small threshold remaining units able encode data hidden unit question longer contributing connections excised network 
process repeated variance unit question reduced maintaining low error 
reason suppose encoding decoding layers size 
fact may encoding decoding layers provide superior performance 
helix example decoder hidden layers linear connections representation output encoder single layer 
kramer uses information theoretic measures choosing size encoding decoding layers fixed representation layer equal encoding decoding layers 
unbounded weights allow amount information pass layer arbitrarily small variance arbitrarily large weights 
weights network bounded 
weight vectors magnitudes larger renormalized epoch 
original helix data plus reconstruction single parameter encoding 
results applied method problems 
closed manifold 
helix 
time series data generated mackey glass delay differential equation 

pixel bit grayscale face images 
number parameter values chosen error threshold maximum magnitude weights value increased give training 
experiments chosen hand reasonable values selected method automated 
static mappings circle helix problem interesting known diffeomorphism circle unit interval 
smooth single parameter encodings cover entire circle region circle left arbitrarily small 
depending initial conditions technique different solutions 
simulations resulted dimensional representation encodings lying circle failure reduce dimensionality 
solutions representations wrapping unit interval circle splitting interval pieces 
initial architecture consisted single unit encoding layer unit decoding layers 
set error threshold 
helix problem interesting data appears dimensional pca 
nldr consistently finds invertible dimensional representation data 
signal reconstructed signal encoding reconstructed signal encoding data mackey glass delay correlation dimension reconstructed signal encoded dimensions 
shows original data network output representation layer stimulated activation ranging 
training data mapped interval single sigmoidal representation unit 
initial architecture consisted single unit encoding layer unit decoding layers 
set error threshold 
nldr applied time series mackey glass problem consists estimation intrinsic dimensionality scalar signal 
classically time series data embedded space high dimension expects geometric invariants preserved 
may significantly overestimate number variables needed describe data 
different series examined parameter settings mackey glass equation chosen intrinsic dimensionality 
data embedded high dimensional space standard technique recoding vectors lagged data 
dimensional representation dimensional data dimensional representation dimensional data 
shows original data reconstruction dimensional data 
allowing higher reconstruction error resulted dimensional representation dimensional data effectively smoothing original signal demers 
shows original data reconstruction dimensional data 
initial architecture consisted unit encoding layers unit decoding layers unit representation layer 
representation layer connected directly output layer 
set error threshold 
faces face image data challenging 
face data pixel bit grayscale images taken cottrell metcalfe considered point dimensional pixel space 
question addressed nldr find low dimensional representations data useful principal components 
data preprocessed reduction principal mackey glass signal reconstruction error bound reconstruction error bound data mackey glass delay correlation dimension reconstructed signal encoded dimensions different error thresholds 
components images 
reduced representations processed nldr 
architecture consisted unit encoding layer unit decoding layer initial representation layer units 
direct connections representation layer output layer 

nldr dimensional representation 
shows images reduction principal components training images reconstruction dimensional encoding 
unable determine dimensions meaningful experiments decoder show points inside convex hull representations project images look faces 
shows reconstructed images linear interpolation face space encodings furthest apart 
useful representations obtained training set identification classification images subjects 
representations train feedforward network recognize identity gender subjects cottrell metcalfe 
images training remaining test set 
network correctly identified training data subjects test set 
network achieved correct gender recognition training test sets 
misclassified subject shown 
informal poll visitors poster denver showed humans classify subject male female 
nldr resulted dimensional encodings face data superficially compresses data approximately bits image bits pixel data compression 
decoder portion network eigenvectors initial processing stored 
amortize bits pixel original images require bits pixel run length encoding 
order achieve data compression larger data set obtained order find underlying human face manifold 
chosen graph eigenvalues point began flatten value reasonable 
original face images reconstruction encoding dimensional data 
images encodings furthest apart reconstructions points equally spaced line joining 
pat subject gender feedforward network classified incorrectly 
method automatically generating non linear encoder decoder high dimensional data 
number representation units final network estimate intrinsic dimensionality data 
results sensitive choice error bound precise relationship unknown 
size encoding decoding hidden layers controlled avoid fitting data set encoded scalar values resolution 
gradient search solve global non linear optimization problem guarantee method find global optimum avoid convergence local minima 
nldr consistently constructed low dimensional encodings low loss 
matthew turk alex pentland making software available extract eigenvectors original face data 
author partially supported fellowships california space institute mcdonnell pew foundation 
pierre baldi kurt hornik neural networks principal component analysis learning examples local minima neural networks 
garrison cottrell paul munro principal components analysis images backpropagation proc 
spie cambridge ma 
garrison cottrell paul munro david zipser image compression backpropagation demonstration extensional programming sharkey noel ed models cognition review cognitive science vol 

garrison cottrell janet metcalfe face emotion gender recognition holons lippmann moody touretzky eds advances neural information processing systems 
david demers dimensionality reduction non linear time series neural stochastic methods image signal processing spie 
mark kramer nonlinear principal component analysis autoassociative neural networks aiche journal 
erkki oja data compression feature extraction autoassociation feedforward neural networks kohonen simula kangas eds artificial neural networks 
nakano internal color representation acquired layer neural network kohonen simula kangas eds artificial neural networks 
