proceedings international joint conference neural networks hierarchical mixtures experts em algorithm michael jordan department brain cognitive sciences mit cambridge ma tree structured architecture supervised learning 
statistical model underlying architecture hi mixture model mixture coefficients mixture components generalized linear models 
learning treated max imum likelihood problem particular expectation maximization em algorithm adjusting parame ters architecture 
develop line learning algorithm pa rameters updated incrementally 
com simulation results robot dynamics domain 
statistical literature machine learning literature divide conquer algorithms increasingly popular 
cart algorithm mars algorithm id algorithm known examples 
algorithms fit surfaces data explicitly dividing input space nested sequence regions fitting simple surfaces constant functions regions 
advantages algorithms include interpretability solutions speed process 
neural network archi tecture close cousin architectures cart mars 
earlier formulate learning problem architec ture maximum likelihood problem 
cur rent expectation maximization em framework derive learning algorithm 
hierarchical mixtures algorithms discuss supervised learning algorithms 
explicitly ad dress case regression input vec tors elements sm output vectors elements xn 
model handles classifi robert jacobs department psychology university rochester rochester ny ix ix expert expert network network level hierarchical mixture ex 
cation problems counting problems outputs integer valued 
data assumed form countable set paired observations dt 
case batch algorithm discussed set assumed finite case line algorithm set may infinite 
propose solve nonlinear supervised learning problems dividing input space nested set regions fitting simple surfaces data fall regions 
regions soft boundaries meaning data points may lie simul multiple regions 
boundaries tween regions simple parameterized surfaces adjusted learning algorithm 
hierarchical mixture experts hme archi tecture shown architecture tree gating networks sit non terminals tree 
networks receive vector input produce scalar outputs simplify presentation restrict level hierarchy 
algorithms describe generalize readily hierarchies arbitrary depth 
see recursive formalism handles arbitrary trees 
partition unity point input 
experi networks sit leaves tree 
expert produces output vector pij input vector 
output vectors proceed tree multiplied gating network outputs summed nonterminals 
expert networks tree linear single output nonlinearity 
refer network generalized linear borrowing terminology statistics ll 
expert network produces output pij generalized linear function input pij uij weight matrix fixed continuous nonlinearity 
vector assumed include fixed component allow intercept term 
regression problems 
identity function experts linear 
binary classification problems 
generally taken logistic function case expert outputs interpreted log odds success bernoulli probability model 
models multiway classification counting rate estimation survival estimation handled readily making choices 
models smoothed piecewise analogs corresponding generalized linear models cf 
ll 
gating networks generalized linear 
top level define linear predictors follows vj vi weight vector 
ith output top level gating network softmax function note gi positive sum gating networks lower level defined similarly yielding outputs obtained softmax function linear predictors output vector nonterminal tree weighted output experts non terminal 
output ith nonterminal second layer level tree pi output top level tree 
note depend input total output nonlinear function input 
probability model hierarchy probabilistic interpretation 
suppose mechanism data generated environment involves nested sequence decisions terminates regressive process maps decisions modeled multinomial random variables 
interpret values gi vf multinomial probabilities associated decision vz conditional multinomial probabilities associated second decision 
statistical model model probabilities particular choice parameterization cf 
eqs 
corresponds log probability model see log linear model special case commonly soft multiway classification 
loglinear model interpret gating networks modeling input dependent multinomial probabilities making particular nested sequences decisions 
particular sequence decisions output assumed generated generalized linear statistical model 
linear predictor vv formed superscript refers true values parameters 
expected value obtained passing linear predictor dj 
output chosen probability den sity mean py dispersion parameter 
denote density ix fe parameter vector includes weights dispersion parameter fj 
assume density member exponential family densities ll 
interpretation dispersion parameter depends particular choice density 
example case dimensional gaussian dispersion parameter covariance matrix assumptions total probability generating mixture proba bilities generating component densities mixture components multi nomial probabilities exponential family densities dispersion parameter particular bernoulli density dispersion parameter 
note includes expert network parame ters ij gating network parameters vp 
note utilize eq 
superscripts refer probability model de fined particular hme architecture irrespective true model 
posterior probabilities developing learning algorithms pre sented remainder prove useful define posterior probabilities associated nodes tree 
terms posterior prior meaning context training system 
refer ties prior probabilities computed input knowledge corresponding target output posterior probability defined input target output known 
bayes rule define posterior probabilities nodes tree follows dependence input parameters simplify notation 
find useful define joint pos probability hij product hj 
quantity probability expert network considered generated data knowledge input put 
emphasize quantities conditional input deeper trees posterior probability associated expert network simply product conditional posterior probabilities path root tree expert 
ke treat problem learning hme architecture maximum likelihood estimation problem 
log likelihood data set log product densities form eq 
yields log likelihood xi git 
wish maximize function respect parameters 
em algorithm sections develop learning gorithm hme architecture expectation maximization em framework 
derive em algorithm architecture consists iterative solution coupled set iteratively reweighted squares problems 
em iterative approach maximum likelihood estimation 
iteration em algorithm composed steps estimation step maximization step 
step involves maximization likelihood function redefined iteration step 
application em generally begins observation optimization likelihood function simplified set additional variables called missing hidden variables known 
context refer observable data incomplete data posit complete data set includes missing variables 
specify probability model links missing variables actual data 
logarithm density defines complete data likelihood lc 
original likelihood referred context incomplete data likelihood relationship likelihood functions motivates em algorithm 
note complete data likelihood random variable missing variables fact unknown 
em algorithm finds expected value complete data likelihood observed data current model 
step dp value parameters pth iteration expectation taken respect dp 
step yields deterministic function step maximizes function respect find new parameter estimates dp arg maxq dp step repeated yield improved estimate complete likelihood process iterates 
iterative step em chooses parameter value increases value expectation complete likelihood 
effect step incomplete likelihood 
dempster proved increase implies increase incomplete likelihood equality obtaining stationary points likelihood increases monotonically sequence parameter estimates generated em algorithm 
practice implies con vergence local maximum 
applying em hme architecture develop em algorithm hme architec ture define appropriate missing data simplify likelihood function 
define variables zij data point 
indi variables interpretation labels specify expert probability model gen erated data point 
choice missing data yields complete data likelihood 
note relationship complete data likelihood eq 
incomplete data likelihood eq 

indicator variables zij allowed logarithm brought inside summation signs substantially simplifying maximization problem 
define step em algorithm expectation complete data hood step requires maximizing dp respect expert network parameters gating network parameters 
examining eq 
see expert network parameters influence hi 
step involves separate note maximization problems maximum likelihood problems gj probability densities 
parameterization ties log likelihoods obtained weighted log likelihoods generalized linear mod els 
efficient algorithm known iter reweighted squares irls available solve maximum likelihood problem models ll 
see discussion irls 
summary em algorithm ob tained involves calculation posterior ties outer loop step solution set weighted irls problems inner loop step 
summarize algorithm fol lows hme 
data pair compute posterior probabilities hi 
current values parameters 

expert solve irls problem observations fj observation weights 

top level gating network solve irls problem observations hf 
observation weights ht 

iterate updated parameter values 
presents approximation algorithm gating networks fit squares maximum likelihood 
case irls inner loop reduces weighted squares problem solved iteration 
function terms pij simulation results gating network parameters influence tested algorithm nonlinear system iden function terms hi sit problem 
data obtained simulation joint robot arm moving dimensional space 
network learn forward dynamics arm mapping twelve coupled input variables output variables 
mapping smooth expect architecture relative error epochs cart na cart na mars na table average values relative error num ber epochs required convergence batch algorithms 
error global fitting algorithm backprop small main interest train ing time 
generated data points training points testing 
epoch pass training set computed rela tive error best set 
relative error computed ratio mean squared error mean squared error obtained learner output mean value puts data points 
compared performance binary hierarchy best linear approximation back propagation network cart algorithm mars algorithm 
hierarchy level hi expert networks gating net works 
expert network output units gating network output unit 
back propagation network hidden units yields approximately number parame ters network hierarchy 
mars algorithm run maximum basis func tions fact function cor responds roughly single expert hme ar chitecture 
table reports average values minimum rela tive error convergence times architec tures 
seen table gation algorithm required passes data converge relative error 
hme algorithm converged similar relative er ror passes data 
cart mars required similar cpu time compared hme algorithm produced accurate fits 
details simulation see si 
shown hme architecture lends self graphical investigation 
dis plays time sequence distributions probabilities training set node tree 
learning taken place posterior probabilities node approximately training set 
training proceeds histograms flatten epoch epoch sequence histogram trees hme architecture 
histogram displays distribu tion posterior probabilities training set node tree 
eventually approaching bimodal distributions posterior probabilities zero training patterns 
evolu tion indicative increasingly sharp splits fit gating networks 
note dency splits formed rapidly higher levels tree lower levels 
line algorithm jordan jacobs derive line algorithm hme architecture techniques re cursive estimation theory lo 
line update rule parameters expert networks recursive equation pj tr rij inverse covariance matrix ex pert network 
matrix updated equation decay parameter 
similar update rules obtained parameters gating networks 
see details 
simulation results line algorithm tested robot dy problem described previous section 
backprop line hme line table average values relative error number epochs required convergence line algorithms 
performance algorithm compared line backpropagation network 
minimum values relative error con vergence times architectures provided table 
line algorithm backpropagation signifi cantly faster corresponding batch algorithm cf 
table 
true line hme algorithm converges passes data 
novel tree structured architec ture supervised learning 
architecture statistical model contact number branches statistical theory includ ing mixture model estimation generalized linear model theory 
learning algorithm archi tecture em algorithm 
major advantage hme approach related decision tree multivariate spline algo rithms cart mars id statistical framework 
statistical framework motivates variance decreasing features hme approach soft aries 
statistical approach provides uni fied framework handling variety data types including binary variables ordered unordered categorical variables real variables input output 
maximum allows standard tools statistical theory brought bear developing inference pro fitting procedures measures uncer tainty architecture 
opens door bayesian approaches useful context unsupervised mixture model estimation 
project supported mcdonnell pew foundation atr auditory visual perception research laboratories siemens iri na tional science foundation office naval research 
michael 
jordan nsf presidential young investigator 
breiman friedman olshen stone 

sion trees 
belmont ca wadsworth international group 
bridle 

probabilistic interpretation feedforward classification network outputs relationships statistical pattern recognition 
fogelman soulie eds neuro computing applica 
new york springer verlag 
cheeseman kelly self stutz taylor freeman 

autoclass bayesian classification system 
fifth international conference machine learn ann arbor mi 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal 
friedman 

multivariate adaptive regression splines 
annals 
jacobs jordan nowlan hinton 

adaptive mixtures local experts 
neural computation 
jordan jacobs 

hierar adaptive experts 
moody hanson lippmann eds advances neural informa tion systems 
san mateo ca morgan kaufmann 
jordan jacobs 

cal experts em 
com putational cognitive science tech 
rep mit cambridge ma 
jordan xu 

convergence re sults em approach experts ar 
computational cognitive science tech 
rep mit cambridge ma 
lo ljung 

theory practice 
cambridge mit press 
ll mccullagh nelder 

general models 
london chapman hall 
quinlan 

induction decision trees 

