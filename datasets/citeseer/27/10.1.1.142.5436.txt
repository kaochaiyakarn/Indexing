schaal 

learning demonstration 
mozer jordan petsche eds advances neural information processing systems pp 
cambridge ma mit press learning demonstration stefan schaal cc gatech edu www cc gatech edu fac stefan schaal college computing georgia tech atlantic drive atlanta ga atr human information processing cho soraku gun kyoto widely accepted learning task scratch prior knowledge daunting undertaking 
humans rarely attempt learn scratch 
extract initial biases strategies approach learning problem instructions demonstrations humans 
learning control investigates learning demonstration applied context reinforcement learning 
consider priming function value function policy model task dynamics possible areas demonstrations speed learning 
general nonlinear learning problems model reinforcement learning shows significant speed demonstration special case linear quadratic regulator lqr problems methods profit demonstration 
implementation pole balancing complex anthropomorphic robot arm demonstrate facing complexities real signal processing model reinforcement learning offers robustness lqr problems 
suggested methods robot learns pole balancing just single trial second long demonstration human instructor 

inductive supervised learning methods reached high level sophistication 
data set prior information nature host algorithms exist extract structure data minimizing error criterion 
learning control learning task defined 
goal learn policy appropriate actions response perceived state order steer dynamical system accomplish task 
task usually described terms optimizing arbitrary performance index direct training data exist learn controller supervised way 
worse performance index may defined long term behavior task problem temporal credit assignment arises credit blame actions past current performance 
setting typical reinforcement learning learning task scratch require prohibitively timeconsuming amount exploration state action space order find policy 
hand learning prior knowledge approach rarely taken human animal learning 
knowledge approach new task transferred previously learned tasks extracted performance teacher 
opens questions learning control profit kinds information order accomplish new task quickly 
focus learning demonstration 
learning demonstration known programming demonstration imitation learning teaching showing received significant attention automatic robot assembly years 
goal replace time consuming manual pro mg ml sin log cos nm max max define mg mlx cos ml ml cos ml sin xx kg diag define xu qx ru pendulum swing cart pole balancing gramming robot automatic programming process solely driven showing robot assembly task expert 
concert main stream artificial intelligence time research driven symbolic approaches expert demonstration segmented primitive assembly actions spatial relationships manipulator environment subsequently submitted symbolic reasoning processes lozano perez latombe dejong 
approaches programming demonstration started include inductive learning components ikeuchi kaiser 
context human skill learning teaching showing investigated kawato wada miyamoto 
complex manipulation task learned anthropomorphic robot arm 
overview projects bakker kuniyoshi 
focus lies reinforcement learning learning demonstration beneficial context 
divide reinforcement learning categories reinforcement learning nonlinear tasks section approximately linear tasks section investigate methods learning learning model reinforcement learning profit data demonstration 
section example task pole balancing placed context actual anthropomorphic robot learn reconsider applicability learning demonstration complex situation 

reinforcement learning demonstration example tasks basis investigation learning demonstration 
nonlinear task pendulum swing limited torque atkeson doya shown 
goal balance pendulum upright position starting hanging downward 
maximal torque available restricted pendulum supported gravity states pumping trajectory necessary similar mountain car example moore timing building momentum pumping overshoot upright position 
approximately linear example known cart pole balancing problem widrow smith barto sutton anderson 
tasks learner information step reward tasks formulated continuous state continuous action problems 
goal task find policy minimizes infinite horizon discounted reward ds left hand equation continuous time formulation right hand equation corresponding discrete time version denote dimensional state vector dimensional command vector respectively 
swing assume teacher provided successful trials starting dif ferent initial conditions 
trial consists time series data vectors sampled hz 
cart pole second demonstration successful balancing represented hz time series data vectors xx demonstrations speed reinforcement learning 
nonlinear task swing applied reinforcement learning learning value function function dyer swing task alternative method learning watkins received limited research continuous state action spaces 
function assigns scalar reward value state entire function fulfills consistency equation arg min clarity equation discrete state action system continuous formulation doya 
optimal policy chooses action state fulfilled 
note computation involves optimization step includes knowledge subsequent state 
requires model dynamics controlled system 
viewpoint learning demonstration function learning offers candidates primed demonstration value function policy model 
scratch primed model primed actor primed actor model trial smoothed learning curves average learning trials learning conditions see text 
performance characterized value system usually able swing properly know upright position 
learning order assess benefits demonstration swing implemented learning suggested doya continuous td ctd learning algorithm 
function dynamics model incrementally learned nonlinear function approximator receptive field weighted regression rfwr schaal atkeson 
differing doya implementation optimal action suggested ctd learn model policy actor barto 
represented rfwr 
learning conditions tested empirically scratch trial trial learning value function model actor scratch 
primed actor initial training demonstration trial trial learning 
primed model initial training demonstration trial trial learning 
primed actor model priming trial trial learning 
shows results learning swing 
trial lasted seconds 
time tup pole spent interval trial taken performance measure doya 
comparing conditions results demonstrate learning pole model demonstration speed learning 
surprising learning function significantly complicated learning model learning process dominated function learning 
interestingly priming actor demonstration significant effect initial performance condition vs 
system knew right away pump pendulum order learn balance pendulum upright position took amount time learning scratch 
behavior due fact theoretically function approximated correctly entire state action space explored densely 
demonstration covered large fraction entire state space expect learning profit 
investigated demonstration prime function combination functions 
results qualitatively shown policy included priming learning traces 
totally surprising 
approximating function just supervised learning requires iterative procedure ensure validity amounts complicated nonstationary function approximation process 
limited amount data demonstration generally approximate value function 
model learning learning model required powerful 
certainty equivalence principle substitute real world planning run mental simulations interaction real world 
reinforcement learning idea scratch primed model nally pursued sutton dyna algorithms discrete state action spaces 
trial explore far con smoothed learning curves average learning trials learning conditions see text swing problem mental simulations 
see explanations interpret graph 
version dyna dyna ctd help learning demonstration 
difference compared ctd section real trial dyna ctd performs mental trials model dynamics acquired far replaces actual pole dynamics 
learning conditions explored scratch trial trial learning model policy scratch 
primed model initial training demonstration trial trial learning 
demonstrates contrast learning previous section learning demonstration significant difference demonstration takes trials accomplish swing stable balancing indicated tup 
note learning scratch significantly faster 
linear task cart pole balancing argue applying reinforcement learning demonstration swing task premature reinforcement learning nonlinear function approximators obtain appropriate scientific understanding 
section turn easier task cart pole balancer 
task approximately linear pole started close upright position problem studied dynamic programming literature context linear quadratic regulation lqr dyer 
learning contrast learning learning watkins singh sutton learns complicated value function depends state command 
analogue consistency equation learning arg min state picking action minimizes optimal action reward function 
advantage evaluating function find optimal policy require model dynamical system controlled value step reward needed 
learning demonstration priming function policy candidates speed learning 
lqr problems bradtke suggested learning method ideally suited learning demonstration extracting policy 
observed lqr function quadratic states commands xu linear policy represented gain matrix extracted demo final kx conversely stabilizing initial policy current function approxi mated recursive squares procedure optimized policy iteration process guaranteed convergence 
demonstration allows extract initial policy linearly regressing time observed command corre typical learning curve noisy simulation cart pole balancer sponding observed states shot learning learning 
graph shows value pole balancing achievable 
shown step reward time seconds policy learning trial 
pole dropped 
eration steps policy basically indistinguishable optimal policy 
caveat learning learn stabilizing initial policy 
model learning learning lqr task learning function classic forms dynamic programming dyer 
stabilizing initial policy current function approximated recursive squares analogy bradtke 
similarly linear model demo cart pole dynamics extracted demonstration linear regression cart pole state vs previous state command vector model refined new data point experienced learning 
policy update hb ha hx ab similar process bradtke find optimal policy system accomplishes shot learning qualitatively indistinguishable 
pointed section efficient learned model performing mental simulations 
model demo policy calculated line policy iteration initial estimate taken identity matrix dyer 
initial stabilizing policy required estimate task dynamics 
method achieves shot learning 
step reward pole balancing actual robot result previous section real performance differences learning learning model learning lqr problems 
explore usefulness methods realistic framework implemented learning demonstration pole balancing anthropomorphic robot arm 
robot equipped hz video stereo vision 
pole marked color blobs tracked real time 
second long demonstration pole balancing provided human standing front robot cameras 
crucial differences comparison simulations 
demonstration vision kinematic variables extracted demonstration 
second visual signal processing ms time delay 
third command robot executed high accuracy due unknown nonlinearities robot 
lastly humans internal state pole balancing policy partially non observable variables 
issues impact kinematic variables implementation robot arm replaces cart cart pole problem 
estimate inverse dynamics inverse kinematics arm acceleration finger cartesian space command input task 
arm heavier pole allows neglect interaction forces pole exerts arm 
pole balancing dynamics reformulated uml cos ml sin sketch anthropomorphic robot arm variables equation extracted demonstration 
omit extension equations 
delayed visual information possibilities dealing delayed variables 
state system augmented delayed commands corresponding delay time xx ut ut ut state predictive controller employed 
method increases complexity policy significantly method requires model 
inaccuracies command execution acceleration command robot execute close exactly 
learning function includes dynamics model dangerous mapping xx contaminated nonlinear dynamics robot arm 
turned learn model reliably 
remedied observing command extracting visual feedback 
internal state demonstrated policy investigations human subjects shown humans internal state pole balancing 
policy observed easily anymore claimed section regression analysis extracting policy teacher find appropriate time alignment observed current state command past 
numerically involved process regressing policy delayed commands endangered singular regression matrices 
consequently easily happens extracts policy demonstration prevents application learning learning described section 
result considerations trustworthy item extract demonstration model pole dynamics 
implementation ways calculating policy state predictive control kalman filter overcome delays visual information processing 
model learned incrementally real time implementation rfwr schaal atkeson 
shows results learning scratch learning demonstration actual robot 
demonstration took trials learning succeeded reliable performance longer minute 
second long demonstration learning reliably accomplished single trial large variety physically different poles demonstrations arbitrary people laboratory 
scratch primed model trial smoothed average learning curves robot pole balancing 
trials aborted successful balancing seconds 
tested long term performance learning system running pole balancing hour pole dropped 

discussed learning demonstration context reinforcement learning focusing learning value function learning model reinforcement learning 
learning value function learning theoretically profit demonstration extracting policy demonstration data prime value function case value function learning extracting predictive model world 
special case lqr problems find significant benefit priming learner demonstration 
contrast model reinforcement learning able greatly profit demonstration predictive model world mental simulations 
implementation anthropomorphic robot arm illustrated lqr problems model reinforcement learning offers larger robustness complexity real learning systems learning value function learning 
model strategy robot learned pole balancing demonstration single trial great reliability 
important message learning approach equally suited allow knowledge transfer incorporation biases 
issue may serve critical additional constraint evaluate artificial biological models learning 
acknowledgments support provided atr human information processing labs german research association alexander humboldt foundation german scholarship foundation 
atkeson 

local trajectory optimizers speed global optimization dynamic programming 
moody hanson lippmann ed adv 
neural inf 
proc 
sys 

morgan kaufmann 
bakker kuniyoshi 

robot see robot overview robot imitation autonomous systems section electrotechnical laboratory tsukuba science city japan 
barto sutton anderson 

neuronlike adaptive elements solve difficult learning control problems 
ieee transactions systems man cybernetics smc 
bradtke 

reinforcement learning applied linear quadratic regulation 
hanson cowan giles 
eds advances neural inf 
processing systems pp 
morgan kaufmann 
kaiser 

acquisition elementary robot skills human demonstration 
international symposium intelligent robotic systems pisa italy 
doya 

temporal difference learning continuous time space 
touretzky mozer hasselmo 
eds advances neural information processing systems 
mit press 
latombe 

approach automatic robot programming inductive learning 
brady paul 
eds robotics research pp 
cambridge ma mit press 
dyer 

computation theory control 
ny academic press 
ikeuchi 

assembly plan observation school computer science carnegie mellon university pittsburgh pa kawato wada 

teaching showing optimization principle 
proceedings international conference artificial neural networks icann pp 
lozano perez 

task planning 
brady hollerbach johnson lozano rez mason 
eds pp 
mit press 
miyamoto schaal koike osu nakano wada kawato 
press 
learning robot bi directional theory 
neural networks 
moore 

fast robust adaptive control learning forward models 
moody hanson lippmann 
eds advances neural inf 
proc 
systems 
morgan kaufmann 
schaal atkeson 

isolation cooperation alternative system experts 
touretzky mozer hasselmo 
eds advances neural information processing systems 
cambridge ma mit press 
dejong 

explanation manipulator learning acquisition planning ability observation 
conference robotics automation pp 
singh sutton 

reinforcement learning eligibility traces 
machine learning 
sutton 

integrated architectures learning planning reacting approximating dynamic programming 
proceedings international machine learning conference 
watkins 

learning delayed rewards 
ph thesis cambridge university uk widrow smith 

pattern recognizing control systems 
comp 
inf 
sciences coins symp 
proc washington spartan 

