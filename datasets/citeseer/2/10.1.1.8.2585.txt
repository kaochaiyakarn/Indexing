bayesian statistics pp 
bernardo berger dawid heckerman smith west eds 
oxford university press hierarchical bayesian models applications information retrieval david blei michael jordan andrew ng university california berkeley usa blei cs berkeley edu jordan cs berkeley edu ang cs berkeley edu summary simple hierarchical bayesian approach modeling collections texts large scale data collections 
text collections posit document generated choosing random set multinomial probabilities set possible topics repeatedly generating words sampling topic mixture 
model intractable exact probabilistic inference approximate posterior probabilities marginal likelihoods obtained fast variational methods 
extensions coupled models joint text image data multiresolution models topic hierarchies 
keywords variational inference methods hierarchical bayesian models empirical bayes latent variable models information retrieval 

field information retrieval broadly concerned problem organizing collections documents media support various information requests part users 
familiar problem returning subset documents response query lies scope information retrieval problems analyzing cross referencing document collection analyzing linguistic structure able say automatic compilation document summaries automatic translation 
class problems involve analyzing interactions users collection including collaborative filtering problem suggestions new users system choices previous users 
clearly mill bayesian statistics information retrieval problem 
view information retrieval system uncertain needs user uncertainty reduced ongoing learning process dialog user population users 
modeling issues particularly surrounding appropriate level resolution view document collection tools hierarchical bayesian modeling clearly appropriate 
problem formulating response query viewed bayesian terms model conditional distribution queries documents conjunction model documents treat problem responding query application bayes theorem zhai lafferty 
problems information retrieval involve preferences choices decision theoretic analysis needed manage complexity tradeoffs arise 
despite natural motivations generally case current information retrieval systems built foundation probabilistic modeling inference 
blei jordan ng important reason severe computational constraints systems 
collections involving tens hundreds thousands documents commonplace methods information retrieval systems reduction document vector smoothed frequency counts possibly followed singular value decomposition reduce dimensionality computations inner products vectors important virtue computational efficiency 
probabilistic modeling displace augment methods done major increase computational load questions done current arsenal bayesian tools 
user sent query search engine generally willing wait markov chain monte carlo simulation converge 
current discuss class hierarchical bayesian models information retrieval 
simple models rich yield intractable posterior distributions maintain computational efficiency variational inference methods 
variational methods yield deterministic approximations likelihoods posterior distributions provide alternative markov chain monte carlo 
particularly apt domain information retrieval fast approximate answer generally useful slow answer greater fidelity 
models discuss instances called bag words models baeza yates ribeiro neto 
viewing words document random variables simply models words exchangeable 
clearly drastic simplification assumption generally deemed necessary information retrieval computational constraints practice viable information retrieval systems built assumption 
case building sufficiently detailed model mixture underlying word distribution hope ameliorate effect exchangeability assumption capture latent structure documents maintaining computational tractability 
believe probabilistic methods important role play information retrieval full bayesian computations precluded size problems domain 
significant empirical bayesian techniques particular fixing hyperparameters maximum likelihood 
comfort provided regard large scale problems study important acknowledge models study inaccurate reflections underlying linguistic reality 

latent variable models text document collection envision number underlying topics example collection may contain documents novels music poetry 
topics may viewed varying levels resolution documents romance novels jazz music russian poetry 
traditional approach treating topics hierarchical clustering represents document vector word counts defines similarity measure vectors word counts applies hierarchical clustering procedure hopes characterizing topics significant problem approach mutual exclusivity assumption document belong single cluster 
textual material tends resist mutual exclusivity words different unrelated meanings documents relevant different topics document relevant jazz music russian poetry 
clustering procedures computational concerns lead aim form divide conquer strategy care taken define clustering procedures suitably flexible 
bayesian information retrieval provide requisite flexibility propose hierarchical bayesian approach 
basic scheme mixture model corresponding exchangeable distribution words 
mixture basic levels 
level finite mixture mixture components viewed representations topics second level latent dirichlet variable provides random set mixing proportions underlying finite mixture 
dirichlet variable viewed representation documents document modeled collection topic probabilities 
dirichlet sampled document finite mixture sampled repeatedly word document 
components selected sampling process multiset viewed bag topics characterization document 
describe basic model detail section treat various extensions remainder 

latent dirichlet allocation basic entity model word take multinomial random variable ranging integers vocabulary size constant 
represent random variable vector components wi component equal 
document sequence words denoted wn nth word 
assume data corpus documents wd 
refer model latent dirichlet allocation lda model 
model assumes document corpus generated follows choose 
choose di 
words wn choose topic zn mult 
choose word wn wn zn multinomial probability conditioned topic zn 
simplifying assumptions basic model remove subsequent sections 
dimensionality dirichlet distribution topic variable assumed known fixed 
second word probabilities parameterized matrix ij wj zi treat fixed quantity estimated 
note left document length distribution unspecified applications conditionally remainder generally omit generative process joint distribution topic mixture topics word document zn wn zn zn simply unique 
model illustrated left 
note distinction lda simple dirichlet multinomial clustering model 
simple clustering model innermost plate contain topic node sampled document dirichlet sampled collection 
lda dirichlet sampled document multinomial topic node sampled repeatedly document 
blei jordan ng 
inference consider problem computing posterior distribution latent variables document drop randomness simplicity find denominator marginal likelihood marginalizing latent variables equation ij wj 
expectation extension dirichlet distribution represented special hypergeometric functions jiang kadane 
unfortunately function infeasible compute exactly due coupling inside summation latent factors 

left graphical model representation lda 
box plate representing replicates 
right graphical model representation variational distribution approximate posterior lda 
note treating parameters fixed constants eq 
show estimate values empirical bayes procedure section 
section consider fuller bayesian model endowed dirichlet distribution 
approximate posterior probability computationally efficient manner variational inference algorithms jordan 
variational inference related importance sampling simplified distribution approximate posterior 
sampling distribution consider family simplified distributions parameterized set variational parameters 
ranging parameters find best approximation family measuring approximation quality terms kl divergence 
essentially convert inference problem problem computing integral optimization problem problem maximizing function 
context graphical models simplified distributions provide approximations variational inference framework generally obtained omitting edges graph 
decouples variables provides tractable approximation posterior 
left illustrates lda model corpus documents 
graph problematic coupling represented arc develop variational approximation defining approximating family distributions choose variational parameters yield tight approximation true posterior 
bayesian information retrieval particular define factorized variational distribution zn illustrated right 
distribution variational dirichlet parameters variational multinomial parameters note parameters conditioned document different set dirichlet multinomial variational parameters 
new model hand obtain approximation minimization kl divergence variational distribution true posterior arg min 
show section take decreasing steps kl divergence converge locally optimizing parameters alternating pair update equations ni exp eq log ni closed form expression expectation eq 
section 
note obtain variational parameters document eqs 
appealing intuitive interpretation 
dirichlet update eq 
yields posterior dirichlet expected observations taken variational distribution 
multinomial update eq 
akin bayes theorem zn wn wn zn zn zn approximated exponential expected value log variational distribution 
iteration algorithm requires nk operations 
empirically find number iterations required single document scales linearly number words document 
yields total number operations scales empirically 
variational inference section derive variational inference equations eq 
eq 

noting parameters log natural parameters exponential family representation dirichlet distribution expected value ki log log digamma function 
derive variational inference algorithm bounding marginal likelihood document jensen inequality jordan log log log log log eq log eq log 
blei jordan ng straightforward show difference left hand side right hand side equation precisely kl divergence eq 
minimizing kl divergence equivalent maximizing lower bound marginal likelihood eq 

letting denote right hand side eq 
eq log eq log eq log eq log eq log 
write eq 
terms model parameters variational parameters 
lines expands terms kj log ni ni log ij kj log log kj log kj kj ni log ni 
sections take derivatives respect expression obtain variational inference algorithm 
variational multinomial 
maximize eq 
respect ni probability nth word generated latent topic observe constrained maximization ki ni 
form lagrangian isolating terms contain ni adding appropriate lagrange multipliers 
iv refer wv zi appropriate recall wn vector exactly component equal select unique wv 
ni ni kj derivatives respect ni obtain ni ni log iv ni log ni kj log iv log ni setting derivative zero yields maximized ni cf 
eq 
ni iv exp kj kj nj 
bayesian information retrieval variational dirichlet 
maximize eq 
respect th component posterior dirichlet parameter 
terms containing kj log simplifies kj log ni kj kj kj kj ni log log 
take derivative respect ni kj setting equation zero yields maximum nj 
ni 
eq 
depends variational multinomial full variational inference requires alternating eqs 
bound converges 
estimation section discuss approximate maximum likelihood estimation parameters 
corpus documents wm variational em algorithm em variational step find parameters maximize lower bound log marginal likelihood log wd 
described quantity computed efficiently 
bound log likelihood wd wd wd exhibits lower bound kl term positive 
obtain variational em algorithm repeats steps eq 
converges find setting variational parameters tighten bound eq 
possible 
simply variational inference training document described previous section 
maximize eq 
respect model parameters 
corresponds finding maximum likelihood estimates approximate expected sufficient statistics computed step 
blei jordan ng maximize respect isolate terms add lagrange multipliers ij log ij take derivative respect ij set zero find terms contain kj log ij log derivative respect gives kj vj ij kj di dj kj di dj coupling derivatives different newton raphson find maximal 
hessian form kj form allows exploit matrix inversion lemma obtain newton raphson algorithm requires linear number operations 

example illustrate lda works examining variational posterior parameters document trec ap corpus harman 
recall nj approximation posterior probability associated ith topic nth word 
examining maxi ni obtain proposed allocation words unobserved topics 
furthermore interpret ith dirichlet parameter maximized eq 
ith dirichlet parameter model plus expected number instances topic seen document 
subtracting posterior dirichlet parameters model dirichlet parameters obtain indication degree factor document 
trained factor lda model subset trec ap corpus 
article collection train william randolph hearst foundation give lincoln center metropolitan opera new york school 
board felt real opportunity mark performing arts act bit important traditional areas support health medical research education social services hearst foundation president randolph hearst said monday announcing 
lincoln center share new building house young artists provide new public facilities 
metropolitan opera new york receive 
school music performing arts taught get 
hearst foundation leading supporter lincoln center consolidated corporate fund usual annual donation 
bayesian information retrieval 
factors largest expected counts article ap corpus 
show fifteen words largest probability factors 
examine article find factors close factors achieve significant expected counts 
looking distribution words factors identify topics mixed random variable form document 

smoothing lda large vocabulary size characteristic information retrieval problems creates serious problems sparsity 
new document contain words appear documents training corpus 
maximum likelihood estimates multinomial parameters assign zero probability words zero probability new documents 
standard approach coping problem smooth multinomial parameters assigning positive probability vocabulary items observed training set jelinek 
laplace smoothing commonly corresponds placing uniform dirichlet prior multinomial parameters 
implemented practice nigam simple laplace smoothing correspond formal integration mixture model setting :10.1.1.43.7517
fact exact integration intractable mixtures reasons exact inference intractable lda model 
utilize variational framework approximate posterior dirichlet data 
variational approximation remainder section 
elaborate basic lda model place dirichlet priors parameters multinomial probabilities zi words topics 
making exchangeability assumption di 
probability data wd simply lda model described 
integral intractable lower bound log probability variational distribution log eq log eq log wd blei jordan ng variational distribution takes form di qd zd wd 
note variational parameter vector scalar qd variational distribution lda defined 
variational inference chooses values variational parameters minimize kl divergence variational posterior true posterior 
derivation similar earlier derivation yields updates ij di exp log iv exp log eq 
refers unique index dn 
estimate values hyperparameters maximize lower bound log likelihood respect expected sufficient statistics taken variational distribution 
maximization 
calculate derivatives log eq log ij kv kv log kv kv maximize newton method 
compute probability previously unseen document form variational lower bound log eq new eq eq new 
optimizing parameters term exactly computed empirical bayes parameter estimation procedure 
optimizing parameters second term simply iterating eqs 
data new document 

empirical results section empirical evaluation lda benchmark cran corpus van rijsbergen croft containing medical abstracts unique terms subset benchmark trec ap corpus containing newswire articles unique terms 
cases held data test purposes trained models remaining 
note preprocessing data removed standard list words 
experimental details provided blei 
compared lda standard models unigram mixture unigrams plsi unigram model simply single multinomial words irrespective bayesian information retrieval document 
mixture unigrams finite mixture multinomials nigam :10.1.1.43.7517
probabilistic latent semantic indexing plsi model precursor lda dirichlet distribution lda replaced list multinomial probability vectors document corpus 
parameterized model tempering heuristic practice smooth maximum likelihood solution 
fit latent variable models em variational em lda exactly stopping criteria average change expected log marginal likelihood 
evaluate predictive performance methods computed perplexity held test set 
perplexity convention language modeling monotonically decreasing likelihood test data thought inverse word likelihood 
perplexity lda plsi plsi temper unigrams unigram number topics perplexity lda plsi plsi temper unigrams unigram number topics 
perplexity results cran left ap right corpora lda plsi mixture unigrams unigram model 
unigram higher dotted line mixture unigrams lower dotted line plsi dashed tempered plsi dash dot lda solid 
illustrates perplexity model corpora different values latent variable models generally better simple unigram model 
plsi model severely overfits tempered values graph manages outperform mixture unigrams tempered 
lda performs consistently better models 
blei 
additional experiments comparing lda related mixture models domains text classification collaborative filtering 

extensions lda model best viewed simple hierarchical module elaborated obtain complex families models increasingly demanding tasks information retrieval 
applications corpora involving sets images annotated text studied model refer corr lda consisting coupled lda models blei jordan 
shown model kinds observables words image blobs latent topics kinds variables 
briefly dirichlet variable parameterize mixing proportions latent topic variable images 
correspondence topic variables images words enforced explicit translational conditional probability 
yields model associate particular words particular regions image 
show annotations comparing corr lda gm mixture joint mixture model words images gm lda pair blei jordan ng 
corr lda model 
gaussian random variable encodes image blobs encodes words 
variables latent topics images words respectively 
note translational conditional probability link enforces correspondence image topics word topics 
lda models translational conditional 
suggested anecdotally annotations substantiated quantitatively blei jordan corr lda model successful annotator models studied 
corr lda tree light sunset water sky gm mixture close tree people mushrooms gm lda water sky tree people grass corr lda 
people tree 
sky jet 
sky clouds 
sky mountain 
plane jet 
plane jet corr lda tree water grass flowers birds gm mixture tree water grass sky field gm lda water sky tree people grass gm lda 
hotel water 
plane jet 
penguin 
plane jet 
water sky 
boats water 
top example images automatic annotations different models 
bottom segmented image labeling 
important extension lda involves attempt capture notion resolution documents discuss music broad sense specialize talk classical music specialize talk chamber music earlier discussion want impose mutual exclusivity assumption levels resolution want allow single document able utilize different levels different times 
words document may viewed associated specific topics words viewed generic 
elaborating lda model obtain multiresolution model consider tree nodes correspond latent topics 
generative process generating document chooses path tree chooses level path selects topic corresponding nonterminal node 
process documents tend share topics basic lda model particularly share topics high topic hierarchy 
hierarchy provides flexible way share strength documents 
bayesian information retrieval pair lda multiresolution lda model intractable exact inference variational approximations readily developed cases 

related lda model hierarchical extension inspired probabilistic latent semantic indexing plsi model hoffman 
plsi model assumes mixture model words document replaces dirichlet lda list multinomial probability vectors document corpus 
viewed highly nonparametric version lda 
plsi suffers overfitting problems unable assign probability mass documents outside training corpus 
lda closely related bayesian approach mixture modeling developed diebolt robert dirichlet multinomial allocation dma model green richardson 
dma model posits dirichlet prior set mixing proportions prior parameters associated mixture component 
mixing proportions component parameters drawn 
data point assumed generated drawing mixture component drawing value corresponding parameters 
main differences lda dma arise role dirichlet random variable representation documents lda setting 
lda draw set mixing proportions multiple times dirichlet dma mixing proportions drawn 
dma new data points assumed drawn single mixture component lda new data points collections words word allocated independently drawn mixture component 

discussion simple hierarchical approach modeling text corpora largescale collections 
model posits document generated choosing random set multinomial probabilities set possible topics repeatedly generating words sampling topic mixture 
important virtue model document characterized topics avoid mutual exclusivity assumption clustering models 
limitation current lda assume number topics user defined parameter 
empirical shown lack sensitivity parameter clearly interest study inference methods variational mcmc see attias green richardson allow parameter inferred data 
focused applications information retrieval problems modeling collections sequences discrete data arise areas notably bioinformatics 
clustering methods analogous information retrieval usefully employed bioinformatics mutual exclusivity assumption underlying methods particularly unappealing biological setting lda style models topics play useful role problems 
acknowledge support nsf iis onr muri 
blei jordan ng attias 

variational bayesian framework graphical models 
advances neural information processing systems solla leen 
mueller eds 
cambridge mit press 
baeza yates ribeiro neto 

modern information retrieval 
new york acm press 
blei jordan 

modeling annotated data 
tech 
rep university california berkeley usa 
blei jordan ng 


latent dirichlet allocation 
journal machine learning research submitted 


multiple hypergeometric functions probabilistic interpretations statistical uses 
amer 
statist 
assoc 

jiang kadane 

bayesian methods censored categorical data 
amer 
statist 
assoc 

diebolt robert 

estimation finite mixture distributions bayesian sampling 
roy 
statist 
soc 

green richardson 

modelling heterogeneity dirichlet process 
scandinavian statist 

harman 

overview text retrieval conference trec 
proceedings text retrieval conference harman ed 
nist special publication 
hofmann 

probabilistic latent semantic indexing 
proceedings nd 
annual international sigir conference hearst tong eds 
new york acm press 
jelinek 

statistical methods speech recognition 
cambridge ma mit press 
jordan ghahramani jaakkola saul 

variational methods graphical models 
machine learning 
nigam lafferty mccallum 

maximum entropy text classification 
ijcai workshop machine learning information filtering joachims mccallum sahami ungar eds 
san mateo ca morgan kaufmann 


maximum likelihood estimation dirichlet distributions 
statist 
computation simulation 
van rijsbergen croft 

document clustering evaluation experiments cranfield collection 
information processing management 
zhai lafferty 

document language models query models risk minimization information retrieval 
sigir conference research development information retrieval croft harper kraft zobel eds 
new york acm press 
discussion steven maceachern ohio state university usa blei jordan ng done favor bayesian statistical community introducing variational inference paradigm illustrating benefits modelling documents 
illustration talk technique works description pictures terms small set words impressive 
suspect paradigm standard methods bayesian analysis problems speed computation essential 
comments focus issues overview authors modelling strategy recommendation call directional assessment adjustment bayesian analysis 
look forward authors views directional adjustment appropriate context adjustment generally appropriate variational inference adjustment compatible variational inference 
authors implement step strategy analysis choose simple quickly computed statistics monitor document summarized word counts second develop simple model generates statistics multiple topics document model conditionally independent choice words drawn bag words bayesian information retrieval topic third approximate fit simple model variational approximation 
strategy writing simplified model fitting exactly approximation bayesian inference 
applications provide qualitative critique model 
simple model retained analysis simplifies computation specification structure prior distribution complex model difficult accepted 
great strength lies focus computational algorithms scale variational approximation fit model 
paragraphs examine strategy simplified context 
illustration contrast simple complex models consider hierarchical models middle stage conditionally normal component stationary ar component 
models induce marginal distribution yi 
context larger model fairly large sample sizes essentially learn marginal distribution yi joint distribution discovered simple analysis 
simple model leads inference model yi complex model replaces distribution dependence structure yielding sampling distribution normal models posterior distribution closed form cases 
simple model distribution mean variance complex model mean variance 
posterior distributions models reflect difference forever ratio posterior variances tending 
tied particulars simple complex models 
general feature appears loosely speaking exhibit positive dependence 
qualitative assessment differences simple model complex model may lead conclude actual distribution statistic bayesian update larger spread distribution formal calculation 
bayesians compelled consider assessment model judge complex posterior distribution lie particular direction adjust formally calculated posterior distribution 
number different perspectives generate directional adjustments 
popular approach adjustment flatten likelihood raising fractional power inflating variance statistic 
context normal theory models approaches coincide 
broader context exponential family adjustment viewed replacing actual sample size smaller effective sample size retaining values sufficient statistics 
early information processing zellner describes adjustment ibrahim chen describe means prior experiments 
long tradition survey sampling directional adjustment 
complex survey administered calculation standard error estimate difficult 
design standard error accounts features sampling design stratification cluster sampling 
information needed calculation standard error unavailable analyst due confidentiality constraints 
surveys standard errors larger calculated treating data simple random sample 
difficulties led notion design effect standard error 
design effect may laboriously calculated number estimators account entire sample blei jordan ng design 
design effect applied inflate standard errors estimators 
notion selecting adjustment creating distribution adjustments ported bayesian statistics device fractional likelihood 
analyst elicits means fraction adjustment posterior sample size 
consider directional assessment latent dirichlet model applied information retrieval problem 
simple model designed word counts documents 
step look features problem suggest word counts relative behavior expected simple model 
examine context multinomial distribution match distributions cell means 
number words document times cell probabilities 
naturally produced hierarchical structure vector cell probabilities drawn distribution words conditionally independent draws cell probabilities 
naturally obtained stratification individual words drawn multinomials differing vectors cell probabilities 
stratification reduce dispersion different vectors cell probabilities fixed proportion 
turning documents main features suggest 
language individual construct 
different individuals writing collection topics consistently different word choices 
effect strong attribute authorship documents individuals 
mosteller wallace examination federalist papers relies strategy 
second feature suggests dynamic changing nature language 
new words enter language check merriam webster lists word bayesian dating 
old words disappear usage frequency changes 
word provides information dating documents 
collections documents written extended period time changes language lead 
context scientific documents authors writing different backgrounds different vocabulary describe concepts 
certain features documents suggest movement 
main effects direction tied structure written language 
effects appear scale entire documents paragraphs sentences 
level document classic training writing essay suggests introduce topic put guts essay wrap things 
word choice presumably different parts document 
indicates presence stratification leads direction 
mid level paragraph generally begins topic sentence details filling remaining sentences 
differential word choice initial remaining sentences paragraph suggests stratification 
lowest level english believe languages tends place main information content sentence 
evidence stratification 
grammatical structure language need nouns verbs adjectives suggests stratification 
experience presence sorts effects move move common 
complete directional assessment judge relative sizes competing effects 
impression document context individual word choice provide far strongest effect leading net 
mind belief exact analysis simple model produce likelihoods sharp leading posterior distribution concentrated 
step directional assessment latent dirichlet analysis judge bayesian information retrieval impact variational approximation posterior analysis 
intuition weaker worked approximation simple cases 
cases examined approximation systematically results distribution concentrated distribution approximated 
difference tied asymmetry kullback liebler divergence 
variational approximation reverses roles usual true near distribution appear asymptotic theory 
taken full model variational approximation simple model suggest adjustment posterior distribution appropriate 
directional adjustment suggests flattening part approximate posterior 
systematic examination problems suggest flattening appropriate 

material supported national science foundation award 
dms 
reply discussion agree steven maceachern important issue line research important issue field probabilistic information retrieval 
distinct reasons posterior obtain overly concentrated respect distributions words attempt model 
model exceedingly simple ignoring linguistic phenomena tend yield larger variability model accounts particular sequential phenomena referred maceachern 
second empirical bayes methodology known yield underestimates posterior variance 
third variational approach utilize inference tends yield approximating distributions overly concentrated 
briefly discuss phenomena outline possible solutions 
exchangeability assumptions underlying lda aimed computational simplicity clearly overly strong 
general goal field information retrieval relax bag words assumptions develop models closer linguistic reality 
view hierarchical bayesian methodology providing natural upgrade path 
introducing latent variables linguistic concepts sense style capture sources variability currently outside model 
particular concept naturally introduced discrete multinomial conditions dirichlet lda yielding mixture dirichlet distributions topic simplex allowing move restrictive assumption corpus captured single dirichlet topic space 
introduce markovian structure example topic variable word conditioned topic variable preceding word 
alternatively consider dirichlet multinomial mixtures subsequences words grams single words 
information available parsing algorithms provide conditioning variables lda model capturing stratification discussed maceachern lead 
cases easy specify natural extensions lda hierarchical bayesian formalism computational issues major concern 
model aimed applications information retrieval scale tens hundreds thousands documents inferential procedures run seconds 
despite large scale problems information retrieval data sparse 
discussed section document word matrices tend sparse new document contain words seen training corpus 
problem subject intense study inspired theoretical species sampling blei jordan ng models generally studied frequentist framework chen goodman 
variational smoothing method outlined section approximate bayesian solution problem 
discussion section provided example advantage moving empirical bayes inference method fuller hierarchical bayesian approach parameter find computational considerations weigh favor simplicity offered empirical bayes approach cf 
parameter lda model 
expect considerations increasing importance consider richer complex hierarchical bayesian models expect empirical bayes continue play important role line research 
regard important note known fact empirical bayes leads underestimates posterior variance carlin louis 
context reduction variability may particularly problematic 
corrections discussed empirical bayes literature kass may provide relief 
convexity variational techniques section known overly concentrated relative posterior distribution approximate 
difficulty addressed number ways 
approach involves higher order approximations kappen general methodology converting variational lower bounds higher order variational bounds 
possible achieve higher accuracy dispensing requirement maintaining bound minka lafferty shown improved inferential accuracy obtained lda model higher order variational technique known expectation propagation general approach involves combining variational methods sampling techniques improve accuracy variational distribution maintaining simple form de freitas ghahramani beal 
example variational distribution serve proposal distribution importance sampler 
table 
perplexity held test set function scaling factor 
scaling perplexity believe research areas extended hierarchical bayesian modeling documents corrections empirical bayes accurate variational approximations eventually lead motivated computationally efficient approximation procedures complexity calibrating various contributions inaccurate assessment variability suggests kinds directional adjustments suggested maceachern play important role 
initial experiment adopted maceachern suggestion rescaled sample size keeping sufficient statistics fixed 
corpus documents estimated factor lda model starting run variational em bayesian information retrieval parameter setting 
results table show perplexity held set documents function scaling factor 
unscaled model poorer scaled models scaling yielding best performance 
preliminary results small corpus indicate simple directional adjustments may useful bayesian modeling text corpora 
additional discussion carlin louis 

bayes empirical bayes methods data analysis 
london chapman hall 
chen goodman 

empirical study smoothing techniques language modeling 
proceedings fourth annual meeting association computational linguistics joshi palmer eds 
san mateo ca morgan kaufmann 
de freitas jen rensen jordan russell 

variational mcmc 
uncertainty artificial intelligence breese koller eds 
san mateo ca morgan kaufmann 
ghahramani beal 

variational inference bayesian mixtures factor analyzers 
advances neural information processing cambridge ma mit press 
ibrahim chen 

power prior distributions regression models 
statist 
sci 

kass 

approximate bayesian inference conditionally independent hierarchical models parametric empirical bayes models 
amer 
statist 
assoc 

kappen 

general lower bounds computer generated higher order expansions 
uncertainty artificial intelligence darwiche friedman eds 
san mateo ca morgan kaufmann 
minka lafferty 

expectation propagation generative aspect model uncertainty artificial intelligence darwiche friedman eds 
san mateo ca morgan kaufmann 
mosteller wallace 

applied bayesian classical inference case federalist papers 
new york springer 
