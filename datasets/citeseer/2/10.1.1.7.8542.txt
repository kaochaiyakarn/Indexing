hidden markov support vector machines altun altun cs brown edu ioannis tsochantaridis cs brown edu thomas hofmann th cs brown edu department computer science brown university providence ri usa presents novel discriminative learning technique label sequences combination successful learning algorithms support vector machines hidden markov models call hidden markov support vector machine 
proposed architecture handles dependencies neighboring labels viterbi decoding 
contrast standard hmm training learning procedure discriminative maximum soft margin criterion 
compared previous methods conditional random fields maximum entropy markov models label sequence boosting hm svms number advantages 
notably possible learn non linear discriminant functions kernel functions 
time hm svms share key advantages discriminative methods particular capability deal overlapping features 
report experimental evaluations tasks named entity recognition part speech tagging demonstrate competitiveness proposed approach 

learning observation sequences fundamental problem machine learning 
facet problem generalizes supervised classification predicting label sequences individual class labels 
known label sequence learning 
subsumes problems segmenting observation sequences annotating observation sequences recovering underlying discrete sources 
potential applications widespread ranging natural language processing speech recognition computational biology system identification 
predominant formalism modeling predicting label sequences hidden markov models hmms variations thereof 
hmms model sequential dependencies treating label sequence markov chain 
avoids direct dependencies subsequent observations leads efficient dynamic programming formulation inference learning 
despite success hmms major limitations 
typically trained non discriminative manner 
ii conditional independence assumptions restrictive 
iii explicit feature representations lack power kernel methods 
propose architecture learning label sequences combines hmms support vector machines svms innovative way 
novel architecture called hidden markov svm hm svm 
hm svms address shortcomings retaining key advantages hmms markov chain dependency structure labels efficient dynamic programming formulation 
continues line research includes maximum entropy markov models memms mccallum punyakanok roth conditional random fields crfs lafferty perceptron re ranking collins collins duffy label sequence boosting altun 
basic commonality hm svms methods discriminative approach modeling fact account overlapping features labels depend directly features past observations 
crucial ingredients added hm svms maximum margin principle kernel centric approach learning non linear discriminant functions properties inherited svms 
proceedings twentieth international conference machine learning icml washington dc 

input output mappings joint feature functions focusing label learning problem outline general framework learning mappings discrete output spaces proposed hm svm method special case hofmann 
framework subsumes number problems binary classification multiclass classification multi label classification classification class taxonomies label sequence learning 
general approach pursue learn discriminant function input output pairs maximize function response variable prediction 
general form arg max 
particular interested setting linear combined feature representation inputs outputs 
apply kernel functions avoid performing explicit mapping may intractable leveraging theory kernel learning 
possible due linearity function kernel joint input output space optimal function dual representation terms expansion ik xi yi finite set samples 
xm ym 
key idea approach extract features input patterns binary classification jointly input output pairs 
compatibility input output may depend particular property conjunction particular property especially relevant simply atomic label internal structure described certain features 
features may turn interact non trivial ways certain properties input patterns main difference approach weston 


hidden markov chain discriminants learning label sequences generalization standard supervised classification problem 
formally goal learn mapping observation sequences 

label sequences 

label takes values label set 
observation sequence consider label sequences fixed length admissible range effectively finite availability training set labeled sequences xi yi 
learn mapping data assumed 
order apply joint feature mapping framework label sequence learning define output space consist possible label sequences 
notice definition suitable parametric discriminant function requires specifying mapping extracts features observation label sequence pair 
inspired hmms propose define types features interactions attributes observation vectors specific label interactions neighboring labels chain 
contrast hmms goal define proper joint probability model 
clear main design goal defining sure computed efficiently viterbi decoding algorithm 
order hold propose restrict label label interactions nearest neighbors hmms general dependencies labels observations particular socalled overlapping features 
formally denote mapping maps observation vectors representation define set combined label observation features st denotes indicator function predicate illustrate point discuss concrete example part speech tagging may denote input feature specific word rain occurring th position sentence may encode th word noun 
st indicate conjunction predicates sequence th word rain th word labeled noun 
notice general may binary real valued may st 
second type features consider deal inter label dependencies st 
terms features partial feature map position defined selecting appropriate subsets features st st 
example hmm uses input label features type tt label label features reflecting order markov property chain 
case hm svms maintain restriction trivially generalized higher order markov chains include features st example larger windows simplest case feature map specified defining feature representation input patterns selecting appropriate window size 
features extracted location simply stacked form 
feature map extended sequences length additive manner 
order better understand definition feature mapping indicate possibly exploit kernel functions revealing rewrite inner product feature vectors different sequences 
definition nonoverlapping features sake simplicity straightforward calculation yields similarity sequences depends number common label fragments inner product feature representation patterns common label 

hidden markov perceptron learning focus line learning approach label sequence learning generalizes perceptron learning proposed context natural language processing collins duffy 
nutshell algorithm works follows 
line fashion pattern sequences xi optimal decoding xi computed 
course generalizations possible example may extract different input features depending relative distance chain 
amounts viterbi decoding order produce highest scored label sequence predicted label sequence correct yi update performed 
weight vector updated difference vector xi yi xi new old 
order avoid explicit evaluation feature map direct primal representation discriminant function derive equivalent dual formulation perceptron algorithm 
notice standard perceptron learning case sufficient store training patterns weight update 
label sequence perceptron algorithm needs store incorrectly decoded sequence call negative pseudo example xi xi 
precisely needs store decoded xi differs correct yi typically results compact representation 
dual formulation discriminant function follows 
maintains set dual parameters xi 
update necessary training sequence xi yi incorrectly decoded simply increments yi decrements 
course practical matter implementation represent non zero 
notice requires keep track values pairs xi 
formulation valid joint feature function label sequences generalized arbitrary joint kernel functions replacing inner product corresponding values case nearest neighbor label interactions additivity sequence feature map eq 
come efficient scheme 
decompose contributions 
shows sufficient keep track label pair incorrectly appeared decoded sequence label particular ob incorrectly decoded 
advantage representation independent number incorrect sequences updated efficiently 
order perform viterbi decoding compute transition cost matrix observation cost matrix hi th sequence 

coefficients transition matrix simply values 
calculation observation cost matrix transition cost matrix viterbi decoding amounts finding argument maximizes potential function position sequence 
algorithm dual perceptron algorithm learning joint feature functions naive implementation 
initialize repeat training patterns xi compute yi arg maxy xi xi xi xj yi yi yi yi yi yi errors order prove convergence algorithm suffices apply theorem collins simple generalization theorem 
theorem 
assume training set xi yi 
training label set candidate labels yi yi 
exists weight vector xi yi xi yi number update steps performed perceptron algorithm bounded maxi xi yi yi 

hidden markov svm goal section derive maximum margin formulation joint kernel learning setting 
generalize notion separation margin defining margin training example respect discriminant function xi yi max xi 
yi maximum margin problem defined finding weight vector maximizes mini obviously standard setting maximum margin classification binary labels restrict norm fix functional margin maxi 
results optimization problem quadratic objective min xi yi max xi 
yi non linear constraint eq 
replaced equivalent set linear constraints xi yi xi yi 
rewrite constraints introducing additional threshold example zi xi yi zi 
straightforward prove proposition 
discriminant function fulfills constraints eq 
example xi yi exists fulfills constraints eq 

introduced functions zi stress basically obtained binary classification problem xi yi take role positive examples xi yi take role negative pseudo examples 
difference binary classification bias adjusted group sharing pattern xi 
additional interaction pseudo examples created example xi yi 
standard procedure derive dual formulation quadratic program 
lagrangian dual max zi zj ki 
zi 
ki xi xj notice equality constraints generalize standard constraints binary classification svms yi result optimality conditions thresholds particular implies yi positive example xi yi support vector corresponding support vectors created negative pseudo examples 

hm svm optimization algorithm fundamental assumptions complete enumeration set label sequences intractable actual solution extremely sparse expect negative pseudo examples possibly small subset support vectors 
main challenge terms computational efficiency design computational scheme exploits anticipated sparseness solution 
constraints couple lagrange parameters training example propose optimize iteratively iteration optimizing subspace spanned fixed obviously repeatedly cycling data set optimizing defines coordinate ascent optimization procedure converges correct solution provided problem feasible training data linearly separable 
prove lemmata 
lemma 
solution lagrangian dual pairs problem eq 
xi xi max yi xi 
proof 
define xi maxy yi xi yi 
optimal threshold needs fulfill xi yi xi 
label sequence xi xi xi xi xi yi xi assumption contradicts kkt complementary 
condition xi lemma 
define matrix xi xj zi zj ki 
dei zi xi ei refers canonical basis vector corresponding dimension 
proof 
dei zi zj ki zi xi 
working set approach optimize th subspace adds negative working set time 
define objective th subspace wi propose maximize arguments keeping fixed 
adopting proof osuna prove result proposition 
assume working set yi solution working set obtained maximize objective wi subject constraints exists negative xi xi adding working set optimizing subject yields strict improvement objective function 
proof 
case training example xi yi support vector working set zero yi yi 
con sider ei yi ei 
difference cost function written wi wi ei yi ei yi ei yi ei yi ei yi ei yi ei yi ei yi xi yi xi yi xi yi xi yi 
choosing small 
case ii training example support vector yi negative pseudo example 
consider ei yi ei yi 
wi wi ei ei ei ei xi xi show xi xi independent 
kkt conditions know xi assumption xi concludes proof 
setting xi proposition justifies optimization procedure coordinate ascent th subspace described algorithm 
notice order compute step perform best viterbi decoding schwarz chow 
definition relevant cost matrices follows procedure outlined section 
algorithm working set optimization hm svms 
yi loop compute arg maxy yi xi xi yi xi return optimize wi loop 
soft margin hm svm non separable case may want introduce slack variables allow margin violations 
investigate case penalties 
min zi xi 
notice introduce slack variable training data point pseudo example want penalize strongest margin violation sequence 
solving lagrangian function get gives penalty term 
similar svm case term absorbed kernel effectively changed kc xi xi xi xi zi zi kc xi xj xi xj common penalty gets optimization problem min zi xi 
slack variable shared negative pseudo examples generated 
lagrangian function case zi xi non negativity constraints dual variables 
differentiating gives box constraints take form 
addition kkt conditions imply means yi 
yi working set approach proposed algorithm different constraints quadratic optimization step 
applications experiments 
named entity classification named entity recognition ner information extraction problem deals finding phrases containing person location organization names temporal number expressions 
entry annotated type expression position expression continuation expression 
generated sub corpus consisting sentences spanish news wire article corpus error named entity classification hmm crf crf hm pc hm svm error 
test error ner task window size fold cross validation 
provided special session conll ner 
expression types corpus limited person names organizations locations miscellaneous names resulting total different labels 
input features simple binary features 
features indicator functions word occurring fixed size window centered word labeled 
addition features encode identity word detailed properties spelling features 
notice features combined particular label indicator functions joint feature map framework 
example features previous word current tag person word dot current tag previous tag non name current tag location intermediate 
order illustrate nature extracted support sequences show example 
example sentence correct labeling seen top 
stands non name entities 
upper case letters stand lower case letters stand continuation types name entities miscellaneous organization continuation 
subset support sequences correct label support sequences depicted positions differ correct 
support sequences maximal selected 
seen support sequences differ positions correct label sequence resulting sparse solutions 
particular example support sequences size noted support sequences training examples yi examples fulfill margin constraints 
pp ya ley tv regional por la junta efe 
example sentence correct named entity labeling subset corresponding support sequences 
labels different correct labels depicted support sequences 
compared performance hmms crfs hm perceptron hm svm test errors fold cross validation 
overlapping features window size experiments 
second degree polynomial kernel hm perceptron hm svm 
soft margin hm svm 
generative model hmm overlapping features violate model observed hmms overlapping features described outperformed ordinary hmms 
reason report results hmms overlapping features 
crfs optimized conjugate gradient method reportedly outperformed techniques minimizing crf loss function minka 
optimizing log loss functions done crfs may result overfitting especially noisy data followed suggestion johnson regularized cost function 
refer crf variant crf 
results summarized demonstrate competitiveness hm svms 
expected crfs perform better hm perceptron algorithm hm pc crfs derivative logloss function step perceptron algorithm uses approximation cf 
collins 
hm svms achieve best results validates approach explicitly maximizing soft margin criterion 

part speech tagging extracted corpus consisting sentences penn treebank corpus part speech pos tagging experiments 
features experi error part speech tagging hmm crf crf hm pc hm svm error 
test error pos task window size fold cross validation 
mental setup similar ner experiments 
total number function tags 
summarizes experimental results obtained task 
qualitatively behavior different optimization methods comparable ner experiments 
discriminative methods clearly outperform hmms hm svms outperform methods 

hm svms novel discriminative learning technique label sequence learning problem 
method combines advantages maximum margin classifier kernels elegance efficiency hmms 
experiments prove competitiveness hm svms terms achieved error rate benchmark data sets 
hm svms advantages methods including possibility larger number expressive features 
currently addressing scalability issue able perform larger scale experiments 
acknowledgments sponsored nsf itr award number iis 
altun hofmann johnson 

discriminative learning label sequences boosting 
advances neural information processing systems 
cambridge ma mit press 
collins 

discriminative training methods hidden markov models theory experiments perceptron algorithms 
proceedings conference empirical methods natural language processing 
collins duffy 

convolution kernels natural language 
advances neural information processing systems pp 

cambridge ma mit press 
hofmann tsochantaridis altun 

learning structured output spaces joint kernel functions 
proceedings sixth kernel workshop 
johnson geman canon chi riezler 

estimators stochastic grammars 
proceedings seventh annual meeting association computational linguistics pp 

lafferty mccallum pereira 

conditional random fields probabilistic models segmenting labeling sequence data 
proceedings eighteenth international conference machine learning pp 

san francisco morgan kaufmann 
mccallum freitag pereira 

maximum entropy markov models information extraction segmentation 
proceedings seventeenth international conference machine learning pp 

san francisco morgan kaufmann 
minka 

algorithms maximum likelihood logistic regression technical report 
department statistics carnegie mellon university 
osuna freund girosi 

training support vector machines application face detection 
proceeding conference computer vision pattern recognition pp 

punyakanok roth 

classifiers sequential inference 
advances neural information processing systems pp 

cambridge ma mit press 
schwarz chow 

best algorithm efficient exact procedure finding hypotheses 
proceedings ieee international conference acoustics speech signal processing pp 

weston chapelle elisseeff sch lkopf vapnik 

kernel dependency estimation 
advances neural information processing systems 
cambridge ma mit press 
