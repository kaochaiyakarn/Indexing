discovering hierarchy reinforcement learning hexq bernhard cse unsw edu au computer science engineering university new south wales unsw sydney australia open problem reinforcement learning discovering hierarchical structure 
hexq algorithm automatically attempts decompose solve model free factored mdp hierarchically described 
searching aliased markov sub space regions state variables algorithm uses temporal state abstraction construct hierarchy interlinked smaller mdps 

bellman stated sheer enumeration solve problems significance 
reinforcement learning size state space scales exponentially number variables 
designers try manually decompose complex problems tractable 
finding decompositions usually art form 
researchers ignored decompositions come pointed desirability automating task boutilier hauskrecht dean lin 
dietterich concluded biggest open problem reinforcement learning discover hierarchical structure 
recognised ashby learning worthwhile environment shows constraint 
type constraint environments repetition sub structures 
ashby stated repetition considerable practical importance regulation large systems 
repetitions commonplace 
evident example molecular level daily routines ce layouts just walking 
reason reinforcement learning scales poorly sub policies walking need context 
sense learn walk reuse skill required 
reinforcement learning agent find learn reusable sub tasks turn employ learn higher level skills cient 
rest describe operation hierarchical reinforcement learning algorithm hexq attempts solve model free mdps ciently finding exploiting repeatable sub structures environment 
algorithm designed automatically discover state temporal abstractions find appropriate sub goals construct hierarchical representation solve mdp 
running example taxi task dietterich illustrate algorithm works 
show results noisy tower hanoi puzzle 

representation assumptions start usual formulation finite mdp discrete time steps states actions sutton barto 
objective find optimal policy maximising expected value discounted rewards represented action value function watkins dayan 
employ semi mdp theory puterman generalizes mdps models variable time decisions 
assume state defined vector state variables 
large mdps naturally described factored form 
consider negative reward non discounted finite horizon mdps stochastic shortest path problems algorithm extended handle general finite mdps 
issue solving mdps ciently largely orthogonal complementary decomposition techniques discussed 
simple step backup learning 
hexq attempts decompose mdp dividing state space nested sub mdp regions 
decomposition possible variables state vector represent features environment change frequent time intervals variables change value frequently retain transition properties context persistent variables interface regions controlled 
example robot nav equally sized rooms interconnecting doorways parr state space represented variables room identifier position room 
room changes frequently position 
position room needs represented consistently allow generalisation rooms example numbering cells top bottom left right room 
representations naturally label repeated sub structures way 
need able find sub policies exit doorway certainty 
clearer sections 
absence conditions partially hexq solve mdp discovering abstractions 
worst case solve flat problem 

taxi domain dietterich created taxi task demonstrate maxq hierarchical reinforcement learning 
maxq structure hierarchy specified user 
domain illustrate hierarchical decomposition automated 
keep description hexq general taxi domain illustrate basic concepts 
start reviewing taxi problem 
taxi domain taxi started random location navigates grid world pick put passenger 
possible source destination locations designated encode respectively 
called taxi ranks 
objective taxi agent go source rank pick passenger navigate passenger taxi destination rank put passenger 
source destination ranks chosen random new trial 
step taxi perform primitive actions move square north south east west pickup putdown passenger 
move wall barrier leaves taxi location unchanged 
successful passenger delivery reward 
taxi executes pickup action location passenger putdown action wrong destination receives reward 
steps reward 
trial terminates successful delivery 
taxi problem formulated episodic mdp state variables location taxi values passenger location including taxi values means taxi destination location values 
deterministic actions illustrations simpler 
results section 
taxi domain 
stochastic actions 
easy see taxi navigate ranks navigation policy intends pick put passenger 
usual flat formulation mdp solve navigation sub task times di erent contexts 
dietterich demonstrated problem solved ciently sub task reuse designing maxq hierarchy 
show hierarchy generated automatically solve problem 

automatic hierarchical decomposition hexq uses state variables construct hierarchy 
maximum number levels hierarchy number state variables 
taxi domain levels 
hierarchy constructed levels numbered bottom 
bottom level level associated variable changes value frequently 
rationale sub tasks appear lower levels need learnt 
level level interacts environment primitive actions 
start observing state variables 
choose variable basis changes value frequently 
partition states represented values variable markov regions 
boundaries regions identified unpredictable see subsection transitions call region exits 
define sub mdps regions learn separate policies leave region various exits 
regions abstracted combined frequently changing state variable form states level hierarchy 
exit policies just learnt actions table 
frequency change taxi domain variables random steps 
variable frequency order passenger location taxi location destination level 
stage semi mdp variable state description actions 
repeat process reduced problem forming hierarchical level state variable 
abstraction reduce number values required simply take cartesian product current level state variable continue construction hierarchy combined state 
top level sub mdp solved recursively calling sub mdp policies actions 
describe process detail 
variable ordering heuristic just lines inner loop programs executed frequently variables change value frequently associated lower levels hierarchy 
conversely variables change frequently set context frequently changing ones 
hierarchy constructed bottom starting variable changes frequently 
order variables allow agent explore environment random set period time keep statistics frequency change state variables 
sort variables frequency change 
taxi table shows frequency order variable random action exploration run 
discovering repeatable regions hexq starts projecting entire state space values variable changes frequently limiting state space values 
obviously agent perception environment highly aliased 
hexq proceeds attempt model state transitions rewards exploring environment random actions 
models state transitions directed graph dg vertices state values edges transitions primitive action 
set period exploration manner transitions unpredictable called exits eliminated graph 
definition exit state action pair signifying action state causes unpredictable transition 
transitions unpredictable state transition reward function stationary probability distribution variable may change value task terminates 
state definition level state see subsection referred exit state 
action primitive level higher levels 
entry state state reached exit 
taxi location variable states entries taxi agent restarted location random episodic trial exit 
left dg connections represent predictable transitions values level state variable 
taxi example directed graph taxi location variable shown 
transitions labelled exits counted edges 
example action pickup putdown state predictable may change passenger location variable reach goal respectively 
predict transitions environment 
state example transitions state action north south east west pickup putdown 
drawn edge represent multiple transitions states avoid cluttering diagram 
exits exits exits exits 
directed graph state transitions taxi location 
exits non predictable state transitions counted edges graph 
describe general procedure decompose dg regions meet purpose valid hierarchical state abstraction 
decompose dg strongly connected components sccs 
cient linear time algorithm procedure cormen 

sccs abstracted nodes form directed acyclic graph dag 
possible connected sccs dag 
require enter state leave exit combine sccs form regions 
regions abstracted form higher level states 
objective maximise size regions minimise number states 
coalition sccs regions may regions connected edges underlying dag 
break forming additional exits entries associated respective regions repeat entire procedure additional regions formed 
regions labelled arbitrarily 
consecutive integers convenience see equation 
region combination sccs exit state region reached entry probability 
regions generally aliased environment 
instances generic regions partition total state space 
region set states actions predictable markov transition reward functions 
define mdp region exit sub goal transition absorbing state 
solution sub mdp policy region move agent exit starting entry 
proceed construct multiple sub mdps unique hierarchical exit state region 
sub mdp policies hexq learnt line form hierarchical dynamic programming directly sub task models uncovered 
taxi example procedure finds hierarchical level region reflected 
region exits 
pickup putdown pickup putdown pickup putdown pickup putdown 
hierarchical exit states create level solve 
state action abstraction position tackle second level hierarchy 
process similar level searching repeatable regions 
states actions abstractions level 
define states second level cartesian product region labels values state variable frequency ordering 
convenient numerical method generating state values follows state value level number regions level frequent state variable value region label level actions available states policies leading region exits level 
actions similar composite actions dietterich 
ect action state invoke execute associated level sub mdp policy 
taxi example region level label 
apply equation level hierarchy states simply correspond values passenger location 
generate states level 
actions policies lead exits listed previously 
generally case di erent states di erent actions 
actions take varying primitive time periods execute semi markov decision problem 
semi mdp variable original mdp uses actions 
pickup pickup pickup pickup putdown putdown putdown putdown level exits 
state transitions passenger location variable level hierarchy 
exits level 
repeat level procedure finding regions exits states actions level 
taxi region exits level shown 
note actions labelled exit notation level 
exits level 
shown 
example exit putdown means passenger taxi navigate location putdown passenger 
exit may may lead goal depending destination location 
generate sub mdps level unique exit states 
way generate level hierarchy variable original mdp 
change predictability criteria test state variables change ones processed 
reach state variable solve top level sub mdp represented final states actions solves mdp 
putdown putdown putdown putdown 
top level sub mdp taxi domain showing actions leading goal 
top level sub mdp variable destination shown 
note nesting description actions 
illustrate execution competent taxi agent hierarchically decomposed problem assume taxi initially located randomly cell passenger rank wants go rank 
top level sub mdp taxi agent perceives passenger destination takes action putdown 
sets subgoal state level english pick passenger 
level taxi agent perceives passenger location executes action pickup 
action sets sub goal state level taxi location rank 
level policy executed primitive actions move taxi location pickup location pickup action executed exit 
level returns control level state transitioned 
level completes instruction takes action putdown 
invokes level primitive actions move taxi location putdown exit 
control returned back hierarchy trial ends passenger delivered correctly 

hierarchical value function hexq similar maxq approach hierarchical execution decomposed value function important di erences 
motivation new decomposition equations automatically solve hierarchical credit assignment problem dietterich non repeatable sub task rewards hierarchy explained 
equations reduce nicely usual value functions flat mdps variable state vector 
definition recursively optimal hierarchical exit function em level sub mdp expected value completing execution action starting state optimal hierarchical policy 
em em primitive reward hierarchical state note includes expected primitive reward immediately sub task exit include rewards accumulated executing subtasks 
name hexq derived function 
recursively optimal hierarchical value function decomposition em max em implementing action manner regions formed ensured action exit sub mdp unintentionally 
special case stochastic shortest path problems avoid pseudo reward functions dietterich 
update region sub policies concurrently policy backups similarly goals updating kaelbling 

stochastic considerations hexq handles stochastic actions rewards 
aspects need attention explained sections 
detecting non markov transitions exits occur see definition state transition reward predictable higher level state variable change value 
deterministic case easy determine exits 
need find transition di erent states reward values action trigger exit condition 
stochastic case need record statistics test hypothesis reward state transitions functions equation come di erent probability distributions contexts 
theory possible determine degree accuracy test individual contexts represented higher level variables 
practice intractable combinations higher level variables grow exponentially 
keep transition statistics shorter period time compare long term average 
objective explicitly test probability distribution stationary 
binomial distribution average probabilities test temporally close sample transition outside upper confidence limit 
happens declare exit 
similarly detect reward function non stationarity kolmogorov smirnov test 
hierarchical greedy policy sub mdp attempt exit region exit determined level agent may slipped closer exit optimal hauskrecht 
solution re evaluate optimum policy level step 
possible uninterrupted exit policies learnt referred hierarchical greedy policy dietterich 

results compares performance hexq maxq flat learner stochastic taxi task navigation actions performing intended time time moving taxi randomly left right intended action 
trials steps passenger delivery flat hexq maxq 
performance hexq vs flat learner maxq stochastic taxi 
graph shows number primitive time steps required complete successive trial averaged runs 
experiments learning rate set 
initial values set actions greedy hierarchy construction described previously 
exploration implicitly forced initial values stochastic actions 
hexq performance improves distinct stages level hierarchy constructed 
trials surpasses performance flat learner 
flat learner start improving performance trial hexq order variables find exits level improve performance 
hexq learns rapidly transfers sub task skills 
investment hierarchy construction breaks trials cumulative number time steps flat learner hexq equal 
additional background knowledge maxq learns rapidly 
version maxq slower convergence evident caused learning values levels simultaneously 
higher level values initially learnt inflated costs lower level partially learnt policies 
terms storage requirements value function flat learner uses table states actions values 
hexq requires mdps level states actions values 
mdps level states actions values 
mdp level states actions values 
total values 
maxq comparison requires values 
hexq course told actions apply di erent levels discover 
second example performance hexq compared flat learner noisy pin tower hanoi puzzle toh disks 
state variables disks 
values represent pin positions 
disk moved pin disk moved process disks pin remain ordered size smallest top 
actions defined disk target pin 
example action move disk pin 
actions disk total 
actions stochastic sense having picked disk probability intended legal move succeed probability disk attempt slip randomly pin 
illegal moves disks remains situ 
reward move 
goal move disks pin 
disks randomly legally placed pins start trial 
learners model free 
trials moves reach goal flat hexq 
performance hexq vs flat learner stochastic tower hanoi puzzle disks averaged runs smoothed 
see hexq learns disk toh fraction number trials flat learner 
hexq takes half number steps converge compared flat learner 
recursive value function implements depth search expensive levels 
issue shared maxq 
reasonable approach evaluate function certain depth 
analogy planning trip new york sydney take consideration side bed get way bathroom day departure 
example limited search depth 
possible toh recursive constraints values level change outcome 
toh alternative action representations 
move require actions 
representation hexq quickly learn deterministic disk toh time complexity space complexity 
general stochastic actions representation fails decompose action disk state exit 

limitations hexq perform worse flat learner relies certain constraints problem allow find decompositions 
subset variables form sub mdps find policies reach exits probability hexq find decomposition 
find sub mdps necessary variables change longer timescale 
requirement able exit certainty ensures learnt sub tasks uncontrollable surprises higher level tasks 
means benign problems navigating multi room domain stochastic actions slip directions hexq decompose 
leave automatically finding conditions exit certainty constraint relaxed 
problem characteristics may result discovery large number exits 
example doorway modelled exit states indicating possible exit left right centre hexq generate solve sub mdps 
designer choose combine exits generate 
improvement extend hexq automatically combine exits leading state 
exit combination heuristics general transition properties entering region having small value di erences suggest 
exit combining problems decomposable composite exit may reached probability 
leave study tradeo intra region value improvement inter region value deterioration exits combined 
heuristics employed hexq variable ordering finding non stationary transitions 
frequency order variables changed hexq may able partially decompose mdp cases ciently 
decompositions different variable combinations orderings discussed 
independent slowly changing random variable sorted top level hierarchy heuristic mdp fail decompose necessary explore variable potential impact 
research may better directed selecting relevant variables task hand finding better sorting heuristics 
second heuristic penalty recognising extra exits simply generate additional overhead hexq 
exits require new policies learnt turn need explored new actions level hierarchy 
detract quality solution terms optimality 
exits missed solution may ected 
alternatively may circuitous worst case fail altogether 
possible incorporate recovery procedures hexq new exits discovered 
stage rely hexq find relevant exits 
deterministic shortest path problems hexq find globally optimal policy 
stochastic actions hexq maxq recursively optimal 

hexq tackled problem discovering hierarchical structure stochastic shortest path factored mdps 
hexq decompose mdps find nested sub mdps policies reach exit certainty 
generally perform problems identify large regions exits contexts sub tasks required 
hexq automated usually required performed designer 
includes defining sub tasks finding sets actions performed sub task finding sub task termination conditions sub goals finding usable state temporal abstractions assigning credit hierarchically 
ongoing generalise hexq building key idea discovering markov sub spaces handle hidden state selective perception 
believe discovery manipulation hierarchical representations prove essential lifelong learning autonomous agents 
ashby 

cybernetics 
london chapman hall 
bellman 

adaptive control processes guided tour 
princeton nj princeton university press 
boutilier dean hanks 

decisiontheoretic planning structural assumptions computational leverage 
journal artificial intelligence research 
cormen leiserson rivest 

algorithms 
cambridge massachusetts mit press 
dean lin 

decomposition techniques planning stochastic domains technical report cs 
department computer science brown university 
dietterich 

hierarchical reinforcement learning maxq value function decomposition 
journal artificial intelligence research 
dietterich 

overview maxq hierarchical reinforcement learning 
sara pp 

hauskrecht meuleau kaelbling dean boutilier 

hierarchical solution markov decision processes macro actions 
fourteenth annual conference uncertainty artificial intelligence pp 



generating hierarchical structure reinforcement learning state variables 
pricai topics artificial intelligence pp 

san francisco springer 
kaelbling 

hierarchical learning stochastic domains preliminary results 
machine learning proceedings tenth international conference pp 

san mateo ca morgan kaufmann 
parr 

hierarchical control learning markov decision processes 
doctoral dissertation university california berkeley 
puterman 

markov decision processes discrete stochastic dynamic programming 
new york ny john sons sutton barto 

reinforcement learning 
cambridge massachusetts mit press 
watkins dayan 

technical note learning 
machine learning 
