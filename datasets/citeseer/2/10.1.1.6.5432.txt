support vector machines classification ioannis tsochantaridis thomas hofmann department computer science brown university box providence ri usa th cs brown edu 
classification deals task solving multiple interdependent classification problems 
key challenge systematically exploit possible dependencies labels improve standard approach solving classification problem independently 
method operates stages stage uses observed set labels learn joint label model predict unobserved pattern labels purely inter label dependencies 
second stage uses observed labels inferred label predictions input generalized transductive support vector machine 
resulting mixed integer program heuristically solved continuation method 
report experimental results collaborative filtering task provide empirical support approach 
standard supervised classification setting inferring single discriminant function finite sample labeled patterns investigated decades 
question additional unlabeled examples received lot attention 
methods include fisher kernel maximum entropy discrimination method maximum likelihood estimation em text categorization training transductive inference kernel expansion methods :10.1.1.20.9305
general hope line research unlabeled data provide useful information pattern distribution exploited improve classification performance inducing improved pattern representation enabling robust estimation discriminant function 
cases certain assumptions guarantee unlabeled data help improve performance 
investigate general setting called classification 
assume multiple binary concepts represented labeling processes denotes joint probability distribution labeled patterns 
concept sample set available samples generated goal simultaneously learn binary classification tasks 
course tasks unrelated apply standard classification method sample set independently 
assume non trivial dependencies labeling processes 
notice setting viewed generalization supervised learning unlabeled data 
classification problem corresponding sample set addition patterns occur sample sets annotated labels labeling processes 
induces rich set structure extrema examples correctly labeled specific concept learned examples labeled process cf 
fig 

intuitively pattern labeled process average unlabeled examples labeled process labeled process labeled process fig 

illustration set relationships patterns labeled different labeling processes 
useful unlabeled pattern particular dependency labeling processes 
example knew priori concepts identical simply union training concepts drastically increase number available training examples 
goal classification exploit dependencies effectively augment available training data order learn accurate classification functions 
motivating example types problems relevance practice consider scenario information filtering multi user multi agent systems user may define personalized categories items text documents movies cds 
particular users may annotate items item relevant label irrelevant label 
preferences categories specific particular person similarities user interests induce dependencies category labels 
example document xi labeled user agent uj provide evidence user agent ul label example particular users shown similar responses items sources evidence important pre xi input space representation ordinarily exploited classification dependencies labeling processes 
closely related technique known collaborative filtering predictions recommendations purely inter label dependencies 
success techniques commercial recommendation systems shows substantial amount cross information contained user profiles 
classification aims combining sources evidence item feature vector representation dependencies labels provided different users 
problem discussed context recommender systems problem combining content collaborative filtering cf 
example 
methods proposed far shown generalize state art discriminative methods incorporate collaborative information 
approach propose decomposed independent stages 
stage deals problem learning probabilistic model inter label dependencies 
words goal stage estimate joint label probability yi yj yk xi pattern xi occurs sample sets 
xi denotes set known labels pattern xi 
marginalization obtain prior probabilities xi 
notice probabilities depend actual feature representation xi just observed partial label vector xi 
estimate depend observation xi refer prior label probability 
second stage sample sets augmented probabilistically labeled examples 
input generalized transductive support vector machine svm produce desired classification functions 
challenge stage combine prior label estimates actual feature representations 
rest organized follows section describes statistical model corresponding learning algorithm compute predictions unobserved labels observed labels 
section deals generalization transductive svm section presents experimental evaluation real world data set 
modeling inter label dependencies section completely ignore pattern representation solely focus modeling inter label dependencies 
denote total number distinct patterns xi sj labels arranged matrix entries referring label th labeling th pattern 
special symbol denote missing entries 
cases matrix sparse sense sj small fraction entries observed 
goal estimate matrix coefficients corresponding expected value label yi model denotes random variable associated label th pattern respect th labeling process 
log likelihood function objective function probabilistic estimates observed matrix natural consider log likelihood log yj log yj want maximize 
notice yi yi definition just measures average log probability true label model leading approximation probabilistic latent semantic analysis model possibilities define joint label model 
investigate probabilistic latent semantic analysis plsa approach 
previously applied model context collaborative filtering starting point classification 
plsa model written form denotes rank approximation assume 
notice total number free parameters model far ifr min 
intuitively think prototype vector probabilistic labels pattern xi coefficients defining convex combination vectors th classification problem 
plsa model clearly bears resemblance soft clustering models concepts probabilistically clustered groups group corresponds super concept characterized vector probabilistic labels patterns 
expectation maximization algorithm fitting model maximize likelihood eq 
respect parameters 
explicitly inserting model log likelihood function ignoring additive constants results log log logarithm sum terms hard optimize follow standard expectation maximization em approach iteratively improving eq 
local maximum reached 
denote parameter estimates time step em procedure 
goal step improve estimate obtained step quantified terms differential log likelihood log ri log ri 
concavity argument jensen inequality differential log likelihood lower bounded follows ij ij log ri ij log ri 
augmenting lower bound eq 
appropriate lagrange multipliers enforce constraints set gradient respect new parameters estimates zero 
yields explicit solution form hr ij hs ij hr ij eq 
corresponds step expectation step eqs 
form step maximization step 
seen previous parameter values enter step equations hr ij variables 
maximize log likelihood alternating steps steps convergence reached 
fact em algorithm converges follows fact log likelihood increased step bounded 
comments voted plsa approach model inter label dependencies 
point due modularity approach options employed combined generalization transductive inference svms subsequent section 
example alternative plsa graphical models bayesian networks label vector yj labeling process treated instance bayesian network consists nodes pattern xi 
approach collaborative filtering pursued 
plan investigate research direction 
transductive svm probabilistic labels support vector machines support vector machine svm popular classification method principle margin maximization 
svms generalize linear discrimination method known maximum margin hyperplane 
assume parameterize linear classifiers weight vector bias term sign 
linearly separable data general hyperplanes separate training data perfectly 
hyperplanes form called version space 
maximum margin principle suggests choose parameters version space maximize minimal distance margin hyperplane training points 
svms generalize idea ways 
order able deal non separable data sets introduces slack variables data point augments objective function additional penalty term 
penalty term usually proportional sum slack variables norm choices include squared error 
norm penalties arrives standard quadratic program soft margin svms minimize subject yi xi denotes number available training patterns 
introducing lagrange parameters inequality margin constraints explicitly solve obtain dual formulation wolfe dual cf 
maximize xi xj subject yi gram matrix coefficients kij xi xj symmetric positive semi definite resulting problem convex quadratic minimization problem 
furthermore svm learning take advantage fact dual function depends gram matrix replace inner products patterns input representation inner product computed kernel functions simply define new gram matrix kij xi xj 
effectively gets non linear classification function original input representation 
details kernel methods 
transductive svms transductive svms aims incorporating additional unlabeled data get reliable estimates optimal discriminant 
key observation discriminant function results small margins unlabeled data point achieve separation matter true label unlabeled data point idea formalized introducing additional integer variables yi model unknown labels optimize joint objective integer variables parameters bor equivalently dual parameters 
primal formulation mainly comprehensible purpose presentation 
assume simplicity labeled patterns numbered unlabeled examples numbered minimize subject yi xi yi xi yi alternative formulations tsvm problem avoid integer variables result non convex optimization investigated 
large problems currently hope find exact solution mixed integer quadratic program 
resort optimization heuristics compute approximate solution 
heuristic proposed optimizes integer variables outer loop solves standard svm qp inner loop :10.1.1.20.9305
proposes keep proportion positive negative labels constant labels yi yj yi yj swapped pairs unlabeled examples xi xj reduces objective function :10.1.1.20.9305
outer loop employs continuation method reduce sensitivity optimization heuristic respect local minima 
starting small value penalty iteratively increased reaches final value notice small values labeled data dominate objective function tsvm solution close svm solution computed exactly 
increased penalty having unlabeled data points close decision boundary increases attention paid configuration unlabeled data points imputed labels yi 
svm probabilistic labels order prior label estimates derived inter label dependencies propose generalize way handle uncertain labels think labeled unlabeled patterns extreme cases uncertain labels 
assume label probabilities yj observed labels 
drop superscript refer generic labeling process 
introduce binary integer variables yi define optimization problem numbering convention minimize dh yi log yi yi log yi subject yi xi yi xi yi function measures cross entropy deterministically imputed labels yi predictions derived inter label model yi 
acts soft penalty penalizes labels deviate prior predictions 
relative weight controls influence penalty relative margin penalty encoded slack variables trading inter label information encoded yi information encoded feature representation xi 
practice cross validation scheme determine optimal value notice special case yi case maximally entropic prior label uncertainty bit formulation reduces tsvm corresponding log ratio term reduce constant 
hyperplane update step labels yi simple 
notice slack variable depend yi associated constraint involves yi 
non negative large values penalized optimal choice max yi min yi xi 
notice data points strictly inside margin tube value positive yi yi 
straightforward initialize small value initialize integer variables yi sign yi repeat repeat convergence integer variable needs changed compute optimal hyperplane integer variables yi re compute integer variables yi parameters fig 

generalized svm algorithm classification 
compute optimal value yi comparing cost induced possible choices max log yi max log yi min min log yi yi sign dlog yi yi log yi yi log yi yi notice yi contributions agreement data point side hyperplane prior probability label higher 
yi contributions conflict case weighting factor determines compare log ratio margin difference favor prior belief location feature vector relative current decision boundary 
complete algorithm described pseudo code fig 
experiments results data generation preprocessing order experimentally verify proposed method classification known eachmovie data set contains movies user profiles total number approximately labels votes 
augmented data set movie synopses descriptions provided 
movie pages automatically crawled parsed indexed 
movies represented vectors xi standard term frequency representation vector space model information retrieval 
able obtain independent popularity plsa model classification baseline features svm table 
classification accuracy results augmented eachmovie data set 
row denotes accuracies obtained ignoring feature representation second row summarizes results svm 
columns refers case model inter label dependencies second column popularity model third column plsa model 
descriptions movies constitutes set pattern experiments 
computational reasons subsampled database randomly selected subset user profiles profiles votes 
actual votes converted binary labels thresholding ratings stars mapped stars label 
user available labels randomly split training set test set 
experiments performed experimental comparison 
users trained svm just feature representation 
experiments restricted attention linear kernels 
provides benchmark purely extracted content information 
trained plsa model predict unobserved labels observed label matrix chosen model coarse optimization predictive log likelihood 
provides benchmark purely label dependencies 
addition investigated simple popularity baseline model estimates expected label uniformly averaging population users 
plsa predictions popularity predictions prior predictions svm algorithm 
table summarizes results terms classification accuracy 
notice inter label dependencies leads significant absolute improvement terms classification accuracy compared svm learning 
clearly demonstrates lot gained treatment compared straightforward approach independently solving classification problem 
shows particular example content features relatively weak discrimination movies available training sample size 
individual words occurring short movie summaries weakly correlated users preferences 
secondly notice features representation yields small consistent improvement performance simple popularity model plsa model inter label dependencies 
despite fact collaborative information labels precise information encoded content descriptions extra information gained feature representation 
sees difference larger case popularity baseline vs gain accuracy 
consider previous experiments shown plsa highly competitive collaborative filtering technique improving trivial 
speculate improvement larger cases feature representation inter label dependencies yield predictions comparable accuracy 
lines currently investigating ways extract stronger features genre information movies 
novel approach jointly solving large scale classification problems interdependent concepts 
proposed method uses state art classification methods svms learn feature representations 
order incorporate inter label dependencies transductive svm framework extended deal weak label information 
efficient optimization heuristic proposed compute approximate solutions resulting mixed integer program 
real world data set proposed method outperforms methods purely feature representation methods account inter label dependencies 
acknowledgments compaq equipment making eachmovie data set available 
part sponsored nsf itr award number iis 

www com 

basu hirsh cohen 
recommendation classification social content information recommendation 
proceedings aaai iaai pages 

blum mitchell 
combining labeled unlabeled data cotraining 
proc 
th annual conf 
computational learning theory pages 

demiriz bennett 
optimization approaches semi supervised learning 
ferris mangasarian pang editors applications algorithms complementarity 
kluwer academic publishers boston 

dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 

research compaq com src eachmovie 

goldberg nichols oki terry 
collaborative filtering weave information tapestry 
communications acm 

hofmann 
unsupervised learning probabilistic latent semantic analysis 
machine learning journal 

hofmann 
people don want 
european conference machine learning ecml 

hofmann puzicha 
latent class models collaborative filtering 
proceedings international joint conference artificial intelligence 

breese heckerman 
empirical analysis predictive algorithms collaborative filtering 
proceedings fourteenth conference uncertainty artificial intelligence 

jaakkola haussler 
exploiting generative models discriminative classifiers 
advances neural information processing systems 
mit press 

jaakkola meila jebara 
maximum entropy discrimination 
neural information processing systems 
mit press 

joachims 
transductive inference text classification support vector machines 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 

nigam mccallum thrun mitchell 
text classification labeled unlabeled documents em 
machine learning 

popescul ungar pennock lawrence 
probabilistic models unified collaborative content recommendation sparse data environments 
th conference uncertainty artificial intelligence 

resnick iacovou suchak bergstrom riedl 
grouplens open architecture collaborative filtering netnews 
proceedings acm conference computer supported cooperative 

salton mcgill 
modern information retrieval 
mcgraw hill new york 

sch lkopf smola 
learning kernels support vector machines regularization optimization 
mit press 

shardanand maes 
social information filtering algorithms automating word mouth 
proceedings computer human interaction conference chi 

szummer jaakkola 
kernel expansions unlabeled examples 
advances neural information processing systems 
mit press 

vapnik 
nature statistical learning theory 
springer verlag berlin 
