mixtures conditional maximum entropy models dmitry pavlov nec labs com nec labs america independence way princeton nj popescul popescul cis upenn edu computer information science university pennsylvania philadelphia pa david pennock david pennock com services pasadena ave rd oor pasadena ca lyle ungar ungar cis upenn edu computer information science university pennsylvania philadelphia pa driven successes application areas maximum entropy modeling gained considerable popularity 
generalize standard maximum entropy formulation classi cation problems better handle case complex data distributions arise mixture simpler underlying latent distributions 
develop theoretical framework characterizing data mixture maximum entropy models 
formulate maximum likelihood interpretation mixture model learning derive generalized em algorithm solve corresponding optimization problem 
empirical results number data sets showing modeling data mixture latent maximum entropy models gives signi cant improvement standard single component maximum entropy approach 
keywords mixture model maximum entropy latent structure classi cation 

maximum entropy maxent modeling long history concept physics working way foundations information theory bayesian statistics jaynes 
years advances computing growth available data contributed increased popularity maxent modeling leading number successful applications including natural language processing berger language modeling chen rosenfeld part speech tagging ratnaparkhi database querying pavlov smyth protein modeling ungar name 
maxent approach attractive properties contributed popularity 
approach guarantees obtained distribution close possible uniform subject speci constraints 
second method semi parametric meaning learned distribution take form adheres constraints 
way maxent modeling able combine sparse local information encoded constraints coherent global probabilistic model priori assuming particular distributional form 
third method capable combining heterogeneous overlapping sources information nally fairly general assumptions maxent modeling shown equivalent maximum likelihood modeling distributions exponential family della pietra 
successful applications maxent modeling area classi cation jaakkola text classi cation particular nigam 
case conditional maxent distributions probabilities class labels feature values learned training data automatically classify feature vectors class membership unknown 
note maximally times hindrance cases exploitable hidden structure exists data expressed constraints 
data sets seemingly complex distributional structures seen generated simpler latent distributions directly observable 
example distribution text broad document collection may complex joint structure broken meaningful topics may modeled mixture simpler topic speci distributions 
mixture models mclachlan basford designed handle just case assumed full distribution composed simpler components 
sense discovering underlying structure data set thought unsupervised learning subtask larger supervised learning problem 
generalize maxent formalism handle mixtures maxent models 
cases data decomposed latent clusters framework leverages extra structural information produce models higher sample log likelihood higher expected classi cation accuracy 
formulate maximum likelihood interpretation mixture model learning derive generalized em gem algorithm dempster solve corresponding optimization problem 
empirical results publicly available data sets showing signi cant improvements standard maxent approach 
data sets tested mixture technique performs worse standard maxent noise tolerance performs signi cantly better 
contrast numerous dimensionality reduction techniques employed supervised learning regarded techniques exploiting latent structure space features mixtures conditional maximum entropy models directly exploit latent structure space original examples documents words text classi cation 
rest organized follows 
section discuss related 
section review de nition standard maxent model 
section presents de nition mixture maxent models main update equations gem algorithm mixture 
experimental results discussed section 
section draw describe directions 

related latent maximum entropy principle introduced general setting wang 
wang 
particular gave motivation generalizing standard jaynes maximum entropy principle jaynes include latent variables formulated convergence theorem associated em algorithm 
derivation em algorithm speci mixture model latent structure describe discuss empirical results approach 
modeling latent structure document space previously employed classi cation setting nigam 
nigam unpublished commercial project distributional clustering baker mccallum pereira maxent modeling combined improve document classi cation accuracy 
fundamental distinction approach latent structure mixture modeling fully integrated em algorithm designed maximize single objective function 
generalized linear mixed models wol nger connell widely marketing research similar approach 
model mixture components non linear potentially powerful 
binary classi cation problems conditional maximum entropy models shown equivalent logistic regression models multi class problems relationship hold 
mixtures multinomial logistic regression models studied past mcfadden train david kenneth 

conditional maxent model consider problem estimating distribution cjd discrete valued class variable vector observations presence constraints distribution 
de ne constraints represent vector set general real valued features 
typically allow class characterized separate set features 
vector observations class label set features classes formal de nition feature class follows nigam value th component vector example text classi cation task document word surgery frequency word surgery document medicine class label follows omit simplify notation emphasize separate set features class 
constraint feature prescribes empirically observed frequency feature equal expectation respect model cjd cjd runs features class label vector left hand side equation represents expectation normalization factor feature respect distribution cjd right hand side expected value normalization factor feature training data 
set features supplied maximum entropy objective function shown lead form conditional maxent model jelinek cjd exp sc normalization constant ensuring distribution sums 
follows drop subscript simplify notation 
exist ecient algorithms nding parameters set equations generalized iterative scaling darroch ratcli improved iterative scaling della pietra 

mixture conditional maxent models pointed advantageous assume data points generated set clusters cluster described distribution cjd cjd prior probability cluster cjd maximum entropy form cjd exp derive generalized em algorithm nding parameters appendix update equations maximum likelihood estimates parameter values 
treatment map estimates obtained imposing gaussian prior chen rosenfeld appendix step nd posterior distribution clusters cluster jd jd step maximize likelihood nding new values parameters cluster memberships obtained step new jdj cluster jd new old small step direction gradient log likelihood ensuring likelihood increases indicator function 
discuss appendix nding exact values parameters maximize likelihood dicult requires solving system non linear equations 
gem algorithm converge sucient likelihood increases mclachlan krishnan 
employ form generalized em algorithm single step gradient ascent parameters step 
worst case time complexity algorithm iteration jdj achieved computation straightforward speed achieved noticing feature values equal making corresponding terms right hand side update equation vanish 
show section sparse data speed ups quite signi cant 
speed ups achieved employing goodman explored direction 

experimental results ran experiments publicly available data sets 
names parameters data sets table 
parameters report number classes nc number features table 
parameters data sets experiments 
number features attributes jdj number training records nc number classes snc jdj product previous columns represents major factor time complexity reported log scale sparsity percent data entries values total jdj 
experiments webkb data conducted subsets attributes containing frequent attributes sparsity index subset reported 
name jdj nc log snc jdj sparsity webkb webkb webkb webkb letter recognition yeast ms web vehicle vowel cover segmentation number data records jdj 
note mentioned previous section product quantities factors number mixture components worst case time complexity algorithm 
report logarithm product fth column show anticipated order magnitude complexity 
column shows sparsity data set ects time complexity 
imagines data organized matrix columns corresponding features jdj rows corresponding data records sparsity reports percentage entries matrix 
mentioned higher sparsity time ecient algorithm webkb data craven contains set web pages gathered university computer science departments 
classes di erent numbers frequent words 
letter recognition yeast ms web vehicle vowel data sets downloaded uc irvine machine learning repository blake merz 
ms web data set predicted user visited free downloads web page rest navigation microsoft com 
remaining data sets solved classi cation task posted uc irvine web site 
data sets experimented components 
split data sets training data held data test data 
heldout data determine training choose best number components 
training mixture model stopped gem algorithm relative increase log likelihood held data 
random restarts gem reduce uence starting point initialization 
best model starts selected classi cation performance held data 
mixture model standard maximum entropy model smoothed 
performance statistics measured include classi cation accuracy log likelihood test set time taken learn model 
smaller vehicle vowel data sets performed respectively fold cross validation averaged results 
table reports results webkb data set 
rst column shows number top frequent attributes training remaining columns labeled number mixture components 
row block table reports classi cation accuracy test data loglikelihood test data time taken train model 
boxed number represents classi cation accuracy best model selected held data 
notice classi cation accuracy best mixture model boxed better accuracy standard maxent selected attribute subsets 
size attribute subset increases accuracy models increases improvement provided mixture smaller 
notice larger attribute subset sizes log likelihood scores mixture model note optimization problems tting standard component maxent likelihood surface mixture may local maxima 
table 
webkb data set performance mixture maxent models components compared standard component maxent model 
stands accuracy test data log likelihood score test data time taken learn model seconds 
di erent row blocks table correspond selecting top attributes data set respect frequency 
boxed number classi cation accuracy best mixture model selected held data 
attributes typically slightly better standard maxent model necessarily translate improvement classi cation 
observed similar phenomenon data sets results report 
time taken train mixture grows roughly linearly number mixture components 
times manageable experiments suggest uncovering potential mixture structure obtaining improvement classi cation worth spending extra time 
furthermore employ advances speeding maximum entropy learning goodman alleviate complexity associated learning time 
recall table demonstrated sparsity webkb data increases growth size attribute set learning 
table turn shows sparse data leads sublinear complexity growth 
instance expect time complexity tting component mixture attributes roughly times higher attributes actual number roughly twice high seconds attributes versus seconds attributes due inherent sparsity data ability take advantage 
table results similar table data sets webkb 
clearly see improvement provided mixture comparison standard maxent loglikelihood scores classi cation accuracy test data 
suggests cases mixture model adequate model data standard maxent model better job capturing structure contained data 
improvement varies depending data set classi cation accuracy ranges fractions percent vehicle data set percent vowel data set 
average accuracy improvement selected models boxed values tables component model 
con dence interval improvement percentage table statistical test improvement observe signi cant greater con dence level test 
algorithms decision trees neural networks reportedly achieve better performance data sets focus solely presenting improvements resulting mixtures single component maxent model 
set experiments con rms previous observation actual time complexity strongly depends sparsity data 
looking complexity terms table expect time performance letter recognition cover data sets roughly 
cover data set substantially sparse results order magnitude decrease actual training time di erence 
conclude mixture maximum entropy models provides valuable modeling tool power exceeding regular maxent 
mixture capable better capturing underlying latent structure data structure exists 
increases modeling power comes expense table 
performance mixture maxent models components compared standard component maxent model various data sets uci repository 
stands accuracy test data log likelihood score test data time taken learn model seconds 
boxed number classi cation accuracy best mixture model selected held data 
name letter recognition yeast ms web vehicle vowel cover segmentation higher time needed model 
data sets ran experiments actual cpu times manageable reduced employing published speed techniques maximum entropy goodman 

methodology classi cation exploits latent structure data mixture maxent models 
de ned mixture maximum entropy models derived generalized em algorithm solving corresponding optimization problem 
experiments publicly available data sets suggest mixture maxent models provide signi cant improvement standard maximum entropy model 
update equations gem algorithm case mixture maximum entropy models smoothed gaussian prior 
idea employing mixture maximum entropy models uncover exploit latent structure data easily generalized domains sequence prediction pavlov pennock chemical naming internet user disambiguation 
baker mccallum 

distributional clustering words text classi cation 
proceedings st acm international conference research development information retrieval sigir pp 

acm press new york 
berger della pietra della pietra 

maximum entropy approach natural language processing 
computational linguistics 
blake merz 

uci repository machine learning databases 
ungar 

maximum entropy methods biological sequence modeling 
pp 

chen rosenfeld 

gaussian prior smoothing maximum entropy models technical report 
carnegie mellon university 
craven dipasquo freitag mccallum mitchell nigam slattery 

learning extract symbolic knowledge world wide web 
proceedings th conference american association arti cial intelligence aaai pp 

aaai press menlo park 
darroch ratcli 

generalized iterative scaling log linear models 
annals mathematical statistics 
david kenneth 

mixed logit repeated choices households choices appliance eciency level 
review economics statistics 
della pietra della pietra la erty 

inducing features random elds 
ieee transactions pattern analysis machine intelligence 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
goodman 

sequential conditional generalized iterative scaling 
association computational linguistics annual meeting 
jaakkola meila jebara 

maximum entropy discrimination technical report mit 
jaynes 

stand maximum entropy 
maximum entropy formalism pp 

cambridge ma mit press 
jelinek 

statistical methods speech recognition 
cambridge 
ma mit press 
mcfadden train 

mixed mnl models discrete response technical report department economics uc berkeley 
mclachlan basford 

mixture models 
marcel dekker new york 
mclachlan krishnan 

em algorithm extensions 
john wiley sons new york 
nigam la erty mccallum 

maximum entropy text classi cation 
ijcai workshop machine learning information filtering pp 

nigam popescul mccallum 
unpublished commercial project 
latent structure document collection improve text classi cation 
whizbang 
labs pittsburgh 
pavlov pennock 

maximum entropy approach collaborative ltering dynamic sparse high dimensional domains 
proceedings neural information processing systems nips appear 
pavlov smyth 

probabilistic query models transaction data 
proceedings seventh acm sigkdd international conference knowledge discovery data mining pp 

new york ny acm press 
pereira tishby lee 

distributional clustering english words 
meeting association computational linguistics pp 

ratnaparkhi 

maximum entropy model part speech tagging 
proceedings conference empirical methods natural language processing 
somerset new jersey association computational linguistics 
wang rosenfeld zhao 

latent maximum entropy principle 
ieee international symposium information theory isit 
wol nger connell 

generalized linear mixed models pseudo likelihood approach 
journal statistical computation simulation 
appendix em algorithm mixture maxent models log likelihood training data generated classes represented vectors observations log log jd jd equation 
assuming simplicity priors parameters objective maximize log likelihood equation subject constraint 
setting lagrange function di erentiating respect yields jd jd standard trick setting em procedure introduce posterior distribution clusters de ne cluster jd jd de nition derivative lagrangian equation rewritten log jd performing di erentiation second term summation yields log jd indicator function 
de nition equation results expression derivative exp sc substituting result equation equation obtain log jd exp sc jd substituting result equation equation yields system equations critical points log likelihood jd note em algorithm converge su cient step direction gradient step proceed step mclachlan krishnan 
suciently small constraints features classes gradient ascent follows jd new old case mixture model directly consider lower bound 
chen rosenfeld nigam set 

case derivation goes lines chen rosenfeld result update equation log jd derivation update equation mixture weights follows steps results rule new jdj 
