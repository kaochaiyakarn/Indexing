large margin methods label sequence learning altun thomas hofmann department computer science brown university providence ri altun cs brown edu th cs brown edu label sequence learning problem inferring state sequence observation sequence state sequence may encode labeling annotation segmentation sequence 
give overview discriminative methods developed problem 
special emphasis put large margin methods generalizing multiclass support vector machines adaboost case label sequences 
experimental evaluation demonstrates advantages classical approaches hidden markov models competitiveness methods conditional random fields 

problem labeling annotating segmenting observation sequences omnipresent areas natural language processing speech recognition information retrieval computational biology 
prominent examples include part speech tagging named entity classification information extraction continuous speech recognition secondary protein structure prediction 
formally problems type cast problem inferring label state sequence 
observation sequence 

predominant formalism modeling label sequences hidden markov models hmms variations thereof 
despite success hmms major shortcomings 
hmms typically trained non discriminant manner maximum likelihood estimation joint sampling model observation label sequences 
second efficient inference learning setting requires questionable conditional independence assumptions 
problem poses challenge finding appropriate objective functions alternatives loglikelihood closely related application relevant performance measures 
second problem developing powerful architectures example allowing direct dependencies label past observations overlapping features efficiently handling higher order combinations input features 
time address shortcomings sacrificing benefits hmms offer dynamic programming formulation decoding inference learning 
focus supervised learning framework assume set labeled training sequences available desired mapping learned 
label sequence learning thought natural extension supervised classification 
particular generalizations competitive large margin methods classification support vector machines svms adaboost problem label sequence learning 

learning architectures setting learning architecture specifies family parameterized discriminant functions assign numerical score pair observation label sequences 
think measuring compatibility observation sequence label sequence discriminant function induces mapping arg max ties arbitrarily broken 
restrict attention discriminant functions linear feature representation 
general functional form 
remaining crucial ingredients architecture extracted features statistics concreteness assure hmm architecture handled special case setting 
hmms extract types features sequence pair 
type features deal label label interactions neighboring labels denote states denotes indicator function enclosed predicate 
features simply count particular combination labels occur neighboring sites 
second type features conjunctively combine input attributes states 
example input described attributes possible states may extract total features type combining input attribute state 
ways designing powerful learning architectures 
may include direct dependencies type eq 
label variable input features example 

called overlapping features input feature included multiple statistics 
second kernel architectures may possible efficient explicit input attributes data may represented kernel functions need methods dual representation data enters values pairs observations 

loss functions risks single objective function label sequences preferable situations choice depend specific application 
consequently discuss number reasonable alternatives 
generalization standard classification loss label sequences define empirical risk training instances zo nx xi yi 
second risk function consider rank loss measures fraction incorrect label sequences ranked higher correct :10.1.1.4.5119:10.1.1.156.2440
order account varying sequence lengths include weights ti sequence ti length th sequence rk nx ti xi xi yi 
expect rank loss scale exponentially sequence length investigated weighting functions log ti ti log experiments 
lastly hamming risk measures zero loss individual labels reduces standard empirical misclassification risk sequential nature data ignored hm ti nx xi xi :10.1.1.4.5119:10.1.1.156.2440
risk functions discontinuous generally difficult optimize 
minimizing empirical risk sufficient ensure generalization performance 
methods discussed sequel understood minimizing upper bound risk functions possibly combined regularization term 

conditional random fields conditional random fields crfs considered state art label sequence learning 
crfs natural generalization logistic regression label sequences 
starting point define conditional probability label sequence observation sequence exp normalization constant 
interpret weights canonical parameters sufficient statistics conditional exponential family 
number algorithms proposed estimate maximizing conditional likelihood equivalently minimizing log nx log yi xi 
include iterative scaling various flavors conjugate gradient descent second order methods approximation methods voted perceptron 
usually regularization term proportional squared norm added resulting penalized likelihood criterion 
negative log likelihood provides upper bound empirical zero risk proposition 
zo log log proof 
distinguish cases xi yi maxy yi xi case log xi yi log log yi xi 
ii xi yi maxy yi xi case log xi yi log log yi xi yi xi summing completes proof 
modification logarithmic risk marginal probabilities individual label variables estimating mg ti nx xi log defines upper bound hamming risk hm uses pointwise decoding function proposition 
arg max pr bound holds log hm mg 
proof 
omitted analogous proposition 
standard numerical optimization procedures conjugate gradient optimize mg computation gradient carried efficiently dynamic programming shown 

hidden markov svms generalization svms label sequence learning 
step propose generalize multiclass separation margin define margin achieved instance max :10.1.1.69.8716
notice implies correct label sequence receives highest score 
general want score correct output maximal larger second best output margin measures 
propose choose maximizing minimal margin arg max mini xi yi 
notice discriminant function linear feature representation minimal margin achieved margin arbitrary large scaling 
standard trick fixing functional margin equivalently minimize squared norm subject margin constraints 
order accommodate margin violations generalize formulation ways 
may add slack variable training sequence 
soft margin svm problem formulated svm min nx xi yi xi yi notice optimal solution slack variables implicitly determined weights max 
pn proposition 
risk svm bound sequence classification loss 
upper proof 
gets xi yi maxy xi xi yi means data point correctly classified xi yi 
ii automatically upper bound xi yi 
alternative svm introduce slack variable training instance sequence leading similar qp svm slack variables iy max xi yi xi 
provides upper bound rank loss proposition 
pn ti iy rk 
proof 
iy xi yi xi implies ranked lower yi case iy establishes bound 
ii iy bound holds trivially contribution pair xi rk 
comparing svm svm notice expect number active inequalities svm smaller compared svm svm penalizes largest margin violation example 
data dependent empirical assertion great practical relevance led focus sparser svm formulation 
main computational challenge optimizing svm posed extremely large number linear inequalities scaling exponentially length sequences 
reasonably expect tiny fraction inequalities active solution 
propose row selection working set procedure incrementally add inequalities problem 
lines derive dual qp lagrange multipliers iy margin inequality 
max iy iy iy iy denotes binary pseudo label yi 
ki denotes inner product training sequences defined ki xi xj 
important point features just involve single label combine implicit feature representation observation vectors 
yt xt index set features combine input attributes states exploit identity ys yt xs xt 
implications significant allows carry advantages non linear svms label sequence case 
step solving observe constraints couple iy training instance adapt strategy proposed optimize subspace associated particular training instance keeping remaining variables fixed 
secondly propose maintain active set label sequences si instance 
full algorithm shown algorithm 
order perform step best viterbi decoding 
algorithm working set optimization 
si yi repeat 
compute arg maxy xi xi yi xi si si optimize iy si remove si iy converged 
label sequence adaboost second large margin method generalization adaboost label sequence learning 
starting point exponential risk function exp nx ti proposition 
exponential risk upper bound rank risk rnk exp proof 
xi yi xi xi xi yi 
ii xi xi yi performing weighted sum instances label sequences completes proof 
way minimizing exponential loss eq 
gradient methods outline derivation boosting algorithm generalizes ada boost algorithm multiclass classification :10.1.1.4.5119
identify set possible super labels xi define sequence distributions dr xi pairs recursively follows dr dr zr 
denotes feature selected th round corresponding update increment zr normal ti ization constant 
initialize ti tj rounds boosting parameters vector pr 
proposition 
number rounds rk zr 
proof 
theorem may greedily optimize upper bound selecting round feature leading minimal zr 
discussed parallel computation zr dynamic programming usually inefficient 
propose compute upper bound zr upper bounds selecting features round boosting 
basic idea inequality valid leading bound linear results zr ake min ik ak max ik max ik ak dr max ik xi max min min ik ik upper lower bound value feature xi taken left hand side eq 
minimized analytically respect give tightest bound 
index achieving smallest upper bound selected 
point quantities involved ak computed features simultaneously single dynamic programming run sequence 
algorithm label sequence adaboost ti initialize ti xi yi initialize 
perform dynamic programming compute ak select minimizing upper bound eq 
compute optimal increment update weight update dr eq 

applications experiments report experiments applications named entity recognition ner part speech tagging pos 
task generated sub corpus consisting sentences spanish news wire article corpus provided special session limited conll ner 
label set corpus consist non name continuation person names organizations locations miscellaneous names resulting total different labels 
tagging application extracted corpus consisting sentences penn treebank corpus 
total number function tags 
input features simple binary features 
features indicator functions word occurring fixed size window centered word labeled 
addition features encode morphological properties spelling features 
table summarizes experimental results 
trained standard hmms hmms overlapping features 
seen discriminative methods outperform hmms 
hm svms achieve best accuracy values 
boosting performs somewhat worse methods advantage lead sparse solution may additional advantages real time settings 

surveyed discriminative methods label sequence learning generalizations large margin methods svm adaboost 
methods combine advantages large margin methods elegance efficiency hmms 
experiments prove competitiveness methods benchmark sets 
currently working large scale experimental evaluation 
ik hmm crf mrf svm exp boost ner os table prediction accuracies various method 
hmm hidden markov model overlapping features brackets crf conditional random fields mrf marginal random fields minimizing mg svm hidden markov support vector machine exp minimizing exponential loss exp boost generalization adaboost 
acknowledgments sponsored nsf itr award number iis 

schapire singer improved boosting algorithms confidence rated predictions machine learning vol 
pp 

altun hofmann johnson discriminative learning label sequences boosting advances neural information processing systems nips 
lafferty mccallum pereira conditional random fields probabilistic models segmenting labeling sequence data proc 
th international conf 
machine learning 
morgan kaufmann san francisco ca pp 

wallach efficient training conditional random fields master thesis university edinburgh 
sha pereira shallow parsing conditional random fields proceedings human language technology naacl 
minka algorithms maximum likelihood logistic regression cmu department statistics tr tech 
rep 
collins discriminative training methods hidden markov models theory experiments perceptron algorithms emnlp 
chen rosenfeld gaussian prior smoothing maximum entropy models carnegie mellon university tech 
rep 
teh roweis alternate objective function markovian fields icml 
weston watkins support vector machines multi class pattern recognition proceedings european symposium artificial neural networks 
crammer singer algorithmic implementation multiclass kernel vector machines journal machine learning research vol 
pp 

friedman hastie tibshirani additive logistic regression statistical view boosting stanford statistics department tech 
rep 
