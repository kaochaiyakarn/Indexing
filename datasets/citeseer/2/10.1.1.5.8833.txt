boston university college engineering thesis language modeling sentence level mixtures iyer electrical engineering university bombay bombay india 
submitted partial fulfillment requirements degree master science fl copyright iyer acknowledgments acknowledge advisor professor mari ostendorf 
thesis possible constant guidance encouragement 
year half productive period research 
acknowledge readers dr lev dr robin rohlicek insightful suggestions ideas better thesis 
kelly linda great deal easier 
great pleasure colleagues 
owen introducing organized coding style fred sanjay helping find bugs exist patiently hearing am burnt crazy day hacking 
helping improve presentations especially fred sanjay borne practice sessions 
am looking forward couple years 
wouldn leave machines especially long standing companions year half 
acknowledge friends singh mani chandra fun filled years country 
times needed help happy times shared 
owe lot sanity happiness 
confidence self esteem time low 
acknowledge family help support encouragement wouldn am 
research supported onr contract number 
iv language modeling sentence level mixtures order 
iyer boston university college engineering major professor mari ostendorf associate professor electrical computer systems engineering language models play important role improving accuracy continuous speech recognizer 
thesis introduce new statistical language model captures long term topic dependencies words sentences 
model includes main contributions 
develop topic dependent sentence level mixture language model takes advantage topic constraints sentence paragraph 
language model markov large search space stage multi pass search strategy recognizer 
second introduce topic dependent dynamic adaptation techniques framework mixture model 
course thesis investigate robust parameter estimation techniques extremely important light sparse data problems language modeling 
model implemented bu speech recognition system provides significant improvement recognition accuracy 
important advantage framework model simple extension existing language modeling techniques easily integrated language modeling advances 
contents background approaches language modeling goodness language model sparse data dynamic language modeling dynamic cache dynamic mixtures mixture weights adapted modeling long distance dependencies trigger language models maximum entropy approach decision trees clustering word histories extended grams summary paradigm speech recognition stochastic segment model vi best rescoring general strategy corpus lexicon resources training evaluation corpora lexicon development evaluation paradigm static mixture language model automatic clustering topics parameter estimation iterative topic model re estimation topic model smoothing implementation details experiments baseline static mixture model dynamic mixture model mixture weight adaptation gram adaptation single gram adaptation mixture gram adaptation implementation details experiments dynamic mixture weight adaptation dynamic cache adaptation vii summary summary suggestions expectation maximization algorithm viii list tables perplexity arpa lm development test set recognition performance arpa development test set turing vs witten bell 
recognition results arpa development evaluation test model implemented set intersection similarity measure viterbi training 
recognition results arpa development evaluation test model implemented inverse document frequency similarity measure viterbi training 
recognition results arpa development evaluation test model implemented inverse document frequency similarity measure em training 
recognition results arpa development evaluation test set intersection similarity metric inverse document frequency similarity metric includes knowledge sources 
perplexity measurements recognition results arpa development evaluation test sets dynamic adaptation sentence level mixture weights 
ix perplexity recognition results arpa development evaluation test sets single component lm 
perplexity recognition results arpa development evaluation test sets component lm 
combined model comprises fractional counts absolute counts perplexity measurement recognition results arpa development evaluation test dynamic cache adaptation content word unigram cache interpolated bigram trigram cache dynamic sentence level mixture weight adaptation includes knowledge sources 
recognition results static language models arpa development evaluation test sets 
perplexity measurement recognition results arpa development evaluation test dynamic cache adaptation content word unigram cache interpolated bigram trigram cache dynamic sentence level mixture weight adaptation 
list figures speech recognition problem terms communication theory 
example parse tree sentence np noun phrase verb cn conjunction noun vp verb phrase 
schematic best paradigm 
xi chapter aim automatic speech recognition accurately transcribe natural speech text 
speech recognizer serves human machine interface example spontaneous dictation transcriber 
futuristic applications see speech recognizer significant player information highway 
achieve goals problems overcome ranging speech speaker variability domain independence real time response 
ongoing efforts deal problems areas connected speech including prosody acoustic language modeling 
areas statistical modeling approaches popular part successful recognition systems today incorporate statistical approach 
example statistical language models model probability different word sequences past years integral part state art speech recognizers 
thesis introduce new statistical language model provides significant improvement recognition accuracy implemented framework boston university speech recognition system 
performance large vocabulary continuous speech recognizer greatly impacted additional information provided stochastic language model 
need language model recognizer immediately seen simple example 
suppose input speech recognizer utterance go room 
possible output acoustic segment recognizer go room 
go room 
go room 
acoustic model able differentiate 
need kind grammar able confidently pick sentence recognized output 
feel simple syntactic grammar able job 
syntax formalized linguists far complete grammar english 
current grammars prove rigid different applications recognition system 
problem humans necessarily speak grammatically 
may awkward phrasing abbreviated structures depend context dialog assume corresponding knowledge listener listener computer 
example air travel information systems atis application basically consists customer queries requests flights fares corresponding computer responses 
simple customer request point dialog computer may round trip denver please 
characterize sentence grammatical sense complete clause noun phrase verb phrase 
applications appropriate overlook correctness grammar sentence simply extract information requested 
deal problems requirements statistical language model suggested 
statistical language model different variations proved important components recognizer 
models consider history previous gamma words estimate probability nth word referred gram language models 
typically due memory computation requirements value restricted bigram trigram language model respectively 
models constrained inability take advantage long distance dependencies greater sentence paragraph 
types long distance constraints illustrated 
grammatical constraints grammatical constraints simple agreement singular plural agreement sentence 
flood waters ran low people built new homes simple example verb tense agreement 
distinction built build acoustic model capability current standard trigram language models 
topic related dependencies illustrate dependency examine error recognition system recognizing utterance standard arpa trigram model recipient joseph webster junior phi beta kappa chemistry grad plans take courses fall art religion music political science 
output recognizer recipient joseph webster junior friday cap chemistry plans take courses fall aren really chin music political science context courses student art religion relevant aren really chin 
current standard trigram model unable take advantage long distance topic related dependencies 
speaker goal style related constraints kind dependencies observed cases human computer interactions simple dictation 
differences language queries information responses information clarification 
clearly reflected case atis corpus 
corpus customer queries answers queries 
queries kind show flights boston denver 
fare code mean 
responses clarifications kind wednesday pm 
round trip please 
efforts stochastic language modeling deal problem long distance constraints approaches outlined underlined words recognized vocabulary 
chapter 
important advance direction dynamic adaptation language model partially observed document 
example consider small clip news article regarding taxes real danger texas economy tax increases soon attacked inadequate 
politicians call creation state personal corporate income tax 
panel appointed recommendations year changes state tax structure 
seen word tax prominent usage clip 
dynamically adapting language model partial document automatically raise frequency word tax 
considered equivalent tracking topic article take advantage short term fluctuations word frequencies 
existing dynamic language models useful tracking dependencies article capturing dependencies sentence 
thesis introduces sentence level mixture model existing techniques statistical language modeling time extends capability standard models take advantage long distance topic related constraints sentences 
incorporate dynamic language modeling techniques framework mixture model adapt fluctuations word frequencies capture dependencies sentences 
rest thesis organized follows 
chapter previous language modeling speech recognition emphasis statistical language models discussed 
follow description speech recognition paradigm language model training corpus chapter 
chapter describes baseline mixture model experimental results obtained course initial thesis 
chapter provides detailed description dynamic adaptation mixture model partially observed document 
chapter outlines significance thesis important drawn 
chapter background statistical techniques speech recognition introduced try model speech information source word sequence transmitted noisy channel vocal tract signal processor shown 
speaker signal processor speech recognizer text text noisy channel speech recognition problem terms communication theory 
mathematical terms denote sequence words 
acoustic signal represented goal speech recognition find word string maximizes probability ja argmax ja bayes formula recognition problem formulated argmax acoustic channel model priori probability word string language source model 
attempt maximize probability ja efficiently depends search algorithms dependence assumptions model 
different approaches language modeling evolved years approaches outlined section 
quantitative evaluation significance statistical language model described section 
mixture model statistical techniques language modeling different problems encountered statistical modeling solutions developed specifically deal problems language modeling discussed section 
section outlines previous area dynamic language modeling language model adaptation important component model 
mixture model aims take advantage long distance topic related dependencies sentences short term fluctuations word frequencies 
efforts model long distance dependencies outlined section 
section summarize main issues relevant thesis 
approaches language modeling syntactic approach language modeling originated statistical techniques mainly natural language understanding 
statistical language models combined successfully acoustic models build accurate recognizers 
section describe syntactic statistical approaches attempts combine approaches 
syntactic language models language models held humans far information simple statistical models 
human model includes knowledge syntax knowledge discourse semantics involved 
syntax theoretically formalized linguists 
syntactic rules developed years speech understanding systems 
example rule sentence parsed noun phrase np verb phrase vp np 
typically technique develop syntactic parser may parse sentence follows 
example dr chang died dr lee appointed chairman 
may parse tree illustrated 
problem incorporating syntactic parsers recognizer ungrammatical sentence unanticipated sentence assigned probability zero 
people speak ungrammatical utterances combined knowledge context semantics perfect sense 
complete formal grammar english reasonable sentences anticipated grammar 
syntax powerful tool successfully incorporated statistical models explained 
statistical language models commonly statistical language modeling technique consider word sequence markov process 
markov process satisfies vp np dr chang died np vp appointed cn dr lee chairman example parse tree sentence np noun phrase verb cn conjunction noun vp verb phrase 
markov property gamma gamma jx gamma gamma jx gamma case word sequence discrete time discrete valued jw gamma gamma jw gamma jw gamma equation represents bigram language model 
idea word sequence markov process extended consider history previous word example trigram language model jw gamma gamma jw gamma gamma strictly speaking markov process referred gram language model 
words case gram models idea predict occurrence word history previous gamma words 
greater history considered greater value better predictive power gram language model 
gram model generally trained large training text produces reasonable non zero probabilities words vocabulary considered speech recognition smoothing techniques 
greater amount training data better model 
value training data increases model exceeds virtual memory computers probabilities difficult access 
example observed particular words text data wall street journal corpus word vocabulary number unique grams number grams number grams 
date value typically restricted large vocabulary tasks 
combined language models simplest combined model case syntactic classes word 
probability sentence bigram model computed jc jc jc gamma represents syntactic class length sequence classes 
model considered hidden markov model hmm 
case hidden markov model observation probabilistic function sequence states sequence markov process 
said hidden underlying markov process observable observed stochastic process 
case combined model referred equation observations words hidden states syntactic classes 
probability computed iterative procedure known forward algorithm :10.1.1.131.2084
previous subsection considered simple example parsing sentence context free grammar 
rules word tagged syntactic class deterministic usually rules derived hand 
seen stochastic context free grammars rules probabilities associated 
rules derived hand estimated complicated algorithms 
goodness language model different statistical language models need compare different models 
brings question determine language model better 
measure goodness language model concepts information theory referred perplexity 
perplexity defined entropy word sequence representative language represents base logarithm typically definition entropy 
entropy random variable takes values xl probability xl respectively entropy random variable defined gamma log suppose take values value equally probable entropy amount information equal bit 
bit sufficient characterize information 
uncertainty predicting occurrence value maximum 
values probable entropy lower number bits required represent information reduced 
occurrence value predicted greater certainty 
means random variable equiprobable values greater entropy correspondingly greater uncertainty random variable values occurring unequal probabilities possibly greater entropy random variable values unequal probability 
extending equation considering random variable consider ergodic random process generates sequence symbols entropy random process defined gamma lim gamma log sequences typical process single long sequence able convey information set sequences property ergodicity 
gamma lim gamma log relate language model 
perplexity language model considered random process model represents different words random process generate sequence words entropy language model gamma lim gamma log word equiprobable occurrence word independent entropy maximum uncertainty occurrence word maximum 
relate occurrence word previous word may able predict occurrence greater certainty entropy lowered 
human language greater number previous words considered lower entropy 
example word seen sample text event number possible words follow event large equally probable 
previous history words considered event probability word higher number choices words consideration smaller 
language model thought information content source chooses words vocabulary size perplexity refer equation 
greater number equiprobable choices branching factor greater difficulty recognition system 
principle lower perplexity language model better language model 
problem perplexity take account acoustic similarity words 
words large branching factor problem words acoustically different 
measures proposed speech decoder entropy try relate language model performance acoustic model considered 
perplexity important measure purposes speech recognition accuracy 
sparse data perplexity decreased making better prior word sequences model 
terms gram approach greater history considered predicting word lower perplexity 
typically value restricted 
reasons discussed earlier number unique grams increases exponentially increases 
reason difficult get reasonable estimates valid grams unobserved occur times training text 
word classification viewed way decreasing number unobserved grams 
syntactic categories evolved linguists robust manner word classification discussed earlier 
syntactic categories semantic word equivalences 
equivalence classes manually chosen domain specific 
approach consider statistical equivalence classes word history gamma gamman mapped equivalence class gamma gamman iterative algorithm maximize likelihood training data 
problem sparse data inherent bigram trigram model word class grams 
case recognizer comes unobserved bigram trigram language model afford assign word sequence probability zero may perfectly valid sequence 
approaches proposed estimating probability unseen grams described 
approaches try deal problem sparse data coming reasonable estimates unobserved grams statistics observed grams 
turing back scheme basic idea scheme reduce unreliable probability estimates distribute probability mass obtained reduction unseen grams 
maximum likelihood ml estimate gram relative frequency estimate 
consider ml estimate unreliable reduce estimate replacing turing estimate basically scales ml estimate discount coefficient 
probability estimates sum reduction leaves probability mass distributed unseen grams lower order gram estimates 
mathematically illustrating scheme bigram language model represent number bigrams training occurred times assuming gamma represent total number bigrams rb maximum likelihood estimate bigram probabilities relative frequency estimates pml count bigram training text 
turing estimate factor called discount coefficient 
relating conditional bigram probabilities case bigram observed training text jw discount coefficient count training text 
bigram observed distribute remaining probability mass proportion unigram counts jw gamma jw decide consider counts higher reliable reduce ml estimates grams counts jw gamma gamma case unobserved bigram equation 
experiments shown choice 
technique useful discount coefficients calculated earlier required 
turing back scheme typically standard arpa gram models 
witten bell back scheme problems turing back estimation prompted modified backing technique witten bell adaptive text compression 
particular turing technique overly complex robust 
problem assumption number grams occur times greater number grams occur times training assumption may hold training texts 
modified back technique suggested observed probability estimates reduced adding small factor denominator subtracting probability mass observed probability estimates 
example number unique words seen context conditional probability bigrams observed context jw bigram probability previously unseen word occurring context jw unigram probability word probability mass obtained reducing observed gram probabilities distributed unseen grams proportion lower order estimates 
interpolation techniques interpolation method concept relying general distributions specific distributions unreliable 
interpolated model developed computing probability estimate normalized combination specific distribution general distribution 
consider example bigram model 
bigram probabilities constitute specific distributions 
specific distributions unreliable example unseen bigrams probabilities zero consider general distribution tying bigram states observed context 
general distribution unigram estimate particular context 
making hard decision specific distribution general distribution case back estimates interpolated model combined estimate jw jw gamma basic interpolation predict unseen data need estimated data training gram models 
estimate interpolation parameters expectation maximization em algorithm held data set refer appendix 
summary interpolation techniques described compared back techniques turing back estimation back estimation bigram trigram models 
results reported section 
due simplicity performance witten bell approach mixture model 
considered interpolation technique aspects model 
dynamic language modeling language models discussed far estimate gram probabilities large text corpus serves training base 
probabilities remain fixed static recognition 
intuitively know see document recognizing certain grams frequent usage reflected static language model especially language general may change time 
example lm training data years reflect bigrams associated current events health care plan civil war bosnia news individuals hillary clinton fidel castro 
idea dynamic language model adapt static model document seen far 
different approaches dynamic language modeling referred language model adaptation outlined approaches 
dynamic cache concept dynamic cache store cache words document seen far 
referred window length 
idea implemented bigram models 
static bigram model second bigram model built partial document cached 
word observed belong cache dynamic model 
belong cache combination static dynamic model 
similar approach taken unigram bigram trigram probabilities estimated cache 
static dynamic trigram model linearly smoothed explained previous subsection 
probability word document computed jw gamma gamma dynamic jw gamma gamma gamma static jw gamma gamma dynamic jw gamma gamma dynamic jw gamma gamma dynamic jw gamma dynamic static jw gamma gamma static jw gamma gamma static jw gamma static interpolating parameters 
interpolating parameters fixed determined experiments em algorithm text training testing purposes 
alternatively interpolation approach handling sparse data back techniques 
similar approach taken caches maintained part speech pos 
words partially observed document stored respective pos cache 
dynamic component adjust probability predicting word class jc 
techniques gave significant reduction perplexity improved recognition accuracy 
dynamic mixtures mixture weights adapted case dynamic mixtures concept adapt changing text styles 
humans domain knowledge sense spoken 
similarly language model tuned dynamically adapted topic discussed 
approach developed kneser steinbiss uses bigram models estimated different text corpora relating different topics example newspaper text scientific writing topic specific models contributes interpolated model uses standard model bigram probability jw gamma estimated mixture topic dependent bigrams 
jw gamma jw gamma refers specific topics refers bigram estimate topic 
interpolation parameters dynamically adapted partially seen document 
equal values iteratively estimate values previous words cached equation new old jw gamma old jw gamma addition topic specific models general model trained training data models interpolation 
typically observed general model higher topic specific models general model greater training base robust estimates 
modeling long distance dependencies gram approach proved simple implement effective recognition system 
typically value restricted due reasons discussed earlier 
low order dependencies able take advantage long distance dependencies sentence 
attempts overcome disadvantages long distance dependencies sentence paragraph outlined 
trigger language models maximum entropy approach rosenfeld colleagues try long distance dependencies defining referred trigger pairs 
presence words significantly correlated example market prices weather sun referred trigger pair word trigger second word trigger observed 
trigger pairs selected mutual information words 
trigger pairs information pairs combined static gram model maximum entropy principle mep 
mep basically constructive criterion setting probability distributions partial knowledge 
biased estimate possible partial knowledge 
partial knowledge case trigger pairs gram statistics 
reformulated constraints expectation different functions probability distribution satisfies constraints highest entropy selected 
main disadvantage trigger lm computational complexity training models recognition 
advantages different knowledge sources easily incorporated model redefining constraints satisfied mep 
example idea dynamic cache explained previous section incorporated formulating constraint making self triggers idea word just observed greater likelihood seen 
decision trees clustering word histories way looking gram models consider map previous history word gamma previous words 
words consider history word equivalent previous gamma words case 
consider greater histories effectively consider greater values naive equivalence class approach decision trees determine equivalence classes 
bahl applied binary decision trees language modeling problem tried take advantage longer histories 
nonterminal node tree question requiring answer 
leaves tree equivalence classes 
objective asking questions reduce uncertainty predicting word questions minimize average entropy leaf selected 
order prevent tuning tree training data training data split sets 
questions minimize entropy set minimize entropy second set discarded 
distinct advantage technique restrict predictor words 
predictor parser information semantic information depending kind questions asked nonterminal nodes 
disadvantage approach requires massive computation construct tree significantly larger search space handle longer term dependency 
extended grams approach modeling long distance dependencies basically extends concept gram words immediately precede appropriate words precede 
wright extended bigrams jw choose useful conditioning word window complex normalization techniques obtain valid probability distribution 
window includes words sentence 
useful word chosen relative information measure 
technique split vocabulary content words function words 
function words closed class words little semantic content content words usually nouns adjectives words occur frequently function words 
extended bigrams case predict word previous content word word content word function word case word function word 
summary chapter discussed previous area statistical language modeling 
described standard gram models problems encountered training testing models 
robust parameter estimation important issue light sparse data training gram models discussed detail smoothing techniques current gram models 
experimented techniques witten bell turing back estimation 
perplexity recognition results obtained implemented witten bell back technique robust estimation gram probabilities model 
disadvantage standard gram model lies fact unable take advantage topic related long distance dependencies sentences 
problem representing long distance dependencies explored earlier stochastic language models 
language model adaptation addresses problem inhomogeneity gram statistics really represents task level dependencies account dependencies sentence 
context free grammars account sentence dependencies costly build grammars 
techniques suggested computationally expensive training models 
thesis introduce sentence level mixture topic dependent language models existing statistical language modeling techniques time takes advantage topic related long distance constraints sentences making simple variations standard model 
addition show mixture model incorporate dynamic language model adaptation 
chapter paradigm speech recognition different approaches taken acoustic modeling continuous speech recognition 
statistical approach plays important role today successful large vocabulary continuous speech recognition systems 
statistical techniques practical training testing large networks connected word sub word units 
systems words modeled terms smaller subword units called phonemes 
acoustic training data transcribed phonetic string statistics phonemes compiled data 
acoustic model trained transcribe speech waveform string phonemes 
linguistic decoder translates string phonemes words 
hidden markov model hmm widely statistical speech recognition systems acoustic modeling 
order construct word models hmm constructed entire word smaller word units referred phones 
models connected network pronunciation dictionary 
acoustic model bu recognition system referred stochastic segment model ssm statistical model somewhat different hmm 
chapter briefly outlines bu recognition system ssm best rescoring formalism 
conclude chapter description language model training corpus evaluation paradigm 
stochastic segment model alternative approach hmm ssm proposed model considers speech segmental level better capture spectral temporal structure duration phone 
ssm variablelength segment speech considered comprises frames speech 
version ssm uses full covariance gaussian density functions assumes frames segment conditionally independent take complete advantage ssm formalism 
segment speech mapped fixed length representation comprising frames samples linear transformation 
model phone regions 
phone model distribution frames mapped regions model phone 
order take advantage effects phone context articulatory effects context dependent phone models 
words phone modeled conditioning distributions left right phone context 
specifically experiments robust context dependent covariance estimates obtained tying covariance parameters classes left right context clustering techniques described 
segment model trained iterative algorithm acoustic training data segmented current models new parameters estimated maximizing likelihood parameters new segmentations 
phone recognition ssm maximizes probability utterance best search ks ks ks best hypothesis max schematic best paradigm 
phone sequence segmentation 
best rescoring general strategy knowledge sources ks useful recognition system 
approaches combining knowledge sources best rescoring illustrated 
input recognition system speech signal utterance 
search algorithm come best sentence hypotheses utterance 
different knowledge sources rescoring best list 
weighted combination scores reranking hypotheses 
case bbn byblos hidden markov model hmm decoder find best 
typically value experiments 
different knowledge sources rescore top hypotheses include subset stochastic segment model ssm hmm acoustic model bbn bbn trigram language model bbn segmental neural net acoustic scores snn phone duration scores number phones number silences language model developed course 
knowledge sources experiments thesis 
noted bbn scores 
weighted linear combination knowledge source scores log likelihoods counts rescoring weights optimized decrease word error 
weights optimized development set kept fixed test sets 
important observation computationally cheap include language model recognition system context best rescoring paradigm expensive beam stack search primarily due fact model markov 
corpus lexicon resources corpus working past year wall street journal corpus wsj 
boston university research groups corpus 
order compare effectiveness different algorithms variability training test data lexicon training data standardized sites 
training evaluation corpora training corpus language model development available bu phases 
initial training data provided linguistic data consortium ldc referred wsj data 
training comprised words data organized form paragraphs sentence 
mainly comprised business news stock market articles obtained wall street journal period 
wsj data initially designed support verbalized punctuation vp non verbalized punctuation dictation 
parallel corpora developed vp language modeling training recognition evaluation 
vp corpus speakers requested articulate explicitly 
corpus speakers requested pronounce 
second phase data collection vp distinctions dropped optional speaker verbalize punctuation 
speakers exact transcriptions speak wsj data take account speaking style variability 
example referred know speakers may say 
bbn processed wsj data account similar variations 
experiments indicated greater training data gives better language models bbn simple processing wsj text data words officially designated training data data wsj data training language models 
words wsj data training language models 
data sets test data recognition experiments 
referred development evaluation data sets 
course thesis wsj speech transcriptions weight estimation purposes typically training language model sites 
summarize 
total words non verbalized pronunciation training data estimating topic general model parameters 
bbn 

remaining words verbalized punctuation data ing vp model smoothing topic models 

addition wsj speech transcriptions held data set estimating interpolation weights 

development test data estimating weights combining scores best rescoring 

tested models evaluation data set comparable results sites available 
fair evaluation speaker independent recognition system test sets development evaluation test sets different speakers difference average recognition results 
additional language model training data drawn publications sources north american business dow jones information services new york times nyt reuters north american business report los angeles times lat washington post wp 
data basically considers business portions sources amounts gb text data 
data training models thesis late release 
additional training data prove beneficial robust topic dependent models 
lexicon frequent words training corpus culled word list 
subset word verbalized punctuation dictionary vp 
standard dictionary refers words training 
speakers vary say abbreviations acronyms 
example may verbalized spoken 
similarly vs incorporated unquote vs quote 
words added standard vp dictionary training 
additional symbols sentence unknown word 
development evaluation paradigm order compare sites data lexicon year sites report test set referred evaluation set 
typically development set provided new ideas implemented tested test set evaluation test set 
word recognition performance reported terms percent word error 
calculated ratio sum insertions deletions substitutions true number words sentence 
addition standard software exists testing statistical significance difference systems evaluating data 
course evaluated condition reported results range word error trigram language models bu baseline performance 
developed tested model phases 
phase vp dictionary training data referred developing models 
evaluate development test set measure perplexity word recognition error 
order compare approach approaches field obtain fair results estimate score combining weights development set report results evaluation set 
chapter static mixture language model discussed long distance dependencies due inhomogeneous nature language different words sequences particular broad topics tasks 
thesis represent long distance dependencies simple model sentence level mixture component language models identified gram statistics specific topic broad class sentences 
approach similar approach kneser steinbiss important difference approaches kneser steinbiss mixtures gram level consider mixtures sentence level 
probability word sequence jw gamma gamman approach advantage static model described chapter dynamic model discussed chapter easily leverage techniques developed adaptive language modeling 
order compute probability sentence manner suggested equation need consider issues 
chapter describes approach automatically obtain different topic data topics explicitly marked corpora 
elaborate parameter estimation smoothing techniques developed experimented order obtain robust topic gram probabilities mixture weights 
conclude discussion results different experiments description baseline static mixture model development 
automatic clustering topics component distributions language model correspond different topics topic mean broad class sentences 
topics specified hand text labels available heuristic rules associated known characteristics task domain 
topics natural groupings data determined automatically approach taken standard wsj language model training data topic labels associated 
conceptual simplicity agglomerative clustering partition training data desired number clusters topics 
reduce clustering time clustering paragraph level relying assumption entire paragraph comes single topic 
paragraph begins singleton cluster 
paragraph pairs progressively grouped clusters computing similarity clusters grouping similar clusters 
basic clustering algorithm follows 
desired number clusters initial number clusters number singleton data samples paragraphs 

find best matched clusters say minimize similarity criterion ij 
merge decrement 
current number clusters go step 
stage desired number partitions training data 
save computation run agglomerative clustering subsets data continue resulting clusters final set clusters 
similarity measure variety similarity measures envisioned 
initially normalized measure number content words common clusters 
refer measure set intersection measure remainder thesis 
paragraphs comprise function words content words stocks theater trading function words contribute identification paragraph belonging particular topic ignored similarity criterion 
letting set unique content words cluster ja number elements number paragraphs cluster specific measure similarity clusters ij ij ja ja ij theta normalization factor avoid tendency small clusters group large cluster small clusters 
approaches clustering automatic sublanguage identification 
method similarity measure combination inverse document frequencies words 
specifically similarity measure ij ja theta ja jja ja number unique words article ja number articles containing word refer measure inverse document frequency measure remainder thesis 
advantage measure function words discarded high frequencies automatically discount 
discovered obtaining reasonably sized clusters need include normalization factor stated equation 
similarity measure implemented ij ij ja theta ja jja implemented clustering metric obtained similar recognition accuracy metrics shown section 
parameter estimation parameter estimation issues require deal 
topic model parameters estimated training data partitioned different topic data 
consider iterative training options em algorithm viterbi style training technique :10.1.1.131.2084
second obtain topic model estimates need smooth estimates account sparse data non topic sentences 
deal second problem double mixtures sentence level gram level 
section explain training algorithms investigated discuss additional smoothing techniques 
iterative topic model re estimation component model conventional gram model 
initial estimates gram models partitions training data obtained clustering procedure suggested witten bell back technique 
clustering algorithm simple topic parameters may robust 
iteratively re estimate parameters sentences belonging topic clustering metric topic gram models 
re estimate parameters expectation maximization em algorithm 
em algorithm technique finding maximum likelihood parameter estimates observed data way incomplete 
case data incomplete know exactly topic cluster certain sentence belongs 
solution estimate missing information topic find maximum likelihood estimate 
em algorithm developed gram parameter estimation witten bell type back explained detail appendix trigram language model trigram probability jth topic model computed jw ij bcd bcq ij bcq ij bcq represents number training sentences ij likelihood sentence belonging jth topic bcd represents number occurrences trigram ith sentence 
similarly bigram probability jth topic model computed jw ij bc bq ij bq ij bq bc represents count bigram ith sentence 
unigram probability estimated ij ij represents count unigram sentence back probabilities computed em extension witten bell technique bigram back jth topic model estimated backoff bcq ij bcq bcq ij bcq ij bcq similarly unigram back computed backoff bq ij bq bq ij bq ij bq em algorithm computationally intensive initially viterbi style training technique iteratively partitions data topic re estimates topic dependent parameters :10.1.1.131.2084
topic max topic max topic jw gamma gamma topic sentences relabeled topics estimate parameters component models newly labeled data witten bell backoff technique 
stopping criterion simple size clusters nearly constant estimation procedure 
em algorithm potentially powerful way estimating parameters assign sentence different topics allowing topics take advantage entire training data 
em algorithm potentially costly may provide significant performance advantages 
evaluate training options experimentally 
topic model smoothing problems associated component models trained training options referred 
models trained subsets training data danger may 
problem case come non topic sentences may able give sentence reasonable probability 
third problem experiments mixture models non verbalized punctuation data bbn data rule verbalized punctuation 
verbalized punctuation grams may reasonable estimates 
deal non topic problem including general model addition component models sentence level 
general model trained entire training data 
alternative approach interpolate general model component models gram level component models general 
smoothing gram estimates component models due sparse data lack verbalized pronunciation training interpolate model trained vp data gram level 
specifically model gamma jw gamma gamma gamma pg jw gamma gamma pg deltaj delta general model trained data verbalized punctuation training set pg deltaj delta general model trained remaining data data additional bbn processed wsj data 
sets interpolation parameters need estimate sentence level mixture weights gram level interpolation weights 
parameters smoothing unseen data non topic unseen trigram need estimate separate data set 
wsj speech transcriptions training estimating parameters 
simplify implementation sets weights estimated separately 
sentences held data set labeled topic data split clusters 
simple maximum likelihood criterion estimate respective cluster uniform weights 
topic cluster new old old number words sentence total number sentences cluster component models estimated estimated way analogous algorithm entire data set 
iterative 
experiments iterated data set assuming fast convergence 
implementation details training data increases number unique trigrams increases 
important issues efficiently language model efficient storage model fast access gram probabilities 
deal problem storage implemented efficient structure model relevant details stored 
experimented techniques store probabilities lesser precision 
technique simplified version sign logarithm number system exploited brown 
second technique involves storing probabilities precision places decimal 
observed techniques reduce storage requirements considerably time affect perplexity recognition accuracy different sets text data 
order decrease access time prevent paging due excessive memory requirements parallelized software subset vocabulary time 
difficult train entire vocabulary simultaneously excessive memory requirements train subset vocabulary time 
training algorithm run simultaneously different machines 
testing compute log probability entire utterance log probabilities different vocabulary subsets summed entire vocabulary considered 
example probability sentence computed trigram language model jw gamma gamma 
log probability log log jw gamma gamma logp jw gamma gamma subset vocabulary currently consideration 
comprises pass compute log probability instances utterance example unigram trigram entire vocabulary considered different log probabilities summed obtain log probability entire utterance 
implementation important case computing power difficult consider clustered models time 
experiments implemented turing back estimation technique witten bell back technique bigram trigram models 
technique simpler implement consistently results small improvements recognition accuracy indicated results models implemented thesis witten bell technique 
interesting observation increase recognition accuracy spite slight increase perplexity 
perplexity back technique corresponding recognition results table 
table perplexity arpa lm development test set recognition performance arpa development test set turing vs witten bell 
model back perplexity recognition bigram turing witten bell trigram turing witten bell mainly experimented single component component mixtures trigram models 
explore tradeoff different number clusters worked component trigram mixtures 
significant difference perplexity recognition accuracy mixture mixture model 
worked component mixture remaining experiments reduce computation storage costs 
recognition error arpa development evaluation test sets ing component cluster models reported table 
experiments data clustered set intersection similarity measure equation viterbi style training technique parameter estimation 
table recognition results arpa development evaluation test model implemented set intersection similarity measure viterbi training 
lm recognition error dev eval bbn lm mixture bu lm bbn bu lm recognition results obtained component cluster models set intersection similarity metric viterbi style parameter estimation indicated gain recognition accuracy 
bbn trigram language model recognition results compared basically general model trained training data 
acoustic model experiments ssm 
investigated combining bu mixture language model bbn trigram language model 
scores combined non linear manner combination mixture term potentially give small improvement accuracy bu language model 
experiment experiments significant gain linearly combining log lm scores 
results extremely encouraging 
step implemented cluster ing inverse document frequency similarity metric referred equation 
rest experimental conditions parameter estimation techniques unchanged 
new clustering metric gave slight improvement recognition accuracy development set mixed results evaluation 
acoustic model ssm 
table recognition results arpa development evaluation test model implemented inverse document frequency similarity measure viterbi training 
lm recognition error dev eval bbn lm mixture bu lm bbn bu lm differences results degenerated improved development evaluation set considered significant 
similarity metrics gave performance experimental conditions remaining unchanged 
decided inverse document similarity measure remaining experiments appears broadly language processing 
experimented small number clusters difficult see coherent topics 
appears current models putting news related foreign affairs politics travel cluster news relating finance stocks prices loans 
discussed earlier chapter expectation maximization algorithm stage clustering computationally expensive robust parameter estimation technique 
clustering data inverse document frequency similarity metric estimated model parameters em algorithm 
recognition results experiment indicated table 
acoustic model ssm 
table recognition results arpa development evaluation test model implemented inverse document frequency similarity measure em training 
lm recognition error dev eval bbn lm mixture bu lm bbn bu lm general consistent large improvement recognition accuracy due em training 
optimization weights development data set indicated bu mixture lm em training heavily weighted relative bbn lm case viterbi trained mixture lm 
additional cost em viterbi mainly increased storage computation felt cost em significant disadvantage 
ssm knowledge sources available form hmm segmental neural net snn acoustic scores 
combine knowledge sources sequence modifications mixture model gave statistically insignificant differences recognition accuracy development evaluation sets 
table reports results system combines ssm hmm snn scores obtained bbn addition bbn trigram model mixture language model 
modifications mixture language model knowledge sources remaining 
denote set intersection similarity metric inverse document frequency similarity metric recognition results development evaluation test sets table 
table recognition results arpa development evaluation test set intersection similarity metric inverse document frequency similarity metric includes knowledge sources 
bu lm recognition error dev eval viterbi training viterbi training em training best reported result evaluation test word error cambridge university htk group trigram language model 
baseline static mixture model experiments conducted static mixture model decided baseline static mixture model comprise ffl clustering similarity metric suggested 
ffl topic gram parameter estimation em algorithm 
ffl sentence level mixture weights smoothing weights estimated wsj acoustic transcriptions 
baseline model experimentation development dynamic language model 
chapter dynamic mixture model parameters gram language models estimated large training corpus 
chapter parameters remained static different test sets 
short term fluctuations word frequencies due reasons 
example certain words prominent usage due sub language topic shift conversation 
word frequencies increase decrease depending speaking style speaker 
dynamic language modeling attempts capture short term fluctuations language easily fits framework sentence level mixture model 
research area dynamic language modeling basically concentrated approaches cache model capturing fluctuations word frequencies dynamic mixture weight adaptation capture sub language topic shifts document 
adapted techniques mixture language model simple variations baseline static model 
chapter outline different dynamic modeling techniques incorporated sentence level mixture model conclude experimental results obtained techniques combination 
mixture weight adaptation static mixture language model described chapter comprises different topic language models 
different models interpolated assuming significant topic shift sentence 
human listener usually conditions expectations said heard conversation 
example dictated material conversation law order frequency judge lawyer cases increase compared words 
conversation shifts law order finance words stocks shares frequent usage 
mixture topic dependent models law finance topic shifts estimates topic reliable place emphasis estimates specific topic models 
considered equivalent identifying topic document tracking topic conversation 
approaches dynamically adapting document 
case adapted sentence level mixture weights changing text styles 
recall underlying equation baseline mixture model jw gamma gamman dynamic mixture weight adaptation re estimate sentence level mixture weights mg reflect increased confidence choice weights data observed 
order dynamically estimate sentence level mixture weights cache partial document observed far 
adapted equation cached partial document new old old number sentences document observed far number words ith sentence 
ideally update mg utterance document observed far 
fortunately adaptation expressed simple recursive update follows init represent initial value sentence level mixture weight kth mixture component estimated equation previously unobserved data 

sentence recognized compute init init length sentence probability ith sentence computed recognition parameters kth mixture component 
represents likelihood sentence belonging kth topic 
current values mg likelihood biased document observed far 
order obtain impartial estimate initial values init mg 

update mixture weights sentence gamma gamma equal number sentences observed far 
significant advantage algorithm able document observed far estimating topic weights 
important advantage technique compared computation storage costs negligible compute sentence computed recognition 
additional storage required save initial mixture weights init mg 
mixture weight adaptation algorithm assumes known session boundaries simply increases weight topic evidence increases 
wanted track topic shifts need measure sliding window 
gram adaptation explained earlier inherent characteristics language dynamically adapting model document observed far topic conversation document 
fluctuations word frequencies speaking style person due specific topic general area 
example area speech recognition concentrates language modeling word language prominent usage compared acoustic 
change speaking style fluctuations word frequencies captured dynamic cache model suggested 
approaches possible partially observed document cached cache flushed documents document article reasonably long sliding window determine contents cache 
caching document observed gram model built cache 
case selected approach caching entire partially observed document wall street journal arpa task long articles 
cache probabilities interpolated static gram probabilities interpolation weights obtained minimizing perplexity independent data set maximizing likelihood unobserved data 
case mixture model additional feature caches maintained different component models alternatively cache maintained separate component dependent counts 
important issues resolved dynamic language modeling cache definition cache mechanism choosing interpolation weights 
variety approaches problems outlining alternatives single model language model section describe extension mixture language model section 
single gram adaptation single component mixture model equation adapted mixture model incorporating cache component jw gamma gamma jw gamma gamma represents static model parameters represents cache model parameters represent weights static cache models respectively 
case static model parameters expanded jw gamma gamma jw gamma gamma pg jw gamma gamma pg deltaj delta general model trained training data pg deltaj delta general model trained data verbalized punctuation training set 
free parameters weight cache language model optionally dynamic weight model static 
initial values optionally estimated maximizing likelihood unobserved training set equation 
assign zero value cache weight assign arbitrarily small value renormalize gram weight estimates 
initially implemented cache standard trigram model 
cache probabilities computed back technique implemented baseline mixture model 
re estimated cache built partially observed document equation previously unseen data 
order alleviate sparse data problems trigram cache conditional bigram trigram cache suggested rosenfeld 
consider observed trigram 
words trigram bigram cache hit previously observed document bigram trigram cache probability jw 
unigram cache hit bigram estimate jw 
case conditional bigram trigram cache cache weighted cache hit 
addition conditional bigram trigram cache rosenfeld indicated rare word unigram cache significant cache component 
idea observing word usually probability observed short duration document 
unigram cache significantly useful high frequency words cache built rare words 
word classified rare threshold frequency frequency word large training base 
case threshold frequency empirically assigned observing range word counts vocabulary large training base 
classification rare word content function word reasoning rare function word provide significant information document observed frequent content word 
case selective unigram cache order reflect importance cache estimates observe document cache weights increased sentence function cache size 
weights reset initial values new document session 
mixture gram adaptation consider approaches extending gram adaptation mixture model cases maintaining separate counts topic component models including general model 
sentence categorized belonging particular component cache likelihood sentence estimated component models 
likelihood sentence belonging particular topic equation 
approach counts determined assigning sentence topic 
second approach fractional counts assigned topic relative likelihoods 
approach topic sentence belongs obtained topic max sentence max refers topic 
equation adapted mixture model incorporating component caches jw gamma gamma jw gamma gamma represents static model parameters component represents cache model parameters component represent static model cache model weights component respectively 
static model parameters kth component expanded single lm case jw gamma gamma jw gamma gamma pg jw gamma gamma deltaj delta kth component model trained kth topic data pg deltaj delta general model defined earlier trained data verbalized punctuation training set 
independent weight estimation data set labeled topics mixture weights kth topic static estimated corresponding kth subset data manner explained model 
static model cache weighted probability topic sentence word observed 
second solution problem maintaining mixture cache models assign sentence topic caches weighted likelihood sentence belonging topic equation 
implies consider weighted counts grams opposed absolute counts allows topic models take advantage partially observed document 
implementing interpolated gram cache framework mixture model requires simple variations back gram model implemented interpolated trigram model back trigram model 
cache model cache probability jw gamma gamma estimated linearly interpolated sum trigram bigram unigram probabilities jw gamma gamma uni bi jw gamma tri jw gamma gamma uni bi tri interpolation weights unigram bigram trigram estimates respectively 
probability estimates simple relative frequencies counts words cache 
interpolation weights empirically estimated development test data 
weights selected minimize recognition error series experiments 
order cache component effectively reflected sentence estimate re estimated manner suggested section 
important difference equation mixture static probability estimates dynamic cache estimates explained section 
case recursive solution approximation ml solution 
adaptation gram level sentence level mixture weights cache reset session 
wall street journal arpa task experimented sessions defined mark article speech particular speaker 
article session boundaries available sessions development evaluation test data 
implementation details pointed earlier advantages sentence level mixture model framework lies fact language modeling advances new modeling concepts easily incorporated model 
dynamic modeling techniques required simple variations baseline static mixture model theoretical level practical implementation problems dealt 
significant problems case topic models online compute correct transcription probabilities 
considering supervised adaptation correct transcription implies original input utterance hypothesized utterance 
addition required re estimate gram level mixture weights component models cache models wsj acoustic transcriptions 
developed dynamic modeling software static probabilities best weight estimation sentences precomputed 
proved big saving terms memory requirements terms computation time development dynamic language model option real time speech recognition 
experiments stochastic segment model acoustic component weight estimation stated 
experiments chapter combined knowledge sources hand include hmm scores segmental neural net scores bbn trigram language model scores 
experiments section discuss experimental results various adaptation techniques investigated 
results obtained task dependent case worked wall street journal arpa task specifically designed language model adaptation short sessions 
average session length data sentences 
language model adaptation test data acoustic scores available time mentioned data set 
session boundaries marked 
hand labeled development evaluation test sets different topic sessions articles 
experiments dynamic modeling techniques supervised adaptation correct sentence transcription available adaptation recognition 
results reported section bbn scores 
dynamic mixture weight adaptation experimented dynamic topic adaptation adapting mixture weights 
number mixture components 
initial sentence level weights computed maximizing likelihood wsj acoustic transcriptions 
mixture weights adapted equation partially observed data 
mixture weights reset initial values session 
results obtained dynamic mixture adaptation reported table 
table perplexity measurements recognition results arpa development evaluation test sets dynamic adaptation sentence level mixture weights 
lm perplexity word error dev eval dev eval baseline static model mixture adapted bu lm significant decrease perplexity improvement recognition error 
typically sentence level weights favored general model cases entirely likelihood sentence general model 
possible explanation problem general model robust estimates topic models twice amount training data compared topic models 
problem may rectified additional training data 
dynamic cache adaptation experiments cache adaptation session boundaries correct transcriptions recognition 
addition sentence level mixture weights dynamically adapted stated 
gives clear indication gain obtained dynamic cache adaptation 
tried techniques dynamic cache adaptation single component mixture model component mixture model 
results obtained experiments summarized table table 
seen tables single mixture component showed greater improvement due dynamic cache adaptation compared mixture component 
results obtained discussed 
hold case 
initially implemented dynamic trigram cache built partially observed document 
separate cache maintained topic 
trigram cache probabilities interpolated component vp model probabilities 
topic vp gram mixture weights estimated maximizing likelihood independent data set wsj acoustic transcriptions 
cache initially assigned arbitrary small initial weight gram mixture weights normalized sum unity 
table perplexity recognition results arpa development evaluation test sets single component lm 
lm perplexity word error dev eval dev eval baseline mixture model conditional bigram trigram interpolated trigram rare word unigram dynamic content word unigram dynamic interpolated content word adaptation correct transcription cached scoring best list transcription 
component mixture model sentence labeled equation separate counts maintained topic 
case single component model cache maintained general model 
order deal problem sparse data caches implemented conditional bigram trigram cache suggested 
re estimating gram weights assigned fixed weight cache case bigram trigram cache hit 
cache weight decided empirically reducing recognition error development test set 
renormalized mixture weights case cache hit 
reduction perplexity significant improvement recognition conditional bigram trigram caches 
order table perplexity recognition results arpa development evaluation test sets component lm 
combined model comprises fractional counts absolute counts lm perplexity word error dev eval dev eval baseline mixture model conditional bigram trigram interpolated trigram rare word unigram dynamic content word unigram dynamic interpolated content word estimate extent adaptation computed number times cache number times cache hit 
number case conditional bigram trigram cache 
dependence low cache hit rate small duration sessions interesting observation hit rate improved cache able link certain words adjectives 
example japan previously observed context new context observed comprised japanese treated entirely new word 
addition cache hits due frequent occurrence function words 
conditional bigram trigram cache due backoff technique assigns probability word pair long word previously observed unigram 
order compare conditional bigram trigram cache interpolated cache experimented interpolated cache model suggested 
interpolation weights unigram bigram trigram cache probabilities respectively experimentally obtained 
cache weight fixed weight case conditional bigram trigram cache 
recognition error increased interpolated cache 
due high weight unigram cache mainly benefited frequent function words 
experiments decided interpolation weights unigram bigram trigram cache respectively 
cache weight case cache hit 
rare words unigram cache provide significant information short term fluctuations word frequencies implemented unigram cache words static general model frequency lower certain threshold 
observing frequent words count approximately training base set threshold count 
experimented values significant gain obtained threshold count 
unigram cache weight directly proportional size cache weight 
cache weight saturated 
classification rare word word content function word 
classification implemented unigram cache content words 
unigram cache weight proportional size cache saturating 
advantages observed unigram cache discounting function words 
approximately decrease perplexity slight improvement recognition accuracy evaluation test set 
addition percentage cache hits increased compared rare word unigram cache 
closer analysis component caches realized major problems dynamic caches test utterances assigned general model 
attributed fact general model twice training data compared topic models comprising robust gram estimates 
tried rectify problem deal sparse trigram caches assigning utterance topic component model weighted relative likelihood sentence 
cache assigned fixed weight case hit zero 
conditional bigram trigram cache weighted counts gave small improvement perplexity recognition error compared conditional bigram trigram cache absolute counts 
combined different dynamic components examine improvement recognition accuracy perplexity provided language model adaptation 
best case experiment considered caches gram level 
unigram cache content words second interpolated bigram trigram cache 
unigram cache weighted proportional size cache saturating 
interpolated bigram trigram cache weighted case cache hit zero 
interpolation weights remained bigram cache trigram cache 
caches stored fractional counts case mixture model 
addition cache component sentence level mixture weights adapted described section 
results reported tables 
summary experimented different dynamic adaptation techniques single component mixture model component mixture model 
dynamic adaptation sentence level mixture weights improve performance dynamic gram adaptation gave small improvement essentially due task hand 
development evaluation data small articles adaptation typically comprising sentences 
implied mixture weights adapt significantly topics reset session boundaries 
significant problem topic adaptation lay fact general model robust probability estimates compared topic models 
due reason sentences assigned general model cache 
cache adaptation gave significant decrease perplexity slight improvement recognition results evaluation test set 
observing nbest list transcription seen differences sentences list function words inserted deleted different places 
cache adaptation general resulted improved probability estimates content words correctly recognized 
small sessions available significant cache adaptation 
important reason low cache hit rate fact caches consider links words japan japanese explained earlier 
observed dynamic adaptation single gram gave slightly better performance static mixture improvement dynamic adaptation mixtures 
conjecture mixture model appropriate short term speech events dynamic language models better longer documents 
data needed test hypothesis empirically 
dynamic mixture model techniques provide significant improvement recognition accuracy sessions adaptation reasonably long additional training data available topic models 
conclude chapter results obtained combining best case table perplexity measurement recognition results arpa development evaluation test dynamic cache adaptation content word unigram cache interpolated bigram trigram cache dynamic sentence level mixture weight adaptation includes knowledge sources 
lm perplexity word error dev eval dev eval baseline mixture bu lm mixture adapted bu lm mixture adapted bu lm dynamically adapted mixture model hmm snn bbn lm scores addition ssm scores 
adaptation achieve best reported bu results task word error shown table 
chapter thesis basic concepts topic dependent sentence level mixture model developed speech recognition system provide significant improvement recognition accuracy 
investigated methods dynamically adapting mixture model partial document observed 
key results contributions summarized section followed discussion possible directions 
summary thesis presents new approach statistical language modeling offers potential capturing topic dependent effects long range sentence level effects conceptually simple variation statistical gram language models 
research theoretical contributions development new language model associated training recognition algorithms practical insights gained implementation experiment 
contributions briefly reviewed general modeling contributions 
topic dependent effects language captured standard gram language models introduced mixture topic dependent models alternative standard gram model 
similar done important difference approaches try capture topic dependent effects sentence level shift topic sentence 
implementa topic dependent mixture model needed different topic labeled data 
automatic clustering algorithm developed automatically classify text belonging topics 
helped take advantage large wall street journal training corpus topic 
investigated techniques robust parameter estimation extremely important light sparse data problems language modeling 
initially iterative viterbi style estimation procedure estimating gram probabilities topic dependent models 
addition introduced general model trained data mixture components order smooth topic dependent estimates 
weights mixture components estimated maximizing likelihood unseen data 
realizing expectation maximization em algorithm powerful method estimating topic dependent model parameters developed em algorithm gram probability estimation combined witten bell type back helped reduce effect sparse data topic models 
advantage em algorithm lies primarily fact topic gram probabilities estimated entire training data reflected improved recognition accuracy 
order provide additional smoothing topic models account verbalized punctuation words introduced general model trained verbalized punctuation data gram level 
gram level mixture weights estimated maximizing likelihood unobserved data 
dynamic language model adaptation previous document history tune language model particular topic easily fits mixture model framework ways 
sentence level mixture weights recursively adapted likelihood respective mixture components previous utterance gram level mixture weights 
second dynamic gram cache model incorporated mixture language model 
mixture model possible component dependent cache models component cache updated sentence likelihood component recognized word string 
alternatively component dependent fractional counts sentence caches 
introduced new variation cache language modeling selective unigram cache including content words 
unigram cache addition interpolated bigram trigram cache gram level 
theoretical contributions important experimental contributions thesis 
experiments include different back techniques similarity measures clustering robust parameter re estimation techniques 
investigated different dynamic language modeling techniques contributions discussed 
area language modeling developed software required implement standard gram language models katz back technique 
consider katz back technique extremely robust implemented back technique suggested 
observed consistent improvement recognition accuracy obtained second technique addition faster computations back probabilities 
initial proved extremely useful mixture language model built foundation 
implemented different similarity measures clustering observed significant improvement recognition accuracy 
clustering algorithm areas language processing text classification 
number topics restricted greater number fragments training data split greater sparse data problems encountered 
investigated techniques robust parameter estimation mixture model extremely important light sparse data problems language modeling small improvement performance em algorithm viterbi algorithm 
static mixture language model comprises details model automatic clustering inverse document frequencies similarity measure clustering parameter estimation topic gram probabilities em algorithm mixture weights estimated unseen data wsj speaker independent speech transcriptions 
results baseline static mixture model dynamically adapted mixture model reported table table wall street journal arpa task 
experiments mainly ssm acoustic scores case improved performance 
combining ssm acoustic scores hmm scores snn scores static bu bbn language models gives best case result close best reported result obtained cambridge university htk group standard trigram language model 
experimented different techniques incorporating cache mixture model including conditional bigram trigram cache interpolated cache selective table recognition results static language models arpa development evaluation test sets 
acoustic model language model word error dev eval ssm trigram bbn ssm mixture trigram bu ssm bbn bu bu bbn unigram cache rare words selective unigram cache content words 
selective content word unigram cache gave benefit adaptation short sessions 
dynamic language modeling provided small improvements recognition accuracy decrease test set perplexity 
results dynamic language model reported table 
model includes sentence level weight adaptation component cache models unigram cache content words interpolated bigram trigram cache 
case component mixture model sentence assigned topics likelihood sentence belonging topic 
ssm scores combine hmm snn bbn language model scores get best result word error 
part research undertaken developed optimization techniques reduce heavy computation required training accessing trigram models reduce memory requirements model length vocabulary amount training data increases 
table perplexity measurement recognition results arpa development evaluation test dynamic cache adaptation content word unigram cache interpolated bigram trigram cache dynamic sentence level mixture weight adaptation 
acoustic model language model perplexity word error dev eval dev eval ssm baseline mixture model ssm mixture adapted bu lm ssm mixture adapted bu lm mixture adapted bu lm suggestions extended ways 
interesting see performance gains achieved increasing number clusters 
observe improvement experiments results may limited amount available training data question re evaluated additional training data available 
useful consider metrics automatic topic clustering multinomial distribution assumption likelihood clustering criterion obtaining constrained topic models 
done area robust parameter estimation 
example gram part speech sequence model base component models topic dependent word likelihoods part speech label natural extension 
approach address problem sparse data part speech sequence probabilities entire training data 
imply model conditional probability word class class previous word topics 
dynamic model framework maintain component caches probabilities words part speech 
repeat efforts larger vocabulary example dictionary 
additional training data gb able exploit full potential mixture model framework larger number robust topic models 
simple static mixture language model useful applications continuous speech transcription 
example topic dependent models topic spotting 
addition mentioned earlier notion topic need related subject area related speaking style speaker goal 
atis task example goal speaker flight information request response clarification error correction reflected language utterance 
representing structure explicitly double benefit improving recognition performance providing information dialog model 
cursory look recognition errors wsj benchmark tests clear topic dependent models dramatically reduce word error rate 
vocabulary words function words represent major source errors 
mixture language model extended incorporate vocabulary words supervised adaptation contexts cache modeling 
list extensions partial important advantage framework simple modification existing language modeling techniques easily integrated language modeling advances 
appendix expectation maximization algorithm explain expectation maximization algorithm developed robust model parameter re estimation referred section 
step observation sentence represent complete observation class belongs represent number classes 
define delta delta delta dimensional vector th place referring kth class 
represent parameters classes represent class probability distributions 
define jz log jc log log represents kth class represents probability kth class 
class probabilities sum log probability single complete observation written log log log jz log dimensional vector elements jz vector elements defined earlier 
assuming sentence identical independent distributed observation likelihood entire set sentences denoted log log represents current estimate expected likelihood observations 
log jy jy data incomplete know class observation belongs expectation jy call expected class vector observation dimensional vector 
single element vector ik jy jz jz jz represents probability observation class parameters similarly uses probability sentence comprises sequence words trigram model jz jw jw gamma gamma initial estimates probabilities obtained labeled data labels clustering 
step maximize separately get new parameters 
maximizing gives ij ik consider trigram estimate trigram th class jw 
jw bcd re estimate bcd argmax bcd argmax bcd ik jz considering topic argmax bcd argmax bcd ij log jz jc probability sentence th class 
equation log jc bcd log bcd bcd number occurrences trigram sentence constant includes remaining probability terms 
fl represent lagrange multiplier required satisfy constraint trigram conditional probabilities observed context backoff estimate bigram referred oe bc sum unity fl represent lagrange multiplier required satisfy constraint back estimate oe bc jth model greater certain minimum 
constraint bcq oe bc represents words observed training context order understand second constraint consider back estimate single gram known topic structure 
represents bigram count represents trigram count represents unique trigrams observed bigram context probability jw estimated witten bell backoff technique bigram backoff estimated analogous manner em estimation define count topic ij bcq unique trigram count bigram ij bcq bcq back constraint oe bc bc bc bc bc ij bcq bc ij bcq bcq including constraints maximizing step argmax bcd ij bcd log bcd fl gamma bcq gamma oe bc fl oe bc constraint equation mle cause oe bc equal 
maximize equation assign oe bc minimum possible value oe bc bc bc bc differentiating equating estimating bcd obtain ij bcd bcd gamma fl bcd ij bcd fl substituting bcq equation fl bcq ij gamma oe bc bc bc bc bc bc bc represents words observed context entire training data 
solving fl bcd obtain bcd ij bcd bc bc equation equation bcd ij bcd bcq ij bcq ij bcq backoff bcq ij bcq bcq ij bcq ij bcq equation holds trigram estimates 
bigram probabilities similarly estimated bc ij bc bq ij bq ij bq unigram backoff estimated backoff bq ij bq bq ij bq ij bq unigram probability estimated ij ij bibliography jelinek mercer roukos principles lexical language modeling speech recognition readings speech recognition edited waibel kai fu lee pp 
morgan kaufmann publishers 
bahl jelinek mercer maximum likelihood approach continuous speech recognition ieee transactions pattern analysis machine intelligence pp 
vol 
pami march 
gazdar mellish natural language processing lisp computational linguistics addison 
schwartz fung nguyen estimation powerful lm small large corpora proc 
int 
conf 
acoust speech signal proc pp 
vol 
april 
rabiner tutorial hidden markov models selected applications speech recognition proc :10.1.1.131.2084
ieee vol 
pp 
february 
zue integrating probabilistic lr parsing speech understanding systems proc 
int 
conf 
acoust speech signal proc vol 
pp 

lari young estimation stochastic context free grammars inside outside algorithm computer speech language pp 

kupiec hidden markov estimation unrestricted stochastic context free grammars proc 
int 
conf 
acoust speech signal proc vol 
pp 

wright jones hybrid grammar bigram speech recognizer system order dependence model proc 
int 
conf 
acoust speech signal proc vol 
pp 

lafferty integrating probabilistic finite state context free models language presentation ieee asr workshop december 
meteer rohlicek statistical language modeling combining ngram context free grammars proc 
int 
conf 
acoust speech signal proc vol 
ii pp 

maltese language model acoustic model information probabilistic speech recognition ieee transactions speech audio proc pp 
february 
ney essen smoothing techniques bigram natural language modeling proc 
int 
conf 
acoust speech signal proc pp 

katz estimation probabilities sparse data lm component speech recognizer ieee transactions acoust speech signal proc vol 
assp number pp 
march 
witten bell zero frequency estimation probabilities novel events adaptive text compression ieee transactions inform 
theory vol 
pp 

jelinek mercer interpolation estimation markov source parameters sparse data pattern recognition practice ed 
gelsema kanal 
dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society vol 
pp 

kupiec probabilistic models short long distance word dependencies running text proc 
arpa workshop speech natural language pp 
feb 
kuhn de mori results stochastic language modeling proc 
arpa workshop speech natural language pp 
feb 
jelinek merialdo roukos strauss dynamic lm speech recognition proc 
arpa workshop speech natural language pp 

kuhn de mori cache natural language model speech recognition ieee transactions pami vol 
pp 

kneser steinbiss dynamic adaptation stochastic lm proc 
int 
conf 
acoust speech signal proc vol 
pp 

lau rosenfeld roukos trigger language models maximum entropy approach proc 
int 
conf 
acoust speech signal proc vol ii pp 
april 
rosenfeld hybrid approach adaptive statistical language modeling appear proc 
arpa workshop human language technology march 
bahl brown desouza mercer tree statistical language model natural language speech recognition ieee transactions acoust speech signal proc vol 
pp 

wright jones lloyd thomas consolidated language model speech recognition proc 
eurospeech vol 
pp 

speech recognition stochastic language model integrating local global constraints proc 
arpa workshop human language technology 
ostendorf roukos stochastic segment model phoneme continuous speech recognition ieee transactions acoust speech signal proc vol 
december 
ostendorf gish stochastic segment modeling estimate maximize algorithm ieee int 
conf 
acoust speech signal processing pp 
new york april 
kimball ostendorf context modeling stochastic segment model ieee transactions signal processing vol 
assp pp 
june 
kannan ostendorf rohlicek maximum likelihood clustering gaussians speech recognition ieee transactions speech audio processing vol 
pp 
july 
kubala bbn byblos february atis benchmark results proc 
arpa workshop speech natural language february 
bates bbn spoken language understanding system proc 
int 
conf 
acoust speech signal proc vol 
ii pp 

austin makhoul schwartz continuous speech recognition segmental neural nets proc 
darpa workshop speech natural language pp 
feb 
ostendorf kannan austin kimball schwartz rohlicek integration diverse recognition methodologies reevaluation nbest sentence hypotheses proc 
arpa workshop speech natural language pp 
february 
paul baker design wall street journal csr corpus proc 
arpa workshop speech natural language pp 
february 
kubala hub spoke paradigm csr evaluation proc 
arpa workshop human language technology pp 
march 
schwartz written language training data spoken language modeling proc 
arpa workshop human language technology march 
duda hart pattern classification scene analysis 
sekine automatic sublanguage identification new text second annual workshop large corpora kyoto japan pp august 
brown acoustic modeling problem automatic speech recognition carnegie mellon university ph dissertation 
pallett fiscus fisher garofolo lund benchmark tests arpa spoken language program proc 
arpa workshop human language technology march 
