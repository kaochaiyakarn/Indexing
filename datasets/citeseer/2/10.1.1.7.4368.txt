embedded software real time signal processing systems design technologies gert goossens member ieee johan van member ieee dirk member ieee werner member ieee clifford pierre paulin member ieee invited increasing embedded software implemented core processor single chip system clear trend telecommunications multimedia consumer electronics industries 
companion issue presents survey application architecture trends embedded systems growth markets :10.1.1.7.4368
lack suitable design technology remains significant obstacle development systems 
key requirements efficient software compilation technology 
especially case fixed point digital signal processor dsp cores cited commercially available compilers unable take full advantage architectural features processor 
due shorter lifetimes architectural specialization processor cores processor designers compelled neglect issue compiler support 
situation resulted increased research activity area design tool support embedded processors 
discusses design technology issues embedded systems processor cores focus software compilation tools 
architectural characteristics contemporary processor cores reviewed tool requirements formulated 
followed comprehensive survey existing new software compilation techniques considered important context embedded processors 
software playing increasingly important role design embedded systems 
especially true personal telecommunications multimedia systems form extremely competitive segments embedded systems market 
cases software runs manuscript received february revised december 
goossens van target compiler technologies leuven belgium mail goossens 
leuven belgium mail 
laboratories sgs thomson microelectronics grenoble france mail imag fr 
paulin sgs thomson microelectronics cedex france mail pierre paulin st com 
publisher item identifier 
ieee processor core integrated large scale integrated vlsi chip 
studies indicate development time embedded system spent software coding :10.1.1.7.4368
confirmation ongoing paradigm shift hardware software time indication software design phase bottleneck system design process 
paradigm shift hardware software increasing amount software embedded system important advantages obtained 
possible include late specification changes design cycle 
second easier differentiate existing design adding new features 
software facilitates reuse previously designed functions independently selected implementation platform 
requires functions described processor independent abstraction level code 
different types core processors embedded systems 
general purpose processors 
vendors offthe shelf programmable processors offering existing processors core components available library element silicon 
microcontroller cores digital signal processor dsp cores available 
system designer point view general purpose processor cores offer quick reliable route embedded software especially amenable low medium production volumes 
application specific instruction set processors 
high volume consumer products system companies prefer design house applicationspecific instruction set processor asip :10.1.1.7.4368:10.1.1.7.4368
customizing core architecture instruction set system cost power dissipation reduced significantly 
crucial portable network powered equipment 
furthermore house proceedings ieee vol 
march processors eliminate dependency external processor vendors 
parameterizable processors 
intermediary previous solutions provided traditional new processor vendors semiconductor departments bigger system companies 
groups offering processor cores basic architecture available versions different register file sizes bus widths optional functional units 
designers select instance best matches application 
software bottleneck system design 
increasing software embedded systems results increased flexibility system designer point view 
different types processor cores introduced typically suffer lack supporting tools efficient software compilers instruction set simulators 
general purpose microcontroller dsp cores supported compiler simulator available processor vendor 
case fixed point dsp processors known code quality produced compilers insufficient :10.1.1.7.4368:10.1.1.7.4368
cases tools standard software compiler techniques developed suited peculiar architecture dsp processors 
case asip compiler support normally 
parameterizable processors asip major problem developing compiler target architecture fixed 
result current day design teams generalpurpose dsp asip cores forced spend large amount time handwriting machine code usually assembly code 
situation obvious economical drawbacks 
programming dsp asip low level abstraction leads low designer productivity 
results massive amounts legacy code easily transferred new processors 
situation clearly undesirable era lifetime processor increasingly short architectural innovation key successful products 
factors act brake expected productivity gain embedded software 
fortunately research community responding situation renewed interest software compilation focusing embedded processors 
main aspects deserve special attention developments architectural retargetability 
compilation tools easily adaptable different processor architectures 
essential cope large degree architectural variation seen dsp asip market pressure results increasingly shorter lifetimes processor architectures 
example asip typically serve product generations 
context retargetable compilation solution provide system designers supporting tools 
code quality 
instruction cycle count compiled machine code comparable solutions designed manually experienced assembly programmers 
words compiled solution exploit architectural features dsp asip architecture 
low cycle count high execution speed may essential cope real time constraints imposed embedded systems 
low instruction count high machine code density especially required machine code program stored chip case contributes low silicon area power dissipation 
note cycle count instruction count different parameters compilers usually try optimize time 
organized follows 
architectural classification embedded processor cores section ii 
section iii introduces problem software compilation embedded context summarizes main issues 
section iv focuses techniques software compilation 
traditional approaches discussed newer research embedded processor context 
section outlook 
ii 
compilation view processor architectures availability efficient supporting tools prerequisite fast correct design embedded systems 
major requirement availability software compilation tools 
section iv different techniques software compilation context embedded processors discussed 
issues emphasized architectural retargetability ability quickly adapt compiler new processor architectures 
retargetable compiler normally architectural model 
compiler generate code sufficient code quality class processor architectures fit model 
users developers software compilers useful indicate class architectures addressed method 
section introduce classification scheme programmable processors 
overview programmable dsp architectures 
compared classification scheme specific emphasizes aspects relevant software compiler 
characterize compiler compiler method terms classes architectures handle successfully characterize processor quickly find suitable compiler support 
classify processor architecture parameters arithmetic specialization data type code type instruction format memory structure register structure control flow capabilities 
parameters explained sequel typical parameter values existing embedded processors telecom consumer applications 
goossens embedded software real time signal processing systems fig 

structure adsp xx processor 
fig 

part instruction set adsp xx processor 
columns show different instruction fields encoded instruction bits listed top 
section refer existing dsp processor means example adsp xx fixedpoint dsp analog devices 
processor chosen features encountered asip adsp xx architecture shown fig 

instruction set processor supports different formats parallel depicted fig 
arithmetic operation alu multiplier parallel memory loads address calculations 
definitions arithmetic specialization compared microprocessor architectures distinguishing feature dsp parallel multiplier accumulator unit 
virtue arithmetic specialization execution correlation algorithms digital filters auto cross correlation speeded significantly 
asip idea arithmetic specialization carried 
specialized arithmetic units introduced controlled processor instruction set way critical sections target algorithms deeply nested loop bodies executed minimal number machine cycles excessive storage intermediate values 
typical example hardware support butterfly function viterbi decoding encountered asip wireless telecom 
data type embedded processor cores consumer telecom applications normally support fixed point arithmetic 
reason floating point units occurring general purpose microprocessors require additional silicon area dissipate power 
floating point arithmetic avoided relatively easily vlsi implementation consumer telecom systems sacrificing numerical accuracy including appropriate scaling operations software hardware 
general purpose dsp different fixed point data types typically encountered 
distinct case adsp xx architecture fig 
important data types bit type alu multiplier operands bit type multiplier shifter results bit accumulator type bit type shift factors bit address type 
conversions data types may provided processor hardware 
consider adsp accumulator register bits wide 
case bit proceedings ieee vol 
march fig 

different code types illustrated multiply accumulate instruction pipelined datapath 
bit subwords called respectively separately addressable source operand different arithmetic operations 
comparable variety data types typically asip bit widths functional units busses memories chosen function application 
example asip private local telephone switch developed northern telecom :10.1.1.7.4368
code type processors instruction level parallelism able execute sequences operations data pipeline 
fig 
shows example implement stage data pipeline 
parallel current multiplication architecture execute accumulation previous multiplication result acc load multiplication operand memory load 
control operations data pipeline different mechanisms commonly computer architecture data stationary time stationary coding 
case data stationary coding instruction part processor instruction set controls complete sequence operations executed specific data item traverses data pipeline 
instruction fetched program memory decoded processor controller hardware sure composing operations executed correct machine cycle 
case time stationary coding instruction part processor instruction set controls complete set operations executed single machine cycle 
operations may processing different data items traversing data pipeline 
case responsibility programmer compiler set maintain data pipeline 
resulting pipeline schedule fully visible machine code program 
code types illustrated fig 

authors observations time stationary coding asip cores general purpose processors type 
multiplication result kept accumulator register need fourth stage store result memory 
furthermore consider address calculations operand loads put additional pipeline stage 
addition operations data pipeline described processor fetch instructions program memory decode instruction decoder 
done instruction pipeline stages preceding data pipeline stages 
processors data stationary code type instruction fetch instruction decode normally done separate instruction pipeline stages 
example data stationary processor fig 
typically pipeline depth cycles fetch decode load multiply accumulate 
processors time stationary code type instruction fetch instruction decode usually done single separate instruction pipeline stages preceding single execution cycle 
processors time stationary code type single fetch decode cycle called processors 
studied intensively microprogramming community contrast processors multiple instruction pipeline stages time data stationary code type referred processors 
processors may exhibit pipeline hazards 
depending processor pipeline hazards may resolved machine code program statically means interlocking processor controller dynamically 
processors interlocking relatively easy program may difficult designer predict exact cycle time behavior 
instruction format distinction orthogonal encoded instruction formats 
orthogonal format consists fixed control fields set independently 
example long instruction word vliw processors orthogonal instruction format 
note instruction bits control field may additionally encoded reduce field width 
case encoded format interpretation instruction bits control fields may different instruction instruction 
correct interpretation deduced value designated bits instruction word special format bits instruction bits fig 
specific opcode bits 
processor instruction decoder translate instruction bits control signals steering different units processor 
processor embedded core application program reside chip 
case processor designers aim restricting instruction word width order reduce chip area especially power dissipation relating program memory accesses 
chip field programmable convenient choose instruction width equal width chip parallel data port instructions loaded note term microcode originally introduced refer lower level control inside processor controller decode execute instructions 
goossens embedded software real time signal processing systems port equal width standard memory components program memory 
reasons general purpose dsp bit wide instruction format 
contrast asip uncommon instruction widths 
cases instruction format typically encoded 
encoding general restricts instruction level parallelism offered processor 
challenging task design asip determine instruction set encoded restricted number instruction bits offering sufficient degree parallelism critical functions target application 
speed requirements typical telecom consumer applications possible design efficient asip relatively high degree instruction encoding 
contrast image processing multimedia applications may require higher amount instruction level parallelism meet speed requirements 
current asip application domains orthogonal instruction formats 
memory structure dsp asip cores efficient memory register structures ensure high communication bandwidth different datapath units datapath memory 
section discuss memory structures register structures treated section 
memory access memory structures classified basis accessibility data program memory von neumann architecture 
processors single memory space store data program 
case older microprocessor architectures cisc type 
harvard architecture 
term refers case data program accessible separate hardware 
applied general purpose risc processors means data program busses separated 
applied dsp processors means data program memory spaces separated 
cases data memory spaces provided address generator 
case adsp xx fig 

remainder assume processor harvard architecture separate data program memory spaces 
case current dsp asip software compiler point view choices addressing modes operand location important issues 
discussed 
addressing modes processors usually support multiple addressing modes data memories immediate direct indirect addressing 
case dsp asip indirected addressing typically implemented address generation units 
units support specialized address operations modulo counting implement circular buffers filter applications counting reversed carry propagation fft applications address post modify instructions allow compute memory address ously current memory access 
essential features supported compiler 
operand location respect operand location classification memory structures relevant load store architecture called register register architecture 
load store architecture arithmetic operations get operands produce results addressable registers 
communication memories registers requires separate load store operations may scheduled parallel arithmetic operations permitted instruction set 
load store concept basic ideas risc architectures 
example adsp xx processor instruction format shown fig 

seen arithmetic operations belonging format operate registers addressed instruction bits 
multiplication results written register alu results written ar 
parallel arithmetic operation load operations executed prepare arithmetic operands instruction cycle registers addressed instruction bits 
memory memory memory register architecture 
case arithmetic instructions specified data memory locations operands 
example tms dsp processor execute multiplication operands respectively residing memory register eventually store result accumulator register 
processor cores encountered embedded systems types 
note case core processor data program memories placed chip reduce board cost access time allowing single cycle access power dissipation 
register structure processor contain register set temporary storage intermediate data values 
discussing register structures detail terms introduced 
homogeneous register set 
register set registers interchangeable 
instruction reads operand writes result register set programmer compiler allowed select element set 
heterogeneous register set 
type register set consists special purpose registers 
case register serve operand result register specific instructions 
likewise instruction read operands write results specific elements register set 
consider example adsp xx processor 
arithmetic operations belonging format fig 
left right operands restricted registers indicated fields instruction bits respectively 
results stored multiplications ar alu operations 
proceedings ieee vol 
march table scope retargetability chess compiler classification scheme parameter supported values data type fixed floating point standard user defined data types code type time stationary harvard multiple data memories instruction format load store addressing modes post modification register structure heterogeneous homogeneous control flow zero overhead loops residual control 
homogeneous case extreme point solution space practical register sets heterogeneous 
words processor positioned axis homogeneous heterogeneous 
register set processor partitioned different register classes 
register class subset processor register set viewed homogeneous point view certain instruction operand result 
example constitutes register class adsp xx processor elements set serve right operand register multiplication format fig 

note register classes contained overlap 
total number register classes processor considered measure heterogeneity 
rough classification existing processor types 
general purpose microprocessors usually relatively homogeneous register set 
case fixedpoint processors register set normally divided register classes data register class address register class 
case floating point architectures floating point registers constitute third class 
general purpose dsp typically parallel multiplier 
compared microprocessor counterparts introduces additional register class store multiplication results 
instruction format encoded restrictions may exist choice source destination registers results additional register classes 
asip typically strongly heterogeneous register set 
reasons twofold 
asip may support different data types result different register classes 
secondly asip designers aim high degree instruction encoding significantly compromising available parallelism target application 
done reducing number instruction bits register addressing favor instruction bits arithmetic memory access operations 
way larger number arithmetic memory access operations executed parallel register structure heterogeneous 
discussion clear optimization register structure important task design asip architecture 
design machine code exploits heterogeneous register structure efficient way nontrivial 
matter fact inefficient heterogeneous register structures prime reasons reported low code quality case commercially available compilers fixed point dsp see section 
control flow dsp asip support standard control flow instructions conditional branching bit values condition code register 
additional measures usually taken guarantee performance presence control flow 
examples typical 
branch penalties usually small zero cycles 
branch penalty delay incurred executing branch due instruction pipeline 
furthermore dsp asip loop instructions 
allows execute body repetitive algorithm spending separate cycles loop control 
feature essential time critical applications 
arithmetic move instructions conditionally executable 
specific cases avoids overhead conditionally loading program counter 
arithmetic operations controlled 
case behavior operation depends specific bit values residual control register written operations 
typical examples saturation modes alu accumulate operations 
interrupt controller supports specialized context saving mechanisms register shadowing minimize context switch times 
classification scheme classification scheme introduced different purposes 
characterize retargetable compiler indicate scope retargetability example table indicates scope retargetability current version chess compiler 
second classification scheme characterize processor quickly identify issues related compiler development 
case model gives indication easily compiler built processor existing compilers suited 
example table shows classification number existing dsp asip architectures 
iii 
issues software compilation software compilation addressed aspect architectural retargetability taken consideration early due continuous evolution processor architectures software compilation lost importance researcher practical user point view see fig 

software compiler community goossens embedded software real time signal processing systems table classification existing dsp asip parameters classification scheme parameter tms mpeg arithmetic specialization plug applic spec 
unit viterbi alu dual accumulator data type fixed point fixed point fixed point fixed point code type time stationarity data stationarity data stationarity time stationarity instruction format encoded encoded encoded orthogonal memory structure harvard data memories load store address 
modes post modification harvard data memories memory reg 
address 
modes post modification harvard data memories memory reg 
address 
modes post modification harvard data memories load store address 
modes post modification register structure heterogenous heterogenous heterogenous heterogenous fig 

evolution retargetable compiler research past decades 
focusing general purpose microprocessors evolved traditional cisc architectures risc parallel vliw superscalar architectures 
dsp processors obviously asip received relatively little attention 
processor vendors offer compilers processors 
cases compilers ports gcc compiler framework distributed free software foundation 
gcc combines number techniques developed compiler community primarily general purpose microprocessors 
free distribution source code gcc ported countless processors retargeted 
examples existing dsp commercial gcc available include analog devices motorola sgs thomson 
de facto pragmatic approach develop compilers freely available environment 
processors close intent gcc fairly quick 
mentioned section companion code generated available compilers fixed point dsp unacceptable quality industrial design teams resort manual assembly coding :10.1.1.7.4368
fortunately emerging market embedded processors initiated revival software compilation research dsp asip early fig 

section iv survey traditional software compilation techniques relevant fig 

anatomy software compiler 
context embedded processors 
addition outline developments software compilation embedded processor architectures 
fig 
shows typical anatomy software compiler 
starting point compilation process application program algorithmic specification language 
compilers embedded processors algorithmic specification language 
drawback standard restricted support different data types 
dsp asip accommodate wide variety fixed point data types 
cases language augmented support user definable data types 
algorithmic specification translated intermediate representation means language dependent front 
intermediate representation kept compiler data base accessible subsequent proceedings ieee vol 
march compilation phases 
known intermediate representations representing algorithm include static single assignment form ssa form control dataflow graph cdfg 
addition algorithmic specification retargetable compiler processor specification available internal model compiler data base 
model may generated automatically starting processor specification language 
examples specification languages internal compiler models representing processors discussed section iv compiler generator tool automatically builds processor specific compiler internal model description processor specification language 
software compilation process traditionally divided high level optimization back compilation 
high level optimizer data flow analysis carried determine required data dependencies algorithm needed build ssa form cdfg 
processor independent optimizations carried reduce number operations sequentiality description 
set optimizations quite standard includes common subexpression elimination dead code removal constant propagation folding 
back performs actual code generation intermediate representation mapped instruction set target processor 
code generation process different phases distinguished code selection operations algorithmic model bound partial instructions supported target processor instruction set 
multiple operations combined partial instruction 
determined covering operations model complex patterns representing partial instruction 
register allocation intermediate computation values bound registers memories 
necessary additional data move operations added 
scheduling phase code generator attempts exploit remaining instruction level parallelism available processor architecture 
partial instructions execute parallel grouped complete instructions assigned machine cycles 
set partial instructions code selection register allocation usually called vertical code final instructions scheduling referred horizontal code 
transformation vertical horizontal code called code compaction 
important note compilers partial ordering operations determined earlier phases code selection register allocation 
matter fact determining vertical ordering partial instructions critical issue code selection register allocation algorithms affects eventual code quality 
termed compiler compiler restricted compiler back code generator generator 
described code generation phases encountered software compilers general purpose microprocessors compilers embedded processors 
traditional compilers cisc processors code selection important code generation phase 
case local register allocation included code selection phase 
current compilers risc processors code selection global register allocation typically done separate phases 
furthermore instruction ordering important issue compilers due possible occurrence pipeline hazards 
advent vliw superscalar processors emphasis put efficient scheduling cope larger amount instruction level parallelism architectures 
today context embedded processors new aspects added problem architectural retargetability requirement high code quality irregular architectures see section 
requirement dual impact compilation methodology instruction level parallelism occurrence heterogeneous register structures different compilation phases strongly interdependent 
order generate high quality code code generation phase take impact phases account 
called phase coupling 
order generate high quality code specialized compiler algorithms may required explicitly take account aspects heterogeneous register structures 
examples sections iv important point larger compilation times tolerated case embedded processors compared general purpose microprocessors 
order tackle new challenges compiler researchers investigating synergies software compilation techniques general purpose processors techniques high level synthesis application specific hardware 
approach motivated fact high level synthesis tools targeting irregular architectures instruction level parallelism 
iv 
survey compilation techniques discussion processor architectures general compiler issues survey provided existing techniques processor modeling software compilation 
processor specification languages efficient powerful models represent required characteristics processor key aspect making software compilation process retargetable 
compilers separate specialized models compilation phases attempts single processor model retargetable compilation supported user friendly processor specification language 
section processor specification terms local global register allocation defined precisely section iv 
goossens embedded software real time signal processing systems fig 

part tree pattern base derived adsp xx instruction format fig 

tree corresponding grammar representation shown 
example source operand registers modeled pattern base 
languages discussed 
processor models compilers treated separately section iv 
netlist languages type processor specification languages describe processor netlist hardware building blocks including datapath memories instruction decoder controller 
approach followed mssq compiler accepts processor specification mimola language 
advantage languages completeness 
netlist may available compiler designer 
furthermore approach requires architectural design completed precludes building compilers architecture exploration phase asip design 
high level languages alternative formalisms high level processor description languages proposed 
idea languages capture information available programmer manual processor 
usually description contains structural skeleton processor essentially declaration storage elements data types description actual instruction set 
example isp language descendant isps 
isp instruction set captured specifying behavior corresponds specific sets instruction bits 
procedural formalism 
nml language proposed 
nml uses attributed grammar 
grammar production rules define composition instruction set compact hierarchical way 
semantics instructions register transfer behavior assembly binary encoding captured attributes 
nml cbc chess compilers 
processor models compilation template pattern bases approach traditional compilers general purpose cisc risc processors represent target processor means template pattern base essentially enumerates different partial instructions available instruction set 
partial instruction represented pattern expressed means algorithm intermediate representation 
fig 
shows example pattern base graphical representation form cdfg patterns 
patterns expressed grammar 
explained section iv code generators restrict allowed template patterns tree structures 
case fig 
pattern computes result value operand values 
corresponding grammar model regular tree grammar production rule describes partial instruction pattern usually prefix linearized form 
terminal grammar symbols correspond operations executed instruction nonterminal symbols may correspond possible storage locations 
reduce number grammar rules common subpatterns factored rule describing subpattern connected remaining rules additional nonterminals 
examples code selector generators regular tree grammars include twig burg iburg graham code generators 
compilers embedded processors adopted iburg code selection phase cbc record spam project compiler :10.1.1.57.5855
cbc compiler regular tree grammar serves input iburg derived automatically nml specification target processor 
similarly record grammar derived mimola specification 
spam regular tree grammar specified user format supported olive code selector generator similar iburg 
compilers embedded processors pattern base include 
template pattern base place describes processor instruction set extended additional structural information reflect processor register structure 
example additional patterns called chain rules may inserted describe discussed section iv graham code generators string grammars textual representation regular tree grammars 
proceedings ieee vol 
march fig 

part isg representation adsp xx processor 
possible moves storage elements 
grouping registers register classes modeled 
specialized processor model proposed trellis diagrams essence combine tree patterns structural information 
graph models alternative approach mainly compilers embedded processors represent processor means graph model 
model advantage readily represent structural information possible describe peculiarities asip architectures 
mssq compiler uses connection operation graph derived detailed processor netlist 
rl compiler place time graph graph captures legal data moves processor 
chess compiler built instruction set graph isg code generation phases 
fig 
shows isg representation part adsp xx processor introduced fig 

isg captures behavioral instruction set structural register structure pipeline behavior structural hazards information 
isg bipartite graph vertices representing structural elements small boxes representing registers memories connections large boxes respectively 
objects annotated enabling condition indicate binary instruction format belong 
edges indicate legal dataflow structural elements 
code selection phase code selection received lot attention software compiler community 
early compilers cisc architectures code selection main code generation phase 
compilers code selection determines local register allocation product 
possible cisc memory register memory memory structure number available registers restricted 
techniques applied risc compilers 
compilers register allocation normally deferred separate code generation phase 
issue phase coupling code selection register allocation received renewed interest compilers dsp asip due occurrence heterogeneous register structures 
shown code generation np complete problem intermediate representations take form directed acyclic graph dag 
optimal vertical code instruction level parallelism generated polynomial time conditions satisfied intermediate representation expression tree called subject tree template pattern base restricted contain tree patterns fig 
represented regular tree grammar see section iv processor homogeneous register structure 
different code generation algorithms developed solve canonical problem 
conditions satisfied practice code generators incorporate extensions basic algorithms 
basic algorithms various practical extensions surveyed 
discussion goossens embedded software real time signal processing systems fig 

code selection symmetrical filter tree pattern base fig 
cdfg application tree structured intermediate representation possible cover 
section restricted code selection phase code generation including approaches incorporate local register allocation code selection process 
context local means register allocation done values subject tree 
techniques global register allocation separate code generation phase discussed section iv 
dynamic programming code selector generators stepwise partitioning code selection problem dynamic programming 
conditions canonical problem satisfied 
phases distinguished code selection problem tree pattern matching locating parts subject tree correspond available tree patterns pattern base 
tree covering finding complete cover subject tree available patterns see fig 

dynamic programming proposed aho johnson assuming homogeneous register structure see condition 
tree pattern matching done straightforward way bottom traversal subject tree leaves root 
tree node method computes minimal cost cover subtrees rooted node 
cost calculations done principle dynamic programming 
cost takes account register utilization subtrees different possible orderings subtrees 
way local register allocation scheduling expression tree consideration included algorithm 
top traversal subject tree tree cover determining minimal cost tree root node 
proven program resulting application dynamic programming strong normal form program 
program consists sequences vertical code strongly contiguous meaning node guaranteed subtree node completely executed subtrees executed 
proves strong normal forms optimal canonical problem introduced 
dynamic programming code selection approaches processors heterogeneous register structure code selectors generated twig beg iburg 
dynamic programming cost calculation 
time separate cost calculated register class nodes subject tree 
keep problem tractable number simplifications 
assumed register class infinite number registers 
secondly costs calculated reflect local register allocation operation ordering 
issues delayed subsequent code generation phases 
technique insert data move operations different register classes processor appropriate 
implementations twig beg iburg tree automaton traverse subject tree tree pattern matching step 
improvement tree automaton code selection provided bottom rewrite system burs theory 
original methods calculate intermediate costs actual code selection burs theory allows shift calculations generation phase code selector 
results faster code selection 
drawback burs theory cost function restricted constant additive model 
burg code selector generator burs theory 
formal treatment tree automaton code selection 
compilers embedded processors adaptations dynamic programming methods described 
adaptations mentioned 
practice intermediate representations graphs trees 
traditional method partition dag representation different expression trees cut dag edge representing value multiple times 
illustrated fig 
derivation tree graph 
dynamic programming techniques applied individual trees results combined allocating registers values shared trees 
extension dynamic programming adopted compilers straightforward pattern matching cbc record spam iburg variations :10.1.1.57.5855
mentioned section ii dsp asip heterogeneous register structure 
dynamic programming code selection extended heterogeneous structures see methods suffer weak phase coupling register allocation 
example consider optimizations spilling data values memory constraint register capacity number registers register class restricted 
issues deferred separate register allocation step 
alternative approach provided presenting combined method code selection register allocation heterogeneous structures 
spilling considered proceedings ieee vol 
march register capacity constraints 
covering problem reformulated path search problem trellis trees produces strong normal form programs 
heterogeneous register structures programs guaranteed optimal 
spam project different approach followed subclass architectures heterogeneous register structure identified optimal vertical code produced dynamic programming code selector :10.1.1.57.5855
architectures necessitate spilling 
approach strong normal form programs produced 
method includes register allocation ordering 
practice formulation applicable tms architecture 
lr parsing processor model regular tree grammar see section iv code selection viewed problem parsing subject tree specified grammar 
matter fact tree automaton methods described see dynamic programming parsing methods regular tree grammars 
practical processors regular tree grammar highly ambiguous derivations may obtained expression represent optimization space code selector 
dynamic programming method allows find optimized solution space 
parsing approaches code selection known long tree automaton methods 
graham code generators developed type grammar interpret string grammar 
subject trees linearized expression trees parsed left right lr parsing technique 
linearization patterns graham code selectors perform tree pattern matching left operand biased fashion generating code subtree code left operand root node selected considering right operand 
may produce inferior results compared dynamic programming methods 
parsing approaches code selection described 
graph matching explained dynamic programming approaches code selection suffer restriction pattern base intermediate representation consist tree structures 
authors proposed pattern matching algorithms directly support dag structures 
code selection algorithm generate optimal vertical code dag processor single register 
algorithm refined support commutative operations architectures similar tms processor 
bundling code selection techniques described hitherto rely availability template pattern base possibly form regular tree grammar essentially enumerates legal partial instructions advance 
alternative approach code selection bundling algorithm required patterns constructed fly traversal intermediate representation 
pattern called bundle legal derived processor model case form graph model see section iv 
early example bundling approach code selection combiner algorithm variation peephole optimization technique gcc compiler 
bundling algorithms developed compilers embedded processors mssq chess 
advantage bundling algorithms support intermediate representations partial instructions graphs trees 
features useful dsp asip context 
disadvantage increased algorithmic complexity compared dynamic programming parsing methods 
note bundling algorithms mssq include local register allocation 
chess register allocation deferred separate compiler phase 
phase coupling supported primarily principle late binding legal bundles exist group operations intermediate representation choice deferred register allocation scheduling phase 
approach illustrated fig 
example fig 
assuming processor specified isg graph model 
possible mappings individual cdfg operations partial instructions determined fig 

cdfg traversed find combinations operations correspond complex partial instructions account operands results read resp 
written available storage elements fig 

rule driven code selection rule driven approaches code generation explored compiler 
approaches combine progressive set refinement phases produce machine code 
phase compilation set rules provided programming environment guides transformation 
critical phase process code selection compiler developer defines virtual machine resembles closely possible instruction set real machine sequential operation 
virtual machine support instruction level parallelism 
issue parallelism deferred code compaction phase 
definition available register sets addressing modes architecture developer specifies set rules map operation patterns instructions virtual machine fig 

fundamental restriction practical implementations assume tree structured patterns 
produce rule base developer disposal set primitives manipulate standard set tree patterns virtual machine instructions available register sets addressing modes 
operands operation trees allocated register sets matchings data types char int ptr float long declared specification 
goossens embedded software real time signal processing systems fig 

code selection bundling approach cdfg mapped fig 
initial mappings cdfg isg vertices construction bundles 
fig 

virtual code selection 
optionally allocation may constrained specific registers register sets virtual machine instructions rules selection 
flexibility important support specialization register functions embedded processors 
register assignment register set performed independently code selection process 
mapping operation patterns virtual machine user supplied transformation rules optimize description generate instructions actual processor 
rule driven compilation provides fast way compiler generation quality compiler directly dependent skills user write adequate transformation rules 
furthermore illustrated generate high quality code user may rewrite source code program level close target instruction set 
example pointer referencing may array variables register allocation may predetermined partly user 
register allocation register allocation phase compiler assigns intermediate computation values storage locations processor 
section iv techniques discussed essentially perform code selection able carry local register allocation decisions fly 
section techniques reviewed essentially perform global register allocation separate code generation phase 
shown techniques able perform remaining code selection decisions fly 
illustrates exact partitioning code generation process different phases nontrivial decided compiler developers architectural context 
graph coloring standard formulation register allocation problem practical implementations terms graph coloring interference graph 
explain basic graph coloring formalism initial assumptions processor homogeneous register structure register set capacity number registers restricted predefined value say code selection accomplished preceding phase execution ordering different instructions determined code selection phase 
execution order determines live range intermediate computation value 
live ranges interference graph constructed 
undirected graph vertices correspond live ranges values edges connect vertices interfering overlapping live ranges 
register allocation equivalent finding acceptable vertex coloring interference graph colors 
heuristic graph coloring algorithms 
fig 
shows interference graph constructed set values live ranges 
example vertices interference graph colored colors resulting different register 
interference graph colored colors register capacity exceeded 
case standard solution temporarily spill values memory 
alternatively values serve operand multiple operations recomputed prior 
transformation called 
chaitin proposed number heuristics spilling proceedings ieee vol 
march fig 

register allocation graph coloring live ranges displayed time axis interference graph 
fig 

alternative register allocations multiplication operand symmetrical fir filter 
route followed indicated bold storage ar storage ar followed mx spilling data memory dm 
alternatives require insertion extra register transfers 
graph coloring procedure called iteratively 
improvements principles described 
practice assumptions may satisfied 
practical processors heterogeneous register structure 
extensions technique proposed take register classes account graph coloring 
furthermore graph coloring approach assumes live range value known 
papers investigate interaction register allocation scheduling 
data routing mentioned extension graph coloring heterogeneous register structures applied general purpose processors typically register classes floating point registers fixed point registers address registers 
dsp asip architectures strongly heterogeneous register structure special purpose registers 
context specialized register allocation techniques developed referred data routing techniques 
transfer data functional units intermediate registers specific routes may followed 
selection appropriate route nontrivial 
cases indirect routes may followed requiring insertion extra operations 
efficient mechanism phase coupling register allocation scheduling essential 
illustration fig 
shows number alternative solutions multiplication operand symmetrical fir filter application implemented adsp xx processor see fig 

techniques data routing compilers embedded processors 
approach determine required data routes execution scheduling algorithm 
approach applied bulldog compiler vliw machines subsequently adapted compilers embedded processors rl compiler cbc 
order prevent combinational explosion problem methods incorporate local greedy search techniques determine data routes 
approach typically lacks power identify candidate values spilling memory 
global data routing technique proposed chess compiler 
method supports different schemes route values functional units 
starts unordered description may introduce partial ordering operations reduce number overlapping live ranges 
algorithm branchand bound searches insert new data moves introduce partial orderings select candidate values spilling 
phase coupling scheduling supported probabilistic scheduling estimators register allocation process 
memory allocation address generation problem related register allocation allocation data memory locations scalar data values intermediate representation 
important goossens embedded software real time signal processing systems memory spills introduced register allocation phase passing argument values case function calls 
values stored stack frame data memory 
memory assignment array data values scope 
approach described memory allocation graph coloring technique comparable register allocation method described previously 
important issue addressing values stack frame memory 
typically pointer maintained stack frame 
conventional architectures updating pointer access may require instructions 
discussed section ii dsp processors asip typically specialized address generation units support address modifications parallel normal arithmetic operations 
implemented means address calculated adding modifier value current address current memory access place 
way address pointer updated instruction cycle penalty 
cases modifier value restricted 
pointer modification supported advantageous allocate memory locations way consecutively ordered memory accesses adjacent memory locations locations close permit pointer modification 
optimization typical dsp processors described context tms processor supports post increment post decrement instructions 
formalism undirected graph vertices corresponding data values edges reflecting preferences neighboring storage locations value pairs 
solution maximal number post increments decrements obtained finding hamiltonian path graph 
np complete problem heuristic algorithms proposed 
approach refined authors 
scheduling explained previous sections compilers cisc processors typically integrate code selection local register allocation instruction ordering single phase 
due lack instruction level parallelism additional scheduling code compaction phase required 
scheduling task essential architectures exhibit pipeline hazards instruction level parallelism 
case risc architectures 
vliw superscalar architectures features 
scheduling task gained importance software compilation architectural paradigms 
dsp processors asip moderate high degree instruction level parallelism 
example processors typically allow data moves parallel arithmetic instructions see example fig 

parallelism restricted sched crucial task targets requirement high code quality implies scarce architectural resources efficiently possible including possibilities data pipelining 
especially true deeply nested blocks algorithmic specification 
local versus global scheduling local scheduler scheduler operates level basic blocks linear sequences code branching intermediate representation 
known local scheduling technique list scheduling 
context embedded processors integer programming scheduling formalisms described 
architecture restricted amount instruction level parallelism local scheduling approach may produce efficient results 
note assumes scheduler access detailed conflict model partial instructions describing precisely structural instruction encoding conflicts 
case parallel architectures including dsp asip may mismatch architectural parallelism offered processor algorithmic parallelism individual basic blocks 
processor resources effectively global scheduling approach required partial instructions moved basic block boundaries 
moves termed code motions 
code motions may applied change semantics program 
fig 
illustrates important types code motions presence conditional branches 
useful code motion moves instructions complete conditional branch 
speculative execution implies conditional instruction executed unconditionally 
special care required assure result effect branch instruction resided originally taken 
copy copy motions result code duplication conditional blocks 
code hoisting means identical instructions mutually exclusive conditional branches merged executed unconditionally 
important class code motions relates iterators program illustrated fig 

loop unrolling standard transformation consecutive iterations loop scheduled large basic block 
software pipelining transformation restructures loop moving operations loop iteration 
transformations result larger amount parallelism eventual loop body 
due frequent occurrence iterators signal processing applications transformations crucial importance compilers dsp asip global scheduling techniques conditional branches global scheduling lot attention context vliw architectures 
techniques reused case embedded processors dsp asip trace scheduling global scheduling technique developed vliw 
execution probabilities conditional branches traces linear sequences proceedings ieee vol 
march fig 

different types code motions global scheduler 
white gray boxes represent unconditional conditional basic blocks respectively 
dotted solid circles represent partial instruction code motion respectively 
dotted arrows symbolize code motion 
fig 

loop transformations global scheduler 
gray boxes represent loop body number iterations indicated left 
white boxes represent loop pre post 
basic blocks identified 
trace scheduled big basic block 
operations trace move original basic block boundaries 
happens bookkeeping mechanism inserts necessary compensation code critical traces guarantee semantical correctness preserved 
improvements bookkeeping mechanism 
related technique superblock scheduling 
superblock linear sequence basic blocks single entry point 
trace structure transformed superblock structure process called tail duplication 
superblocks scheduled way similar traces 
compared trace scheduling advantage bookkeeping mechanism simplified considerably 
percolation scheduling complete set semantics preserving transformations move instructions basic blocks 
instruction moved repeatedly upward direction adjacent basic blocks 
drawback strategy longer moves useful code motion fig 
possible incremental moves adjacent blocks compose long move beneficial 
global code motion technique proposed types motions supported require code duplication 
technique proposed concept region scheduling 
global code motion tool dsp asip architectures fast probabilistic estimators schedule length register occupation order trade different possible code motions 
actual scheduling done separate phase 
advantage code motion tool invoked earlier stages code generation register allocation 
software pipelining techniques software pipelining divided categories iteratively call local scheduler evaluate effect certain moves incorporate software pipelining single global scheduling algorithm 
modulo scheduling converts conditional branches loop straight line code subsequently applies local scheduling algorithm pipelines loop 
loop folding iterative approach software pipelining 
step algorithm local list schedule computed loop body 
schedule partial instructions selected moved loop iterations 
similar strategy added global code motion tool mentioned previously 
result code motions conditional branch boundaries software pipelining incorporated algorithm 
examples global scheduling algorithms perform software pipelining include enhanced pipeline scheduling 
methods partial loop unrolling parallel schedule composed unrolled loop followed step 
note vliw architectures special hardware support facilitate implementation software pipelining 
typically special register sets dedicated data communication loop iterations 
motivated section companion application architectural trends embedded processor cores represent key component contemporary systems telecommunication multimedia :10.1.1.7.4368
core processor technology created new role general purpose dsp addition clear important asip products manufactured large volumes asip clearly cost efficient power dissipation reduced significantly 
advantages obtained giving flexibility programmable solution 
lack suitable design technologies support phases processor development application programming remains significant obstacle sys goossens embedded software real time signal processing systems tem design teams 
goals motivate increased research effort area cad embedded system design 
focused primarily issue software compilation technologies embedded processors 
starting point observation commercially available compilers especially fixed point dsp unable take full advantage architectural features processor 
case asip compiler support due lack retargeting capabilities existing tools 
compilers employing traditional code generation techniques developed software compiler community 
techniques primarily developed general purpose microprocessors highly regular architectures homogeneous register structures architectural peculiarities typical fixed point dsp past years new research efforts emerged area software compilation focusing embedded dsp asip research teams operating frontier software compilation high level vlsi synthesis 
synergy disciplines resulted number new techniques modeling irregular instruction set architectures higher quality code generation 
code quality issue architectural retargetability gaining lot attention 
retargetability essential feature software compilation environment context embedded processors due increasingly shorter lifetime processor due requirement asip outlined main architectural features contemporary dsp asip relevant software compilation point view 
classification architectures number elementary characteristics 
proper understanding processor architectures prerequisite successful compiler development 
addition survey existing software compilation techniques considered relevant context dsp asip telecom multimedia consumer applications 
survey covered research retargetable software compilation embedded processors 
addition retargetable software compilation important design technology issues discussed 
authors believe increasingly important 
system level algorithmic optimizations 
specifications systems produced precise knowledge implications hardware software cost 
important savings possible carrying system level optimizations control flow transformations optimize memory power cost data memories 
system partitioning interface synthesis 
problems hardware synthesis software compilation reasonably understood design glue components done manually error prone 
synthesis real time kernels 
kernel takes care run time scheduling tasks account interaction system environment 
cases general purpose operating systems 
solutions expensive terms execution speed code size 
research focusing automatic synthesis lightweight application specific kernels obey userspecified timing constraints 
paulin na goossens embedded software real time signal processing systems application architecture trends proc :10.1.1.7.4368
ieee issue pp 

morse increasing importance software electronic design vol 
jan 
paulin trends embedded systems technology industrial perspective hardware software design de micheli sami eds 
boston kluwer 
tuned risc devices deliver top performance electronic design pp 
mar 
van someren arm risc chip programmer guide 
reading ma addison wesley 
berger application specific dsp personal communications applications proc 
expos 
symp june 
new dsp core ee times jan 
development platform generation dsp products proc 
th int 
conf 
signal proc 
applic 
technol oct 
core specification doc 
sgs thomson grenoble france jan 
dsp oriented benchmarking methodology proc 
int 
conf 
signal proc 
applic 
technol oct 
marwedel goossens code generation embedded processors 
boston kluwer 
goossens programmable chips consumer electronics telecommunications architectures design technology hardware software design de micheli sami eds 
boston kluwer 
lee programmable dsp architectures part part ii ieee assp mag dec jan 
adsp user manual norwood analog devices 
tms tms lc tms vc fixed point digital signal processors houston texas 
architecture pipelined computers 
new york mcgraw hill 
hennessy patterson computer architecture quantitative approach 
san mateo ca morgan kaufmann 
ellis bulldog compiler vliw architectures 
cambridge ma mit press 
mpeg audio decoder consumer applications proc 
ieee custom 
circ 
conf may 
architecture programming vliw style programmable video signal processor proc 
th acm ieee int 
symp 
microarchitecture nov 
clarke wilson philips vliw dsp multimedia ee times nov 
chess retargetable code generation embedded dsp processors code generation embedded processors marwedel goossens eds 
boston kluwer 
stallman porting gnu cc free software foundation june 
van modeling hardware specific data types simulation compilation hw sw design proc 
th workshop synth 
syst 

mixed technol nov 
proceedings ieee vol 
march cytron efficiently computing static single assignment form control dependence graph acm trans 
prog 
lang 
syst vol 
pp 
oct 
mcfarland parker high level synthesis digital systems proc 
ieee vol 
pp 

feb 
van data flow graph exchange standard proc 
europe 
design autom 
conf mar pp 

aho compilers principles techniques tools 
reading ma addison wesley 
local microcode compaction techniques acm comp 
surveys vol 
pp 
sept 
johnson superscalar microprocessor design 
englewood cliffs nj prentice hall 
vegdahl phase coupling constant generation optimizing microcode compiler proc 
th micro pp 

mimola language vers 
techn 
rep univ dortmund sept 
bell newell computer structures readings examples 
new york mcgraw hill 
barbacci instruction set processor specifications isps notation applications ieee trans 
computer jan 
describing instruction set processors nml proc 
europe 
design test conf mar 
tool specific machine descriptions code generation embedded processors marwedel goossens eds 
boston kluwer 
aho johnson optimal code generation expression trees acm vol pp july 
aho code generation tree matching dynamic programming acm trans 
prog 
lang 
syst vol 
pp 
oct 
fraser burg fast optimal instruction selection tree parsing acm sigplan notices vol 
pp 
apr 
engineering simple efficient code generator generator acm lett 
prog 
lang 
syst vol 
pp 
sept 
graham new method compiler code generation proc 
th acm ann 
symp 
principles prog 
lang 
leupers marwedel instruction selection embedded dsp complex instructions proc 
europe 
design autom 
conf sept 
araujo malik optimal code generation embedded memory nonhomogeneous register architectures proc :10.1.1.57.5855
th int 
symp 
syst 
synthesis sept 
leupers marwedel bdd frontend retargetable compilers proc 
europe 
design test conf mar pp 

paulin flexible firmware development environment embedded systems code generation embedded processors marwedel goossens eds 
boston kluwer pp 

wess code generation trellis diagrams code generation embedded processors marwedel goossens eds 
boston kluwer pp 

nowak marwedel verification hardware descriptions retargetable code generation proc 
th acm ieee design autom 
conf june pp 

rimey hilfinger compiler applicationspecific signal processors vlsi signal processing vol 

new york ieee press pp 

van graph processor model retargetable code generation proc 
europe 
design test conf mar 
bruno sethi code generation register machine acm vol 
pp 
july 
aho code generation expressions common subexpressions acm vol 
pp 
jan 
sethi ullman generation optimal code arithmetic expressions acm vol 
pp 
oct 
beg generator efficient back ends proc 
acm sigplan conf 
prog 
lang 
design pp 

optimal code generation expression trees application burs theory proc 
th acm symp 
principles prog 
lang pp 

proebsting simple efficient burs table generation proc 
sigplan conf 
prog 
lang 
design 
wilhelm maurer compiler design 
reading ma addison wesley 
instruction set matching selection dsp asip code generation proc 
europe 
design test conf feb 
graham experiment table driven code generation proc 
sigplan symp 
compiler construction pp 

ganapathi fisher affix grammar driven code generation acm tr 
prog 
lang 
syst vol 
pp 
apr 
formal language model microcode synthesis formal vlsi specification synthesis ed 
amsterdam north holland pp 

liao code generation optimization embedded digital signal processors ph dissertation mit june 
davidson fraser code selection object code optimization acm trans 
prog 
lang 
syst oct 
design application retargetable peephole optimizer acm trans 
prog 
lang 
syst vol 
pp 
apr 
marwedel tree mapping algorithms predefined structures proc 
ieee int 
conf 
computer aided design nov pp 

experience developing microcode highlevel language proc 
th annu 

workshop oct 
industrial experience rule driven retargetable code generation multimedia applications proc 
ieee acm int 
symp 
syst 
synthesis sept 
chaitin register allocation spilling graph coloring acm sigplan notices vol pp june 
chow hennessy priority coloring approach register allocation acm trans 
prog 
lang 
syst oct 
callahan koblenz register allocation hierarchical graph coloring acm sigplan notices june 
briggs register allocation graph coloring ph dissertation rice univ houston apr 
pinter register allocation instruction scheduling new approach sigplan notices june 
dependence conscious global register allocation programming language system architecture gutknecht ed 
berlin springer 
phase ordering register allocation instruction scheduling code generation concepts tools techniques giegerich graham eds 
berlin springer 
hartmann combined scheduling data routing programmable asic systems proc 
europe 
conf 
design autom mar pp 

data routing paradigm efficient datapath synthesis code generation proc 
th acm ieee int 
symp 
high level synth may pp 

malik memory bank register allocation software synthesis asip proc 
ieee int 
conf 
computer aided design nov pp 

optimizing stack frame accesses processors restricted addressing modes software practice experience vol 
pp 
feb 
liao storage assignment decrease code size acm sigplan notices vol 
pp 
june 
leupers marwedel algorithms address assignment dsp code generation proc 
ieee int 
conf 
computer aided design nov 
davidson experiments local microcode compaction horizontal machines ieee trans 
computers vol 
pp 
july 
goossens embedded software real time signal processing systems wilson integrated approach retargetable code generation proc 
th acm ieee int 
symp 
high level synthesis may pp 

leupers marwedel time constrained code compaction dsp proc 
th int 
symp 
syst 
synthesis sept pp 

register optimization scheduling real time digital signal processing architectures ph dissertation 
univ leuven belgium nov 
global scheduling high level synthesis code generation embedded processors ph dissertation 
univ leuven belgium nov 
fisher trace scheduling technique global microcode compaction ieee trans 
computers vol 
pp 
july 
gross ward suppression compensation code acm trans 
prog 
lang 
systems oct 
hwu superblock effective technique vliw superscalar compilation supercomputing 
nicolau percolation scheduling parallel compilation technique tech 
rep tr cornell univ may 
bernstein code duplication assist global instruction scheduling proc 
acm micro pp 

allan enhanced region scheduling program dependence graph proc 
acm micro pp 

lam software pipelining effective scheduling technique vliw machines proc 
acm sigplan conf 
prog 
lang 
design implement pp 

goossens loop optimization register transfer scheduling dsp systems proc 
th ieee acm design autom 
conf june 
ebcioglu nakatani new compilation technique loops unpredictable branches vliw architecture nd workshop lang 

paral 
comp aug pp 

su method global software pipelining proc 
acm micro 
rau efficient code generation horizontal architectures compiler techniques architectural support proc 
th annu 
symp 
comp 
apr pp 

su software pipelining vliw architecture optimizing compiler proc 
micro pp 

address calculation retargetable compilation exploration instruction set proc 
rd acm ieee design autom 
conf june 
gert goossens member ieee photograph biography see issue pp 

johan van member ieee received degree electrical engineering katholieke universiteit leuven belgium 
working ph degree retargetable software compilation technology university 
founded target compiler technologies leuven belgium responsible product development 
research assistant 
worked design chip architecture gsm mobile phone joint project micro electronics centre bell telephone belgium 
dirk member ieee received degree electrical engineering ph degree applied sciences katholieke universiteit leuven belgium respectively 
founded target compiler technologies leuven belgium responsible research development 
technical staff member micro electronics centre leuven belgium started research assistant high level synthesis projects initiated chess retargetable software compilation project 
werner member ieee received degrees electrical engineering le antwerp belgium katholieke universiteit leuven belgium respectively 
received ph degree electrical engineering katholieke universiteit leuven 
founded target compiler technologies responsible product development 
vlsi design methodologies division micro electronics centre leuven belgium worked high level synthesis techniques high throughput applications retargetable software compilation applicationspecific dsp cores 
high level synthesis 
received sc 
degree electrical engineering national taiwan university taiwan sc 
ph degrees applied sciences katholieke universiteit leuven belgium respectively 
member micro electronics centre retargetable software compilation group 
research assistant initially working area clifford photograph biography see issue pp 

pierre paulin member ieee photograph biography see issue pp 

proceedings ieee vol 
march 
