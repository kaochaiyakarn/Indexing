cost sensitive learning cost proportionate example weighting propose evaluate family methods converting classifier learning algorithms classification theory cost sensitive algorithms theory 
proposed conversion cost proportionate weighting training examples realized feeding weights classification algorithm done boosting careful subsampling 
give theoretical performance guarantees proposed methods empirical evidence practical alternatives existing approaches 
particular propose costing method cost proportionate rejection sampling ensemble aggregation achieves excellent predictive performance publicly available datasets drastically reducing computation required methods 
highly non uniform misclassification costs common variety challenging real world data mining problems fraud detection medical diagnosis various problems business decision making 
cases class rare cost recognizing examples belonging class high 
domains classifier learning methods take misclassification costs account perform 
extreme cases ignoring costs may produce model useless classifies example belonging frequent class misclassifications frequent class result large cost 
body attempted address issue techniques known cost sensitive learning machine learning data mining communities 
current cost sensitive learning research falls categories 
concerned making particular classifier learners cost sensitive 
second uses bayes risk theory assign example lowest risk author address toyota technological institute chicago east th street second floor press building chicago il 
zadrozny john langford naoki abe mathematical sciences department ibm watson research center yorktown heights ny class :10.1.1.26.6251:10.1.1.15.7095
requires estimating class membership probabilities case costs nondeterministic requires estimating expected costs 
third category concerns methods converting arbitrary classification learning algorithms cost sensitive ones :10.1.1.15.7095
described belongs category 
particular approach akin pioneering domingos metacost general method converting cost sensitive learning problems cost insensitive learning problems :10.1.1.15.7095
method distinguished properties simpler theoretical performance guarantees involve probability density estimation process metacost estimates conditional probability distributions bagging classifier procedure belongs second category bayes risk minimization mentioned 
family proposed methods motivated folk theorem formalized proved section 
theorem states altering original example distribution multiplying factor proportional relative cost example classifier learner accomplish expected cost minimization original distribution 
representing samples drawn challenging may 
basic methods doing transparent box supply costs training data example weights classifier learning algorithm 
ii black box resample weights 
transparent box approach applied arbitrary classifier learners applied including classifier uses data calculate expectations 
show empirically method gives results 
black box approach advantage applied classifier learner 
turns straightforward sampling replacement result severe overfitting related duplicate examples 
propose employ cost proportionate rejection sampling realize approach allows independently draw examples method comes theoretical guarantee worst case produces classifier achieves approximate cost minimization applying base classifier learning algorithm entire sample 
remarkable property subsampling scheme general expect technique subset examples compromise predictive performance 
runtime savings possible sampling technique enable run classification algorithm multiple draws subsamples average resulting classifiers 
method call costing rejection sampling aggregation 
costing allows arbitrary cost insensitive learning algorithm black box order accomplish cost sensitive learning achieves excellent predictive performance achieve drastic savings computational resources 
motivating theory methods folk theorem assume examples drawn independently distribution domain input space classifier binary output space importance extra cost associated example 
goal learn classifier minimizes expected cost ex ci training data form indicator function value case argument true 
model explicitly allow cost information prediction time include cost feature available 
formulation cost sensitive learning terms number example general cost matrix formulations typical cost sensitive learning output space binary 
cost matrix formulation costs associated false negative false positive true negative true positive predictions 
cost matrix example entries false positive true negative false negative true positive relevant example 
numbers reduced false positive true negative false negative true positive difference cost classifying example correctly incorrectly controls importance correct classification 
difference importance 
setting general sense importance may vary example example basis 
basic folk theorem states examples drawn distribution ex formulate problem way output space binary nontrivial scope 
say folk theorem result appears known straightforward derive results decision theory published 
optimal error rate classifiers optimal cost minimizers data drawn theorem 
translation theorem distributions exists constant ex classifiers proof 
ex ci ci ex ci ne despite simplicity theorem useful right hand side expresses expectation want control choice left hand side probability errs distribution 
choosing minimize rate errors equivalent choosing minimize expected cost similarly approximate error minimization equivalent approximate cost minimization prescription coping cost sensitive problems straightforward re weight distribution training set importances training set effectively drawn doing correct general manner challenging may topic rest 
transparent box weights directly general conversion examine importance weights different learning algorithms accomplish classification 
call transparent box approach requires knowledge particular learning algorithm opposed black box approach develop 
mechanisms realizing transparent box approach described number weak learners boosting describe completeness 
classifier learning algorithm weights effectively learns data drawn requirement easy apply learning algorithms fit statistical query model 
shown learning algorithms divided components portion calculates approximate expected value function query portion forms queries uses output construct classifier 
example neural networks decision trees naive bayes classifiers learning algorithm query reply pairs query oracle 
statistical query model 
constructed manner 
support vector machines easily constructible way individual classifier explicitly dependent individual examples statistics derived entire sample 
finite data precisely calculate ex tation high probability approximate expectation set examples drawn independently underlying distribution learning algorithm decomposed simple recipe weights directly 
simulating expectation method equivalent importance sampling distribution modified expectation unbiased monte carlo estimate expectation learning algorithm fit model may possible incorporate importance weights directly 
discuss incorporate importance weights specific learning algorithms 
naive bayes boosting naive bayes learns calculating empirical probabilities output bayes rule assuming feature independent output xi xi probability estimate expression thought function empirical expectations formulated statistical query model 
example xi just expectation xi xi divided expectation specifically compute empirical estimate xi respect need count number training examples output having xi th input dimension 
compute empirical estimates respect simply sum weight example counting examples 
property implementation boosted naive bayes 
incorporate importance weights adaboost give importance weights weak learner iteration effectively drawing examples subsequent iterations standard adaboost rule update weights 
weights adjusted accuracy corresponds expected cost widely decision tree learner 
standard way incorporating example weights original algorithm intended handle missing attributes examples missing attributes divided fractional examples smaller weight growth tree 
facility quinlan implementation boosted 
support vector machine svm algorithm learns parameters describing linear decision rule sign smallest distance training example decision boundary margin maximized 
works solving optimization problem minimize subject yi xi constraints require examples training set classified correctly slack training example lies wrong side decision boundary corresponding greater 
upper bound number training errors 
factor parameter allows trade training error model complexity 
algorithm generalized non linear decision rules replacing inner products kernel function formulas 
svm algorithm fit statistical query model 
despite possible incorporate importance weights natural way 
note ci ci importance example upper bound total cost 
modify ci controls model complexity versus total cost 
svmlight package allows users input weights ci works modified feature documented 
black box sampling methods suppose transparent box access learner 
case sampling obvious method convert distribution examples obtain cost sensitive learner translation theorem theorem 
turns straightforward sampling case motivating propose alternative method rejection sampling 
sampling replacement sampling replacement sampling scheme example drawn distribution examples drawn create new dataset method pass appears useful example effectively drawn distribution fact poor performance result technique essentially due overfitting fact examples drawn independently elaborate section experimental results section 
sampling replacement solution problem 
sampling replacement ex ample drawn distribution example drawn set process repeated drawing smaller smaller set weights examples remaining set 
see method fails note replacement times set size results original set assumption drawn distribution desired 
cost proportionate rejection sampling sampling scheme called rejection sampling allows draw examples independently distribution examples drawn independently rejection sampling examples obtained drawing examples keeping accepting sample probability proportional accept example probability constant chosen max leading name rejection sampling 
rejection sampling results set generally smaller furthermore inclusion example independent examples examples drawn independently know examples distributed independently cost proportionate rejection sampling create set learning algorithm guaranteed produce approximately cost minimizing classifier long learning algorithm achieves approximate minimization classification error 
theorem 
correctness cost sensitive sample sets cost proportionate rejection sampling produces sample set achieves classification error practice choose max maximize size set data dependent choice formally allowed rejection sampling 
introduced bias appears small 
precise measurement small interesting theoretical problem 
approximately minimizes cost ex ex ci proof 
rejection sampling produces sample set drawn independently assumption outputs clas translation theorem theorem know ex ci ex ci sample complexity cost proportionate rejection sampling accuracy learned classifier generally improves monotonically number examples training set 
cost proportionate rejection sampling produces smaller training set factor expect worse performance entire training set 
turns case agnostic model formalizes notion probably approximately optimal learning arbitrary distributions definition 
learning algorithm said agnostic pac learner hypothesis class sample complexity sample size dis tributions classification error rate output best achievable member probability sample size exceeds analogy formalize notion cost sensitive agnostic pac learning 
definition 
learning algorithm said cost sensitive agnostic pac learner hypothesis class cost sensitive sample complexity sample size distributions expected cost output best achievable member probability sample size exceeds formalization compare pac learning sample complexity methods applying base classifier learning algorithm sample obtained cost proportionate rejection sampling applying algorithm original training set 
show cost sensitive sample complexity method lower bounded 
theorem 
sample complexity comparison fix arbitrary base classifier learning algorithm suppose respectively cost sensitive sample complexity applying original training set applying rejection sampling 
proof 
cost insensitive sample complexity base classifier learning algorithm 
function exists exists theorem holds vacuously 
upper bound cost misclassifying example cost sensitive sample complexity original training set satisfies distribution forces classification error optimal distribution constructed forces cost optimal assigning cost examples errs 
theorem noting central limit theorem implies cost proportionate rejection sampling reduces sample size factor sample complexity rejection sampling ln ln fundamental theorem pac learning theory states 
equation implies ln note grows faster linear finishes proof 
note linear dependence sample size achievable ideal learning algorithm practice super linear dependence expected especially presence noise 
theorem implies cost proportionate rejection sampling minimizes cost better sampling worst case distributions 
remarkable property sampling scheme generally expects predictive performance compromised smaller sample 
rejection sampling distill original sample obtains sample smaller size informative original 
cost proportionate rejection sampling aggregation costing original training sample different runs cost proportionate rejection sampling produce different training samples 
furthermore fact rejection sampling produces small samples means time required learning classifier generally smaller 
take advantage properties devise ensemble learning algorithm repeatedly performing rejection sampling produce multiple sample sets learning classifier set 
output classifier average learned classifiers 
call technique costing costing learner sample set count 
rejection sample acceptance probability 
hi 
output sign hi goal averaging improve performance 
empirical theoretical evidence suggesting averaging useful 
empirical side people observed performance bagging despite throwing away fraction samples 
theoretical side considerable proves ability overfit average classifiers smaller naively expected large margin exists 
preponderance learning algorithms producing averaging classifiers provides significant evidence averaging useful 
note despite extra computational cost averaging computational time costing generally smaller learning algorithm sample set weights 
case learning algorithms running times superlinear number examples 
empirical evaluation show empirical results real world datasets 
selected datasets publicly available cost information available example basis 
datasets direct marketing domain 
data mining domains cost sensitive credit card fraud detection medical diagnosis publicly available data lacking 
datasets kdd dataset known challenging dataset kdd competition available uci kdd repository 
dataset contains information persons donations past particular charity 
decision making task choose donors mail request new donation 
measure success total profit obtained mailing campaign 
dataset divided fixed way training set test set 
set consists approximately records known person donation person donated donation 
percentage donors 
mailing solicitation individual costs charity 
donation amount persons respond varies 
profit obtained soliciting individual test set profit attained winner kdd competition 
importance example absolute difference profit mailing mailing individual 
mailing results donation amount minus cost mailing 
mailing results zero profit 
positive examples respondents importance varies 
negative examples fixed 
dataset dataset obtained dataset library nominal fee 
contains customer buying history customers known catalog 
decision making task choose customers receive new catalog maximize total profit catalog mailing campaign 
information cost mailing catalog available fixed 
percentage respondents 
purchase amount customers respond varies 
case kdd dataset importance example absolute difference profit mailing mailing customer 
positive examples respondents importance varies 
negative examples fixed 
divided dataset half create training set test set 
baseline comparison profit obtained mailing catalog individual training set test set 
experimental results transparent box results table top shows results naive bayes boosted naive bayes iterations svmlight kdd datasets importance weights 
importance weights classifiers label examples positive resulting small negative profits 
costs weights learners results improve significantly learners 
cost sensitive boosted naive bayes gives results comparable best far dataset complicated methods 
optimized parameters svm crossvalidation training set 
weights setting parameters prevented algorithm labeling examples negatives 
weights best parameters kdd method weights weights naive bayes boosted nb svmlight method weights weights naive bayes boosted nb svmlight table 
test set profits transparent box 
polynomial kernel degree kdd linear kernel 
parameter setting results impressive 
may hard problem classifiers data noisy 
note running svmlight dataset takes orders magnitude longer adaboost iterations 
failure achieve profits importance weights probably related fact facility incorporating weights provided algorithm heuristic 
far situations weights fairly uniform case fractional instances due missing data 
results indicate suitable situations highly nonuniform costs 
fact non trivial incorporate costs directly existing learning algorithms motivation black box approaches 
black box results table shows results applying learning algorithms kdd data training sets different sizes obtained sampling 
size repeat experiments times different sampled sets get mean standard error parentheses 
training set profits original training set draw sampled sets 
results confirm application sampling implement black box approach result poor performance due overfitting 
large differences magnitude importance weights typical example picked twice 
table see increase sampled training set size consequence number duplicate examples training set training profit larger test profit smaller 
examples appear multiple times training set learning algorithm defeat complexity control mechanisms built learning algorithms example suppose decision tree algorithm divides training data growing set construct tree kdd training test training test training test nb bnb svm training test training test training test nb bnb svm pruning set prune tree complexity control purposes 
pruning set contains examples appear growing set complexity control mechanism defeated 
markedly see phenomenon learning algorithms 
general size resampled size grows larger difference training set profit test set profit 
examples obtain test set results giving weights directly boosted naive bayes svm 
fundamental difficulty samples drawn independently particular density probability observing example twice independent draws probability sampling replacement greater 
sampling replacement fails sampled set constructed independently 
shows results costing kdd datasets base learners respectively 
repeated experiment times calculated mean standard error profit 
results table 
kdd case resampled set examples importance examples varies important examples 
examples set positive original dataset percentage positives 
version yields profits exceptional performance dataset 
case set examples importances vary widely fewer examples large importance kdd case 
percentage positive examples set original dataset 
learning svms kernels section default setting table 
profits sampling replacement 
kdd nb bnb svm nb bnb svm table 
test set profits costing 
section saw feeding weights directly svm obtain profit kdd dataset dataset 
obtain profits respectively 
require parameter optimization faster train 
reason speedup time complexity svm learning generally superlinear number training examples 
discussion costing technique produces cost sensitive classification cost insensitive classifier black box access 
simple method fast results excellent performance achieves drastic savings computational resources particularly respect space requirements 
property especially desirable applications cost sensitive learning domains involve massive amount data fraud detection targeted marketing intrusion detection 
desirable property reduction applies theory concrete algorithms 
reduction allows automatically apply results cost insensitive classification cost sensitive classification 
example profit profit costing nb kdd dataset costing nb dataset profit profit costing bnb kdd dataset costing bnb dataset kdd profit profit costing kdd dataset costing dataset 
costing test set profit vs number sampled sets 
bound error rate implies bound expected cost respect distribution additional property reduction especially important cost sensitive learning theory young relatively unexplored 
direction multiclass cost sensitive learning 
classes minimal representation costs weights 
reduction cost insensitive classification weights open problem 
data set library 
direct marketing association new york ny 
www dma org shtml domingos metacost general method making classifiers cost sensitive :10.1.1.15.7095
proceedings th international conference knowledge discovery data mining 
drummond holte exploiting cost sensitivity decision tree splitting criteria 
proceedings th international conference machine learning 
ehrenfeucht haussler kearns valiant 
general lower bound number examples needed learning 
information computation 
elkan boosting naive bayesian learning technical report 
university california san diego 
elkan foundations cost sensitive learning 
proceedings th international joint conference artificial intelligence 
fan stolfo zhang chan misclassification cost sensitive boosting 
proceedings th international conference machine learning 
profit profit costing svm kdd dataset costing svm dataset freund schapire decision theoretic generalization line learning application boosting 
journal computer system sciences 
bay uci kdd archive 
university california irvine kdd ics uci edu 
joachims making large scale svm learning practical 
advances kernel methods support vector learning 
mit press 
joachims estimating generalization performance svm efficiently 
proceedings th international conference machine learning 
kearns schapire sellie efficient agnostic learning 
machine learning 
kearns efficient noise tolerant learning statistical queries 
journal acm 
class probability estimation classification decisions 
proceedings th european conference machine learning 
quinlan boosting bagging 
proceedings thirteenth national conference artificial intelligence 
quinlan programs machine learning 
san mateo ca morgan kaufmann 
valiant theory learnable 
communications acm 
von neumann various techniques connection random digits national bureau standards applied mathematics series 
zadrozny elkan learning making decisions costs probabilities unknown 
proceedings th international conference knowledge discovery data mining 
