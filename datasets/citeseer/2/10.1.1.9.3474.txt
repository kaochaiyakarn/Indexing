invariant pattern recognition semidefinite programming machines graepel microsoft research cambridge uk microsoft com ralf herbrich microsoft research cambridge uk microsoft com knowledge local invariances respect pattern transformations greatly improve accuracy classification 
previous approaches regularisation generation virtual transformed examples 
develop new framework learning linear classifiers known transformations semidefinite programming 
new learning algorithm semidefinite programming machine able find maximum margin hyperplane training examples polynomial trajectories single points 
solution sparse dual variables allows identify points trajectory minimal real valued output virtual support vectors 
extensions segments trajectories transformation parameter learning kernels discussed 
experiments taylor expansion locally approximate rotational invariance pixel images usps find improvements known methods 
central problems pattern recognition exploitation known invariances pattern domain 
images invariances may include rotation translation shearing scaling brightness lighting direction 
addition specific domains handwritten digit recognition may exhibit invariances line thinning thickening non uniform deformations 
challenge combine training sample knowledge invariances obtain classifier 
possibly straightforward way incorporating invariances including virtual examples training sample generated actual examples application invariance fixed method virtual support vectors 
images subjected transformation describe highly non linear trajectories manifolds pixel space 
tangent distance approximates distance trajectories manifolds distance tangent vectors planes value kind distance classifier 
approach tangent prop incorporates invariance directly objective function learning penalising large values derivative classification function transformation parameter 
similar regulariser applied support vector machines :10.1.1.20.754
take idea considering trajectory combination training vector transformation 
data machine learning commonly represented vectors consider complex training examples represented usually infinite set constitutes trajectory goal learn linear classifier separates training trajectories belonging di erent classes 
practice may standard training example di erentiable transformation representing invariance learning problem 
problem solved transformation approximated transformation polynomial taylor expansion form 

approach powerful theorem nesterov states set polynomials degree non negative entire real line convex set representable positive semidefinite psd constraints 
optimisation formulated semidefinite program sdp 
recall sdp linear objective function minimised subject linear matrix inequality lmi minimise subject mm 

lmi means required positive semidefinite bv reveals lmi constraints correspond infinitely linear constraints 
expressive power enforce constraints training examples constraints required hold values representability theorem non negative polynomials develop learning algorithm semidefinite programming machine maximises margin polynomial training samples support vector machine ordinary single vector data :10.1.1.15.9362
semidefinite programming machines linear classifiers polynomial examples consider binary classification problems linear classifiers 
training sample 
aim learning weight vector classify examples sign assuming linear separability training sample principle empirical risk minimisation recommends finding weight vector 

constitutes linear feasibility problem easily solved perceptron algorithm 
additionally requiring solution maximise margin leads known quadratic program support vector learning :10.1.1.15.9362
order able cope known invariances generalise setting feasibility problem find 
omit explicit threshold presentation 
svm version space version space left approximated trajectories rotated usps images dashed line dotted line 
features mean pixel intensities top bottom half image 
right set weight vectors consistent images top trajectories bottom 
version space smaller determines weight vector precisely 
dot corresponds separating plane left plot 
require weight vector classify correctly transformed training example value transformation parameter 
situation illustrated 
general set constraints leads complex di cult solve feasibility problem 
consequence consider transformations polynomial form polynomial example represented polynomial row vectors 
problem written find 
equivalent finding weight vector polynomials non negative proposition nesterov paves way sdp formulation problem 
proposition sd representation non negative polynomials 
set polynomials non negative real line sd representable 
polynomial non negative 

polynomial exists 
proof 
polynomial written statement implies statement non negative polynomial written sum squared polynomials coe cient vector polynomial maximising margins polynomial samples develop sdp formulation learning maximum margin classifier polynomial constraints 
known sdps include quadratic programs special case 
squared objective minimised replacing auxiliary variable subject quadratic constraint written lmi schur complement lemma minimise subject 
constitutes sdp fact block diagonal matrix psd diagonal blocks psd 
sake illustration consider case simplest non trivial case 
matrix reduces scalar translates standard svm constraint linear case 
require psd resulting optimisation problem formulated terms second order cone program socp matrices involved 
case resulting program constitutes genuine sdp 
sake illustration consider case 
polynomial degree fully determined coe cients 
symmetric matrix degrees freedom require auxiliary variable training example general polynomial degree coe cients symmetric matrix degrees freedom require auxiliary variables 
dual program complementarity consider dual sdps corresponding optimisation problems 
sake clarity restrict presentation case 
dual general sdp maximise mm tr subject 
tr introduced matrix dual variables 
complementarity conditions optimal solution read dual formulation matrix combined part complementarity conditions reads maximise subject 
characteristic polynomial matrix quadratic solutions 
condition lower eigenvalue non negative expressed second order cone constraint 
socp formulation applicable solved ciently sdp formulation 
define extrapolated training examples program quadratic objective psd constraints formulated standard sdp form easily solved standard sdp solver addition complementarity conditions reveal optimal weight vector expanded analogy corresponding result support vector machines :10.1.1.15.9362
remains analyse complementarity conditions related example related constraints 
assuming primal dual feasibility obtain 
solution trace translates 
relations enable characterise solution propositions proposition sparse expansion 
expansion terms sparse examples support vectors may non zero expansion coe cients lie margin det 
furthermore case implies 
proof 
assume derive contradiction 
conclude proposition 
furthermore conclude det assumption implies exists inserting leads contradiction 
det implies fact ensures holds 
proposition truly virtual support vectors 
examples lying margin satisfying det det exist optimal weight vector written proof 
sketch det 
need consider case exists cases ruled complementarity conditions 
proposition possible identify examples expansion optimal weight vector corresponding values transformation parameter 
extends idea virtual support vectors semidefinite programming machines capable finding truly virtual support vectors explicitly provided training sample 
sdp solver lmi parser matlab see www user tu chemnitz de html 
extensions optimisation segment applications may desirable enforce correct classification entire trajectory polynomial example 
particular polynomial local approximation global invariance restrict example segment trajectory 
consider corollary proposition 
corollary sd representability segment 
set polynomials non negative segment sd representable 
proof 
sketch consider polynomial non negative non negative 
proposition shows restrict examples segment ectively doubling degree polynomial 
version experiments section 
note matrix sparse resulting polynomial contains powers 
multiple transformation parameters practice desirable treat transformation 
example handwritten digit recognition transformations rotation scaling translation shearing thinning thickening may relevant 
unfortunately proposition holds polynomials variable 
statement may generalised polynomials variable psd matrix polynomial non negative monomial 
means optimisation subset polynomials considering polynomials degree 
denotes gradient denotes hessian operator 
note scaling behaviour regard number parameters benign naive method adding virtual examples training sample grid 
procedure incur exponential growth number examples approximation exhibits linear growth size matrices involved 
learning kernels support vector machines derive popularity flexibility added kernels 
due space restrictions discuss kernels detail 
dual starting point assuming taylor expansion crucial point order represent polynomial trajectory feature space need di erentiate kernel function 
assume feature map kernel function corresponding sense 
exist polynomials variable non negative written sum squares sd representable 
svm error error error linear classifier learned representations usps digits see details 
note support vector truly virtual directly supplied algorithm inset zoom 
mean test errors classifiers learned svm vs see text virtual svm vs algorithm independent training sets size digit classification tasks 
taylor expansion carried inner product expression data points di erentiated respectively times reads kernel trick may help avoid sum feature space dimensions cost additional terms product rule di erentiation 
turns polynomials degree exact calculation elements kernel matrix needs approximated ciently practice 
experimental results order test illustrate known usps data set pixel images handwritten digits 
considered transformation rotation angle calculated second derivatives image representation smoothed gaussian variance 
purpose illustration calculated simple features averaging second pixel intensities respectively 
shows plot training examples digits quadratically approximated trajectories 
examples separated solution restricted segment trajectory 
propositions weight vector expressed linear combination truly virtual support vectors supplied training sample directly see inset 
second experiment probed performance algorithm full feature set pixel intensities training sets size versus classification tasks digits usps data set 
task digits class rotated digits class compared performance algorithm performance original support vector machine svm virtual support vector machine measured independent test sets size :10.1.1.15.9362
takes support vectors ordinary svm run trained sample contains support vectors transformed versions rotated quadratic approximation 
results shown form scatter plots errors tasks 
clearly account invariance useful leads performance superior ordinary svm 
performs slightly better attributed pre selection support vectors transformation applied 
expected increasing number transformations performance improvement pronounced high dimensions volume concentrated boundary convex hull polynomial manifold 
introduced semidefinite programming machines means learning infinite families examples terms polynomial trajectories generally manifolds data space 
crucial insight lies sd representability nonnegative polynomials allows replace simple non negativity constraint algorithms support vector machines positive semidefinite constraints 
demonstrated performance small data sets expected modern interior point methods possible scale problems data points particular primal space number variables number features 
expectation supported resulting sdp structured sense block diagonal small blocks 
ii may su cient satisfy constraints version perceptron algorithm semidefinite feasibility problems necessarily maximising margin 
open questions remain training multiple parameters cient application kernels 
interesting obtain learning theoretical results regarding fact ectively infinite number non iid training examples 
chapelle scholkopf :10.1.1.20.754
incorporating invariances non linear support vector machines 
dietterich becker ghahramani editors advances neural information processing systems pages cambridge ma 
mit press 
cortes vapnik :10.1.1.15.9362
support vector networks 
machine learning 
graepel herbrich shawe taylor 
semidefinite programming perceptron learning 
thrun saul scholkopf editors advances neural information processing systems 
mit press 
nemirovski 
lectures modern convex optimization 
lecture notes summer school modern convex optimization 
nesterov 
squared functional systems optimization problems 
roos terlaky zhang editors high performance optimization pages 
kluwer academic press 
rosenblatt 
perceptron probabilistic model information storage organization brain 
psychological review 
scholkopf 
support vector learning 
oldenbourg verlag munchen 
tu berlin 
download www kernel machines org 
simard lecun denker 
transformation invariance pattern recognition tangent distance tangent propagation 
orr editors neural networks tricks trade 
springer 
vandenberghe boyd 
semidefinite programming 
siam review 
