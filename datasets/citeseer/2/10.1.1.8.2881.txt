loss function analysis classi cation methods text categorization fan li yiming yang cs cmu edu yiming cs cmu edu carnegie mellon univ forbes avenue pittsburgh pa usa presents formal analysis popular text classi cation methods focusing loss functions minimization essential optimization methods decomposition loss model complexity enables cross method comparisons common basis optimization point view 
methods include support vector machines linear regression logistic regression neural network naive bayes nearest neighbor rocchio style multi class prototype classi ers 
theoretical analysis including new derivations provided method evaluation results methods reuters benchmark corpus 
linear regression neural networks logistic regression methods examples show properly tuning balance training set loss complexity penalty signi cant impact performance classi er 
linear regression particular tuning complexity penalty yielded result measured macro averaged outperformed text categorization methods evaluated benchmark corpus including support vector machines 

text categorization active research area machine learning information retrieval 
large number statistical classi cation methods applied problem including linear regression logistic regression lr neural networks nnet naive bayes nb nearest neighbor knn rocchio style support vector machine svm approaches yang liu yang joachims mccallum nigam zhang oles lewis 
methods published need sound theoretical framework cross method comparison 
machine learning focusing regularization classi cation methods analysis loss functions step direction 
vapnik vapnik de ned objective function svm minimizing expected risk test examples decomposed risk components empirical risk re ects training set errors classi er inverse margin width re ects far positive negative training examples category separated decision surface 
minimization training set errors maximization margin width criteria optimization svm 
balancing criteria referred regularization classi er degree regularization controlled parameter method section 
svm extremely successful text categorization resulting best performance benchmark evaluations joachims yang liu lewis 
hastie 
hastie general framework estimating potential model making classi cation errors slightly different terminology loss generalization error corresponding expected risk training set loss corresponding empirical risk model complexity corresponding margin related risk svm 
framework compared alternative ways penalize model complexity including akaike information criterion aic bayesian information criterion bic minimum description length mdl criterion 
interestingly compared di erences training set loss functions svm llsf lr adaboost way sensitivity methods respect classi cation errors training examples easily compared section 
valuable analyze broader range classi cation methods similar fashion comparison methods explicitly terms inductive biases respect training examples terms penalty functions model complexity 
need formal analysis optimization criterion method form loss function decomposes training set error term model complexity term 
formal analysis available literature popular text categorization methods nave bayes knn rocchio style classi ers 
primary contribution er study classi ers popular text categorization including svm linear regression logistic regression neural networks rocchio style prototypes knn nave bayes 
provide derivations loss function decomposition rocchio style nb knn multi class prototypes prototypes reported 
show importance properly tuning amount regularization controlled examinations llsf lr nnet regularization 
compare performance classi ers properly tuned regularization validation benchmark corpus reuters text categorization 
organization remaining parts follows section outlines classi ers provides formal analysis loss functions 
section describes experiment settings results 
section summarizes ers concluding remarks 

loss functions classi ers order compare di erent classi ers common basis need loss functions uni ed form 
call rst term training set loss second term complexity penalty regularizer 
notation rest training data consists pairs yn vector ip represents values input variables ith training example 
scalar speci ed class label positive examples negative examples category consideration vector consists parameters linear classi er estimated training data 
scalar classi er output input quantity shows system output agrees truth label agree completely quantity large positive number negative number large absolute value indicates poor prediction classi er 
linear mapping special case mapping input output simplicity way classi cation analysis methods generalized way classi cation problems 
focus analysis 
accordingly form training set losses di erent classi ers 
norm represented norm represented note purposely chose de ne horizontal vector vertical vector conveniently write dot product vice versa frequently seen derivations 

svm svm extremely successful text categorization 
multiple versions svm exist linear svm analysis partly clarity simplicity analysis partly linear svm performed versions svm text categorization evaluations joachims 
svm emphasizes generalization capability classi er hastie loss function class form training set loss single training example de ned rst term right hand side formula cumulative training set loss second term complexity penalty functions vector optimization svm nd minimizes sum terms formula 
words optimization svm driven training set loss driven norm vector determined squared sum coecients re ects sensitivity mapping function respect input variables 
value controls trade terms weight algorithmically determined training phase svm second term relative rst term 
formula transformed dual form solved quadratic programming 
kind analysis loss function svm new course 
fact part svm theory researchers vapnik hastie 
point start framework carry formal analysis classi ers chosen study consistent fashion classi ers formally analyzed manner 

linear squares fit llsf linear regression called linear squares fit llsf literature performed competitively svm high performing classi ers including knn logistic regression text categorization evaluations yang 
llsf similar linear svm sense learn linear mapping training data 
optimization criterion estimating strictly minimization training set error terms sum squared residuals 
loss function de ned llsf expanding right hand side rearranging obtain equivalent formula desired form llsf adding regularizer training set loss obtain loss functions regularized llsf called ridge regression 
rocchio style rocchio style classi ers widely text categorization simplicity relatively performance lewis 
construct prototype vector category centroid positive training examples centroid negative training examples 
classifying new document rocchio style classi er computes dot product cosine value new document vector prototype vector category thresholds value classi cation decision respect category 
simplicity analysis restrict discussion dot product equivalent cosine similarity input document class prototype vectors normalized 
assumption rocchio style linear classi er scoring function prototype vector 
centroid positive training examples class centroid negative training examples prototype de ned parameter rocchio style method value weight negative centroid relative positive centroid empirically determined held subset training data 
show regularized loss function rocchio style classi er bn order minimize loss function need take rst order derivative formula respect set zero dl bn easy see formula just solution 
words formula loss function rocchio style classi er trying minimize 
presenting loss function fashion enables compare rocchio style approach classi cation methods basis loss function analysis 
observing formula interesting 
loss function consists parts classi ers analyzed far 
rst part training set loss positive examples second part training set loss negative examples third part complexity training set loss single training example depends positive negative example 
ci bnc 
multi class prototype classi er multi class prototype classi er just prototype abbreviation simpler rocchio style 
rocchio style positive examples construct prototype category 
method de ned setting parameter zero formula 
accordingly regularization loss prototype method training set loss single training example ci including prototype study enables compare di erences positive training examples prototype opposed positive negative examples rocchio style 
allows compare di erences xed centroid prototype versus varied centroid text example knn describe 

knn knn popular text categorization simplicity performance benchmark evaluations yang liu lewis 
knn similar prototype training examples inside neighborhood local test example non zero loss 
nearness neighbor knn measured cosine similarity test example training example equivalent dot product vectors normalized 
simplicity analysis restrict discussion assumption normalized vectors 
assumption knn locally linear classi cation function vector coecients rk training examples nearest test example local centroid positive examples category classi cation decision test example obtained thresholding dot product 
need formally analyze exactly knn optimizing 
de ning loss function form rk setting rst order derivative right hand side zero yields coecient vector formula 
say optimization criterion knn minimization loss function formula training set error component rst term complexity penalization component second term 
accordingly training set loss single training example ci knn analyzing knn optimization criterion form loss function reported knowledge 
note emphasize local nature classi cation knn 
loss function depends test example strongly di knn classi ers 

logistic regression lr logistic regression methods shown performance competitive svm llsf knn evaluations benchmark collections yang zhang oles 
estimates conditional probability form yj def exp learns regression coecients order maximize 
equivalent minimizing training set loss de ned logarithmic form log log exp regularized version lr zhang oles loss function form log exp 
neural networks nnet neural networks shown competitive performance text categorization evaluations yang liu yang 
restrict analysis twolevel hidden layers neural network 
hidden layers similar lr sense estimate form exp objective function minimize 
loss function form consistent comparable classi ers need write 
training set loss adding regularization term yields loss function regularized nnet 
naive bayes nb restrict analysis popular multinomial nb classi er mccallum nigam 
estimates posterior probability test document member category formula cjd jc wk prior probability category probability occurring chance jc probability word conditioned category count word document logarithm sides formula yields alternative scoring function category respect document log cjd log jc log log rewriting formula jc log log cjd log log log log log log log rst term shows nb linear classi er respect input vector second term logarithm prior probability varies category making common categories preferable rare categories invariant respect third term ect scoring ranking categories eliminated consideration 
optimization nb regards estimation model parameters training data nc ck sc number positive training examples category ck frequency word positive training examples ck total number word occurrences category show relate parameter estimates nb loss function form compare nb classi ers basis 
vector represent ith training document elements document term frequencies individual words vector sum represent term frequencies category de consider nb smoothing simplicity 
consider nb laplace smoothing 
note jc 
de ne loss function form ck log minimize loss function take rst order partial derivative respect set zero ck clearly ck sc just solution 
means loss function formula optimal objective function nb estimate parameters jc equivalent maximizing likelihood ck subject 
rewrite formula function cp log word probability elements positive numbers 
means norm vector 
substituting terms yields loss function form furthermore log def 
substituting terms yields loss function form ke ke successfully decomposed nb loss function form 
note discussed nb smoothing known important ectiveness nb 
easy see second term formula ke overly sensitive estimation errors elements log jc numbers negative large absolute values word probabilities near zero 
loss function nb laplace smoothing common nb 
estimate ck sc represent vector 
note elements negative numbers log probabilities 
loss function nb ck log table 
training set loss functions regularizers classi ers classi er training set loss regularizer regularized llsf regularized lr log exp regularized layer nnet svm rocchio bnc nc prototype nc knn rk nb smoothing ke nb laplace smoothing ke ke ke comparing formula nb smoothing see correction laplace smoothing third term prevents coecients large 
words prevents classi cation decisions overly sensitive small changes input 
formulas show unique property nb classi ers uence term loss functions causes amounts regularization vary di erent categories 
knowledge rst time property explicit loss function analysis 
theoretical weakness desirable property nb requires research 

comparative analysis loss functions classi ers summarized table 
regularized classi ers nb regularizers form vector norm multiplied constant weight category speci 
regularized llsf nnet lr exactly regularizer svm di erences methods training set loss functions 
prototype nb hand exactly terms training set loss fundamentally di erent regularizer terms 
curve training set loss individual training examples shown classi er misclassi cation loss shown comparison 
axis represents loss axis value examples correctly categorized classi er assuming classi cation decisions obtained thresholding zero examples misclassi ed 
examples perfectly scored sense scores classi er total agreement true scores see llsf gives highest penalty misclassi ed examples negative large absolute value nnet gives errors lowest penalty 
words llsf tries hard correctly classify outliers relatively small scores nnet focus outliers 
correctly classi ed examples large positive value llsf method penalizes heavily 
svm nnet lr tend ignore examples giving zero near zero penalties 
hand rocchio nb prototype knn give examples minus loss neglecting 
noticed lines prototype nb linear function non zero slope positive examples slope negative examples 
re ects fact positive training examples category train category speci models methods 
knn similar sense loss functions local depending neighborhood input example omit lines knn gure 
rocchio style hand uses positive negative examples construct category prototype linear lines non zero slopes loss functions 
convenience show speci case rocchio style parameter nc gure lines positive negative examples 

empirical evaluation conducted sets experiments set global comparison classi ers text categorization benchmark collection reuters corpus version yang liu www cs cmu edu yiming set examining ectiveness loss lr svm rocchio neg pos llsf layer nnet prototype nb positive prototype nb negative 
training set loss functions classi ers individual classi ers 
chose llsf lr nnet regularizer easily implemented plugged term non regularized versions classi ers add tunable regularization component classi ers example svm obvious 
nb prototype rocchio svm knn reg lr reg nnet reg llsf performance classifiers micro avg 
macro avg 

performance reuters shows results classi ers test set reuters corpus 
macro micro averaged reported conventional performance measures text categorization evaluations yang liu 
parameters tuned fold cross validation training data 
feature selection applied documents preprocessing step training classi ers criterion feature selection classi ers 
nb selected top features 
rocchio style implemented version lewis top features set parameter 
prototype top features 
knn set top features micro avg 
performance measure top features macro avg 
performance measure 
regularized llsf lr nnet layer svm features selection 
cross validation tune classi cation threshold category probably main reason svm results better reported yang liu joachims 
test compare macro scores regularized llsf svm regularized llsf signi cantly better 
shows performance curves regularized llsf nnet lr validation set subset training data respect varying value controls amount regularization 
clearly performance classi ers depends choice value curves peak values larger zero 
llsf nnet particular having regularization properly chosen signi cant improvements cases regularization 
macro averaged curves chose regularized llsf regularized nnet lr evaluation methods test set 
heuristically tuned relative weight positive examples negative examples classi ers rst set learning rate positive examples learning rate negative examples second duplicated positive examples category number positive examples negative training examples equal 
compares results regularized llsf regularized nnet published results llsf nnet regularization data collection yang liu 
clearly new results signi cantly better previous results methods regularization played important role making di erence 
note yang liu llsf truncated svd get solution nnet layers 
scores directly comparable just indicative 
value reg llsf reg nnet reg lr value reg llsf reg nnet reg lr 
performance respect varying amounts regularization llsf reg llsf nnet reg nnet performance classifiers micro avg 
macro avg 

llsf nnet regularization 
concluding remarks loss function analysis classi cations methods popular text categorization 
main research ndings optimization criteria methods loss function consists terms training set loss regularizer complexity penalty 
proofs methods rocchio prototype knn nb new 
decomposition enables insightful comparison classi cation methods terms 
regularized llsf nnet lr exactly regularizer svm differences methods training set loss functions 
evaluation results reuters corpus show performance methods quite competitive macro micro averaged scores despite theoretical di erences loss functions 
regularization signi cant performance improvements llsf nnet reuters 
regularized llsf particular performed surprisingly training set loss function monotonic considered weakness method theoretical analysis hastie 
macro averaged performance best score reported reuters corpus statistically signi cantly outperforming svm best study 
new derivation shows nb regularizer form ke radically di erent regularizer classi cation methods 
explanation suboptimal performance nb requires research 
hastie tibshirani friedman 
elements statistical learning data mining inference prediction 
springer series statistics 
joachims 
text categorization support vector machines learning relevant features 
proceedings european conference machine learning ecml springer 
lewis yang rose li 
rcv new text categorization test collection appeared journal machine learning research 
mccallum nigam 
comparison event models naive bayes text classi cation 
aaai workshop learning text categorization 
vapnik 
nature statistical learning theory 
springer new york 
yang chute 
example mapping method text classi cation retrieval 
acm transactions information systems 
yang liu 
re examination text categorization methods 
acm conference research development information retrieval pp 
yang 
evaluation statistical approaches text categorization 
journal information retrieval vol pp 
zhang oles 
text categorization regularized linear classi cation methods 
information retrieval 
