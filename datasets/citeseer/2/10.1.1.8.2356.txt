hmms coupled hmms multi channel eeg classification variety coupled hmms chmms proposed extensions hmm better characterize multiple interdependent sequences 
introduces novel distance coupled hmm 
compares performance hmm chmm models multi channel eeg classification problem 
results show approaches examined multivariate hmm low computational complexity surprisingly outperforms models 
classification eeg important part brain computer interfaces 
overview brain computer interface systems pfurtscheller 
summarize approaches linear discrimination analysis lda artificial neural networks ann hmms classifying features extracted raw eeg data 
neural network eeg analysis modified exploit time information 
example pfurtscheller time delayed neural network collect features adaptive autoregressive ar model 
hmms heavily researched past decades especially speech recognition area successfully applied wide variety applications including eeg classification 
works eeg classification usually apply hmms feature vectors extracted ar model digital signal processing techniques 
huang mean frequency features calculated fft spectrum detecting arousal state changes 
pfurtscheller compare lda hmms bandpass filtered feature vectors experiment structure parameters hmms 
penny roberts conclude experiments synthetic data hmms capable detecting nonstationary changes perfect eeg analysis 
point operating hmms ar coefficients fundamentally flawed windowing procedure ar models may lead incorrect estimates state state transitions hmm model 
hmms model scaled raw eeg data extracted features 
approach avoids need expert knowledge construct feature extractor 
bother construct features raw data modeled 
experimental results strongly support feasibility shi zhong joydeep ghosh department electrical computer engineering university texas austin austin tx usa ghosh ece utexas edu approach 
furthermore want model multiple eeg channels simultaneously eeg data come correlated time series multiple electrodes scalp 
ways 
hmm multivariate gaussian observations straightforward approach 
univariate hmm channel combining hmms 
chmm models proposed better model multiple interacting time series processes better hmms :10.1.1.50.8099
generalized hmm models suggested enrich hmm model specific applications 
propose new chmm formulation named distance coupled hmm related mixed memory markov models 
examine sophisticated models eeg classification problem compare performance simple aforementioned hmm models 
organization follows 
section ii reviews hmms discusses chmm formulations 
section iii details formulation extended forward backward procedure training algorithm 
section iv presents experimental results eeg classification problem 
section concludes 
ii 
hmm chmm models hidden markov models standard hmm model uses discrete hidden state time summarize information observation time depends current hidden state 
hidden state time sequence hmm markov chain 
order hmms gaussian observations observation distribution normal state state sequence order markov chain 
hmm unrolled time slices shown fig 

standard hmm usually denoted triplet 
prior probability distribution hidden states 
aij aij transition probability distribution hidden states 
discrete observation case observation distribution bj bj 
continuous observation case observation distribution usually modeled mixture ot ot ot st st fig 

order hmm model 
empty circles st hidden states shaded ones ot observation nodes 
gaussians bj jl cjl observation vector modeled cjl mixture weight jl mean vector th gaussian mixture state covariance matrix th mixture state gaussian density function 
modeling eeg time series set length observation vector number channels 
call hmm scalar observations univariate hmm hmm multivariate gaussian observations multivariate hmm 
basic problems interest hmms evaluating likelihood observation sequence hmm finding hidden state sequence corresponding observation model learning parameters model set observations evaluation learning hmm exploit efficient forward backward inference procedure 
inference exact standard hmm approximate complex models discussed section 
coupled hmms various extended hmm models solve coupled sequence data analysis problems complex human action recognition traffic modeling analysis 
new models aim enhance capabilities standard hmm model complex architectures able utilize established methodologies em algorithm standard hmm models 
typical examples literature chmms event coupled hmms factorial hmms fhmms input output hmms iohmms shown fig :10.1.1.50.8099

fig 
specific type coupled hmms developed frey huang modeling class loosely coupled time series onsets events coupled time 
bengio frasconi develop iohmms fig 
address input output sequence pair modeling problem 
iohmms may viewed superset chmms hidden states previous time slice treated inputs current time slice inputs iohmm hidden states chmm inherently fig 

various new hmm architectures 
empty circles hidden states shaded ones observation nodes lightly shaded ones input nodes 
standard coupled hmms event coupled hmms factorial hmms input output hmm 
different 
certain independence assumption inputs apply hidden states em algorithm general chmms 
fhmm shown fig 
enriches representation power hidden states putting multiple hidden state chains hmm 
model training difficult impossible number hidden state chains large 
approximate inferences 
focuses chmms state model time depends states models including time 
fig shows hmm chains coupled 
chains coupled state transition probability standard hmm model 
hidden state model time easy see number free parameters transition probability matrix number hidden states chain exponential number hmms coupled 
desirable feature hinders accurate parameter learning 
variations standard chmm model size inference problems tractable 
coupled hmms proposed brand :10.1.1.50.8099
brand substitutes joint conditional probability product marginal conditional probabilities formulation erroneous right hand side properly defined probability density sum 
roberts decoupled forward variable hmm chain chmm approximation true forward variables 
computational complexity reduced exponential number hmm chains 
kwon murphy chmm model freeway traffic 
cast chmm general framework called dynamic bayesian network dbn approximate inference done boyen koller bk algorithm 
murphy weiss examine factored version bk algorithm complexity cn length sequence maximum fan node 
saul jordan reduce number parameters eq 
representing linear combination conditional marginals 
results model call mixed memory markov model joint transition probability cp develop em algorithm introducing missing variable equation training model 
propose section representation eq 
motivated specific distance coupled application 
develop different training algorithm model 
motivation iii 
formulation special parameters directly characterize coupling strengths 
replaces joint conditional probability linear combination marginals eq 
uses combination weights represent coupling strengths objects formulation motivated project involving signals emitted interacting mobile objects degree coupling depends time varying distance objects longer distances implying weaker coupling monotonic fashion 
retains power standard chmm capable modeling interactions reduced parameter space 
elements standard hmms add parameter coupling cients 
proposed model characterized new interaction parameter formulation 
forward backward procedure forward backward procedure essential part hmm inference problem 
formulation exact forward backward procedure needs exponentially large number forward backward variables 
resort approximate inference number coupled hmm chains large 
hmms coupled extended forward backward variables defined jointly hmms jc ot st st jc jc ot ot st st jc easy check variables simply decoupled 
computational complexity forwardbackward procedure practical 
note time complexity standard hmm just 
slightly modified forward variable calculated time cn hmm chain 
modified forward variable calculated inductively follows 
initialization 
induction ot ij 
termination seen factored version exact forward procedure 
experiments show calculated way close true training algorithm new forward variable produces reasonably models 
refer exact inference exact forward procedure factored inference modified forward procedure 
learning em algorithm derived learning shown saul jordan 
algorithm amounts statistics similar forward variables need factored approximation 
take different approach em train iterative algorithm transformation described baum 
transformation motivated optimality condition standard lagrange multiplier method leads iterative reestimation procedure 
convergence procedure guaranteed theorem 
theorem homogeneous polynomial zn nz zi zi zi zj zj maps zi zi satisfies zi zi 
fact strict inequality holds zi critical point derive iterative optimization procedure learning parameters 
simplicity restrict discussion optimization respect parameters subject similar stochastic constraints discussion respect easily duplicated 
lagrangian respect constraints associated ij undetermined lagrange multipliers 
easy verify locally maximized ij ij ij ik ik similar arguments parameters 
reestimation formula suggested equation exactly transformation shown eq 

transformation applied general likelihood functions polynomials positive coefficients necessarily homogeneous relaxation baum sell 
advantage learning algorithm applied minimize complex objective functions em algorithm may difficult derive 
difference standard hmm case optimization algorithm simple form calculating derivative likelihood function 
standard hmm training algorithm derivatives reduce form forward backward variables needed 
case need calculate derivatives iteratively rippling time just way calculate forward variables 
fortunately add computational complexity similar forward procedures derivatives pass time needed calculate forward variables derivatives 
eeg data iv 
experimental results real eeg data downloaded uci kdd archive website 
data arose large study examine eeg correlates genetic 
groups subjects study alcoholic control 
subject exposed stimuli pictures objects chosen snodgrass picture set 
contains measurements sampled hz second electrodes placed scalp 
extracted archive eeg datasets call eeg dataset eeg dataset respectively 
dataset contains measurements subjects alcoholic control subject 
eeg dataset alcoholic subject control subject fig 

eeg data samples alcoholic subject control subject 
contains measurements electrodes subject feature sequences electrodes data sample 
data scaled magnitude plotted fig 

seen extracting discriminative features data highly nontrivial task 
eeg dataset contains measurements electrodes subject sample runs chosen test samples 
goal recognize correct subject class normal alcoholic test sample contains feature sequences 
recognition accuracy measured percentage test samples recognized correctly 
eeg signals believed highly correlated sleep stages brain cells 
number sleep stages geva 
set number hidden states hmms model eeg data 
experimental setting want mention details training hmms 
juang levinson sondhi point mixture gaussians observation model hmm results singularity problems training 
suggest solving problem re training different initialization 
method dealing singularity problem 
rabiner find empirical study accurately estimating means gaussians critical learning models continuous hmms 
experiment smyth uses clever initialization scheme means algorithm locate means 
adopt strategy 
experiments scale eeg values avoid severe mismatches data initial random models 
eeg dataset training samples sample test samples subject 
eeg dataset fold cross validation don sample test samples 
full dataset partitioned equal sized sets set turn serves test data rest training data 
accuracy experiment averaged sets 
repeat experiment times model get average standard deviation classification accuracy experiments 
compare models total eeg classification task 
type model train models subject class 
test sample fit trained models classify higher log likelihood value 
models hmm univariate hmms trained feature sequences 
hmm univariate hmms trained feature sequences 
combined hmm models combining hmm hmm 
test sample sum loglikelihood value fitting feature sequence hmm fitting feature sequence hmm recognize sample summed log likelihood 
multivariate hmm bivariate hmms trained feature feature sequence pairs 
fhmm exact bivariate fhmms exact inference 
fhmm approximate bivariate fhmms approximate inference structured variational approximation 
exact exact inference 
factored approximate inference 
bk exact exact boyen koller algorithm applied chain chmm structure 
bk factored factored boyen koller algorithm 
fhmm models software package downloaded www cs toronto edu zoubin 
bk models bnt toolbox created murphy downloaded website www cs berkeley edu 
results discussions classification accuracy results eeg dataset shown table results eeg dataset table ii 
column lists models 
middle column presents average accuracies confidence intervals standard deviation 
column gives results tests method compared best multivariate hmm 
value means difference methods statistically significant 
method compared value shown tables 
models hmm achieve reasonably recognition accuracies indicating raw eeg data successfully modeled hmms 
accuracies table ii generally lower table table classification results eeg dataset model accuracy value hmm hmm combined hmm multivariate hmm fhmm exact fhmm approximate exact factored bk exact bk factored table ii classification results eeg dataset model accuracy value hmm hmm combined hmm multivariate hmm fhmm exact fhmm approximate exact factored bk exact bk factored sample test samples experiments eeg dataset 
multi channel models fhmms bk algorithms complex ones complex structure parameters combined hmm multivariate hmm 
comparing table ii see relative performance complex models compared simpler ones combined hmm multivariate hmm drops dataset eeg eeg 
think simpler models generalize better relatively better sample test data 
multivariate hmm emerges best approach datasets highlighted tables 
outperforms models significantly better highlighted methods 
reason may eeg experiments multiple electrodes placed head state space correlations accurately captured covariance matrix 
hmm hmm worst models channel classification information methods 
hmm performs better hmm datasets suggest feature informative dis feature 
combined hmm performs fairly boosting accuracy single channel hmms dramatically simple combining strategy 
exact superior performance eeg computationally infeasible model channels 
factored perform probably due inaccurate approximation forward variables 
interests improve approximation 
fhmm models similar multivariate hmm complex hidden state structure hidden states 
performance middle class 
performance bk algorithms probably designed specifically hmm chmm structures 
compared hmm chmm approaches including new chmm formulation multi channel eeg classification problem 
results show simple multivariate hmm superior classification accuracy low computational complexity 
multivariate hmm models interdependence sequences observation covariance matrix results suggest interactions eeg channels modeled observation space 
modeling state space chmm approaches necessarily translate better results due increasing model complexity additional associated assumptions 
proceed directions modeling channels 
apply hmm chmm approaches simultaneously model channels eeg data 
eeg data 
plan experiment larger eeg datasets part data validation set model selection probably increase classification accuracy 
better approximate inference chmm 
chmms exact inference give results computationally infeasible 
done reduce computational complexity retaining modeling power 
henri providing eeg data ghahramani providing fhmm matlab code murphy providing bnt software package 
partially supported nsf ecs gift intel 
pfurtscheller motor imagery direct brain computer communication proceedings ieee vol 
pp 
july 
ernst gert pfurtscheller timedependent neural networks eeg classification ieee transactions rehabilitation engineering vol 
pp 
december 
rabiner tutorial hidden markov models selected applications speech recognition proceedings ieee vol 
pp 

huang chung kuo ling ling tsai chen eeg pattern recognition arousal states detection classification proceedings ieee international conference neural networks vol 
pp 

pfurtscheller hmm offline classification eeg data technik june 
penny roberts gaussian observation hidden markov models eeg analysis tech 
rep tr imperial college london october 
matthew brand coupled hidden markov models modeling interactive processes tech :10.1.1.50.8099
rep mit media lab 
roberts estimation coupled hidden markov models application interaction modelling proc 
ieee int 
conf 
neural network signal processing vol 
pp 

bengio frasconi input output hmms sequence processing ieee trans 
neural networks vol 
pp 
september 
ghahramani jordan factorial hidden markov models machine learning vol 
pp 

lawrence saul michael jordan mixed memory markov models decomposing complex stochastic processes mixtures simpler ones machine learning vol 
pp 

baum inequality associated maximization technique statistical estimation probabilistic functions markov processes inequalities vol 
pp 

matthew brand oliver alex pentland coupled hidden markov models complex action recognition proc 
ieee int 
conf 
computer vision pattern recognition pp 

kwon murphy modeling freeway traffic coupled hmms tech 
rep university california berkeley may 
frey huang event coupled hidden markov models proc 
ieee int 
conf 
multimedia exposition vol 
pp 

xavier boyen daphne koller approximate learning dynamic models advances neural information processing systems denver colorado morgan kaufmann 
kevin murphy yair weiss factored frontier algorithm approximate inference dbns proceedings th conference uncertainty artifical intelligence seattle washington august 
baum inequality applications statistical estimation probabilistic functions markov process model ecology bulletin ams vol 
pp 

baum sell growth transformations functions manifolds pacific journal mathematics pp 

bay uci kdd archive kdd ics uci edu irvine ca university california department information computer science 
amir geva dan brain state identification forecasting acute pathology unsupervised fuzzy clustering eeg temporal patterns fuzzy neuro fuzzy systems medicine abraham kandel jain eds chapter pp 

crc press 

juang levinson sondhi maximum likelihood estimation multivariate mixture observations markov chains ieee trans 
inform 
theory vol 
pp 

rabiner 
juang levinson sondhi properties continuous hidden markov model representations technical journal vol 
pp 

padhraic smyth clustering sequences hidden markov models advances neural information processing systems mozer jordan petsche eds 
vol 
pp 
mit press 
