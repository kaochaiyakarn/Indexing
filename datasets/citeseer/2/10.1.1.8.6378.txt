bethe free energy contrastive divergence approximations undirected graphical models yee teh thesis submitted conformity requirements degree doctorate philosophy graduate department computer science university toronto copyright yee teh bethe free energy contrastive divergence approximations undirected graphical models yee teh doctorate philosophy graduate department computer science university toronto machine learning community tackles complex harder problems graphical models needed solve problems larger complicated 
result performing inference learning exactly graphical models expensive approximate inference learning techniques prominent 
variety techniques approximate inference learning literature 
thesis contributes new ideas products experts class models hinton bethe free energy approximations yedidia 
contribution developing new poe models continuous valued domains 
developed rbmrate model discretized continuous valued data 
applied face recognition demonstrate abilities 
developed energy models ebms flexible probabilistic models building blocks consist energy terms computed feed forward network 
show standard square noiseless independent components analysis ica bell sejnowski viewed restricted form ebms 
extending relationship ica describe sparse complete representations data inference process trivial simply ebm 
bethe free energy approximations contribution theory relating belief propagation iterative scaling 
show belief propagation iterative scaling updates derived fixed point equations constrained minimization bethe free energy 
allows develop new algorithm directly minimize bethe free energy apply bethe free energy learning addition inference 
describe improvements efficiency standard learning algorithms undirected graphical models 
ii am people years graduate student friendship patience kindness making life enjoyable apologize mentioning name person owe know just 
foremost want geoff hinton wonderful supervisor great inspiration life 
geoff generous time effort spent advising financial assistance conference trips jokes shared 
past months generously taken time busy schedule read thesis give useful comments 
committee members radford neal rich zemel brendan frey fahiem bacchus lawrence saul interesting discussion useful feedback thesis 
learned tremendously interacting collaborating wonderful people toronto gatsby 
particular owe max welling morris great collaborators friends 
max great unofficial supervisor long time collaborator bethe free energy related project times bad 
morris brian sallans proofreading thesis 
doing research fun life graduate student dull wonderful friends years 
enjoyed wild parties laid back gatsby bunch london 
time spent jo ben gil exeter road especially monthly cafe 
coming back toronto geoff got know great people including darrell colin jean jade may 
fun things feel am 
enjoyed saturday hockey various clinton street raja friends 
iii dedication parents teh ah pong chow brothers yee yee yee qin iv contents outline thesis 
background 
rate coded restricted boltzmann machines face recognition 
energy models sparse overcomplete representations 
bethe free energy inference 
bethe free energy learning 
background graphical models 
undirected graphical models 
directed graphical models 
em algorithm 
exact inference 
belief propagation 
junction tree algorithm 
markov chain monte carlo sampling 
approximating free energy 
variational approximations 
advanced mean field methods 
loopy belief propagation 
flexible models efficient exact inference 
maximum entropy models 
products experts 
discussion 
rate coded restricted boltzmann machines face recognition products experts continuous valued domains 
restricted boltzmann machines 
rate coded restricted boltzmann machines 
rbmrate facial modelling 
rbmrate face recognition 
feret database 
popular face recognition methods 
face pair rbmrate model 
comparative results 
receptive fields learned rbmrate 
discussion 
energy models sparse overcomplete representations 
square ica 
causal generative approach 
information maximization approach 
equivalence approaches 
square ica input noise 
overcomplete generalizations ica 
causal generative approach 
information maximization approach 
energy models 
relating ebms causal generative ica 
relating ebms information maximization 
parameter estimation energy models 
hybrid monte carlo sampling 
experiment blind source separation 
experiments feature extraction 
speech 
natural image patches 
cedar digits 
feret faces 
vi discussion 
bethe free energy inference 
markov networks 
ordinary inference 
belief propagation 
bethe free energy 
generalized inference 
iterative scaling 
approximate generalized inference 
bethe approximation 
relationship bp 
algorithms minimize bethe free energy 
direct fixed point algorithms 
constraining leaves trees 
graphs cycles 
experiments 
speed convergence 
accuracy estimated marginals 
discussion 
bethe free energy learning 
maximum entropy 
junction trees 
unifying propagation scaling 
constrained maximization 
efficient scheduling 
loopy iterative scaling 
region graphs 
junction graph method 
cluster variation method 
experiment 
discussion 
vii discussion products experts energy models 
bethe free energy approximations 
bibliography viii list algorithms em expectation maximization 
variational em 
contrastive divergence learning energy models 
hybrid monte carlo sampling 
loopy loopy iterative scaling 
bp iterative scaling loopy belief propagation 
ups unified propagation scaling trees 
ups unified propagation scaling 
ups jt unified propagation scaling junction trees 
ix list figures restricted boltzmann machine 
alternating gibbs sampling terms learning rules poe 
normalizing face images database 
examples processed faces 
weights learned rbmrate 
image shows weights adjacent hidden unit called receptive field 
reconstructions faces model 
cell left image original right reconstruction gibbs sampling iteration 
weights learned rbmrate restricted non negative 
rbmrate model pairs faces 
error rates methods test sets 
bars group correspond left right rank rank rank rank rank error rates 
rank error rate percentage test images similar gallery images incorrect 
left test image 
months right similar images returned rbmrate 
human observers find correct match test image 
example features learned rbmrate 
pair receptive fields constitutes feature 
example features learned non negative weight constraints 
different methods non gaussian linear components analysis 
independence properties types models 
directed graphical model corresponding causal generative approach ica 
undirected graphical model ebm 
directed graphical model representation ebm auxiliary variables clamped 
mapping information maximization approach 
evolution amari distance various algorithms blind source separation problem averaged runs 
note hmc converged just fast exact sampling algorithm exact algorithm exact slightly faster 
sudden changes amari distance due annealing schedule 
final amari distances various algorithms blind source separation problem averaged runs 
boxes lines lower quartile median upper quartile values 
whiskers show extent rest data 
outliers denoted 
filters complete ebm 
filters row ones largest power indicating represent important features 
filters second row randomly drawn remaining filters 
corresponding power spectra 
distribution power time frequency learned speech features 
envelope filter absolute value hilbert transform computed squared 
squared envelope power spectrum thresholded mapping values greater half peak value rest zero 
gaps smaller samples time samples frequency filled 
outer product templates computed weighted total power filter added diagram 
learned filters natural images 
spatial layout size filters learned natural image patches described position size bars 
polar plot frequency tuning orientation selectivity filters learned natural image patches centre cross peak frequency orientation response describing bandwidth 
learned filters cedar digits 
filters plotted whitened space clarity 
eigenfaces largest eigenvalue plotted rowwise descending eigenvalue order 
subset feature vectors learned ebm ordinary face data 
top row hand picked bottom row randomly selected 
subset feature vectors learned architecture randomly selected 
xi architecture hierarchical non linear energy model 
non linearities indicated sigmoidal units output layer 
energies contributed output variables layers number output variables need correspond number input variables 
belief propagation tree structured markov networks 
ij message sent node node ji message 
messages represented solid dotted arrows affect 
scheduling propagation updates ups 
observed nodes grey hidden nodes white 
propagation updates required scaling updates nodes path node node arrows 
scheduling scaling updates ups 
updates performed depth manner tree 
numbers describe order nodes visited 
black arrows forward traversals white arrows backtracking traversals clamping hidden nodes graph singly connected 
hidden nodes white observed nodes grey clamped nodes black 
little nodes replicas 
speed convergence various algorithms 
box lines median upper lower quartiles whiskers describe extent data 
algorithm subroutine considered converged beliefs change 
plot shows mean absolute errors various settings axis axis 
top plots show errors loopy bottom plots show errors ups 
inset shows cases black loopy converge iterations linear damping slowly increasing 
loopy bp converge errors calculated current beliefs iterations 
ordering satisfying running intersection property distribute iterative scaling change rest graph 
shafer shenoy propagation updates 
computing clique distributions messages 
computing separator distributions messages 
xii dashed lines messages correct solid line denotes message updated correct 
example graphical model 
junction graph model 
notice separator bottom contains 
region graph constructed cluster variational method 
examples images 
images binarized thresholding 
average log likelihoods training data function junction graph clique size 
curve averaged models certain maximal clique size corresponding junction tree 
models treewidth plotted standard deviation log likelihoods 
circles denote log likelihood exact maximum likelihood parameters junction graph coincides junction tree 
average distances various models 
notations 
xiii chapter machine learning researchers faced difficult task creating algorithms learn interpret understand data collected world just people 
main types learning tasks resulting different classes algorithms 
supervised learning models trained extract specific information inputs making teaching signals associate desired output input 
model trained predict desired outputs inputs 
currently popular supervised learning methods multi layer perceptrons support vector machines gaussian processes 
reinforcement learning agents act world 
rewards agents depending desirable current state world agents learn act way maximize long term rewards 
reinforcement learning viewed semi supervised learning paradigm agents told doing 
supervised reinforcement learning teaching signal provided system 
teaching signal defines goal system agree teacher maximize expected reward 
unsupervised learning signal provided models simply sense world observations 
generally means extract certain invariances infer hidden causes rise observations 
chapter 
years principled approach unsupervised learning generally adopted probabilistic model world constructed actual observations get high probability 
thesis follows modelling approach unsupervised learning uses graphical models 
addition machine learning graphical models studied variety fields including statistics applied probability data compression communication graph theory 
cope complex problems graphical models applied approximate methods learning inference graphical models proposed 
thesis describe contributions approximate learning inference 
parts thesis products experts bethe free energy approximations 
contribution developing new models continuous valued domains 
led applications face recognition interesting novel extension independent components analysis complete case 
bethe free energy approximations contribution developing new theory connecting belief propagation iterative scaling 
allow develop algorithms minimizing bethe free energy learn graphical models bethe free energy approximations 
outline thesis background chapter describe popular techniques approximate inference learning 
serve backdrop place contributions thesis 
describe directed undirected graphical models related theory exponential families em algorithm section 
described exact inference algorithms particular belief propagation junction tree algorithm section 
section describe markov chain monte carlo sampling inference 
section describes major classes approximate inference algorithms variational approximations section ad chapter 
mean field methods section loopy belief propagation section 
understood approximating free energy interesting links 
dealing partially observed directed graphical models approximate inference algorithm translates directly approximate learning algorithm em algorithm 
section deals undirected graphical models 
fully observed inference trivial learning hard due partition function 
describe maximum entropy models section products experts section 
rate coded restricted boltzmann machines face recognition chapter describe new product experts model continuous domains 
particular describe extension restricted boltzmann machines handle discretized continuous values 
section describe restricted boltzmann machines detail extension discretized continuous values 
apply rated coded restricted boltzmann machines face modelling section face recognition section 
show rate coded restricted boltzmann machines comparable popular models face recognition 
models plenty room development improve recognition accuracy 
energy models sparse overcomplete representations rate coded restricted boltzmann machines step modelling continuous values products experts limited model discretized values 
chapter describe new class products experts called energy models naturally handle continuous values 
energy models interesting relationship independent component analysis take independent component analysis starting point chapter 
section give overview relationship including views independent component analysis 
show views reduce model square noiseless case section differ chapter 
complete case sections 
discuss learning energy models contrastive divergence section show section contrastive divergence learning gives results compared exact methods 
section gives simulation results showing energy models extracting interesting features 
bethe free energy inference particular chapter deals unified propagation scaling algorithm directly minimizing bethe free energy 
section describe generalization inference minimizing kl divergence subject certain observational constraints 
section describe approximations generalized inference bethe free energy show interesting relationship belief propagation iterative scaling 
section describe various algorithms minimize bethe free energy approximation generalized inference culminating unified propagation scaling 
section describe experiments comparing unified propagation scaling loopy belief propagation 
bethe free energy learning chapter described theory relating belief propagation iterative scaling derive algorithm minimize bethe free energy inference 
chapter fact iterative scaling standard algorithm learn undirected graphical models show apply theory learn undirected graphical models 
section describe maximum entropy framework relationship maximum likelihood learning undirected graphical models 
derive iterative scaling exact learning algorithm undirected graphical models 
section describe improvements efficiency iterative scaling junction trees 
ideas developed chapter propose section unified propagation scaling junction trees algorithm improves efficiency iterative scaling junction trees 
learning tractable section propose approximations chapter 
region graph free energies loopy iterative scaling optimize approximate free energies 
section show loopy iterative scaling working simple problem 
chapter background graphical models significant advances machine learning decade 
graphical model operations typically performs inference learning 
typically costly operations variety schemes proposed literature approximate sidestep 
survey 
graphical models years graphical models gained prominence viable class probabilistic models suitable unsupervised learning jordan 
graphical models graphs probabilistic semantics 
nodes graph represent variables structure graph describes conditional independencies variables 
particular lack edge nodes means conditionally independent observations subset nodes 
visible nodes representing observations hidden nodes representing unobserved causes world 
graphs directed undirected edges giving rise classes graphical models shall describe section 
graphical model main operations performs 
train graphical model assign high probability observations infer posterior chapter 
background distribution hidden variables observations 
learning assume graphical structure fixed fine tune parameters assign high probability observations 
learn structure graphical model bayesian approach average structures parameters cooper herskovits lam bacchus heckerman friedman beal ghahramani 
inference typically need posterior distribution explicitly expectation certain functions posterior distribution non linear combinations expectations 
example functions delta functions indicating value subset variables expectations marginals posterior 
learning inference graphical models intimately tied 
em algorithm introduced section variants common methods learning graphical models subset variables observed 
part algorithm involves inferring posterior distribution hidden nodes observations 
undirected graphical models looser notions learning inference seen dual see wainwright 
explore relationship contexts directed undirected graphical models 
subsections describe undirected directed graphical models 
keep exposition simple deal completely observed case random variable observed learning 
partially observed case described subsection em algorithm 
undirected graphical models take set variables empirical distribution wish learn parameters undirected graphical model 
typically average number delta functions particular training sample 
undirected graphical models represent affinities nodes undirected edges 
chapter 
background clique graph potential non negative function states clique 
probability distribution product cliques undirected graph normalization constant called partition function 
set conditional independency statements undirected graphical model expresses 
disjoint sets variables 
path node node contains node say separates conditional independencies expressed undirected graphical model exactly disjoint subsets separates hammersley clifford theorem relates family distributions vary potential functions set conditional independencies expressed undirected graphical model clifford 
particular distribution expressed satisfies 
conversely distribution satisfies satisfies expressible settings potentials 
converse slightly weaker certain degenerate distributions satisfy see lauritzen 
suppose potential function parameters expected log likelihood log log log learn parameters maximize expected log likelihood respect differentiating respect get log log log clique totally connected subset nodes 
chapter 
background second term obtained differentiating log log log functions log xc usually easy compute 
cliques small means term easy just average training data note dealing completely observed case 
second term trickier requires expectation intractable 
case normal routine take samples approximate second term averaging samples hinton sejnowski zhu 
chapter describe approach loopy belief propagation 
note calculating second term construed inference expectation model distribution taken posterior observation 
instance learning requiring inference shall encounter 
em algorithm section cases interest log potential functions linear parameters log fixed vector valued function 
exp undirected graphical model describes exponential family sufficient statistics functions natural parameters may linearly independent representation minimal 
learning rule simplifies log setting derivatives zero see learning attempts match sufficient statistics model sufficient statistics data case attained log likelihood concave parameters 
note products experts described section nice property 
chapter 
background note understand learning process estimating natural parameters sufficient statistics inference essentially computing sufficient statistics set natural parameters 
learning inference dual case exponential families 
duality simple consequence beautiful theory exponential families called information geometry 
elaborate refer reader amari gupta wainwright chap 
brief excellent 
aspect duality maximum likelihood maximum entropy explored section chapter 
directed graphical models directed graphical models causal models represent cause effect relationships nodes directed edges edge means parent direct cause probability distribution nodes jpa pa parents node set conditional independencies represented directed graphical models slightly complex undirected ones 
disjoint sets nodes 
say separates node path node node holds edges containing path point inwards descendants set conditional independencies represented directed graphical model disjoint subsets separates chapter 
background show set distributions satisfying conditional independencies exactly distributions having form need strictly positive time 
notice individual distributions family satisfy conditional independencies addition 
suppose conditional distribution jpa parameterized derivative log likelihood respect log log jpa partition function involved learning directed graphical models simpler 
addition derivative depends single solving decoupled 
note true completely observed case 
subsection describe partially observed case dependencies parameters introduced unobserved variables iterative scheme em algorithm needed optimize 
em algorithm sections discussed learning graphical models variables observed 
build models certain variables unobserved hidden modelling reasons wish discover hidden causes observed data increases modelling flexibility 
simple example illustrating reasons mixture models 
discuss learning graphical models directed undirected partially observed data em algorithm dempster neal hinton 
assume nodes partitioned sets visible observed variables hidden unobserved variables 
empirical distribution training set model distribution yj fx yg parameters 
chapter 
background log probability generating observations log yj log yj dy em algorithm algorithm iterative procedure iteration starts initial produces better estimate new algorithm em expectation maximization 
initialize values 

repeat convergence criterion met 
expectation step inference step 
fills unobserved variables current posterior distribution xjy xjy 
normally done independently training observation 
maximization step learning step 
set new maximize complete data likelihood assuming current posterior correct new argmax log yj xjy dx dy maximizing complete data likelihood step find new improves likelihood 
called generalized em gem algorithm 
em gem algorithms shown decrease log likelihood new 
assuming bounded technical conditions differentiability apply shown algorithm converge local maximum dempster 
simple models linear gaussian models tree structured graphical models step exactly efficiently carried 
complex models need approximate inference step 
possibility approximate samples chapter 
background posterior obtained markov chains see section 
deterministic approximations inference section section describe complex models allow exact inference 
directed graphical models step usually simple harder undirected graphical models due partition function 
result sophisticated algorithms iterative proportional fitting ipf generalized iterative scaling gis discussed section required 
contrastive divergence approach section problem approximately optimizing different function log likelihood 
theory em algorithm generalized neal hinton em viewed coordinate minimization variational free energy var log yj log xjy xjy dx dy step minimizes var respect step minimizes var respect 
step normally performed independently training observation expressed var upper bound equality exactly xjy xjy support 
easily seen rearranging var log yj kl xjy kp xjy dy term kl divergence xjy xjy kl kp log dx formulation analogous generalized steps generalized steps update approximate posterior xjy lower kl xjy kp xjy wake sleep algorithm learn helmholtz machine dayan recognition networks morris morris similar idea 
parameterize xjy follow physics convention free energies minimized neal hinton negated maximized 
chapter 
background single set parameters improve approximation step gradient descent reverse kl divergence 
minimization possible distributions xjy intractable restrict xjy come tractable family distributions resulting lower bound 
larger family give smaller kl divergence better bound seen 
starting point variational approximations discussed section 
var analogous free energy statistical physics 
link enabled methods developed statistical physics community neural computations generalized belief propagation vice versa 
methods discussed sections 
exact inference section describe algorithms doing exact inference graphical models 
start simple case graphical model tree 
belief propagation algorithm section 
generalize junction tree algorithm general graphical models section 
belief propagation belief propagation bp exact inference algorithm tree structured graphical models pearl 
trees directed undirected rest section shall deal undirected trees 
directed trees easily converted undirected ones simply dropping directionality edges suppose tree shall denote vertices ij shall mean edge random variable associated state ij marginal pairwise potentials tree structured general directed graphs including polytrees directed trees root need graph dropping directionality add edges parents node 
chapter 
background undirected graphical model ij ij edge ij ij message vice versa ji 
bp iteratively updates messages rules new ij ij nj ki nj indicates neighbours message updates ordered efficiently needs updated convergence 
number edges updates 
example forward backward algorithm hmms just bp applied chain messages updated forward backward pass converges passes 
beliefs obtained messages ji ij ij nj ki ni lj convergence show beliefs exactly marginal distributions 
bp applied graphical models cycles 
particular algorithm just described applied directly pairwise markov networks 
pairwise markov network undirected graphical model take edges cliques form distribution exactly 
loopy bp pairwise markov networks approximate algorithm 
section elaborate exactly approximation junction tree algorithm general graphical models junction tree algorithm popular algorithm exact inference jensen cowell 
section shall briefly describe construction junction trees propagation algorithms run junction trees 
prove nice graph theoretic results relating junction trees 
please see jensen cowell information 
chapter 
background directed graphical models converted undirected ones 
essentially means adding edges parents node dropping directionality original edges 
form distribution directed graphical model stays undirected 
moralization simply sure term corresponds clique undirected model 
undirected graphical model collect nodes clusters topology clusters forms junction tree 
edge graph cluster tree nodes labelled clusters clusters intersection subset cluster path tree 
typically obtained eliminating nodes time 
node elimination ordering add edges neighbours node form clique 
eliminating nodes maximal cliques resulting graph forms clusters junction tree 
junction tree collect potentials original graphical model potentials cluster junction tree distribution equivalent product clusters junction tree 
number propagation algorithms junction trees 
foremost shafer shenoy ss shafer shenoy hugin jensen 
ss propagation direct generalization bp junction trees 
neighbouring clusters junction tree separator messages sc sc going back 
messages updated ss propagation rules sc ns product separators neighbouring 
schedule updates message needs updated 
convergence marginal chapter 
background distributions exactly beliefs sc sc sc hugin propagation hand updates beliefs directly 

neighbouring clusters separator 
updates schedules require twice number clusters updates convergence 
fact show ss propagation hugin propagation equivalent identifying beliefs messages 
computational complexity junction tree algorithm exponential size largest cluster 
smaller graph computational speed significant 
unfortunately graphical models interest cluster sizes large practical 
loopy bp generalizations natural extensions efficient approximate inference yedidia 
proposed algorithm learning undirected graphical models essentially junction tree algorithm inner loop 
chapter describe generalization algorithm loopy bp ideas 
markov chain monte carlo sampling approximate inference graphical models samples posterior distributions 
distributions complex multi dimensional typically probability mass concentrated small regions space 
result simple monte carlo sampling schemes suitable 
markov chain monte carlo mcmc sampling viable sampling method distributions 
mcmc sampling versatile applicable virtually circumstances wide range methods choose 
asymptotically correct prepared wait long chapter 
background markov chains converge obtain large number samples 
computationally expensive leading various issues trade offs 
large literature mcmc sampling introduce issues methods relevant neural computation 
refer neal gilks 
thorough reviews 
mcmc sampling ergodic markov chain transition conditional distribution jx constructed equilibrium distribution desired posterior distribution xjy 
starting simple initial distribution markov chain simulated sampling jx time step jx dx distribution assume xjy absolutely continuous respect measure 
ergodic xjy respect 
independent samples desirable reduce variance sampler 
get independent samples run multiple markov chains convergence get sample 
wasteful markov chains take long converge 
sample taken markov chain 
fact better samples markov chain initial burn period 
number chains run length burn period number samples obtain chain optimized empirically 
multi dimensional distributions state space negligible probability markov chain small random changes state step 
distribution multi modal hard markov chain move mode markov chain move regions low probability separating modes 
methods tackling problem including simulated annealing kirkpatrick neal simulated tempering parisi neal metropolis coupled mcmc sampling geyer entropic sampling lee 
markov chains small step sizes move state space 
chapter 
background result exhibit random walk behaviour 
random walk distance travelled grows square root time spent 
moving mode requires significant amount time 
hybrid monte carlo methods relaxation neal proposed reduce random walk behaviour 
result multiple modes random walk behaviour time required markov chain converge equilibrium distribution mixing time quite long 
know mixing time chain upper bound derive typically large 
methods detecting convergence proposed perfect 
surprisingly developed techniques coupling generate exact samples equilibrium distribution markov chains propp wilson 
getting exact samples eliminates problem convergence detection 
efficient markov chains satisfying certain monotonicity property 
bounding chains non monotonic chains including markov chains machine learning 
result exact sampling methods widespread machine learning 
methods coupling way useful techniques mcmc sampling neal 
sequence distributions defined interpreted approximations posterior distribution converges posterior 
fact shown var var sallans 
mcmc step thought generalized steps 
course really compute obtain samples gradient ascent step useful way thinking sampling part stochastic gradient ascent step gradient parameters estimated samples approximate posterior generalized step updates approximate posterior exactly 
taken light samples mcmc step estimating parameters 
parameters updated chain converged 
efficient computational resources 
algorithm guaranteed improve var stochastically long learning rate small decreases slowly parameters chapter 
background converge ml solutions 
called brief gibbs sampling hinton 
drawback algorithm mcmc samples stored training case easily generalized online learning 
hinton proposed gibbs sampling step start markov chain fixed state simulate markov chain steps take samples ignoring chain converged 
steps improve var step take account changes approximate posterior parameters changed algorithm guaranteed stochastically improve var practice tends better original brief gibbs sampling variance samples smaller contrastive divergence learning algorithm products experts section viewed improvement scheme 
mcmc techniques useful need evaluate expectations respect posterior distribution asymptotically unbiased manner 
asymptotically unbiased applicable circumstances preferred choice statisticians 
due computational resources required get samples reduce variance acceptable level running chain steps reduce bias acceptable level seen practical inference learning machine learning 
techniques brief gibbs sampling efficient mcmc practical learning 
approximating free energy variety approximation methods understood approximating intractable free energy minimizing resulting approximate free energy 
includes variational approximations advanced mean field methods statistical physics loopy belief propagation 
discuss detail subsections 
personal communication sallans 
chapter 
background variational approximations recall variational free energy section var log yj log xjy xjy dx dy log yj kl xjy kp xjy dy var upper bound negative log likelihood em coordinate minimization var respect 
var intractable upper bound introducing additional auxiliary variables resulting expression tractable jordan 
called variational approximation 
algorithms derived variational approximations deterministic orders magnitude faster mcmc sampling 
improve upper bound var convergence guaranteed easily detected 
guaranteed improve likelihood step likelihood improve bound get tighter 
posterior distribution xjy intractable expensive minimize var respect minimize var respect assuming comes tractable family distributions 
called block variational approximation see tighter bound equivalent having family distributions approximates posterior terms kl divergence 
rest section shall deal block variational approach 
yj graphical model posterior distribution defined undirected graphical model obtained original graphical model conditioning observed nodes 
moralized graph dense computing xjy intractable 
approximate posterior graphical model obtained removing edges tractable case assumptions imposed jordan 
calls block variational method commonly distinguish approach described previous paragraph 
best distribution may obtained removing edges 
chapter 
background exactly additional conditional independencies expressed removal edges graphical model 
edges remove tighter resulting bound 
simplest approximation obtained removing edges assuming hidden variables independent observations called naive mean field mf approximation popular variational approximation simplest 
removes edges posterior worst approximation get removing edges 
handle explaining away effects essentially dependencies hidden variables conditioned observations 
mf approximation elaborated statistical physics point view section 
obtains better bound retaining edges posterior graphical model resulting distribution tractable 
careful edges retain coupled hidden variables dependent approximation bound var tighter 
termed structured variational approximation 
looking note minimizing var respect equivalent decreasing kl divergence xjy 
kl divergence symmetric equivalent decreasing kl xjy kq xjy 
mf approximations means optimal jy marginal posterior jy 
general reversed kl divergence means costly xjy small xjy large reverse 
posterior consists separated modes single modal distribution try model single mode posterior capture 
certain applications important capture modes 
example medical diagnosis applications observations symptoms hidden variables diseases better conservative report possible disease occurrences 
cases suitable approximations try decrease kl divergence frey minka kappen rodriguez morris 
chapter 
background see minimizing var respect interpreted maximizing regularizing term kl xjy kp xjy dy encourages xjy close xjy possible 
xjy simpler distribution xjy tend simpler way 
hand encourages model simple posteriors control complexity learned model 
hand resulting distribution accurate try ignore certain dependencies hidden variables 
exciting advance development variational bayesian learning ghahramani beal 
prior distribution observations fy correct bayesian learning compute posterior jy yj yj learning probability observation jy jy posterior fx coupled parameters model components mixture model permuted affecting probabilities exact bayesian learning intractable simple models mixtures gaussians 
approach approximate posterior jy single maximum posteriori map estimate 
em algorithm gives 
distribution yj exponential family natural parameters prior conjugate yj variational approximation independent 
variational free energy var log log dx variational em algorithm algorithm 
prior conjugate yj functional form yj terms 
chapter 
background algorithm variational em 
initialize simple distribution 

repeat convergence criterion met 
variational step maximize var respect exp eq log 
variational step maximize var respect exp eq log advantage method computational cost variational em order normal em practice marginally expensive normal em algorithm 
variational step done performing normal step single pseudo observation variational step done simply extracting certain sufficient statistics 
propagation algorithms kalman filter needed inference generalized variational case ghahramani beal 
modest additional costs advantages enormous 
lower bound evidence model 
compare models perform model selection 
resulting model simply flexible powerful single estimate 
avoid fitting integrate parameters number hyperparameters independent model size 
advanced mean field methods section introduced naive mf approximation variational approximation posterior assumed factorized 
term mean field originated statistical physics roughly means replace fluctuating field surrounding unit mean value 
field surrounding unit influence units unit 
naive mf approximation simplest wide spectrum methods statistical physics 
chapter 
background section shall describe methods developed applied inference graphical models 
information please refer saad opper 
illustrate ideas keeping notation simple shall focus developing mf methods inference boltzmann machines bm 
boltzmann machines undirected graphical models potentials involve nodes 
denote nodes graph variable associated ij weight connecting nodes bias node probability state fx defined exp ij ij partition function 
suppose number nodes observed recall variational free energy var eq ij ij log notational simplicity defined omitted constant log added inverse temperature 
mentioned shall assume 
naive mf approximation assume factorizes suppose mean node eq set eq fm naive mf free energy mf ij ij log log minimizing mf respect get mf fixed point equations new sigmoid ij suppose assume mean values eq node assume minimum attainable var called gibbs free chapter 
background energy gibbs minff var eq ig wish minimize gibbs respect value minimizes gibbs exact posterior marginals jy minimizing gibbs usually tractable 
gibbs free energy important fertile territory approximation schemes 
particular expand gibbs terms evaluate expansion 
called expansion 
gibbs gibbs gibbs 
gibbs 
deriving terms expansion requires quite amount ingenuity terms readily obtained gibbs log log gibbs ij ij gibbs ij ij see terms correspond mf understand naive mf approximation bad simply linear approximation true free energy 
including third term obtains tap free energy tap 
setting derivatives tap respect get tap equations sigmoid ij ij extra term right called reaction term 
tap longer lower bound var tap equations guaranteed converge 
converge tend converge better estimates true posterior marginals mf equations 
conceivable extra terms derived obtain better estimates posterior marginals 
shall view bethe approximation section chapter 
background light 
directed graphical models gibbs free energy particularly nice form similar approaches expand quantities interest kl divergences variants tap equations derived tanaka kappen rodriguez 
expansion understand contribution different substructures bm gibbs free energy posterior distribution 
term appearing expansion diagram drawn summarize contribution term follows vertex drawn index appearing term edge drawn connecting vertices ij appears term 
easily seen terms appearing expansion diagrams subgraphs bm treating multiple edges connecting vertices single edge 
obviously terms strongly irreducible corresponding diagram split removing vertex georges yedidia welling teh 
expansion tree contain relatively simple terms form ij function cycles inference hard bms introduce complex terms gibbs free energy 
shall revisit matter section try understand loopy belief propagation works 
different mf method called linear response correction 
initial observation study system represented model studying perturbations vary parameters system 
particular consider varying biases original value say small variations 
express distribution partition function functions variations direct differentiation get log log setting shows recover cumulants chapter 
background derivatives log words log partition function cumulant generating function desired distribution mf approximation obtain estimates approximate correlation hidden nodes example naive mf approximation bm differentiate get ij ij matrix ij th element 
linear response correction fully visible bm learning weights biases estimated directly empirical pairwise correlations marginals single matrix inversion kappen rodrguez 
mf methods applied machine learning field theoretic method cavity method bethe approximation 
methods refer opper winther 
loopy belief propagation relation bethe approximation discussed section 
summary important remember methods guaranteed converge simplest mf approximation bound variational free energy compute 
careful converge give estimates posterior accurate simplest mf approximation 
example tap bethe approximations introduced section shown frequently converge weights small high evidence nodes observed 
advanced mf methods suited part learning algorithm em suitable accurate estimates posterior marginals required convergence 
technically cumulant generating function logarithm moment generating function log ep easy see log log log constant 
chapter 
background loopy belief propagation section shall discussing loopy belief propagation loopy bp 
loopy bp just algorithm bp iterations applied alteration graphs cycles 
junction tree algorithm loopy bp approximate inference algorithm guaranteed converge 
converge converges quickly approximation typically quite accurate murphy :10.1.1.32.5538
loopy bp applied successfully error correcting codes turbo decoding mceliece machine vision freeman pasztor 
matter fact turbo decoding discovered realization loopy bp appropriate graph spurred research behaviour loopy bp 
initial experimental results showed loop bp converges produce estimates posterior marginal distributions accurate obtained naive mf methods weiss murphy :10.1.1.32.5538
experiments showed loopy bp converges graphs longer fewer strongly coupled cycles 
theory equal amounts double counting developed shows loop bp give correct map estimates certain class balanced networks weiss 
elaborated linking unrolled networks gibbs measures computation trees jordan 
breakthrough understanding loopy bp yedidia 
discovered fixed points loopy bp exactly stationary points bethe free energy 
notation section bethe free energy bethe ij ij log ij log ij ij log ij log number neighbours node beliefs ij need satisfy marginalization constraints ij 
note constraints imply beliefs marginal distributions single distribution 
chapter 
background expressed adding lagrange multipliers 
messages loopy bp related lagrange multipliers message updates just set consistency equations satisfied stationary points bethe consistency equations derived setting derivatives lagrangian respect beliefs zero 
experimentally stable fixed points loopy bp local minima bethe heskes proven fact local minima 
bethe approximation gibbs free energy gibbs 
consider bethe gibbs bm 
bethe exact tree deduce general bms bethe consists exactly terms expansion form ij exact relationship loopy bp advanced mf methods tap 
explains finding loopy bp performs better graphs fewer longer cycles weakly coupled nodes 
graphs terms expansion bethe terms tend smaller involve product ij ij term small bethe approximation gibbs want minimize gibbs free energy bethe free energy bethe approximation gibbs free energy sense try minimize bethe free energy 
loopy bp just variety methods minimize 
belief optimization bo method directly minimizes bethe respect beliefs iteration welling teh 
bo works boltzmann machines 
chapter describe ups algorithm guaranteed minimize bethe bo ups works general pairwise markov networks 
stable nature bo ups suitable part learning algorithm 
welling teh showed boltzmann machine case bethe tap equivalent second order bethe higher order terms 
suggests bethe approximation accurate tap approximation 
confirmed standard usages term lagrangian classical mechanics describe movements masses constrained optimization constructed objective includes original objective minimized constraints 
thesis shall second meaning common machine learning community 
chapter 
background experimentally showed bo performs better tap 
strongly coupled clusters nodes loops bethe approximation bad 
account errors introduced clusters beliefs spanning clusters cluster adding extra terms approximation 
generalization bethe free energy called kikuchi free energy 
larger clusters accurate kikuchi approximation 
willing large clusters kikuchi free energy exact 
models know strongly coupled clusters just locally connected units neighbouring pixels images practice small clusters needed approximation 
derive generalized bp algorithm fixed points stationary points kikuchi free energy yedidia yedidia 
reasonably small clusters generalized bp shown converge gives accurate estimates beliefs yedidia 
flexible models efficient exact inference starting model intractable inference learning possibility start model efficient exact inference sidestep need approximate inference techniques 
maximum entropy models products experts discussed subsections 
disadvantage models deal partition function learning 
maximum entropy models approaches restrict application models calculate partition function brute force mcmc sampling 
products experts idea approximate learning method called contrastive divergence 
maximum entropy models maximum entropy principle probabilistic modelling states knows process introduce distortion modelling process chapter 
background simplest distribution 
achieved distribution maximum entropy compact state space uniform distribution 
know modelled process terms certain constraints distribution distribution uniform possible maximum entropy subject constraints satisfied 
shall deal particular constraints expressed expectations functions process 
constraints represented way maximum entropy distributions constraints exactly exponential families 
commonly encountered models fact exponential families 
empirical distribution wish model 
vector valued function vector features 
constraint distribution expressed lagrange multipliers easy show maximum entropy distribution form exp lagrange multipliers chosen partition function 
note determine require knowledge vary maximum entropy distributions form exactly exponential family sufficient statistics functions sufficient statistics natural parameters 
satisfy constraints fact maximum likelihood parameters exponential family empirical distribution duality maximum entropy maximum likelihood important aspect information geometry referred section 
popular exponential families gaussians simple features constraints 
model complex processes extend distribution hidden variables example graphical models sigmoid belief networks 
note hidden variables integrated model general exponential family 
hidden variables useful purposes example discover model hidden causes observations inferring posterior distributions intractable 
want model process terms high likelihood possibility simply complex features 
approach chapter 
background taken maximum entropy modelling maxent 
note arbitrarily complex deterministic function inference trivial just evaluate feature log likelihood probability distribution hidden variables 
long distribution allows efficient inference inferring distribution hidden variables efficient 
products experts approach section 
apply maximum entropy probabilistic modelling problems solve 
find set features della pietra zhu :10.1.1.43.7345:10.1.1.43.7345
log probability training data log log log log just negative entropy 
limit large training set overfitting concern choose feature set maximum entropy distribution minimum entropy maximum likelihood 
zhu 
called minimax entropy principle 
usually achieved double loop algorithm finds maximum entropy inner loop greedily adds feature maximizes likelihood outer loop 
avoid fitting preference simpler features usually greedy search 
second problem maximum entropy modelling determine parameters satisfy constraints 
news log likelihood log convex function unique global maximum bad news due partition function expensive integrate determine 
simplest method optimize gradient ascent 
log easily computed data 
second term coming partition function requires expectation 
normally intractable approximate inference typically evaluate log likelihood constant due partition function usually purposes 
inference mean calculating expectations 
chapter 
background procedure required estimate expectations 
normally highly multimodal techniques variational approximation suitable problem typical variational techniques upper bound lower bound log likelihood case 
gibbs sampling simulated annealing applicable method conditional distribution variable computed easily 
chapter introduce approximate method learn maxent models approximating free energy 
gradient ascent troublesome learning rate tweaked performance different learning rates required different features regions 
improved iterative scaling iis general procedure determine need set parameters learning rate della pietra :10.1.1.43.7345:10.1.1.43.7345
iis needs sample estimate needs calculate exactly 
assume iteration iis updated amount unique solution equation exp sum components 
equations decoupled solved independently rest 
constant solved directly obtain log reduces generalized iterative scaling gis algorithm darroch ratcliff 
true example multinomial distributions exactly features value rest 
constant effective way solving newton method care converge berger 
gentle iis proof see berger details consult berger 
iis gis updating parameter iteration update chapter 
background subset parameters time keeping rest fixed 
case updated 
extreme case gis reduces iterative proportional fitting ipf algorithm deming stephan 
ipf steps larger gis iis steps smaller 
ipf sensitive noise sampling 
cases sampling required ipf preferable gis iis teh welling 
maximum entropy modelling successfully applied unsupervised modelling textures zhu word morphology della pietra :10.1.1.43.7345:10.1.1.43.7345
effective maximum entropy modelling complex conditional distributions xjz efficiently computed sampling 
examples applications maximum entropy nigam berger lafferty 
products experts product experts poe way combining multiple models produce complex model 
model called expert set hidden variables parameters product defined xj parameters models fx hidden variables 
hinton information 
examples restricted boltzmann machines rbm smolensky products hmms brown hinton 
initial results indicated viable natural language processing brown hinton hand written digit recognition hinton face recognition teh hinton reinforcement learning sallans hinton 
way combining models mixture models 
mixture model localist representation product model uses distributed representation 
mixture chapter 
background component chosen time model observation 
component model aspects observation specializing small region space models 
mixture model sharply tuned individual components high dimensional spaces mixture need exponentially components able model observations 
hand product model expert model feature observation experts cooperate model observation 
result efficient models high dimensions 
marginalizing yj yj exp log yj poe viewed special case maximum entropy distribution feature comes differentiable parameterized family log likelihood simpler distribution 
maximum entropy distribution thought simple case poe expert fixed shape parameter determine sharp expert distribution maximum entropy modelling features black box manner evaluated fixed unknown 
result greedy search needed find small set features gives reasonably performance terms likelihood 
may optimal way finding features 
features involved come differentiable parameterized family gradient ascent likelihood potentially efficient method finding useful features 
approach taken 
dividing xjy jy hidden variables expert conditionally independent experts 
hand see hidden variables marginally dependent interact visible variables result inference poe straightforward expert simple generating sample troublesome chapter 
background gibbs sampling convergence speeding method simulated annealing 
situation completely reversed causal model single layer independent hidden units layer visible units layer sigmoid belief network factor analysis 
causal model hidden units marginally independent conditionally dependent observations visible layer 
causal models easy generate sample distribution hard perform inference 
inference useful common practice ability generate samples desirable 
just maximum entropy models maximizing log likelihood suitable training poe fixed number experts computational cost evaluating sampling partition function 
hinton proposed optimizing function coined contrastive divergence cd learning efficient 
starting empirical distribution consider gibbs sampling sample equilibrium distribution yj 
gibbs sampling proceeds follows fixing visible units sample sample distribution obtained gibbs iterations starting jy yjx dy dx 
contrastive divergence defined cd kl kq kl kq note cd nonnegative exactly zero model fit data perfectly 
cd reasonable function minimize 
differentiating respect cd eq log yj eq log yj dy cd terms easily estimated samples third term troublesome 
hinton showed experimentally third chapter 
background term safely ignored leaving terms estimate gradient respect problem cd learning small having gibbs sampling mix properly allows models trained cd learning reconstructing data modelling data terms high likelihood 
facilitate mixing various methods example weight decay adding regularizers encourage reconstruction different data 
shown rbm trained cd learning competitive sense high log likelihood rbm trained standard boltzmann machine bm learning rule achieved computational resources bm learning better cd learning practical purposes 
possibility optimizing parameters iterated conditional modes icm besag 
index visible variables fy nj visible variables define pseudo likelihood pl log jy nj pseudo likelihood approximation log likelihood try reconstruct visible units 
discrete random variable pseudolikelihood easily optimized partition function jy nj exactly computed 
icm simply gradient ascent pl derivative respect pl jy nj log jy nj log jy nj define pl nj jy nj number visible variables 
log jy nj log yj log nj show reduces pl log yj pl log yj icm reduces cd learning particular gibbs sampling procedure iteration chosen randomly sampled conditional distribution personal communication osindero 
chapter 
background jy nj 
advantage icm opposed normal cd learning icm exactly maximizing pseudo likelihood guaranteed converge 
icm computationally intensive cd learning 
example case rbm iteration icm requires operations iteration normal cd learning requires ij operations number experts visible variables respectively 
practice find cd learning converges need expensive icm 
discussion section covered major techniques inference learning graphical models particular emphasis approximate methods 
rest thesis describe contributions techniques 
chapter rate coded restricted boltzmann machines face recognition chapter explore simple extension restricted boltzmann machines allows model discretized continuous valued random variables 
achieved replicating single unit multiple times number active replicas represent value random variable 
called rate coded restricted boltzmann machines rbmrate 
show rbmrate successfully applied tasks facial modelling recognition 
fact rbmrate model discretized bounded data unsatisfactory leaves room improvement 
problems rbmrate serve lessons learned motivation energy models chapter 
products experts continuous valued domains successful products experts restricted boltzmann machines products hidden markov models defined essentially discrete domains hinton brown hinton sallans hinton 
hinton described poe expert model mixture axis aligned chapter 
rate coded restricted boltzmann machines restricted boltzmann machine 
gaussian uniform distribution shown product models works simple examples 
problems studied thesis example modelling images faces natural scenes 
extensions mixtures factor analyzer uniform distribution mixtures factor analyzers unsuccessful 
chapter describe new classes poe problems interested 
rate coded restricted boltzmann machines rbmrate described chapter chapter energy models ebms introduced 
restricted boltzmann machines restricted boltzmann machine rbm boltzmann machine layer visible units single layer hidden units hidden hidden visible visible connections smolensky hinton sejnowski 
see 
indices visible hidden units respectively 
distribution fv fh ij ij weight connecting normalizing constant 
absorbed biases visible units weights having activation technically improper distribution due uniform distribution 
ways correct additional broad gaussian expert poe proper define uniform distribution bounded support 
support broad assume completely uniform region space interested 
chapter 
rate coded restricted boltzmann machines hidden units say fixed 
similarly hidden unit biases 
pulling sum exponential ij see rbm poe hidden unit corresponding expert 
poe observations posterior factorized distribution jv exp ij inference rbm easier general boltzmann machine causal belief network hidden layer 
conversely hidden units rbm marginally dependent rbm may easily learn population codes units highly correlated 
hard causal belief networks hidden layer generative model causal belief net assumes marginal independence 
rbm trained standard boltzmann machine learning algorithm follows noisy unbiased estimate gradient data log likelihood hinton sejnowski 
just requires samples prior distribution 
mcmc sampling prior take long time approach equilibrium sampling noise resulting estimate obscure gradient 
result gradient ascent log likelihood data effective learning algorithm 
resort contrastive divergence train 
learning rule 
ij hjv completed data distribution distribution running step mcmc sampling starting visible units conditionally independent hidden activities vice versa gibbs sampling alternate updating hidden units parallel updating visible units parallel 
illustrates chapter 
rate coded restricted boltzmann machines data time reconstruction fantasy alternating gibbs sampling terms learning rules poe 
process 
step mcmc sampling performed noise estimate significantly lower try sample prior steps 
reduction noise comes expense introducing bias learned weights 
rate coded restricted boltzmann machines continuous valued data binary units rbm far ideal 
describe simple way increase representational power changing inference learning procedures rbm 
idea replicate units rbm multiple times number active replicas unit represent discretized continuous value 
imagine visible unit replicas identical weights hidden units 
far hidden units concerned difference particular replicas turned number active replicas matters 
group replicas represent different discretized values 
sampling visible units hidden activities replicas share computation probability turning select replicas probability simply binomial distribution trials probability 
trick hidden units replicate times 
summary shown inference sampling rbm easily accommodate replicas 
easily show learning unaffected replace corresponding number active replicas 
chapter 
rate coded restricted boltzmann machines precise suppose continuous values deal bounded 
model values proportion active replicas group 
particular define aggregate random variable number active replicas corresponding similar rescale weights ij ij keep distribution 
distribution aggregate variables induced rbm model ij seen binomial extension boltzmann machines non rbm extension similar 
dealing model original binary rbm 
refer unit mean aggregate variable associated corresponding group replicas 
similarly ij ij unit rbm represents single neuron brain replica trick seen way simulating neuron time interval may produce multiple spikes constitute rate code 
reason call model rate coded restricted boltzmann machines rbmrate 
rbmrate simple way model discretized continuous values 
render discretization finer increasing number replicas group 
unfortunately means conditional distribution aggregate variable say neighbours tends delta function centred mean variance aggregate variable say 
trade finer discretizations complex modelling capacity 
show sections successfully applied tasks facial modelling recognition 
chapter describe generalization naturally accommodate continuous values discretization bounding continuous values certain range trade discretization resolution model complexity 
chapter 
rate coded restricted boltzmann machines rbmrate facial modelling test concept applied rbmrate modelling face images 
trained rbmrate images faces feret database phillips 
images include head parts shoulder background 
working images contain irrelevant information worked normalized face images 
normalization graphically depicted involves stages original image 
locate centres eyes hand 
rotate translate image eyes located preset locations 
crop image subsample pixels 
mask background face fixed mask leaving pixels oval shape 
equalize intensity histogram oval follows compile histogram pixel intensities cropped images separately image ordered pixels intensities altered intensities intensity histogram matches histogram 
shows examples processed face images 
masking background inevitably looses contour face contains useful information 
histogram equalization step removes lighting effects removes relevant information skin tone 
ideal case information modelled needed discriminative tasks identity expression recognition 
unfortunately face modelling researchers able extract information accurately time removing unwanted pose lighting information 
developments image segmentation example malik 
potential accurately 
chapter 
rate coded restricted boltzmann machines normalizing face images database 
examples processed faces 
rbmrate model applied hidden units replicas visible units replicas 
model trained contrastive divergence parallel gibbs sweeps iterations mini batches size 
approximations replaced expectation product expectations expected value computing probability activation hidden units 
continued stochastically chosen discretized firing rates hidden units computing step reconstructions data hidden activities transmit unbounded amount information data reconstruction 
empirically replacing expected value degrade performance computation efficient 
replacing expected value decreases noise estimate contrastive divergence learning rule 
learned weights shown 
units encode global features chapter 
rate coded restricted boltzmann machines weights learned rbmrate 
image shows weights adjacent hidden unit called receptive field 
probably image normalization ensures strong long range correlations pixel intensities 
compared face images model reconstructions 
original face image mean activities hidden units inferred 
reconstruct image hidden activities model 
reconstructions generally 
notice glasses reconstructed occur dataset 
inspired lee seung tried enforce local features restricting weights non negative 
achieved resetting negative weights zero weight update 
shows hidden receptive fields learned 
features left features local code features eye brows cheeks 
features left global clearly capture effects face lighting direction changed 
chapter 
rate coded restricted boltzmann machines reconstructions faces model 
cell left image original right reconstruction gibbs sampling iteration 
weights learned rbmrate restricted non negative 
chapter 
rate coded restricted boltzmann machines inferred hidden activities model understood representation original face image 
representation computed efficiently image hidden units poe conditionally independent 
representation efficiently summarizes original image pixels hidden unit activities loss information seen 
facial representation processing example recognizing expression identity individual image 
apply rbmrate model identity recognition section 
unfortunately representation faces section fruitfully applied face identity recognition take account fact certain variations faces important face recognition variations 
suitable rbmrate model pairs images 
rbmrate face recognition encouraged results previous section applied rbmrate task face recognition 
face recognition difficult task number individuals typically large test training images individual differ expression pose lighting date taken 
addition important application security biometric verification face recognition allows evaluate different kinds algorithm learning recognize compare objects requires accurate representation fine discriminative features presence relatively large individual variations 
difficult exemplars individual 
time large number individuals typical face recognition applications facial models efficient algorithms infer activities facial features 
need efficiency rules powerful models disposal researcher result successful applications face recognition employ simple models principal components analysis independent components analysis facial features simply linear combinations pixel intensities 
chapter 
rate coded restricted boltzmann machines combination need flexible powerful facial features need efficient inference ideal face recognition 
explore application rbmrate models section 
feret database version feret database phillips contains frontal face images individuals taken period years 
remove information irrelevant face recognition image normalized previous section 
images database gallery training set training set face pair network consists pairs faces gallery belonging individual 
distinct pairs creating training set face pairs rbmrate 
evaluate strengths weaknesses rbmrate versus face recognition methods divided remaining images disjoint sets testing distinct condition 
test sets 
expression set contains images different individuals 
individuals image training set taken lighting conditions time different expression 
training set includes pairs images differ expression 

days test set contains images individuals 
individuals images session training set images taken session days earlier test set 
test set evaluates face recognition methods changes lighting conditions slight variations appearance hair styles ups beard growth 
individuals gallery set images individuals identities known 
test image compared images gallery individual similar similarity measure identified person test image 
similarity measure typically computed model example eigenfaces turk pentland model principal components analysis trained training set 
images gallery training set differ disjoint 
test sets disjoint training set gallery 
chapter 
rate coded restricted boltzmann machines photographed similar fashion images training set 

months test set just 
days test set time sessions months 
test set evaluates major changes set different rooms cameras lighting conditions appearance 
interesting reflects conditions real life applications face recognition 
set contains images individuals 
images individuals included training set 

glasses test set contains images different individuals 
individuals images training set taken session day 
training test pairs individual differ pair glasses 
test set evaluates major change appearance glasses controlling changes lighting conditions set 
training set includes images half glasses half individuals 
popular face recognition methods compared rbmrate popular face recognition methods simplest correlation returns similarity score angle images represented vectors pixel intensities 
correlation performed better euclidean distance score 
second method eigenfaces turk pentland projects images principal component subspaces returns similarity score angle projected images 
principal components determined training set 
principal components removed manually determined coding features invariant face recognition 
tends improve recognition performance 
case omitted principal component encodes changes lighting conditions components chapter 
rate coded restricted boltzmann machines 
omitting principal component improved recognition performance test sets 
expression reasonable 
expression images taken lighting conditions know want information face recognition 
third method fisherfaces 
method eigenfaces projecting images subspace principal components maximizing variance projected images fisherfaces projects images subspace maximizes individual variances minimizes individual variances training set 
intuition want clusters corresponding different classes individuals separated possible improve discrimination 
subspace dimension projection 
final method shall call ppca proposed moghaddam moghaddam 
method models differences images individual probabilistic principal components analysis ppca moghaddam pentland tipping bishop differences images different individuals ppca 
difference images returns similarity score likelihood ratio difference image ppca models 
best performing algorithm september feret test phillips 
dimensional class class models respectively 
numbers moghaddam 
gives best results simulations 
face pair rbmrate model simple way rbmrate face recognition train single rbmrate model faces section identify face finding gallery image produces hidden activity vector similar produced face 
eigenfaces chapter 
rate coded restricted boltzmann machines ij ij face face rbmrate model pairs faces 
turk pentland recognition take account fact variations faces important recognition types variations 
correct trained rbmrate model pairs different images individual model pairs decide gallery image best paired test image 
model similar section 
hidden units replicas visible units replicas 
model trained pairs different face images belonging individual 
weights connecting hidden unit pixel second images ij ij respectively see 
find necessary weight sharing possible principle 
note obvious way share weights ij ij desirable means hidden units describe variations faces single individual see 
weight sharing imposed pairs hidden units ij ij ij ij preserve symmetry pair images individual training chapter 
rate coded restricted boltzmann machines set reversed pair set 
trained model image pairs distinct pairs iterations batches learning rate weights learning rate biases momentum parameters chosen rbmrate learn quickly diverging learning diverge simply step sizes large 
face pair define goodness fit model negative free energy model hjv ij log hjv note general 
quantities approximately equal training set symmetric respect switching faces pair 
account fact certain face images intrinsically easier model introduce balanced similarity measure determine identity individual take test image find gallery image maximizes 
presence face images pixels efficiency face recognition methods important concern 
face pair network shares advantage methods eigenfaces fisherfaces comparisons test gallery images low dimensional feature space high dimensional space pixel intensities 
show comparison 
image pair ij total input hidden unit image mean exp 
learning rates larger reported teh hinton proportion active replicas number replicas 
chapter 
rate coded restricted boltzmann machines goodness fit model algebraic manipulation ij log log log exp goodness fit similarity images computed efficiently total inputs hidden units precompute store total inputs gallery images test image compute compute similarity test image gallery images efficiently 
comparative results shows error rates methods test sets 
results averaged random partitions dataset improve statistical significance 
correlation eigenfaces perform poorly 
expression probably attempt ignore individual variations methods 
models poorly 
months test set unfortunate test set real applications 
rbmrate performed best 
expression fisherfaces best 
days 
glasses eigenfaces best 
months 
results show rbmrate competitive perform better methods 
demonstrate difficulty 
months test set produced 

months test set intrinsically difficult harder loss contour skin tone information 
receptive fields learned rbmrate shows weights hidden units training rbmrate 
units encode global features image normalization ensures strong long range correlations pixel intensities easily captured rbmrate 
maximum size weights weights having magnitudes smaller 
note hidden unit activations range 
chapter 
rate coded restricted boltzmann machines error rates corr eigen fisher rbmrate corr eigen fisher rbmrate error rates corr eigen fisher rbmrate error rates corr eigen fisher rbmrate error rates methods test sets 
bars group correspond left right rank rank rank rank rank error rates 
rank error rate percentage test images similar gallery images incorrect 
left test image 
months right similar images returned rbmrate 
human observers find correct match test image 
shows rbmrate discovered features fairly constant images class features differ substantially class 
example top left feature may encode presence faces feature may code prominent right eyebrows faces feature right may encode chapter 
rate coded restricted boltzmann machines example features learned rbmrate 
pair receptive fields constitutes feature 
fact mouth shape may change images individual 
rbmrate learn local features constraining weights nonnegative 
shows hidden receptive fields learned 
features left features local code features mouth shape changes third column eyes cheeks fourth column 
features left global clearly capture fact direction lighting differ images person 
constraining weights non negative strongly limits representational power rbmrate perform worse methods test sets 
discussion introduced extension called rbmrate models discretized continuous values 
unit rbmrate models understood representing number spikes neuron time interval 
applied rbmrate models face modelling recognition showed pro chapter 
rate coded restricted boltzmann machines example features learned non negative weight constraints 
duce efficient facial models comparable popular methods face recognition 
face recognition methods linear models plenty room development example prior knowledge constrain weights adding additional layers hidden units model correlations hidden activities hinton 
improvements translate improvements rate recognition 
remarks jonathon phillips graciously providing feret database 
earlier version chapter appeared neural information processing systems conference held denver colorado december teh hinton 
gatsby charitable foundation funding project 
chapter energy models sparse overcomplete representations chapter introduce natural extension continuous valued domains 
energy models ebms expert simply potentially non linear function data contributes energy term 
probability distribution data space defined boltzmann distribution corresponding sum energies 
assume energies boltzmann distribution properly defined exponentiated negative energies normalizable 
expert simply non linearly transformed output linear filter data ebms turn novel interesting generalization square noiseless independent components analysis ica 
ica popular linear non gaussian model blind source separation extraction features sounds images 
focus chapter describe simpler case relationship ica 
particular show energy approach ica equivalent established approaches ica square noiseless case gives novel interesting extensions ica complete case 
section describe views ica causal generative view pearlmutter parra mackay cardoso information maximization view bell sejnowski new energy view :10.1.1.56.3619
sections chapter 
energy models describe square noiseless ica complete extensions causal generative filtering perspectives detail 
section describe energy view ica relationship views 
describe contrastive divergence learning energy ica section experiments sections close discussion section 
dominant ways understanding ica bottom filtering approach top causal generative approach 
information maximization view bell sejnowski aim maximize mutual information observations non linearly transformed outputs set linear filters 
causal generative view pearlmutter parra mackay cardoso hand aim build density model independent non gaussian sources linearly combined produce observations :10.1.1.56.3619
main point chapter show third energy view ica combines bottom filtering approach goal fitting probability density observations 
parameters energy model specify deterministic mapping observation vector feature vector feature vector determines global energy 
probability density defined normalization factor integral numerator possible observation vectors 
energy view interesting suggests novel tractable ways extending ica complete multi layer models 
relationship perspectives depicted 
general quite different equivalent square noiseless case discussing energy models term feature source reasons clear discuss extensions complete case 
chapter 
energy models linear components analysis density causal generative models filtering approach modelling approach information maximization energy models different methods non gaussian linear components analysis 
number sources features equals number observations observation noise 
complete representations applied successfully wide range problems researchers argued complete representations sources features observations 
apart greater model flexibility reported advantages range improved robustness presence noise simoncelli compact easily interpretable codes mallat zhang superresolution chen 
natural way extend causal generative view complete case retain assumption sources independent model generate data accept consequence observation vector creates posterior distribution multiplicity source vectors 
posterior distribution sources conditionally dependent due effect known explaining away general distribution unfortunate property computationally intractable 
natural way extend information maximization view complete representations retain simple deterministic feedforward filtering observations chapter 
energy models conditional distribution source vectors observation observation source vectors unconditional distribution independent assumption independent assumption dependent rejecting away independent deterministic dependent explaining away deterministic independent energy causal square models complete complete independence properties types models 
mutual information objective function 
manifold possible filter outputs typically consist space square case equivalence causal generative models breaks 
energy view ica complete continues proper density model retains computationally convenient property features deterministic function observation vector 
abandons marginal independence features call sources 
useful way understanding difference energy density models causal generative density models compare independence properties 
summarizes similarities differences 
table reminds different views equivalent square case absence observations sources marginally independent 
posterior distribution source vectors conditioned observation vector collapses point absence noise sources trivially independent posterior distribution 
causal generative approach conditional independence sources seen fortuitous consequence sources observations avoiding noise observations retained complete case 
energy view conditional independence features treated basic assumption remains true complete case 
consider energy contributed activity feature chapter 
energy models energy model negative log probability dimensional non gaussian distribution 
combinations feature activities occur observation space maps restricted manifold feature space 
marginal dependence features complete energy model understood considering illuminating infinitely inefficient way generating unbiased samples energy density model 
sample features independently prior distributions negative exponentials individual energy contributions reject cases feature activities correspond valid observation vectors 
features complete feature values accepted observation vector consistent feature values assuming non degeneracy 
process rejecting away creates dependencies activities different features 
confusion marginal conditional independencies various models avoided term independent component analysis 
linear component analysis linear relationship sources features observations 
applications unmixing sound sources causal generative view clearly appropriate energy view strong prior belief sources marginally independent 
applications real aim model probability density data discover interpretable structure data extract representation useful controlling action raw data 
applications priori reason preferring causal generative view energy view characterizes observation vector representing degree satisfies set learned features 
square ica section briefly review standard models ica 
expositions ica comon entropy linearly transformed input vectors contrast chapter 
energy models function find statistically independent directions input space 
ica algorithms ultimately reduce optimizing sort contrast function overview try mention 
focus reviewing general approaches ica causal generative approach pearlmutter parra mackay cardoso information maximization approach bell sejnowski :10.1.1.56.3619
subsequent sections compare canonical approaches proposed energy approach particular explore consequences making different models complete 
consider real valued input denoted dimensionality dimensional source feature vector denoted section consider special case number input dimensions equal source features causal generative approach causal generative approach sources assumed independent distribution factors inputs simply linear combinations sources 
assume noise inputs square invertible matrix called mixing matrix 
inverting relationship inverse mixing matrix called filter matrix row acts linear filter inputs 
aim recover statistically independent source signals linearly mixed observations turns possible statistical properties chapter 
energy models sources non gaussian shall assume probability distribution sources modelled non gaussian prior distributions 
relation sources inputs deterministic may view change coordinates 
deriving expression probability distribution inputs accomplished transforming expression space jacobian transformation det det columns learning proceeds averaging log likelihood model data distribution derivatives respect gradient ascent log ij log ij ij th entry ij ij th entry matrix information maximization approach alternative neurally plausible approach ica put forward bell sejnowski assumed certain transformation applied inputs 
monotone squashing function sigmoid set linear filters 
argued maximizing mutual information outputs inputs equivalent maximizing entropy due deterministic relation technically sources gaussian 
data distribution underlying distribution observed data sampled 
practice replace empirical distribution training set 
fact information maximization approach ica proposed followed causal generative approach 
approaches reverse order intuitive exposition 
note mutual information measured respect data distribution chapter 
energy models lead independent components 
effect understood decomposition entropy individual entropies mutual information 
maximizing joint entropy involves maximizing individual entropies minimizing mutual information making independent 
approach best described filtering approach just squashed version filter outputs contrast causal generative approach think generated top manner 
equivalence approaches square representations information maximization approach turns equivalent causal generative interpret 
cumulative distribution function 
pearlmutter parra mackay cardoso :10.1.1.56.3619
seen observing entropy written negative kl divergence change variables follows dy log dx log det det jacobian transformation ij basic algebra shown det fact exactly equal satisfies 

cumulative function maximizing entropy equivalent maximizing log likelihood model 
note sources distributed 
mixed transformation maps input variables independent uniformly distributed variables range 
interval case sigmoid 
geometric interpretation help section 
chapter 
energy models square ica input noise previous section shown causal generative approach special case square mixing matrix noise equivalent information maximization approach 
equivalence break consider noise model inputs 
case longer deterministic relationship inputs sources 
straightforward write probabilistic model joint distribution sources inputs xjs unfortunately isotropic gaussian noise longer true mixing matrix optimal reconstruction sources simply 
typically computes maximum posteriori map value mean depending objective posterior distribution sjx 
overcomplete generalizations ica equivalence approaches breaks sources input dimensions consider complete representations data 
review complete generalizations ica approaches 
causal generative approach arguably natural way extend ica framework complete representations causal generative approach 
corresponding directed graphical model depicted 
noiseless inputs finding probable state corresponding particular input translates optimization problem map argmax log problem typically hard solved efficiently certain choices 
instance lewicki sejnowski argued choosing priors chapter 
energy models inputs sources inputs features inputs auxiliary vars features directed graphical model corresponding causal generative approach ica 
undirected graphical model ebm 
directed graphical model representation ebm auxiliary variables clamped 
laplacian problem mapped standard linear program 
soften optimization problem introducing noise model inputs 
instance spherical gaussian noise model noise variance find joint probability density distribution sources inputs xjs leads maximization problem reconstruct sources inputs map argmax jx log maximum likelihood learning noisy model em procedure involves averaging posterior distribution sjx 
unfortunately inference problem intractable general approximations needed 
literature find range approximate inference techniques applied problem 
olshausen field posterior approximated delta function map value 
iteration learning data vector maximization needs performed lewicki sejnowski argued approximation significantly improved gaussian distribution map value constructed matching fact situation slightly better variational point view 
show improve bound log likelihood jointly maximizing note extra condition mixing matrix needed prevent collapsing 
chapter 
energy models second derivatives locally laplace approximation 
attias girolami variational approach replaces true posterior tractable approximation adapted better approximate posterior 
mcmc sampling methods gibbs sampling may employed solve inference problem approximately olshausen 
notably different variation generative theme bayesian approach taken hyvarinen 
prior distribution possible mixing matrices introduced favors orthogonal basis vectors columns 
argue role jacobian det det aj precisely encourage orthogonality basis vectors reasonable assumption remove jacobian favor extra prior 
resultant expression easily extended complete representations 
want stress causal generative models lead difficult inference problems 
contrast generating unbiased samples distribution relatively straightforward sample source values independently priors subsequently sample input variables conditional gaussian 
information maximization approach section information maximization approach ica discussed simple case number inputs equal number sources noise assumed inputs 
natural question objective generalized complete representations 
possibility advocated 
define parametrized non linear mapping inputs outputs maximize mutual information amounts maximizing entropy outputs 
note approach best classified filtering approach inputs mapped subset possible outputs image mapping forms lower dimensional manifold output assumed columns unit lengths 
chapter 
energy models space space wx mapping information maximization approach 
space see 

showed objective translates maximizing expression entropy dx log det jacobian defined data distribution 
energy models interpreting ica filtering model inputs describe different way generalizing ica complete representations 
energy models ebm preserve computationally attractive property features simple deterministic functions inputs stochastic latent variables causal generative model 
consequence complete setting posterior collapses point stands sharp contrast complete causal models define posterior distribution sources 
fact complete ebms feature values allowed values lie image mapping similar information maximization approach different causal generative approach source values allowed 
mapping feature parameters features chapter 
energy models assigning energy possible observation vector follows probability defined terms energy boltzmann distribution denotes normalization constant partition function dx standard ica non gaussian priors implemented having number sources input dimensions log furthermore special case standard ica normalization term tractable simplifies det columns filters energy model suggests thinking ica filtering model observations linearly filtered causal generative model independent sources linearly mixed 
hinton teh interpreted filters linear constraints energies serving costs violating constraints 
energies corresponding heavy tailed distributions sharp peak zero means constraints frequently approximately satisfied strongly penalized grossly violated 
new approach natural include constraints input dimensions 
note marginal independence sources modelling note additive form energy leads product form probability distribution called product experts poe model hinton 
chapter 
energy models assumption complete causal models longer true features ebms general 
posterior reduces point features inputs trivially independent semantics probabilistic models consistent undirected graphical models depicted 
means inference ebms trivial 
hand sampling distribution difficult involves mcmc general 
precisely opposite causal generative models inference hard sampling easy 
relating ebms causal generative ica discuss proposed complete ebms relate causal generative approach ica 
previous section argued number input dimensions matches number features ebm strictly equivalent standard ica described section 
assume features input dimensions 
consider ica model added auxiliary input dimensions denote total input space 
add additional filters new variables features denote total filter matrix jf 
assume new filters chosen invertible new enlarged space fully spanned 
enlarged ica model write probability distribution det gj columns write probability density conditional distribution xjz dx det gj terms cancelled 
choose observed values auxiliary chapter 
energy models variables written xjz dx course just ebm partition function dx note derivation independent precise choice filters long span extra dimensions 
previous section seen ebm may interpreted undirected graphical model conditional independence features inputs 
discussion may conclude interpret ebm conditional distribution xjz directed graphical model auxiliary variables clamped 
see 
clamping extra nodes introduce dependencies features phenomenon explaining away 
words features constrained requirement creates dependencies 
relating ebms information maximization section saw information maximization approach complete representations maximizes entropy 
fact quantity det equation general normalized complete case prevents expression negative kl divergence 
define probability density det normalization constant minimizing kl divergence kl jjp equivalent maximizing log likelihood model 
importantly consistent definition ebm choose energy log det tr log chapter 
energy models energy density model simple interpretation terms mapping 
mapping depicted shown coordinates define parametrization manifold 
hard show distribution transformed precisely uniform distribution manifold space normalization constant may interpreted volume manifold 
minimizing kl divergence kl jjp interpreted mapping data manifold high dimensional embedding space data distributed uniformly possible 
relation information maximization approach summarized expression kl jjp log manifold volume term describes fit model data second term simply entropy uniform distribution manifold 
relative energy approach maximizing mutual information stronger preference increase volume manifold directly related entropy 
note square case manifold exactly image space volume fixed reduces exactly kl divergence kl kp 
complete case experiments decide approach preferable circumstances 
parameter estimation energy models section proposed energy models probabilistic models complete representations 
discuss fit free parameters models filters efficiently data 
section address issue 
describe usual maximum likelihood method training models 
complete models show maximum likelihood practical solution non trivial partition function 
light propose estimation method energy models called contrastive divergence hinton 
biased method chapter 
energy models show bias acceptably small compared gain efficiency training complete models ease generalize method new intricate models 
distribution observed data model distribution reason notation apparent section 
approximate possible 
standard measure difference kullback leibler kl divergence kl kp log dx fixed minimizing kl divergence equivalent maximizing log likelihood data model energy models derivative kl divergence respect weight ij kl kp ij ij ij learning proceed derivative gradient descent kl divergence data distribution model distribution 
ij kl kp ij update rule understood lowering energy surface locations data term time raising energy surface locations data model predicts high probability second term 
eventually result energy surface low energy high probability regions data high energy low probability 
second term rhs obtained derivative log partition function respect ij square ica case log partition function exactly log det second term evaluates ij th entry matrix model complete analytic form partition function exact computation generally intractable 
second term expectation model distribution possibility markov chain monte carlo chapter 
energy models mcmc techniques approximate average samples see neal 
method inherits advantages drawbacks associated mcmc sampling 
estimate obtained consistent bias decreases zero length chains increased easily adaptable complex models 
main drawback method expensive markov chain run steps approaches equilibrium distribution hard estimate steps required 
variance mcmc estimator usually high 
reduce variance independent samples needed incurring additional computational costs 
estimating derivative accurately mcmc sampling slow unreliable due high variance 
argue unnecessary estimate derivatives averaged equilibrium distribution order train energy model data 
average derivatives different distribution resulting truncating markov chain fixed number steps 
idea called contrastive divergence learning proposed hinton improve computational efficiency reduce variance expense introducing bias estimates parameters respect maximum likelihood solution 
ideas involved contrastive divergence learning 
start markov chain data distribution initialize markov chain vague distribution gaussian large variances 
reason usually vague initial distributions mode equilibrium distribution chance visited chain 
help overcome problematic feature markov chains low mixing rate chain enters mode distribution hard escape different mode 
argue starting data distribution preferable training data contains examples various modes model distribution ought 
learning modes model distribution roughly correspond modes data distribution number samples mode approximately matches number data vectors mode 
reduces vari chapter 
energy models ance derivative estimates 
main danger technique certain spurious modes devoid actual data accidentally created learning may go unnoticed 
second idea contrastive divergence run markov chain iterations equilibrium 
chains started data distribution iterations consistent tendency move away data distribution provides valuable information adapt parameters model 
intuitively parameters model updated markov chain tend move away data distribution want markov chain data distribution 
combining ideas described defining distribution random variable th iteration markov chain contrastive divergence learning algorithm implemented quantity update filters ij 
ij ij ij relative maximum likelihood learning replaced equilibrium distribution markov chain initialized data distribution algorithm gives pseudo code contrastive divergence learning ebms 
notice order compute average second term samples produced markov chains initialized corresponding data vectors term 
uniformly sampling initial states markov chains data vectors reduces variance 
addition filter weights ij additional parameters instance model shape energies update rules similar fit data 
standard ica correspond learning shape prior densities 
model distribution flexible perfectly model data distribution notation initial distribution markov chain limit distribution 
case finite data replace data distribution empirical distribution mixture chapter 
energy models algorithm contrastive divergence learning energy models 
run convergence criterion met run time limit exceeded 
compute gradient total energy respect parameters average data cases 
run mcmc samplers steps starting data vector keeping sample chain 

compute gradient total energy respect parameters average samples 
update parameters 
ij data ij samples ij learning rate number samples mini batch 
markov chain mixes properly contrastive divergence learning fixed point maximum likelihood solution hard see maximum likelihood solution markov chain change model distribution implies derivatives precisely cancel 
general expect contrastive divergence learning trade bias variance see williams 
apart may happen certain markov chains spurious fixed points exist contrastive divergence learning examples see mackay 
hybrid monte carlo sampling section described detail cd learning markov chains 
argued unnecessary markov chain approach equilibrium important choose markov chain mixes fast reduce delta functions 
case smooth model distribution able perfectly fit empirical data distribution argument fails 
fact may expect incur certain bias respect maximum likelihood solution 
chapter 
energy models bias cd learning 
subsection describe hybrid monte carlo hmc flexible class markov chains 
hmc mixes sufficiently fast experiments ebms reported chapter 
see section neal depth description hmc related techniques theory 
hmc thought type metropolis hastings sampling 
metropolis hastings sampling proposal distribution sample candidate previous state rejection rule accept reject state rejected state 
keep acceptance rate reasonably high candidate typically involves small change previous state result metropolis hastings sampling exhibits random walk behaviour 
idea hmc move candidate away keeping acceptance rate reasonably high deterministic dynamical system 
reduces random walk behaviour improves mixing rate markov chain 
ebm energy function hmc requires ability evaluate ability calculate gradient easily calculated ebm 
introduce momentum variable equal dimensionality define hamiltonian kkk hamiltonian understood total energy system kkk kinetic energy potential energy 
distribution kkk means gaussian distributed zero mean unit covariance 
hamiltonian dynamics dynamical equations dx dk time parameter 
intuitively dynamics describe movements ball unit mass frictionless surface 
ball position momentum position chapter 
energy models ball located height giving potential 
shown distribution invariant evolving dynamics 
algorithm hybrid monte carlo sampling 
iteration start previous sample 
sample initial momentum vector zero mean unit covariance gaussian 

simulate hamiltonian dynamics time leapfrog steps size 
number leapfrog steps step size user defined parameters 

accept new sample probability min hmc markov chain obtain sample 
marginalizing see sample sample 
algorithm describes iteration hmc 
intuition iteration hmc previous state discard sample new momentum vector simply gibbs sampling factors leaves invariant 
hamiltonian dynamics applied fixed amount time time start resulting taken state markov chain 
leaves invariant operation leaves invariant 
course simulate hamiltonian dynamics leapfrog discretization 
leapfrog discretization designed chapter 
energy models preserve phase space volume measurable set values position momentum mapped set measure 
crucial preserve invariance discretized dynamics 
leapfrog discretization introduces errors corrected final rejection step 
notice third step leapfrog step combined step leapfrog step error discretization affected step size scales turn affects acceptance rate 
find optimal step sizes quite sensitive particular problem phase learning start learning model distribution quite vague large step size learning small step size needed 
step size parameter acceptance rate parameter adjust step size actual acceptance rate vicinity desired rate 
adjustment done cd step involve multiple hmc steps avoid affecting equilibrium property markov chain 
simple adjustment rule desired rate empirical rate user defined parameter 
experiment blind source separation collaboration max welling assessed performance contrastive divergence learning algorithm 
compared hmc implementation contrastive divergence exact sampling algorithm bell sejnowski algorithm standard blind source separation problem 
model number input source chapter 
energy models dimensions energy model defined log model strictly equivalent noiseless ica model sigmoidal outputs bell sejnowski 
data consisted sixteen second stereo cd recordings music sampled khz recording sampled factor randomly permuted time index rescaled unit variance 
resulting samples channels linearly mixed standard routine diagonal diagonal whitened presentation various learning algorithms 
whitening process simple linear transformation called zca covariance matrix identity observed improve speed convergence 
hmc implementation contrastive divergence uses outer loop step hmc simulation sample turn consists leapfrog steps step sizes adapted simulation acceptance rate 
algorithm hmc 
noiseless ica possible sample efficiently true equilibrium distribution causal generative view 

fair number samples equal number data vectors mini batch 
compute partition function evaluate second term exactly 
precisely bell sejnowski algorithm implemented exact 
parameter updates performed mini batches data vectors 
learning rate annealed iterations learning momentum factor speed convergence 
initial weights sampled gaussian standard deviation 
learning monitored amari distance true unmixing matrix 
figures note recovering sound sources input dimensions sensors possible energy model marginally independent 
prepared barak pearlmutter available cs unm edu bap demos html 
available sound media mit edu ica bench 
iterations 
amari distance amari measures distance matrices chapter 
energy models amari distance iterations learning curves hmc exact evolution amari distance various algorithms blind source separation problem averaged runs 
note hmc converged just fast exact sampling algorithm exact algorithm exact slightly faster 
sudden changes amari distance due annealing schedule 
show results various algorithms sound separation task 
figures show deterministic method exact performs slightly better sampling methods hmc probably due variance induced sampling 
importantly shows learning brief sampling hmc performs learning samples equilibrium distribution 
main experiment need sample equilibrium distribution order learn filters validates ideas contrastive divergence learning 
tions scalings ab ij maxk ab ik ab ij maxk ab kj chapter 
energy models hmc exact comparison final amari distances amari distance algorithms final amari distances various algorithms blind source separation problem averaged runs 
boxes lines lower quartile median upper quartile values 
whiskers show extent rest data 
outliers denoted 
experiments feature extraction examples features delivered algorithm standard datasets 
firstly demonstrate performance typical ica tasks determining complete set features speech natural images 
show algorithm applied cedar cdrom dataset hand written digits lastly feature vectors learned algorithm applied feret database human faces 
experiments described section energy function form log 
corresponds modelling data product dimensional student distributions degrees freedom hinton teh 
energy function chapter 
energy models chosen simplicity versatility describing super gaussian distributions 
algorithmic formulation allows arbitrary energy functions results may improved systematic tailoring energy function particular datasets 
speech max welling performed experiment test model extract meaningful filters speech data 
recordings male speakers timit database uttering sentence don ask carry rag 
sentences sampled khz ms segments segment corresponding samples extracted random locations 
presentation learning algorithm data centred mean removed whitened 
features trained contrastive divergence step hmc sampling consisting leapfrog steps 
mini batches size learning rate annealed iterations 
filters initialized small random values momentum speed parameter learning 
show features whitened domain power spectra 
recall times filters extracted dimensions input space energy model longer equivalent causal ica model 
shows distribution power time frequency 
interesting structure khz filters localized finely tuned frequency average 
phenomenon reported abdallah plumbley 
natural image patches tested algorithm standard ica task determining independent components natural images 
data set data set van hateren van der available ftp phys rug nl pub samples 
chapter 
energy models filters complete ebm 
filters row ones largest power indicating represent important features 
filters second row randomly drawn remaining filters 
corresponding 
schaaf 
logarithm pixel intensities taken image patches centred whitened 
patches patch size 
trained network features contrastive divergence step hmc sampling consisting leapfrog steps 
step size adaptive acceptance rate approximately 
unconstrained small weight decay encourage features localize 
initialized random vectors length initialized 
trained learning rate momentum factor 
result sensitive settings parameters 
random sample learned features whitened domain shown 
roughly ordered increasing spatial frequency 
hand counted total features localized spatial frequency domain 
features described gabor functions 
analyze set learned filters fitted gabor function form lewicki olshausen feature extracted parameters frequency chapter 
energy models time ms frequency khz distribution power time frequency learned speech features 
envelope filter absolute value hilbert transform computed squared 
squared envelope power spectrum thresholded mapping values greater half peak value rest zero 
gaps smaller samples time samples frequency filled 
outer product templates computed weighted total power filter added diagram 
location extent spatial frequency domains 
summarized figures show filters form nice tiling spatial frequency domains 
see figures filters learned multiple scales larger features typically lower frequency 
see emphasis horizontal vertical filters 
effect observed previous papers van hateren van der schaaf lewicki olshausen probably due 
chapter 
energy models learned filters natural images 
cedar digits collaboration simon osindero applied ebms images hand written digits 
real valued digits br set cedar cdrom 
digits available divided equally classes 
mean image entire dataset subtracted datum digits whitened 
network features trained manner natural image patches 
chapter 
energy models spatial layout size filters learned natural image patches described position size bars 
random subset learned filters shown 
easier discern structure learned filters whitened domain pixel space 
note superficial similarity filters natural scene experiments 
addition straight edge filters see curved filters 
interpret results set stroke detectors modelling space strokes chapter 
energy models frequency polar plot frequency tuning orientation selectivity filters learned natural image patches centre cross peak frequency orientation response describing bandwidth 
gives rise full digit set 
feret faces collaboration simon osindero applied ebms database frontal face images feret face recognition evaluation phillips 
data pre processed standard manner aligning faces normalizing pixel intensities cropping central oval shaped region additional pre processing step centred data retained projections leading principal components input dimensions algorithm 
projections normalized variance 
trained ebm features contrastive divergence hmc code pre processing part project evaluating face recognition algorithms ross beveridge bruce draper 
available www cs edu index htm 
chapter 
energy models learned filters cedar digits 
filters plotted whitened space clarity 
sampling sets leapfrog steps 
unconstrained 
initialized values gaussian distribution vector norm weight vector rescaled 
initialized 
learning rate whilst learning rate shows leading eigenvectors plotted face images shows subset filters learned model 
learned filters somewhat chapter 
energy models eigenfaces largest eigenvalue plotted rowwise descending eigenvalue order 
subset feature vectors learned ebm ordinary face data 
top row hand picked bottom row randomly selected 
subset feature vectors learned architecture randomly selected 
global pixels non zero weight 
addition global features localized features focussing glasses eyes smiling furthermore global features described archetypical faces similar learned rbmrate see global features appear mainly capture structure illumination face 
features learned sparse natural images hand written digits 
reasons twofold 
face images normalized pixel values highly correlated 
second model imposes energy terms sparsity constraint outputs filters actual pressure filter weights sparse 
impose sparsity weights force chapter 
energy models non negative 
bartlett 
proposed interesting solution referred architecture 
idea invert roles filter outputs weights ebm impose sparsity constraint weights 
describe technique context causal generative square ica shown equivalent ebm equal numbers filters inputs 
matrix columns correspond training images 
square ica understood decomposition au columns basis vectors columns sources generated images columns 
sparse prior imposed entries equivalent ebms gives features 
transpose decomposition place sparse prior entries decomposition interpreted ica columns basis vectors sources 
performing ica columns matrix context important number columns greater number rows note satisfy condition 
number rows greater columns span subspace space 
case perform ica spanned subspace projecting columns subspace 
related issue learning ica efficient whiten columns unit covariance 
typical way handle issues pca bartlett 
desired number principal components 
take number columns rows 
columns principal components columns diagonal matrix associated eigenvalues diagonal 
construct projected matrix columns whitened chapter 
energy models represents original columns projected space 
ica applied matrix framework terms causal generative square ica 
making relationship square ica ebms extend framework easily overcomplete case ebms 
particular apply complete ebms columns illustrates results take 
features shown outputs filters learned ebm rows 
approach leads features highly localized space model imposes sparse prior outputs filters 
results qualitatively similar described bartlett 

discussion chapter re interpreted standard ica algorithm energy model studied extension complete representations 
shown parameters ebm filter weights energy function efficiently estimated contrastive divergence learning 
number experiments standard data sets shown ebms efficiently extract useful features high dimensions 
believe ebms provides flexible modelling tool trained efficiently uncover useful structure continuous valued data 
contrary causal generative models complete ica features ebm exhibit marginal dependencies 
advantage allowing dependencies model fast inference 
causal generative models assumption marginal independence leads inference needs approximated iterative data dependent scheme 
role iterations understood suppressing activity relevant features producing sparse code 
causal generative models complete representations expected produce compact sparse codes fact emphasized desirable olshausen field 
surprisingly chapter 
energy models output layer output layer input variables architecture hierarchical non linear energy model 
non linearities indicated sigmoidal units output layer 
energies contributed output variables layers number output variables need correspond number input variables 
shown slow iterative process fact needed produce sparse complete representations 
suggest enriching ebms inhibitory lateral connections achieve goal suppressing relevant features order produce sparser representation 
preliminary experiments mean field approach implement lateral successful learning density models slow due iterative optimization data case 
powerful generalization ebms hierarchical non linear architecture output activities computed feed forward neural network layer may contribute total energy 
welling 
layer model studied second layer performed local averaging non linearly transformed activities layer 
resulted topographic ordering filters orientation location frequency changing smoothly filter 
builds topographic ica hyvarinen hoyer interesting relationship layer ebm approximation train topographic ica model hyvarinen hoyer 
chapter 
energy models fit multi layer ebms data backpropagation compute gradients energy respect data vector hmc sampling weights weight updates 
algorithm applies backpropagation unsupervised setting combines contrastive divergence learning named contrastive backpropagation hinton 
contrastive backpropagation learning procedure quite flexible 
puts constraints smoothness activation functions energy functions procedure easily modified recurrent neural networks contain directed cycles running forward pass predetermined number steps defining energy smooth function time history activations 
backpropagation time rumelhart werbos obtain required derivatives 
data vector change forward pass recurrent network 
possible model sequential data video sequences running network forward time sequence running backward time compute derivatives required hmc sampling updating weights 
energy approach ica stems previous 
fact ebm type poe defines density continuous valued domains 
discretization involved ebms natural models rbmrate model chapter 
explore various extensions ebms mentioned apply ebms face recognition compare rbmrate standard face recognition algorithms 
remarks earlier version chapter written collaboration max welling simon osindero geoffrey hinton submitted journal machine learning research special issue independent component analysis teh 
necessary assumption energy defined boltzmann distribution normalizable 
chapter 
energy models reported experiments performed solely partially collaborators kindly letting report 
particular max welling performed experiments feature extraction speech version blind source separation experiment appeared hinton 

simon osindero run feature extraction experiments digits faces obtained qualitative results figures reported experiments 
yair weiss peter dayan pointing possible relationship ebms ica 
ontario government gatsby charitable foundation funding 
chapter bethe free energy inference variational techniques mcmc sampling loopy belief propagation emerged years powerful alternative approximate inference 
fixed points loopy belief propagation correspond exactly stationary points approximate free energy called bethe free energy yedidia 
unfortunately loopy belief propagation converge 
chapter takes obvious step proposes alternative loopy belief propagation converge stationary point bethe free energy 
call algorithm unified propagation scaling interesting relationship loopy belief propagation classical iterative scaling algorithm 
show number toy experiments applicability unified propagation scaling 
belief propagation bp standard algorithm perform exact inference tree structured graphical models pearl lauritzen spiegelhalter 
iterative algorithm messages encoding evidence passed neighbouring nodes 
applied graphs cycles loopy bp algorithm guaranteed converge computed marginal distributions necessarily exact converges pearl 
chapter 
bethe free energy inference understood double counting evidence messages passed node back cycle weiss 
empirically loopy bp converge produced estimates marginal distributions surprisingly accurate murphy :10.1.1.32.5538
loopy bp discovered coding community decoding algorithms turbo codes gallager low density parity check codes loopy bp particular graphical models frey mackay mackay neal 
advance possible codes approach shannon information theoretic limit mceliece motivated interest analyzing understanding behaviour accuracy loopy bp algorithm context error correction decoding 
initial analysis loopy bp terms unwrapping networks revealed interesting results weiss 
example networks single loop loopy bp converge maximum posterior assignment correct estimated marginals overly confident 
jordan line loopy bp related gibbs measures infinite trees yielded strong result conditions loopy bp converge 
analysis technique interprets loopy bp tree reparameterization distribution represented graphical model wainwright 
line analysis led useful bounds errors obtained loopy bp converges 
theoretical foundation loopy bp discovery fixed points algorithm correspond exactly stationary points bethe free energy yedidia 
bethe free energy understood approximation exact gibbs free energy georges yedidia welling teh 
respect strong relations loopy bp variational mean field methods jordan saad opper 
line research led generalized bp algorithms fixed points stationary points sophisticated kikuchi free energy yedidia 
empirically fixed points loopy bp local minima coding literature loopy bp called sum product algorithm generalized distributive law 
chapter 
bethe free energy inference bethe free energy leading conjectures loopy bp converge local minima bethe free energy yedidia 
conjecture proven heskes 
showed local minima need stable fixed point loopy bp 
loopy bp converges local minima bethe free energy bethe free energy reasonable approximation true gibbs free energy obvious step derive algorithm minimizes bethe free energy converges 
aim chapter 
solve generalization inference minimizing approximate kl divergence 
algorithm derive closely related loopy bp iterative scaling 
unified propagation scaling ups algorithm reduce convergent alternative loopy bp generalized inference reduces ordinary inference 
section describe markov networks chapter section describe inference review background loopy bp bethe free energy 
section introduce generalization inference iterative scaling algorithm 
section propose bethe free energy approximation kl divergence derive fixed point equations perform approximate generalized inference 
show sense fixed point equations related loopy bp section describe various algorithms minimize bethe free energy including unified propagation scaling ups algorithm 
section shows experimental simulations various algorithms section concludes 
markov networks consider discrete pairwise markov network node set edge set denote neighbours ij denote edges graph 
variable associated node denote domain values value node set fx xn similarly chapter 
bethe free energy inference single pairwise potentials ij 
distribution represented ij ij normalization constant partition function 
slightly different parameterization 
ij ij number neighbours node ij ij drop cumbersome notation 
rest chapter dealing inference pairwise markov networks 
generalized graphical models reductions weiss freeman 
ordinary inference set visible nodes set hidden nodes 
random variables observe infer approximate posterior distribution xh observations ordinary inference observe visible nodes take particular value effect observation absorbed potentials new ik posterior distribution xh visible nodes neighbouring edges removed graph potentials altered 
reason discuss ordinary inference explicitly refer visible hidden nodes graph 
take ordinary inference determining marginal distributions interest distribution 
section chapter 
bethe free energy inference ji ij belief propagation tree structured markov networks 
ij message sent node node ji message 
messages represented solid dotted arrows affect 
generalize notion inference minimum divergence ideas 
case explicitly refer visible hidden nodes 
belief propagation belief propagation bp inference procedure tree structured markov networks 
iterative algorithm messages passed neighbouring nodes 
see 
message ij node node encodes contribution subtree rooted nodes left affects distribution messages updated update rule ij ij nj ki messages ki encode contribution rest subtree distribution messages converged marginal distributions computed ki ij nj ki ni lj simple important property message updates messages going direction tree independent 
illustrated chapter 
bethe free energy inference solid dashed arrows represent messages going direction 
consequence property scheduling message updates converge tree 
waiting incoming messages ki updated updating ij obtain particularly efficient scheduling message needs updated algorithm converges 
particular instantiation schedule known routines 
bethe free energy belief propagation applied graphs cycles loopy bp yedidia 
showed fixed points correspond exactly stationary points bethe free energy 
bethe free energy approximation gibbs free energy arose statistical physics georges yedidia welling teh 
beliefs ij estimates pair wise single site marginal distributions desired distribution beliefs need satisfy marginalization normalization mn constraints ij fb ij ij fb bethe free energy defined bethe ij ij log ij ij log impose mn constraints lagrange multipliers ji lagrangian bethe ji ij neighbours setting derivatives lagrangian respect beliefs lagrange multipliers zero identifying ji nj log ki derive equations beliefs replacing ij fixed point updates messages 
chapter 
bethe free energy inference generalized inference section generalize notions posterior distributions inference minimum kl divergence ideas show methods approximate inference adapted scenario 
fixed distribution observed marginal distributions define generalized posterior distribution minimizes kl divergence kl qkp log log subject constraints call constraints observational obs constraints 
generalized inference process determine generalized posterior 
avoid confusion explicitly ordinary inference normal inference confusion inference mean generalized inference chapter similarly posteriors 

fx subset nodes similarly subgraph observation generalized posterior 
proof satisfy constraints form xh 
kl qkp xh xh log xh log xh xh log xh log log xh xh log xh log log bx minimizing gives xh 
chapter 
bethe free energy inference observation shows constrained marginals delta functions observations hard generalized posterior reduces trivial extension ordinary posterior explaining term generalized inference 
note observed distributions nodes soft evidence 
soft evidence observational process jx associated observe observation act bias affecting marginal distribution fixed 
iterative scaling generalized inference constrained minimum divergence problem standard way solving lagrange multipliers 
lagrange multiplier enforcing 
lagrange multiplier enforcing fact sum 
lagrangian kl qkp solving setting derivatives zero see generalized posterior log optimal simply introduces partition function normalize 
plugging back lagrangian get dual cost function bo log kl qkp convex dual cost concave maximum coincides minimum kl qkp subject obs constraints generalized posterior terms optimal 
maximizing dual cost coordinate wise ascent gives chapter 
bethe free energy inference classical iterative scaling algorithm deming stephan 
iteration lagrange multiplier updated scaling update intuitively updates current posterior marginal node matches constraint 
undo marginal constraints nodes scaling update applied iteratively nodes convergence 
specific case generalized iterative scaling gis algorithm darroch ratcliff updates lagrange multipliers subset nodes ju show gis maximizes lower bound dual cost step converges maximum see berger 
intuitive understand gis parallel gis steps performing updates parallel damping steps algorithm guaranteed converge 
approximate generalized inference notice generalized posterior form potentials altered lagrange multipliers 
ordinary inference needed compute current marginals required scaling update 
singly connected belief propagation bp compute required marginals 
exact inference sampling algorithms monte carlo markov chain mcmc usually computationally 
alternative approximate inference algorithms variational methods loopy bp estimate required marginals 
efficient produce biased estimates potentially leading converging 
example consider node boltzmann machine weight biases desired means nodes 
naive mean field chapter 
bethe free energy inference naive tap equations estimate marginals required converge 
converge theoretical understanding accuracy algorithm 
principled approach approximate kl divergence derive algorithms minimize approximation subject obs constraints 
example variational approximations jordan upper bound kl divergence variational means assuming comes tractable parametric family minimizing upper bound respect variational parameters obs constraints 
upper bound chosen minimization tractable 
note approach generally variational approximations outer loop 
section describe bethe free energy approximation kl divergence 
fixed point equations minimizing bethe approximation derived 
fixed point equations reduce bp updates hidden nodes updates observed nodes 
consequence loopy bp approximate marginals required outer loop turns particular scheduling fixed point equations 
bethe free energy fairly understood quite accurate regimes murphy yedidia welling teh conclude loopy bp viable approximate generalized inference technique :10.1.1.32.5538
section describe better algorithms approximate generalized inference bethe free energy 
bethe approximation bethe approximation assume factored follows ij ij beliefs satisfy mn constraints 
note need normalized need beliefs marginals 
exact tree structured 
plugging approximation kl divergence regrouping terms chapter 
bethe free energy inference get kl qkp bethe log partition function bethe approximation approximation kl divergence accounts pair wise correlations neighbouring variables 
variational approximations bethe approximation upper bound general 
log fixed wish minimize bethe subject mn obs constraints 
lagrange multipliers ji impose marginalization constraints 
lagrange multipliers impose normalization observational constraints reduces simply keeping ij normalized keeping fixed shall ignore clarity 
resulting lagrangian bethe ji ij setting derivatives respect ij ji zero get theorem subject mn obs constraints stationary point bethe ij ij ji ij ji lagrange multipliers fixed points updates ji nj ik ik ji ij ij chapter 
bethe free energy inference relationship bp fixed point updates closely related bp identifying messages ij ij ji get bp updates ij ij nj ki shown yedidia 
different identification ji nj ki 
see identification equivalent 
rewriting terms messages find ij ij ji extend physical analogy understand message bouncing step messages going observed node get bounced back altered process 
hard observation reduces ij ij bx bouncing back messages going node get absorbed 
alternatively understand message bouncing steps updates 
theorem updating ji equivalent updating identify ji ji theorem states unexpected result scaling updates just fixed point equations minimize bethe required marginals computed exactly loopy bp 
loopy bp approximate marginals required just particular scheduling fixed point equations 
algorithms minimize bethe free energy fixed point equations determine conditions beliefs stationary point bethe free energy bethe determine chapter 
bethe free energy inference algorithm find stationary point bethe free energy find local minimum 
section describe various algorithms try find stationary point local minimum bethe section describe algorithms directly fixed point updates 
algorithms guaranteed converge guaranteed converge local minima converge 
sections describe ups algorithm guaranteed converge local minimum saddle point bethe free energy 
direct fixed point algorithms simplest algorithm inspired yedidia 
run fixed point updates hope converge minimum bethe fixed point updates run series parallel 
algorithm loopy 
theorem states loopy converges converge stationary point bethe simulations find gets local minimum global minimum 
loopy necessarily converge especially variables strongly coupled 
reasons fail converge 
firstly loopy bp component may fail converge 
serious past result indicate loopy bp fails bethe approximation accurate welling teh 
secondly component may fail converge run sequentially estimated marginals inaccurate 
show section serious problem loopy way promote convergence damp loopy updates 
works practice damping critical success loopy loopy bp murphy :10.1.1.32.5538
common ways damp propagation updates linear geometric 
algorithm loopy loopy iterative scaling 
run convergence criterion met run time limit exceeded 
perform fixed point updates 

run time limit exceeded return failed converge 
chapter 
bethe free energy inference algorithm bp iterative scaling loopy belief propagation 

run convergence criterion met run time limit exceeded 
run convergence criterion met run time limit exceeded 
perform bp fixed point updates 

run time limit exceeded return failed converge 

perform scaling update 

run time limit exceeded return failed converge 
linear damping messages updated ij ij ij nj ki damping factor geometric damping ij ij ij nj ki damping preserves fixed points algorithm making converge 
damping scaling updates fact gis updates understood geometric damping 
way promote convergence mitigate second problem running scaling updates series approximate required marginals inner phase loopy bp 
bp algorithm illustrated algorithm 
theorem shows bp just particular scheduling loopy bp gets rid convergence problems related running scaling updates parallel marginals converged inherits accuracy loopy converging 
run loopy bp convergence scaling update bp particularly efficient 
subsections describe possibility efficient algorithm fixed point equations guaranteed converge damping 
chapter 
bethe free energy inference constraining leaves trees section derive ups efficient algorithm minimizing bethe free energy tree observed nodes leaves tree 
section ups subroutine algorithm minimizes bethe free energy general graphs 
suppose tree observed nodes leaves tree bethe free energy exact mn constraints satisfied bethe kl qkp ij ij particular bethe convex subspace defined mn constraints 
fixed point equations run sequentially converge unique global minimum 
question schedule fixed point updates efficiency 
exactly bp update exactly update common scheduling alternately run phase bp tree convergence perform single scaling update 
schedule essentially implements bp procedure bp exact tree 
notice bp phase need propagate messages node scaling update going occur compute marginals required scaling update propagate messages away node previous scaling update occurred maintain marginalization consistency beliefs 
loop algorithm requires scaling update jej propagation updates 
quite clear efficient scheduling 
illustrate example 
consider tree 
firstly suppose just run bp tree performed scaling update node perform scaling update node 
need calculate marginal node 
run phase bp tree get lagrange multipliers node ones altered bp phase messages relevant calculating altered lying path node 
general running full bp phase tree need perform bp updates node previous scaling update node scaling update 
chapter 
bethe free energy inference scheduling propagation updates ups 
observed nodes grey hidden nodes white 
propagation updates required scaling updates nodes path node node arrows 
secondly improve bp scheduling scaling updates appropriately 
consider 
suppose scheduled perform scaling updates repeatedly nodes order 
inefficient schedule run bp update nodes twice loop scaling updates 
better schedule update nodes order 
case run relevant bp update exactly loop 
general subtree consist edges lay path nodes consists exactly edges propagation updates performed scaling updates 
note 
observed node 
perform updates depth search manner starting example illustrated 
node visited perform scaling update node edge traversed going tree backtracking corresponding propagation update performed 
depth search nodes visited 
scaling updates propagation updates performed node second 
depth search implements efficient propagation schedule describing 
note relevant propagation update performed exactly implements efficient scheduling scaling updates 
resulting algorithm ups summarized algorithm 
loop ups steps requires jv scaling updates je propagation updates 
steps chapter 
bethe free energy inference scheduling scaling updates ups 
updates performed depth manner tree 
numbers describe order nodes visited 
black arrows forward traversals white arrows backtracking traversals 
required beliefs consistent satisfy mn constraints nodes visited steps 
requires je propagation updates 
graphs cycles graphs cycles bethe exact convex 
fact exact trees find local minimum saddle point 
idea clamp number hidden nodes current marginals rest hidden nodes singly connected apply ups 
ups converged clamp different set hidden nodes apply ups 
algorithm understood coordinate descent minimize bethe respect nodes iteration 
set clamped nodes loop graph contains node define graph obtained follows 
node replicate times connect replica neighbour nodes 
shown graph 
clearly singly connected 
define ij original nodes chapter 
bethe free energy inference algorithm ups unified propagation scaling trees 

run propagation updates convergence 

root depth searches 
run convergence criterion met 
sequence nodes visited depth search starting note je 

perform scaling update 
perform propagation update 

run propagation updates convergence 
similarly 
denote trees define number neighbours node regrouping terms bethe show theorem bc distribution subspace defined bc mn obs constraints bethe kl kp bc log bc minimize bethe minimize kl kp individually 
solve ups 
clamping marginals nodes reduced problem solved ups observed nodes taken include algorithm algorithm 
clear bethe bethe fact scaling propagation updates fixed point equations finding stationary points bethe chapter 
bethe free energy inference clamping hidden nodes graph singly connected 
hidden nodes white observed nodes grey clamped nodes black 
little nodes replicas 
algorithm ups unified propagation scaling 
initialize beliefs fb ij 
convergence criterion met 
find set nodes loop broken 
ups set bethe mn obs constraints satisfied theorem converge local minimum saddle point bethe mn obs constraints satisfied 
notice obs constraints delta functions generalized inference reduces ordinary inference ups reduces convergent alternative loopy bp 
experiments section report experiments feasibility various algorithms 
experiment compared speed convergence various algorithms 
second experiment compared accuracy fastest algorithms ups loopy chapter 
bethe free energy inference experiments boltzmann machines square lattice structure shown 
observed nodes edges lattice rest hidden nodes 
states valued 
weights sampled randomly gaussian mean standard deviation biases sampled gaussian standard deviation mean incoming weights 
means biases shifted small mean values approximately 
fact show symmetry mean values exactly 
desired observational marginals sampled gaussian mean standard deviation different values distribution observed marginals quite different 
speed convergence compared speed convergence algorithms loopy bp gis bp parallel gis marginals estimated loopy bp ups clamping columns nodes iteration ups hv alternatingly clamping rows columns 
tested algorithms networks 
find result sensitive settings long algorithms able converge damping 
result shown 
bp gis bp slow loopy bp phase expensive 
ups ups hv better bp gis bp inner loops cheaper scaling updates lagrange multipliers run frequently 
see ups hv faster ups information propagated faster network 
loopy fastest 
experiment shows converges frequently trade speed loopy stability ups 
chapter 
bethe free energy inference gis bp bp ups ups hv loopy number updates speed convergence various algorithms 
box lines median upper lower quartiles whiskers describe extent data 
algorithm subroutine considered converged beliefs change accuracy estimated marginals compared accuracy estimated marginals obtained ups particular loopy possible types constraints shown 
case constraint marginals delta functions generalized inference reduces ordinary inference loopy reduces loopy bp ups convergent alternative loopy bp 
case enforce obs constraints problem estimating marginals prior 
general trend loopy bp ups comparable perform worse weights get larger biases get smaller evidence 
confirms results welling teh 
see loopy bp converge ups estimates better loopy bp estimates 
reason ups tends converge solutions marginal distributions close uniform loopy bp converge 
detailed description see welling teh 
cases set corresponding spread respectively 
cases ups loopy equally chapter 
bethe free energy inference ordinary inference obs constraints plot shows mean absolute errors various settings axis axis 
top plots show errors loopy bottom plots show errors ups 
inset shows cases black loopy converge iterations linear damping slowly increasing 
loopy bp converge errors calculated current beliefs iterations 
converged ups continued perform loopy converge 
loopy bp converged ups performed cases high damping conclude loopy failure converge due performing scaling updates accurate marginals available 
concluding see ups comparable loopy generalized inference reduces ordinary inference presence obs constraints better 
discussion chapter shown approximating kl divergence bethe free energy leads viable algorithms approximate generalized inference 
find interesting fruitful relationship loopy bp 
novel algorithm ups chapter 
bethe free energy inference convergent alternative loopy bp ordinary inference 
interesting extensions approximate generalized inference cluster nodes get accurate approximations kl divergence analogous kikuchi free energy handle marginal constraints subsets nodes 
explore algorithms minimize bethe including cccp algorithm yuille 
chapter explore ideas developed section particular approximating kl divergence bethe free energy close relationship loopy bp problem approximately learning undirected graphical models 
remarks earlier version chapter appeared fifteenth international conference neural information processing systems teh welling 
jonathan yedidia yair weiss alan yuille robert cowell geoffrey hinton zoubin ghahramani interesting discussions suggestions 
gatsby charitable foundation funding 
chapter bethe free energy learning previous chapter showed perform approximate inference direct minimization bethe free energy 
important concept cast loopy belief propagation bp iterative scaling updates fixed point equations single constrained optimization problem 
chapter extend results derived previous chapter learning undirected graphical models 
junction trees standard algorithm learning undirected graphical models 
combining bp updates single framework derive efficient message updating protocol known effective ipf 
junction tree intractably large maximum clique size propose maximize approximate constrained entropy region graphs yedidia 
unfortunately unclear generalize ups case propose loopy version ipf new objective 
show yields accurate estimates weights undirected graphical models simple experiment 
junction trees widely efficient representations probability models defined graphs 
instance perform exact inference bayesian networks typically transforms chapter 
bethe free energy learning directed graph junction tree computes posterior probability cliques junction tree local propagation rules 
known schemes purpose hugin propagation jensen shafer shenoy propagation shafer shenoy 
junction trees indispensable learning graphical models data iterative proportional fitting ipf procedure known iterative scaling 
effective ipf procedure represents joint probability distribution terms clique marginals junction tree alternates updating parameters model ipf propagating change rest model junction tree 
structured problems junction tree representation reduces space time complexity ipf procedure drastically 
result chapter decrease time complexity effective ipf procedure 
shown ipf junction tree propagation updates fixed point equations maximum entropy problem certain constraints 
unifying view lifts strict separation effective ipf procedure ipf junction tree propagation allows efficient ipf junction tree propagation updates 
graphs maximum clique size corresponding junction tree intractably large problem needs tackled approximations 
extent propose framework closely related exciting technique approximate inference variously known loopy belief propagation sum product algorithm generalized distributive law yedidia 
knowledge close relationship propagation ipf updates propose procedure performs approximate ipf region graphs natural extensions junction trees may contain cycles designed smaller clique sizes 
loopy procedure consists running fixed point equations solve stationary points constrained approximate entropy similar region graph free energies 
section describe maximum entropy problem focus chapter chapter 
bethe free energy learning classical iterative scaling algorithm 
show relationship maximum entropy maximum likelihood learning graphical models 
section describe effective ipf procedure 
section describes unifying view efficient schedule 
section deals approximate ipf region graphs section shows simple experiment efficacy approximation 
section closes discussion extensions 
maximum entropy section review maximum entropy framework relationship maximum likelihood learning undirected graphical models 
set nodes 
node associated variable denote finite domain values value set nodes variable associated nodes domain values simplicity write xn nn similarly family subsets clusters cluster joint distribution random variables family distributions consistent distribution satisfying marginals chapter assume consistent 
case maximum entropy extension argmax entropy log domain maximization probability simplex 
lagrange multipliers impose marginal constraints enforce extension feature expectations straight forward described section 
chapter 
bethe free energy learning normalization 
lagrangian zeroing derivatives respect log expresses primal variables terms dual variables 
solve substitute obtain dual cost bp log original cost function concave maximum coincides minimum dual cost function maximum entropy extension terms optimal 
dual cost function convex solved coordinate wise descent 
classical iterative scaling algorithm deming stephan updates log 
terms primal variables understand update setting marginal 
fact equivalent primal update maximum entropy framework intimately related maximum likelihood learning undirected graphical models della pietra :10.1.1.43.7345:10.1.1.43.7345
clusters graphical model distribution expressed graphical model form exp chapter 
bethe free energy learning parameters model normalizing partition function 
empirical distribution obtained set fully observed training data 
average log likelihood data log log easily seen negative maximum entropy dual cost function 
distribution equivalent form graphical model derived maximum entropy considerations marginal constraints 
note case distributions simply marginal distributions empirical distribution consistent 
junction trees straight forward implementation iterative scaling uses simple probability table represent 
jxj grows exponentially jn computational cost algorithm high iterative scaling update requires jxj time jxj space 
section review effective ipf procedure reduces computational costs drastically structured domains 
short effective ipf uses junction tree represent simple probability table 
step iterative scaling distribution structure undirected graphical model nodes clusters corresponding graph edge nodes cluster triangulating graph maximal cliques form junction tree separators separating 
construction cluster contained maximal clique decomposable respect junction tree representing straight probability table represent set smaller chapter 
bethe free energy learning ordering satisfying running intersection property distribute iterative scaling change rest graph 
tables fp cliques tables consistent neighbouring cliques separating consider primal iterative scaling update 
clique containing iterative scaling update performed changes distribution inconsistent tables 
maintain consistency propagate change rest junction tree standard phase illustrated 
ordering cliques satisfying running intersection property unique 
amounts 
essence replacing marginal new marginal information carried marginals flows outward original cluster cliques relatively small junction tree representation efficient straight probability table 
max jx jxj 
iterative scaling tables separators required computed marginalizing neighbouring clique 
cliques form tree local consistency equivalent global consistency encountered unfortunately true cliques form tree see section 
equivalently phase iterative scaling update compute required marginal bach jordan 
chapter 
bethe free energy learning update followed jsj propagation updates 
time storage requirements iterative scaling update 
unifying propagation scaling section introduced junction trees simply computational tool improve efficiency iterative scaling procedure 
show section propagation updates iterative scaling updates derived fixed point equations constrained maximization problem 
consequence intermixed schedule iterative scaling junction tree propagation updates converge maximum entropy solution 
allows flexibility designing efficient schedules updates 
particular propose new scheduling requires jsj propagation updates perform jaj iterative scaling updates 
efficient algorithm section 
constrained maximization consider constrained maximization problem argmax fpc psg nx domains probability simplexes 
constraints satisfied distributions cliques separators consistent combined single distribution 
cost function means specific case original maximum entropy problem assumed decomposable respect junction tree 
section shows maximum entropy extension decomposable respect junction tree 
chapter 
bethe free energy learning mc mc shafer shenoy propagation updates 
computing clique distributions messages 
computing separator distributions messages 
marginal distributions maximum entropy extension form solution problems equivalent 
lagrange multipliers solve 
cs impose marginal consistencies sure normalized 
identify clique impose constraint 
ajc cg 
lagrangian xv xs cs cns solving lagrangian find marginal distributions cs xs ac cs xs cs updated fixed point equations log xs cns cs cliques separated separators neighbouring see iterative scaling updates fixed point equations solve lagrangian 
chapter 
bethe free energy learning identifying messages potentials cs cs xs easily shown cns cs identified shafer shenoy propagation update junction trees shafer shenoy 
marginal distributions cs cs shafer shenoy updates depicted 
intuitive effective perform iterative scaling primal updates dual updates 
similarly propagation updates deal directly clique marginals desirable 
hugin propagation hugin propagation shown equivalent shafer shenoy identify messages marginal distributions 
efficient scheduling previous subsection shows iterative scaling shafer shenoy propagation updates fixed point equations solve maximum entropy problem 
cost function concave space constraints satisfied fixed point equations guaranteed converge global optimum 
imply efficiency various schedules 
propose particular class schedules efficient sense defined 
chapter 
bethe free energy learning understand iterative scaling update changing lagrange multiplier current estimate true marginal distribution satisfy constraint 
hand propagation updates compute required marginal distributions store current lagrange multipliers 
propagation updates run convergence iterative scaling update true 
schedule effective ipf procedure 
clear schedule inefficient calculates marginal distributions exactly needed iterative scaling update 
hand propagation updates converged iterative scaling update performed calculated marginal distribution exact 
result iterative scaling update effective 
view issues shall show proposed schedule unified propagation scaling junction trees ups jt efficient satisfies properties iterative scaling update performed current estimate required marginal exact iterative scaling updates propagation update performed ensure exact second iterative scaling update 
algorithm ups jt unified propagation scaling junction trees 
initialize junction tree uniform 

initialize messages lagrange multipliers uniform 

initialize clique 
convergence criterion met 
perform iterative scaling updates clusters identified 
choose clique neighbouring 
perform propagation update note ups jt prescribe ordering visit cliques implicit requirement cliques visited times 
possible ordering chapter 
bethe free energy learning ct ct ups jt mc dashed lines messages correct solid line denotes message updated correct 
visit cliques depth search manner junction tree 
guarantees lagrange multiplier updated total jsj propagation updates 
efficient ordering effective ipf procedure takes jsj propagation updates iterative scaling update 
ups algorithm chapter uses scheduling 
compared effective ipf procedure see ups jt performs multiple iterative scaling updates clique importantly substituted full phase single propagation update 
satisfies condition 
prove induction condition satisfied shafer shenoy propagation 
inductive hypothesis iteration incoming messages correct 
time trivially true 
depict step ups jt algorithm time 
updated lagrange multipliers clique required step ups jt algorithm choose neighboring clique tree perform scaling update 
required condition need exact marginal 
compute collect evidence clique propagating inward leaves 
note messages dashed arrows message solid arrow unchanged scaling update node recompute get correct marginal clique chapter 
bethe free energy learning loopy iterative scaling ups jt efficient algorithm entropy maximization cliques junction tree small 
complexity scales exponentially size maximal clique 
combat propose approximate algorithm named loopy iterative scaling region graphs 
region graphs yedidia 
define region graph acyclic directed graph vertices labelled subsets nodes regions 
top layer consists family large regions cover original graph cluster contained 
directed edge exist parent child nodes associated child subset nodes associated parent 
vertex region associate counting number super super consists regions strictly contain top layer regions 
valid region graph fulfill conditions node subgraph induced vertices containing node connected sum counting numbers subgraph add 
region graph associate approximate entropy fp generalized belief propagation algorithms maximize see yedidia 
details 
collection fp approximate marginals satisfying local consistency constraints child region region entropy note region graph contain cycles distribution consistent fp may globally consistent 
chapter 
bethe free energy learning tion problem argmax fp solutions approximations marginals true maximum entropy distribution 
optimal lagrange multipliers approximate solutions maximum likelihood parameters doing maximum likelihood training graphical model 
solving region entropy maximization problem hard show fixed point equations scaling updates lagrange multipliers fixed point equations generalized belief propagation 
lagrange multipliers associated constraints potentials top layer regions ar top layer region containing jr rg 
loopy iterative scaling updates performed convenient scheduling convergence unfortunately guaranteed 
unfortunately technique clamping units chapter derive convergent alternatives loopy iterative scaling extended 
reason chapter observational constraints clamping constraints consistent adding constraints clamping marginals subset nodes easily constraints inconsistent 
conceivable algorithms maximize lower bounds approximate entropy yuille 
region graphs conveniently translate loopy iterative scaling algorithms clear construct valid region graph family large regions 
distinct methods described literature junction graphs called cluster variation method 
junction graph method junction graph layer region graph large regions called cliques children called separators 
region graph simple structure typically ignore directionality edges 
node require subgraph constructed chapter 
bethe free energy learning example graphical model 
junction graph model 
notice separator bottom contains 
region graph constructed cluster variational method 
cliques separators containing node form undirected tree 
note condition stronger region graph condition automatically ensures counting numbers subgraph sum 
property equivalent running intersection property junction trees guarantees junction graphs look junction trees locally 
example junction graph shown 
cluster set variety junction graphs consistent spectrum junction trees perform exact entropy maximization 
junction graphs small cliques poor approximations admit efficient algorithms maximize approximate entropy 
fact aji mceliece show collection subsets nodes particular easy construct junction graph cliques consist precisely subsets collection define separators intersections pairs cliques node construct subgraph induced cliques separators containing node delete nodes separators remove separators empty subgraph tree 
junction graphs particularly convenient propagation updates reduce precisely junction tree updates 
approximate entropy reduces chapter 
bethe free energy learning bethe entropy fp cluster variation method alternative road constructing valid region graphs provided cluster variation method 
start family large regions cluster contained 
children large regions defined intersections children intersections intersections 
process repeated non trivial region added 
resultant layered region graph counting numbers assigned regions automatically valid corresponding approximate entropy known kikuchi entropy 
corresponding loopy iterative scaling algorithm generalized belief propagation algorithms described yedidia 

experiment explored behaviour loopy iterative scaling junction graphs simple task learning weights pairwise markov networks 
training data consists binary images hand written pre processed cedar dataset see 
generated models fit data 
generated assign reasonably high likelihood data time junction trees manageable sizes 
computed maximum likelihood tree 
obtained calculating mutual information training data pairs pixels 
maximum likelihood tree maximum weight spanning tree weights mutual informations 
maximum likelihood tree models generated randomly sampling edges connecting nodes distance apart layer include regions subsets regions layer 
chapter 
bethe free energy learning examples images 
images binarized thresholding 
maximum likelihood tree 
junction trees constructed triangulating resulting graphs node elimination stage node neighbors chosen 
maximal clique sizes junction trees vary 
second constructed junction graphs approximate entropy 
model starting edges cliques junction graph range junction graphs constructed growing cliques node time making sure resulting cliques contained cliques corresponding junction tree 
stage node added clique increase total mutual information largest removing cliques subsumed cliques 
junction graphs constructed loopy iterative scaling learn parameters scaling propagation updates damped geometrically 
encourage convergence propagation updates iterated convergence scaling updates performed 
convergence improving measures find loopy iterative scaling runs converge note loopy iterative scaling guaranteed converge 
loopy iterative scaling converged assessed accuracy results total mutual information defined sum mutual informations pairs nodes cliques 
chapter 
bethe free energy learning junction graph clique size log likelihood junction graph clique size distance average log likelihoods training data function junction graph clique size 
curve averaged models certain maximal clique size corresponding junction tree 
models treewidth plotted standard deviation log likelihoods 
circles denote log likelihood exact maximum likelihood parameters junction graph coincides junction tree 
average distances various models 
notations 
measures 
plot log likelihoods data learned models show distances empirical marginal distributions learned models averaged edges approximation reasonably accurate accuracy improves increase maximum clique size junction graph 
discussion chapter shown propagation iterative scaling junction trees unified fixed point equations solving certain constrained maximum entropy problem 
insight proposed efficient scheduling iterative scaling junction trees 
graphs maximal clique size prohibitively large proposed chapter 
bethe free energy learning loopy iterative scaling algorithm region graphs 
number important extensions methods discussed chapter 
firstly finding distribution maximum entropy find distribution minimum relative entropy kl divergence distribution relative entropy defined kl pkp log uniform distribution minimizing kl pkp equivalent maximizing entropy extension useful learn undirected graphical models certain potentials fixed 
potentials cast minimum relative entropy framework 
results unifying junction tree propagation iterative scaling carry decomposable respect junction tree 
secondly results straightforwardly extended constraints feature expectations marginals 
cluster associated vector valued feature constrained maximum entropy problem maximum likelihood problem undirected graphical model features 
lagrange multipliers imposing expectation constraints weights corresponding features undirected graphical model required expectations obtained averaging training set 
algorithms discussed chapter open way efficient training maximum entropy models della pietra conditional maximum entropy models lafferty thin junction trees bach jordan :10.1.1.43.7345:10.1.1.43.7345
chapter proposed novel algorithms learning fully observed undirected graphical models 
models partially observed standard method train em algorithm 
posterior distribution intractable number researchers looked approximating steps loopy belief propagation frey kannan 
chapter 
bethe free energy learning global cost function steps minimizing statements accuracy convergence properties algorithms 
exciting research direction extend framework partially observed case approximate em algorithm steps derived fixed point equations minimizing region free energy 
final approximate free energy methods proposed approximate inference 
methods applied learning partially observed directed graphical models 
example variational methods 
chapter shown methods applied learning undirected graphical models 
looked maximum likelihood learning solving maximum entropy problem applied approximations entropy 
insight opens possibilities approximate learning undirected graphical models described detail 
acknowledgments remarks earlier version chapter appeared ninth international workshop artificial intelligence statistics teh welling 
kevin murphy questioning theory chapter applied learning francis bach michael jordan interesting discussions 
geoffrey hinton inspiration support ontario government funding 
chapter discussion thesis described contributions modelling ebms performing approximate inference learning bethe free energies 
products experts energy models new class flexible density models unsupervised learning 
obtained combining simpler density models experts multiplying densities normalizing 
advantage combination data represented efficiently distributed fashion experts keeping inference tractable 
deal intractable partition function learning contrastive divergence learning employed 
initial applications continuous valued domains successful 
chapters described new classes effective modelling data 
extended model discretized continuous values 
done replicating units rbm group replicas model single discretized value 
showed rbmrate model successfully applied tasks face modelling recognition 
face recognition interesting model pairs faces belonging chapter 
discussion individual modelled single faces 
allows model features common face images individual shape nose facial changes affect identity individual mouth movements glasses 
identify individual test image pair test image gallery image find gallery image matched test image 
requires model inference done easily cheaply role perfect poe 
class models inference done easily cheaply linear gaussian models principal components analysis factor analysis 
interesting compare simple models proved quite successful face recognition model pairs faces 
note model complex distributions 
allows room improvements hopefully translate better face recognition performance 
interesting direction research determine experts poe contribute improving face recognition rates experts 
improve efficiency inference improve recognition rates experts captured useful features faces contributing noise similarity measure pruned 
pruning experts done cross validation class discriminability analysis bartlett 

rbmrate entirely satisfactory model continuous valued domains data bounded discretized 
ebms natural models data 
ebms generalizations expert need normalized distribution expert called energy term easily computed function data 
constraint integral data space sum energy terms infinite 
constraint needed ebm defines proper distribution data space 
typically easy impose restricting functional form experts letting negative log heavy tailed density functions 
chapter explored ebms single layer linear features chapter 
discussion ebms flexible models data 
example ebms defined feedforward recurrent neural networks welling 
defined dynamical data recurrent neural networks 
explore various extensions multiple layers recurrent connections dynamical systems 
obvious direction apply ebms face recognition 
bethe free energy approximations yedidia 
showed fixed points loopy bp exactly stationary points bethe free energy important steps understand bethe free energy understand relationship stable fixed points loopy bp bethe free energy 
initial aim derive algorithm directly minimize bethe free energy 
welling teh derived algorithm boltzmann machines gradient descent 
led better understand nature bethe free energy terms expansion 
showed naive mean field naive tap methods low order bethe free energy 
experiments verified mean field tap approximations accurate bethe free energy approximation 
chapter derive algorithm directly minimize bethe free energy 
algorithm works general case discrete pairwise markov networks 
algorithm ideas 
firstly uses fact bethe free energy convex network tree structured 
secondly interesting relationship loopy bp fixed point equations derived trying minimize bethe free energy subject constraints certain node marginals 
algorithm derived called ups algorithms directly minimizes bethe free energy 
algorithm cccp yuille 
contrast ups graph theoretic intuition bethe free energy convex chapter 
discussion graph tree cccp algebraic intuition bethe free energy consists convex concave terms 
ups cccp double loop algorithm 
iteration outer loop concave terms upper bounded linearly 
resulting bound bethe free energy convex fixed point equations derived guaranteed find global minimum 
inner loop consists running fixed point equations convergence 
convex concave decomposition powerful seen fact easily adapted general case kikuchi free energy 
exciting development heskes uses similar convex concave decomposition show stable fixed points loopy bp minima bethe free energy 
relationship loopy bp fact standard algorithm learn parameters undirected graphical models allows apply ideas developed chapter learning 
described chapter 
particular describe ups jt efficient version standard effective ipf procedure 
fact bethe free energy simplest hierarchy approximate free energies region graphs yedidia proposed range approximate learning algorithms free energies hierarchy 
showed get results simple experiment hand written digits junction graphs 
unfortunately bethe free energy generalizations region graphs general convex loopy guaranteed converge 
interesting develop convergent alternatives loopy interesting direction taken wainwright 
start form bethe free energy lower bounds log likelihood 
cost function convex derived fixed point equations called tree reweighted bp guaranteed find global optimum 
interesting compare accuracy tree reweighted bp versus loopy analyze accuracies wainwright 

interesting direction research derive region graph free energies give tighter bounds log likelihood 
bibliography abdallah plumbley 
edges independent components natural images independent components natural sounds 
international conference independent component analysis blind signal separation 
aji mceliece 
generalized distributive law free energy minimization 
proceedings allerton conference communication control computing volume 
amari cichocki yang 
new algorithm blind signal separation 
advances neural information processing systems volume pages 
amari 
methods information geometry volume translations mathematica monographs 
oxford university press 
attias 
independent factor analysis 
neural computation 
bach jordan 
thin junction trees 
advances neural information processing systems volume 
bartlett movellan sejnowski 
face recognition independent component analysis 
ieee transactions neural networks 
press 
beal ghahramani 
variational bayesian em algorithm incomplete data application scoring graphical model structures 
bayesian statistics volume 
bibliography bell sejnowski 
information maximisation approach blind separation blind deconvolution 
neural computation 
hespanha kriegman 
eigenfaces versus fisherfaces recognition class specific linear projection 
european conference computer vision 
berger 
improved iterative scaling algorithm gentle 
www cs cmu edu maxent html 
berger della pietra della pietra 
maximum entropy approach natural language processing 
computational linguistics 
besag 
discussion 
bulletin international statistical institute 
brown hinton 
products hidden markov models 
proceedings artificial intelligence statistics 
cardoso 
infomax maximum likelihood blind source separation 
ieee signal processing letters 
chen donoho saunders 
atomic decomposition basis pursuit 
siam journal scientific computing 
clifford 
markov random fields statistics 
welsh editors disorder physical systems 
oxford science publications 
comon 
independent component analysis new concept 
signal processing 
cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
cowell 
inference bayesian networks 
jordan editor learning graphical models 
kluwer academic publishers 
bibliography darroch ratcliff 
generalized iterative scaling log linear models 
annals mathematical statistics 
dayan hinton neal zemel 
helmholtz machines 
neural computation 
della pietra della pietra lafferty 
inducing features random fields 
ieee transactions pattern analysis machine intelligence 
deming stephan 
square adjustment sampled frequency table expected marginal totals known 
annals mathematical statistics 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
freeman pasztor 
learning estimate scenes images 
advances neural information processing systems volume 
frey kannan 
accumulator networks local probability propagation 
advances neural information processing systems volume 
frey mackay 
revolution belief propagation graphs cycles 
neural information processing systems volume 
frey jaakkola moran 
sequentially fitting inclusive trees inference noisy networks 
advances neural information processing systems volume 
mit press 
friedman 
bayesian structural em algorithm 
proceedings conference uncertainty artificial intelligence volume 
georges yedidia 
expand mean field theory high temperature expansions 
journal physics 
bibliography geyer 
markov chain monte carlo maximum likelihood 
computing science statistics proceedings symposium interface 
ghahramani beal 
variational inference bayesian mixtures factor analysers 
advances neural information processing systems volume 
mit press 
ghahramani beal 
propagation algorithms variational bayesian learning 
advances neural information processing systems volume 
mit press 
gilks richardson spiegelhalter 
markov chain monte carlo practice 
chapman hall 
girolami 
variational method learning overcomplete representations 
neural computation 
gupta editor 
differential geometry statistical inference volume lecture notes monograph series 
institute mathematical statistics 
heckerman 
tutorial learning bayesian networks 
jordan editor learning graphical models 
mit press 
heskes 
stable fixed points loopy belief propagation minima bethe free energy 
advances neural information processing systems volume 
hinton 
training products experts minimizing contrastive divergence 
neural computation 
hinton sallans ghahramani 
hierarchical community experts 
jordan editor learning graphical models 
kluwer academic publishers 
hinton sejnowski 
learning relearning boltzmann machines 
rumelhart mcclelland editors parallel distributed processing explorations microstructure cognition 
volume foundations 
mit press 
bibliography hinton teh 
discovering multiple constraints frequently approximately satisfied 
proceedings seventh conference uncertainty artificial intelligence pages 
hinton teh welling osindero 
contrastive backpropagation 
preparation 
hinton welling teh osindero 
new view ica 
proceedings international conference independent component analysis blind signal separation volume 
hyvarinen hoyer 
layer sparse coding model learns simple complex cell receptive fields topography natural images 
vision research 
hyvarinen 
estimating overcomplete independent component bases image windows 
journal mathematical imaging vision 
press 
jensen 
bayesian networks 
ucl press london 

effective implementation iterative proportional fitting procedure 
computational statistics data analysis 
jordan editor 
learning graphical models 
kluwer academic 
jordan ghahramani jaakkola saul 
variational methods graphical models 
jordan editor learning graphical models 
kluwer academic publishers 
kappen rodriguez 
mean field theory graphical models 
saad opper editors advanced mean field methods 
mit press 
kappen rodrguez 
efficient learning boltzmann machines linear response theory 
neural computation 
bibliography kirkpatrick gelatt vecchi 
optimization simulated annealing 
science 
lafferty mccallum pereira 
conditional random fields probabilistic models segmenting labeling sequence data 
international conference machine learning volume 
lafferty mccallum pereira 
conditional random fields probabilistic models segmenting labeling sequence data 
proc 
th international conf 
machine learning pages 
morgan kaufmann san francisco ca 
lam bacchus 
learning bayesian belief networks approach mdl principle 
computational intelligence 
lauritzen 
graphical models 
oxford university press 
lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
journal royal statistical society 
lee seung 
learning parts objects non negative matrix factorization 
nature october 
lee 
new monte carlo algorithm entropic sampling 
physical review letters 
lewicki olshausen 
probabilistic framework adaptation comparison image codes 
opt 
soc 
am 
optics image science vision 
lewicki sejnowski 
learning overcomplete representations 
neural computation 
bibliography mackay 
maximum likelihood covariant algorithms independent components analysis 
wol ra phy cam ac uk mackay abstracts ica html 
mackay 
failures step learning algorithm 
wol ra phy cam ac uk mackay abstracts gbm html 
mackay neal 
near shannon limit performance low density parity check codes 
electronics letters 
malik belongie leung shi 
contour texture analysis image segmentation 
international journal computer vision 
mallat zhang 
matching pursuits time frequency dictionaries 
ieee transactions signal processing 
parisi 
simulated tempering new monte carlo scheme 
letters 
hinton 
recognizing hand written digits hierarchical products experts 
advances neural information processing systems volume 
mit press 
mceliece mackay cheng 
turbo decoding instance pearl belief propagation algorithm 
ieee selected areas communication 
minka 
family algorithms approximate bayesian inference 
phd thesis massachusetts institute technology department electrical engineering computer science 
moghaddam pentland 
probabilistic visual learning object representation 
ieee transactions pattern analysis machine intelligence 
moghaddam pentland 
eigenfaces probabilistic matching face recognition 
ieee international conference automatic face gesture recognition 
bibliography morris 
recognition networks approximate inference bn networks 
phd thesis institute technology 
murphy weiss jordan 
loopy belief propagation approximate inference empirical study 
proceedings conference uncertainty artificial intelligence volume 
morgan kaufmann publishers 
neal 
probabilistic inference markov chain monte carlo methods 
technical report crg tr university toronto department computer science 
neal 
sampling multimodel distributions tempered transitions 
statistics computing 
neal 
annealed importance sampling 
statistics computing 
neal 
circularly coupled markov chain sampling 
technical report revised university toronto department statistics 
neal hinton 
view em algorithm justifies incremental sparse variants 
jordan editor learning graphical models 
kluwer academic publishers 
nigam lafferty mccallum 
maximum entropy text classification 
proceedings ijcai machine learning information filtering 
olshausen field 
emergence simple cell receptive field properties learning sparse code natural images 
nature 
olshausen field 
sparse coding overcomplete basis set strategy employed 
vision research 
olshausen 
learning sparse codes mixture gaussians prior 
advances neural information processing systems volume pages 
bibliography opper winther 
naive mean field theory tap equations 
saad opper editors advanced mean field methods theory practice 
mit press 
pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann publishers san mateo ca 
pearlmutter parra 
context sensitive generalization ica 
proceedings international conference neural information processing 
phillips moon rauss rizvi 
feret september database evaluation procedure 
international conference audio video biometric person authentication 

convergence condition tap equations infinite ranged ising glass model 
journal physics 
propp wilson 
exact sampling coupled markov chains applications statistical mechanics 
random structures algorithms 
rumelhart hinton williams 
learning internal representations error propagation 
rumelhart mcclelland pdp research group editors parallel distributed processing explorations microstructure cognition 
volume foundations 
mit press 
saad opper editors 
advanced mean field methods theory practice 
mit press 
sallans 
hierarchical community experts 
master thesis university toronto canada 
sallans hinton 
free energies represent values multiagent reinforcement learning task 
advances neural information processing systems volume 
mit press 
bibliography shafer shenoy 
probability propagation 
annals mathematics artificial intelligence 
sompolinsky lee 
information maximization approach overcomplete recurrent representations 
advances neural information processing systems volume pages 
simoncelli freeman adelson heeger 
shiftable multi scale transforms 
ieee transactions information theory 
smolensky 
information processing dynamical systems foundations harmony theory 
rumelhart mcclelland editors parallel distributed processing explorations microstructure cognition 
volume foundations 
mit press 
tanaka 
information geometry mean field approximation 
saad opper editors advanced mean field methods 
mit press 
jordan 
loopy belief propagation gibbs measures 
proceedings conference uncertainty artificial intelligence volume 
teh hinton 
rate coded restricted boltzmann machines face recognition 
advances neural information processing systems volume 
mit press 
teh welling 
passing bouncing messages generalized inference 
technical report gatsby computational neuroscience unit ucl 
teh welling 
unified propagation scaling algorithm 
advances neural information processing systems volume 
teh welling 
improving efficiency iterative proportional fitting procedure 
proceedings artificial intelligence statistics 
teh welling osindero hinton 
energy models sparse overcomplete representations 
submitted journal machine learning research 
bibliography anderson palmer 
solution solvable model spin glass 
phil 
mag 
tipping bishop 
probabilistic principal component analysis 
technical report ncrg neural computing research group aston university 
turk pentland 
eigenfaces recognition 
journal cognitive neuroscience 
van hateren van der schaaf 
independent component filters natural images compared simple cells primary visual cortex 
proceedings royal society london 
wainwright 
stochastic processes graphs cycles geometric variational approaches 
phd thesis department electrical engineering computer science mit 
wainwright jaakkola willsky 
new class upper bounds log partition function 
proceedings conference uncertainty artificial intelligence volume 
wainwright jaakkola willsky 
tree reparameterization approximate estimation loopy graphs 
advances neural information processing systems volume 
weiss 
belief propagation revision networks loops 
technical report memo massachusetts institute technology 
weiss 
correctness local probability propagation graphical models loops 
neural computation 
weiss freeman 
optimality solutions max product belief propagation algorithm graphs 
ieee transactions information theory 
bibliography welling hinton osindero 
learning sparse topographic representations products student distributions 
advances neural information processing systems volume 
welling teh 
belief optimization binary networks stable alternative loopy belief propagation 
uncertainty artificial intelligence 
welling teh 
approximate inference boltzmann machines 
appear artificial intelligence 
werbos 
backpropagation time 
proceedings ieee 
williams 
analysis contrastive divergence learning gaussian boltzmann machines 
technical report edi inf rr institute adaptive neural computation division informatics university edinburgh 
yedidia 
idiosyncratic journey mean field theory 
saad opper editors advanced mean field methods 
mit press 
yedidia freeman weiss 
generalized belief propagation 
advances neural information processing systems volume pages 
yedidia freeman weiss 
constructing free energy approximations generalized belief propagation algorithms 
technical report tr mitsubishi electric research laboratories 
yuille 
cccp algorithms minimize bethe kikuchi free energies convergent alternatives belief propagation 
neural computation 
zhu wu mumford 
minimax entropy principle application texture modelling 
neural computation 
