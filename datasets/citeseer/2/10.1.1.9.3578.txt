online classification budget crammer computer sci 
eng 
hebrew university jerusalem israel cs huji ac il kandola royal holloway university london egham uk cs rhul ac uk yoram singer computer sci 
eng 
hebrew university jerusalem israel singer cs huji ac il online algorithms classification require vast amounts memory computation time employed conjunction kernel functions 
describe analyze simple approach fly reduction number past examples prediction 
experiments performed real datasets show proposed algorithmic approach single epoch competitive support vector machine svm batch algorithm accesses training example multiple times 
motivation kernel methods widely data modeling prediction conceptual simplicity outstanding performance real world tasks 
support vector machine svm known algorithm finding kernel linear classifiers maximal margin 
kernel trick provide effective method deal high dimensional feature spaces model complex input phenomena embedding inner product spaces 
despite generalization error upper bounded function margin linear classifier notoriously difficult implement classifiers efficiently 
empirically translates long training times 
number alternative algorithms exist finding maximal margin hyperplane inspired rosenblatt perceptron algorithm line learning algorithm linear classifiers 
svms inspired number modifications enhancements original perceptron algorithm 
incorporate notion margin learning prediction processes whilst exhibiting empirical performance practice 
examples algorithms include relaxed online maximum margin algorithm approximate maximal margin classification algorithm alma margin relaxed algorithm mira conjunction kernel functions :10.1.1.21.2693
notable limitation kernel methods computational complexity amount computer memory require store called support patterns grows linearly number prediction errors 
number attempts speed training testing svm enforcing sparsity condition 
devise online algorithm sparse generalizes 
achieve goal algorithm employs insertion deletion process 
informally thought revising weight vector example prediction mistake 
event occurs algorithm adds new erroneous example insertion phase immediately searches past examples appear redundant addition deletion phase 
describe making adjustment algorithm allows modify standard online proof techniques provide bound total number examples algorithm keeps 
organized follows 
sec 
formalize problem setting provide brief outline method obtaining sparse set support patterns online setting 
sec 
theoretical algorithmic details approach provide bound number support patterns constitute cache 
sec 
provides experimental details evaluated real world datasets illustrate performance merits sparse online algorithm 
ideas 
problem setting algorithms focuses online additive algorithms classification tasks 
problems typically stream instance label pairs 

assume instance vector label belongs finite set section assume relax assumption sec 
describe experiments datasets consisting labels 
dealing task predicting new labels thresholded linear classifiers form sign commonly employed 
vector typically represented weighted linear combination examples 
instances referred support patterns 
assumption output classifier solely depends inner products form kernel functions easily employed simply replacing standard scalar product function satisfies mercer conditions 
resulting classification rule takes form sign sign 
majority additive online algorithms classification example known perceptron share common algorithmic structure 
online algorithms typically rounds 
tth round online algorithm receives instance computes inner products sets predicted label sign 
algorithm receives correct label evaluates exact value parameter depends specific algorithm classification 
result test negative algorithms modify implicitly set 
algorithms modifies classification predetermined update rule 
informally consider update decomposed stages 
firstly algorithms choose non negative value exact choice parameter algorithm dependent 
secondly prediction vector replaced linear combination current vector example third optional stage see example norm newly updated weight vector scaled 
various online algorithms differ way values parameters set 
notable example online algorithm perceptron algorithm set 
algorithms relaxed online maximum margin algorithm approximate maximal margin classification algorithm alma margin relaxed algorithm mira described framework constants simple ones employed perceptron algorithm :10.1.1.21.2693
important computational consideration needs employing kernel functions machine learning tasks 
amount memory required store called support patterns grows linearly number prediction errors 
input tolerance 
initialize set 
loop 
get new instance predict sign 
get new label update 
insert 

set 
compute 


output sign 
aggressive perceptron algorithm variable size cache 
shift focus problem devising online algorithms budget conscious attempt keep number support patterns small 
approach attractive reasons 
firstly training time classification time reduced significantly store fraction potential support patterns 
secondly small number support patterns intuitively simpler exhibit generalization properties complex classifiers large numbers support patterns 
see instance formal results connecting number support patterns generalization error 
input 

loop choose 
exists return 
remove example 


return 

sec 
formal analysis algorithmic details approach 
provide general overview restrict number support patterns online setting 
denote indices patterns constitute classification vector round received 
online classification algorithms discussed keep enlarging example added deleted 
online algorithm receives examples performance classifier improves past examples may redundant removed 
put way old examples may inserted cache simply due lack support patterns early rounds 
examples observed old examples replaced new examples location closer decision boundary induced online classifier 
add new stage online algorithm discard old examples cache suggest modification online algorithm structure follows 
adding inserting tth scan cache seemingly redundant examples examining margin conditions old examples example discard classifier cache updating setting 
pseudocode budget conscious version aggressive perceptron algorithm fig 

say algo rithm employs variable size cache limit explicitly number support patterns attempt discard patterns possible cache 
similar modification described aggressive perceptron online classification algorithms outlined 
particular modification mira algorithm experiments :10.1.1.21.2693
analysis section provide main formal result algorithm described previous section 
informally theorem states actual size cache algorithm builds inversely proportional square best margin achieved data 
form bound common numerous online learning algorithms classification 
bound size cache common settings corresponding bounds number prediction mistakes 
bound depends margin algorithm check new example added cache discard old examples attaining large margin 
clearly larger value add examples cache 
theorem 
input sequence algorithm fig 

denote max 
assume exists vector unit norm classifies entire sequence correctly margin min 
number support patterns constituting cache proof proof theorem mistake bound perceptron algorithm 
prove theorem bound compare bounds 
denote weight ith example round stage algorithm 
similarly denote weight ith example round stage calling fig 
procedure 
analogously denote corresponding instantaneous classifiers 
derive lower bound bounding term recursive manner 
ct ct 
turn upper bound recall example may added cache removed cache single time 
write telescopic sum 

consider different scenarios may occur new example 
case insert tth example cache 
case 
second scenario example inserted cache discarded rounds inserted condition hold 
combining assumption examples enclosed ball radius get scenario occurs example inserted cache round removed cache round 
previous case bound value summands equ 
input tolerance cache limit initialize set 
loop 
get new instance predict sign 
get new label update 
remove example find arg max 
update 
remove 
insert 

set 
compute output sign 
aggressive perceptron algorithm fixed size cache 
form cache update know summarizing cases see examples persist cache contribute factor bound telescopic sum equ 
rest examples contribute bound 
bound norm follows 
finish proof applying cauchy swartz inequality assumption 
combining equ 
equ 
get gives desired bound 
experiments section describe experimental methods compare performance standard online algorithms new algorithm described 
describe shortly variant sets hard limit number support patterns 
experiments designed aim trying answer questions 
effect number support patterns generalization error measured terms classification accuracy unseen data second algorithm described fig 
able find optimal cache size able achieve best generalization performance 
examine question separately modified version algorithm described fig 
restricted fixed bounded cache 
modified algorithm refer fixed budget perceptron name 



training examples test examples classes attributes mnist letter usps table description datasets experiments 
simulates original perceptron algorithm notable difference 
number support patterns exceeds pre determined limit chooses support pattern cache discards 
modification number support patterns exceed pre determined limit 
modified algorithm described fig 

algorithm deletes example seemingly attains highest margin removal example line fig 

despite simplicity original perceptron algorithm generalization performance datasets remarkable 
year number additive online algorithms developed shown better performance number tasks 
preferred embed ideas online algorithm start higher baseline performance 
chosen margin relaxed algorithm mira baseline algorithm exhibited generalization performance previous experiments additional advantage designed solve multiclass classification problem directly recourse performing reductions :10.1.1.21.2693
algorithms evaluated natural datasets mnist usps letter characteristics datasets summarized table 
comprehensive overview performance various algorithms datasets 
algorithms evaluated online implausible specific ordering examples affect generalization performance 
report results averaged random permutations usps letter random permutations mnist 
free parameter optimization carried simply values reported :10.1.1.21.2693
specifically margin parameter set algorithms datasets 
homogeneous polynomial kernel degree training mnist usps data sets rbf kernel letter data set 
variance rbf kernel identical :10.1.1.21.2693
evaluated performance algorithms total 
algorithm standard mira online algorithm incorporate budget constraints 
second algorithm version mira described fig 
uses fixed limited budget 
enumerated cache size limit experiment performed 
different sizes tested dataset dependent dataset evaluated different sizes 
note enumeration done online fashion goal employing algorithm fixed size cache underscore merit truly adaptive algorithm 
third algorithm version mira described fig 
adapts cache size running algorithms 
report additional results multiclass version svm :10.1.1.21.2693
whilst algorithm online training process considers examples algorithm serves gold standard algorithm want compare available www research att com yann available ftp tuebingen mpg de available www ics uci edu mlearn mlrepository html mnist support patterns test error fixed adaptive svm mira usps support patterns test error fixed adaptive svm mira letter support patterns test error fixed adaptive svm mira mnist support patterns training online errors fixed adaptive mira usps support patterns fixed adaptive mira letter support patterns training online errors fixed adaptive mira mnist support patterns training margin errors fixed adaptive mira usps support patterns fixed adaptive mira letter support patterns fixed adaptive mira results data sets mnist left usps center letter right 
point plot designates test error axis vs number support patterns axis 
algorithms compared svm mira mira fixed cache size mira variable cache size 
performance 
note multiclass svm report results best set parameters coincide set parameters online training 
results summarized fig 
composed different plots organized columns 
plots corresponds different dataset mnist left usps center letter right 
plots axis designates number support patterns algorithm uses 
results fixed size cache connected line emphasize performance dependency size cache 
top row columns shows generalization error 
axis designates test error algorithm unseen data training 
looking error algorithm fixed size cache reveals broad range cache size algorithm exhibits performance 
fact mnist usps sizes test error algorithm better svm test error 
naturally fix correct size hindsight question algorithm variable cache size viable automatic size selection method 
analyzing datasets turn reveals case algorithm obtains similar number support patterns test error compared svm method 
results somewhat impressive letter dataset contains examples class 
possible explanation algorithm fewer chances modify distill cache 
results remarkable online algorithms single pass data variable size method finds cache size making comparable svm terms performance 
mira algorithm incorporate form example insertion deletion algorithmic structure obtains poorest level performance terms generalization error terms number support patterns 
plot online training error number support patterns row fig considered fly validation generalization performance 
plots indicate fixed adaptive versions algorithm datasets low online training error translates generalization performance 
comparing test error plots online error plots see nice similarity qualitative behavior errors 
online error easy evaluate choose cache size fixed size algorithm 
third row gives online training margin errors translates directly number insertions cache 
see test error compactness algorithm variable cache size come price 
algorithm significantly insertions cache fixed size version algorithm 
upper sets plots indicate surplus insertions taken care excess deletions result performance 
summary online algorithm variable cache svm obtains similar levels generalization number support patterns 
svm somewhat better aspects letter dataset online algorithm simpler implement performs single sweep training data 
summary described analyzed new sparse online algorithm attempts deal computational problems implicit classification algorithms svm 
proposed method empirically tested performance size resulting classifier error rate comparable svm 
possible extensions enhancements 
currently looking alternative criteria deletions examples cache 
instance weight examples relay information importance accurate classification 
incorporating prior knowledge insertion deletion scheme prove important 
hope enhancements proposed approach viable alternative svm batch algorithms 
authors john shawe taylor helpful comments discussions 
research partially funded eu project 
ist 
crammer singer :10.1.1.21.2693
ultraconservative online algorithms multiclass problems 
machine learning research 
gentile 
new approximate maximal margin classification algorithm 
journal machine learning research 
learning algorithms optimal stability neural networks 
journal physics 
li long 
relaxed online maximum margin algorithm 
machine learning 

convergence proofs perceptrons 
proceedings symposium mathematical theory automata volume xii pages 
rosenblatt 
perceptron probabilistic model information storage organization brain 
psychological review 
reprinted neurocomputing mit press 
vapnik 
statistical learning theory 
wiley 
