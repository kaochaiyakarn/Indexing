proceedings th international conference machine learning icml pp 
sydney australia july semi supervised clustering seeding basu cs utexas edu department computer sciences university texas austin tx banerjee ece utexas edu department electrical computer engineering university texas austin tx raymond mooney mooney cs utexas edu department computer sciences university texas austin tx semi supervised clustering uses small amount labeled data aid bias clustering unlabeled data 
explores labeled data generate initial seed clusters constraints generated labeled data guide clustering process 
introduces semi supervised variants kmeans clustering viewed instances em algorithm labeled data provides prior information conditional distributions hidden category labels 
experimental results demonstrate advantages methods standard random seeding cop kmeans previously developed semi supervised clustering algorithm 

learning tasks large supply unlabeled data insucient labeled data expensive generate 
semi supervised learning combines labeled unlabeled data training improve performance 
semi supervised learning applicable classi cation clustering 
supervised classi cation known xed set categories category labeled training data induce classi cation function 
semi supervised classi cation training exploits additional unlabeled data frequently resulting accurate classi cation function blum mitchell ghahramani jordan 
unsupervised clustering unlabeled dataset partitioned groups similar examples typically optimizing objective function characterizes partitions 
semi supervised clustering labeled data unlabeled data obtain better clustering 
explores labeled data generate seed clusters initialize clustering algorithm constraints generated labeled data guide clustering process 
proper seeding biases clustering region search space reducing chances getting stuck poor local optima simultaneously producing clustering similar user speci ed labels 
initial labeled data represent relevant categories semi supervised clustering semi supervised classi cation algorithms categorization 
domains knowledge relevant categories incomplete 
semi supervised classi cation semi supervised clustering group data categories initial labeled data extend modify existing set categories needed re ect regularities data 
introduces semi supervised variants kmeans clustering macqueen initial labeled data seeding 
motivate algorithms expectation maximization em framework dempster showing seeding explained conditional distribution hidden category labels 
results experiments demonstrating advantages methods standard random seeding alternative semisupervised kmeans algorithm 

background kmeans clustering algorithm iterative relocation partitions dataset clusters locally minimizing average squared distance data points cluster centers 
set data points fx 
xn kmeans algorithm creates partitioning fx 
represent partition centers objective function kmeans kx locally minimized 
cop kmeans algorithm cop kmeans semisupervised variant kmeans initial background knowledge provided form constraints instances dataset clustering process 
types constraints link instances cluster link instances di erent clusters clustering process generate partition satis es constraints 
developed semi supervised variants kmeans compared cop kmeans 
spkmeans algorithm spherical kmeans spkmeans algorithm standard kmeans applied data vectors normalized unit norm data points lie unit sphere dhillon 
assuming kx eqn 
get kx clustering problem equivalently formulated maximizing objective function spkmeans spkmeans algorithm gives local maximum objective function 
spkmeans algorithm computational advantages sparse high dimensional data vectors common domains text clustering 
reason spkmeans experiments 

algorithms section explain semi supervision incorporated kmeans algorithm seeding propose variants kmeans algorithm seeds give mathematical motivation proposed algorithms 
algorithm seeded kmeans input set data points fx 
xn number clusters set initial seeds output disjoint partitioning fx kmeans objective function optimized method 
sh 
repeat convergence 
assign cluster assign data point cluster set arg min kx 
estimate means jx 

seeded kmeans algorithm algorithm constrained kmeans input set data points fx 
xn number clusters set initial seeds output disjoint partitioning fx kmeans objective function optimized method 
sh 
repeat convergence 
assign cluster assign cluster set 
assign cluster set arg min kx 
estimate means jx 

constrained kmeans algorithm seeding dataset previously mentioned kmeans clustering dataset generates partitioning fx kmeans objective locally minimized 
called seed set subset data points supervision provided follows user provides cluster partition belongs 
assume corresponding partition typically atleast note get disjoint partitioning fs seed set belongs supervision 
partitioning seed set forms seed clustering guide kmeans algorithm 
semi supervised kmeans algorithms seeded kmeans seed clustering initialize kmeans algorithm 
initializing kmeans random means mean lth cluster initialized mean lth partition seed set 
seed clustering disjoint subsets union initialization seeds steps algorithm 
algorithm detail fig 

constrained kmeans seed clustering initialize kmeans algorithm described seeded kmeans algorithm 
subsequent steps cluster memberships data points seed set re computed assign cluster steps algorithm cluster labels seed data kept unchanged labels non seed data re estimated 
algorithm detail fig 

constrained kmeans seeds kmeans algorithm user speci ed labeled data keeps labeling unchanged algorithm 
seeded kmeans user speci ed labeling seed data may changed course algorithm 
constrained kmeans appropriate initial seed labeling noise free user want labels seed data change appropriate presence noisy seeds 
aspects algorithms studied detail experiments sec 

semi supervised kmeans em em algorithm general method nding maximum likelihood estimate parameters underlying distribution generally probabilistic data generation process set observed data incomplete missing values 
denotes observed data denotes current estimate parameter values denotes missing data step em algorithm computes expected value log likelihood log distribution zjx bilmes 
shall demonstrate semi supervision provided kmeans algorithm essentially determines conditional distribution expectation computed 
shall take closer look assumptions distribution em framework solving kmeans problem ect semi supervision evident 
kmeans clustering algorithm essentially em algorithm mixture gaussians certain assumptions 
data generation process kmeans assumed follows rst gaussian chosen prior probability distribution data point sampled distribution chosen gaussian 
fx 
xn set data points want cluster missing data cluster assignment data points 
takes values 
kg conditioned datapoint consideration 
denote deriving kmeans assume prior distribution gaussians uniform gaussian identity covariance 
parameter set consists just means 
assumptions show bilmes zjx log log 
kx jx kx jx constant 
assuming jx arg min kx replacing eqn 
note expectation term comes negative known kmeans objective function additive constant 
problem maximizing expectation complete data log likelihood assumptions minimizing kmeans objective function 
keeping mind assumption eqn 
kmeans objective written kmeans kx jx missing data kmeans problem conditional distributions jx 
knowledge distributions solves problem normally way compute 
semi supervised clustering framework user provides information data points speci es corresponding conditional distributions 
example data points link constraint sec 
jx jx identically distributed 
fact data points transitive closure connected set link constraints identically distributed 
semi supervision essentially provides information conditional distributions jx 
assumption eqn 
derived assuming covariance gaussians letting kearns 
standard kmeans initial supervision means chosen randomly initial data points assigned nearest means subsequent step 
explained point dataset possible conditional distributions associated satisfying eqn 
corresponding means belong 
assignment data point random cluster rst step similar picking conditional distribution random possible conditional distributions 
seeded kmeans initial supervision equivalent specifying conditional distributions jx seed points speci ed conditional distributions seed data just initial step algorithm jx reestimated steps algorithm 
constrained kmeans initial step seeded kmeans 
di erence seed data points initial labels conditional distributions jx kept unchanged algorithm conditional distribution non seed points re estimated step 
experiments spkmeans framework sec 

framework point lies unit sphere kx expectation term eqn 
equivalent zjx log jx maximizing spkmeans objective function equivalent maximizing expectation complete data log likelihood step em algorithm 

experiments experiments data sets cmu newsgroups data yahoo 
news data 
dataset ran algorithms seeded kmeans constrained kmeans cop kmeans 
random kmeans means initialized mean entire data randomly perturbing times fayyad 
technique initialization results unsupervised kmeans previous dhillon 
compared performance methods datasets varying seeding noise levels fold cross validation 
dataset spkmeans underlying kmeans algorithm kmeans variants 
datasets newsgroups dataset newsgroups collection messages collected di erent usenet newsgroups messages newsgroups chosen dataset partitioned newsgroup name 
experiments mc toolkit creating vector space model text documents 
newsgroups dataset mc generated vocabulary words message represented sparse vector dimensional space tfidf weighting 
yahoo 
news series yahoo 
news dataset collection yahoo 
news articles belonging di erent yahoo 
categories 
vector space model set yahoo 
words data points reside dimensional space tfidf weighted 
text datasets non content bearing stopwords high frequency words low frequency words removed methodology dhillon 

original newsgroups dataset datasets generated small newsgroups contains random subsample documents newsgroups different newsgroups selects di erent newsgroups original newsgroups dataset alt atheism rec sport baseball sci space newsgroups selects similar newsgroups original newsgroups dataset comp graphics comp os comp windows 
dataset small newsgroups created study ect dataset size performance algorithms 
different newsgroups newsgroups generated study ect data separability algorithms 
evaluation measures evaluation measures experiments 
objective function kmeans spkmeans higher objective function better performance 
measure take account user labeling data 
measure mutual information mi determines amount statistical information shared random variables representing cluster user labeled class assignments data points 
mi computed methodology strehl 

www ai mit edu people newsgroups www cs utexas edu users dm ftp ftp cs umn edu users boley seed fraction seeded kmeans constrained kmeans cop kmeans random kmeans 
comparison mi algorithms newsgroups data noise fraction learning curves cross validation algorithms dataset generated learning curves fold cross validation 
studying ect seeding dataset set aside test set particular fold 
training sets di erent points learning curve obtained remaining data varying seed fraction steps results point learning curve obtained averaging folds 
clustering algorithm run dataset mi measure calculated test set 
studying ects noise seeding similar learning curves generated keeping xed fraction seeding varying noise fraction 
seed noise generation seeded kmeans constrained kmeans seeds point learning curve selected dataset corresponding seed fraction 
cop kmeans link link constraints generated speci ed seeds 
cluster centers chosen randomly chosen link constraints participates enforced items chosen instance link assigned new cluster chosen center cluster 
real life application semi supervision provided human user chance supervision may erroneous cases 
simulate labeling noise experiments changing labels fraction seed examples random incorrect value 

analysis results mi respect seeding zero noise case semi supervised algorithms perform better unsupervised algorithm terms mi measure figs 
irrespective size dataset 
constrained kmeans performs seeded kmeans uses correct user bias introduced user labeled seeds execution algorithm case 
constrained kmeans cop kmeans treat seeds constraints fact constrained kmeans uses seeds initialize clusters opposed cop kmeans necessarily results having better performance cases zero noise 
fact ect seeding important cases fig 
seeded kmeans performs signi cantly better cop kmeans 
objective function respect seeding mi measure increases increase seed fraction semi supervised algorithms behavior objective function depend user bias provided user labeled seeds consistent assumptions kmeans 
category structure created user labeling dataset satis es kmeans assumptions data partition induced seeding close optimal partition kmeans known converge local optimum case fig 
devroye 
hand user bias inconsistent kmeans assumptions constrained seeding result convergence suboptimal solution fig 

note necessarily maintain assignments seed points subsequent iterations objective function decrease due con ict bias constrained kmeans keep seeds constraints objective function decreases increase seeding 
random kmeans uses seeds behavior independent con ict 
dataset separability semi supervision gives substantial improvement unsupervised clustering datasets dicult cluster sense lot overlap clusters newsgroups fig 
datasets easily separable different newsgroups fig 
improvement marginal 
dataset easily separable bad local minima random kmeans easily nd cluster structure 
datasets overlapping cluster struc seed fraction seeded kmeans constrained kmeans cop kmeans random kmeans 
comparison mi algorithms small newsgroups data noise fraction seed fraction seeded kmeans constrained kmeans cop kmeans random kmeans 
comparison mi algorithms yahoo 
data noise fraction ture seeding important factor helping algorithm nd clustering 
mi measure separable dataset general higher overlapping dataset high seeding harder problem solve 
performance incomplete seeding ran initial experiments incomplete seeding seeds speci ed cluster fig 
seen mi metric decrease substantially increase number categories showing semi supervised clustering algorithms extend seed clusters generate clusters order regularity data 
performance respect noise noise increased performance constrained kmeans cop kmeans starts degrade compared 
cop kmeans constrained kmeans keep noisy seeds subsequent seed fraction seeded kmeans constrained kmeans cop kmeans random kmeans 
comparison objective functions algorithms small newsgroups data noise fraction seed fraction seeded kmeans constrained kmeans cop kmeans random kmeans 
comparison objective functions algorithms yahoo 
data noise fraction iteration algorithm seeded kmeans abandon noisy seed labels subsequent iterations fig 

seeded kmeans quite robust noisy seeding take full advantage seeding gives algorithm initialization 
statistical signi cance section tested various datasets 
example small newsgroup dataset signi cant seed fraction rst aspects discussed tailed paired test 
noise experiments signi cant noise fraction 

related semi supervised classi cation algorithms shown improvements classi cation accuracy seed fraction seeded kmeans constrained kmeans cop kmeans random kmeans 
comparison mi algorithms newsgroups data noise fraction seed fraction seeded kmeans constrained kmeans cop kmeans random kmeans 
comparison mi algorithms different newsgroups data noise fraction purely supervised algorithms training blum mitchell transductive support vector machines svms joachims semi supervised em ghahramani jordan nigam 
semi supervised clustering previous done labeled data aid clustering modifying clustering objective functions incorporate labeled data demiriz iterative user feedback cohn conditional distributions auxiliary space sinkkonen kaski 
previous cluster initialization includes comparisons data dependent data independent initialization techniques meila heckerman estimation modes data distribution initialization fayyad 
number categories constrained kmeans seeded kmeans cop kmeans random kmeans 
comparison mi algorithms newsgroups data seed fraction noise fraction seeded kmeans constrained kmeans cop kmeans random kmeans 
comparison mi algorithms small newsgroups data seed fraction 
connection general em framework interpretation semi supervision terms conditional distributions widens applicability proposed methods variety clustering problems 
important concept probabilistic soft seeding semi supervision gives algorithm probabilities seeds belonging various cluster labels explicitly stating cluster belongs 
terms conditional distribution need assumption eqn 
anymore conditional distributions multinomial distribution de ned cluster labels 
semi supervision probabilistic seeding applicable learning tasks volcano detection planet surface images smyth 

semi supervised clustering uses labeled data aid search bias partitioning unlabeled data conceptual groups 
seeded kmeans constrained kmeans semi supervised clustering algorithms labeled data form initial clusters constrain subsequent cluster assignment 
methods viewed instances em algorithm labeled data provides prior information conditional distributions hidden category labels 
experimental results demonstrate advantages methods standard random seeding cop kmeans alternative semi supervised kmeans algorithm 
particular seeding constraints robust semi supervised method sensitive noise imperfections supervised data 

acknowledgment research supported nsf iis ecs mcd fellowship awarded university texas austin 
bilmes 

gentle tutorial em algorithm application parameter estimation gaussian mixture hidden markov models technical report icsi tr 
icsi 
blum mitchell 

combining labeled unlabeled data training 
proc 
th annual conf 
computational learning theory 
cohn caruana mccallum 

semi supervised clustering user feedback 
unpublished manuscript 
available www cs cmu edu mccallum 
demiriz bennett embrechts 

semi supervised clustering genetic algorithms 
arti cial neural networks engineering annie 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistical society 
devroye lugosi 

probabilistic theory pattern recognition 
springer verlag 
dhillon fan guan 

ecient clustering large document collections 
data mining scienti engineering applications 
kluwer academic publishers 
fayyad reina bradley 

initialization iterative re nement clustering algorithms 
proceedings fourth international conference knowledge discovery data mining kdd pp 

ghahramani jordan 

supervised learning incomplete data em approach 
advances neural information processing systems pp 

joachims 

transductive inference text classi cation support vector machines 
proc 
th intl 
conf 
machine learning icml 
kearns mansour ng 

information theoretic analysis hard soft assignment methods clustering 
proc 
th conf 
uncertainty arti cial intelligence uai pp 

macqueen 

methods classi cation analysis multivariate observations 
proc 
th berkeley symposium mathematical statistics probability pp 

meila heckerman 

experimental comparison clustering initialization methods technical report msr tr 
microsoft research 
nigam mccallum thrun mitchell 

text classi cation labeled unlabeled documents em 
machine learning 
sinkkonen kaski 

semisupervised clustering conditional distributions auxiliary space technical report 
helsinki university technology 
smyth fayyad burl perona 

inferring ground truth subjective labelling venus images 
advances neural information processing systems 
strehl ghosh mooney 

impact similarity measures web page clustering 
workshop arti cial intelligence web search aaai pp 

cardie rogers 

constrained means clustering background knowledge 
proc 
th international conference machine learning icml 
