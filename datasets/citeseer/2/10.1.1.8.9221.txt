distance metric learning kernels tsang james kwok department computer science hong kong university science technology clear water bay hong kong email cs ust hk propose feature weighting method works input space kernel induced feature space 
assumes availability similarity dissimilarity information number parameters transformation depend number features 
feature weighting regarded performing nonparametric kernel adaptation 
experimental results toy real world datasets show promising results 
classification clustering algorithms rely inner product distance measure patterns 
examples include nearest neighbor classifiers radial basis function networks kernel methods means clustering 
commonly euclidean distance metric pp gamma assumes features equal importance violated real world applications 
remedy large number feature weighting methods proposed 
typically weight allocated feature resulting diagonally weighted euclidean distance pp gamma gamma aa gamma weight feature diagonal matrix ii 
ignores possible correlation features 
alternative fully weighted euclidean distance gamma aa gamma arbitrary matrix may lead excessive number free parameters 
various methods determine example set aa inverse covariance matrix leading called mahalanobis distance 
possibility maximize ratio intra class variance inter class variance globally locally 
limitation feature weighting methods designed classification problems require availability class label information 
discussed easier obtain similarity dissimilarity information may know certain pairs patterns similar dissimilar 
xing proposed distance metric learning method utilizes information convex programming 
involves iterative procedure projection eigen decomposition costly number features large 
limitation methods number parameters weights increases number features 
easily kernelized feature space induced kernels gaussian kernel infinite dimensional 
note feature weighting methods proposed specifically support vector machines svms 
select features input space feature space 
propose feature weighting method works input kernel induced feature spaces 
basic idea simple echoed literature similar patterns pushed dissimilar patterns pulled apart 
proposed method assumes availability similarity dissimilarity information 
importantly kernel methods number parameters related number patterns dimensionality patterns 
allows feature weighting possibly infinite dimensional feature space 
alternatively view method modifying distance metric feature space performing nonparametric kernel adaptation 
rest organized follows 
sections ii iii describe proposed distance metric learning methods input feature space respectively 
experiment results toy real world datasets section iv section gives concluding remarks 
ii 
input space metric learning patterns input space assume inner product defined ij mx theta positive semi definite psd matrix 
psd ij written ij aa theta matrix 
corresponding distance ij ij gamma aa gamma main question find suitable transformation sequel correspond original metric aa correspond learned metric 
primal dual problems denote sets containing similar dissimilar pairs sizes ns nd respectively 
order improved tightness similar patterns better separability dissimilar patterns consider shrinking distance similar patterns expanding distance dissimilar 
words ij minimized ij ij imposed 
sequel expansion enforced requiring ij gamma ij fl fl variable maximized 
may able enforce condition perfectly pairs general 
svms slack variables introduced optimization problem 
notice matrix basically projects patterns set useful features 
ideally set small meaning small rank aa rank aa rank desirable 
assume eigen decomposition aa uu rank aa rank kk direct minimization zero norm difficult approximate euclidean norm kk optimization 
discussion leads primal problem similar form svm min cs ns ij cd gamma fl nd ij respect variables fl ij subject constraints fl pair ij gamma ij fl gamma ij ij cs cd non negative tunable parameters 
solve constrained optimization problem standard method lagrange multipliers 
lagrangian function cs ns ij cd gamma fl nd ij gamma ff ij ij gamma ij gamma fl ij gamma ij ij gamma fl cs ns gamma aa gamma cd gamma fl nd ij gamma ff ij gamma aa gamma gamma gammafl ij gamma ij ij gamma fl setting derivatives respect primal variables zero obtain assuming non singular aa ff ij gamma gamma gamma cs ns gamma gamma cd ff ij gamma ff ij cd nd gamma ij substituting dual problem max ff ij ff ij gamma gamma gamma xk ff ij ff kl gamma gamma cs ns xk ff ij gamma gamma subject cd ff ij ff ij cd nd quadratic programming qp problem nd variables independent input dimensionality solved standard qp solvers 
qp problem suffer problem local maximum 
xing hand formulated case experiments 
geometrically means neighborhood point extend infinity 
metric learning problem convex programming problem 
free local optimum involves iterative procedure comprising projection eigen decomposition 
costly especially input dimensionality high 
obtain modified inner product corresponding distance metric 
support vector range lagrange multipliers studied examining karush kuhn tucker kkt conditions primal problem ff ij ij gamma ij gamma fl ij ij ij fl shown ij gamma ij fl ff ij cd nd fl ff ij fl ff ij cd nd similar svms 
lagrange multipliers nonzero upper bound constraints ij gamma ij fl exactly met 
lagrange multipliers zero constraints met larger margin corresponding patterns appear final solution support vectors 
lagrange multipliers upper bound constraints may violated 
corresponding ij may nonzero corresponding pair generates error 
margin fl obtained follows 
consider pair dissimilar patterns corresponding ff ij satisfies ff ij cd nd fl ij gamma ij implementation obtain average value fl pairs satisfy criterion 
obtain cd ff ij cd ff ij ff ij cd ff ij cd nd ff ij nd recall nd ff ij support vectors ff ij 
lower bound fraction support vectors 
fl kkt condition yields cd ff ij cd ff ij cd nd ff ij cd nd nd inequality holds second summation includes subset nonzero ff ij 
recall error pairs ff ij upper bound cd nd upper bound fraction errors 
analogous results 
iii 
feature space metric learning kernel function delta delta denote corresponding feature map oe ij oe oe 
results section ii depend explicitly input patterns easily kernelized replacing oe 
example objective function dual problem ff ij ii jj gamma ij gamma xk ff ij ff kl ik gamma il gamma jk jl cs ns xk ff ij ik gamma il gamma jk jl modified inner product oe oe effectively leads new kernel function delta delta oe aa oe ff ij ai gamma aj ib gamma jb gamma cs ns ai gamma aj ib gamma jb corresponding distance metric obtained 
iv 
experiments section perform experiments class toy dataset real world datasets table 
mentioned section proposed algorithm assumes availability similarity dissimilarity information 
simplify experiment setups class label information standard classification problems 
sets constructed defining patterns similar belong class dissimilar 
patterns randomly selected form training set learning metric classifier remaining patterns testing 
proposed method learn distance metric input space corresponds linear kernel feature space induced gaussian kernel exp gamma 
experiments fi gamma kx gamma xn training patterns 
nearest neighbor classifier employed original euclidean learned metrics classification 
shows data distributions distance metrics 
seen similar patterns ionosphere sonar wine uci repository microarray dna microarray dataset colon cancer www tuebingen mpg de bs people weston 
clustered dissimilar patterns separated 
table ii reports classification accuracies averaged repeated trials experiment 
general learned metric leads higher accuracies 
euclidean metric 
learned metric 
fig 

data different distance metrics 
propose feature weighting method uses similarity dissimilarity information modify distance metrics input space kernel induced feature space 
kernel methods number parameters transformation related number patterns number features 
learning method provides new means nonparametric kernel adaptation 
acknowledgments research partially supported research council hong kong special administrative region 
aha 
feature weighting lazy learning algorithms 
liu motoda editors feature extraction construction selection data mining perspective 
kluwer norwell ma 
table datasets experiments 
dataset dim class patterns toy ionosphere sonar wine microarray table ii nearest neighbor classification results 
dataset kernel euclidean learned metric metric toy linear rbf ionosphere linear rbf sonar linear rbf wine linear rbf microarray linear rbf blake keogh merz 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html university california irvine department information computer sciences 
peng gunopulos 
adaptive metric machine pattern classification 
leen dietterich tresp editors advances neural information processing systems cambridge ma 
mit press 
duda hart 
pattern classification scene analysis 
wiley new york 

adaptive scaling feature selection svms 
advances neural information processing systems cambridge ma 
mit press 
hastie tibshirani 
discriminant adaptive nearest neighbor classification 
ieee transactions pattern analysis machine intelligence june 
peng dai 
adaptive kernel metric nearest neighbor classification 
proceedings international conference pattern recognition quebec city canada 
scholkopf smola williamson bartlett 
new support vector algorithms 
neural computation 
scholkopf smola 
learning kernels 
mit 
weston mukherjee chapelle pontil poggio vapnik 
feature selection svms 
leen dietterich tresp editors advances neural information processing systems cambridge ma 
mit press 
wettschereck aha mohri 
review empirical evaluation feature weighting methods class lazy learning algorithms 
artificial intelligence review 
xing ng jordan russell 
distance metric learning application clustering side information 
advances neural information processing systems cambridge ma 
mit press 
