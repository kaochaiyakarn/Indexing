dynamics boosting cynthia rudin ingrid daubechies princeton university progr 
appl 
comp 
math 
fine hall washington road princeton nj ingrid math princeton edu robert schapire princeton university department computer science olden st princeton nj schapire cs princeton edu order understand adaboost dynamics especially ability maximize margins derive associated simplified nonlinear iterated map analyze behavior low dimensional cases 
find stable cycles cases explicitly solve adaboost output 
considering adaboost dynamical system able prove warmuth conjecture adaboost may fail converge maximal margin combined classifier nonoptimal weak learning algorithm 
adaboost known coordinate descent method known algorithms explicitly aim maximize margin adaboost arc gv 
consider differentiable function coordinate ascent yield maximum margin solution 
simple approximation derive new boosting algorithm updates slightly aggressive arc gv 
adaboost algorithm constructing strong classifier training set weak learning algorithm 
weak classifier produced weak learning algorithm probability misclassification slightly 
strong classifier smaller probability error test data 
adaboost boosts weak learning algorithm achieve stronger classifier 
adaboost practical boosting algorithm due success number similar boosting algorithms introduced see 
adaboost maintains distribution set weights training examples requests weak classifier weak learning algorithm iteration 
training examples misclassified weak classifier current iteration receive higher weights iteration 
result final combined classifier thresholded linear combination weak classifiers 
adaboost empirically suffer badly overfitting large number iterations 
lack overfitting attributed adaboost research partially supported nsf iis ccr ani afosr 
ability generate large margin leading better guarantee generalization performance 
possible achieve positive margin adaboost shown approximately maximize margin 
particular known adaboost achieves margin largest margin possibly attained combined classifier bounds appear 
subsequent boosting algorithms emerged adaboost arc gv main outline adaboost attempt explicitly maximize margin expense lowering convergence rate trick design update combined classifier maximizes margin fast rate convergence robust 
extensive theoretical empirical study adaboost unknown adaboost achieves maximal margin solution best upper bound probability error margin bounds 
limiting dynamics linearly inseparable case fully understood basic questions dynamics adaboost common case unknown 
instance know limit large number rounds adaboost eventually cycles base classifiers behavior chaotic 
study dynamics adaboost 
simplify algorithm reveal nonlinear iterated map adaboost weight vector 
iterated map gives direct relation weights time weights time including renormalization providing concise mapping original algorithm 
provide specific set examples trajectories iterated map converge limit cycle allowing calculate adaboost output vector directly 
interesting cases governing dynamics case optimal weak classifiers chosen iteration optimal case case permissible non optimal weak classifiers may chosen non optimal case 
optimal case weak learning algorithm required choose weak classifier largest edge iteration 
non optimal case weak learning algorithm may choose weak classifier long edge exceeds maximum margin achievable combined classifier 
natural notion non optimality boosting provides natural sense measure robustness 
large scale experiments gap theoretical bounds warmuth conjectured adaboost necessarily converge maximum margin classifier non optimal case adaboost robust sense 
practice weak classifiers generated cart heuristic weak learning algorithm implying choice need optimal 
section show conjecture true low dimensional example 
low dimensional study provides insight adaboost large scale dynamical behavior 
adaboost shown breiman coordinate descent algorithm particular exponential loss function 
minimizing function ways necessarily achieve large margins process coordinate descent responsible 
section introduce differentiable function maximized achieve maximal margins performing coordinate ascent function yields new boosting algorithm directly maximizes margins 
new algorithm adaboost formula choose direction ascent descent iteration adaboost chooses optimal direction new setting 
approximate update rule coordinate ascent function derive algorithm updates slightly aggressive arc gv 
proceed follows section introduce notation state adaboost algorithm 
decouple dynamics adaboost binary case reveal nonlinear iterated map 
section analyze dynamics simple case case hypothesis misclassified point 
example find stable cycles 
cycles show adaboost produces maximal margin solution optimal case result generalizes mm 
produce example promised show adaboost necessarily converge maximal margin solution non optimal case 
section introduce differentiable function maximize margin coordinate ascent approximate coordinate ascent update step derive new algorithm 
simplified dynamics adaboost training set consists example 
denote distribution weights training examples iteration expressed column vector 
denote transpose 
denote total number classifiers produced weak learning algorithm 
classifiers binary finite may large 
weak classifiers denoted assume list appears 
construct matrix ij ij training example classified correctly hypothesis 
unnormalized coefficient classifier final combined hypothesis denoted final combined hypothesis ada 
remains unused 
simplex dimensional vectors positive entries sum denoted margin training example defined ada equivalently edge hypothesis respect training data weighted probability error training set weighted 
goal find normalized vector maximizes min call minimum margin training examples margin classifier 
adaboost algorithm reduction iterated map 
adaboost optimal case 
input matrix number iterations max 
initialize 
loop max argmax ln position 

output combined max max iteration distribution computed step classifier maximum edge selected step weight classifier updated step 
note wlog omit unused columns 
adaboost reduced iterated map 
map gives direct relationship normalization step account automatically 
initialize iteration adaboost 
reduced iterated map 
argmax 

ij derive map consider iteration defined adaboost reduce follows 
ij ij ln ij ij ij ij ij ij ij ij define ij ij find likewise ij find reduction complete 
check see 
dynamics low dimensional adaboost introduce simple input matrix analyze convergence adaboost optimal case 
consider larger matrix show adaboost fails converge maximum margin solution non optimal case 
consider input matrix corresponding case training examples weak classifier misclassifies example 
add additional hypotheses chosen adaboost 
maximum value margin 
adaboost achieve result 
optimal case argmax consider dynamical system simplex defined reduced map 
triangular region vertices 
similarly regions see 
satisfy dynamics restricted edges triangle vertices iteration see 
component second component component second component left regions space classifiers respectively selected 
right weight vectors max restricted lie edges inner triangle 
position triangle position triangle position triangle position triangle component weight vector second component weight vector component weight vector second component weight vector upper left iterated map unfolded triangle 
axes give coordinates edges inner triangle 
plot shows 
upper right map iterated twice showing stable fixed points cycle 
lower left timesteps adaboost showing convergence cycle 
small rings indicate earlier timesteps adaboost larger rings indicate timesteps 
concentric rings positions cyc cyc cyc 
lower right timesteps adaboost random matrix 
axes vs reduced dimensional phase space iterated map stable fixed points orbits length 
consider periodic orbit length cyc cyc cyc 
clearly cycle starting cyc adaboost choose 
cyc 
computing cyc yields cyc way adaboost cycle hypotheses fact cycle cyc cyc cyc 
find cycles hypothesized cycle length exists visiting hypothesis turn reduced equations section solve cycle coordinates 
give outline proof global stability map contraction small perturbation cycle diminish yielding local stability cycles 
needs consider dimensional map defined unfolded triangle iteration trajectory lands triangle 
map iterates piecewise continuous monotonic piece find exactly interval mapped see 
consider second iteration map 
break unfolded triangle intervals find region attraction fixed cycle fact triangle union regions attraction 
convergence cycles fast shows absolute slope second iterated map fixed points 
combined classifier adaboost output combined cyc cyc cyc 
min combined adaboost produces maximal margin solution 
case generalized classifiers having misclassified training example case periodic cycles length contraction persist cycles stable 
note low dimensional case tried periodic cycles larger lengths exist contraction iteration harder show stability 
give example show non optimal adaboost necessarily converge maximal margin solution 
consider input matrix omitting unused columns matrix maximal margin 
optimal case adaboost produce value cycling columns recall non optimal case 
consider initial condition dynamics 
justified choosing optimal choice 
iteration yields satisfying choose 
third iteration choose fourth iteration find cycle cycle previous example extra dimension 
manifold cycles non optimal case lies cycle case value margin produced cycle 
established adaboost robust sense described weak learner required choose optimal hypothesis iteration required choose sufficiently weak classifier maximum margin solution necessarily attained 
practice may possible adaboost converge maximum margin solution hypotheses chosen slightly non optimal notion non optimal natural notion shown adaboost may converge 
note matrices maximum margin solution may attained non optimal case example simple matrix analyzed attained general shown example 
saying way adaboost converge non optimal solution fall wrong cycle may non cyclic ways algorithm fail converge maximum margin solution 
note algorithms mentioned section new algorithms section fixed points periodic orbits 
coordinate ascent maximum margins adaboost interpreted algorithm coordinate descent 
algorithms adaboost arc gv attempt maximize margin explicitly coordinate descent 
suggest boosting algorithm aims maximize margin explicitly arc gv adaboost coordinate ascent 
important note adaboost new algorithm choose direction descent ascent value formula argmax lends credence conjecture adaboost maximizes margin optimal case direction adaboost chooses direction choose maximize margin directly coordinate ascent 
function adaboost minimizes coordinate descent consider lim minimize original normalized yield maximum margin 
process coordinate descent awards adaboost ability increase margins simply adaboost ability minimize consider different function bears resemblance boosting objective ln ln verified nice properties concave function fixed value maximum occurs limit importantly min margin 
ln inequality equality examples achieve minimal margin second inequality holds took term 
performing coordinate descent adaboost perform coordinate ascent choice direction iteration argmax dg argmax ij ln 
terms second term depend term proportional direction chosen adaboost 
consider distance travel direction 
ideally maximize respect dg ij ij analytical solution maximization dimensional performed quickly 
approximate coordinate ascent algorithm avoids line search approximation maximization problem ij ij 
solve analytically ln ln max 
consider properties iteration scheme 
update strictly positive case positive margins due von neumann min max theorem equation mind max max min min preliminary proofs value increases iteration approximate coordinate ascent algorithm algorithms converge maximum margin solution non optimal case 
new update aggressive adaboost slightly aggressive arc gv 
algorithm mention adaboost different sort update 
converges combined classifier attaining margin inside interval log steps guarantee asymptotic convergence fixed 
boosting algorithms require minimization non convex functions choose compare simple updates adaboost due fast convergence rate adaboost arc gv 
adaboost arc gv algorithm initially large updates conservative estimate margin 
adaboost updates initially small estimate edge 
iterations margin adaboost adaboost arc gv approximate coord ascent coord ascent arc gv adaboost approximate coord 
ascent coord 
ascent iterations margin adaboost approximate coordinate ascent arc gv left performance algorithms optimal case random input matrix right adaboost arc gv approximate coordinate ascent synthetic data 
shows performance adaboost arc gv adaboost parameter set approximate coordinate ascent coordinate ascent line search iteration reduced randomly generated matrix optimal case 
adaboost settles cycle shown updates remain consistently large causing grow faster converge faster respect values cycle happen produce optimal margin solution adaboost quickly converges solution 
approximate coordinate ascent algorithm slightly aggressive updates adaboost closely aligned coordinate ascent slower 
adaboost methodical convergence rate convergence initially slower speeds 
artificial test data designed follows example points constructed randomly lies corner hypercube set sign indicates th component th weak learner ij 
expected convergence rate approximate coordinate ascent falls adaboost arc gv 
nonlinear iterated map defined adaboost understand update rule low dimensional cases uncover cyclic dynamics 
produced example show adaboost necessarily maximize margin non optimal case 
introduced coordinate ascent algorithm approximate coordinate ascent algorithm aim maximize margin directly 
direction ascent agrees direction chosen adaboost algorithms 
open problem understand dynamics cases 
robert schapire 
brief boosting 
proceedings sixteenth international joint conference artificial intelligence 
robert schapire yoav freund peter bartlett wee sun lee 
boosting margin new explanation effectiveness voting methods 
annals statistics october 
gunnar manfred warmuth 
maximizing margin boosting 
proceedings th annual conference computational learning theory pages 
gunnar manfred warmuth 
efficient margin maximizing boosting 
journal machine learning research submitted 
leo breiman 
prediction games arcing classifiers 
neural computation 
michael collins robert schapire yoram singer 
logistic regression adaboost bregman distances 
machine learning 
rosset ji zhu trevor hastie 
boosting regularized path maximum margin classifier 
technical report department statistics stanford university 
