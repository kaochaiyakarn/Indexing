ieee transactions pattern analysis machine intelligence vol 
september adaptive sparseness supervised learning rio figueiredo senior member ieee goal supervised learning infer functional mapping set training examples 
achieve generalization necessary control complexity learned function 
bayesian approaches done adopting prior parameters function learned 
propose bayesian approach supervised learning leads sparse solutions irrelevant parameters automatically set exactly zero 
ways obtain sparse classifiers laplacian priors support vector machines involve hyper parameters control degree sparseness resulting classifiers parameters adjusted estimated training data 
contrast approach involve hyper parameters adjusted estimated 
achieved hierarchical bayes interpretation laplacian prior modified adoption jeffreys noninformative hyperprior 
implementation carried expectationmaximization em algorithm 
experiments benchmark data sets show proposed approach yields state art performance 
particular method outperforms svms performs competitively best alternative techniques involves tuning adjustment sparseness controlling hyperparameters 
index terms supervised learning classification regression sparseness feature selection kernel methods expectationmaximization algorithm 
learning problem supervised learning formalized problem inferring function training set xn yn usually inputs dimensional vectors xi xi xi ir continuous ir context regression classification problems categorical nature binary 
obtained function evaluated generalizes accurately performs new data assumed follow distribution training data 
usually functionf assumed fixed structure depend set parameters say coefficients linear classifier weights feedforward neural network case goal estimate training data 
known achieve generalization necessary control complexity learned function 
complex may follow irrelevant properties particular data set trained usually called overfitting 
conversely overly simple function may rich capture true underlying relationship underfitting 
known problem addressed variety formal tools see 
overview methods proposed approach discriminative versus generative learning supervised learning formulated generative approach discriminative approach 
basically author institute telecommunications department electrical computer engineering instituto superior cnico lisboa portugal 
mail mtf lx pt 
manuscript received mar revised sept accepted mar 
recommended acceptance del 
information obtaining reprints article please send mail tpami computer org ieeecs log number 
ieee published ieee computer society generative approach involves estimating joint probability density classification usually done learning called densities xjy probability class 
regression done example representing joint density kernel method gaussian mixture see 
joint probability function estimate optimal bayesian decision rules derived standard bayesian decision theory machinery 
discriminative approach focus learning function directly training set 
known discriminative approaches include linear generalized linear models nearest neighbor classifiers tree classifiers feed forward neural networks support vector machines svm kernel methods 
usually computationally demanding discriminative approaches tend perform better especially small training data sets see 
approach described falls discriminative category 
bayesian discriminative learning gaussian priors bayesian approach complexity control discriminative learning consists prior favoring simplicity sense function learned vector hyperparameters 
usual choice analytical computational tractability zero mean gaussian prior appears different guises ridge regression weight decay 
gaussian priors heart nonparametric gaussian processes approach roots earlier spline models regularized radial basis functions 
main disadvantage gaussian priors control structural complexity learned function 
components say coefficient linear classifier happens irrelevant set figueiredo adaptive sparseness supervised learning exactly zero 
structural simplification additional tests 
sparseness sparse estimate defined irrelevant redundant components exactly zero 
sparseness desirable supervised learning reasons sparseness leads structural simplification estimated function 
obtaining sparse estimate corresponds performing feature variable selection 
kernel methods generalization ability improves degree sparseness key idea svm 
possible ways achieve sparse estimates zero mean laplacian gaussian prior expf ijg exp ij denotes norm parameter density 
sparseness inducing nature laplacian prior equivalently penalty regularization point view known exploited areas :10.1.1.35.7574
svms approach supervised learning leading sparse structures 
approaches laplacian priors svms hyperparameters controlling degree sparseness obtained estimates 
parameters commonly adjusted crossvalidation methods may time consuming 
proposed approach propose bayesian approach sparse regression classification main advantage absence parameters controlling degree sparseness 
achieved building blocks 
hierarchical bayes interpretation laplacian prior normal independent distribution proposed robust regression 
jeffreys noninformative second level hyperprior spirit expresses importantly 
expectation maximization em algorithm yields maximum posteriori map estimate observation noise variance case regression 
experimental evaluation proposed method sections synthetic real data show method performs competitively better state art methods svm 
related approaches method formally conceptually related automatic relevance determination ard concept underlies proposed relevance vector machine rvm 
rvm exhibits state art performance beats svms terms accuracy sparseness 
approach rely type ii maximum likelihood approximation ard rvm modeling assumptions lead marginal posteriori probability function mode located simple em algorithm 
detailed explanation difference approach rvm section 
organization section reviewing linear regression laplacian priors introduce approach 
section extends formulation classification problems probit generalized linear model 
experimental results reported section 
section concludes presenting concluding remarks including brief discussion limitations approach possible directions research 
regression linear regression gaussian prior consider regression functions linear respect parameter vector dimensionality denote xk ihi hk vector fixed functions input called features 
functions form include known formulations 
linear regression xd case 
nonlinear regression set fixed basis functions usually 
kernel regression xn xi symmetric kernel function svm rvm regression case 
follow standard assumption output variables training set contaminated additive white gaussian noise yi xi wi wn set independent zero mean gaussian samples variance yn likelihood function yj vj stands gaussian density mean covariance evaluated called design matrix identity matrix 
element denoted hij hij hj xi 
zero mean gaussian prior covariance ja jy gaussian mode known expression course maximum posteriori map estimate gaussian prior 
proportional ieee transactions pattern analysis machine intelligence vol 
september identity say called ridge regression 
obtain known ordinary squares estimate may undesirably large variance ill conditioned matrix 
regression laplacian prior favor sparse estimate laplacian prior adopted yk expf ijg expf case posterior probability density jy longer gaussian 
map estimate longer linear function arg min fkh yk kvk pi denotes squared euclidean norm 
regression criterion named lasso absolute shrinkage selection operator :10.1.1.35.7574:10.1.1.35.7574
understand norm induces sparseness notice moving vector away coordinate axes increases thel norm norm 
example ffiffi ffiffi ffiffi ffiffiffi 
particular case orthogonal matrix gives insight thel penalty 
case solved separately arg min sgn ijg denotes component positive part operator defined ifa ifa sgn sign function 
observe absolute value threshold estimate exactly zero estimate obtained subtracting constant equal threshold absolute value rule called soft threshold see fig 
widely wavelet signal image estimation 
hierarchical bayes view laplacian prior consider zero mean gaussian ij ij variance exponential hyper prior ij exp possible integrate ij ij ij expf ijg obtaining laplacian density 
shows laplacian prior equivalent level hierarchical bayes model zero mean gaussian priors independent exponentially distributed variances 
equivalence previously exploited derive em algorithms fig 

solid line estimation rule produced em algorithm jeffrey hyperprior orthogonal case 
dashed line rule 
dotted line soft threshold rule obtained laplacian prior 
robust regression laplacian noise models 
related equivalence considered 
sparse regression em hierarchical decomposition laplacian prior allows expectation maximization em algorithm implement lasso criterion 
done simply regarding hidden missing data 
observe complete log posterior logp jy easily obtained fact jy yj yj andp gaussian densities canbe carried analytically constant course adopt conjugate inverse gamma prior influence prior estimate small 
case easy obtain map estimate observe may resort em algorithm produces sequence estimates fort applying alternating steps step 
computes expected value respect missing variables complete log posterior current parameter estimates observed data usually called function logp step 
updates parameter estimates maximizing function respect parameters arg max em algorithm converges local maximum posteriori probability density function jy yj explicitly marginal prior figueiredo adaptive sparseness supervised learning gaussian simply conditional gaussian prior iteration 
find particular expressions function parameter updates modelling assumptions expressed summarize yj constant yk ij ky exp diag diagonal matrix inverse variances observed data missing data 
applying logarithms flat logp jy logp yj logp log ky second complete log posterior linear respect see terms depend step reduces computing conditional expectation current estimates denote diag observe ij ij depend forj ij 
exponential hyperprior elementary integration yields bi exp bi exp diag jb jb function obtained plugging place log ky step estimate update equations consists maximizing jb respect yielding arg max ky hb arg max log ky ky summarizing step implemented constitute step 
em algorithm computationally efficient way solve see methods proposed :10.1.1.35.7574
simple implement serves main goal open way adoption different 
comparison rvm position explain difference em approach relevance vector machine rvm simplicity consider known omit notation 
rvm modeling assumptions similar zero mean gaussian prior ij ij variance integrating hyper parameters explicitly implicitly em algorithm maximum likelihood estimate obtained marginal likelihood yj plugged posteriori jb 
bayesian literature known empirical bayes type ii maximum likelihood approach 
getting rid hyperparameter jeffreys prior question remains adjust controls degree sparseness proposal remove model replacing exponential hyperprior generically denoted noninformative jeffreys hyperprior prior expresses ignorance respect scale see importantly parameter free 
see prior scale invariant suppose change measurement units scale expressed defines new variable constant expressing change units scale 
applying rule change variable probability density function ieee transactions pattern analysis machine intelligence vol 
september fig 

logistic rescaled probit link functions 
retain prior notice prior normalizable integral finite called improper prior 
course prior longer leads laplacian prior prior see details 
shown experimentally prior strongly induces sparseness leads performance 
computationally choice leads minor modification em algorithm described matrix diag notice absence free parameter step 
get insight behavior scheme consider case orthogonal matrix 
case equations em algorithm decouple final estimate coefficient function corresponding fig 
plot function obtained em algorithm initialized comparison purposes plot known hard soft threshold estimation rules 
observe resulting estimation rule rules large values estimate close equal limit approaching behavior hard rule gets smaller estimate progressively shrunken approaching behavior soft rule threshold estimate exactly zero hard soft rules reason sparseness achieved 
important implementation detail elements expected go zero convenient deal imply handling arbitrarily large numbers 
turns rewrite step hu diag avoiding inversion elements 
necessary obtain inverse matrix simply solve corresponding linear system dimension number nonzero elements 
classification generalized linear models classification problems formulation somewhat complicated due categorical nature output variable 
standard approach consider generalized linear model glm 
class problem glm models probability observation belongs say class nonlinearity applied output linear regression function 
formally nonlinearity called link function denoted ir glm defined jx defined section 
logistic regression link function exp shown fig 

important feature glm approach yields class probabilities hard classifier obtain optimal classifiers different cost functions 
example cost function simply misclassification error classifier obtained thresholding 
probit common link logistic function adopt probit model standard gaussian cumulative distribution function cdf xj dx rescaled probit plotted fig 
logistic function notice indistinguishable 
course logistic probit functions rescaled horizontally scale implicitly absorbed learning probit classifier em fundamental reason choice probit model simple interpretation terms hidden variables 
consider hidden variable zero mean unit variance gaussian noise sample wj 
classification rule obtain probit model jx training data xn yn consider corresponding vector hidden missing variables zn zi xi wi figueiredo adaptive sparseness supervised learning fig 

mean number nonzero components versus number nonzero components dotted line identity 
course zi observed 
simple linear regression likelihood unit noise variance zj 
observation suggests em algorithm estimate treating missing data 
promote sparseness adopt hierarchical prior regression ij ij andp jeffreys prior 
complete log posterior hidden vectors logp logp zj yj dropping terms depend notice similar differences noise variance missing plays role observations regression equations 
expected value course similar regression case depends accordingly consider diagonal matrix defined 
addition need zj notice complete log posterior linear respect expressed closed form zij bt xi bt xi bt xi bt xi bt xi bt xi yi yi expressions easily derived noticing conditionally gaussian mean bt xi zero yi right truncated zero yi 
step similar regression case hu playing role observed data 
summarizing classification case step corresponds step carried 
table relative improvement modeling error methods experimental results regression variable selection linear regression example illustrates proposed method variable selection standard linear regression regression equation consider sequence vectors dimensional increasing number nonzero components 
specifically 
obtain random design matrices procedure obtain data points unit noise variance :10.1.1.35.7574:10.1.1.35.7574
obtain estimates design matrix 
fig 
shows mean number estimated nonzero components function true number 
method exhibits ability find correct number nonzero components manner 
adaptive consider experimental conditions studied :10.1.1.35.7574:10.1.1.35.7574
cases design matrices generated described :10.1.1.35.7574:10.1.1.35.7574
compared relative modeling error defined improvement respect ordinary squares solution methods studied lasso adjusted cross validation cv generalized crossvalidation gcv subset selection crossvalidation see details :10.1.1.35.7574:10.1.1.35.7574
shown table method performs comparably best method case involves tuning adjustment parameters computationally simpler faster 
kernel regression study performance method kernel regression xn xi xi kernel function 
type representation support vector machine svm regression relevance vector machine rvm regression 
examples gaussian kernels ieee transactions pattern analysis machine intelligence vol 
september kx xik xi exp fig 

kernel regression example dotted line true sin dots noisy observations 
solid line estimated function 
circles data points corresponding kernels nonzero weight support points parameter controls kernel width 
considering synthetic example studied true function sin see fig 

compare results rvm variational rvm ran algorithm generations noisy data 
results summarized table includes svm results reported 
applied method known boston housing data set random partitions full data set training samples test samples table shows results versus svm rvm regression reported 
table mean root squared errors mean number kernels sin function boston housing examples fig 

average repetitions test error rates quadratic classifiers versus dimensionality 
tests method performs better rvm svm regression require adjustment sparseness related parameter course results depend choice kernel width rvm svm results 
classification feature selection linear quadratic classifiers linear quadratic higher order classifiers method may seen feature selection criterion embedded learning algorithm 
illustrate consider gaussian classes covariance matrices equal identity means zeros ffiffiffi ffiffiffi dimensions 
optimal bayes error rate independently course optimal classifier data linear uses dimensions data 
trained classifier linear quadratic functions functions see section include components squares pair wise products total number features 
trained standard bayesian plug classifier obtained estimating mean covariance class 
classifiers trained samples class tested independent test sets size class 
fig 
shows resulting average repetitions test set error rate function ofd 
fig 
reports similar experiment linear classifiers learned training sets samples class 
results show proposed method exhibits smaller performance degradation irrelevant features included compared common plug classifier 
kernel classifiers final experiments address kernel classifiers 
regression case gaussian kernels see 
figueiredo adaptive sparseness supervised learning fig 

average repetitions test error rates linear classifiers versus dimensionality 
fig 

classification boundary ripley data 
data points corresponding selected kernels marked squares 
experiment uses ripley synthetic data set class bimodal mixture gaussians optimal error rate problem 
fig 
shows points training set resulting classification boundary learned algorithm 
training samples needed support selected kernels support vectors svm marked small squares 
table shows average test set error final number kernels classifiers learned random subsets size original training samples 
comparison table includes results reported rvm variational rvm svm classifiers 
method comparable rvm clearly better svm specially terms sparseness data set 
allow comparisons chose value 
additional tests known benchmark real data problems pima indians diabetes goal decide subject diabetes measured variables crabs problem consists sex crabs geometric measurements wisconsin breast cancer tast produce benign malignant 
available www stats ox ac uk pub prnn 

available www stats ox ac uk pub prnn 

available www stats ox ac uk pub prnn 

available www ics uci edu mlearn html 
table mean test error number kernels ripley synthetic data set table numbers test errors data sets studied diagnosis set numerical features 
pima case predefined training samples test samples 
crabs problem predefined training samples test samples 
problem total samples results reported obtained averaging random partitions training samples test samples 
prior applying algorithm inputs normalized zero mean unit variance customary kernel methods 
kernel width set pima crabs problems 
table reports numbers errors achieved proposed method state art techniques 
pima crabs data sets algorithm outperforms techniques 
data set method performs nearly best available alternative 
running time learning algorithm implemented matlab second crabs data set seconds pima problems 
note kernel classifiers obtained algorithm kernels sparse pima crabs data sets respectively 
compare support vectors half training set selected svm reported pima data set 
svm learning algorithm extended class problems classifiers class versus 
ways extending binary classifiers multiclass problems focus issue 
test performance ieee transactions pattern analysis machine intelligence vol 
september table test error glass data estimated fold crossvalidation method multiclass problem forensic glass data set class problem features 
classification error rate estimated fold cross validation 
results table show method outperforms best method referred gaussian process gp classifier implemented mcmc 
gp mcmc classifier requires hours computer time learned seconds 
concluding remarks introduced new sparseness inducing prior related laplacian prior 
key feature absence hyperparameters adjusted estimated achieved parameter free jeffreys prior 
experiments publicly available benchmark data sets regression classification shown state art performance 
particular data sets considered approach outperforms support vector machines gaussian process classifiers involves tuning adjusting sparseness controlling hyperparameters 
current form kernel version approach applicable large scale problems large training sets handwritten digit classification considered 
fact due need step solve linear system number training points computational requirements scale third power computational issue topic current interest researchers kernel methods intend focus 
known limitation kernel methods need adjust kernel parameter gaussian kernel width 
course results method strongly depend adequate choice parameters formulation contribute solution problem 
acknowledgments earlier versions 
supported foundation science technology portuguese ministry science technology project posi sri 

available www stats ox ac uk pub prnn 
figueiredo adaptive sparseness jeffreys prior proc 
advances neural information processing systems dietterich becker ghahramani eds 
cambridge mass mit press pp 

figueiredo jain bayesian learning sparse classifiers proc 
ieee computer soc 
conf 
computer vision pattern recognition vol 
pp 

chen haykin different facets regularization theory neural computation vol 
pp 

cristianini shawe taylor support vector machines kernel learning methods 
cambridge cambridge univ press 
neal bayesian learning neural networks 
new york springer verlag 
ripley pattern recognition neural networks 
cambridge cambridge univ press 
vapnik statistical learning theory 
new york john wiley 
figueiredo gaussian radial basis function approximations interpretation extensions learning strategies proc 
int conf 
pattern recognition vol 
pp 

breiman friedman olshen stone classification regression trees 
crc press 
williams prediction gaussian processes linear regression linear prediction learning graphical models jordan ed netherlands kluwer pp 

ridge regression biased estimation nonorthogonal problems technometrics vol 
pp 

williams barber bayesian classification gaussian priors ieee trans 
pattern analysis machine intelligence vol 
pp 
dec 
wahba spline models observational data 
soc 
industrial applied math 
siam philadelphia 
poggio girosi networks approximation learning proc 
ieee vol 
pp 

chen donoho saunders atomic decomposition basis pursuit siam scientific computation vol 
pp 

tibshirani regression shrinkage selection lasso royal statistical soc :10.1.1.35.7574
vol 
pp 

williams bayesian regularization pruning laplace prior neural computation vol 
pp 

lange normal independent distributions applications robust regression computational graphical statistics vol 
pp 

figueiredo nowak wavelet image estimation empirical bayes approach jeffreys noninformative prior ieee trans 
image processing vol 
pp 

berger statistical decision theory bayesian analysis 
new york springer verlag 
mackay bayesian non linear modelling energy prediction competition maximum entropy bayesian methods ed 
kluwer pp 

bishop tipping variational relevance vector machines proc 
th conf 
uncertainty artificial intelligence pp 

tipping relevance vector machine proc 
advances neural information processing systems solla leen 
ller eds 
cambridge mass mit press pp 

donoho johnstone ideal spatial adaptation wavelet shrinkage biometrika vol 
pp 

moulin liu analysis multiresolution image denoising schemes generalized gaussian complexity priors ieee trans 
information theory vol 
pp 

absolute shrinkage equivalent quadratic penalization perspectives neural computing niklasson boden eds 
springer verlag pp 

osborne new approach variable selection squares problems ima numerical analysis vol 
pp 

mccullagh nelder generalized linear models 
london chapman hall 
figueiredo adaptive sparseness supervised learning fahrmeir multivariate statistical modeling generalized linear models 
new york springer verlag 
albert chib bayesian analysis binary response data am 
statistical assoc vol 
pp 

seeger bayesian model selection support vector machines gaussian processes kernel classifiers proc 
advances neural information processing systems solla leen 
ller eds 
cambridge mass mit press pp 

williams seeger nystrom method speedup kernel machines proc 
advances neural information processing systems leen dietterich tresp eds 
cambridge mass mit press pp 

rio figueiredo sm received ee msc phd degrees electrical computer engineering instituto superior cnico engineering school technical university lisbon lisbon portugal respectively 
assistant professor department electrical computer engineering communication theory pattern recognition group institute telecommunications lisbon 
held visiting position department computer science engineering michigan state university east lansing 
scientific interests include image processing analysis computer vision statistical pattern recognition statistical learning 
dr figueiredo received portuguese ibm scientific prize 
associate editor journals pattern recognition letters ieee transactions image processing ieee transactions mobile computing guest forthcoming special issues journals ieee transactions pattern analysis machine intelligence ieee transactions signal processing 
senior member ieee member ieee computer society 
information computing topic please visit digital library computer org publications dlib 
