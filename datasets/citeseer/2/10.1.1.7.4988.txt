rules thumb data engineering jim gray shenoy december technical report ms tr microsoft research advanced technology division microsoft microsoft way redmond wa 
ieee scheduled appear ieee international conference data engineering san diego april 
personal material permitted 
permission reprint republish material advertising promotional purposes creating new collective works resale redistribution servers lists reuse copyrighted component works obtained ieee rules thumb data engineering jim gray shenoy microsoft research mass amherst 
gray microsoft com shenoy cs umass edu rules thumb design data storage systems 
briefly looks storage processing networking costs ratios trends particular focus performance price performance 
amdahl ratio laws system design need slight revision years major change increased ram 
analysis indicates storage cache database web data save disk bandwidth network bandwidth people time 
surprisingly minute rule disk caching cache rule web caching 

engineer data intuition rules thumb 
rules folklore 
rapid changes technology rules need constantly reevaluated 
article attempt document main rules engineering database systems 
design article assesses technology trends predicts sizes systems 

storage performance price rules thumb consequence moore law posits circuit densities increase years 
means memories get times larger years decade 
means memory data grows rate creating need extra bit addressing months 
comfortable bit address spaces rare find machine memory 
years need extra address bits address gb memories bit addresses larger computers market 
today computer architectures give bit logical addressing mips alpha powerpc sparc itanium bit addressing 
physical addressing bits bits growing bit months 
rate take decades exceed bit addressing 
moore law originally applied random access memory ram 
generalized apply microprocessors disk storage capacity 
disk capacity improving leaps bounds improved fold decade 
magnetic aerial density gone megabits square inch late 
disks spin times faster times smaller years ago data rate improved fold see 
today disks store gb access times milliseconds kilobyte accesses second transfer rates mbps maps megabyte accesses second scan time minutes 
disks cost approximately tb today tb lower performance ide drives packaged powered network served 
years form factor storing nearly terabyte support transfer rate mbps 
rate take nearly hours scan disk 
prices nearing tb 
tpi mbps magnetic disk parameters vs time year disk capacity improved fold years consistent moore law transfer rate mbps improved time 
metrics tracks inch tpi thousands bits linear inch track megabits second media spins mbps square inch appeared proceedings ieee international conference data engineering feb san diego ca 
copyright institute electrical electronics engineers personal material including hard copy reproduction permitted 
permission reprint republish distribute material part commercial purposes obtained ieee 
information obtaining permission send mail message intellectual property rights administrator 
ratio disk capacity disk accesses second increasing decade 
capacity bandwidth ratio increasing decade 
changes implications disk accesses precious disk data time 
reduce disk accesses large transfers small ones favoring sequential transfers mirroring raid 
elaborate points 
reduce disk accesses caching popular hot pages main memory writing log changes disk 
reduces random reads converts random writes sequential log writes 
periodically written data needs checkpointed disk minimize redo restart checkpoint done background piggybacking ios sorted nearly sequential 
important optimizations database systems today 
decade disk pages grown kb kb poised grow 
years typical small transfer unit probably kb large transfer units megabyte 
random access costs seek time half rotation time transfer time 
transfer sequential seek time transfer entire track rotation time 
track sized sequential transfers maximize disk bandwidth arm utilization 
move sequential io underway 
mentioned caching transaction logging log structured file systems convert random writes sequential writes 
large benefits database systems operating systems 
techniques continue yield benefits disk accesses precious 
argument favor mirrors double read bandwidth data item cost extra access write 
raid uses disk accesses write improves read bandwidth data requests go different disks 
years ago disks offered kilobyte accesses second gb data minute disk scan times 
current disks offer gb data minute scan times 
mb vs mb 
modern disk data needs data years ago 
fact hot data migrated ram disk cost mb era times ram costs today 
disk data afford live ram today 
large main memories way cool data disk 
way store data multiple times spread reads copies suggesting mirroring 
great progress tape storage tapes store gb 
drive tape costs stores gb 
drives provide mbps data rates scan time days 
tape archives deliver approximately zero maps typical 
tape archive half cost terabyte disk storage tape provide easy access data cost random tape access times higher second disk vs accesses second tape 
years situation dramatic compelling 
tape capacities expected improve faster tape speed access time expected stay making access problem dramatic days scan archive 
historically tape disk ram maintained price ratios 
disk storage expensive tape ram expensive disk 
today buy gb tape gb disk dell scsi expensive gb memory 
ratios translate gb gb gb giving ratio storage 
offline tapes put tape robot price tape rises tb packaged disks tb 
brings ratios back 
fair say storage cost ratios 
cost mb ram declines time decade 
disk ram price ratio price decline suggests economical put disk today economical put ram years 
striking thing storage cost calculations disk prices approaching tape prices 
raid mirroring parity administrators sacrifice disk storage capacity protect disk media failures 
administrators discovering may able backup terabyte tape takes long time restore terabyte 
see stores looming horizon administrators moving strategies maintain multiple disk versions online restore database tape 
increasingly sites need online time replicating entire state remote site online copies data 
site fails offers access data failed site recover data stored second site 
essence disks replacing tapes backup devices 
tapes continue data interchange law holds see someday data interchange go internet net means tape frequently data interchange 
storage prices dropped low storage management costs exceed storage costs similarly pc management costs exceed cost hardware 
rule thumb needed data administrator gb storage 
time gb disk cost dollars sense optimizing monitoring disk space 
today dollars buy tb tb disk storage shop carefully 
today rule thumb person manage tb tb storage tb typical 
storage management tools struggling keep growth storage 
designing decade need build systems allow person manage pb store 
summarizing storage rules thumb moore law things get better years 
need extra bit addressing months 
storage capacities increasing decade 
storage device throughput increasing decade 
disk data decade 
disk page sizes increase decade 
ram storage cost ratios approximately 

years dram cost disk costs today 

person administer dollars disk storage tb storage today observations disks replacing tapes backup devices 
mirroring parity save disk arms 

amdahl system balance rules gene amdahl famous rules thumb 
data engineering famous ones 
amdahl parallelism law computation serial part parallel component maximum speedup 
amdahl balanced system law system needs bit io second instruction second mips mbps 

amdahl memory law mb mips ratio called alpha 
balanced system 

amdahl io law programs io instructions 
amdahl law changed years 
parallelism law algebra remains true relevant day 
thing surprising year old laws survived speeds sizes grown orders magnitude ratios changed factors 
re evaluate amdahl io laws look transaction processing performance council benchmark systems 
systems carefully tuned appropriate hardware benchmark 
example oltp systems tend small disks benchmarks arm limited tend appropriate number controllers 
paragraphs evaluate amdahl balanced system law concluding current technology amended say 
amdahl revised balanced system law system needs mips instruction rate io rate measured relevant workload 
sequential workloads tend low cpi clocks instruction random workloads tend higher cpi 

alpha mb mips ratio rising 
trend continue 

random io happen instructions 
rule sequential ios larger instructions io higher sequential workloads 
amdahl balanced system law complex interpret new world quad issue pipelined processors 
table summarizes analysis 
theory current mhz intel processors able execute instructions second amdahl io law suggests mhz processor needs mbps disk bandwidth numbers rounded 
real benchmarks processors demonstrate clocks instruction sequential workloads tpc clocks instruction random io workloads tpc 
larger translate mips sequential mips random workloads 
turn amdahl law says processors need mbps sequential io bandwidth mbps random io bandwidth cpu respectively 
benchmark hp mhz processors disks 
translates disks cpu mbps raw disk bandwidth cpu mbps controller bandwidth cpu consistent amdahl prediction mbps 
amdahl law predicts system needs mbps io bandwidth 
kb pages disk implies disks processor number comparable disks dell 
tpc results mentioned approximately gigabyte ram processor 
mips column table approximately mb mips 
intel ia processors limited gb memory 
considers hp ibm sun systems gb limit gb cpu gb cpu gb 
roughly translates mb mips mb mips 
argued people working main memory databases example disk ios precious moving relatively larger main memories 
alpha mb mips ratio rising 
execution interval 
instructions executed io 
essence instructions byte instructions io ios kb 
dichotomy sequential random workloads tpc benchmarks lot random io instructions kb ios sequential workloads instructions kb ios 
summary amdahl laws rules thumb sizing io memory systems 
major changes mips rate measured assuming cpi sequential ios larger random ios instructions io higher sequential workloads alpha mb mips ratio rising trend continue 
changes speeds capacities striking amdahl ratios continue hold 
interestingly hsu smith young came similar detailed study tpc workload behaviors 
detailed study shows wide spectrum behaviors workloads workload 

networking law george predicted network bandwidth triple year years 
far prediction approximately correct 
individual wavelength channels run gbps 
wave division multiplexing gives channels fiber 
multi links operating laboratory 
companies deploying thousands miles fiber optic networks 
verge having high speed gbps wide area networks 
telecom competition works links inexpensive 

law deployed bandwidth triples year 

link bandwidth improves years 
paradoxically fastest link microsoft campus today gbps wan link pacific northwest 
takes gbps ethernet links saturate wan link 
lan speeds rise gbps gbps switched point topoint networking 
latency due speed light ms round trip north america europe asia forever second bandwidth allow design systems cache data locally quickly access remote data needed 
traditionally high speed networking limited software overheads 
cost sending message time bytes bandwidth sender receiver cpu costs typically instructions instructions byte 
send kb cost instructions millisecond cpu time 
transmit time bytes mbps ethernet millisecond lan cpu limited transmit time limited 
rule thumb traditional message systems 
network message costs instructions instructions byte 

disk io costs instructions instructions byte 
disk ios efficient compared network io 
disk ios just messages disk controller 
substantial strides understanding simple question 
networking community offloaded protocol nics ide ata memory aggressively buffer requests correct errors 
checksumming fragmentation assembly dma table amdahl balanced system law parameters tpc benchmarks www tpc org 
cpi varies workloads io sizes vary instructions byte similar amd ahl prediction instructions byte bit io instruction 
mhz cpu cpi mips kb io io disk disks disks cpu mb cpu ins io byte amdahl tpc random tpc sequential added high speed nics 
gone banner system area networking san virtual interface architecture 
current revision rule thumb 
cpu cost san network message clocks clock byte 
particular possible rpc microseconds move gbps node node processor half busy doing network tcp ip tasks 
network carries packets second clocks bytes second clocks mhz machine clocks spare useful 
currently costs dollar send mb wan see table odlyzko local disk lan access times expensive 
price gap decline decade 
suggested subsequent sections bandwidth sufficient inexpensive local disks act cache commonly data buffer pre fetched data 

caching location location location processor clock speeds improving parallelism processor 
modern processors capable issuing instructions parallel pipelining instruction execution 
theory current quad issue intel processors able execute instructions second instructions clock clocks second 
practice real benchmarks see cpi clocks instruction 
cpi rising processor speeds memory latency improvements 
memory subsystem feed data processor fast keep pipelines full 
architects added level level caches processors order improve situation programs data locality architects mask cache misses 
software designers learning careful program data placement cache sensitive algorithms locality give speedups current processors 
processor speeds continue memory speeds increasing incentives software designers look algorithms small instruction cache footprints predictable branching behavior predictable data locality clustered sequential access 
hardware trend design huge way multiprocessors operate shared memory 
systems especially prone instruction stretch bus cache interference processors causes processor slow 
getting performance massive smps require careful attention data partitioning data locality processor affinity 
alternative design opts nodes io bus bandwidth dataflow programming model communicating high speed network 
designs rise impressive performance example sort speed computer systems doubling year years combination increased node speed year parallelism year 
terabyte sort nearly processors disks research microsoft com gray sort benchmark 
argument little scalable design tries leverage fact mainframe mini commodity price ratios approximate 
mainframes cost times commodity components semi custom mini computers markup commodity components see prices comparable systems www tpc org benchmarks 
cluster advocates admit little design efficient argue cost effective 
general rule thumb bigger better locality better 
rules evolved disk data locality caching 
possible quantitatively estimate cache disk page memory trading memory consumption disk arm utilization 
mentioned disk arms precious 
disk costs accesses second disk access second costs 
advantageous spend save access second 
buys mb dram cache size save access second investment 
ask question frequently disk resident object accessed justify caching main memory 
rent ram space balance cost access 
analysis shows seconds randomly accessed data term call technology ratio approximately second term called economic ratio varies today 
breakeven interval minutes minutes randomly accessed pages 
sequentially accessed data technology ratio approximately mb pages pages second break interval seconds 
analysis gives rules 
minute random rule cache randomly accessed disk pages re minutes 

minute sequential rule cache sequentially accessed disk pages re minute 
time constants rising slowly technology evolves 
related rule seen spend byte ram save mips 
argument goes ram costs mb today get extra mips intel extra dollars approximately 
marginal cost instruction second approximately marginal cost byte 
fifteen years ago ratio intel vlsi processors expensive 

spend byte ram save ips 
consider web page caching 
logic similar minute rule decide pays cache web pages 
basic diagram shown link speed varies kbps intranets modem speeds kbps wireless speeds kbps 
case modem wireless links assume local browser cache 
high speed links cache browser cache proxy cache 
case proxy assume fast connection user cache mb lan time cost accessing data remote proxy disk significantly larger local disk 
assumptions consider questions web caching improve response times 
web page cached 
large web cache 
assume average web object kb 
define remote response time access object server 
local response time access object cache 
cache hit ratio fraction requests cache satisfies response time improvement remote local remote remote local estimate remote local 
remote consists server response time download network time 
server response time queuing delay service time range milliseconds seconds 
assume response time seconds 
download time network depends network conditions link speeds 
wan links typically shared user bandwidth smaller typical link bandwidth link server may reduce bandwidth request 
assume effective lan wan bandwidth kb time transmit kb object tenth second remote seconds dominated server time 
modem bandwidth available dial link kbps 
compression effective bandwidth twice start overheads 
assume effective modem bandwidth kb modem transmit time kb object seconds remote seconds 
mobile user wireless link gets kb takes seconds download kb object remote seconds 
ignore fact mobile systems compress data objects smaller 
summarizing remote high speed connection modem connection wireless connection local depends details fundamentally local access avoids server time wait seconds examples object browser cache local access avoids transmission time 
local access saves local milliseconds 
local ms browser cache ms proxy cache intranet proxy cache modem proxy cache wireless proxy cache studies indicate proxy cache upper bound 
anecdotal evidence suggests browser hit ratios smaller assume 
browser cache 
assuming hr human cost second costs cents 
table client cache server link client cache server link 
client side proxy web cache improves response time eliminate link transmission times server times 
computes response time savings response time improvement equation left 
user requests hour uses web hours year benefit caching cents hour cents hour 
hypothetical user savings year year 
balanced cost disk store pages mentioned earlier buy lot disk space 
hypothetical user accessing kb pages mb 
dollar worth disk space 
having computed savings cached page table compute point caching page begins pay 
table calculation 
column table estimates download costs odlyzko table assumes wireless kbps link costs minute hr 
second column assumes desktop disks cost gb years mobile storage devices expensive 
break cost storing page happens storage rent matches download cost 
download cost components network time table people time fourth column table shows calculation ignoring people time case break interval year decades 
people time included interval rises decades 
case table indicates caching attractive cache page referenced years longer lifetime system 
certainly assumptions questionable astonishing thing wide spectrum assumptions concludes cache strategy desirable 
table change time 
network speeds predicted increase network costs predicted drop 
column time may drop months day 
column time grow people time grows value cost technology decline 
summary technology trends suggest web page caching continue popular especially bandwidth limited mobile devices 
cost cache web accesses year 
users requests hour hit ratio cache gets hits new objects user hour 
hour kb user day 
kb user day 
cases penny day 
conclude simple cache strategy default 
calculations suggest simple rule 
cache web pages chance re referenced lifetime 
web object lifetimes bi modal tri modal cases 
studies show median lifetimes days tens days 
average page day lifetime ignoring modalities non uniform access 
heuristic recognized high velocity pages improve usability showing stale cached pages save cache storage 
major assumption calculations server performance continue poor seconds average 
popular servers tend slow web site owners investing servers bandwidth 
declining costs web site owners invest reduce second response time second 
happens web cache people cost savings evaporate need caching purely save network bandwidth download time believe scarce resource mobile devices 

summary data stores huge 
biggest challenge easy access manage 
automating tasks data organization accesses protection 
table shows benefits browser proxy caching pennies saved assuming people time worth hr 
connection cache remote seconds local seconds hit rate people savings page lan proxy lan browser modem proxy modem browser mobile proxy mobile browser table caching deal cache web pages re years 
kb download network cost kb storage mo time break cache storage time people cost download table time break internet lan months years modem months years wireless months years disk technology overtaking tapes time disks morphing tape devices primarily sequential access optimize disk arms 
ram improvements encourage build machines massive main memory 
main change amdahl balanced system law alpha mips dram size rising 
network bandwidth improving rate challenges design assumptions 
lan san software streamlined longer bottleneck 
may allow re centralization computing 
data caching important optimization 
disk caching follows minute random rule minute sequential rule 
web caching encourages designs simply cache pages 

ibm www storage ibm com ultra htm 
brewster kahle private communication archive org data heat number times data accessed second 
dell www tpc org results individual results dell dell es pdf hp www tpc org new result hresult 
idc id hennessy patterson computer architecture quantitative approach 
morgan kaufman san francisco isbn keeton patterson raphael baker performance characterization quad pentium pro smp oltp workloads acm isca 
june 
ailamaki dewitt hill wood 
dbmss modern processor time go 
vldb pp 
sept 
garcia molina park rogers performance memory 
acm sigmetrics performance evaluation review may 
pp 

hsu smith young behavior production database workloads tpc benchmarks analysis logical level 
tr ucc csd uc berkeley nov 
gray cost messages acm podc virtual interface architecture www org fiber keeps promise get ready 
bandwidth triple year 
forbes april 
www forbes com asap htm odlyzko economics internet utility utilization pricing quality service www research att com amo doc networks html arpaci dusseau anderson culler hellerstein patterson rivers 
cluster river making fast case common 

gray graefe minute rule years sigmod record tewari dahlin vin kay hierarchies design considerations distributed caching internet ieee icdcs june 
wolman voelker sharma cardwell karlin levy scale performance cooperative web proxy caching acm sosp pp dec 
gwertzman seltzer world wide web cache consistency usenix annual technical conference jan 
kelley reeves optimal web cache sizing scalable methods exact solution feb appear th int 
conf web caching content delivery workshop may lisbon portugal 
ai eecs umich edu papers pdf 
