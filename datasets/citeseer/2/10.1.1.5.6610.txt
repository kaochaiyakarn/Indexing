coarse grain parallel programming jade presents jade language allows programmer easily express dynamic coarse grain parallelism 
starting sequential program programmer augments sections code parallelized data usage information 
compiler run time system information concurrently execute program respecting program data dependence constraints 
jade significantly reduce time effort required develop maintain parallel version imperative application serial semantics 
introduces basic principles language compares jade existing languages presents performance sparse matrix cholesky factorization algorithm implemented jade 
goal research provide programming language support exploiting coarse grain concurrency concurrency unit serial computation executes instructions 
major reasons automatic techniques extract static parallelism sequential programs fully exploit available coarse grain concurrency 
programmer high level knowledge necessary decompose program coarse grain tasks 
information lost program encoded conventional programming language 
second important exploit irregular data dependent concurrency available program runs 
large grain size possible profitably amortize dynamic overhead required exploit source concurrency 
research supported part darpa contract 
monica lam martin rinard computer systems laboratory stanford university ca attempts provide programming language support coarse grain concurrency 
proposed languages support explicitly parallel programming paradigm 
programmer directly manage concurrency constructs create synchronize parallel tasks 
management burden complicates programming process making parallel programming timeconsuming error prone activity programming conventional sequential language 
introduces new programming language called jade supports coarse grain concurrency sequential imperative programming paradigm 
jade programmers augment sequential program high level dynamic data usage information 
jade implementation uses information determine operations executed concurrently violating program sequential semantics 
compiler information extract statically available concurrency jade run time system capable analyzing data usage information extracting dynamically available concurrency 
jade implementation responsible managing parallel activity physical hardware machine dependent optimizations provided tailoring implementation different architectures 
jade simplifies programming preserving familiar sequential imperative model programming enhances portability providing machine specific optimizations jade implementation 
power away low level details critical language designed support concurrency jade allows programmer express data usage information level abstraction appropriate granularity parallelism 
programmer groups units serial execution tasks structures data shared multiple tasks shared data objects 
programmer express task side effects terms high level side effect specification operations shared data objects 
design objects determines parallelization synchronization granularity 
organization follows 
introduce basic programming paradigm illustrating programmer attaches simple data usage infor mation program tasks jade implementation uses information run program parallel 
section show providing detailed information task data usage jade programmer achieve sophisticated concurrency patterns 
section describes programmers build shared data objects high level side effect specification operations 
complete jade programming example close discussion comparison languages designed express coarse grain concurrency 
basic programming paradigm jade programmer provides program knowledge required efficient parallelization implementation combines machine knowledge information map computation efficiently underlying hardware 
jade programmer responsibilities task decomposition programmer starts serial program uses jade constructs identify program task decomposition 
side effect specification programmer provides dynamically determined specification side effects task performs shared data objects accesses 
jade implementation performs activities constraint extraction implementation uses program serial execution order tasks side effect specifications extract dynamic inter task dependence constraints parallel execution obey 
synchronized parallel execution implementation maps tasks efficiently hardware enforcing extracted dependence constraints 
programmer expresses program task decomposition side effect specifications extensions existing sequential languages 
jade extensions include data type define shared data objects additional language constructs 
extensions implemented fortran 
shared data objects data accessed multiple tasks identified shared data objects 
programmers declare tasks side effects applying side effect specification operations shared data objects 
example rd read operation specifies task read object wr write operation specifies task write object rw read write operation specifies task read write object 
programmer responsibility ensure declared side effect specification operations correspond way task accesses data 
jade implementation provides common shared data types create shared data objects section describes jade programmers define shared data types 
executing program parallel jade implementation preserves program semantics maintaining serial execution order tasks conflicting side effect specifications 
example tasks write shared data object conflicting side effects execute sequentially program serial execution order 
jade implementation preserve serial execution order task writes shared data object task reads object 
course tasks accessing disjoint sets objects reading object execute concurrently 
illustrate basic jade programming paradigm presenting pronounced construct 
jade programmers construct declare piece code execute specified set side effects shared data objects 
side effect specification parameters task body task body operationally jade implementation creates task executes construct task body section contains serial code executed task runs 
task executes may certain variables enclosing environment parameterize behavior 
programmer gives list variables parameters section 
task creation time jade implementation preserves values variables copying task context 
programmer uses side effect specification section declare side effects task perform shared data objects 
specification arbitrary piece code containing side effect specification operations shared objects 
conceptually jade implementation determines task side effects task creation time executing specification 
specification contain control flow constructs conditionals loops function calls programmer may information available run time declaring task side effects 
jade program concurrency pattern completely orthogonal procedural decomposition 
simple example illustrates requirement concurrently executable tasks come procedure invocation 
double int index int index double int modify 
example repeatedly applies elements accessed indirectly index array index 
assume modifies side effect 
invocations modifying different elements array execute concurrently conversely invocations modifying element obey code original sequential order 
programmer code parallel version example jade follows int index int index int rw int modify 
programmer identifies invocation separate task 
converts objects modifies shared data objects case shared doubles uses rw operation declare task read write parameter 
describe program operational interpretation 
program executes loop sequentially jade implementation creates new task invocation copying address array new task context 
implementation analyzes task side effect specification infers new task execute tasks preceding iterations include side effect specification finished 
example illustrates simple jade programs execute processor runs program serially periodically creating tasks statements processors pick execute 
jade implementation uses serial task creation order determine relative execution order tasks conflicting side effects 
simple model parallel computation synchronization takes place task boundaries 
task run acquires shared data objects access releases acquired objects termination 
possible express concurrency patterns parallel applications just parallel applications complex concurrency patterns requiring periodic inter task synchronization 
section jade constructs allow programmers express complex synchronization patterns 
decoupling parallelism synchronization restricted form synchronization supports unnecessarily serialize computation cases task access shared data object occurs long task starts running task access shared data object occurs long task terminates 
procedure provides concrete example forms unnecessary serialization 
wr rd rw double wr procedure generates tasks 
tasks execute sequentially preserve serial semantics 
unnecessary serialization comes fact second task access finishes statement 
statement task able execute concurrently statement second task 
second unnecessary serialization comes fact second task accesses statement finishes 
statement second task able execute concurrently statement third task 
sections show eliminate sources unnecessary serialization 
way achieve full concurrency break second task tasks 
solution inferior modification motivated examining code second task 
solution requires shared object 
need manage new serial tasks may cause extra overhead 
solution bypasses problems allowing tasks synchronize execute 
analyzing construct observe simultaneously specifies kinds side effect information positive side effect information negative side effect information 
construct specifies positive side effect information stating task body carry declared side effects 
task execute perform side effects violating program serial semantics 
specifies negative side effect information stating task body side effects declared side effects 
task run concurrently piece code long side effects conflict 
operationally positive side effect information causes synchronization negative side effect information creates opportunities concurrency 
providing construct specifies positive side effect information construct specifies negative side effect information allow programmer create tasks incrementally acquire shared data objects accessed 
general forms constructs side effect specification code body side effect specification parameters task body task body construct specifies side effects shared data objects code body immediately perform 
code body access shared data objects side effect specification section proceed say demands objects 
provides positive side effect information code body may declare additional side effects nested jade constructs 
operationally jade implementation suspends execution statement previously created tasks dependence conflict statement completed 
constructs create synchronization concurrency 
construct specifies code side effects declared side effects 
performing side effects shared data task body declare side effects nested constructs 
task body immediately access shared data objects side effect specification section say claims objects 
operationally jade implementation executes statement creates immediately executable task containing code continues execute code construct 
subsequently created task need wait finish side effect specification conflicts 
returning example programmer second task demand accessed task 
wr rd rw double rd rw wr statements execute concurrently 
second third tasks execute sequentially second task hold claim read terminates 
eliminate source unnecessary serialization programmer able specify task completed declared side effect longer needs access corresponding shared data object 
jade provides negative side effect specification construct just purpose 
general form construct 
side effect specification construct dynamically enclosed task specifies task body remaining computation perform side effects side effect specification 
programmers reduce enclosing task specified set side effects 
reduction may eliminate conflicts enclosing task tasks occurring sequential execution order 
tasks may able execute soon executes 
absence tasks wait enclosing task terminated 
example programmer allow statements second task third task execute concurrently 
wr rd rw double rd rd rw wr hierarchical concurrency way delay task demands shared data objects maintaining underlying serial execution order side effects objects 
express hierarchically structured concurrency patterns 
enclosing subtasks constructs programmer easily create execute synchronize multiple parallel computations simultaneously 
consider example new new brd brd encloses group subtasks produce entire procedure run concurrently parts program need value example illustrates task need specify side effects just externally visible ones 
need claim visible outside task body 
general rules define legal claims demands access shared data object dynamically enclosed construct declares access declare task body accesses externally visible shared data objects 
applying data abstraction synchronization jade tasks synchronize pieces data access 
preceding examples tasks accessed synchronized fine grain objects doubles 
coarse grain tasks access coarse pieces data 
example parallel matrix algorithms access matrix rows columns 
unit synchronization match granularity data access shared data objects tasks support synchronization coarse pieces data tasks access 
jade programmers build shared data objects synchronization type called tokens 
token functions synchronization data abstraction carrying dependence conceptual unit data 
class notation abstraction apparent 
fortran syntactic sugar bundle tokens data basic programming ideas 
token side effect specification operations rd wr rw specifying respectively read side effect write side effect read write side effect 
simplest case programmer augments data type token side effect specification operations example class public double private token token public void rd token rd void wr token wr void rw token rw example demonstrates jade programmers tokens build complex shared data objects 
sparse matrix data structure stores compressed columns linear array 
index array gives column starting location linear array array gives element row index 
computation synchronizes matrix columns programmer simply associates token column 
class sparse public double data max elements int max elements int max columns int class public sparse private token token max columns public void col rd int col token col rd void col wr int col token col wr void col rw int col token col rw void col cm int col token col cm int rw int modify column 
token carries dependence column sparse matrix consists arbitrary number contiguous data elements 
general shared data object synchronization granularity need correspond syntactic data declaration unit 
token side effect operations primitives jade programmer define object side effect specification operations match way program uses data 
example routine accesses sparse matrix columns matches type side effect specification interface 
expressing side effects terms tokens clarify dependence structure 
example clarified structure possible compiler discover static independence loop iterations 
described jade enforces serial semantics maintaining sequential order writes accesses data 
possible relax execution order exploiting higher level semantics user defined operations 
consider histogram example 
addition commutative associative histogram increments commute 
implementation need enforce individual read write dependence constraints long increments execute mutual exclusion 
example commuting mutually exclusive updates see sparse cholesky factorization algorithm section 
programs contain commuting updates jade tokens support commutative update cm side effect specification operation addition basic rd wr rw operations 
view synchronization smoothly generalizes individual memory locations read write operations data types associated side effect specification operations synchronization rules 
shared data object concept provides effective synchronization framework concurrent object oriented programming 
programming example current jade implementation consists run time system preprocessor translates jade code fortran code containing calls run time system 
implementation runs encore multimax silicon graphics iris 
implemented applications include sparse cholesky factorization algorithm due rothberg gupta perfect club benchmark mdg vlsi routing system due rose parallel program cyclic reduction column oriented matrix algorithm 
illustrate jade realistic example show rothberg gupta sparse cholesky factorization algorithm implemented jade 
factorization algorithm supernodes groups adjacent columns identical nonzero structure 
example supernodes matrix 
serial computation processes supernodes left right 
supernode generates internal sparse matrix task graph factor supernodes super matrix left right super super int super columns col super rw col rw super int super super columns col super updates rd super cm col int super int col super col jade sparse cholesky factorization class definition section 
side effects supernode specified column accesses internal update supernode accesses update 
interface internal update claims data column supernode granularities 
compare jade version rothberg gupta parallel version implemented anl macro package 
internal external update task rothberg gupta program 
program explicitly spawns thread processor 
external updates statically partitioned threads internal updates managed task queue 
actual factorization begins program number external updates supernode 
time external update completes code decrements external column supernode count checks zero 
code explicitly enqueues supernode internal update task queue 
threads notified internal update completed 
code highly optimized minimal run time overhead 
speedup sequential anl macros jade encore multimax sparse cholesky factorization number processors jade anl macro package speedups compare performance versions algorithm 
performance measured speedup factor relative extremely efficient serial sparse cholesky factorization algorithm 
speedup figures factorization phase computation matrix bcsstk module offshore platform boeing sparse matrix collection 
factored matrix columns nonzeros supernodes generated jade tasks average floating point operations task 
performance numbers collected encore multimax jade anl macro package implementations 
observe performance optimized anl program running single processor comparable sequential program indicating anl program low overhead 
hand jade general run time system higher overhead 
fortunately jade program scales reasonably processor implementation running times faster uniprocessor jade program 
currently working optimizations improve performance jade run time system 
discussion comparison jade designed support parallel execution computations expressible sequential program 
successfully parallelize program jade programmer ensure inherent concurrency keep target machine busy 
cases programmer need global variables eliminate unnecessary sequencing constraints caused data reuse 
cases programmer may need different algorithms inherent concurrency 
jade designed machines single address space large scale dash multiprocessor development stanford 
machines long latency associated remote data accesses important reuse cached data possible 
current jade implementation identifies tasks access tokens schedules tasks processor 
tasks able reuse data brought cache previously executed task 
current implementation requires underlying shared address space 
jade run machine separate address spaces implementation generate communication required transfer shared data processors 
current language explicitly associate tokens data dependence carry 
jade implementation generate communication know actual pieces data task touch 
plan extend language tokens explicitly associated data represent 
association possible implement jade machines separate address spaces 
jade design principles set apart programming languages 
principle jade provides implicit concurrency synchronization relaxing sequential program execution order 
jade implementation enforces data dependence constraints programmer preserve structure semantics serial program parallel version 
second principle jade supports data abstraction jade programmers specify side effect information high level operations shared data objects 
section examine ramifications principle comparing jade explicitly parallel programming languages 
compare jade languages designed express concurrency available serial programs 
explicit concurrency synchronization major issue parallel programming language design question correctly synchronize tasks 
section compare jade approaches provide constructs create explicitly synchronize parallel tasks 
task queue model common way synchronize coarse grain parallel computation threads package thread creation low level synchronization primitives implement explicit task queue 
programmer breaks program set tasks task enabled put task queue predecessors dependence graph terminated 
free processors grab run enabled tasks 
programmer enforce inter task data dependence constraints inserting synchronization primitives tasks touch data 
direct management code distributed program text encoding global synchronization pattern terms provided low level synchronization primitives 
synchronization code creates new explicit connections parts program access data making program harder create modify 
program concurrency pattern changes programmer go program modifying distributed pieces synchronization code 
programmers jade high level interface task queue model computation 
jade programmers provide local data usage information jade implementation uses extract implement global task dependence graph 
jade programmers manage synchronization add new explicit connections pieces code 
jade programs easier modify maintain corresponding task queue versions 
major advantage direct task queue implementation efficiency programmer control machine fairly low level special purpose synchronization strategies tailored application hand 
jade general purpose synchronization strategy may efficient difference negligible computations large grain size 
explicit communication operations proposed parallel programming languages provide explicit communication operations move data parallel tasks 
programmers insert operations tasks data production consumption points synchronize computation 
example languages csp ada occam provide synchronous message passing operations 
major problem approach producers consumers agree order relative time data transfer 
linda supports tightly coupled programming style providing global tuple space asynchronous operations insert read remove data 
tuple spaces support mutual exclusion asynchronous producer consumer synchronization presence absence data 
tuple spaces support frequently synchronization mechanisms counting semaphores 
mechanisms easily synchronize restricted dependence patterns support synchronization patterns required enforce general dependence constraints 
task queue model programmers implementing applications general dependence constraints directly encode program global synchronization pattern provided synchronization mechanisms low level primitives 
example linda sparse cholesky factorization application directly implements task graph synchronization pattern counting semaphores 
global control languages approach control language directly express application global concurrency pattern 
reasons efficiency programming convenience actual pieces code control language invokes carry computation written serial imperative language fortran brief list approaches 
schedule allows programmers give system set tasks explicit specification task dependence graph 
schedule executes tasks obeying dependence constraints 
programmers coarse grain dataflow languages express concurrency synchronization dataflow graphs 
execution dataflow graph provides synchronized concurrency 
strand programmer expresses program global concurrency structure committed choice concurrent logic programming paradigm 
suspension unbound logic variables provides synchronization simultaneous goal satisfaction provides concurrency 
languages centralize synchronization distributing program programmer directly implement program global synchronization structure 
approaches burden programmer additional programming paradigm force programmer alternative paradigm lowest level granularity 
function method modifiers languages augment semantics function invocation provide concurrency synchronization 
example multilisp futures enforce producer consumer sequence constraint function creating data caller consuming return value 
mechanism works synchronizing returns asynchronously invoked functions methods concurrent object oriented languages cool provide synchronization mechanism 
futures designed synchronize multiple updates mutable shared data central feature object oriented programming 
concurrent object oriented languages programmer specify method mutually exclusive access receiver run 
possible parallelize cool application adding mutual exclusion modifiers sequential program 
implement applications general dependence constraints programmers resort futures mutual exclusion low level concurrency synchronization primitives 
example cool sparse cholesky factorization algorithm synchronized counting completed column updates 
jade languages programmers directly manage program global concurrency structure implement applications general dependence constraints 
jade programmer simply express parallel applications implicitly parallel paradigm 
structure semantics original sequential program preserved parallel version 
jade programmer need provide local data usage information jade implementation responsible directly managing program global concurrency structure 
jade programs synchronization data flow tasks occurring earlier sequential execution order tasks occurring 
unidirectional flow allows jade implementation suppress spawning face excess concurrency risking deadlock 
means jade express parallel algorithms requiring bidirectional task communication 
jade design sacrifices generality order fully support sequential imperative programming paradigm 
absence static optimizations hierarchically structured concurrency jade implementation creates tasks sequentially 
serial task creation may cause significant performance loss grain size small 
drive minimum grain size jade applicable currently investigating static analysis detect simple common parallel structures substitute general parallelization synchronization approaches specialized solutions 
parallelizing serial programs compare jade approaches designed parallelize programs written sequential programming languages 
control concurrency direct way endow sequential language synchronized concurrency augment language explicit control constructs fork join par par doall statements 
constructs allow programmer spawn independent processes program blocks spawned processes terminate 
programmer responsibility ensure race conditions serial parallel semantics identical 
major drawbacks control oriented approach 
parallel constructs express irregular task dependence patterns common parallel idioms requiring periodic intertask synchronization 
second force programmer destroy program structure moving concurrently executable pieces code artificial spawn point 
consequently program may harder understand need code transformations may discourage programmer exploiting possible sources parallelism program 
jade hand allows programmers preserve structure original program parallel version 
jade programmers need move concurrently executable pieces code common invocation point jade constructs easy natural exploit synchronized concurrency module procedure boundaries 
jade dynamic side effect specification capabilities support creation irregular data dependent concurrency 
data usage concurrency languages jade allow programmer express concurrency side effect specification constructs 
fx memory locations partitioned finite statically determined set regions 
programmer declares regions memory function touches part function type 
fx implementation uses static type checking algorithm verify correspondence procedure declared actual side effects 
implementation information execute parts program conflicting side effects concurrently 
finite set regions determined compiletime enables fx type checker verify correctness specification severely limits scope supported concurrent behavior 
means dynamically created variables mapped static region reduces opportunities concurrency 
importantly aggregate array single region 
side effect specification imprecision dramatically reduces amount expressible concurrency especially programs main source concurrency tasks access disjoint regions array 
refined disjoint statement allows programmer create set access restricted aliases break array disjoint pieces 
programmer refer data aliases indicate lack dependence accesses different aliases 
compiler analysis may disambiguate alias names 
disjoint statements dynamic different views adopted different times computation 
jade jade differs fx refined unique support abstraction 
programmer need describe side effects entire tasks individual functions 
simplifies programming gives system valuable information suitable decomposition computation parallel tasks 
second side effect specification terms user defined functions user defined objects 
jade implementation sees computation performed conceptual level abstraction programmer 
furthermore enforcing individual read write ordering individual memory locations higher semantic knowledge relax sequential execution order commutative updates common example 
usage information form static dynamic forms parallelism detected exploited 
fx refined programmer implementation responsible ensuring task side effect specification correctly summarizes actual side effects 
presence explicit association tokens data represent jade statically check correctness possible necessary insert dynamic checks ensure specification correct 
overhead safety checks intolerable production mode programmer program debugging stage 
jade needs check included side effect specification task debugging run checks program correct respect set input data 
correctness independent timing parallel execution 
jade supports exploitation coarse grain concurrency sequential imperative programming paradigm 
jade programmers augment sequential program high level dynamic data usage information jade implementation uses information concurrently execute program respecting data dependence constraints 
jade provides implicit concurrency freeing programmer burden explicit concurrency management 
jade programmers construct express simple concurrency patterns synchronization takes place task boundaries 
high level interface task queue model computation 
constructs support complicated concurrency structures requiring periodic inter task synchronization 
jade supports expression full range coarse grain concurrency including irregular concurrency available program runs 
jade support expression concurrency available procedure boundaries allows jade programmers retain structure original program parallel version 
jade programmers preserve program structure decisions reasons design 
jade token data type supports creation shared data objects high level side effect specification operations 
jade programmers express tasks side effect information level abstraction tasks access shared data objects 
plan extend jade tokens explicitly associated data represent 
association allow implement jade machines separate address spaces 
investigate improve performance jade compiler technology 
acknowledgments jennifer anderson dan scales participating fruitful discussions jade 
jennifer wrote jade preprocessors dan implemented jade versions 
edward rothberg help sparse cholesky factorization code 
babb ii 
large grain data flow scheduler parallel processing 
proceedings international conference parallel processing august 
berry perfect club benchmarks effective performance evaluation supercomputers 
international journal supercomputer applications 
carriero gelernter 
applications experience linda 
proceedings acm symposium parallel programming pages new haven conn july 
carriero gelernter 
write parallel programs guide 
acm computing surveys september 
chandra gupta hennessy 
language parallel programming 
gelernter nicolau padua editors languages compilers parallel computing pages 
mit press cambridge ma 
dongarra sorenson 
portable environment developing parallel fortran programs 
parallel computing 
duff grimes lewis 
sparse matrix test problems 
acm transactions mathematical software 
foster taylor 
strand new concepts parallel programming 
prentice hall englewood cliffs 
halstead jr multilisp language concurrent symbolic computation 
acm transactions programming languages systems october 
hammel gifford 
fx performance measurements dataflow implementation 
technical report mit lcs tr mit november 
hoare 
communicating sequential processes 
prentice hall englewood cliffs 
kong 
refined update 
gelernter nicolau padua editors languages compilers parallel computing pages 
mit press cambridge ma 
lenoski laudon gharachorloo gupta hennessy 
directory cache coherence protocol dash multiprocessor 
proceedings th annual international symposium computer architecture may 
inmos occam programming manual 
prentice hall englewood cliffs 
lucassen 
types effects integration functional imperative programming 
technical report mit lcs tr mit august 
lusk overbeek portable programs parallel processors 
holt rinehart winston 
united states department defense 
manual ada programming language 
dod washington january 
ansi mil std 
rose 
parallel global router standard cells 
proceedings th design automation conference pages june 
rothberg gupta 
efficient sparse matrix factorization high performance workstations exploiting memory hierarchy 
appear acm transactions mathematical software 
rothberg gupta 
techniques improving performance sparse matrix factorization multiprocessor workstations 
proceedings supercomputing pages november 
biswas korner browne 
task level dataflow language 
journal parallel distributed computing 
tokoro 
concurrent programming 
yonezawa tokoro editors object oriented concurrent programming pages 
mit press cambridge ma 
