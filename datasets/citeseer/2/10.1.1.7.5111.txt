evolutionary algorithms noisy environments theoretical issues guidelines practice hans georg beyer university dortmund department computer science xi dortmund germany beyer cs uni dortmund de devoted effects fitness noise eas evolutionary algorithms 
short history research field performance gas genetic algorithms ess evolution strategies hyper sphere test function evaluated 
shown main effects noise decrease convergence velocity residual location error observed gas ess 
different methods improving performance hypotheses working mechanisms discussed 
method rescaled mutations analyzed depth es sphere model 
shown method needs advanced self adaptation techniques order take advantage theoretically predicted performance gain 
troubles current self adaptation techniques discussed directions research worked 
key words evolutionary algorithms ga es ep noisy fitness data convergence properties optimization noise convergence improvement techniques supposed advantage evolutionary algorithms eas eas believed noisy environments 
contrast traditional optimization methods strongly rely deterministic information order find optimal solutions 
believed convergence stability eas rests observations evolution nature appears highly disturbed deception noise 
living beings adapted environment 
provided preprint submitted elsevier preprint november darwinian evolution regarded optimization kind algorithms designed accord darwin paradigm obey similar properties 
second ea applications cope noisy fitness information 
noise helpful evolutionary search 
papers published field evolutionary algorithms eas explicitly dedicated problem noisy fitness measurements 
may appear surprise reader practice cope statistical measurement errors 
example trying optimize operation machine tool tuning machine control parameters outcome produced identical parameters fixed 
example concerns field computer simulations numerical errors simulation technique monte carlo simulations discrete event simulations may produce noisy results 
cases improving outcome risky endeavor sure improvement obtained certain control parameter change real improvement 
noise may deceive decision making 
quite clear degree deception function relative noise level 
noise strength small compared effective signal strength deception relatively seldom event 
small small expected noise level gradually increased 
extreme case large noise level large compared effective signal strength easily treated useful selection information measured improvement result desirable control parameter change result noise equal likelihood 
parameter changes eas usually unbiased random parameters optimized perform random walk 
directed evolution 
diffusion behavior observed result distance optimum increases time 
extremes zero noise total noise working domain living beings eas 
goals investigate influence noise performance genetic algorithms gas evolution strategies ess 
want emphasize similarities performance behavior glance different algorithms 
expedient consider simple fitness functions objective function optimized ea order universal phenomenon functions optimized 
furthermore simple fitness functions sphere model considered allow analytical treatment performance behavior algorithms 
advantage compare theoretical predictions performance real algorithm 
see discrepancies theory practice may serve driving force improvements algorithms developed 
rest organized follows 
section presents short history works dealing explicitly performance investigations eas noisy fitness data 
section devoted effects noise simple test function sphere model 
performance gas ess compared providing astonishing results 
section summarizes techniques convergence improvement introduces method rescaled mutations 
having general viewpoint sections devoted ess 
section technique rescaled mutations analyzed called es frame sphere model 
analysis show method improve convergence optimum real valued search spaces proves difficult achieve real algorithms affords control standard deviation isotropic mutations 
usual way ess self adaptation sa 
rescaled mutation technique sa considerable problems tune mutation strength algorithm benefit rescaling effect 
section provide advanced sa techniques able drive algorithm optimal working regime 
close short summary outlook section 
short history noise related high noise levels observed nature hypothesis darwinian evolution optimization infers eas highly noise resistant 
unfortunately june serious investigations field probably dates back early 
rechenberg tried evaluate es evolution strategy theoretical performance measures simple fitness functions dimensional sphere definition see dimensional corridor 
succeeded calculating progress rate noisy es 
expected progress corridor direction decreases zero increasing noise strength respective definitions see 
sphere model wait notation refer kind selection ess parents generate offspring 
parents generation obtained selecting best offspring case version 
version parents obtained older generation offspring 
comma strategies parents die definition plus strategies parents survive infinitely long called elitist selection 
years 
fitzpatrick grefenstette published empirical results noisy fitness evaluations gas genetic algorithms 
obvious way improving ga performance noise reduction resampling averaging number fitness measurements keeping control parameters optimized constant increases number fitness evaluations factor alternatively increase population size fixed amount cpu time total number fitness evaluations question arises allocate resources order get maximal performance 
fitzpatrick grefenstette best performance test function achieved small sample sizes large population sizes general rule 
ess evolution strategies sphere model beyer shown solving pending progress rate problem opposite case possible 
predictions initiated empirical research es field hammel back confirming beyer progress rate theory 
interestingly evidences preference resampling enlarging population size hold recombinant ess 
due lack theory results taken care 
see performance real es strongly depend correct working sa self adaptation mechanism controlling mutation strength es 
concerning influence sa performance similar observations angeline compared self adaptive evolutionary programming see fogel gaussian mutation rule mutation strength log normal mutation rule usually preferred ess definition see point eq 

reported gaussian rule outperforms log normal rule set test functions 
performance differences significant experimental conditions appear orders magnitude 
lack theory understand observations 
observation indicates reliability convergence global optimum improved certain amount noise 
similar observations kauffman investigated es adaptive walk algorithm 
authors bit mutation operator performs bit moves 
peaks melting effect leaving local attractors accomplished simulated annealing selection mutation operator allowing moves hamming distance larger 
rana 
came similar assessments added noise helpful fitness functions initial phases search 
theoretical related noise field gas traced back early goldberg rudnick developed extensions schema theorem order account sampling noise called collateral noise 
investigations mainly intended derive models population sizing problem 
population sizing presence fitness noise considered goldberg refined model harik :10.1.1.57.7053
basic idea models comes paradigm building block assembly order obtain final optimum solution right building blocks flow 
decision process ga selecting right building blocks disturbed finite population size sampling schema fitness variance collateral noise 
calculating probability selecting right building blocks depending population size approach estimating population size 
fitness noise regarded additional independent noise source easily incorporated population sizing approach 
approach appears totally different progress analysis ess resulting equations similar respect functional structure see point 
performance analyses defined sense gas noisy fitness evaluations relatively new 
date back miller firstly presents phd thesis usable population sizing model includes fitness noise 
overview main results miller goldberg 
aforementioned spirit es performance analysis treating ga dynamical system analyzing expected fitness change time :10.1.1.57.7053
analysis mainly fitness dynamics onemax bit counting function regarded counterpart sphere model binary search spaces 
due similarities theoretical approaches expect similarities compare results 
differences approximations neglect effects sampling finite populations leading miller assertion fitness noise affect proportionate selection 
see simple simulations see fig 
section conditionally correct 
finite sample size effects addressed ph rattray published rattray shapiro applied theory onemax bit counting function perceptron learning problem binary weights ga boltzmann selection special kind nonlinear proportionate selection 
main messages effect noise removed choosing population size exp fi oe stands population size ga noise fi boltzmann selection strength oe standard deviation gaussian fitness noise 
mentioned similar relation hold es 
gives clue effects noise associated problems similar eas evolutionary algorithms 
goals search similarities behavior gas eas respect influence fitness noise optimization performance 
section exactly serves goal 
effect fitness noise eas difficult task evaluate performance eas real world problems theoretical means 
due intrinsic probabilistic behavior usually excluded calculate dynamics optimization process exact analytical numerical markov models 
approximations asymptotic correct approaches 
derivation approximations rely simple objective functions 
regard flaw current ea theories progress ea theory understanding working algorithms obtained starting simple models gradually proceed complicated ones 
section optimization performance ess gas simple dimensional sphere model evaluated experiments 
investigate influence different fitness noise levels mean value dynamics residual distance population known optimum solution parameter space 
introducing test fitness function optimized point devoted standard genetic algorithms 
point applies tests different variants evolution strategies 
surprise see qualitative behavior gas ess similar 
observation motivate look similarities theoretical es ga results far 
fitness noise models order get feeling fitness noise degrades ea performance consider simple fitness model optimized leaves certain chance succeed theoretical analysis 
ga field onemax bit counting function purpose 
es field dimensional sphere model prominent test function defined real valued search spaces 
sequel concentrate sphere 
decision fact developed theory noisy ess 
parameter vector optimized optimal parameter vector general sphere model fitness function reads ky gamma yk ky gamma yk usually monotonic function ky gamma yk gaussian noise term zero mean standard deviation oe oe assuming normal noise distribution may regarded approximation reality maximum entropy principle 
furthermore simplify considerably 
pdf probability density function noise reads oe exp gamma oe note noise strength oe function allowing modeling relative measuring errors 
evaluate performance standard gas sess standard ess special sphere model gamma oe oe const special fitness model delta ff oe ff derive dependent progress rate formula es rescaled mutations 
fitness models optimum noise switched 
model maximization max model minimization min symbol noise switched 
performance sess noisy sphere model ga performance experiments eq 
serves fitness model 
standard ga goldberg binary coding bits parameter individual string length leading parameter space dimension 
coding done way optimum point exactly expressed bit string order avoid genotype phenotype approximation errors 
search interval gamma chosen intention ensure oe 
recombination done uniform crossover syswerda probability 
selection method investigated proportionate selection realized roulette wheel selection goldberg tournament selection tournament size tourn called binary tournaments tournament size tourn 
selection methods realized replacement selected parents put back parental pool reproduction 
mutation rate set zero random initialization population required 
shows dynamics gas 
plots average results obtained independent evolution runs 
left pictures population size pop size right pictures display case 
picture population average fitness hf residual distance hri optimum dimensional parameter space plotted 
upper curves belong hf lower curves hri showing influence different noise strengths oe 
displaying residual distance hri somewhat unusual ga performance investigations delivers additional information population approaches optimum dimensional phenotype parameter space 
infer pictures hri approaches steady state value hri obviously steady state value monotonically increasing function noise strength oe oe remains certain distance optimum 
population get nearer optimum 
interesting observation concerns selection types 
proportionate selection appears insensitive selection technique influence oe sufficiently large oe performs equally better selection techniques 
miller statement steady state value depends noise strength oe oe curves upper pictures identical oe observes separation certain number generations 
population size increased separation shifted larger generation numbers 
easily show generation gas pop size prop sel 
generation gas pop size prop sel 
generation gas pop size tourn generation gas pop size tourn generation gas pop size tourn generation gas pop size tourn fig 

ga dynamics hf hri averages noise levels different population sizes left pictures right pictures 
upper pictures case proportionate selection displayed middle obtained binary tournament selection bottom pictures tournament selection tourn 
note curves generation 
due gene convergence 
premature convergence time increased increase population size 
influence noise totally removed 
behavior peculiarity proportionate selection 
holds tournament truncation selection see lower pictures discussed 
apart population size selection pressure important influence dynamics 
rule thumb increasing tournament size larger values usually reduces residual distance speeds convergence velocity 
brought price faster premature gene convergence see lower pictures 
gene convergence avoided mutation rate increases value observed simulations 
premature gene convergence shifted higher generation numbers increasing population size clearly seen comparing lower pictures 
furthermore important note elitism keeping track best solution far helpful scenario fitness seemingly best individual may result large noise fluctuation see discussion es 
es performance claimed ess specially tailored optimization realvalued parameter spaces 
expect generally outperform sga sphere model 
see noise comes play algorithm classes exhibit similar behavior 
short definitions es algorithms 
es self adaptation tune single component standard deviation oe mutation operator test 
mutations produced isotropic gaussian random vectors oe selection performed truncation selects best individuals offspring parents generation 
es algorithms expressed compact form order notation delta select mth best describes selection 
best refers objective optimization 
stand maximization minimization just index number mth best individual 
individual genome self adaptive ess comprise object parameter set optimized strategy parameter set individual inherited vector 
case isotropic gaussian mutations just scalar oe generation lth individual 
notations es sa self adaptation reads random oe oe delta exp oe generation counter random integer number sampled anew furthermore applied schwefel version sa log normal mutations 
reasonable choice learning parameter ess sphere prove progress coefficient definition see eq 

es simple population mutation selection algorithm recombination 
contrast es uses recombination extreme form called multi parent recombination 
versions recombination 
intermediate takes simply average individuals produce new descendant 
version recommended strategy parameter recombination 
second version dominant recombination known global discrete recombination 
transfers randomly chosen parental components offspring resembling generalized version uniform crossover 
dominant recombination recommended treatment object parameters sa self adaptation desired 
self adaptive es isotropic gaussian mutations reads oe oe delta exp random oe stands ith component vector normally distributed random generators sampled anew order performance comparisons suitable initialization ess chosen 
fair starting condition vectors concentrated randomly chosen point distance optimum roughly hri observed measured ga simulations hri 
choice initial oe misplaced optimal local performance example considered chosen intention demonstrate sa capabilities 
shows simulation results strategies left pictures right pictures 
upper curves obtained es 
noise case oe es approaches optimum fast reduces hri beating sga gets stuck 
relatively small noise level oe sga performs better 
striking hf goes far noise free maximum 
best largest fitness value produced contributes hf hf observed behavior reflects just outliers produced oe noise term 
words introducing elitism keeping track best individual far noisy eas qualitatively improve convergence behavior 
elitism exclude divergence 
remarkable property proven beyer es sphere model 
take formula describes steady state behavior noisy es appears special case formula derived eq 
es oe equal sign holds vanishing mutation strength oe 
called progress coefficient see eq 
basically expectation th order statistics standard normal variate 

formula exactly holds asymptotic case yields usable results verify upper pictures 
formula explains observations far 
shows monotonously increases oe second decreases increasing population size ln 
similar findings rattray shapiro gas influence noise order statistics reader referred 
generation es tau generation es tau generation es tau generation es tau generation es tau generation es tau fig 

es dynamics hf hri averages noise levels 
note upper pictures larger vertical scale order display hf values 
populations start hri indicated black box symbol 
es initial mutation strength oe large 
see middle pictures population initially driven away optimum hri curves coming 
note hf curves bottom pictures fairly optimum 
due high mutation strength oe generated sa 
ess removed certain extend choosing exp oe third residual distance increases square root parameter space dimension 
bad news indicates fixed noise level higher dimensional problems harder optimized 
explicitly tested reasonable assume gas suffer problem 
considering ga population sizing model goldberg comes qualitative statement :10.1.1.57.7053
population sizing equation oe oe oe variance collateral oe fitness noise measures signal difference competing schemata parameter depending significance level building block decision 
interest apart oe influence consonant observations es experiments counts number competing schemata 
assuming related parameter space dimension const clear population size scaled 
refined sizing model harik compared results ess see discussion eq 

best individual obviously clever policy 
switching parent numbers considerably improve convergence behavior inferred middle pictures 
furthermore recombination gives additional performance gain 
theory calculating cases 
general tendencies easily verified experiments finds oe putting knowledge intuition formulae general sphere model read es ff oe cff es ff oe progress coefficients defined respectively 
predictive power relatively high 
equation works satisfactorily correctly describes scaling behavior ff 
apart question steady state value speed convergence interest 
defined expected change generation called progress rate hri gamma hri large values produce steeper falling hri curves desirable 
progress rate ess slower ess provided oesa works correctly 
calculating fitness model main goals es theory 
example es rescaled mutations treated section 
guidelines improving ea performance noise argue driving guidelines general ea performance sphere model somewhat bold 
arguments taken account 
seen noise deteriorates final location optimum 
mainly affects phase optimization 
phase ea usually decided attractor multimodal fitness landscape 
considering attractors locally approximated bilinear form approximated sphere model certain mean curvature pp 

second analytical formulae obtained sphere model allow discussion parameters influencing behavior ea 
certain hope verified experiments properties derived saved transferred complicated fitness functions real world applications 
third know theory ea perform test function 
large discrepancy theoretical predictions performance real algorithm suspect error theory wrongly working algorithm 
motor developments 
furthermore see performance improving techniques inferred empirical ga research ga theory building block competition models 
different measures improve performance noisy eas resampling sizing population size inheritance rescaled mutations 
discussed subsections 
resampling sizing population resampling simple measure improve convergence optimum eas 
individual genome fitness measured times averaged yielding fitness const oe var oe noise strength reduced factor assuming fitness calculation time consuming part ea resampling technique come free 
alternatively raise population size factor fitzpatrick grefenstette findings point direction 
es opposite usually hold 
es times resampling gives better results delta es cases 
things change parental population 
effects obtained choosing free fitness calculations 
provided correct effect population ess compared generalized form ff oe order es times resampling 
interpret oe times resampling implicitly performed parental population size generalized form es times resampling ff oe cff delta es ff oe cff assumed delta es perform slightly better es resampling 
remains question choose fixed population size main goal minimize applying lead condition max function relatively broad maximum choice long gamma sufficiently large 
asymptotic limit conjecture ln 
calculate optimum maximizing ln result experiments performed order test applicability formula 
recombination bring additional convergence gain seen formula compared 
additional gain mainly result similarity extraction called genetic repair effect takes place parameter space 
ask optimum asymptotically exact formula exp gamma phi gamma gamma minimal reached max find condition phi gamma gamma 
phi gamma inverse function cdf cumulative distribution function standard normal variate 
phi finds 
note considerably deviates recommended noiseless case 
hammel back strategy leading concluded resampling preferred 
result wrong population sizing 
simulations carried get definite answers question resampling vs population sizing 
choosing right population size discussed viewpoint building block decision making gas 
approach goldberg mentioned point 
refined model proposed harik 

sizing equation reads gamma gamma ln oe bb order building block failure probability selecting wrong building block number build exact fixed optimal ratio depends asymptotically finds sphere model 
ing blocks individual binary genome bitstring 
discussion parameters interest oe bb oe bb average building block variance called signal difference 
oe bb comprise different noise sources contain fitness noise oe scaling property model respect oe bb reads oe bb 
interesting see scaling property obtained eq 
case optimal es cr ff results mentioned finds oe ffq 
interprets expected fitness distance optimal fitness signal difference similar scaling behavior oe obtained 
open question functional similarity scaling properties population size gas ess just incidence reveals deeper connection 
inheritance rescaled mutations resampling technique require additional fitness evaluations 
restricted quasi continuous search spaces requires scaling mutations 
proposed rechenberg 
asymptotic analysis version limitations implementation issues published 
dependent progress rate analysis performed section improved sa method proposed section 
idea version perform large mutations parental state due large mutations offspring worse fitness parent 
mutation produced best offspring rescaled length factor es perform large search steps result larger fitness differences hopefully significant noise level 
having right direction es reduced size step direction 
result serves parent generation 
order fit algorithm frame similar formulate offspring notation 
easily done offspring generated author grateful goldberg pointed functional similarity 
finds obtains gamma substitution gets gamma expressed new offspring offspring mutations previous generation 
formulate self adaptive es rescaled mutations exp oe oe oe gamma expressing es way generalization es straightforward 
version intensively tested experiments 
investigation remains research 
section shown rescaling rule algorithm allows gamma gamma gamma 
far sphere model concerned 
real algorithm exhibits poor performance es experiments 
lines usually expected 
discussion interesting issue postponed section 
progress rate analysis rescaled mutation technique section provides calculation progress rate dimensional sphere model 
technical section 
readers just interested basic ideas progress rate analysis read point immediately skip result eq 

establish link ga theory 
section organized follows 
general ideas approach 
calculation defined performed parts put 
having approximative formula predictive power tested experiments section showing yields far better results asymptotic formula derived 
section convergence behavior es discussed 
derivation general approach progress rate expected parental distance change defined eq 

parent ess generation ky gamma yk gamma ky gamma yk krk gamma rk new parental state 
decompose mutation component pointing direction optimum residual vector perpendicular part gammax er decomposition depicted left picture takes rescaling account 
inserted best offspring offspring fig 

decomposition mutations part optimum direction defined unity vector er perpendicular vector 
left picture displays rescaling best mutation right depicts decomposition arbitrary mutation 
substituted obtains omitting generation counter ae krk gamma fl fl fl fl fl fl fl fl oe ae krk gamma fl fl fl fl gamma er fl fl fl fl oe recalling krk gets gamma gamma point formula hold fitness functions 
step introducing normalized quantities oe oe oe independent fitness model meaningful intended dimensional sphere model 
normalized progress rate reads gamma gamma expectation expression depends scalar random variates pdfs probability density functions depend fitness model mutation distribution parameter space dimension population size actual parental state hope get closed analytical expression 
approximations developed 
approximation introduced removes outer efg gamma gamma neglecting fluctuations mean values efx efh asymptotically correct gaussian mutations sum gamma squared independent oe random variates 
central limit theorem holds efh fulfilled single mutation similar arguments hold varfxg oe small compared gamma oe exact proof requires taylor expansion values omitted 
left problem determining intermediate step taken calculation pdf mutation induced noisy fitness distribution 
calculation mutation induced noisy fitness distribution point fitness model enters stage 
analyze minimization task model 
order keep calculations simple possible case ff considered 
cr ff noise free fitness offspring generated parental state mutation see right picture 
conditional pdf offspring noise perturbed fitness reads oe exp gamma gamma oe pdf offspring distance optimum known calculate pdf dr general calculation difficult 
due restriction ff suffices calculate 
consider right picture 
geometry reads gamma gamma rx due spherical symmetry mutations confused sphere model assumption components new decomposition obey normal distribution density original components oe oe 
interpreted sum distribution normal distribution oe 
distribution approximated normal distribution approximately normal 
simplest way get mean standard deviation normal approximation directly calculate oe gamma knowing moments random variate oe oe oe obtains simple calculations oe oe oe oe oe normal approximation reads oe exp gamma gamma oe oe calculate inserting account oe oe exp gamma gamma cs ff oe exp gamma gamma oe oe ds closed integration possible ff 
ff values treated series expansions 
order calculate gamma oe oe substituted leading oe gamma gamma exp gamma gamma coe oe gamma oe oe dt integral solved gamma gamma exp gamma dt exp gamma oe symbol standard deviation noisy offspring fitness obtain oe exp gamma gamma oe oe oe oe oe cdf cumulative distribution function reads phi gamma oe oe note phi cdf standard normal distribution 
connected error function phi erf calculation order calculate need pdf denoted 
mutants generated best provided accepted best 
leads component mutation oe distributed oe gamma oe due isotropy mutations 
counts number different possibilities best 
remains determination acceptance probability 
consider probability fitness interval jx gamma jx certain value jx jx order best remaining gamma mutants values larger jx minimization considered 
occurs single individual probability jx gamma jx gamma jx 
gamma independent mutants fulfill jx gets probability jx jx gammap jx gamma writing jx gamma qjx gamma gamma conditional pdf qjx determined 
recalling noise distribution conditional pdf neglecting term order get tractable integrals allowed efx efh obtains qjx oe exp gamma gamma cr crx gamma oe pdf calculate qjx qjx qjx distribution gamma degrees freedom 
apply normal approximation method led 
easily finds gamma oe gamma oe get tractable integrals forced gamma sequel 
error approximation order neglected sufficiently large pdf reads oe exp gamma gamma oe oe integration performed 
writing obtain qjx oe oe exp gamma gamma cr crx gamma oe theta exp gamma gamma oe oe du substitution gamma oe oe integral reads qjx oe gamma gamma exp gamma gamma oe oe gamma cr crx gamma cn oe oe dt applying get qjx oe exp gamma gamma cr crx gamma cn oe oe oe oe oe result inserted 
substituting gamma gamma oe oe considering phi gamma phi gammat account obtains oe oe gamma phi gamma exp gamma gamma oe oe crx oe dt expected value obtained oe oe oe gamma xe gamma oe gamma phi gamma exp gamma cr oe gamma oe oe dt dx changing order integration substituting oe gets oe oe oe gamma phi gamma gamma se gamma exp gamma oe gamma oe oe ds dt inner integration carried shown gamma se gamma exp gamma ds exp gamma hold 
simple calculation definition oe oe account obtains oe gamma te gamma phi gamma dt contains known progress coefficient gamma te gamma phi gamma dt normalized noise strength oe oe oe jq dq dr specializes noise free fitness cr ff oe oe ff ff oe oe cr rewrite means 
obtain oe oe oe oe calculation derivation follows ideas previous subsection 
sketch way 
pdf written 
analogy qjh gamma gamma conditional pdf qjh obtained qjx eq 
integrating possible states pdf eq 

analogous qjh qjx dx substitution oe integral qjh oe gamma exp gamma oe gamma oe dt integration formula auxiliary parameter oe analogous oe oe gets qjh oe exp gamma gamma cr gamma oe inserting acceptance probability expression substitution gamma gamma oe oe argument applying symmetry relation standard normal cdf phi gamma phi gammat obtains oe oe gamma phi gamma exp gamma gamma oet coe gamma oe dt expected value calculated writing oe oe oe exp gamma gamma oe oe theta phi gamma exp gamma cu oet gamma coe oe dt du substitution gamma oe oe changing integration order arrive phi gamma oe oe theta oe oe gamma exp gamma oe oe oe oe ds dt inner integral treated 
account gets phi gamma oe gamma gamma oe oe gamma dt term sum integrand simplified dt phi phi gamma gamma 
integral yields oe second term basically contains expressed progress coefficient oe gamma coe oe applying normalization obtain oe gamma oe oe oe oe putting things progress rate formula obtained plugging 
order comparable progress rate formulae simplified approximation square root taylor expansions 
sake simplicity abbreviations oe oe oe oe progress rate reads gamma gamma oe oe gamma oe oe oe upper bound quadratic part delta delta delta neglected sufficiently large obtains gamma gamma oe gamma oe oe gamma gamma oe ab oe taylor expansion gamma gamma gets gamma oe oe gives final result oe oe oe oe oe oe gamma oe gamma progress rate formula contains special cases calculated oe gets dependent noise free formula derived 
oe oe gets asymptotic noisy formula reads oe oe oe gamma oe order see influence fixed normalized noise strength oe contour plots oe es 
infer plots long optimum oe oe combination maximizes expected progress increasing optimum shifts larger oe show oe progress rate maximum 
effect noise totally removed maximum progress rate noise free case 
come back related convergence properties section 
interesting compare asymptotic result results miller goldberg ga field 
similar standard approach es theory ga treated dynamical system 
basically interested generational change system 
investigated expected change fitness values known quality gain es theory gamma hf assuming normally distributed population fitness miller goldberg obtained oe oe oe oe population variance fitness called selection intensity adopted quantitative genetics see blumer 
es es es fig 

influence es considered oe 
contour lines plots constant values 
selection defined 
equation compared asymptotic quality gain formula es sphere model 
results finds gamma ffq const delta comparing account sees functional structure gain term recovered population variance oe ga equal appropriately normalized mutation strength es 
remarkable observation population variance ess produced applying mutation operator single parent 
significant difference due loss term 
reflects detrimental effect mutations neglected mutation strength sufficiently small 
hand may speculate validity domain 
due normal approximation approach deviations expected 
model miller consider selection 
effects recombination mutation explicitly covered analysis 
second normal distribution assumption violated successive approach ga optimum noise free fitness values bounded optimum 
skewness fitness distribution neglected correction terms incorporated 
comparison experiments shown asymptotic formula gives satisfactory results small long oe large 
see oe chosen relatively large order high convergence velocity 
es example oe considered demonstrates limitations asymptotic theory 
shows results called generation experiments 
fixed mutation strength fig 

simulations es oe displayed dots parameter space dimensions 
noisy fitness model kyk oe oe ky oe 
dashed curve shows standard es rescaling 
oe parent randomly placed distance optimum 
algorithm applied generation parental distance change serves estimate oe 
see curves provide satisfactory predictions data points simulated 
furthermore clear asymptotic formula curve suited parameter setting 
derivations fitness model cr ff ff 
just noisy case rescaled mutations possible derive approximate formula ff 
lowest level approximation unchanged 
simply change oe normalization rule 
shows results es simulations setting left picture displays ff case right ff 
see formula fig 

left picture fitness model kyk oe oe ky koe right picture fitness model kyk oe oe ky oe see 
applied ff generally expect larger approximation errors 
convergence properties rescaled mutation technique depending choice strategy parameters oe noise level oe es exhibits different convergence behavior 
discuss different scenarios derive evolution criterion predicts general behavior es 
progress rate key understanding convergence properties measures generational distance change gamma normalization obtains gamma oe oe difference equation governs mean value dynamics es 
maximal performance fastest decrease obtained maximal seen fixed oe optimum oe choice 
question arises real world scenario 
algorithm expected nearly optimum state oe oe oe const realistic assumption 
answer question shall postponed section 
second recall definition oe oe jq jr const oe proportional jq jr defined oe function interpretable ff postulated ff constant 
differential equation ff solved yielding cr ff fitness function ratio oe interpreted relative measurement error oe const oe ff independent fitness model peculiarity depending fixed relative measurement error oe ff es converge 
calculated means evolution criterion introduced 
oe const case oe const practical interest 
main subjects section 
seen oe const implies convergence residual distance derivation lower bound prepared 
progress rate displayed function oe convergence divergence hold limit case yields oe oe curve oe oe 
shows curves obtained es parameters 
es es convergence divergence fig 

convergence card es rescaled mutation 
right picture explains read curves left 
curve belongs special setting states oe oe es divergent calculated 
non divergence hold eq 
yields simple calculation evolution criterion oe oe oe oe see bounded values oe oe convergence sign ensured increasing population size ln increasing recommended measure require additional fitness calculations 
evolution criterion allows calculation residual distance oe const 
refers condition equal sign consider 
resolving oe gives oe oe gamma oe oe oe definition applied fitness model cr ff gives oe oe ff cr ff equating solve ff oe oe gamma oe oe gamma neglects oe eq 
turned inequality negative term increases get simple expression ff oe equal sign holds vanishing oe note eq 
contains special case 
obviously increasing es rescaled mutations defined allows arbitrary reduction residual distance take advantage rescaling technique excursion es theory prepared application side 
learned theory rescaling technique allows reduction residual distance 
furthermore able control mutation strength appropriately detrimental effect noise convergence velocity progress rate reduced 
order take advantage rescaling technique self adaptation sa part algorithms way es dynamically learns right mutation strength 
theoretical analysis full blown sa es 
rely simulation experiments 
subsection standard oe sa evaluated 
shown oe sa considerable problems learn right mutation strength 
advanced oe sa rules proposed evaluated section 
performance standard oe self adaptation rule order take advantage rescaling technique progress rate nearly maximal 
looking figures gets feeling optimal performance obtained normalized mutation strengths oe larger oe standard es oe holds 
approximative oe formula derived gives oe optimum oe duty sa self adaptation part real algorithm see 
second line change mutation strength oe way generation oe oe roughly fulfilled 
standard sa techniques implemented algorithm ess long fitness noise free 
noisy case algorithm works 
realizes see upper pictures 
unfortunately things change worse rescaled mutations noisy fitness come play 
example algorithm tested es oe oe 
shows hri dynamics upper pictures average normalized mutation strength oe lower picture 
case dealing labeled sa ln standard 
case es converges slowly 
looking oe lower left picture see oe far small 
considerably smaller noise free case oe displayed generation es tau kappa sa ln standard sa ln rescaled kappa sa tp gamma sa tp gamma kappa sa daemon control sa ln standard noiseless case generation es tau kappa sa ln rescaled kappa sa tp gamma sa tp gamma kappa sa daemon control sa ln standard noiseless case generation es tau kappa sa ln standard sa ln rescaled kappa sa tp gamma sa tp gamma kappa sa daemon control sa ln standard noiseless case generation es tau kappa sa ln standard sa ln rescaled kappa sa tp gamma sa tp gamma kappa sa daemon control sa ln standard noiseless case fig 

evolution dynamics various sa techniques es 
left pictures parameter space dimension noise strength oe 
curves obtained averaging independent es runs 
right pictures oe obtained averaging es runs 
upper pictures display hri dynamics lower display average normalized mutation strength oe sigma 
sa ln standard noiseless case optimum oe roughly reached 
adapting smaller oe values desired general tendency 
observes divergence 
lower right picture shows extreme oe fluctuations fill picture left 
hri leaves plotting interval generations 
wonder rescaled mutation technique reach theoretical values 
advantage sphere model desired oe calculate mutation strength oe needed 
normalization holds oe oe daemon knows tune oe deterministically 
distance optimum known sphere model daemon task easily implemented 
dynamics obtained control depicted labeled sa daemon control 
oe chosen 
verify means eq 
obtained 
measured bit theoretical limit 
clearly see rescaling technique works 
obvious problems arise sa mechanism produces small oe values exhibits large oe fluctuations disturb evolution process 
item needs discussion done 
advanced oesa rule rescaled oe mutations order understand oe dynamics observed standard oesa remember actual parental oe inherited offspring owning best fitness value 
rescaling takes place selection 
feedback fitness parent produced rescaling 
words oesa learn optimum oe rescaling 
author opinion hope find solution problem radically change basic sa idea 
course imagine meta ess aging mechanism mechanisms violate time locality oesa algorithm 
strategies stochastic behavior ea fully determined parental state history states dating back generation markov process higher order 
open interesting new oe control techniques stick possibilities inherent frame time locality 
proposals improve standard oesa algorithm 
originated rechenberg transfers idea rescaled mutations object parameters strategy parameters 
mutation oe done multiplication see line algorithms full analogy drawn expressed logarithm law 
case es parental oe obtained oe random belongs best offspring oe oe delta ln oe ln oe ln rescaling rule reads ln oe ln oe oe ln oe analogy 
section eq 
expressed best offspring corresponding mutation 
comparison yields ln oe ln oe oe gamma ln ln write full oe oesa es xi oe oe oe oe gamma oe gamma line modified compared 
xi stands random number generator produces standard oesa lognormal generator exp 
type generator point rule discussed 
oe rescaling rechenberg proposed xi oe exp oe basically proportional scaling learning parameter 
oe intention produce larger oe changes allow reliable detection direction oe change 
selection promising direction inherited rescaled 
experiments performed author indicate main effect smoothing stochastic oe dynamics 
effect observed lower right picture 
curve label sa ln rescaled kappa astonishing smooth especially compared sa ln standard 
hri performance slow 
convergence velocity worse initial oe chosen small 
oe rescaling slows sa dynamics 
advanced oesa rules unsymmetric point mutations hybrid rescaling second mutation rule occasionally ess symmetric point rule proposed rechenberg 
special case general point rule introduced reads xi fl fl fl sample uniform random number generator 
symmetrical case obtained fl 
symmetrical point rule exhibits behavior lognormal rule stationary oe values small author idea bias mutations oe increase 
probability increasing oe larger 
accomplished choosing fl 
evolution dynamics produced unsymmetric point rule displayed fl 
order separate different effects oe rescaling switched oe curves obtained labeled sa tp gamma 
case observes hri dynamics approaches vicinity steady state generations 
looking oe dynamics sees steady state distribution oe higher average compared mutation rules discussed 
brought expense large oe fluctuations due averaging left pictures right pictures independent evolution runs real fluctuations factor respectively larger displayed 
average oe may optimum oe oe fluctuations necessarily deteriorate convergence velocity due general property curves see bounded oe interval oe 
fluctuations outside interval contribute negative progress net progress performance degrades convergence velocity slows turns divergence 
observed upper right picture 
curve sa tp gamma exhibits slow unsteady convergence behavior average oe roughly 
neglecting fluctuations expect performance comparable sa daemon control curves oe chosen 
see fl provides larger oe values due large oe fluctuations advantage get lost 
goal reduce fluctuation variance changing mean 
accomplished oe rescaling technique 
algorithm unsymmetric point mutations oe 
hybrid performs considerably better variants 
performance curves labeled sa tp gamma kappa 
hri curve comes close daemon curve 
case shows slightly different behavior indicating existence time scales fully understood 
case hybrid oesa technique enables rescaled mutation technique reach theoretically predicted value 
recommended investigations 
open question concerns choice fl oe answer reached frontiers es research 
summary outlook optimization noisy uncertain environments regarded favorite application domains evolutionary algorithms 
compared practical relevance effects noise influence performance eas gained little attention ea research 
research field noisy eas infancy 
focussed different topics 
gave short overview research done established ea classes ga ep evolutionary programming es 
second goal show peculiarities arise noise selection process 
may appeared surprise effects fitness noise similar gas ess reduction convergence velocity deterioration final optimum location quality 
tested quantified sphere model clear effects universal nature 
quantification simple ga test functions onemax remains done 
anyway effects kinds eas 
importance practitioners aware facts methods improving convergence properties 
main convergence improvement techniques resampling population sizing identified need ea theory giving answers sizing population 
evolution strategies ess important recombination recommended 
real valued search spaces mutation rescaling techniques may alternatively 
technique integer search spaces tested 
es theory discussed es rescaled mutations section main directions investigated ffl development corresponding theory ess 
ffl analysis oesa rules oe 
derivation formulae substantiated respective progress rate theory 
theory provide deeper insight convergence improving mechanism place populations 
badly understood issue es theory concerns self adaptation sa 
seen performance real algorithm mainly large scale fluctuations endogenous strategy parameters 
investigation sa techniques analytically clever experiments high priority techniques currently need improved 
author hope initiate investigations direction 
acknowledgments result research performed author heisenberg fellow deutsche forschungsgemeinschaft dfg 
author grateful kalyanmoy deb irfan helpful comments 
editors special edition inviting contribute issue 
rechenberg 
evolutionsstrategie optimierung technischer systeme nach prinzipien der biologischen evolution 
frommann holzboog verlag stuttgart 

beyer 
theory evolution strategies asymptotical results theory 
evolutionary computation 
fitzpatrick grefenstette 
genetic algorithms noisy environments 
langley editor machine learning special issue genetic algorithms volume pages 
kluwer academic publishers dordrecht 
hammel back 
evolution strategies noisy functions 
improve convergence properties 
davidor manner 
schwefel editors parallel problem solving nature pages heidelberg 
springer verlag 
back hammel 
evolution strategies applied perturbed objective functions 
michalewicz schaffer 
schwefel fogel kitano editors proc 
ieee conf 
evolutionary computation pages 
ieee press piscataway nj 
angeline 
effects noise self adaptive evolutionary optimization 
fogel angeline back editors proceedings fifth annual conference evolutionary programming pages 
mit press cambridge ma 
fogel 
evolutionary computation 
ieee press new york 
kauffman 
adaptive walks noisy fitness measurements 
molecular diversity 
rana whitley 
searching presence noise 

voigt ebeling rechenberg 
schwefel editors parallel problem solving nature pages heidelberg 
springer 
goldberg rudnick 
genetic algorithms variance fitness 
complex systems 
goldberg 
genetic algorithms search optimization machine learning 
addison wesley reading ma 
goldberg deb clark :10.1.1.57.7053
genetic algorithms noise sizing populations 
complex systems 
harik cant paz miller goldberg 
gambler ruin problem genetic algorithms sizing populations 
proceedings ieee int conf 
evolutionary computation icec pages indianapolis 
ieee press piscataway nj 
miller 
noise sampling efficient genetic algorithms 
phd thesis university illinois urbana champaign urbana il 
illigal report 
miller goldberg 
genetic algorithms selection schemes varying effects noise 
evolutionary computation 
rattray 
modelling dynamics genetic algorithms statistical mechanics 
phd thesis university manchester cs department manchester uk 
rattray shapiro 
noisy fitness evaluation genetic algorithms dynamics learning 
belew vose editors foundations genetic algorithms 
morgan kaufmann san mateo ca 
ackley 
connectionist machine genetic hillclimbing 
kluwer academic publishers boston 
jaynes 
stand maximum entropy 
levine tribus editors maximum entropy formalism pages 
syswerda 
uniform crossover genetic algorithms 
schaffer editor proc 
rd int conf 
genetic algorithms pages san mateo ca 
morgan kaufmann 
rechenberg 

schneider editors der und pages 
springerverlag berlin 
rechenberg 
evolutionsstrategie 
frommann holzboog verlag stuttgart 

schwefel 
evolution optimum seeking 
wiley new york ny 

beyer 
theory evolution strategies self adaptation 
evolutionary computation 

beyer 
theory evolution strategies theory 
evolutionary computation 
back 
schwefel 
overview evolutionary algorithms parameter optimization 
evolutionary computation 

beyer 
theory evolution strategies benefit sex theory 
evolutionary computation 
arnold balakrishnan 
course order statistics 
wiley new york 

beyer 
alternative explanation manner genetic algorithms operate 
biosystems 
hans paul schwefel 
collective phenomena evolutionary systems 
kiss editors problems constancy change complementarity systems approaches complexity papers st annual meeting int soc 
general system research volume pages budapest 

int soc 
general system research 

beyer 
zur analyse der 
university dortmund 

beyer 
mutate large inherit small 
analysis rescaled mutations es noisy fitness data 
eiben back schoenauer 
schwefel editors parallel problem solving nature pages heidelberg 
springer 

beyer 
theory evolution strategies progress rates quality gain strategies nearly arbitrary fitness functions 
davidor manner 
schwefel editors parallel problem solving nature pages heidelberg 
springer 
blumer 
mathematical theory quantitative genetics 
clarendon press oxford 

