dynamic feedback effective technique adaptive computing presents dynamic feedback technique enables computations adapt dynamically different execution environments 
compiler uses dynamic feedback produces different versions source code version uses different optimization policy 
generated code alternately performs sampling phases production phases 
sampling phase measures overhead version current environment 
production phase uses version overhead previous sampling phase 
computation periodically resamples adjust dynamically changes environment 
implemented dynamic feedback context parallelizing compiler object programs 
generated code uses dynamic feedback automatically choose best synchronization optimization policy 
experimental results show synchronization optimization policy significant impact performance computation best policy varies program program compiler unable statically choose best policy dynamic feedback enables generated code exhibit performance comparable code manually tuned best policy 
performed theoretical analysis provides certain assumptions guaranteed optimality bound dynamic feedback relative hypothetical unrealizable optimal algorithm uses best policy point execution 
efficient implementation abstraction depends environment 
example best consistency protocol software distributed shared memory system depends access pattern parallel program 
best data distribution dense matrices distributed memory machines depends different parts program access matrices 
best concrete data structure implement data type depends 
best algorithm solve pedro sponsored praxis xxi program junta nacional de ao cient portugal holds fulbright travel 
martin rinard supported part alfred sloan research fellowship 
pedro martin rinard department computer science engineering building university california santa barbara santa barbara ca cs ucsb edu problem depends combination input hardware platform execute algorithm 
cases impossible statically choose best implementation best implementation depends information input data dynamic program characteristics hardware features difficult extract unavailable compile time 
programmer program characteristics currently faced unattractive alternatives manually tune program environment settle suboptimal performance 
developed new technique dynamic feedback enables programs automatically adapt different execution environments 
compiler uses dynamic feedback produces different versions code 
version uses different optimization policy 
generated code alternately performs sampling phases production phases 
sampling phase generated code measures overhead version current environment running version fixed time interval 
production phase uses version overhead previous sampling phase 
running production phase fixed time interval generated code performs sampling phase 
environment changed generated code dynamically adapts different version production phase 
see dynamic feedback part general trend adaptive computing 
complexity systems capabilities compilers increase compiler developers find automatically apply large range transformations way statically determining transformations deliver results program executed 
problem acute emergence new computing paradigms mobile programs internet 
extreme heterogeneity systems defeat implementation adapt different execution environments 
dynamic feedback example adaptive techniques enable compilers deliver performance modern computing systems 
describes dynamic feedback context parallelizing compiler object languages 
compiler generates parallel code uses synchronization constructs operations execute atomically 
experimental results show resulting synchronization overhead significantly degrade performance 
developed set synchronization transformations set synchronization optimization policies transformations reduce synchronization overhead 
unfortunately best policy different different programs may vary dynamically different parts program 
furthermore best policy depends infor mation global topology manipulated data structures dynamic execution schedule parallel tasks unavailable compile time 
compiler unable statically choose best synchronization optimization policy 
implemented compiler generates code uses dynamic feedback automatically choose best synchronization optimization policy 
experimental results show dynamic feedback enables automatically generated code exhibit performance comparable code manually tuned best policy 
contributions contributions presents technique dynamic feedback enables systems automatically evaluate different implementations source code evaluation choose best implementation current environment 
shows apply dynamic feedback context parallelizing compiler object programs 
generated code uses dynamic feedback automatically choose best synchronization optimization policy 
presents theoretical analysis characterizes worstcase performance systems dynamic feedback 
analysis provides certain assumptions guaranteed optimality bound dynamic feedback relative hypothetical unrealizable optimal algorithm uses best policy point execution 
presents experimental results automatically generated parallel code 
results show generated code exhibits performance comparable code manually tuned best synchronization optimization policy 
structure remainder structured follows 
section briefly summarize analysis technique commutativity analysis compiler 
section summarizes issues affect performance impact synchronization optimization algorithms 
section describes implementation details applying dynamic feedback problem choosing best synchronization policy 
section presents theoretical analysis 
section presents experimental results 
discuss related section conclude section 
commutativity analysis compiler uses commutativity analysis automatically parallelize serial object programs 
programs structure computation sequence operations objects 
compiler analyzes program granularity determine operations commute generate result regardless order execute 
operations computation commute compiler automatically generate parallel code 
code executes operations computation parallel 
experimental results indicate approach effectively parallelize irregular computations manipulate dynamic linked data structures trees graphs 
ensure operations execute atomically compiler augments object mutual exclusion lock 
automatically inserts synchronization constructs operations update objects 
operations acquire object lock perform update release lock 
synchronization constructs ensure operation executes atomically respect operations access object 
synchronization optimizations practice overhead generated synchronization constructs reduced performance 
developed synchronization optimization algorithms 
algorithms designed parallel programs generated compiler mutual exclusion locks implement critical regions 
critical region acquires mutual exclusion lock performs computation releases lock 
computations mutual exclusion locks may incur kinds overhead locking overhead waiting overhead 
locking overhead overhead generated execution constructs successfully acquire release lock 
waiting overhead overhead generated processor waits acquire lock held processor 
computation releases lock lock possible reduce locking overhead eliminating release acquire 
synchronization optimization algorithms statically detect computations repeatedly release reacquire lock 
apply lock elimination transformations eliminate intermediate release acquire constructs 
result computation acquires releases lock 
effect optimization coalesces multiple critical regions acquire release lock multiple times single larger critical region includes original critical regions 
larger critical region course acquires releases lock 
reduction number times computation acquires releases locks translates directly reduction locking overhead 
figures example synchronization optimizations reduce number executed acquire release constructs 
presents program inspired barnes hut benchmark described section uses mutual exclusion locks body interaction operations execute atomically 
presents program application synchronization optimization algorithm 
algorithm interprocedurally lifts acquire release constructs loop body interactions operation 
transformation reduces number times acquire release constructs executed 
overly aggressive synchronization optimization algorithm may introduce false exclusion 
false exclusion may occur processor holds lock extended period computation originally part critical region 
processor attempts execute critical region uses lock wait processor release lock processor executing computation needs critical region 
result increase waiting overhead 
excessive false exclusion reduces amount available concurrency turn decrease performance 
synchronization optimization algorithms mediate trade locking overhead waiting overhead 
transformations reduce locking overhead may increase waiting overhead vice versa 
synchronization optimization algorithms differ policies govern lock elimination transformation extern double interact double double class body private lock mutex double pos sum public void interaction body void interactions body int void body interaction body double val interact pos pos mutex acquire sum sum val mutex release void body interactions body int int interaction unoptimized example computation extern double interact double double class body private lock mutex double pos sum public void interaction body void interactions body int void body interaction body double val interact pos pos sum sum val void body interactions body int mutex acquire int interaction mutex release optimized example computation original apply transformation default placement acquire release constructs 
default placement operation updates object acquires releases object lock 
bounded apply transformation new critical region contain cycles call graph 
idea limit severity false exclusion limiting dynamic size critical region 
aggressive apply transformation 
general amount overhead depends complicated dynamic properties computation global topology manipulated data structures run time scheduling parallel tasks 
experimental results show synchronization optimizations large impact performance benchmark applications 
unfortunately best policy 
best policy depends information available compile time compiler unable statically choose best policy 
implementing dynamic feedback compiler generates code executes alternating sequence serial parallel sections 
parallel section generated code uses dynamic feedback automatically choose best synchronization optimization policy 
execution starts sampling phase continues production phase 
parallel section periodically resamples adapt changes best policy 
discuss specific issues associated implementing general approach 
detecting interval expiration obtain optimality results section generated code sampling phase execute policy fixed sampling time interval 
production phase execute fixed production time interval production intervals typically longer sampling intervals 
compiler uses values control lengths sampling production intervals target sampling interval target production interval 
start interval generated code reads timer obtain starting time 
executes code periodically polls timer reads timer computes difference current time starting time compares difference target interval 
comparison enables code detect interval expired 
implementation issues determine effectiveness approach potential switch points general possible switch policies specific potential switch points execution program 
rate potential switch points occur execution determines minimum polling rate turn determines quickly generated code responds expiration current interval 
benchmark applications parallel section executes parallel loop 
potential switch point occurs iteration loop generated code tests expiration current interval time completes iteration 
benchmark applications individual iterations loops small processor respond reasonably quickly expiration interval 
polling overhead polling overhead determined large part overhead reading timer 
currently generated code uses timer stanford dash machine 
overhead accessing timer approximately microseconds negligible compared sizes iterations parallel loops benchmark applications 
synchronous switching generated code switches policies synchronously 
interval expires processor waits barrier processors detect interval expired arrive barrier 
strategy ensures processors policy sampling interval 
measured overhead accurately reflects overhead policy 
synchronous switching avoids possibility interference incompatible policies 
potential drawback synchronous switching processor wait processors detect expiration current interval proceed interval 
effect significant negative impact performance iterations parallel loop executes long time relative iterations sampling interval 
combination especially bad policy example synchronization optimization policy serializes computation iterations loop execute significant time relative sampling interval cause poor performance 
timer precision precision timer places lower bound size interval 
timer tick interval expires 
general expect precision timer cause problems 
generated code uses target sampling intervals milliseconds length 
systems provide timers resolution 
issues combine determine effective sampling interval minimum time start interval time processors detect interval expired proceed interval 
theoretical analysis section formulates optimality results terms effective sampling interval 
switching policies sampling phase generated code switch quickly different synchronization optimization policies 
current compiler generates versions parallel section code 
version uses different synchronization optimization policy 
advantage approach code policy available enables compiler switch quickly different policies 
currently generated code simply executes switch statement parallel loop iteration dispatch code implements current policy 
potential disadvantage increase size generated code 
table presents sizes text segments different versions benchmark applications 
potential problem arise dynamic feedback choose best synchronization optimization policy 
synchronization optimization policies compatible possible concurrently execute different versions affecting correctness computation 
expect applications dynamic feedback different policies may incompatible concurrent execution different versions may cause computation execute incorrectly 
data object files compiled applications linking include code applications code libraries 
serial version original serial program original version uses original synchronization optimization policy dynamic version uses dynamic feedback 
general increases code size quite small 
due part algorithm compiler locates closed subgraphs call graph optimization policies 
compiler generates single version method subgraph version synchronization optimization policy 
application version size bytes serial barnes hut original dynamic serial water original dynamic serial string original dynamic table executable code sizes bytes considered dynamic compilation produce different versions parallel sections required 
approach reduce amount code point time significantly increase amount time required switch policies sampling phases 
alternative viable situations sampling phases significantly longer set benchmark applications tolerate 
possible compiler generate single version code synchronization optimization policies 
idea generate conditional acquire release construct sites may acquire release lock synchronization optimization policies 
site flag controls executes construct acquire release site tests flag determine acquire release lock 
scenario generated code switches policies changing values flags 
advantage approach guarantee code growth disadvantage residual flag checking overhead conditional acquire release site 
measuring overhead choose policy overhead generated code measure overhead 
compiler instruments code collect measurements locking overhead generated code computes locking overhead counting number times computation acquires releases lock 
number computed incrementing counter time computation acquires lock 
locking overhead simply time required acquire release lock times number times computation acquires lock 
waiting overhead current implementation uses spin locks 
hardware exports construct allows computation attempt acquire lock return value indicates lock acquired 
acquire lock computation repeatedly executes hardware lock acquire construct attempted acquire succeeds 
computation increments counter time attempt acquire lock fails 
waiting overhead time required attempt fail acquire lock times number failed acquires 
execution time amount time computation spends executing code application 
time measured reading timer processor starts execute application code reading timer processor finishes executing application code 
processor subtracts time second time adds difference running sum 
measured execution time includes waiting time time spent acquiring releasing locks 
possible subtract sources overhead obtain amount time spent performing useful computation 
measurements allow compiler evaluate total overhead synchronization optimization policy 
total overhead simply lock overhead plus waiting overhead divided execution time 
total overhead zero 
compiler uses total overhead choose best synchronization optimization policy policy lowest overhead best 
potential concern instrumentation overhead 
experimental results indicate overhead little effect performance 
measure overhead generating versions applications single statically chosen synchronization optimization policy 
execute versions instrumentation turned instrumentation turned 
performance differences instrumented uninstrumented versions small indicates instrumentation overhead little impact performance 
choosing sampling production intervals sizes target sampling production intervals significant impact performance generated code 
excessively long sampling intervals may degrade performance executing non optimal versions code long time 
sampling interval short may yield accurate measurement overhead 
worst case inaccurate overhead measurement may cause production phase wrong synchronization optimization policy 
expect minimum absolute length sampling interval different different applications 
practice little difficulty choosing default values applications 
fact possible target sampling intervals small applications minimum effective sampling intervals large provide overhead measurements accurately reflect relative overheads production phases 
achieve performance production phase long profitably amortize cost sampling phase 
practice major component sampling cost time spent executing non optimal versions 
section presents theoretical analysis characterizes long production phase relative sampling phase achieve optimality result 
current implementation dynamic feedback length parallel section may limit performance 
current implementation executes sampling phase parallel section 
parallel section contain computation production phase desired length computation may unable successfully amortize sampling overhead 
possible eliminate potential problem generating code allows sampling production intervals span multiple executions parallel phase 
code maintain separate sampling production intervals parallel section allow intervals contain multiple executions section 
practice little difficulty choosing target production intervals applications 
applications perform target production intervals range seconds 
early cut policy ordering cases expect individual sources overhead monotonically nondecreasing monotonically nonincreasing set possible implementations 
locking overhead example increases policy goes original bounded aggressive 
waiting overhead hand decrease policy goes original bounded aggressive 
properties suggest early cut limit number sampled policies 
aggressive policy generates little waiting overhead original policy generates little locking overhead need sample policy 
may possible improve sampling phase trying extreme policies going directly production phase overhead measurements indicate policy significantly better 
may possible improve sampling phase ordering policies 
generated code sample policy done past 
measured overhead continued acceptable generated code go directly production phase 
theoretical analysis section theoretical analysis worst case performance dynamic feedback 
compare dynamic feedback hypothetical unrealizable algorithm uses best policy 
start observing constraint fast overhead policy may change impossible obtain meaningful optimality result sampling algorithm overhead policy may change dramatically right sampling phase 
impose constraint changes overheads different policies bounded exponential decay function 
assume values measured sampling phase accurately reflect actual overheads start production phase 
assume production phase executes completion possible relax assumption 
worst case dynamic feedback algorithm relative optimal algorithm occurs policy lowest overhead sampling phase 
case dynamic feedback algorithm arbitrarily select sampled policies lowest overhead production phase 
maximum difference performance dynamic feedback algorithm performance optimal algorithm occurs overhead selected policy increases maximum bounded rate overheads policies decrease maximum bounded rate 
analyze scenario derive conservative bound worst case performance dynamic feedback algorithm relative optimal algorithm 
establish notation 
variable effective sampling interval length production interval pn different policies 
computation starts sampling phase 
phase dynamic feedback algorithm executes policies sampling interval derive overhead measurements vn policies 
overhead proportion total execution time spent executing lock constructs waiting processors release locks 
overhead varies zero computation executes lock construct computation performs useful 
worst case multiple policies overhead sampling phase lowest sampled overhead 
loss generality assume dynamic feedback algorithm executes policy production interval 
assume time production phase policy overheads bounded exponential decay function rate decay 
worst case overhead function policy hits bound define amount useful performed policy pi period time oi dt dynamic feedback algorithm performs amount production phase worst case analysis conservatively assumes useful takes place sampling phase 
dynamic feedback algorithm total amount useful performed sn units time sampling production phases 
turn attention optimal algorithm 
begins executing multiple policies lowest overhead assume time time units policy overheads bounded exponential decay function worst case overhead function policies say policy hits bound case optimal algorithm execute policy time units total useful performed interval conservative assumption sn time units optimal algorithm executes policy overhead words performs sn units time period 
compare amounts performed worstcase dynamic feedback algorithm best case optimal algorithm time period sn sn sn sn discuss conditions obtain guaranteed performance bound dynamic feedback algorithm relative optimal algorithm 
start defining precise way compare policies definition policy pi worse policy pj time interval decay rate effective sampling interval number policies desired performance bound definition yields inequality determines possible choose production interval dynamic feedback algorithm guaranteed worse optimal algorithm 
inequality characterizes values guaranteed deliver desired performance 
sn conceptually things happening inequality 
production interval long successfully amortize sampling time sn 
second production interval short dynamic feedback algorithm detects policy overhead changes quickly avoid executing inefficient policy long time 
words inequality bounds 
decay rate small dynamic feedback algorithm perform relative optimal algorithm obtain bound 
cases impossible choose satisfies conditions 
possible choose inequality identifies feasible region guaranteed satisfy conditions 
graphically illustrates range feasible values production interval example values 
inequality pro constraint values feasible region exp production interval seconds feasible region production interval vides insight various relationships 
increases range feasible values increases 
increases range feasible values decreases 
show determine optimal value worst case assumptions 
define optimal value value minimizes worst case difference performed unit time optimal dynamic feedback algorithms 
equation defines difference 
sn sn sn sn sn finding root derivative solving yields equation 
value satisfies equation optimal value possible numerical methods solve 
sn example values optimal value 
experimental results section presents experimental results characterize dynamic feedback works benchmark applications 
applications barnes hut hierarchical body solver water simulates water molecules liquid state string builds velocity model geology oil wells 
application serial program performs computation interest scientific computing community 
barnes hut consists approximately lines code water consists approximately lines code string consists approximately lines code 
prototype compiler parallelize application 
parallelization completely automatic programs contain pragmas annotations compiler performs necessary analyses transformations 
compare performance impact different synchronization optimization policies compiler flags obtain different versions application 
version uses original policy uses bounded policy uses aggressive policy final version uses dynamic feedback 
report results applications running processor stanford dash machine running modified version irix operating system 
programs compiled irix cc compiler optimization level 
barnes hut table presents execution times different versions barnes hut 
presents corresponding speedup curves 
experimental results input data set bodies 
static versions original bounded aggressive execute instrumentation required compute locking waiting overhead 
dynamic version version uses dynamic feedback contain instrumentation uses locking waiting overhead measurements determine best synchronization optimization policy 
version processors serial original bounded aggressive dynamic table execution times barnes hut seconds strictly speaking dynamic version needs execute instrumented code sampling phase 
instrumentation overhead significantly affect performance production phase simply executes instrumented code best version previous sampling phase 
approach inhibits code growth eliminating need generate instrumented uninstrumented versions code 
speedup aggressive dynamic feedback bounded original number processors speedups barnes hut application synchronization optimization policy significant impact performance aggressive version significantly outperforming original bounded versions 
performance dynamic version quite close aggressive version 
table presents locking overhead different versions barnes hut 
execution times correlated locking overhead 
versions dynamic number executed acquire release constructs locking overhead vary number processors varies 
dynamic version number executed acquire release constructs increases slightly number processors increases 
numbers table dynamic version processor run 
version executed acquire absolute locking release pairs overhead seconds original bounded aggressive dynamic table locking overhead barnes hut absolute performance varies synchronization optimization policy performance different versions scales approximately rate 
indicates synchronization optimizations introduced significant false exclusion 
reason application exhibit perfect speedup compiler unable parallelize section computation 
large numbers processors serial execution section bottleneck 
investigate overheads different policies change time produced version application small target sampling production intervals 
instrumented version print measured overhead sampling interval 
presents data processor run form time series graph main computationally intensive parallel section forces section 
benchmark executes forces section times 
gap time series lines corresponds execution serial section code 
shows measured overheads stay relatively stable time 
sampled overhead original bounded aggressive execution time seconds sampled overhead barnes hut forces section processors discuss characteristics application relate minimum effective sampling interval forces section 
computation section consists single parallel loop 
table presents mean section size number iterations parallel loop mean iteration size 
mean section size mean execution time forces section serial version intended measure amount useful section 
generated code checks expiration sampling production intervals granularity loop iterations sizes loop iterations important impact size minimum effective sampling interval 
mean section size number iterations mean iteration size seconds milliseconds table statistics barnes hut forces section version small target sampling production intervals measure minimum effective sampling intervals different synchronization optimization policies 
version sampling production intervals small possible application characteristics words actual intervals length minimum effective sampling intervals 
instrumented version measure length actual sampling interval data compute mean minimum effective sampling interval policy 
table presents data processor run 
expected mean minimum effective sampling intervals larger roughly comparable size mean loop iteration size 
differences mean minimum effective sampling intervals correlated differences lock overhead 
lock overhead increases amount time required execute iteration increases 
versions significant waiting overhead increases amount time required execute iteration translate directly increases mean minimum effective sampling interval 
consider impact varying target sampling production intervals 
performance numbers table target sampling interval set milliseconds target production interval set seconds 
target sampling interval small ensure minimum effective sampling interval target sampling interval deter version mean minimum effective sampling interval milliseconds original bounded aggressive table mean minimum effective sampling intervals barnes hut forces section processors mined length actual sampling interval 
target production interval long ensure parallel section finished executed sampling phase 
execution parallel section consisted sampling phase production phase 
table presents mean execution times forces section running processors combinations target sampling production intervals 
performance relatively insensitive variation target sampling production intervals 
target sampling production intervals identical means computation spends approximately times long sampling phase production phase section runs approximately slower best combination 
target sampling target production interval interval second seconds seconds seconds seconds seconds seconds table mean execution times varying production sampling intervals barnes hut forces section processors seconds water table presents execution times different versions water 
presents corresponding speedup curves 
experimental results input data set molecules 
static versions original bounded aggressive execute instrumentation required compute locking waiting overhead 
dynamic version needs instrumentation apply dynamic feedback algorithm version contains instrumentation 
version processors serial original bounded aggressive dynamic table execution times water seconds application synchronization optimization policy significant impact performance 
processor aggressive version performs best 
number processors increases aggressive version fails scale bounded version outperforms aggressive original versions 
performance results speedup bounded dynamic feedback original aggressive number processors speedups water indicate false exclusion causes poor performance aggressive version 
performance dynamic version close performance bounded version exhibits best performance 
table presents locking overhead different versions water 
original bounded dynamic versions execution times correlated locking overhead 
versions dynamic number executed acquire release constructs locking overhead vary number processors varies 
dynamic version processors number executed acquire release constructs close bounded version slight increase number processors increases 
processor dynamic version executes approximately number acquire release constructs aggressive version 
numbers table dynamic version processor run 
version executed acquire absolute locking release pairs overhead seconds original bounded aggressive dynamic table locking overhead water instrumented parallel code determine water exhibit perfect speedup 
presents waiting proportion proportion time spent waiting overhead 
data collected program counter sampling profile execution 
clearly shows waiting overhead primary cause performance loss application aggressive synchronization optimization policy generates false exclusion severely degrade performance 
water computationally intensive parallel sections interf section section 
figures precisely waiting proportion sum processors amount time processor spends waiting acquire lock held processor divided execution time program times number processors executing computation 
sampled overhead waiting proportion aggressive bounded original number processors waiting proportion water execution time seconds original bounded sampled overhead water interf section processors sampled overhead aggressive original execution time seconds sampled overhead water section processors time series graphs measured overheads different synchronization optimization policies 
interf section generated code bounded aggressive policies 
compiler generate aggressive version sampling phases execute original bounded versions 
similar situation occurs section case code original bounded versions 
barnes hut overheads relatively stable time 
gaps time series graphs correspond executions serial parallel sections 
tables parallel section statistics sections 
tables mean minimum effective sampling intervals sections 
expected mean minimum effective sampling intervals versions aggressive version section larger roughly comparable corresponding mean iteration sizes 
mean minimum effective sampling interval aggressive version section significantly larger original version 
attribute difference fact aggressive policy serializes computation described section increases effective sampling interval 
mean section size number iterations mean iteration size seconds milliseconds table statistics water interf section mean section size number iterations mean iteration size seconds milliseconds table statistics water section version mean minimum effective sampling interval milliseconds original bounded table mean minimum effective sampling intervals water interf section processors version mean minimum effective sampling interval milliseconds original aggressive table mean minimum effective sampling intervals water section performance numbers table target sampling interval set milliseconds target production interval set seconds 
combination ensured execution parallel section consisted sampling phase production phase 
tables execution times interf sections running processors combinations target sampling production intervals 
interf section combinations yield approximately performance 
attribute uniformity fact performance versions section original bounded versions dramatically different 
target production intervals seconds performance section quite sensitive choice target sampling interval 
dramatic difference section performance aggressive original versions 
case intuitively expect performance target sampling target production interval interval second seconds seconds seconds seconds seconds seconds table mean execution times varying production sampling intervals water interf section processors seconds target sampling target production interval interval second seconds seconds seconds seconds seconds seconds table mean execution times varying production sampling intervals water section processors seconds increase increases target production interval decrease increases target sampling interval 
address ways data fail conform expectation 
execution times virtually identical target production intervals seconds 
attribute uniformity fact execution section terminates seconds 
extending target production interval seconds effect execution 
second execution times virtually identical target production interval seconds target sampling intervals seconds 
attribute data fact execution section terminates seconds fact minimum effective sampling interval aggressive policy greater seconds 
executions question consist aggressive sampling interval length executions original sampling interval original production interval section completes execution 
executions spend identical amounts time executing aggressive original versions 
execution time decreases target production interval seconds target sampling interval increases seconds seconds 
effect caused fact minimum effective sampling interval original version smaller seconds minimum effective sampling interval aggressive version larger seconds 
program spends larger proportion sampling phase executing efficient original version target sampling interval seconds target sampling interval seconds 
effect associated section exacerbates performance impact 
target sampling interval seconds section completes sampling phases production phases 
target sampling interval seconds section performs computation original sampling intervals complete executed third aggressive sampling interval 
net effect increase target sampling interval significant reduction amount time spent executing inefficient aggressive sampling intervals 
string string bounded policy produces parallel code original policy 
report performance results original aggressive dynamic policies 
table presents execution times different versions string 
presents corresponding speedup curves 
experimental results big input data set 
static versions original aggressive execute instrumentation required compute locking waiting overhead dynamic version includes instrumentation 
version processors serial original aggressive dynamic table execution times string seconds speedup original dynamic feedback aggressive number processors speedups string string aggressive policy completely serializes computation 
version fails scale 
execution time dynamic version comparable execution time original version small loss performance processors 
table presents locking overhead different versions string 
dynamic version processors number executed acquire release constructs slightly original version 
number increases slightly number processors increases 
processor dynamic version executes approximately times fewer acquire release constructs original version 
numbers table dynamic version processor run 
instrumented parallel code determine string exhibit perfect speedup 
presents waiting proportion 
clearly shows waiting overhead primary cause performance loss application aggressive synchronization optimization policy generates false exclusion serialize computation 
presents time series graphs measured overheads different synchronization optimization policies sampled overhead version executed acquire absolute locking release pairs overhead seconds original aggressive dynamic table locking overhead string waiting proportion aggressive original number processors waiting proportion string execution time seconds aggressive original sampled overhead string section processors mean section size number iterations mean iteration size seconds milliseconds table statistics string section version mean minimum effective sampling interval original milliseconds aggressive milliseconds table mean minimum effective sampling intervals string section main computationally intensive parallel section section 
collected data setting target sampling production intervals second instrumenting code print measured overhead sampling interval 
barnes hut water overheads relatively stable time 
gaps time series graphs correspond executions serial parallel sections 
table presents parallel section statistics pro section 
table presents mean minimum effective sampling intervals 
mean minimum effective sampling interval original version larger roughly comparable iteration size 
section water aggressive version significantly larger original version 
reason aggressive version serializes computation 
performance numbers table target sampling interval set milliseconds target production interval set seconds 
combination ensured execution parallel section consisted sampling phase production phase 
table presents execution times section running processors combinations target sampling production intervals 
expected section dramatic efficiency differences versions performance increases increases target production interval decreases increases target sampling interval 
target sampling target production interval interval second seconds seconds seconds seconds seconds seconds table mean execution times varying production sampling intervals string section processors seconds discussion application best static synchronization optimization policy different applications 
furthermore performance differences significant processors best version barnes hut approximately faster worst water best times faster worst string best times faster worst 
cases dynamic feedback allows dynamic version exhibit performance close best static policy better best static policy 
compiler automatically generate robust code performs variety environments eliminates need programmer manually tune program best synchronization optimization policy 
related researchers developed systems collect information dynamic characteristics programs information improve performance 
discuss approaches profiling dynamic type feedback techniques improving performance object oriented languages adaptive execution techniques dynamic techniques parallelizing loops 
discuss dynamic compilation efficient implementations parallel function calls related synchronization optimization 
profiling profiling standard way obtain information dynamic characteristics program 
approach program instrumented executed collect profiling data 
program recompiled profiling data guide policy decisions compiler 
profiling context object oriented languages predict frequently occurring class receiver object call site 
information drive optimizations inline methods predictions class receiver 
profiling guide decisions inline procedures programs drive instruction scheduling algorithms help place code minimize impact memory hierarchy aid register allocation direct compiler frequently executed parts program compiler apply optimizations 
brewer describes system uses statistical modeling automatically predict algorithm best combination input hardware platform 
different algorithms implemented hand automatically generated single specification 
system uses profiling characterize performance different algorithms different hardware platforms 
dynamic feedback differs profile feedback adapt dynamically current execution environment hoping environment similar environment profiling run program 
dynamic feedback adjust changes occur single execution 
profile approaches collect single aggregate set measurements entire execution environment changes take place single execution 
adaptive execution techniques researchers recognized need dynamic performance data optimize execution 
approaches set control variables parameterize algorithm implementation 
example control variable prefetch distance algorithm prefetches data accessed loop 
typically programmer defines set observable variables feedback function uses observable variables produce values control variables 
changes values observable variables propagate feedback function change control variables program responds modifying behavior 
ideally observable variables control variables feedback function defined program maximizes performance range dynamic environments 
dynamic feedback similar spirit approaches important difference 
dynamic feedback general technique designed choose discrete potentially quite different implementations 
approaches designed tune control variables context single algorithm 
dynamic dispatch optimizations object oriented languages method invoked call site depends dynamic class receiver object 
call site may invoke different methods algorithm determines method invoke called dynamic dispatch algorithm 
researchers proposed adaptive optimizations improving efficiency dynamic dispatch 
standard mechanism collect data indicates methods tend invoked call sites insert type test checks common types 
dynamic type feedback designed direct compiler attention parts program benefit optimization 
method optimized generated code continues collect data drive optimizations reverse poor implementation choices 
sense dynamic feedback similar dynamic type feedback techniques generate code dynamically adapts execution environment 
run time analysis speculative execution certain circumstances lack statically available information may prevent compiler parallelizing program 
systems address problem parallelizing programs dynamically information available program runs 
inspector executor approach dynamically analyzes values index arrays automatically parallelize computations access irregular meshes 
jade implementation dynamically analyzes tasks access data exploit concurrency coarse grain parallel programs 
speculative approaches optimistically execute loops parallel rolling back computation parallel execution violates data dependences 
major difference dynamic feedback runtime techniques dynamic feedback designed automatically choose implementations deliver functionality 
implementation equally valid may perform best current environment 
runtime techniques goal clearly parallelize computation compiler simply lacks information necessary 
postpone decision apply optimization run time information available 
dynamic compilation dynamic compilation systems enable generation code run time 
delaying compilation run time provides compiler information concrete values input parameters compiler may able generate efficient code 
existing research focused providing efficient mechanisms dynamic compilation 
see dynamic compilation way generate different implementations dynamic feedback samples find best implementation 
advantage elimination potential code growth memory hold generated code deallocated code executed significant period time 
compiler dynamically regenerate code dynamic feedback algorithm needs sample performance 
major drawback overhead required perform compilation dynamically 
overhead concern program executed sampling phases infrequently dynamic compilation overhead amortized away long production phases 
synchronization optimizations applies dynamic feedback problem choosing best synchronization granularity 
previous research produced analyses transformations reducing synchronization overhead different synchronization optimization policies 
plevyak zhang chien developed similar synchronization optimization technique access region expansion concurrent object oriented programs 
access region expansion designed reduce overhead sequential executions programs address trade lock overhead waiting overhead 
goal simply minimize lock overhead 
parallel function calls researchers developed efficient implementations parallel function calls 
implementations dynamically match amount exploited parallelism amount parallelism available parallel hardware platform selecting efficient sequential call full parallel call 
selection dynamic measure difference currently exploited available amounts parallelism 
presents new technique dynamic feedback enables computations adapt dynamically different execution environments 
compiler uses dynamic feedback produces different versions source code version uses different optimization policy 
dynamic feedback automatically chooses efficient version periodically sampling performance different versions 
implemented dynamic feedback context parallelizing compiler object programs 
generated code uses dynamic feedback automatically choose best synchronization policy 
experimental results show dynamic feedback enables compiler automatically generate code exhibits performance comparable code manually tuned best synchronization optimization policy 
see dynamic feedback part general trend adaptive computing 
complexity systems capabilities compilers increase compiler developers find automatically apply large range transformations way statically determining transformations deliver results program executed 
problem acute emergence new computing paradigms mobile programs internet 
extreme heterogeneity systems defeat implementation adapt different execution environments 
dynamic feedback example adaptive techniques enable compilers deliver performance modern computing systems 
amarasinghe lam 
communication optimization code generation distributed memory machines 
proceedings sigplan conference programming language design implementation sigplan notices 
acm july 
anderson lam 
global optimizations parallelism locality scalable parallel machines 
proceedings sig plan conference programming language design implementation sigplan notices 
acm july 
auslander philipose chambers eggers bershad 
fast effective dynamic compilation 
proceedings sigplan conference program language design implementation philadelphia pa may 
barnes hut 
hierarchical nlogn force calculation algorithm 
nature pages december 
brewer 
high level optimization automated statistical modeling 
proceedings fifth acm sigplan symposium principles practice parallel programming santa barbara ca july 
chambers ungar 
customization optimizing compiler technology self dynamically typed object oriented programming language 
proceedings sigplan conference program language design implementation portland june 
chang mahlke chen hwu 
profile guided automatic inline expansion programs 
software practice experience may 
chen mahlke warter hwu 
profile assisted instruction scheduling 
international journal parallel programming april 
cox fowler 
adaptive cache coherency detecting migratory shared data 
proceedings th international symposium computer architecture may 
rinard 
synchronization transformations parallel computing 
proceedings fourth annual acm symposium principles programming languages paris france january 
acm 
engler 
vcode retargetable extensible fast dynamic code generation system 
proceedings sigplan conference program language design implementation philadelphia pa may 
lebeck reinhardt hill larus rogers wood 
application specific protocols user level shared memory 
proceedings supercomputing november 
fernandez 
simple effective link time optimization modula programs 
proceedings sigplan conference program language design implementation la jolla ca june 
freudenberger schwartz sharir 
experience setl optimizer 
acm transactions programming languages systems january 
goldstein schauser culler 
lazy threads implementing fast parallel call 
journal parallel distributed computing august 
graham kessler mckusick 
gprof call graph execution profiler 
proceedings sigplan symposium compiler construction boston ma june 
grove dean garret chambers 
profile guided receiver class prediction 
proceedings tenth annual conference object oriented programming systems languages applications austin tx october 
gupta banerjee 
demonstration automatic data partitioning techniques parallelizing compilers multicomputers 
ieee transactions parallel distributed systems march 
harris michelena 
tomographic string inversion 
th annual international meeting society exploration geophysics extended abstracts pages 
holzle ungar 
optimizing dynamically dispatched calls run time type feedback 
proceedings sigplan conference program language design implementation orlando fl june 
kennedy kremer 
automatic data layout high performance fortran 
proceedings supercomputing san diego ca november 
kiczales 
black box open implementation 
ieee software january 
knuth 
empirical study fortran programs 
software practice experience 
lee leone 
optimizing ml run time code generation 
proceedings sigplan conference program language design implementation philadelphia pa may 
lenoski 
design analysis dash scalable directory multiprocessor 
phd thesis stanford ca february 
leung zahorjan 
improving performance runtime parallelization 
proceedings fourth acm sigplan symposium principles practice parallel programming pages san diego ca may 
mohr kranz halstead 
lazy task creation technique increasing granularity parallel programs 
proceedings acm conference lisp functional programming pages june 
morris 
ccg prototype code generator 
proceedings sigplan conference program language design implementation toronto canada june 
pettis hansen 
profile guided code positioning 
proceedings sigplan conference program language design implementation white plains ny june 
plevyak karamcheti zhang chien 
hybrid execution model fine grained languages distributed memory multicomputers 
proceedings supercomputing san diego ca november 
plevyak zhang chien 
obtaining sequential efficiency concurrent object oriented languages 
proceedings second annual acm symposium principles programming languages 
acm january 
rauchwerger padua 
lrpd test speculative run time parallelization loops privatization reduction parallelization 
proceedings sigplan conference program language design implementation la jolla ca june 
rinard 
commutativity analysis new analysis framework parallelizing compilers 
proceedings sigplan conference program language design implementation philadelphia pa may 
www cs ucsb edu martin pldi ps 
rinard scales lam 
heterogeneous parallel programming jade 
proceedings supercomputing pages november 
romer lee bershad chen 
dynamic page mapping policies cache conflict resolution standard hardware 
proceedings usenix symposium operating systems design implementation pages monterey ca november 
saavedra park 
improving effectiveness software prefetching adaptive execution 
proceedings conference parallel algorithms compilation techniques pact boston ma october 
saltz wu 
multiprocessors run time compilation 
concurrency practice experience december 
singh weber gupta 
splash stanford parallel applications shared memory 
computer architecture news march 
wall 
global register allocation link time 
proceedings sigplan symposium compiler construction 
acm june 
