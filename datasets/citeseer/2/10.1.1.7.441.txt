robustness regularized linear classification methods text categorization jian zhang school computer science carnegie mellon university forbes avenue pittsburgh pa jian zhang cs cmu edu yiming yang school computer science carnegie mellon university forbes avenue pittsburgh pa yiming cs cmu edu real world applications require classi cation documents situations small number features mis labeled documents rare positive examples 
investigates robustness regularized linear classi cation methods svm ridge regression logistic regression situations 
compare methods terms loss functions score distributions establish connection optimization problems generalization error bounds 
sets controlled experiments reuters corpus conducted investigate robustness methods 
results show ridge regression promising candidate rare class problems 
categories subject descriptors information systems applications miscellaneous pattern recognition models statistical pattern recognition applications text processing general terms algorithms performance reliability keywords robustness text categorization svm ridge regression logistic regression 
supervised learning methods applied text categorization including nearest neighbor classi ers produces permission block copyright information sig alternate cls 
supported acm 
permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
sigir july august toronto canada 
copyright acm 
decision trees bayesian probabilistic classi ers neural networks regression methods svm lots empirical studies text categorization done years investigate di erent aspects classi cation methods 
text categorization problems characterized dealing high dimensional sparse data usually accompanied distributed categories 
characteristics text categorization di erent classic pattern classi cation problems 
real world applications require classi cation documents conditions 
restrictions space time classi ers need space trained faster fewer features 
vectorized les need stored reused reduce storage test time signi cantly expensive part load test data 

mis labeled training documents crucial resources classi ers training documents labeled human 
cases mis labeled documents candidate classi ers tolerant labeling errors degree 

small number training documents important applications small number positive training examples available ltering task information retrieval 
candidate classi ers perform reasonably rare positive training data 
hastie gave insightful analysis robustness classi ers loss functions 
goodness analysis depends intrinsic characteristics data 
knowledge study text categorization done 
yang liu compared empirical results common rare categories 
showed performance degradation size positive data decrease exploration results comparable categories 
mainly zhang studies regularized linear classi cation methods applications text categorization 
mainly focus addressing issues study classi ers text categorization conditions small number features noisy settings mis labeled data rare positive data 
design sets controlled experiments investigate behaviors regularized linear methods ridge regression regularized logistic regression linear kernel svm conditions establish connection optimization problems generalization error bounds 
analyze di erent behaviors case rare positive data reveals property loss functions 
rest organized follows linear methods regularizations introduced section 
section discusses methods terms loss functions score distributions generalization error bounds implementations 
experimental setup reported section results reported section compare methods conditions give analysis 
section summarizes main results 

popular classi cation methods linear classi cation methods methods linear decision boundaries positive negative classes linear regression logistic regression linear kernel svm naive bayes classi er linear discriminant analysis perceptron algorithm compared methods linear methods simpler trained model easier interpret 
important text categorization shown ective performances top classi ers 
de ne binary classi cation problem linear methods follows 

xn yn training set vector training instance 
linear classi er tries nd weight vector intercept threshold label label function speci ed linear classi er 
augment input vector weight vector absorb intercept linear model task speci loss function goal minimize expected loss mined underlying unknown distribution data 
classi cation problems loss function usually convex upper bound classi cation error avoids hardness directly minimizing classi cation error 
order model empirical risk minimization erm usually tries minimize objective function empirical data min see erm uses uniform distribution empirical data replace unknown distribution 
empirical loss pure objective erm erm may data favoring complex functions 
regularization ective way prevent tting successfully applied methods 
regularized version usually looks min controls learning complexity hypothesis family coecient controls balance model complexity empirical loss 
linear svm svm statistical learning theory uses principle structural risk minimization empirical risk minimization 
regarded high performance classi ers domains including text categorization 
limit discussion linear kernel svm popular kernel text categorization computationally cheaper kernels 
linear svm tries nd hyperplane maximum margin decision boundary linear separable case equivalent minimizing norm weight vector linear constraints minw subject linear non separable case introducing slack variables optimization problem augmented minw subject represents cost coecient slack variable measures far away corresponding data point falls wrong side margin 
dual form optimization problem written follows max subject theoretically practically meaningful 
karush kuhn tucker kkt conditions know data points lagrangian coecients zeros contribute nal decision boundary data points called support vectors remaining data points called vectors 
text categorization people usually small portion training data points support vectors 
note svm treated regularized method loss function rewritten arg min max max augmented vector svm people penalize intercept svm 
constraints svm dual form changed algorithm 
penalizing intercept bring trivial modi cations ridge regression logistic regression 
linear regression regularization problem linear regression tries nd linear function training data 
squares algorithm popular estimation method linear regression equivalent maximum likelihood estimation uenced gaussian noise 
squares algorithm computes weight vector minimization squared loss model output arg min arg min solution give close form solution matrix singular means multiple minimizers objective function 
particularly text categorization number features larger number training documents matrix singular cases 
solution pseudo inverse matrix built top computation singular value decomposition svd 
solution ridge regression original objective function adding discuss case objective function arg min wg close form solution note transformation matrix guaranteed non singular provided 
result unique solution ridge regression optimization problem solved simple algorithm 
logistic regression regularization logistic regression widely statistics years received extensive studies machine learning due close relation svm adaboost 
text categorization widely squares algorithm svm 
method applied text categorization compared linear classi cation methods shows performance comparable svm 
logistic regression tries model conditional probability yjx model tted maximizing conditional log likelihood 
binary classi cation problems conditional probability modeled exp yw maximum likelihood estimation equivalent minimizing arg min log exp solution problem may nite suppose training data linear separable separating weight vector 
weight vector provided separate training data smaller objective function value 
solution unbounded 
order solve problem resort regularized version arg min log exp wg hessian matrix objective function de ned exp exp identity matrix 
straight forward show hessian matrix positive de nite equivalent convexity objective function 
regularization hessian matrix bounded away nice property numerical algorithms 

analysis loss functions distributions previous discussions unify objective functions linear classi cation algorithms see methods norm regularization term di erence method associated particular loss function 
ridge regression 
regularized logistic regression log exp 
linear svm maxf meaningful compare loss functions mis classi cation error help understand di erent behaviors methods 
density logistic regression positive data negative data density svm positive data negative data ridge regression positive data negative data distributions positive negative data test data feature size mis classification ridge regression svm logistic regression yw mis classification svm logistic regression ridge regression loss functions see graphs simply derive formulas loss functions convex functions upper bounds mis classi cation error 
believed squared loss robust loss functions loss function uenced extreme data see graph 
disadvantage squared loss penalizes correctly classi ed data points output values larger 
property rst second derivatives objective function behaved solved simple optimization techniques 
svm loss linear logistic loss close linear 
loss functions sensitive extreme data points compared squared loss 
non di erentiable characteristic svm loss function harder solve methods 
show ective loss functions plot score distributions methods test data gure 
see dense distributions loss close zero shows data linear separable great degree 
notice overlapping ridge regression small compared methods consistent performance reported section 
generalization error rst refer theorems vapnik theorem suppose set learning functions binary classi cation adjustable parameters bound holds probability remp log log df expected risk remp empirical risk size training data vc dimension learning model mis classi cation error loss underlying data distribution unknown 
theorem subset separating hyperplane de ned 
xn yn satisfying vc dimension bounded min theorems know linear separable data shrink hypothesis space putting limit norm weight vector linear classi cation methods potentially reduce vc dimension reduce generalization error bound 
hand know optimization problem minf subject equivalent problem min function constraint convex 
appropriately choosing limiting hypothesis space constraint establishes connection regularized methods generalization error bound regarded major theoretical advantage svm 
implementation issue real world applications computational eciency important issue 
experiments sv light linear kernel svm model iterative algorithms variants gauss seidel solving ridge regression regularized logistic regression 
algorithm ridge regression simple ecient methods 
advantage ridge regression apply collections contain large number categories ohsumed rst compute matrix inverse independent category followed computation individual category 

experimental setup reuters modapte split data collection standard testbed text categorization 
document collection multiple labels split classi cation problem multiple binary classi cation problems respect category 
numbers stopwords removed words converted lowercase stemming 
infrequent features occur times ltered 
feature selection done information gain category binary term weighting applied words document titles treated di erent words document bodies 
precision recall evaluate methods combined form de ned rp order compare global performance di erent methods 
know gives weight categories mainly uenced performance rare categories data collection due skewed category distribution reuters collection 
contrary dominated performance common categories 
conduct experiments sets conditions 
robustness terms small number features compare classi ers performance number features varies 
particularly examine performance case features features information gain 

robustness terms noise level mis labeled data randomly pick portion training examples ip labels 
performance measured respect percentage training examples 

robustness terms rare positive training data order study di erent behaviors classi ers case rare positive training data top common categories data collection simulate process reducing available percent positive training data 
suppose features independent categories 

results discussions performance vs feature size subsection show methods perform relatively small number features 
thresholds regularization coecient 
chosen maximizing training data fold cross validation 
results shown gure 
results common categories features listed table consistent previous published results 
gure see features methods acceptable performance applications 
hand methods behave differently ridge regression best logistic regression worst 
di erence results right graph ridge regression svm logistic regression signi cant performance mainly uenced rare categories believe methods di erent behaviors case rare categories 
experiments rare positive data explore di erences 
table performance svm ridge regression regularized logistic regression reuters top categories feature size positive training examples svm rr lr earn acq money fx grain crude trade interest wheat ship corn money supply dlr performance vs noise level order conduct experiments noisy settings randomly pick training data respectively ip labels 
thresholds regularization coecient tuned fold cross validation condition sure term play active role method resist noisy data 
reported versus noise level gure 
results methods drop dramatically noise level greater 
explained follows categories macro test signi cant level 
feature size micro categories micro vs feature size logistic regression svm ridge regression feature size macro categories macro vs feature size logistic regression svm ridge regression micro macro vs feature size noise level logistic regression svm ridge regression vs noise level percentage mis labeled data data collection relatively small due way manipulate data ip negative data positive overwhelm amount remaining positive data correct 
hard learn target correctly small categories leads poor performance 
results show svm superior methods noise level initially goes anticipated ridge regression works reasonably failed signi cant test compared svm consider large number decisions number document times number categories 
tells text collection squared loss acceptable certain amount example results svm ridge regression noise level signi cant level micro test svm wins di erent decisions 
mis labeled documents 
performance vs rare positive data cases small number positive examples available classi ers 
want examine di erent behaviors methods cases help explain performance di erences gure 
way deal rare class classi cation problems re weight training data change cost function asymmetric amount positive data play important roles negative data 
methods adapted version slightly modifying original optimization problem arg minf jd jc jd wg sets positive negative examples jd jd cardinalities measures relative importance positive negative data jd jc jd normalization factor set independent number examples 
apply approach experiments reasons methods adapted way help re ect di erent characteristics methods 
second want examine capabilities dealing rare class natural way adaptation changes data prior distribution 
weight ratio positive negative data need chosen empirically category crossvalidation may unstable rare class 
design experiments directly examine di erent behaviors methods changing optimization objective function 
order examine performance condition choose common categories terms number positive training examples see table data collection 
categories relatively common randomly hold portion positive examples investigate behaviors available amount positive data changes 
order results stable results averaged times percent positive data times averaged 
results reported terms best possible test data avoids tuning thresholds 
shows results common categories shown table available percent positive data changes 
results see results vary category category logistic regression worse svm ridge regression 
ridge regression performs slightly better svm small categories con rms results gure 
note methods amount negative data larger positive data objective function value initially dominated negative data 
logistic loss strictly greater correctly classi ed data optimization process keep pushing majority correctly classi ed negative data loss function long role objective function larger comparable rare positive data sacri ce performance positive data lead poor score 
svm loss squared loss point exactly zero loss nite yw value contrast logistic loss goes yw goes performance rare class drop dramatic logistic regression 

concluding remarks controlled study robustness regularized linear classi cation methods text categorization 
discussed loss functions related score distributions establishing connection optimization targets generalization error bounds 
experiments investigated performance conditions small number features noisy settings rare positive data 
performance di erences compared analyzed 
concluding remarks theoretically methods treated shrinking hypothesis space performing optimization similar generalization error bounds 
practically perform selected features 
noisy settings svm better logistic regression ridge regression 
ridge regression indicated loss function performs worst 
ridge regression better svm small number positive examples available 
show logistic regression performs badly case give explanations 

acknowledgments anonymous reviewers helpful comments 
research sponsored part national science foundation nsf eia iis part dod award 
opinions authors necessarily re ect sponsors 

dumais platt heckerman sahami 
inductive learning algorithms representations text categorization 
cikm 
golub loan 
matrix computations rd edition 
johns hopkins university press baltimore md 
hastie tibshirani friedman 
elements statistical learning data mining inference prediction 
new york 
springer 
joachims 
text categorization support vector machines learning relevant features 
universit dortmund report 
joachims 
text categorization support vector machines learning relevant features 
european conference machine learning ecml pages berlin 
springer 
lewis ringuette 
comparison learning algorithms text categorization 
proceedings third annual symposium document analysis information retrieval sdair nevada las vegas 
university nevada las vegas 
luenberger 
linear nonlinear programming 
addison wesley new york 
luenberger 
optimization vector space methods 
john wiley sons new york 
sch hull pedersen 
comparison classi ers document representations routing problem 
th ann int acm sigir conference research development information retrieval sigir pages 
tikhonov 
solutions incorrectly formulated problems regularization method 
soviet math 
dokl volume pages 
tikhonov arsenin 
solutions ill posed problems 
winston washington 
vapnik 
nature statistical learning theory 
springer new york 
vapnik 
statistical learning theory 
john wiley sons new york 
yang chute 
example mapping method text categorization retrieval 
acm transaction information systems tois 
yang liu 
re examination text categorization methods 
th ann int acm sigir conference research development information retrieval sigir pages 
yang pedersen 
comparative study feature selection text categorization 
fisher editor fourteenth international conference machine learning icml pages 
morgan kaufmann 
zhang oles 
text categorization regularized linear classi methods 
information retrieval volume pages 
available positive data earn logistic regression svm ridge regression available positive data acq logistic regression svm ridge regression available positive data money fx logistic regression svm ridge regression available positive data grain logistic regression svm ridge regression available positive data crude logistic regression svm ridge regression available positive data trade logistic regression svm ridge regression available positive data interest logistic regression svm ridge regression available positive data wheat logistic regression svm ridge regression available positive data ship logistic regression svm ridge regression available positive data corn logistic regression svm ridge regression available positive data money supply logistic regression svm ridge regression available positive data dlr logistic regression svm ridge regression vs available amount positive data feature size 
