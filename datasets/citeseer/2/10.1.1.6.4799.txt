th international conference automated planning scheduling trento italy june june synthesis hierarchical finite state controllers pomdps develop hierarchical approach planning partially observable markov decision processes pomdps policy represented hierarchical finite state controller 
provide foundation approach discuss extensions pomdp framework allow formalize process abstraction hierarchical controller constructed 
describe planning algorithm uses programmer defined task hierarchy constrain search space finite state controllers prove algorithm converges hierarchical finite state controller optimal limited defined sense related concept recursive optimality 
hierarchical approaches planning long history 
organizing construction plan different levels abstraction hierarchical planner leverage structure planning problem computational gain decomposing related sub problems solved independently 
years interest hierarchical approaches planning problems formalized markov decision processes sutton precup singh parr russell dietterich 
markov decision processes mdps adopted ai community framework decision theoretic planning closely related research reinforcement learning 
exceptions hernandez mahadevan pineau roy thrun theocharous mahadevan hierarchical approaches mdps assumes complete observability 
discuss extension hierarchical approach partially observable mdps pomdps 
pomdps model planning problems include uncertainty effects actions uncertainty state world due example imperfect unreliable sensors 
pomdps provide general realistic model decision theoretic planning computationally challenging 
especially important find ways exploiting problem structure solving pomdps including hierarchical structure 
copyright american association artificial intelligence www aaai org 
rights reserved 
eric hansen rong zhou department computer science engineering mississippi state university mississippi state ms hansen cs edu consider approach hierarchical planning leverages programmer defined task hierarchy decompose pomdp number smaller related pomdps 
solve sub problems policy iteration algorithm pomdps represents policy finite state controller hansen :10.1.1.41.7263
show combine controllers represent solutions subproblems hierarchical finite state controller 
prove planner converges hierarchical finite state controller optimal limited defined sense related concept recursive optimality introduced dietterich evaluate hierarchical policies completely observable mdps 
best knowledge optimality proof hierarchical approach pomdps 
provide foundation results discuss extensions pomdp framework including theory indefinite horizon pomdps elements theory partially observable semi markov mdps allow formalize process abstraction hierarchical controller constructed 
foundation similar foundation hierarchical approaches completely observable mdps 
organized follows 
brief review relevant background pomdps previous hierarchical decomposition mdps 
discuss represent policy pomdp hierarchical finite state controller particular model effects actions take form finitestate controllers 
develop theory pomdps discuss theory partially observable semi markov mdps 
foundation develop planning algorithm finds hierarchical finitestate controllers pomdps analyze convergence 
preliminary computational results discuss number topics including exploration complementary relationship hierarchical decomposition state abstraction 
background brief review relevant background pomdps previous hierarchical approaches mdps 
pomdps policy iteration consider discrete time pomdp defined tuple finite sets states actions observations respectively reward function represents reward received state action set markovian state transition observation probabilities represents probability making transition state state action represents probability observing action results transition state objective optimize function sequence rewards example expected total discounted reward infinite horizon discount factor 
state process directly observed possible maintain vector probabilities states denoted called belief state denotes probability process state action taken followed observation successor belief state denoted determined revising state probability bayes theorem follows 
known belief state updated bayesian conditioning sufficient statistic summarizes information necessary optimal action selection 
gives rise standard approach solving pomdps 
problem transformed equivalent completely observable mdp continuous dimensional state space consisting possible belief states denoted form pomdp solved iteration dynamic programming operator updates value function follows max 
space belief states continuous smallwood sondik proved dp operator preserves piecewise linearity convexity value function 
means value function represented finite set dimensional vectors real numbers 
value belief state defined follows max vi 
vi representation value function allows dp operator computed exactly algorithms developed cassandra littman zhang 
turn dp operator core step exact algorithms solving infinite horizon pomdps value iteration policy iteration 
briefly review policy iteration algorithm due hansen plays central role 
researchers pointed policy pomdp represented finite state controller fsc value function fsc computed exactly solving system linear equations sondik kaelbling littman cassandra 
elegant correspondence nodes fsc vectors piecewise linear convex representation value function 
method policy evaluation hansen showed interpret dp operator pomdps method policy improvement transforms fsc improved fsc 
starting initial fsc algorithm creates series improved fscs repeating steps 
policy evaluation step computes value function fsc solving system linear equations vi index node fsc action associated node index successor node observed 
note node associated unique vector vi 
policy improvement step computes dp operator uses updated value function improve fsc leveraging fact vector corresponds potential new node fsc 
node associated action possible observation transition node current fsc 
potential new nodes modify current fsc follows 
node identical node current fsc change 
node added fsc 
vector corresponding added node pointwise dominates vector corresponding node current fsc dominated node merged new node 
algorithm prunes fsc node corresponding vector updated value function long reachable node corresponding vector 
merging pruning nodes allows fsc shrink grow size 
algorithm monotonically improves fsc guaranteed converge optimal fsc finite number iterations 
dp operator improve fsc fsc optimal 
starting belief state pomdp initial node fsc selected node corresponding vector optimizes equation 
empirical results indicate policy iteration times faster value iteration due fact takes fewer iterations converge complexity policy evaluation negligible compared complexity dynamic programming operator pomdps 
policy iteration plays central role convergence guarantees represents policy fsc representation allows extend concepts action abstraction action decomposition pomdps natural way 
hierarchical approaches mdps approaches hierarchical decomposition completely observable mdps developed sutton pre cup singh parr russell hauskrecht dietterich 
briefly review common elements approaches 
guide developing hierarchical approach pomdps 
approaches treat closed loop policies represent complex behaviors actions 
ensure action terminates returns control higher level associated set terminal states 
soon terminal state entered action stops 
closed loop policy corresponding action take varying indefinite number steps finish execution mdp actions formalized semi markov decision process generalization mdp model allows actions durations vary probabilistically 
closed loop policy corresponding action hand crafted may correspond policy result solving lower level mdp 
case lower level mdp typically indefinite horizon episodic mdp policy eventually terminates 
cases programmer defined task hierarchy decompose mdp collection sub mdps various levels abstraction 
sub mdp action set contains primitive actions original mdp actions correspond policies sub mdps 
commitment hierarchical decomposition policy may cause sub optimality restricting space policies considered solving mdp 
alternative definitions optimality introduced 
policy said hierarchically optimal best policies consistent constraints hierarchical decomposition 
policy said recursively optimal mdp task hierarchy optimal policy policies subtasks recursively optimal 
distinction pertains optimization relative independent calling context 
achieve hierarchical optimality policies subtasks contextdependent dependent policies higher levels hierarchy 
dietterich introduced concept recursive optimality allow policies context free 
recursive optimality guarantees policies subtasks locally optimal respect context executed 
concept recursive optimality strong concept hierarchical optimality 
believed advantages 
example dietterich claims allows greater state abstraction 
interesting hierarchical planning pomdps relies heavily approximation hernandez mahadevan pineau roy thrun theocharous mahadevan 
develop hierarchical approach pomdps allows guarantee quality policy similar limited forms optimality defined hierarchical approaches mdps 
discussion hierarchical fscs develop extensions pomdp framework allow 
describe hierarchical planner guaranteed converge limited form recursive optimality 
hierarchical finite state controllers hierarchical approach pomdps represent policy fsc 
representation policy reflects action decomposition corresponding task hierarchy represented hierarchical fsc 
preparation defining hierarchical fsc define fsc follows 
definition finite state controller fsc tuple set controller states called nodes distinguished initial node mapping nodes actions node transition function set terminal nodes 
include terminal nodes important fscs terminate order allow sub controller pass control back higher level controller 
representing policy fsc allows adopt concept action abstraction 
definition action fsc treated single action 
executing action generates sequence actions fsc begins action associated initial node continues series actions depends sequence observations received ends action associated terminal nodes 
refer set primitive actions refer action set may contain actions 
define hierarchical fsc allowing action associated node fsc correspond action fsc 
definition hierarchical fsc tuple defined mapping nodes actions may actions 
require hierarchical fsc finite number levels lowest level fscs hierarchy primitive actions 
course hierarchical fsc special case fsc 
hierarchical fsc expanded conventional flat fsc replacing actions corresponding fscs 
assume hierarchical fsc policy pomdp 
actions lowest level primitive actions action set pomdp associated observations observation set pomdp 
actions associated null observation explain moment kind observation discussed 
reward function transition observation probabilities pomdp describe compute reward function transition observation probabilities action hierarchical fsc 
advantage representing policy pomdp fsc treat policy action compute exact model effects follows 
reward function reward function action computed policy evaluation method fsc 
terminal node fsc dimensional vector vt set equal immediate reward vector action associated terminal node 
non terminal node fsc dimensional vector vi computed solving system linear equations equation 
immediate reward function action vector corresponds initial node 
represents expected cumulative reward received starting fsc initial node continuing terminal node reached 
state transition probabilities state transition probabilities action computed standard method computing absorption probabilities markov chain 
insight policy evaluation fsc pomdp fixes markov chain state space cross product absorbing states chain associated terminal node fsc 
pi denote probability making transition state state pomdp executing fsc node continuing termination 
pi computed assuming terminal node fsc pt computing state transition probabilities nonterminal node solving system linear equations pi 
state transition probabilities action corresponding fsc set equal initial node fsc 
node starting state pi fsc corresponding action guaranteed terminate 
analogy concept proper policy bertsekas define proper fsc follows 
definition proper fsc eventually terminates probability 
give conditions fsc represents policy pomdp guaranteed terminate 
method computing state transition probabilities termination guaranteed 
observation probabilities face interesting question 
associate observation action 
information provided executing action 
possibility consider entire execution history fsc corresponding action observation 
clearly infeasible observation space infinite 
possibility consider observation received fsc observation received action associated terminal node observation associated action 
may turn useful allow compute observation probabilities action way similar compute state transition probabilities 
discuss near 
assume null observation associated action 
necessary assumption help simplify exposition 
conveys intuition modularity hierarchical policy 
associating null observation action history information accumulated course executing fsc treated local information unavailable higher level control 
note memory happened invoked preserved available higher level 
discuss relax assumption null observation associated action 
mean duration times consider aspect model action mean number primitive time steps action takes execute 
information needed appropriate discounting semi markov decision processes includes actions discount factor 
mean duration action measured number primitive time steps computed solving system linear equations ni ni mean number primitive steps termination fsc starting non terminal node mean duration action 
assume equal primitive action 
terminal node nt mean duration time action conditioned underlying state initial node fsc corresponding action 
system equations assumes proper fsc appropriately modified fsc terminate probability 
indefinite horizon pomdps definition fsc includes set terminal nodes 
framework hierarchical control important terminal nodes allow sub controller pass control back higher level controller 
section discuss represent guarantee termination generally policy pomdp 
question received little attention literature pomdps crucial hierarchical approach control 
course finite horizon pomdps policy terminates fixed number time steps 
planning problems number time steps needed achieve objective fixed advance flexible model needed 
formalize sub pomdp infinite horizon pomdp policy may terminate control may passed back higher level 
hierarchical control completely observable mdps sub mdp formalized mdp 
allows policy terminate indefinite number time steps certain conditions possible ensure eventual termination probability 
indefinite horizon mdps include class stochastic shortest path problems bertsekas 
problems include non empty set terminal states 
states entered policy stops execution 
terminal states viewed goal states stochastic shortestpath problems typically model decision theoretic planning problems involve goal achievement case hierarchical control subgoal achievement 
obvious extend theory stochastic shortest path problems pomdps 
stochastic shortest path problem termination policy requires recognition terminal state reached 
pomdp underlying state environment directly observable 
special case theory stochastic shortest path pomdps developed assumes pomdp contains special observation reliably indicates terminal state reached 
assuming terminal states completely observable general solution 
know attempt characterize indefinite horizon pomdps 
section propose framework pomdps believe general natural 
thinking problem slightly different way 
traditional framework stochastic shortest path problems termination policy associated set terminal states 
view termination decision think natural natural associate termination policy set terminal actions set terminal states 
define terminal action special action soon taken associated reward received execution policy terminates 
leads definition 
definition indefinite horizon pomdp pomdp action set includes terminal actions 
interesting note examples pomdps literature essentially indefinite horizon pomdps 
examples include known tiger door problem kaelbling littman cassandra maze navigation problems 
theory indefinite horizon pomdps formalized infinite horizon pomdps reset actions reset action similar terminal action represents step plan examples resets problem starting state 
stopping policy repeatedly solves problem 
accumulates reward acquired solving problem repeatedly infinite horizon requires discount factor 
optimal policy discounting may optimal policy indefinite horizon problem discounted 
obviously possible define indefinite horizon com advantage approach defining pomdps obvious require recognition underlying state process 
course define state dependent reward function terminal action way large reward reward received stopping state corresponds goal state reward large penalty received 
identify goal states useful way 
longer require termination policy depends recognition particular goal state reached 
model goal achievement tasks defining indefinite horizon pomdp way policy terminates expected value doing optimized belief distribution underlying states including goal states tradeoff cost continued plan execution reward reaching goal state 
framework may optimal completely certain underlying state goal state 
definition indefinite horizon pomdp allows partially observed goal states fits nicely definition fsc includes set terminal nodes 
policy indefinite horizon pomdp represented fsc terminal nodes fsc exactly nodes associated terminal action 
described mechanism policy pomdp terminate consider guarantee terminates 
important note hierarchical approach require eventually terminates important sub controllers usually terminate 
method modelling actions represented fscs planning sub controller nonzero probability terminating discount factor may required case ensure reward function action finite 
identify important class pomdps termination guaranteed 
consider class undiscounted indefinite horizon pomdps non terminal action incurs negative reward 
call class problems negative pomdps 
theorem negative pomdps fsc policy iteration terminates proper fsc 
follows fact policy doesn terminate unbounded negative value single node fsc selects terminal action better policy terminate single iteration dp operator 
adapt dp algorithm pomdps pletely observable mdps similar way 
fact results framework general theory stochastic shortest path problems 
stochastic shortest path problem defined terminal actions terminal states restricting set states terminal action taken terminal states 
addition framework lets define mdps policy terminate state doing maximizes expected value 
may advantages setting complete observability discuss 
solve indefinite horizon pomdp 
modification simple 
terminal action action executed policy optimize terminal actions dp operator create updated set vectors 
simply include reward vectors terminal action current set vectors dp operator constructs updated set vectors 
essentially allows new fsc nodes link terminal nodes fsc allowing terminal nodes successor nodes 
bound sub optimality solution dp computed standard way multiplying bellman residual represents maximum error time step term represents upper bound expected number time steps termination 
discounted case represents expected time termination interpretation discount factor non zero probability terminating time step bertsekas 
case negative pomdps upper bound ex pected time till termination maxs vt mins vt greatest difference reward terminating state zero minus largest reward smallest cost non terminal action added account execution time terminal action 
case sub optimality fsc policy iteration pomdps bounded result 
theorem policy iteration converges optimal fsc finite number iterations 
discounted infinite horizon pomdps result straightforward extension similar result value iteration hansen :10.1.1.41.7263
claim holds negative pomdps give proof separate 
partially observable semi markov decision processes weak optimality hierarchical control completely observable mdps adopts framework semi markov decision processes smdps 
natural expect wellfounded approach hierarchical control pomdps adopt framework partially observable semi markov decision processes 
consider possibility 
note dp algorithm completely observable smdps need consider actual time interval actions needs consider expected time interval mean action duration discounted optimality criterion 
case mean action duration relevant affects degree discounting 
partially observable case complicated 
consider formula updating belief state denotes belief state results action resulting observation interval joint probability distribution transition interval state action taken current state fact resulting belief state conditioned actual transition interval action resulting observation means transition interval considered optimization discounting 
dp operator sum possible transition intervals possible observations order exactly update value function follows max result special cases dp operator computed exactly 
propose concept limited optimality 
definition weak optimal policy policy optimal policies conditioned transition interval duration actions 
straightforward prove exact dp algorithm pomdps converges weak optimal policy pos mdp 
discounted case consider expected action durations appropriate discounting 
case ignores possibility policy conditioned actual duration actions 
means policy iteration algorithm searches space fscs node transition function conditioned observation observation duration action 
hierarchical pomdps contain actions corresponding fscs adoption criterion weak optimality reasonable second reason chosen ignore information relevant optimization 
update belief state correctly need consider duration action entire history observations received course executing action 
way ensure exact belief updates true optimality 
chose ignore information decided return null observation limited information action 
full history information exact belief updates clearly infeasible enumerate possible histories optimization know discusses possibility computing exact solutions focuses finite horizon case white 
action durations discrete finite horizon bounds duration action set possible action durations finite enumerated 
follows optimal value function piecewise linear convex dp 
result extend natural way indefinite infinite horizon case 
algorithm just infeasible enumerate possible action durations 
extend definition weak optimality actions follows 
definition weak optimal policy actions policy optimal policies conditioned duration actions history information execution action explicitly modelled observation function 
adopt definition unavoidable reasonable context hierarchical decomposition 
coincides intuition designed task hierarchy memory needed solve subtask local subtask safely ignored higher level abstraction 
maxq decomposition previous sections developed theoretical framework hierarchically structured pomdps policy represented hierarchical fsc 
believe provide foundation generalizing hierarchical approaches mdps completely observable partially observable case 
example show hierarchical approach completely observable mdps dietterich maxq approach generalized pomdps 
reason considering maxq approach significant effort generalize pomdps pineau roy thrun pineau thrun 
pineau maxq approach improve scalability value iteration pomdps 
hierarchical algorithm number attractive features shows impressive empirical performance 
example successfully control mobile robotic assistant nursing home 
approach relies approximations especially modelling actions guarantees quality policy finds 
section alternative generalization maxq approach represents actions fscs uses policy iteration planning algorithm value iteration 
lets limited defined claims quality policy planner 
review maxq approach pineau generalization describe alternative 
key idea maxq approach decompose mdp collection smaller mdps programmer defined task hierarchy 
shows simple example task hierarchy 
leaves task hierarchy correspond primitive actions 
internal nodes represented boxes correspond tasks mdps policy treated action mdp corresponding node task hierarchy 
action set mdp contains primitive actions represented child nodes task hierarchy 
subtask hierarchy represents sub goal hierarchical decomposition original mdp 
policy subtask terminates sub goal achieved 
completely observable case termination associated set terminal states sub goal modelled associating rewards terminal states 
dietterich allows designer task hierarchy create artificial sub goals helps decompose problem defining pseudo reward function mdp corresponding subtask 
follow pineau associating internal node task hierarchy pomdp 
primitive pomdps primitive actions pomdps actions possibly primitive actions 
adopt pineau general approach solving pomdp represented way 
exact dp algorithm compute policy subproblem traversing task hierarchy bottom order 
computing policy treat action higher level pomdp 
requires computing reward function transition model action 
approach departs approach pineau ways 
represent policy fsc formalize pomdp task hierarchy indefinite horizon pomdp defined earlier allow termination fsc requiring recognition underlying terminal state policy iteration value iteration solve pomdp 
representing policy fsc allows compute exact model action 
pineau treat policy subtask single action reward transition model approximated create action node fsc representing policy 
approach fsc converted set actions nodes 
computing models actions adds complexity model computation step compute model nodes fsc simultaneously 
increase size action set parent pomdp hierarchy 
dramatically increase complexity solving parent pomdp practice 
worst case complexity dp operator depends exponentially size observation set polynomially size vector set linearly size action set 
second actions associated null observation 
number observations principally affects complexity dp operator set actions null observation increases complexity parent pomdp negligibly 
mention additional element generalization maxq approach 
programmer specifies pseudo reward function subtask hierarchy associate pseudo reward function pseudo action defined terminal action 
recall introduced terminal actions framework pomdps 
pseudo reward function artificial view pseudo action similarly artificial assume takes zero time steps execute 
repeat error bound task hierarchy bottom order policy iteration improve create fsc task compute error bound fsc root task compute model actions corresponding nodes fsc maxq hierarchical policy iteration 
earlier showed pomdp includes actions policy iteration converges weak optimal fsc 
means action may sub optimal associated bounded error 
show propagate error bounds actions task hierarchy bound error hierarchical fsc 
theorem sub optimality fsc policy iteration solving pomdp bounded bellman residual upper bound mean time policy terminates defined paragraph preceding theorem max bound sub optimality action action set pomdp error bound associated action 
idea sub optimality fsc increase sub optimality fsc uses action needs considered calculating error bounds 
effect maximum error action added residual pomdp includes action 
error bound lowest level fscs computed setting lowest level fscs primitive actions 
error bound fscs include actions action set computed theorem error bound hierarchical fsc error propagates top task hierarchy 
error bound greater pre specified algorithm traverse hierarchy bottom order improve hierarchical fsc reduce error bound 
algorithm may converge quickly focusing effort improving sub controller contributes error 
pseudocode shows simple outline structure planning algorithm called policy iteration 
outer loop repeats traversal task hierarchy desired error bound achieved ensures eventual convergence 
theorem maxq hierarchical policy iteration converges recursively weak optimal fsc finite time 
note extending maxq approach pomdps added qualifications dietterich recursive optimality result completely observable mdps 
guarantee optimality guarantee policy iteration find optimal fsc 
second guarantee weak optimality node transition function fsc conditioned observations taxi domain corresponding task hierarchy dietterich 
duration actions choose ignore history information local fsc corresponding action 
example implemented maxq hierarchical policy iteration algorithm solve simple pomdps including partially observable version taxi problem motivating example dietterich 
shows grid taxi navigates 
taxi pick drop passenger locations labelled problem state variables location taxi possibilities location passenger possibilities corresponding taxi destination possibilities total states 
primitive actions navigation actions deterministically move taxi square north south east west pickup action putdown action 
reward navigation action 
pickup action reward taxi passenger location 
putdown action reward taxi destination passenger taxi 
model problem negative pomdp putdown action terminal action 
note add pseudo terminal actions pseudo reward functions dietterich hierarchical decomposition 
problem partially observable assume taxi observe exact location 
observe wall placement directions immediately adjacent location 
simple observation function notation column row denote square grid sets states aliased observation distinguished observation 
states unique observation reliably identified 
total number observations 
location taxi partially observable state variables location passenger destination completely observable 
creates pomdp hybrid structure state variables partially observable completely observable exploited solve problem efficiently 
particular unnecessary optimize value function belief simplex states belief states impossible give uncertainty state variables 
optimize separate value functions belief simplices combination values completely observed variables 
dimension simplex corresponding partially observable locations taxi 
simple adaptation dp algorithm pomdps exploits hybrid structure described choi 

allows solve state pomdp needing address issue state abstraction topic defer 
maxq hierarchical policy iteration algorithm finds hierarchical fsc problem recursively optimal weak sense defined earlier 
policy iteration guaranteed converge optimality remind reader detect convergence optimality 
takes minutes pentium iv ghz processor 
controller nodes number actions created solving problem 
complex part policy node responsible navigating location location significantly aliased 
interesting note optimal sub controller guarantee termination location guarantees termination 
respect illustrates indefinite horizon model 
extensions outlined approach hierarchical planning pomdps actions represented fscs 
approach practical developments needed 
section briefly discuss important extensions leave 
task dependent state abstraction currently exact dp algorithms solve pomdps require optimizing value function belief simplex defined states 
state abstraction absolutely necessary scaling hierarchical approach solving pomdps 
preliminary state abstraction pomdps shows dp algorithm find exact approximate solutions number states limited number hansen feng feng hansen 
important motivation hierarchical approach mdps opportunity leverage hierarchical decomposition perform task dependent state abstraction 
means state distinctions relevant achieving subgoal may irrelevant achieving subgoal considered ignored depending context 
decomposing problem hierarchically solving subtask subtask basis opportunities state abstraction may dramatically increased 
proved significant advantage hierarchical approaches completely observable mdps dietterich andre russell explored hierarchically structured pomdps pineau thrun 
claim hierarchical approach pomdps imposes limit size state space number state distinctions may considered time long pomdp corresponding subtask solved dp 
similar reasons imposes limit size hierarchical fsc 
needing state abstraction dp algorithm pomdps need state abstraction computing transition model action requires solving system linear equations equation 
especially critical state abstraction step number equations quadratic number states 
observations actions earlier simplifying assumption action associated null observation 
means action treated black box history observations received course executing invisible higher level controller 
reasonable assumption approach hierarchical decomposition 
means memory sub controller solve sub problem local memory needed higher level controller 
assumption justified structure problem offers great computational leverage 
times useful sub controller pass history information back higher level controller invoked 
done associating observation function action 
obviously infeasible pass entire execution history action back higher level controller observation observation space infinite 
ways return useful summary 
pomdp corresponding subtask terminal actions example associate distinct observation 
recall terminal action corresponds distinct terminal node sub controller 
set terminal nodes partitions possible histories observation associated terminal action viewed summary relevant information history 
sense viewed form observation abstraction 
example useful dietterich describes fickle taxi problem passenger changes destination action navigating destination executing 
action represented fsc terminal nodes destination changed destination easily passed back higher level controller observation 
case size observation space equal number terminal nodes 
point simply possible return history information sub controller way designer task hierarchy control limit information returned 
designed task hierarchy expect small amount relevant history information returned sub controller higher level controller invoked 
described approach hierarchical planning pomdps policy represented fsc 
key idea approach representing action fsc allows compute exact transition reward model 
turn allows create defined pomdp provide theoretical guarantees quality policy hierarchical algorithm 
covered lot ground developing approach topics raised deserve elaboration study 
tried detail convince reader approach plausible worth investigating 
conclude noting fsc plan representation policy pomdp contrast representation policy mapping belief states actions 
may policy representation closer traditional ai plan representations easier leverage techniques hierarchical action decomposition proved helpful scaling traditional ai planning algorithms 
acknowledgments anonymous reviewers helpful comments 
supported part nsf iis nasa nag 
andre russell 
state abstraction programmable reinforcement learning agents 
proceedings th national conference artificial intelligence aaai 
bertsekas 
dynamic programming optimal control vols 
ii 
belmont ma athena scientific 
cassandra littman zhang 
incremental pruning simple fast exact method partially observable markov decision processes 
proceedings th annual conference uncertainty artificial intelligence uai 
choi zhang yeung 
solving markov decision processes 
proceedings th international workshop artificial intelligence statistics 
dietterich 
hierarchical reinforcement learning maxq value function decomposition 
journal artificial intelligence research 
feng hansen 
approximate planning factored pomdps 
proceedings th european conference planning ecp 
hansen feng 
dynamic programming pomdps factored state representation 
proceedings th international conference artificial intelligence planning scheduling aips 
hansen 
finite memory control partially observable systems 
ph dissertation university massachusetts amherst 
hansen 
solving pomdps searching policy space 
proceedings th conference uncertainty artificial intelligence uai 
hauskrecht kaelbling dean boutilier 
hierarchical solution markov decision processes macro actions 
proceedings th conference uncertainty artificial intelligence uai 
hernandez mahadevan 
hierarchical memory reinforcement learning 
advances neural information processing systems proceedings conference nips 
mit press 
kaelbling littman cassandra 
planning acting partially observable stochastic domains 
artificial intelligence 
parr russell 
reinforcement learning hierarchies machines 
advances neural information processing systems proceedings conference nips 
mit press 

partially observed stochastic shortest path problems 
proceedings th ieee conference decision control cdc 
pineau thrun 
integrated approach hierarchy abstraction pomdps 
carnegie mellon university technical report cmu ri tr 
pineau roy thrun 
hierarchical approach pomdp planning execution 
workshop hierarchy memory reinforcement learning icml 
smallwood sondik 
optimal control partially observable markov processes finite horizon 
operations research 
sondik 
optimal control partially observable markov processes infinite horizon discounted costs 
operations research 
sutton precup singh 
mdps semi mdps framework temporal abstraction reinforcement learning 
artificial intelligence 
theocharous mahadevan 
approximate planning hierarchical partially observable markov decision processes robot navigation 
proceedings ieee international conference robotics automation icra 
white 
procedures solution finite horizon partially observed semi markov optimization problem 
operations research 
