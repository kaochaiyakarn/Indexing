training support vector machines application face detection appear proceedings cvpr june puerto rico 
edgar osuna 
robert freund federico girosi center biological computational learning operations research center massachusetts institute technology cambridge ma investigate application support vector machines svms computer vision 
svm learning technique developed vapnik team bell labs 
seen new method training polynomial neural network radial basis functions classifiers 
decision surfaces solving linearly constrained quadratic programming problem 
optimization problem challenging quadratic form completely dense memory requirements grow square number data points 
decomposition algorithm guarantees global optimality train svm large data sets 
main idea decomposition iterative solution sub problems evaluation optimality conditions generate improved iterative values establish stopping criteria algorithm 
experimental results implementation svm demonstrate feasibility approach face detection problem involves data set data points 
years problems object detection image classification received increasing amount attention computer vision community 
cases problems involve concepts face people expressed terms small meaningful set features feasible approach learn solution set examples 
complexity problems extremely large set examples needed order learn task desired accuracy 
known relevant features problem data points usually belong high dimensional space example image may represented grey level values eventually filtered bank filters dense vector field puts correspondence certain prototypical image 
need general purpose pattern recognition techniques handle large data sets gamma data points high dimensional spaces gamma 
concentrate support vector machine svm pattern classification algorithm developed vapnik team bell labs 
:10.1.1.15.9362:10.1.1.54.9934
svm seen new way train polynomial neural network radial basis functions classifiers 
techniques train mentioned classifiers idea minimizing training error usually called empirical risk svms operate induction principle called structural risk minimization minimizes upper bound generalization error 
implementation point view training svm equivalent solving linearly constrained quadratic programming qp problem number variables equal number data points 
problem challenging size data set larger thousands 
show large scale qp problem type posed svm solved decomposition algorithm original problem replaced sequence smaller problems proved converge global optimum 
order show applicability approach svm core classification algorithm face detection system 
problem solve involves training classifier discriminate face non face patterns data set points 
plan follows rest section briefly introduce svm algorithm geometrical interpretation 
section solution problem training svm decomposition algorithm 
section application face detection problem section summarize results 
support vector machines section briefly sketch svm algorithm motivation 
detailed description svm chapter 
start simple case linearly separable classes 
assume data set labeled examples gamma wish determine infinite number linear separate data smallest generalization error 
intuitively choice hyperplane leaves maximum margin classes margin defined sum distances hyperplane closest point classes see 
classes non separable look separating hyperplane small margin 
separating hyperplane larger margin 
better generalization capability expected 
hyperplane maximizes margin minimizes quantity proportional number misclassification errors 
trade margin misclassification error controlled positive constant chosen 
case shown solution problem linear classifier sign solution qp problem minimize gamma subject gamma gamma ij turns small number coefficients different zero coefficient corresponds particular data point means solution determined data points associated non zero coefficients 
data points called support vectors ones relevant solution problem data points deleted data set solution obtained 
intuitively support vectors data points lie border classes 
number usually small vapnik showed proportional generalization error classifier 
real life problem solved linear classifier technique extended order allow non linear decision surfaces 
easily done projecting original set variables higher dimensional feature space oe oe formulating linear classification problem feature space 
solution form sign nonlinear original input variables 
face point problems choice features oe done way leads rich class decision surfaces computation scalar product computationally prohibitive number features large example case wants feature space span set polynomials variables number features exponential 
possible solution problems consists letting go infinity choice ff ff ff eigenvalues eigenfunctions integral operator kernel positive definite symmetric function 
choice scalar product feature space particularly simple ff equality comes schmidt theorem positive definite functions see pp 

qp problem solved exactly eq 
exception matrix elements ij 
result choice svm classifier form sign 
table list choices kernel function proposed vapnik notice lead known classifiers decision surfaces known approximation properties 
kernel function type classifier exp gamma gaussian rbf polynomial degree tanh gamma theta multi layer perceptron table possible kernel functions type decision surface define training support vector machine mentioned training svm large data sets samples difficult problem approach kind problem decomposition 
give idea memory requirements application described section involves training samples amounts quadratic form matrix delta entries need bytes floating point representation gigabytes memory 
order solve training problem efficiently take explicit advantage geometrical interpretation introduced section particular expectation number support vectors small components zero 
order decompose original problem think solving iteratively system keeping fixed zero level components associated data points support vectors optimizing reduced set variables 
convert previous description algorithm need specify 
optimality conditions conditions allow decide computationally problem solved optimally particular iteration original problem 
section states proves optimality conditions qp 

strategy improvement particular solution optimal strategy defines way improve cost function frequently associated variables violate optimality conditions 
strategy stated section 
presenting optimality conditions strategy improving cost function section introduces decomposition algorithm solve large database training problems section reports computational results obtained implementation 
optimality conditions qp problem solve minimize gamma subject gamma upsilon gamma pi upsilon ae ae pi associated kuhn tucker multipliers 
positive semi definite matrix kernel function positive definite constraints linear kuhn tucker kt conditions necessary sufficient optimality 
kt conditions follows rw upsilon gamma pi upsilon gamma pi upsilon pi gamma gamma order derive algebraic expressions optimality conditions assume existence consider possible values component 
case equations kt conditions gamma results easily show strictly equality holds noticing combining expression immediately obtain 
case equations kt conditions gamma ae defining noticing gamma conclude equation fact required kt multiplier ae positive 

case equations kt conditions gamma gamma applying similar algebraic manipulation described case obtain strategy improvement optimality conditions derived previous section essential order devise decomposition strategy takes advantage expectation zero guarantees iteration objective function improved 
order accomplish goal partition index set sets way optimality conditions hold subproblem defined variables set called working set 
decompose vectors set 
decomposition statements clearly true ffl replace changing cost function feasibility subproblem original problem 
ffl replacement new subproblem optimal 
follows equation assumption subproblem optimal replacement done 
previous statements lead proposition proposition optimal solution subproblem defined operation replacing satisfying generates new subproblem optimized yields strict improvement objective function 
proof assume existence assume proof analogous gammay 
ffl gamma ffi ffi ffl 
notice consider ffi gamma ffi th th unit vectors notice pivot operation handled implicitly letting ffi holding 
new cost function written gamma gamma ffie gamma ffi ffie gamma ffi ffie gamma ffi ffi gamma gamma ffi gamma ffi gamma ffi gamma choosing ffi small 
decomposition algorithm suppose define fixed size working set jbj big contain support vectors small computer handle optimize solver 
decomposition algorithm stated follows 
arbitrarily choose jbj points data set 

solve subproblem defined variables 
exists replace solve new subproblem 
notice algorithm strictly improve objective function iteration cycle 
objective function bounded convex quadratic feasible region bounded algorithm converge global optimal solution finite number iterations 
gives geometric interpretation way decomposition algorithm allows redefinition separating surface adding points violate optimality conditions 
sub optimal solution points violating optimality conditions inside sigma area 
decision surface redefined 
points inside sigma area solution optimal 
notice size margin decreased shape decision surface changed 
implementation results implemented decomposition algorithm minos solver sub problems 
information minos see 
computational results section obtained real data face detection system described section 
figures show training time number support vectors obtained training system data points 
discontinuity graphs data points due fact data points collected phase bootstrapping face detection system see section 
means data points points misclassified previous version classifier quite accurate points border classes hard classify 
shows relationship training time number support vectors 
notice curve smoother 
means number training data predictor training time depends heavily number support vectors add large number data points increasing training time new data points contain new support vectors 
report number global iterations number times decomposition algorithm calls solver function support vectors 
notice jump global iterations go samples adding difficult data points 
memory requirements technique quadratic size working set points data set working set variables ended mb ram 
working set variables approximately mb ram 
current technique deal problems support vectors empirically working set size larger number support vectors 
order overcome limitation implementing extension decomposition algorithm deal large numbers support vectors say 
svm application face detection images section introduces support vector machine application detecting vertically oriented unoccluded frontal views human faces grey level images 
handles faces wide range scales works different lighting conditions moderately strong shadows 
face detection problem defined follows input arbitrary image digitized video signal scanned photograph determine human faces image return encoding location 
face detection computer vision task applications 
direct relevance face recognition problem important step fully automatic human face recognizer usually locating faces unknown image 
face detection potential application human computer interfaces surveillance systems census systems standpoint face detection interesting example natural challenging problem demonstrating testing potentials support vector machines 
object classes phenomena real world share similar characteristics example tumor anomalies mri scans structural defects manufactured parts successful general number samples time hours number samples number support vectors number support vectors number samples training time sparc station vs number samples number support vectors vs number samples training time sparcstation vs number support vectors 
notice number support vectors better indicator increase training time number samples number global iterations performed algorithm 
methodology finding faces svm generalize spatially defined pattern feature detection problems 
important face detection object detection problems difficult task due significant pattern variations hard parameterize analytically 
common sources pattern variations facial appearance expression presence absence common structural features glasses light source distribution shadows system works scanning image patterns possible scales uses svm core classification algorithms determine appropriate class face non face 
previous systems problem face detection approached different techniques years 
techniques include neural networks detection face features geometrical constraints density estimation training data labeled graphs clustering distribution modeling 
previous works results sung poggio rowley reflect systems high detection rates low false positive rates 
sung poggio clustering distance metrics model distribution face non face manifold neural network classify new pattern measurements 
key quality result clustering combined mahalanobis euclidean metrics measure distance new pattern clusters 
important features approach nonface clusters bootstrapping technique collect important non face patterns 
drawback technique provide principled way choose important free parameters number clusters uses 
similarly rowley problem information design connected neural network trained classify faces non faces patterns 
approach relies training nn emphasizing subsets training data order obtain different sets weights 
different schemes arbitration order reach final answer 
approach face detection svm uses prior information order obtain decision surface technique detect kind objects digital images 
svm face detection system system detects faces exhaustively scanning image face patterns possible scales dividing original image overlapping subimages classifying svm determine appropriate class face non face 
multiple scales handled examining windows taken scaled versions original image 
specifically system works follows 
database face non face theta pixel patterns assigned classes respectively train svm polynomial kernel function upper bound 

order compensate certain sources image variation preprocessing data performed ffl masking binary pixel mask remove pixels close boundary window pattern allowing reduction dimensionality input space 
step important reduction background patterns introduce unnecessary noise training process 
ffl illumination gradient correction best fit brightness plane subtracted unmasked window pixel values allowing reduction light heavy shadows 
ffl histogram equalization histogram equalization performed patterns order compensate differences illumination brightness different cameras response curves 
decision surface obtained training run time system images contain faces misclassifications stored negative examples subsequent training phases 
images landscapes trees buildings rocks sources false positives due different textured patterns contain 
bootstrapping step successfully sung poggio important context face detector learns examples ffl negative examples abundant negative examples useful learning point view difficult characterize define 
ffl classes face non face equally complex non face class broader richer needs examples order get accurate definition separates face class 
shows image bootstrapping misclassifications non face examples 

training svm incorporate classifier run time system similar sung poggio performs operations ffl re scale input image times 
ffl cut theta window patterns scaled image 
ffl preprocess window masking light correction histogram equalization 
ffl classify pattern svm 
ffl pattern face draw rectangle output image 
false detections obtained version system 
false positives non face examples training process experimental results test run time system sets images 
set contained high quality images face image 
set contained images mixed quality total faces 
sets tested system sung poggio 
order give true meaning number false positives obtained important state set involved pattern windows set 
table shows comparison systems 
run time svm system approximately times faster system sung poggio 
reason technique introduced burges allows replace large numbers support vectors smaller number points necessarily data points speed run time considerably 
report result system test images 
notice system able handle small degree non frontal views faces 
database contain example occluded faces system usually misses partially covered faces ones bottom picture 
system deal degree rotation image plane data base contains number virtual faces obtained rotating face example degrees 
report support vectors obtained face non face patterns 
represent images points fictitious dimensional space draw arbitrary boundary classes 
notice placed support vectors classification boundary accordingly geometrical interpretation 
notice non face support vectors just random non face patterns non face patterns quite similar faces 
test set test set detect false detect false rate alarms rate alarms svm sung table performance svm face detection system summary novel decomposition algorithm train support vector machines large data sets say data points 
current version algorithm deal support vectors machine mb ram implementation technique currently development able deal larger number support vectors say memory 
demonstrated applicability svm embedding svm face detection system performs comparably state art systems 
reasons investigating svm 
fact svms founded mathematical point view approximate implementation structural risk minimization induction principle 
free parameters svms positive constant parameter associated kernel case results face detection system non faces faces picture circles represent face patterns squares represent non face patterns 
border classes represented support vectors system 
notice non face support vectors similar faces 
degree polynomial 
technique appears stable respect variations parameters 
expected value ratio number support vectors total number data points upper bound generalization error number support vector gives immediate estimate difficulty problem 
svms handle high dimensional input vectors appropriate computer vision problems clear features allowing user represent image possibly large vector grey levels authors tomaso poggio vladimir vapnik michael oren constantine papageorgiou useful comments discussion 
boser guyon vapnik 
training algorithm optimal margin classifier 
proc 
th describes research done center biological computational learning department brain cognitive sciences artificial intelligence laboratory mit 
research sponsored nsf contract asc award includes funds arpa provided hpcc program arpa onr contract muri contract additional support provided daimler benz metal industries siemens ag 
edgar osuna supported gran de 
acm workshop computational learning theory pages pittsburgh pa july 
carel 
detection localization faces digital images 
pattern recognition letters 
burges 
simplified support vector decision rules 
international conference machine learning pages 

cortes vapnik 
support vector networks 
machine learning 
kruger malsburg 
determination face position pose learned representation graphs 
technical report ruhr universitat january 
moghaddam pentland 
probabilistic visual learning object detection 
technical report mit media laboratory june 
murtagh saunders 
large scale linearly constrained optimization 
mathematical programming 
riesz sz nagy 
functional analysis 
ungar new york 
rowley baluja kanade 
human face detection visual scenes 
technical report school computer science carnegie mellon university november 
sung poggio 
example learning view human face detection 
memo mit lab december 
le cun 
original approach localisation objects images 
ieee proc 
vis 
image signal process august 
vapnik 
nature statistical learning theory 
springer new york 
yang huang 
human face detection complex background 
pattern recognition 
