universal prediction neri merhav meir feder july consists overview universal prediction information theoretic perspective 
special attention notion probability assignment loss function directly related theory universal data compression 
probabilistic setting deterministic setting universal prediction problem described emphasis analogy differences results settings 
index terms universal prediction probability assignment universal coding stochastic complexity redundancy capacity bayes envelope entropy loss function linear prediction finite state machine 
supported israel science foundation administered israeli academy sciences humanities 
merhav department electrical engineering technion israel institute technology haifa israel 
mail merhav ee technion ac il 
feder department electrical engineering systems tel aviv university tel aviv israel 
mail meir eng tau ac il 
sequence predicted past 
prediction 
questions frequently encountered applications 
generally speaking may wonder related past 
evidently relation known advance useful prediction 
reality knowledge relation underlying model normally unavailable inaccurate calls developing methods universal prediction 
roughly speaking universal predictor depend unknown underlying model performs essentially model known advance 
survey describes research universal prediction carried years scientific disciplines information theory statistics machine learning control theory operations research 
emphasized attempt cover comprehensively entire volume done problem area 
aim point highlights principal methodologies authors personal information theoretic perspective 
new results derivations detail 
historically information theoretic approach prediction dates back shannon related prediction entropy proposed predictive estimate entropy english language 
inspired shannon created mind reading machine predicts human decisions 
time kelly showed equivalence gambling turn definitely form prediction information 
cover rissanen rissanen langdon recognized date universal prediction intimately related universal lossless source coding 
decades starting pioneering davisson ziv lempel ziv rissanen langdon theory practice universal coding greatly advanced :10.1.1.14.2892
state art knowledge area sufficiently mature shed light problem universal prediction 
specifically prediction schemes fundamental performance limits lower bounds stemming universal coding derived 
relation universal coding universal prediction main theme aspects algorithms performance bounds 
describe prediction problem general 
observer sequentially receives sequence observations alphabet time instant having seen gamma gamma observer predicts outcome generally decision observed past gamma associated prediction decision actual outcome loss function measures quality 
depending particular setting prediction problem objective minimize instantaneous loss time average expected value quantities 
obviously prediction ordinary sense special case estimate gamma estimation performance criterion hamming distance discrete squared error gamma continuous 
special case general examples assigning weights probabilities possible values outcome 
example weather man may assess chance rain tomorrow making commitment rain 
clearly informative ordinary prediction described gives assessment degree confidence reliability associated prediction 
terms described prediction problem conditional probability assignment gamma non negative function gamma integrates sums unity gamma observing performance assessed respect suitable loss function decrease monotonically probability assigned actual outcome jx gamma 
important loss function kind self information loss function referred log loss function machine learning literature 
probability assignment fb xg function defined gamma log logarithms taken base specified 
reasons discussed section self information loss function plays central role literature prediction survey 
return prediction problem general form 
quite clearly solutions problem sought particular assumptions data generating mechanism exact objectives 
classical statistical decision theory see assumes known probabilistic source generates data reasonable objective minimize expected loss 
optimum strategy minimizes expected loss past efl jx gamma gamma dp xjx gamma random variables denoted capital letters 
suitable assumptions stationarity ergodicity optimum prediction fb expected loss sense optimum sense minimizing sure asymptotic time average see 
gamma gamma quantity gamma inf dp xjx gamma referred conditional bayes envelope gamma example fx binary source delta delta hamming distance jx gamma jx gamma conditional bayes envelope gamma gamma jx gamma jx gamma gamma jx gamma gamma gamma jx gamma gamma addition underlying source known gaussian class linear predictors allowed known linear function gamma special case causal wiener filter see chap 

self information loss case gamma gamma minimizes gamma log jx gamma gamma best probability assignment true 
conditional bayes envelope gamma differential entropy gamma gamma gamma gammae log jx gamma gamma 
classical theory wiener prediction theory assumes source known realistic interesting situation occurs unknown existent 
second case probabilistic data generating mechanism data considered arbitrary deterministic 
cases fall category universal prediction problem referred probabilistic setting second called deterministic setting 
elaborate settings 
probabilistic setting probabilistic setting objective normally minimize expected cumulative loss asymptotically large simultaneously source certain class 
universal predictor fb gamma depend time keeps difference ef eu gamma jx gamma vanishingly small large cumulative bayes envelope eq 
represents performance optimal predictor tuned stationary ergodic source sequence limit referred asymptotic bayes envelope coincides cesaro theorem lim turn exists non increasing monotonicity 
self information loss case entropy rate means goal universal prediction equivalent universal coding 
essentially levels universality degree uncertainty regarding source 
universality respect indexed classes sources 
suppose source unknown member certain indexed class fp index set 
commonly designates parameter vector smooth parametric family families finite alphabet memoryless sources th order markov sources state sources ar gaussian sources index sets finite sets possible 
interesting issues 
devise universal prediction schemes asymptotically attain defined sense second performance bounds apply universal predictor 
analogously universal coding terminology extra loss referred redundancy 
redundancy bounds useful establish necessary conditions existence universal schemes limitations rate convergence 
dictated certain measure richness class fp furthermore redundancy bound vanish universal schemes defined sense exist question universality extended achieving bound 
self information loss prediction explicitly characterize bounds demonstrate certain universal schemes 
universality respect large classes sources 
suppose know source markov unknown finite order stationary ergodic mixing certain sense 
large classes quantitative characterizations uniform redundancy rates exist 
hope weak universality term mentioned defined means universality attained non uniform convergence rate 
weak universality obtained necessary sufficient conditions existence universal schemes 
hierarchical universality 
level goal devise universal schemes respect sequence index sets sources may necessarily structure nesting ae positive integer common example class th order markov sources alphabet 
prior knowledge may source index belongs straightforward approach consider big class seek universal schemes respect drawback approach pessimistic sense convergence rate slow existent rich class 
markov example falls category level union falls second level 
turns certain situations possible achieve redundancy rate essentially small known priori 
gives rise elegant compromise levels universality 
keeps fast convergence rates level sacrificing generality class sources second level 
deterministic setting setting observed sequence assumed randomly drawn probability law individual deterministic sequence 
difficulties defining universal prediction problem context 
associated setting desired goal 
formally sequence perfect prediction function defined gamma prediction problem seemingly boils triviality 
second difficulty way 
deterministic predictor fb delta adversary sequence time instant chosen maximize 
difficulty fundamental means limitations class allowed predictors severe overfitting effect tailors predictor sequence strongly fact anticipating completely misses essence prediction causal sequential mechanism 
limit class allowed predictors fb delta reasonable way 
example class predictors implementable finite state machines fsm states markov structured predictors form gamma gammak gamma 
limitations sense virtue avoiding reflect real life situations limited resources memory computational power 
stated formally class predictors seek sequential predictor fb universal sense independent time average loss gamma asymptotically gamma universal predictor need necessarily causal predictor minimizes average loss may definition depend entire sequence second difficulty mentioned alleviated allowing randomization 
words predictions generated random certain probability distribution depends past 
note different discussed case probability assignment assigned probability distribution randomization 
analogously probabilistic case distinguish levels universality accordance richness class level corresponds indexed class predictors dual mentioned indexed class sources 
examples parametric classes predictors finite state machines number states fixed order markov predictors predictors neural nets number neurons finite sets predictors second level corresponds large classes class finite state predictors specifying number states operating infinitely long sequences third level corresponds hierarchical universality parallels probabilistic setting 
nature reported results somewhat similar probabilistic approach important differences algorithmic aspects existence theorems performance bounds 
outline follows 
section devoted motivation justification self information loss function performance criterion prediction 
section probabilistic setting discussed great emphasis self information loss case fairly understood 
section deterministic setting described special attention similarity difference probabilistic setting 
section devoted concept hierarchical universality settings 
section summarizes open problems directions research 
self information loss function mentioned earlier self information loss function central role universal prediction 
section discuss motivations justifications loss function measure prediction performance 
explained predictive probability assignment outcome general informative estimating value outcome reasonable loss function monotonically decreasing assigned probability actual outcome 
self information loss function defined eq 
clearly satisfies requirement possesses desirable features fundamental importance 
advantage self information loss function technical 
convenient logarithmic function converts joint probability functions equivalently products conditional probabilities cumulative sums loss terms 
suits framework general prediction problem described 
technical convenience deeper significance 
known self information manifests degree uncertainty amount information occurrence event 
conditional self information past reflects ability deduce information past minimum uncertainty 
evidently prediction self information loss function lossless source coding intimately related 
relation stems fact gamma log ideal code length respect probability function delta 
code length implemented sequentially desired precision arithmetic coding 
conversely code length function translated probability assignment rule 
direct application loss minimization problem area prediction gambling case gamma represents distribution money invested possible values outcome 
self information loss function dictates exponential growth rate amount money time 
paradigm predictive probability assignment basis dawid prequential principle 
motivation prequential principle prediction probability assignment testing validity statistical models 
probability assignment behaves empirically expected true probabilistic model 
example fx binary sequence fb jx gamma probabilities assigned satisfy gamma law large numbers 
discussed requirements central limit theorem law iterated logarithm behavior confidence intervals 
interestingly turns predictive probability assignment self information loss criterion useful purpose testing validity statistical models described 
reason certain source governs data true conditional probability gamma gamma minimizes gamma log jx gamma gamma simpler words maximum achievable assigned probability true property shared specific loss functions see 
shannon mcmillan breiman theorem certain ergodicity assumptions true expected value sense surely 
combining prequential principle shannon mcmillan breiman theorem probabilistic model data gamma minimize gamma log jx gamma average self information loss 
perspective observe sequential probability assignment mechanism gives rise probability assignment entire observation vector jx gamma 
conversely consistent probability assignment satisfies gamma gamma provides valid sequential probability assignment jx gamma gamma choice fb self information loss prediction completely equivalent choice assigns maximum probability maximum likelihood estimation 
discussion far focused motivating self information loss function 
motivation studying universal prediction self information loss case sheds light universal prediction problem loss functions 
direct way look self information loss prediction mechanism generates probability distribution underlying source unknown non existent 
plausible approach prediction problem general loss function generate time instant prediction functional self information loss conditional probability assignment 
example squared error loss case reasonable predictor conditional mean associated gamma hopefully tends true conditional probability discussed 
seen probabilistic setting technique successful deterministic setting modification required 
way self information loss prediction serves yardstick prediction loss functions notion exponential weighting 
certain situations minimization cumulative loss corresponds maximization exponentiated loss gammaff ff turn treated altogether auxiliary probability assignment 
certain important special cases solution probability assignment problem translates back solution original problem 
see usefulness exponential weighting technique tool deriving lower bounds induced corresponding strong lower bounds self information loss case 
probabilistic setting problem probability assignment outcome past self information loss function 
explained problem completely equivalent finding probability assignment entire data sequence 
mentioned earlier source known clearly optimal minimizes expected self information loss prediction induced true underlying source gamma delta gamma gamma 
average cumulative loss entropy unknown wish assign certain probability distribution depend unknown extra loss entropy gamma log gamma gamma log jjq pkq nth order information divergence relative entropy corresponding lossless compression problem pkq coding redundancy normalized symbol difference average code length entropy 
course minimizations pkq sources fpg time contradictory 
problem universal probability assignment finding compromise uniformly close possible information divergence sense class sources 
shall elaborate notion simultaneous divergence minimization 
explained theory universality splits levels degree uncertainty regarding source 
conceptually simplest case source known belong indexed class sources fp index parameter vector index set 
look prediction view point probability assignment start self information loss criterion survey part largely taken theory universal coding 
indexed classes sources self information loss function describe common approaches universal probability assignment indexed classes sources 
plug approach versus mixture approach natural approach universal prediction respect indexed class sources fp called plug approach 
approach time instant index parameter estimated line gamma maximum likelihood estimator estimate gamma prediction true parameter value conditional probability assigned jx gamma 
plug approach may quite certain regularity conditions 
intuitively estimator statistically consistent jx gamma continuous gamma estimated probability assignment may converge true conditional probability probabilistic sense 
convergence property hold center cauchy density estimated sample mean rate convergence crucial importance 
true general better estimation conditional probability necessarily yields better self information loss performance 
plug approach essence heuristic approach lacks substantiated deep theoretical justification general 
alternative approach henceforth referred mixture approach generating convex combinations mixtures sources class fp specifically certain non negative weight function integrates unity thought prior define mixture probability mass density function tuples qw dw appropriate choice weight function mixture qw shall see turns possess certain desirable properties motivate definition universal probability measure 
universal measure induces conceptually simple sequential probability assignment mechanism defined jx gamma qw jx gamma qw qw gamma interesting note theorem predictive probability function induced mixture fp represented mixture conditional probability functions fp jx gamma weighting function posterior probability density function gamma qw jx gamma dw jx gamma jx gamma jx gamma gamma dw gamma gamma log gamma dw gamma expression manifests interpretation exponential weighting probability assignment performance log gamma data seen far points correspond performance past rewarded exponentially higher weights prediction outcomes 
exponential weighting important concept 
elaborate broader context lower bounds algorithms sequential prediction general loss functions probabilistic deterministic setting 
class binary memoryless bernoulli sources prfx mixture approach delta uniform leads known laplace prediction 
suppose gamma contains zeros gamma ones qw jx gamma gamma gamma gamma case thought plug algorithm interpreted biased version maximum likelihood estimator 
bias clearly desirable sequential regime naive maximum likelihood estimator gamma assign zero probability occurrence turn result infinite loss 
bias gives rise plausible symmetry consideration absence data gamma assign equal probabilities 
case estimator form fi fi fi 
weight functions dirichlet family yield different bias terms slight differences performance see 
discussion carries general finite alphabet memoryless sources discussed markov chains 
kept mind general family sources fp mixture approach necessarily boil plug algorithm choice weight function dramatic impact performance 
case theoretical guidance regarding choice accomplished forthcoming subsection establish theoretical justification mixture approach fairly strong sense 
interestingly section motivated deterministic setting loss functions loss function 
minimax maximin universality seen eq 
excess loss associated probability assignment underlying source jjq 
fundamental justification mixture approach simple fact arbitrary probability assignment exists probability assignment qw convex hull fp mixture jjq jjq simultaneously means seeking universal probability assignment loss optimality reasonable sense confine attention merely convex hull class fp interesting fact tell select weight function delta mixture qw additional observations 
mentioned earlier wish find probability assignment independent unknown guarantees certain level excess loss minimum achievable loss known nth order entropy 
referring eq 
suggests solve minimax problem inf sup kq inf sup dw kq value quantity normalizing called minimax redundancy denoted literature universal coding 
glance approach somewhat pessimistic worst case approach 
fortunately cases interest means minimax asymptotically achieves entropy rate uniformly rapidly shall see shortly minimax approach loss case pessimistic tend zero 
view discussion previous paragraph minimax optimal mixture sources class 
alternative minimax criterion maximin criterion definition strong bayesian flavor gives rise mixture approach seemingly different point view 
idea unknown postulate prior probability density function performance probability assignment judged respect normalized weighted average redundancy jjq dw kq easy see minimizes just qw defined resultant average redundancy qw exactly mutual information theta random variables theta joint probability density function 
arbitrary question arises appropriate choice 
adopt worst case approach favorable prior maximizes inf solve maximin problem sup inf value normalized referred maximin redundancy denoted gamma important note gamma supremum theta allowable interpretation capacity channel theta defined class sources 
definition source thought conditional probability function channel output channel input theta 
refer channel capacity capacity class sources fp denote identical gamma notions minimax maximin universality defined davisson context universal coding see 
years davisson observed gallager independently davisson ryabko minimax maximin solutions equivalent gamma furthermore mixture qw capacity achieving prior theta minimax maximin optimal 
result referred redundancy capacity theorem universal coding 
capacity measures richness class sources 
pointed sensitive distances sources class effective number essentially distinct sources 
example source generates probability infinite divergence distance source generates 
mixture level tuples normalized divergence capacity fp small 
remarkable fact theory universal coding intimately related channel capacity 
importance significance redundancy capacity theorem fairly deep broader context probability assignment prediction 
face point problem universal probability assignment equivalently universal prediction self information loss function respect indexed class sources fairly addressed 
important issues considered 
concern comes practical aspect 
explicit evaluation proposed minimax maximin probability assignment trivial 
capacity achieving prior hard evaluate general 
furthermore computed explicitly corresponding mixture qw induced conditional probabilities qw jx gamma hard compute 
contrast plug approach relatively easy implement 
shall return earlier example mixtures bernoulli sources generally finite alphabet memoryless sources see fortunately satisfactory approximations available 
second technical point evaluation capacity asymptotic behavior crucial importance 
mentioned earlier capacity measures complexity richness class sources uniform redundancy rates achievable strong universality 
means class sources rich vanish grows bound longer hope uniformly small redundancy rates 
shall see examples 
problem calls attention predictor sequential probability assignment mechanism proposing really sequential sense horizon prescribed advance 
reason capacity achieving prior depends general 
possible remedy problem computability seek fixed prior independent achieves capacity asymptotically lim theta nc 
fortunately possible important examples 
mentioned earlier minimax approach pessimistic essence fact special concern tend zero grows 
reason jjq nc guarantees lower bound jjq nc valid source class 
maximin point view tells holds true sense weighted average jjq respect optimality qw seemingly somewhat weak grounds 
closer inspection reveals right hand side eq 
essentially lower bound stronger sense discussed 
strong converse theorem turns self information loss case remarkable concentration shown jjq gamma ffl nc ffl values 
term means total probability mass points property respect asymptotically approximation tends unity 
means right hand side eq 
slightly reduced multiplied factor gamma ffl lower bound values 
referring uniform upper bound means sources class lie near surface sphere divergence sense radius nc centered qw considering fact assumed virtually structure class sources quite surprising 
roots explained discussed detail relation competitive optimality property self information function see 
technical concern class finite alphabet sources finite capacity achieving prior discrete support points corollary 
strictly speaking measure ignores points outside support term sources meaningful 
fortunately important examples find smooth weight function independent asymptotically achieves capacity 
solves difficulty horizon dependency problem mentioned earlier 
alternative remedy general version strong converse result allows arbitrary weight function tells qw optimal points note jjq may depend general uniformity property lost 
result fact stronger version redundancy capacity theorem detailed generalizes known strong converse universal coding theorem due rissanen smooth family fp capacity behaves log dimension parameter vector 
rissanen award winning show strong converse theorem applies sources time 
reader referred see detailed discussion theorem significance general perspective rissanen particular 
examine examples light findings 
examples simplest example ng sources pn class weight function represented vector wn nonnegative numbers summing 
case described concentration phenomenon sharper theorem general case jjq nc 
words sources lie exactly surface divergence sphere qw sources fp easily distinguishable sense reliably identify sources generated vector redundancy capacity class nearly log channel input decoded channel output small error probability 
case tends uniform ng best mixture qw essentially uniform mixture 
sources easily distinguishable redundancy capacity smaller 
thought situation channel noisy alternatively effective number distinct sources smaller extreme case pn expected fact source class 
revisit bernoulli example generally class memoryless sources finite alphabet size obviously parametric class natural parameterization letter probabilities gamma degrees freedom 
mentioned earlier discrete finite alphabet case depends horizon difficult compute 
turns smooth parametric families bounded parameter set considered sensitivity exact shape qw long bounded away zero fact nice prior essentially achieves leading term capacity gamma log differences performance different choices reflected higher order terms 
specifically clarke barron derived accurate asymptotic formula redundancy associated mixture jjq gamma ln ln ji ji determinant fisher information matrix fp see takeuchi barron extensions general exponential families 
maximin setting weighted average jjq asymptotically maximized neglecting term prior maximizes second term known jeffreys prior ji ji case ji inversely proportional square root product letter probabilities turn special case dirichlet prior general form proportional product arbitrary fixed powers dirichlet mixtures qw conditional probabilities derived easy closed form expressions 
generalizing earlier bernoulli example size alphabet parametric family jeffreys prior get universal probability assignment qw jjx gamma gamma number occurrences gamma 
uniform prior leads laplace estimator discussed earlier special case dirichlet prior 
noted jeffreys prior asymptotically achieves capacity induces asymptotically maximin probability assignment 
interestingly observed asymptotically minimax slightly modified obtain minimax optimality 
results extend general parametric families certain regularity conditions detailed cited papers 
main point remembered parametric classes choice crucial terms performance 
gives rise freedom selecting prior implementational considerations availability closed form expressions mixtures conjugate priors 
just seen example dirichlet prior classes memoryless sources 
example consider case fp family gaussian memoryless sources mean variance 
clearly qw respect gaussian prior gaussian case 
idea conjugate priors carries natural manner general exponential families 
pointed extensions redundancy capacity theory classes sources capacities proportional number attributed general notion dimensionality induced hellinger distance kullback leibler distance vc dimension extensions wider classes sources exhibit different behavior redundancy capacity 
general underlying information theoretic principle remains richness class measured shannon capacity 
examples classes sources necessarily parametric 
general loss functions turns satisfactory solutions universal prediction problem self information loss function may prove useful general loss functions 
intuitively suitable continuity conditions optimal predictor respect estimator jx gamma close optimum true conditional probability 
generally speaking minimum self information loss probability assignments essentially maximum likelihood estimates cf 
section statistically consistent situations requirement satisfied 
specifically discrete alphabet case denote underlying source consider universal probability assignment qw kq nc pinsker inequality see chap 
problem concavity square root function jjq gamma gamma jx gamma log jx gamma jx gamma ln delta gamma gamma jp jx gamma gamma jx gamma ln delta gamma gamma jp jx gamma gamma jx gamma general loss function gamma arg min fl jx gamma gamma denotes expectation respect gamma arg min eq fl jx gamma gamma eq denotes expectation respect assume non negative bounded constant 
inequality get gamma gamma gamma jx gamma gamma gamma gamma jx gamma jp jx gamma gamma jx gamma gamma gamma gamma jp jx gamma gamma jx gamma gamma gamma gamma jp jx gamma gamma jx gamma ln words optimum predictor respect universal probability assignment qw ln close optimum simultaneously important result existence universal predictors uniformly rapidly decaying redundancy rates self information criterion sufficient condition existence predictors general loss functions 
point comments order assumption boundedness weakened 
example left side eq 
thought generalized divergence upper bounded terms variational distance adopted boundedness assumption simplify exposition 
second comment upper bound eq 
tight true redundancy rate faster certain situations 
example minimum mean square error fixed order universal linear predictors redundancy rates small log upper bound gives log 
question arises provide precise characterization achievable redundancy rates tight upper lower bounds respect general loss functions 
natural way handle question take minimax maximin approach similarly self information loss case 
minimax predictor fb minimizes sup gamma sup dw gamma unfortunately known closed form expression minimax predictor general loss function 
game theoretic arguments tell minimax problem equivalent maximin problem 
analogously self information loss case maximin problem defined supremum inf fb dw gamma inf fb gamma dw non negative weight functions delta integrate unity 
general minimax maximin problems known equivalent convex concave cost functions 
case eq 
affine concave remaining condition set allowable predictors convex convex fb 
condition holds example jb gamma xj ff ff 
maximin optimal predictor clearly minimizes fl jx gamma gamma worst case choice maximizes qw gamma dw general maximizing may agree capacity achieving prior defined self information loss case 
similarly eq 
considerations justify approach bayes optimal prediction respect mixture fp pointed certain cases parametric case prediction performance sensitive exact choice definition vanishingly small minimax redundancy rates guarantee uniform convergence bayes envelope 
self information loss case general loss function necessarily concentration phenomenon points lie nearly redundancy level 
example bernoulli case hamming distance optimal predictors predicts predicts prfx smaller larger 
easy find zero redundancy predictor half sources class non trivial lower bound redundancy applies sources 
concept exponential weighting cases possible derive strong lower bounds hold points time 
specifically assume estimate subtraction operation gamma welldefined loss function form ae gamma function ae monotonically increasing monotonically decreasing ae 
derive lower bound ae gamma gamma holds points predictor fb depend 
extend lower bound universal minimum mean square error prediction gaussian arma processes rissanen 
assume ae delta sufficiently steep sense dz define log moment generating function gamma log dz oe inf sd gamma function oe interpreted differential entropy associated probability function tuned ae expectation operation respect predictor fb consider probability assignment ds gamma gamma delta locally bounded away zero prior gamma log approximated follows 
gamma log delta inf delta ae gamma gamma gamma log delta oe ae gamma gamma log small remainder term 
strong converse self information loss case points gamma log oe ae gamma gamma log gamma ffl ffl sufficiently large oe delta concave interchanging order expectation operator function oe decrease expression right hand side line eq 
oe ae gamma gamma gamma ffl gamma log gamma ffl sufficiently large oe monotonically non decreasing gives lower bound ae gamma gamma lower bound tight 
evidently tightness depends defined satisfies reverse inequality predictor 
turn case self information lower bound achievable universal predictive coding models prediction error gamma gamma memoryless process marginal 
referring case bound non trivial oe entropy rate case lower bound suggests converse previous statement conditions uniform redundancy rates existence universal predictors uniformly rapidly decaying redundancy rates self information criterion necessary condition existence predictors general loss functions 
summary suitable regularity conditions uniform redundancy rate general self information loss function 
furthermore oe requirement bound non trivial log bernoulli case possible achieve zero redundancy half sources mentioned earlier log bound meaningless 
consider important example bound useful 
ae zero mean gaussian density function variance 
log moment generating function ln gamma delta differential entropy oe ln ed 
gamma gamma exp ae gamma ffl gamma ln gamma oe fp class gaussian arma sources driving noise variance oe ln eoe ln obtain gamma gamma oe expf gamma ffl ln oe gamma ffl ln bound obtained rissanen known tight autoregressive case 
example class gaussian sources fv zero mean gaussian noise power oe deterministic signal power lim sup limited relative bandwidth normalized limited 
ln eoe ln oe capacity band limited gaussian channel gives lim inf gamma gamma oe exp ae gamma ffl ln oe oe oe oe gammaffl bound recall corresponding universal probability assignment problem solved mixture qw respect capacity achieving input gaussian qw gaussian 
qw turn factored product qw jx gamma conditional densities gaussian density exponent depends gamma gamma delta linear predictor asymptotic variance expf ln 
oe 
power spectral density input process 
shown techniques similarly bayesian linear predictor asymptotically attains bound 
approach derivation lower bounds performance universal schemes proposed broader context multi armed bandit problem 
line tight upper lower bounds redundancy rates class uniformly schemes sense adapting underlying source 
results confined case finite set 
large classes sources far discussed classes sources exists uniform redundancy rate terms capacity self information loss case 
capacity may may tend zero predictive self information performance compression ratio corresponding universal code log alphabet size provided sufficiently large 
means degree compression non uniform probability assignment achievable sources time longer hope approach entropy 
section focus wider classes sources property longer exist 
classes rich self information loss case finite predictive probability assignment exists source class gamma log log gamma 
words total breakdown terms loss performance similar behavior loss functions 
happens instance class stationary ergodic sources class finite order markov sources limiting order classes represented infinite unions nested index sets ae ae 
universal schemes approach entropy rate generally asymptotic bayes envelope may exist insist uniform redundancy rates 
words weakly universal schemes available 
example lempel ziv algorithm predictive probability assignment induces weakly universal class stationary ergodic sources finite alphabet :10.1.1.14.2892
necessary sufficient conditions existence weak universality 
straightforward observation analysis similar eq 
sufficient condition existence weakly universal predictor general bounded loss function existence predictor probability assignment case 
predictive probability assignment respect self information loss function crucial importance 
view fact fundamental problem context estimating conditional probabilities 
cover raised question possible produce consistent estimates conditional probabilities jb xjx gamma gamma xjx gamma surely 
bailey gave negative answer question see ryabko proposition pointed positive result orenstein similar question 
states sided stationary binary process possible estimate value xjx gamma gammat strongly consistently 
proposed estimates finite order markov approximations order depends data 
similar estimator xjx gamma turns converge true value sense weaker sure sense 
estimator shown bailey give log jx gamma jx gamma surely 
algoet gave extension orenstein results general alphabets simplified 
simplified estimator empirical averages finite alphabet case expense losing strong consistency property 
estimator consistent self information sense stationary lim ae log jx gamma gamma jx gamma gammat oe implies consistency sense 
line research concentrates square error loss function 
minimum mean square error predictor known source conditional mean gamma efx jx gamma gamma direction focuses consistent estimation conditional mean 
gaussian processes unknown covariance function davisson shown kth order linear predictor empirical covariances gives asymptotic cumulative mean square error behaves oe ln oe residual error optimal kth order linear prediction known covariances 
letting grow sufficiently slowly time conditional mean infinite past eventually attained 
general stationary processes sample averages certain spacing time instants order estimate efx jx gamma fixed time instant 
modha considered mixing processes proposed estimator slow increase prediction memory complexity regularization methods 
limitation method depends knowledge mixing rate 
meir proposed complexity regularization method spirit complexity class allowable predictors limited finite vapnik chervonenkis vc dimension 
general loss function algoet see special case log optimum investment proved strong ergodic theorems cumulative loss 
known station ary ergodic source shown strategy minimizes conditional mean past optimal sure limit time average loss 
unknown empirical estimates conditional probability provided 
plugging estimates true universal schemes obtained ergodic property 
deterministic setting traditional probabilistic setting prediction described previous section assumes data generated mechanism characterized statistical terms memoryless source markov source generally arbitrary stationary ergodic source 
seen observer estimates line explicitly plug approach implicitly mixture approach conditional probability outcome past uses estimate prediction outcomes 
comes deterministic setting individual data sequences underlying philosophy substantially different 
longer assumption ensemble sequences generated underlying probabilistic mechanism arbitrary deterministic individual sequence 
best prediction strategy possibly fixed sequence 
realize stated question completely trivial meaningless 
explained formally sequence perfect predictor suffers zero loss particular sequence 
time particular predictor extremely bad sequences 
evidently tailoring predictor particular sequence hope track strategy predictor sequential regime inherent task prediction 
root overfitting effect lies fact allowed discussion freedom choice predictor 
loosely speaking freedom amount information choice predictor large amount information conveyed sequence 
roughly speaking situations algorithm learns data heart performing task expect 
unavoidable limit freedom choice predictors certain class 
limited class allowable predictors henceforth referred comparison class target class denoted single universal predictor competes best predictor simultaneously sense gamma asymptotically gamma 
universal predictor need necessarily predictor choice predictor minimizes average loss may depend definition entire sequence difference performance sequential universal predictor best predictor manifests regret choice optimal predictor best done retrospect known entire sequence advance 
loosely speaking fairly strong duality probabilistic deterministic setting 
certain assumptions limitations data sequences encounter prior limitations class prediction algorithms way 
deterministic setting frequently considered stronger appealing underlying model better connected practical situations known probabilistic mechanism generates data hand algorithmic resources limited 
facts shed light duality probabilistic deterministic setting quite frequently comparison class defined collection predictors obtained optimal solutions certain class sources parallel probabilistic setting 
example fixed predictors gamma constant independently gamma optimal memoryless stationary sources linear predictors sufficient gaussian case markov predictors adequate markov processes 
cases remarkable degree duality analogy results obtained deterministic setting corresponding probabilistic setting notwithstanding considerable difference concepts 
specifically results individual sequence setting completely analogous probabilistic counterparts probabilistic source replaced empirical measure extracted individual sequence respect certain sufficient statistics induced structure section similar previous section manifest analogy 
certain aspects scenarios diverge shall see 
similarly previous section emphasis information theoretic point view largely focuses self information loss function 
indexed comparison classes analogy indexed class sources extensively discussed previous section probabilistic setting considerable attention literature dual comparison classes deterministic setting 
indexed comparison class predictors class represented fb designates index index set 
similarly subsection index set finite set ng positive integer may may grow countably infinite set continuum compact subset real line higher dimensional euclidean space parameter smooth parametric class combination 
noted cases defined optimum predictor certain member indexed class sources cf 
subsection 
self information loss analogy section consider self information loss function equivalently probability assignment problem individual sequences 
words goal sequentially assign universal probability mass function jx gamma observed sequence gamma log essentially small gamma log max jx gamma sequence uniformly possible 
shtarkov demonstrated possible minimizing quantity max gamma log gamma gamma log max jx gamma specifically minimax optimal probability assignment attained normalized maximum likelihood function max jx gamma normalization factor max jx gamma readily seen definition gamma log gamma log max jx gamma log universal probability function essentially assigns uniformly high probabilities assigned best member comparison class provided grow exponentially rapidly example fb class finite alphabet memoryless probability assignments jx gamma designating vector gamma free letter probabilities easy show method types grows asymptotically proportion eq 
behaves log turn behavior obtained smooth parametric families probabilistic setting 
number gamma gamma log interpretation deterministic analogue minimax redundancy capacity maximization redundancy probabilistic setting replaced maximization possible sequences intuitively gamma measure richness comparison class predictors addition capacity probabilistic setting 
turns relations quantities 
demonstrate relation gamma operational notion capacity maximum reliable transmission rate note ng quantity interpreted delta probability correct decision hypotheses testing problem involving sources jx gamma induced predictors uniform prior true max means sources fp far apart distinguishable high probability minimax redundancy essentially log compare example section 
countably infinite continuum finite subset ng gives lower bound manner 
grows normally decreases product np kept large long smaller ncn transmit rate capacity allows keeping close unity 
maximum achievable product np achieved rates capacity 
easy show directly gamma smaller class sources probability assignments indexed implies necessary condition existence minimax universality deterministic setting existence parallel property dual probabilistic setting 
smooth parametric case gamma behave log precisely see rissanen log log ji gamma log log ji turns richer indexed classes may exhibit considerably larger gap quantities see example arbitrarily varying sources 
main drawback ml probability assignment obviously practical side hard compute general importantly horizon dependent sequence length prescribed 
alleviate difficulty maximum likelihood max jx gamma exponentially approximated mixture laplace integration 
specifically case stationary memoryless probability assignments shtarkov proposed dirichlet jeffreys prior mixture leads purely sequential probability assignment ajx gamma gamma number occurrences letter gamma mentioned earlier section family sequential probability assignments arise dirichlet weighting general 
interesting property dirichlet addition jeffreys prior family asymptotically ml probability assignment 
specifically gamma defined max gamma log jx gamma gamma gamma log max jx gamma log const constant larger obtained refinements extensions result carried 
specifically xie barron introduce dual notion maximin redundancy regret value coincides gamma show jeffreys mixture asymptotically maximin asymptotically constant regret sequences empirical pmf internal simplex 
similarly probabilistic setting asymptotically minimax problematic sequences boundary simplex 
slight modification jeffreys mixture depends horizon dependent asymptotically minimax maximin 
weinberger merhav feder studied problem universal probability assignment individual sequences self information loss function respect comparison class probability assignments implementable finite state machines fixed number states 
accurate formulas regarding higher order redundancy terms 
shown log behavior minimax sequences tight lower bound sequences types defined respect finite state probability assignments 
result parallels optimality universal probability assignments probabilistic setting cf 
section 
context interesting note shown contrast probabilistic setting plug approach fails general comes individual sequences 
elaborate results section context hierarchical comparison classes 
general loss functions problem universal sequential prediction decision making individual sequences general loss functions definitely wider problem area special case probability assignment self information loss function discussed far section 
fact classical problem area various scientific disciplines concentrated primarily case constant predictors predictors yields certain fixed prediction regardless observed past 
example certain value may suggest predict outcome binary sequence may assign probability outcome 
seemingly interesting comparison class past information entirely ignored 
motivation carefully studying simple comparison class fundamental examining comparison classes sophisticated predictors 
example order markov predictor characterized jx gamma jx gamma thought binary case combination fixed predictors operating respectively subsequences corresponding time instants ftg follow gamma gamma 
having observation problem boils back constant predictors 
example closely related self information portfolio selection optimal investment stock market :10.1.1.56.1067
model goal maximize asymptotic exponential growth rate capital current investment strategy depends past 
corresponding loss function framework gamma log dimensional vectors nonnegative components components sum unity 
vector represents return monetary unit investment opportunities stocks vector characterizes fraction current capital allocated stock 
cover cover ordentlich techniques similar self information loss described develop sequential investment algorithm related universal coding results similar flavor 
universal sequential strategy competes best constant investment strategy 
results viewed extension self information loss special case vector zero component corresponding current alphabet letter 
sequential compound decision problem examples loss functions closely related self information loss consequently techniques results considerably different 
comparison class constant strategies general loss functions studied somewhat general setting referred sequential compound decision problem robbins thoroughly investigated researchers disciplines mathematical statistics game theory control theory see 
fundamental findings compound sequential decision problem summarized theory bayes decision rules includes notion bayes envelope best achievable target performance functional empirical pmf sequence analysis basic properties 
turn combined theory provides simple necessary sufficient conditions player case predictor repeated zero sum game reach certain performance level case bayes envelope strategy opponent player case nature chooses adversary sequence 
sequential compound decision problem general setting sense observer assumed access noisy versions sequence loss function minimized associated clean sequence expected cumulative loss probabilistic limit respect ensemble noise processes 
hannan taken gametheoretic approach develop upper bounds decay rate regret showing convergence rate gamma finite alphabet finite strategy space case rate gamma gammaff continuous case provided loss minimizing strategy functional underlying empirical pmf bayes response satisfies lipschitz condition order ff 
ff normally case means convergence rate log similarly self information loss case seen 
essential ideas underlying analysis techniques simple sandwich argument see easy show bayesian envelope upper lower bounded average loss associated strategies 
current strategy upper bound optimal data seen far gamma lower bound imagined strategy allowed access optimization strategy lower bound sees merely outcome upper bound 
comparison class constant strategies bayes envelope depends sequence empirical pmf additional observation perturbs current empirical pmf term proportional appropriate smoothness conditions ff instantaneous losses upper lower bound differ quantity scales proportionally averaged integers gives log 
fortiori difference upper bound bayes envelope regret exceed log 
important special cases loss function bayes response discontinuous 
happens example prediction binary sequences criterion relative frequency mispredicted outcomes bayes response respect class constant predictors binary depends relative frequency zeroes 
case randomization sequential prediction strategy discontinuity point see necessary order achieve target performance problematic sequences empirical pmf visit infinitely discontinuity points 
cost randomization considerable slowdown rate convergence bayes envelope 
binary case example rate convergence parallel probabilistic setting randomization needed cf 
section fast 
van shown case smooth loss functions convergence rate tightly upper bounded gamma log certain regularity conditions channel observer receives noisy measurements 
investigated convergence rates special case square loss function gamma various sets assumptions 
papers deal general case comparison class consists markov strategies importance emphasized 
line prediction expert advice completely different point view taken primarily learning theorists studies paradigm referred line prediction expert advice see 
previously defined terminology basic assumption comparison class consists finitely predictors referred experts 
absolutely assumptions structure relationships experts 
goal devise sequential universal prediction algorithm performs essentially best experts individual sequence 
examined earlier scenario context self information loss function finite index set ng necessary minimax price universality need exceed log worst case probability assignments correspond distinguishable sources 
interestingly behavior essentially continues take place general sufficiently regular loss functions 
vovk littlestone warmuth proposed independently sequential prediction algorithm regret respect best expert exceeds log constant depends solely loss function heart algorithm remarkable similarity mixture approach concretely notion exponential weighting discussed section special case self information loss 
idea constant chosen consider weighted average prediction ith expert time weight assigned expert time 
weights time instant nonnegative numbers summing unity 
intuitively assign higher weights experts proven better past 
reasonable thing eq 
assign expert weight proportional gammaj gamma summation defined zero uniform initial weighting 
fortunate exists strategy easy see strategy serve purpose 
true condition suggests conceptually simple algorithm 
initialization set 
prediction choose prediction time satisfies eq 


update receiving update weight function 
iteration increment go 
follows immediately definition algorithm exponent cumulative loss associated fb satisfies gammaj gammaj max gammaj min ln crucial question remains addressed regarding conditions eq 
satisfied 
put question perspective observe self information loss function functions gammaj probability measures tuples 
weighted average mixture probability measure represented gammaj certain fb probability assignment corresponding finite mixture 
general function delta delta may closed convex combinations 
fortunately shown fairly mild regularity conditions see details guaranteed condition holds provided chosen case regret small ln important loss functions self information loss square error loss satisfy conditions 
example function concave case linear prediction squared error loss conditions exp clear weighted average experts predictions suitable solution 
unfortunately important loss functions loss function jx gamma bj 
means loss functions regret behave log decays slower rate cases handled separately 
algorithm interesting fact turns sense ln asymptotic lower bound maximum regret 
unfortunately weak point lower bound maximum taken sequences fx possible sets experts 
algorithm asymptotically optimal extremely pessimistic sense special concern large 
left desired stronger bound depends relationships experts 
extreme example experts identical fact expert expect obtain zero regret 
intuitively formal number experts replaced notion effective number distinct experts analogy extension role played capacity gamma self information loss case 
best knowledge date reported results kind literature lugosi characterized minimax regret upper lower bounds binary sequences hamming loss function constructive algorithm 
drawback associated algorithm 
algorithm practice implement parallel prediction algorithms proposed experts computationally demanding large contrast situation certain special cases experts correspond finite state machines number states 
cases explicit implementation finite state machines parallel 
spite shortcomings problem line prediction expert advice attracted fairly attention years quite reported extensions modifications variations theme see summary line learning 
extension especially interesting tie setting compound sequential decision problem sense predictor accesses noisy observations loss function remains terms clean outcomes 
clearly weighting algorithm form directly implementable perfect feedback loss associated past expert advice 
large comparison classes section natural analogue case large classes sources probabilistic setting large comparison classes predictors normally uniform redundancy rates 
general level consider nested infinite sequence index sets ae ae union strictly speaking index set members form smallest integer basic property different index sets subsection rich finite sequence minimum cumulative loss predictors indexed zero 
words freedom confronting undesirable overfitting effect discussed earlier 
happens important examples consists class finite state predictors undetermined finite number states class markov predictors specifically linear predictors unspecified finite order quite clearly situations degrees freedom tailor perfect predictor finite sequence earlier definition cf 
subsection target performance min meaningless 
lead modify definition target performance 
key principle doing keep asymptotic regime 
fix ideas consider infinite sequence designates outcomes similarly subsection define min assumed index set type discussed subsection 
asymptotics grow bound define lim sup lim sup operation manifests worst case approach sequence necessarily ergodic limit may exist worry worst performance level obtained infinitely define target performance lim limit clearly exists fu monotonically non increasing sequence elements obtained minimizations increasing sets predictors 
limit taken asymptotic regime meets mentioned requirement 
problem devise universal prediction algorithm fb asymptotically achieves 
popular applications general scenario consists strategies implementable finite state machines means corresponds class finite state machines states 
specifically member defined functions function referred state function describes evolution state machine kg recursion gamma gamma initial state fixed 
function describes strategy time depends idea model state variable represents limited information machine memorize past gamma purpose choosing current strategy 
important special case finite state machine states th order markov machine called finite memory machine gammak gamma 
ziv lempel described famous target performance spirit context data compression individual sequences finite state machines :10.1.1.14.2892
best lim sup compression ratio obtained finite state encoders infinitely long individual sequences defined sense referred finite state compressibility known lempel ziv algorithm lz shown achieve finite state compressibility sequence 
ziv lempel extended definition compression dimensional arrays images additional ingredient defining scanning strategy 
results spirit obtained sequential gambling individual sequences comparison class gambling strategies implementable finite state machines 
gambling problem completely analogous data compression precisely probability assignment self information loss function see discussed subsection results largely similar ziv lempel :10.1.1.14.2892
formal setting somewhat compliant general definition cumulative loss minimization loss term depends outcome :10.1.1.14.2892
results turn provided trigger comparison class finite state predictors binary sequences studied hamming loss function defined 
words case simply estimate value outcome performance measure relative frequency prediction errors 
analogously quantity special case called finite state predictability similarly confined class kth order markov predictors correspondingly defined called markov predictability main pointed :10.1.1.14.2892
finite state predictability markov predictability equivalent means sufficient confine attention markov predictors order achieve finite state predictability 
worthwhile note probabilistic setting result expected certain mixing conditions effect remote past fades away time evolves immediate past stored state markov predictor essential 
comes individual sequences finding trivial sequence arbitrary parallel assumption mixing fading memory 
proof result stems pure information theoretic considerations 
second largely algorithmic side 
turns prediction strategy corresponds probability assignments incremental parsing procedure lz algorithm see asymptotically achieves finite state predictability 
incremental parsing procedure sequentially parses sequence distinct phrases new phrase shortest string identical previously parsed phrase 
reason incremental parsing procedure works markov predictor time varying order long run large time phrases longer longer 
consequently markov predictability finite state predictability eventually attained 
deep point lies simple fact incremental parsing algorithm originally developed building block compression algorithm serves engine probability assignment mechanism useful prediction 
gives rise idea probability assignment induces universal probability measure context individual sequences 
loosely speaking means universal probability measure proportional lz lz codeword length 
turn thought extension shtarkov ml probability assignment known upper bound vanishingly small terms max maximum taken finite state sources fixed number states 
problem extended directions simultaneously alphabet loss function assumed general 
classes predictors deterministic finite state predictors considered randomized finite state predictors state function randomized families linear predictors results turn carry general case 
additional result theorem see relates individual sequence setting back probabilistic setting :10.1.1.14.2892
tells suitable regularity conditions stationary ergodic process gamma quantity defined respect finite state markov predictors agrees surely probabilistic performance measure inf efl jx gamma gamma special case result finite state compressibility surely equal entropy rate stationary ergodic source :10.1.1.14.2892
important example corresponds case class linear predictors order linear predictability 
stationary ergodic case cited result suggests probability coincides variance innovation process residual linear prediction error ffl exp ln 

power spectral density process 
duality certain classes sources corresponding classes predictors quite straightforward relatively small indexed parametric classes result establishes parallel duality large class stationary ergodic sources large class finite state predictors markov predictors 
hierarchical universality far focused substantially different situations universal prediction take place probabilistic setting deterministic setting universality respect indexed class relatively small opposed universality respect large class uniform redundancy rates exist 
extreme situations reflect interplay conflicting goals fast decay redundancy rates hand universality respect classes wide general possible 
example lempel ziv algorithm data compression predictive probability assignment universal stationary ergodic sources memoryless source encountered algorithm gives redundancy rate slower universal scheme tailored class memoryless sources see 
basic assumption section large class sources probabilistic setting predictors deterministic setting represented countable union sequence index sets may necessarily certain structure nestedness ae ae 
probabilistic setting example naturally comes mind class discrete kth order markov sources union large class finite order markov sources 
furthermore finite alphabet case slightly extend class take closure respect information divergence distance measure include class stationary sources 
stationary source approximated divergence sense sequence markov sources growing order theorem theorem 
examples hierarchical probabilistic models finite state sources deterministic randomized state functions ii tree sources iii noisy versions signals representable countable families basis functions iv arbitrarily varying sources sources countable alphabets referred sequences classes growing alphabets vi piecewise stationary memoryless sources 
examples dual comparison classes deterministic setting 
refers probabilistic deterministic setting term class corresponds class sources probabilistic setting comparison class predictors deterministic setting 
view discussion paragraphs natural question arises point devise universal predictor enjoys benefits small indexed class large class 
words possible universal predictor respect large class additional property performs essentially best universal predictor indexed subclass probabilistic setting means fortunate source happens member relatively small indexed class memoryless source redundancy regret essentially best universal predictor smaller class 
analog deterministic setting universal predictor large class behave similarly best universal predictor certain indexed comparison subclass 
note question meaningful merely finite countably infinite union reason uniform redundancy rate redundancy capacity denoted self information loss case larger subset 
case treating just big class best thing 
probabilistic setting ryabko address interesting question described nested sequence classes markov sources self information loss universal coding 
generally speaking ryabko idea apply conceptually simple part code referred twice universal code 
part code codeword integer length log log log second part universal code respect chosen minimize total codeword length 
clearly code attains redundancy min obviously exceeds true value behaves log markov case additional term affect rate convergence uniform redundancy rates simultaneously entire class markov sources asymptotically optimal behavior alternative part code transformed easily prediction scheme mixture approach 
specifically problem prediction self information loss suggested solution probability assignment formed stage mixture integers 
observation mixture approach appropriately chosen weight functions worse part scheme 
see assume fl satisfy kraft inequality equality improved consider stage mixture gammal dw gammal qw capacity achieving prior gamma log gamma log max gammal qw min gamma log qw left side corresponds performance mixture approach rightmost side corresponds performance part scheme optimum mixture class 
message individual sequence mixture approach worse part approach 
point explored developed examples hierarchical classes finite state machines view fact term right side lower bound sequences fairly strong sense cf 
section 
course chain inequalities continues hold expectations probabilistic setting 
turns probabilistic setting mixture approach worse part approach optimal approach sharper deeper sense 
extension result optimality qw cf 
section holds hierarchies classes theorem stage mixture arbitrary weight functions fw delta classes gammal positive integers simultaneously minimizes essence redundancy points classes addition capacity achieving prior minimum redundancy decomposed sum terms capacity underlying class second extra redundancy term reflects additional cost universality respect unknown term upper bounded log assume classes easily distinguishable sense exists model order estimator small average error probability theorem asymptotically tight bound 
means case distinguishable classes optimal performance level higher order term considerably larger large classes easily distinguishable mixture approach yields smaller second order redundancy term part coding approach continues give guidelines regarding choice equivalently fl 
noted monotone non increasing sequence probabilities log log optimum redundancy distinguishable case asymptotically attained universal code integers 
viewpoint sequential predictive probability assignment part method method mixtures directly implementable minimizing depends entire fw may depend possible alternative non sequential minimization line estimation plugin 
algorithm spirit proposed weinberger rissanen feder hierarchies tree sources probabilistic setting estimator associated context case algorithm context 
fortunately probability error estimating decays sufficiently rapidly leave leading redundancy term unaffected 
deterministic setting shown method plug estimate sequences resulting redundancy higher achieved class known advance 
mixture approach useful probabilistic setting deterministic setting giving reason prefer 
overcome problem mentioned fact weights mixture index depend horizon fixed weight functions 
fortunately mentioned section cases replaceable mixture weights depend asymptotically achieve capacity 
point necessary address major practical concern computationally feasible implement stage mixture probability assignment 
specifically seen sections important examples mixture single indexed class easily implementable reasonably easy implement second stage mixture possibly infinitely classes 
unfortunately positive answer question general level 
willems shtarkov award winning provided positive answer question finite hierarchies classes tree sources efficient recursive method referred context tree weighting 
method optimal individual sequence sense eq 

hierarchies countably infinitely classes implementation issue unresolved 
examples demonstrated countably infinite mixture collapses finite 
happens contributions mixtures corresponding certain threshold turn identical merged combined weight ii problem normally grows computational burden computing mixtures time instant large time elapses 
far discussed hierarchical universal prediction solely self information loss function 
said loss functions 
apparently deduce self information loss function loss functions way done sections 
aware reported topic 
mention directions pursued explicitly 
helmbold schapire combined exponential weighting mechanism line prediction expert advice respect absolute error loss function context tree weighting algorithm willems shtarkov competing best pruning decision tree 
hierarchical linear prediction individual sequences square error loss function 
papers linear prediction problem transformed gaussian sequential probability assignment problem 
universal assignment obtained stage mixture linear prediction coefficients model order 
mixture parameters gaussian prior mixture evaluated analytically 
probability assignment attained mixture correspond directly universal predictor fortunately correspondence certain range values predicted sequence 
proper choice prior predictor scaled finite range sequence values 
addition mixture model order performed computationally efficient way lattice filters possible linear predictors model order largest order weighted efficient recursive procedure complexity larger conventional linear predictor model order noted plug estimator parameter resulting rls algorithm leads universal prediction albeit slower rate mixture approach 
resulting universal linear predictor implemented tested experimentally practical communication signal processing problems 
directions attempt provide overview current state art problem area universal prediction 
explained definitely meant full encyclopedic survey scientific done topic 
aim mention important concepts authors point view 
summarize concepts briefly 
seen problem universal prediction studied extensively probabilistic deterministic setting 
common features shared settings 
self information loss case plays central role stems facts 
important loss function right reasons explained section 
main reasons view prediction problem probability assignment self information loss function arises natural manner 
ii self information loss case theory fairly mature understood iii results lower bounds algorithms loss functions obtained self information loss function 
second common feature probabilistic deterministic settings large degree parallelism theories universal prediction universality respect small indexed classes universality respect large classes hierarchical universality bridges 
remarkable degree analogy quantitative results obtained settings cases 
fundamental connections stationary ergodic sequences best attainable performance level deterministic definition agrees surely probabilistic counterpart 
differences minimax redundancy rates deterministic setting different probabilistic setting 
plug approach predictive probability assignment works instances probabilistic setting normally approach deterministic setting 
minimax redundancy deterministic setting different probabilistic setting 
randomization necessary deterministic setting probabilistic setting 
interesting messages term probability assignment originally comes probabilistic world meaningful pure deterministic setting 
fact far trivial 
efficient algorithmic tools obtaining probability assignments incremental parsing procedure lempel ziv algorithm 
see theoretical problems interesting consider research 
mentioned body ffl develop solid general theory universal prediction general loss functions parallel extension theory self information loss function 
derive tighter stronger lower bounds general loss functions probabilistic setting deterministic setting 
example framework prediction expert advice take account relations experts assuming worst set experts 
ffl extend results universal prediction respect comparison class finite state machines case noisy observations 
ffl impose limitations resources universal sequential predictor 
example comparison class finite state predictors states universal predictor guarantee redundancy certain level 
challenges best efforts researchers far 
explored 
agrawal anantharam asymptotic adaptive allocation schemes controlled processes finite parameter case ieee trans 
autom 
contr 
vol 
ac pp 
march 
agrawal anantharam asymptotic adaptive allocation schemes controlled markov chains finite parameter case ieee trans 
autom 
contr 
vol 
ac pp 
march 
algoet universal schemes prediction gambling portfolio selection ann 
probab vol 
pp 
april 
algoet strong law large numbers sequential decision uncertainty ieee trans 
inform 
theory vol 
pp 
may 
algoet cover asymptotic optimality asymptotic equipartition properties log optimal investment ann 
probab 
vol 
pp 

bailey sequential schemes classifying predicting ergodic processes ph dissertation stanford university 
bernardo posterior distributions bayesian inference roy 
statist 
soc 
vol 
pp 

blackwell analog minimax theorem vector payoffs pac 
math vol 
pp 

blackwell controlled random walks proc 
int 
congress math vol 
pp 
amsterdam north holland 
blum line algorithms machine learning web site www cs cmu edu afs cs cmu edu user avrim www papers pubs html 
blumer minimax universal noiseless coding markov sources ieee trans 
inform 
theory vol 
pp 
november 
cesa bianchi freund helmbold haussler schapire warmuth expert advice ann 
acm symp 
theory computing pp 

cesa bianchi freund helmbold warmuth line prediction conversion strategies proc 
eurocolt pp 
oxford 
cesa bianchi lugosi sequential prediction individual sequences relative set experts preprint 
clarke barron information theoretic asymptotics bayesian methods ieee trans 
inform 
theory vol 
pp 
may 
clarke barron jeffreys prior asymptotically favorable entropy risk statist 
plan 
inf vol 
pp 
august 
cover universal gambling schemes complexity measures kolmogorov chaitin technical report dept statistics stanford university october 
cover open problems information theory proc 
moscow inform 
theory workshop pp 
ieee press new york ny 
cover king convergent gambling estimate entropy english ieee trans 
inform 
theory vol 
pp 
july 
cover competitive optimality huffman code ieee trans 
inform 
theory vol 
pp 
january 
cover 
universal portfolios math 
finance vol 
pp 
jan 
cover ordentlich universal portfolios side information ieee trans 
inform 
theory vol 
pp 
march 
cover thomas elements information theory wiley new york 
csisz ar korner information theory coding theorems discrete memoryless systems academic press new york 
csisz ar shields redundancy rates renewal processes ieee trans 
inform 
theory vol 
pp 
november 
davisson prediction error stationary gaussian time series unknown covariance ieee trans 
inform 
theory vol 
pp 
october 
davisson universal noiseless coding ieee trans 
inform 
theory vol 
pp 
november 
davisson minimax noiseless universal coding markov sources ieee trans 
inform 
theory vol 
pp 
march 
davisson leon garcia source matching approach finding minimax codes ieee trans 
inform 
theory vol 
pp 
march 
davisson mceliece wallace efficient universal noiseless source codes ieee trans 
inform 
theory vol 
pp 
may 
dawid position potential developments personal views statistical theory prequential approach discussion roy 
statist 
soc 
vol 
part pp 

dawid inference likelihood prequential frames discussion roy 
statist 
soc 
vol 
pp 

dawid prequential data analysis current issues statistical inference ghosh pathak eds ims lecture notes monograph series pp 

dawid vovk prequential probability principles properties web site www stat wharton upenn edu seq members vovk index html 
degroot optimal statistical decisions mcgraw hill new york 
markowsky wegman learning probabilistic prediction functions proc 
th ieee symp 
foundations computer science pp 

elias minimax optimal universal codeword sets ieee trans 
inform 
theory vol 
pp 
july 
feder gambling finite state machine ieee trans 
inform 
theory vol 
pp 
september 
feder merhav hierarchical universal coding ieee trans 
inform 
theory vol 
pp 
september 
feder merhav gutman universal prediction individual sequences ieee trans 
inform 
theory vol 
pp 
july 
feder singer universal data compression linear prediction proc 
dcc pp 

universal methods coding case unknown statistics proc 
th symp 
inform 
theory moscow pp 

freund schapire game theory line prediction boosting proc 
th ann 
workshop computational learning theory pp 

gallager information theory reliable communications wiley new york 
gallager source coding side information universal coding unpublished manuscript 
int 
symp 
inform 
theory october 
sequential compound estimation ann 
math 
statist vol 
pp 

gray entropy information theory springer verlag new york 
van der meulen universal source code infinite source alphabet ieee trans 
inform 
theory vol 
pp 
january 
hannan approximation bayes risk repeated plays contributions theory games vol 
ann 
math 
studies pp 
princeton 
hannan robbins asymptotic solutions compound decision problem completely specified distributions ann 
math 
statist vol 
pp 

haussler general minimax result relative entropy ieee trans 
inform 
theory vol 

pp 
july 
haussler kivinen warmuth sequential prediction individual sequences general loss functions appear ieee trans 
inform 
theory 
haussler opper mutual information metric entropy cumulative relative entropy risk appear ann 
statist vol 
december 
haussler opper general bounds mutual information parameter conditionally independent observations proc 
th annual workshop computational learning theory colt pp 

helmbold schapire predicting nearly best pruning decision tree machine learning vol 
pp 

ziv fixed database universal data compression limited memory ieee trans 
inform 
theory vol 
pp 
november 
jeffreys invariant form prior probability estimation problems proc 
soc 
london part vol 
pp 

robust noiseless source coding game theoretic approach ieee trans 
inform 
theory vol 
pp 
july 
kelly jr new interpretation information rate bell sys 
tech 
vol 
pp 

unified approach weak universal source coding ieee trans 
inform 
theory vol 
pp 
november 
ergodic theorem constrained sequences functions bullet 
amer 
math 
soc 
vol 
pp 

laplace law succession universal encoding ieee trans 
inform 
theory vol 
pp 
january 
performance universal encoding ieee trans 
inform 
theory vol 
pp 
march 
lai robbins asymptotically efficient adaptive allocation rules adv 
appl 
math vol 
pp 

langdon note lempel ziv model compressing individual sequences ieee trans 
inform 
theory vol 
pp 

laplace sur la des causes par les de del sciences pp 

reprinted laplace complete vol 
pp 
gauthier paris 
english translation 
laplace sur les approximations des formulas qui sont functions de tres nombres sur leur application aux de des sciences de paris pp 

reprinted laplace complete vol 
pp 

gauthier paris 
english translation lempel ziv complexity finite sequences 
ieee trans 
inform 
theory vol 
pp 
january 
littlestone long warmuth line learning linear functions proc 
rd ann 
acm symp 
theory computing pp 

littlestone warmuth weighted majority algorithm information computation vol 
pp 

szpankowski average redundancy rate lempel ziv code ieee trans 
inform 
theory vol 
pp 
january 
class codes designed bayes decision theory ieee trans 
inform 
theory vol 
pp 
september 
meir performance bounds nonlinear time series prediction preprint 
meir merhav stochastic complexity learning realizable unrealizable rules machine learning vol 
pp 

merhav feder universal schemes sequential decision individual sequences ieee trans 
inform 
theory vol 
pp 
july 
merhav feder strong version redundancy capacity theorem universal coding ieee trans 
inform 
theory vol 
pp 
may 
merhav feder gutman properties sequential predictors binary markov sources ieee trans 
inform 
theory vol 
pp 
may 
miller goodman smyth loss functions minimize conditional expected values posterior probabilities ieee trans 
inform 
theory vol 
pp 
july 
modha universal prediction stationary random processes preprint 
nonparametric inference ergodic stationary time series ann 
statist vol 
pp 

algoet weakly convergent nonparametric forecasting stationary time series ieee trans 
inform 
theory vol 
pp 
march 
extended set compound estimation problem nonregular family distributions ann 
inst 
stat 
math vol 
pp 

opper haussler bounds predictive errors statistical mechanics supervised learning phys 
rev lett 
vol 
pp 

opper haussler worst case prediction sequences log loss mathematics information coding extraction distribution springer verlag edited cybenko leary rissanen 
orenstein guessing output stationary process israel math vol 
pp 

papoulis probability random variables stochastic processes mcgraw hill series electrical engineering third edition 
weinberger ziv upper bounds probability sequences emitted finite state sources redundancy lempel ziv algorithm ieee trans 
inform 
theory vol 
pp 
january 
rissanen generalized kraft inequality arithmetic coding ibm res 
develop vol 
pp 

rissanen modeling shortest data description automatica vol 
pp 

rissanen universal coding information prediction estimation ieee trans 
inform 
theory vol 
pp 
july 
rissanen complexity strings class markov sources ieee trans 
inform 
theory vol 
pp 

rissanen fisher information stochastic complexity ieee trans 
inform 
theory vol 

pp 
january 
rissanen langdon universal modeling coding ieee trans 
inform 
theory vol 
pp 
january 
robbins asymptotically solutions compound statistical decision problems proc 
second berkeley symp 
math 
stat 
prob pp 

convex analysis princeton university press princeton nj 
ya 
ryabko encoding source unknown ordered probabilities problems inform 
trans pp 
october 
ya 
ryabko twice universal coding problems inform 
trans vol 
pp 
jul sep 
ya 
ryabko prediction random sequences universal coding problems inform 
trans vol 
pp 
apr june 
samuel asymptotic solution sequential compound decision problem ann 
math 
statist pp 

samuel convergence losses certain decision rules sequential compound decision problem ann 
math 
statist pp 

redundancy lempel ziv incremental parsing rule ieee trans 
inform 
theory vol 
pp 
january 
conditional expectations stationary processes verw 
gebiete vol 
pp 

schwarz estimating dimension model ann 
statist vol 
pp 

shannon prediction entropy printed english bell sys 
tech 
vol 
pp 

shannon mind reading machine shannon collected papers wyner sloane eds pp 
ieee press 
shields uniform redundancy rates exist ieee trans 
inform 
theory vol 
pp 
march 
shields weiss universal redundancy rates class processes exist ieee trans 
inform 
theory vol 
pp 
march 
dynamic decision problems multi user systems ph dissertation technion november 
kov universal sequential coding single messages problems inform 
trans vol 
pp 
july september 
singer feder universal linear prediction parameters model orders appear ieee trans 
signal processing 

takeuchi barron asymptotically minimax regret exponential curved exponential families preprint 
van sequential compound decision problem theta finite loss matrix ann 
math 
statist vol 
pp 

admissible solutions extended finite state set sequence compound decision problems 
anal vol 
pp 

vitter optimal prefetching data proc 
foundations computer science pp 

vovk aggregating strategies proc 
rd ann 
workshop computational learning theory pp 
san mateo ca 
vovk game prediction expert advice proc 
rd ann 
workshop computational learning theory pp 
new york ny 
weinberger merhav feder optimal sequential probability assignment individual sequences ieee trans 
inform 
theory vol 
pp 
march 
weinberger rissanen feder universal finite memory source ieee trans 
inform 
theory vol 
pp 
may 
wiener extrapolation interpolation smoothing stationary time series mit press cambridge ma 
willems shtarkov context tree weighting method basic properties ieee trans 
inform 
theory vol 
pp 
may 
xie barron asymptotic minimax regret data compression gambling prediction submitted ieee trans 
inform 
theory 
xie barron minimax redundancy class memoryless sources ieee trans 
inform 
theory vol 
pp 
march 
yu lower bounds expected redundancy nonparametric classes ieee trans 
inform 
theory vol 
pp 
january 
ziv coding sources unknown statistics part probability encoding error ieee trans 
inform 
theory vol 
pp 
may 
ziv lempel universal algorithm sequential data compression ieee trans 
inform 
theory vol 
pp 
july 
ziv lempel compression individual sequences variable rate coding ieee trans :10.1.1.14.2892
inform 
theory vol 
pp 
september 
ziv lempel universal coding dimensional data ieee trans 
inform 
theory vol 
pp 
january 

