informal version personal scalable clustering joydeep ghosh dept ece univ texas austin formal version appears handbook data mining ye ed lawrence erlbaum assoc chapter pp 

contents clustering techniques brief survey partitional methods 
hierarchical methods 
discriminative vs generative models 
assessment results 
internal model unsupervised quality 
external model free semi supervised quality 
visualization results 
clustering challenges data mining scalable clustering data mining scalability large number records patterns 
scalability large number attributes dimensions 
balanced clustering 
sequence clustering techniques case study similarity clustering market baskets web logs case study impact similarity measures web document clustering similarity measures sampler 
clustering algorithms text data sets 
comparative results 
clustering software summary clustering defined process grouping collection patterns distinct segments clusters suitable notion closeness similarity patterns 
clusters show high similarity group low similarity patterns belonging different groups 
applications customer product segmentation clustering primary goal 
crucial intermediate step needed data analysis view placement cluster utility exploration stage enterprise miner leading data mining software sas institute 
applied statistical technique cluster analysis studied extensively years disciplines including social sciences hartigan jain dubes 
clustering fundamental data analysis step pattern general concept 
may desire group similar species sounds gene sequences images signals database records possibilities 
chapter shall words pattern data point record object interchangeably appropriate denote entities clustered 
clustering studied fields machine learning statistical pattern recognition type unsupervised learning rely predefined class labeled training examples duda 
serious efforts perform effective efficient clustering large datasets started years emergence data mining 
classic approaches clustering include partitional methods means hierarchical agglomerative clustering unsupervised bayes mode finding density approaches 
variety soft clustering techniques fuzzy logic statistical mechanics data point may belong multiple clusters different degrees membership 
main reason diversity techniques clustering problem easy conceptualize may quite difficult solve specific instances 
quality clustering obtained method data dependent methods uniformly inferior method works best types datasets 
approach certain underlying assumptions data suitable specific scenarios method best situations 
example correct guess initialization means clustering works fine data captured mixture identical isotropic gaussians gaussians identical diagonal covariances distance degrade rapidly deviates assumptions 
non gaussian distributions high dimensional space better random partitioning regularly turns near empty clusters 
conceptual clustering maximizes category utility measure predictability improvement attribute values clustering popular machine learning community applicable data binary categorical small numerosity values 
nice books survey articles classical clustering methods including ones jain briefly mention section 
intent chapter highlight clustering issues techniques relevant data mining applications 
illustrate new challenges posed certain large datasets acquired scientific domains world wide web explain newer requirements scalability balancing describe state art techniques developed address challenges 
clustering techniques brief survey typical pattern clustering activity involves steps jain dubes 
suitable pattern representation 
definition proximity suitable distance similarity measure objects 
clustering 
data abstraction 
assessment output steps unavoidable optional depend specific application 
step pertains obtaining features attributes appropriately represent pattern 
features binary quantitative continuous discrete numeric values qualitative unordered attributes color ordered ones small medium large 
pattern record database features provided attributes records 
situations patterns raw form speech signals images decide appropriate pre processing steps filtering representational form autocorrelation coefficients fourier coefficients 
may try available features clustering process prudent subset original features smaller number derived features 
process called feature selection feature extraction respectively array methods available approaches duda 
cases features explicit human judgement relied determine similarity patterns 
example may ask human subject rate similarity smells scale signifying subjectively totally different smells signifying identical smells 
situations restricted methods multi dimensional scaling torgerson solely pairwise similarity values 
common case pattern represented point dimensional space 
case obvious candidate choice step euclidean distance points measure inversely related proximity 
general distances take values zero infinity similarity ranges concepts related smooth monotonically decreasing function similarity distance 
insert shows patterns represented numeric values 
euclidean distance determine similarity clustering points groups reasonable 
coordinates measure qualitatively different features say age salary difficult convert measurements common dimensional vector space reals example may realize compressed vertical axis compared horizontal axis 
case patterns belong cluster 
pattern represented dimensional point euclidean distance suitable 
data different spreads different attributes may prefer compensate distance normalizes spread attribute unit variance 
direction data points important cosine angle data vectors may appropriate 
patterns complex sequences trees determining similarity difficult 
choice distance similarity measure profoundly impact cluster quality needs take care step 
effect proximity measure cluster quality examine issue detail section clustering web documents represented high dimensional sparse vectors consider clustering sequences section 
move step actual clustering process 
broadly types approaches partitional methods partition data set clusters 
immediate problem determine best value done guess application requirements trying running clustering algorithm times different values selecting solutions suitable evaluation criterion 
hierarchical methods give range nested clusterings obtained agglomerative bottom divisive top procedure 
practical situations post processes output partitional method merging splitting clusters done isodata jain 
methods flavor approaches 
look clustering techniques bit detail 
partitional methods square error hard clustering iterative relocation 
algorithms popular long emergence data mining 
set objects dimensional space input parameter partitioning algorithm organizes objects clusters total deviation object cluster representative minimized 
popular representatives means algorithm algorithm 
means algorithm iterative algorithm minimize squares error criterion 
cluster represented center mean samples 
centers cluster representatives initialized typically random selection data objects solutions clustering algorithm small subset data 
sample labeled index nearest similar center 
classical means nearest determined smallest euclidean distance 
subsequent re computing mean cluster moving geometric center data points assigned cluster previous step re assigning cluster labels iterated convergence fixed labeling iterations 
complexity algorithm 
median algorithm similar cluster representative constrained actual data points geometric center 
involves extra computations 
data model underlying means algorithm mixture gaussians identical spherical covariances 
best correct guess data naturally clumped spherical regions comparable spreads 
data deviates significantly scenario cluster quality poor 
remains popular partitional methods 
self organizing map kohonen popular topology preserving clustering algorithm nice visualization properties 
related competitive learning line version means input patterns time step closest mean input consideration moved bit closer input 
cluster representatives logically arranged simple typically dimensional topology 
topology induces neighborhood function representatives cluster representative adjusted move closer input currently considered logical neighbors move lesser extent direction 
mechanism causes logically nearby cluster representatives cover nearby areas input space 
resulting mapping inputs means akin projections principal surface chang ghosh 
map formed input projected nearest representative conveniently visualized logical space 
density methods 
clustering methods developed notion density 
typically regard clusters dense regions objects feature space separated regions relatively low density 
methods filter noise discover clusters arbitrary shape 
methods called mode seeking methods cluster located local mode maxima data density function 
particularly popular low dimensional applications spatial data clustering 
unfortunately general amount data required estimate local density level accuracy increases exponentially number data dimensions manifestation famous curse dimensionality friedman 
simple parametric form data density known apriori impractical apply density methods data hundreds dimensions 
key role scale evident density methods 
finite number patterns try model continuous density function patterns drawn various degrees smoothing scale 
cluster centers located local maxima estimated density function 
amount smoothing increases number maxima decrease 
fact obtain range clusters point cluster smoothing big cluster entire data infinite smoothing 
scale space theory says salient clusters appear wide range scales smoothing values 
clustering algorithms proposed data mining largely database community density closely related grid approaches 
take quick look representative proposals 
reader referred chapter han kamber details 
dbscan algorithm judges density neighborhood object sufficiently high number points distance object greater minpts number points 
condition satisfied points considered belong cluster 
addition region grown points adjacent regions sufficient density considered part cluster 
means clusters restricted spherical take arbitrary shapes long contiguous regions adequately high density exist 
property useful low dimensional applications involving small number parameters estimated easily spatial clustering 
unfortunately number clusters discovered dependent parameters minpts determined apriori 
dbscan relies user ability select set parameters 
critically density data varies significantly region region natural clusters sparser area density clusters higher immediate surroundings merging clusters regions higher density 
tough problem solve involves looking data locally adaptive scale 
dynamic adaptation capability provided chameleon algorithm described section 
cluster ordering method called optics recognizes leverages issue scale dbscan 
producing data set clustering explicitly optics computes augmented cluster ordering automatic interactive cluster analysis essentially providing clusters multiple scales 
achieved noting denser clusters satisfying minpts criterion smaller value subclusters clusters obtained larger value 
concurrently obtain results user multiple values feedback determine proper threshold levels 
dbscan optics algorithms reduced nlogn spatial index structures 
different density approach models density point analytically sum influence functions data points 
influence function essentially shape window classical parzen windows technique density estimation 
influence function chosen model data somewhat higher dimensional data handled 
compute sum influence functions efficiently grid structure utilized 
perform experimentally better dbscan careful selection clustering parameters density noise thresholds required addition suitable choices influence functions 
grid clustering approach tries density method efficient imposing grid data structure data space 
data space finite number cells 
records scanned counts cell updated cells sufficiently high counts indicate clusters 
quality clustering amount computation required increase number cells grid 
multi resolution grid employed improves efficiency particularly merging high density cells finer level granularity cell coarser level 
note problems higher dimensions fixed number bins dimension total number cells increases exponentially number dimensions input 
examples grid approaches proposed data mining include sting explores statistical information stored grid cells clusters objects wavelet transform method clique represents grid density approach high dimensional data space 
grid approach usually efficient direct density approach cost quality loss grid structure fit distribution data 
graph methods 
graph methods transform clustering problem combinatorial optimization problem solved graph algorithms related heuristics 
typically objects clustered undirected weighted graph constructed vertices points edge connecting vertices weight equal suitable similarity measure corresponding objects 
choice similarity measure quite depends problem domain jaccard coefficients market baskets normalized dot products text set edges removal partitions graph pair wise disjoint sub graphs called edge separator 
objective graph partitioning find separator minimum sum edge weights 
striving minimum cut objective number objects cluster typically kept approximately equal reasonably balanced equal sized clusters obtained 
min cut problem np complete graph techniques approximate solutions heuristics 
fact expensive step clustering graph partitioning usually computation similarity matrix efficient heuristics exist obtaining edge separators 
situations attributes sparse words text document efficient compute similarity object attributes pairs objects 
efficient clustering achieved solving corresponding bipartite graph partitioning problem dhillon 
known graph clustering algorithms include spectral rock chameleon karypis 
spectral leverages fact second lowest eigenvector laplacian graph provides indication min cut connected graph 
recursively obtain partitions 
rock agglomerative hierarchical clustering technique categorical attributes 
uses binary jaccard coefficient thresholding criterion form unweighted edges connecting data points 
key idea rock define transitive neighbor relationship addition simple neighbors adjacency matrix pairs having common neighbors adjacency matrix considered neighbors 
common neighbors define interconnectivity clusters merge clusters 
akin classic shared neighbor clustering concept pattern recognition 
chameleon starts partitioning data large number small clusters partitioning nearest neighbor graph 
subsequent stages clusters merged closeness dynamically adjusted measure localized distances inter connectivity 
subclass graph partition algorithms hypergraph partitioning 
hypergraph graph edges connect vertices hyperedges 
clustering problem formulated finding hypergraph 
minimum cut removal set hyperedges minimum edge weight separates hypergraph unconnected components 
object xj maps vertex vj 
feature maps hyperedge connecting vertices non zero frequency count feature 
weight hyperedge chosen total number occurrences data set 
importance hyperedge partitioning proportional occurrence corresponding feature 
minimum cut hypergraph unconnected components gives desired clustering 
efficient packages hmetis exist han partitioning 
advantage approach clustering problem mapped graph problem explicit computation similarity approach computationally efficient assuming close linear performing hypergraph partitioner 
soft clustering 
soft clustering pattern belong multiple clusters different degrees association kumar ghosh 
natural viewpoint fuzzy clustering degree membership pattern cluster decreases distance cluster center increases leading fuzzy means algorithm widely applied 
expectation maximization em framework dempster applied clustering yields soft partitions 
assume observed points xi come underlying parametric distribution mixture probability density functions observed point parameter vector ph density function parameterized commonly component density function dimensional gaussian mean covariance gaussian representing hth component ph components represents cluster ch points assumed come exactly components belong corresponding cluster 
clustering problem best possible estimate component densities certain assumptions assign cluster label data points component generated 
set random variables indicator functions denoting point comes cluster 
clustering algorithm tries find values model parameters maximize log likelihood data log xi note known directly solve maximum likelihood estimation problem clustering obviously known 
classical way address problem find parameters expected value log likelihood maximized expectation computed conditional distribution dempster 
expected log likelihood el directly solved known 
problem solved expectation maximization technique starts arbitrary choice iteratively improves estimates trying maximize current estimate expected log likelihood 
technique guaranteed converge local maxima expected log likelihood 
soft clustering method gives soft memberships points clusters 
hierarchical methods hierarchical method creates hierarchical decomposition data objects 
results displayed dendogram 
leaves object cluster height sub clusters merged signifies distance groups 
cluster merge considerable length vertical axis relatively stable 
dendogram formed ways bottom top 
bottom approach called agglomerative approach starts object forming separate group 
successively merges objects groups closest distance measure termination condition satisfied 
depending distance clusters measured distance closest pair cluster farthest pair average pairs obtains single link complete link average link variants agglomerative clustering 
single link efficient approximate algorithm minimum spanning tree solution tends give elongated clusters sensitive noise 
complete link average link methods computationally expensive yield compact clusters 
dendogram shown single link algorithm 
complete link group fused group 
insert top approach called divisive approach starts objects cluster 
successive iteration cluster split smaller clusters measure termination condition satisfied 
example bisecting means steinbach dataset partitioned groups means algorithm 
means applied split larger groups till adequate number clusters obtained 
divisive methods popular algorithms bisecting means quite efficient number clusters required small give results 
earlier hierarchical clustering methods suffered oversimplified measures splitting merging 
merge split operation done irreversibly 
simple rigid approach result erroneous clusters 
order enhance effectiveness hierarchical clustering algorithms methods chameleon karypis utilize complex robust principles splitting merging clusters 
discriminative vs generative models clustering algorithms essentially try solve properly formulated optimization problem 
conceptually speaking helpful categorize clustering algorithms generative discriminative approaches optimization problem approached 
em soft clustering algorithm described earlier prototypical example generative approach 
approach assumes patterns belonging various clusters generated probabilistic pattern generation process dependent certain parameters 
quite defined mapping parameters data clusters cases explicit mapping 
soft clustering em example assumed data points produced mixture gaussians natural choice cluster means centers component gaussians 
generative clustering algorithm tries best estimate parameters underlying generative obtains data clusters estimates 
framework corresponds model parametric approaches classical pattern recognition 
parameter estimation done maximum likelihood ml maximum aposteriori map mean posterior techniques 
interesting note estimates converge value limit large datasets distinction terribly important data mining context 
discriminative approach assumption regarding source method generation patterns 
assumed patterns exist space defined distance similarity measure pair patterns 
patterns placed separate groups patterns cluster high similarity dissimilar patterns clusters indicated suitable objective function 
roughly corresponds memory non parametric approaches classical pattern recognition 
classical agglomerative approaches examples discriminative model clustering 
graph partitioning clustering follows paradigm trying minimize objective cut cut sums weights vertices partition respectively 
depending vertex weights specified versions objective ratio cut normalized cut obtained 
extensions formulation way partitioning case simple variants problem solved max flow min cut theorems leighton rao 
problems practical interest quite extremely difficult get exact theoretical solutions 
result algorithms formulations rely approximations including intelligent heuristics empirical evidence karypis kumar 
assessment results fundamentally different ways evaluating quality results delivered clustering algorithm 
internal criteria formulate quality function data similarities 
typically quality measured compactness clusters cases separation different clusters 
internal criterion valid choice features associated similarity measure appropriate problem considered 
fact ugly duckling theorem watanabe states somewhat unintuitive fact way distinguish different classes objects compared possible features 
consequence arbitrary objects equally similar domain knowledge 
contrast external criteria judge quality additional external information clusterer class labels 
internal model unsupervised quality sum squared errors sse popular internal quality measure sse sse error simply distance data point representative center cluster assigned 
directly measures average compactness clusters 
easily shown means algorithm tries greedily optimize objective arrives local optimum 
note transform sse quality measure ranging desired indicates perfect clustering 
sse objective viewed perspective generative model 
assuming data generated mixture multi variate gaussians identical diagonal covariance matrices sse objective equivalent maximizing likelihood observing data adjusting centers minimizing weights gaussian mixture 
edge cut clustering posed graph partitioning problem objective minimize edge cut 
formulated quality maximization problem objective ratio remaining edge weights total pre cut edge weights 
note quality measure trivially maximized restrictions sizes clusters 
edge cut measure fair compared clusterings balanced min cut objective typically modified penalize highly imbalanced solutions done equation 
category utility gluck category utility function measures quality increase predictability attributes clustering 
category utility defined increase expected number attribute values correctly guessed partitioning expected number correct guesses knowledge 
weighted average categories allows comparison different sized partitions 
category utility defined maximize predictability attributes clustering appropriate low dimensional clustering problems preferably dimension categorical variable small cardinality 
combined measures may employ combined measure looks compactness clusters separation clusters 
measure comprehensive 
variety combined measures evaluated zhao karypis 
external model free semi supervised quality drawback internal quality measure fair comparisons clusterings choices vector representation similarity distance measure 
example edge cut cosine similarity meaningful evaluation euclidean means 
applications consensus internal quality measure clustering 
situations patterns categorized labelled external source external quality measure 
class evaluation measures compare start performance kind clustering regardless models similarities particularly suitable goal clustering indicate underlying classes 
note situations class conditional densities multimodal high degree overlaps class labels may represent natural clusters data 
remember classification ground truth indicated class labels available clustering algorithm 
simplest external evaluation measure purity interpreted classification accuracy assumption objects cluster classified members dominant class cluster 
comprehensive measure purity entropy 
just considering number objects dominant class purity entropy takes entire distribution account 
purity entropy biased favor large number clusters 
fact criteria globally optimal value trivially reached cluster single object 
alternative precision recall standard measures information retrieval community 
cluster viewed results query particular category precision fraction correctly retrieved objects recall fraction correctly retrieved objects matching objects database 
measure employed combine precision recall single number baeza yates ribeiro neto 
purity entropy measure biased larger number clusters 
fact favors coarser clusterings 
mutual information cover thomas symmetrically defined mutual information set class labels cluster labels superior measure external quality 
objects classified categories number objects category objects clustered groups number objects cluster denote number objects cluster category normalized mutual information quality measure strehl mi logk 
mutual information suffer biases purity entropy fmeasure 
singletons evaluated perfect 
random clustering mutual information limit 
best possible labeling evaluates classes balanced 
external criteria enable compare different clustering methods fairly provided external ground truth quality 
case study document clustering sec 
normalized mutual information preferred choice evaluation unbiased measure usefulness knowledge captured clustering predicting category labels 
visualization results dimensional data readily visualized original feature space 
visualization higher dimensional data clusters largely divided popular approaches 
dimensionality reduction selection dimensions generally projecting data dimensions 
dimensions correspond principal components scalable approximation thereof fastmap faloutsos lin 
noteworthy method dhillon projects data points plane passes selected cluster centroids yield discrimination optimal dimensional projection 
projections useful medium number dimensions large 
immediate question degradation occurs data projection 
highly data domain dependent 
text mining linearly projecting dimensions affect results latent semantic indexing projection lower dimensions leads substantial degradation dimensional projections limited utility 
nonlinear projections studied chang ghosh 
recreating dimensional space similarity graph done multidimensional scaling torgerson 

parallel axis plots show object line parallel axis 
technique rendered ineffective number dimensions number objects gets high 

kohonen self organizing map som kohonen provides innovative powerful way clustering enforcing constraints logical topology imposed cluster centers 
topology dimensional readily visualize clustering data 
essentially dimensional manifold mapped typically higher dimensional feature space trying approximate data density maintaining topological constraints 
mapping bijective quality degrade rapidly increasing dimensionality feature space data largely confined lower order manifold space chang ghosh 
multi dimensional scaling mds associated methods face similar issues 
visualization techniques relevant data mining chapter book 
visualization technique display results case study section involves smart reordering similarity matrix 
reordering data points visualization contexts example cluster analysis genome data eisen 
clustering challenges data mining spite rich literature tradition broad range tools existing methods clustering severely challenged applications involving certain large datasets acquired scientific domains world wide web 
addition scalability problems may arise difficult nature data 
example data may high dimensional highly noisy lots outliers exhibit broad regions clusters 
data may readily amenable vector representation sequence data weblogs structured data xml documents clustering applications pose additional requirements evaluation visualization results 
illustrate nature complex characteristics new requirements concrete data mining application scenarios 
transactional data analysis 
large market basket database involves thousands customers product lines 
record corresponds store visit customer customer multiple entries time 
transactional database conceptually viewed product feature attribute customer object matrix th entry non zero customer bought product transaction 
case entry represents pertinent information quantity bought total amount paid 
customers buy small subset products visit corresponding feature vector column describing transaction high dimensional large number products sparse features zero 
transactional data typically highly non gaussian 
big customers show outliers significant filtered 
way reduce feature space consider dominant products attribute selection practice leaves hundreds products considered 
product popularity tends follow zipf distribution heavy tail popular products readily ignored typically higher profit margins 
roll operation reduce number products results corresponding loss resolution granularity 
feature extraction transformation typically carried derived features lose semantics original ones sparsity property 
preprocessing left clustering non gaussian vectors hundreds dimensions 
note majority data mining clustering algorithms approximate means local data densities approaches severely hampered curse dimensionality spaces 
current solutions avoid curse grossly simplifying data view 
years back talked large retail stores behalf data mining knowledge discovery part standard approach segment customers macro level view example modeling person numbers recency frequency monetary value variety tenure plus demographics information 
approaches fail capture actual purchasing behavior complex ways taste brand preferences price sensitivity academia popular approach data transaction reduced unordered set purchased items 
techniques apriori algorithm variants improvements determine associations rules 
unfortunately results loss vital information differentiate buying gallon milk milk weight importance buying apple vs buying car clearly different situations business perspective 
fact clients interviewed approach satisfactory 
clustering transactional data introduces new requirements 
clusters balanced comparable size measure importance number customers products revenue represented comparable amounts resources number sales teams marketing dollars shelf floor space allocated strehl ghosh 
balancing global attribute difficult achieve locally iterative greedy methods 
second cluster easily characterized results visualized interpreted non technical person store manager 
eliminates clustering post processing choices 
effects need taken care example clustering annual data separately looking summer winter patterns mask important seasonal associations gupta ghosh 
final note text documents represented bag similar characteristics high dimensionality sparsity clustering problem somewhat severe vector normalization effective domain lsi project documents dimensions degradation 

generation clickstream clustering 
visitor website characterized set sessions session comprising sequence pages visited time spent page information typed clustering visitors clickstream information helps website provide customized content users making sticky enhancing user experience 
cluster sets sequences entries having symbolic numeric attributes 
current commercial solutions tivoli weblog analyzer store preprocessed log data app servers supplementary information search queries entered data warehouse olap tools look summary information essentially order statistics answer questions visitors came page came went click 
typically detailed sequence time information simply ignored studies banerjee ghosh show ignored quantities substantial difference cluster quality certain websites 
research papers gloss details sophisticated deal sequential nature data frequent path markov model analysis scale clustering visitors rich sequence representations scale incrementally adapt newly acquired data rates gbytes day remains open challenge 

clustering coupled sequences 
tools hidden markov models hmms widely clustering classifying sequences symbolic numeric data 
fundamental assumption underlying tools sequences independently generated 
applications sequences quite coupled 
example consider different eeg channels measured simultaneously patient sets movement sequences obtained dancing fighting pair humans zhong ghosh brand 
researchers started describe data types variety coupled hmm formulations develop scalable algorithms clustering coupled sequence data 
applications abound specially bio informatics biomedical processing 

large scale remote sensing 
determining type ground cover airborne sensors prototypical remote sensing application received new life wider cheaper availability high resolution hyperspectral data 
inputs classification problem vectors dimensions representing energy measured contiguous narrow bands measured pixel kumar 
exploiting spatial spectral temporal correlations high dimensional data challenging real problem providing ground truth requires actual field expensive highly subjective 
clustering data measured nearby region relating clusters obtained labelled region eliminate easier pixels reduce costs incremental ground 
effectively translates problem combining multiple clusters having guaranteed correspondence clusters underlying ground cover classes 
problem related active learn ing semi supervised learning difficult dynamic nature class distributions proportions change time space 
application areas described involve complex data sets sequences coupled sequences high dimensional non gaussian data additional requirements constraints balancing large number clusters distributed processing presence legacy clusters 
challenges encountered data mining applications include presence mixed attribute types numerical binary categorical ordinal record high amounts noise domain constraints 
may need employ anytime algorithms provide range cluster quality vs clustering time tradeoffs incrementally adaptable algorithms quickly modify existing clustering response additional data 
issues demand powerful general approaches clustering problem 
scalable clustering data mining scalability sense computational tractability central issue data mining 
main aspects computational scalability scalability large number records patterns algorithms near linear records kept secondary memory desirable minimize number disk accesses scans 
scalability large number attributes dimensions high dimensional spaces peculiar properties may severely handicap methods lower dimensions 
scalable large number processors wants implement algorithm parallel distributed machines 
ideally total computation involved split near equal parts executed parallel little communication overheads 
cases near linear speedups obtained 
fortunately clustering algorithms readily parallelized efficient ways dhillon modha just focus aspects 
scalability large number records patterns scalability respect number patterns records important issue classical works clustering serves key distinguishing feature data mining oriented proposals rastogi shim 
holy grail achieve high quality clustering near linear algorithm involves small number passes database 
improve scalability directions sequential building ii space data partitioning iii sampling iv limiting comparisons 
sequential building 
approaches start core clustering small number records 
rest data examined response record cores adjusted new clusters formed 
leader clustering algorithm jain classical example sequential building approach 
sequential building specially popular outof core methods idea scan database form summarized model main memory 
subsequent refinement summarized information restricted main memory operations resorting disk scans 
general sequential building approaches suitable line streaming data results typically weaker quite sensitive sequence incoming data points 
nice illustration means algorithm approximated way fayyad 
underlying generative model means mixture gaussians spherical covariance 
secondly sufficient statistics single gaussian mean variance readily estimated finite sample knows number datapoints sample sum values sum squared values 
triplet values updated new point added simply incrementing number count adding value value squared point running sum sum squared values respectively 
scalable method fayyad exploits observations 
regular means seeds chosen say sampling database number points initially equal cluster sum sum squared values initialized clusters 
rest data scanned point close cluster centers triplet statistics cluster updated 
data copied main memory 
scan sets triplets representing cluster points represent outliers clusters captured initialization 
hope information easily contained main memory subsequently refined yield clusters 
fact may adequate space simultaneously update means solutions stemming different set initializations scan data 
subsequently solutions merged obtain single solution robust initialization 
authors showed method applied soft means uses em algorithm 
refinements discussed 
note goal having single pass approximation multi pass means achieved methods escape inherent limitations means best applies numeric data distributed spherical clouds size 
noteworthy sequential building approach birch stands balanced iterative reducing clustering hierarchies zhang 
uses index tree incrementally built data scanned 
node tree triplet number sum sum squared values records node updated new record passed node children 
balancing tree done splitting node represents records 
single disk scan highly scalable method obtained desirable incremental update capability retaining limitations means 
addition number clusters pre determined multiple runs post processing required 
space data partitioning 
suppose partitioning input space nearly disjoint regions 
regions viewed bins count number records fall bin determined single database scan 
merging adjacent dense bins contiguous regions comprising bins relatively high density obtained region viewed cluster 
binning crude clustering technique 
key issue specify bins 
wide variety choices primarily try bins equal volumes feature space space partitioning bins representing roughly equal amounts data data partitioning 
note databases variety mechanisms efficient indexing space partitioning trees trees variants data partitioning kdb trees exist 
fact view lower node trees representing cluster records 
may sense partitioning structures clustering 
addition rolling data concept hierarchies provides natural aggregating mechanisms 
prior domain knowledge helpful pre segmenting data indices partitioning methods 
data space partitioning methods typically tractable dimensional data judicious hybrid approaches data tens attributes may partitioned chakrabarti mehrotra 
estimating pre determining populating partitions increasingly difficult higher dimensions significant overlaps hyper rectangles occurrences empty areas increasingly common 
sampling 
sampling applied slower algorithm scalable possible cost quality 
idea cluster subset records quick heuristic allocate remaining records clusters obtained step 
applications document browsing quality takes speed sampling algorithms buckshot scatter gather approach cutting useful 
note sample clustered quadratic algorithm time 
nearest cluster center allocate non sampled points obtains kn time 
related randomized approaches partition set points clusters comparable size sublinear time producing solution high probability indyk 
data mining context sampling suggested way scalability clarans cure rock sampling particularly effective scaling balanced clustering approaches explained sec 

underlying clusters comparable number patterns modest sample high probability containing representatives cluster ensuring quality clusters obtaining sampled data reasonable banerjee ghosh 
limited comparisons 
clustering global procedure common formulations np complete clusters sought 
iterative heuristics means medoids linear compare data point cluster representatives points 
consequently popular vulnerable initial choices cluster representatives 
method limited comparisons handicapped global metric balancing desired 
scalability large number attributes dimensions dealing large number dimensions difficult aspect typically addressed reducing number attributes pre processing phase applying clustering algorithm directly high dimensional data 
briefly examine ways obtaining derived feature space reduced dimensionality looking impact different classes clustering algorithms 
reduction done selection subset suitable criteria transforming original set attributes smaller linear projections principal component analysis pca nonlinear means extensive approaches feature selection extraction long studied particularly pattern recognition community mao jain duda 
techniques succeed reducing number derived features tens loss information variety clustering visualization methods applied reduced dimensionality feature space 
problem may tractable data distribution simple structure captured generative approach 
example features approximately cluster conditionally independent reasonably characterized superposition small number gaussian components identical isotropic covariances case means directly applied high dimensional feature space results 
com ponents different covariance matrices diagonal number parameters grow quadratically unsupervised bayes mixture density modeling em fruitfully applied 
results random projections mixtures gaussians indicate project distributions log space estimate parameters projected space project back original space obtaining excellent results process dasgupta 
tractable solution data accounted dimensional manifold high dimensional embedding space 
principal surfaces nonlinear pca self organizing map som multi dimensional scaling mds efficient custom formulations fastmap faloutsos lin effectively applied 
different approach assume clusters interest regions specified small subset attributes 
leads subspace clustering approaches clique agrawal cac tus ganti 
clique dimension binary categorical discretized bins 
algorithm noting dense regions dimensions dense projections sized subset dimensions 
dense clusters projecting data individual dimensions 
cartesian product dimensional clusters examined identify dense regions dimensional space 
approach flavor apriori algorithm finding association rules goal pruning search space 
analogy itemset dense region specified choices attributes 
clusters rectangular nature easy specify rule 
clearly algorithm simplistic assumptions data significant clusters interest axis parallel data categorical readily discretized 
cactus interesting algorithm specialized categorical data 
suppose performed feasible dimensionality reduction preprocessing stage reduction incur significant information loss 
reduced space hundreds dimensions curse dimensionality issue formidable 
friedman 
vast majority data mining oriented clustering proposals numerical data focus efficient approximation means medoids density estimation 
approaches inherit fundamental limitations underlying generative data models particular difficulty highly non gaussian data spaces dimensionality 
data mining papers scalable high dimensional clustering report results simple data types tens attributes agrawal 
fair algorithms dbscan intended low dimensional problems spatial clustering fine scope 
try avoiding clustering high dimensional spaces tractable similarity space consists similarity value typically number pair objects original feature space 
graph partitioning hierarchical clustering applied highdimensional data long suitable domain specific similarity measure 
advantage working similarity space easily cater complex data types variable length sequences records mixed attributes structured data xml files assuming suitable measure similarity pairs complex objects 
case studies give specific examples approach 
balanced clustering balancing usually criteria clustering algorithms applications prefer quality 
fact algorithms means single link agglomerative clustering quite give clusters widely varying sizes 
certain approaches tend give balanced clusters 
example bisecting means strives balancing partitioning larger clusters complete average link variants agglomerative clustering general give balanced results single link algorithm 
approaches birch limit diameter cluster recombination methods isodata provide amount balancing 
mentioned previously constrained partitioning similarity graph automatically incorporates balancing objective function 
online methods notable approach balancing frequency sensitive competitive learning originally formulated remedy problem utilization parts codebook vector quantization 
introduces conscience mechanism multiplicatively scales distortion distance codebook vector cluster representative input number times exemplar winner past 
highly winning representatives discouraged attracting new inputs 
scheme quite efficient adapted clustering documents represented unit length bag words vectors promising results banerjee ghosh 
balancing global attribute conflicts scalability demands 
natural clusters data may equal size 
open relaxed balancing application demands balanced results 
middle ground constraints balancing means tight balancing graph partitioning achieved clustering balanced algorithm merging clusters subsequently 
sequence clustering techniques analysis variable length sequential patterns different analysis data types expressed efficiently points finite dimensional vector space 
elements sequence discrete symbols web page accesses protein gene sequences continuous numeric values biological time series eeg data speech signals gene expression data 
complex sequence data include involve significant long term temporal dependencies noise corrupted measurements multiple channels sequences coupled zhong ghosh 
clustering sequences relatively explored increasingly important data mining applications web usage mining bioinformatics 
existing sequence clustering approaches types data best viewed discriminative vs generative paradigm explained sec 

discriminative approaches determine distance similarity function sequence pairs resort traditional clustering 
sequences symbols drawn alphabet commonly distance functions measure cost converting sequence elementary operations insertion particular alphabet inserted deletion alphabet deleted substitution alphabet replaced transposition different alphabets swapped 
popular distance functions basic operations levenshtein edit distance levenshtein allows insertions deletions replacements 
simplified definition operations cost 
rephrased minimal number insertions deletions replacements strings equal 
literature problem cases called string matching differences mannila 
measures include known hamming distance episode distance allows insertions longest common subsequence distance allows insertions deletions section case study clustering web site visitors weblogs 
numeric sequences expert knowledge heuristics dynamic programming techniques dynamic time warping 
general determining similarity measure continuous sequences highly nontrivial 
calculating similarity measure pairs sequences computationally inefficient 
alternative way cluster sequences symbols pad sequences equal sized 
sequence viewed vector length equal padded sequence length components simply obtained corresponding positions sequence 
suitable existing vector clustering algorithm applied 
note padding may introduce artifacts severely compromise clustering results 
parametric model approaches hand attempt learn generative models data model corresponding particular cluster 
type models specified priori mixture hmms 
model structure number gaussians mog model number hidden states hmm determined model selection techniques li biswas parameters estimated em algorithm optimizes likelihood criterion 
sequence data generative model clustering typically hidden markov models natural fit smyth cadez difficulty constructing feature vectors directly finding similarity measures sequences 
case study similarity clustering market baskets web logs section case study clustering customers products transactional data summarizing opossum optimal partitioning sparse similarities metis framework described strehl ghosh 
issues challenges task discussed section 
solution form relationship approach tries side step curse dimensionality working suitable similarity space original high dimensional attribute space 
intermediary similarity space suitably tailored satisfy business criteria requiring customer clusters represent comparable amounts revenue 
efficient scalable graph partitioning clustering techniques applied space 
output clustering algorithm re order data points resulting permuted similarity matrix readily visualized dimensions clusters showing bands 
step determine suitable measure similarity market baskets 
extended jaccard similarity measure transactions xa xb xb xa xb xa xb xt xb denotes transpose subscript indicates norm considered 
measure turns suitable measure transactional data 
note features binary xa xb reduced sets items jaccard coefficient measures ratio cardinalities intersection union sets 
similarity measures pair transactions forms similarity graph partitioned pieces described section 
want cluster comparable importance striving find edge separator minimum sum edge weights partitions graph disjoint pieces balancing constraint imposed sets bound size largest cluster size measured number customers cluster amount revenue corresponding customers 
resultant constrained multi objective graph partitioning problem ably handled software package metis karypis kumar 
operates phases coarsening ii initial partitioning iii refining 
sequence successively smaller coarser graphs constructed heavy edge matching 
secondly initial partitioning constructed heuristic algorithms graph growing spectral bisection 
third phase coarsened partitioned graph undergoes boundary kernighan lin refinement 
phase vertices swapped neighboring partitions boundaries 
ensures neighboring clusters related non neighboring clusters 
metis extremely fast efficient 
added benefit imposing ordering clusters exploited visualize clustering results terms re ordered similarity matrix rendered grayscale image 
insert visually compares results means opossum provides clusters similar size revenue 
symmetric customer customer matrix reordered customers cluster contiguous 
cluster boundaries marked horizontally vertically red lines dividing matrix rectangular regions 
gray level entry proportional behavioral similarity customer customer average amount diagonal diagonal rectangles represents degree similarity corresponding clusters 
information interactively post processing cluster split merge decisions please see www strehl com demos 
numbers figures show ranges number customers cluster total revenue cluster obtained corresponding algorithm 
example means results singleton small clusters big cluster customers total customers 
opossum customer balanced solution yields cluster sizes revenue balanced version obtained changing weight formula sizes ranging comparable total dollar amounts 
fact indicate superior results indicators compactness isolation balancing additional constraint balancing 
favorable comparative results obtained popular clustering methods strehl ghosh 
insert table compact useful way profiling cluster products look descriptive discriminative features 
market basket data done looking cluster highest revenue products unusual revenue drivers products highest revenue lift 
revenue lift ratio average spending product particular cluster average spending entire dataset 
table top descriptive discriminative products customers revenue balanced clusters shown see 
customers cluster example spent money smoking cessation gum average 
interestingly fold average spending smoking cessation gum customers spend times blood pressure related items 
customers lead unhealthy lifestyle eager change 
cluster seen highly compact cluster christmas shoppers characterized greeting card candy purchases 
note opossum extra constraint clusters comparable value 
may force larger natural cluster split may case causing similar clusters 
christmas gift shoppers table top cluster moderate cluster big cluster smaller equal revenue contribution 
reinforced looking 
show versatility methodology successfully cluster web documents web usage sessions represented sparse high dimensional vectors strehl strehl ghosh 
method key drawback addressed sampling bootstrapping 
case study impact similarity measures web document clustering document clusters provide structure organizing large bodies text efficient browsing searching 
example advances internet search engines www vivisimo com www metacrawler com exploit document cluster analysis 
purpose document commonly represented vector consisting suitably normalized frequency counts words terms 
bag words representation ordering words remembered 
document typically contains small percentage words 
consider document multi dimensional vector dimension document size vocabulary tens thousands 
vectors sparse positive ordinal attribute values 
dataset looks similar market basket data described previous section important differences document vectors typically normalized unit length longer documents dominate short articles external class label available document just quality clusters 
data suitable similarity measures affect clustering results different types clustering approaches 
summarize results strehl order address question 
similarity measures sampler section introduce similarity measures relevant describing relationship documents illustrate properties 
summarize types algorithms perform different measures see chosen measures affect cluster quality 
consider choices obtaining measure similarity conversion distance metric 
minkowski distances lp xa xb xi xi standard metrics geometrical problems 
obtain euclidean distance 
possibilities converting distance metric inf closest similarity measure closest monotonic decreasing function 
euclidean space chose relate distances similarities 
consequently define euclidean normalized similarity xa xb xa xb important desirable properties see discussion commonly adopted xa xb xa xb lacks 
distance functions mahalanobis distance normalizes features covariance matrix 
due high dimensional nature text data covariance estimation inaccurate computationally intractable normalization done need document representation stage typically applying tf idf 
cosine measure 
popular measure similarity text normalizes features covariance matrix clustering cosine angle vectors 
cosine measure xb xa xb xa xb captures scale invariant understanding similarity 
stronger property cosine similarity depend length xa xb xa xb 
allows documents composition different totals treated identically popular measure text documents 
due property samples normalized unit sphere efficient processing 
pearson correlation 
collaborative filtering correlation predict feature highly similar mentor group objects features known 
normalized pearson correlation defined xa xb xa xa xb xb xa xa xb xb denotes average feature value dimensions 
note definition pearson correlation tends give full matrix 
important correlations proposed spearman correlation spearman works rank orders 
extended jaccard similarity 
binary jaccard coefficient measures degree overlap sets computed ratio number shared attributes words xa xb number possessed xa xb 
example sets binary indicator vectors xa xb cardinality intersect cardinality union rendering jaccard coefficient 
binary jaccard coefficient retail market basket applications 
strehl ghosh extended binary definition jaccard coefficient continuous discrete non negative features 
extended jaccard computed xb xa xb xa xb xt xb equivalent binary version feature vector entries binary 
extended jaccard similarity strehl ghosh retains sparsity property cosine allowing discrimination collinear vectors show subsection 
similarity measure highly related extended jaccard dice coefficient xa xb xt xb xa xb 
dice coefficient obtained extended jaccard coefficient adding xt xb numerator denominator 
omitted behaves similar extended jaccard coefficient 
dis similarity measures 
dis similarity measures mutual neighbor edit distance possible jain 
similarity measures discussed ones deemed pertinent text documents previous studies salton 
clustering algorithms text data sets comprehensive comparison needs examine clustering algorithms multiple datasets 
results compare approaches means km graph partitioning gp hypergraph partitioning hgp dimensional self organizing map som see section details variants involving different similarity measures means graph partitioning 
random partitioning added baseline yielding eleven solutions total 
datasets chosen 
data parsed yahoo 
news web pages boley downloaded ftp ftp cs umn edu dept users boley series 
pages categories highly uneven distributions 

data contains roughly postings newsgroup topics lang 
groups balanced newsgroups highly related 
data www mit edu newsgroups 

cmu web kb project craven web pages industry sectors yahoo 
selected 
industry contributes pages 
reu 
reuters distribution available lewis www research att com primary topic keyword category 
unique primary topics data 
categories highly imbalanced 
data sets encompass large variety text styles 
documents vary significantly length wrong category dead links little content images 
hub pages yahoo 
refers usually top level branch pages 
tend similar bag words content different classes contact information search windows welcome messages news content oriented pages 
contrast content reu written news agency messages 
belong category 
words stemmed porter suffix stripping algorithm frakes reu 
data sets words occurring average times document counted yield term document matrix 
excludes words generic words new rare words 
comparative results clustering quality measured mutual information balance compare results eleven approaches document data sets 
data set number clusters set twice number categories reu data set small categories 
greater number clusters classes allows multi modal distributions class 
example xor problem classes clusters 
data set results averaged trials 
experiment involved randomly chosen sample documents entire corpus 
averages sets averaged results indicate mean performance standard variation bars 
high level summary shown brevity trends clear macro level 
axis indicates quality terms mutual information axis lists clustering method indicating similarity measure cosine pearson correlation jaccard euclidean 
clear cosine correlation jaccard graph partitioning approaches best text data followed non metric kmeans approaches clearly non metric dot product similarity measure necessary quality 
due conservative normalization depending data set maximum obtainable mutual information perfect classifier 
tends 
mutual information quality approximately better random excellent result 
hypergraph partitioning constitutes third tier 
euclidean techniques including som perform poorly 
surprisingly som delivers significantly better random results despite limited expressiveness implicitly euclidean distances 
success som explained fact euclidean distance locally meaningful cell centroids locked cluster 
approaches behaved consistently data sets slightly different scale caused different data sets complexities 
performance best followed reu 
interestingly gap gp km techniques wider 
low performance reu probably due high number classes widely varying sizes 
insert compare amount balancing metric bal max 
balance highest value possible indicates clusters size 
advantages graph partitioning getting balanced results clear 
graph partitioning explicitly tries achieve balanced clusters second tier hypergraph partitioning balanced technique followed non metric means approaches 
poor balancing shown som euclidean means 
interestingly change significantly means approaches number samples increases 
graph partitioning approaches quickly approach perfect balancing expected explicitly designed 
case study shows specific domain certain similarity measures significantly outperform clustering 
text clustering suitable normalization document vectors helpful 
clustering software clustering fundamental widely routine surprising wide variety clustering software available commercially freeware 
reasonable statistical data mining package clustering module 
common implementations variant means hierarchical agglomerative clustering 
comprehensive list public domain online software clustering www pitt edu software html www com software clustering html lists clustering software data mining umbrella 
summary clustering fundamental data analysis step widely studied decades disciplines 
data mining applications associated datasets brought new clustering challenges scalability working data secondary memory obtaining comparable sized clusters 
chapter highlighted issues described advances successfully addressing 
acknowledgments 
alexander strehl banerjee inputs chapter 
research supported part nsf ecs awards intel ibm tivoli 
agrawal agrawal gehrke gunopulos raghavan 

automatic subspace clustering high dimensional data data mining applications 
acm sigmod pages 
krishnamurthy chen melton 

competitive learning algorithms vector quantization 
neural networks 
baeza yates ribeiro neto baeza yates ribeiro neto 

modern information retrieval 
addison wesley new york 
banerjee ghosh banerjee ghosh 

clickstream clustering weighted longest common subsequences 
workshop web mining st siam conference data mining pages 
banerjee ghosh banerjee ghosh 

scaling balanced clustering 
proc 
nd siam int conf data mining pages 
boley boley gini gross han hastings karypis kumar mobasher moore 

partitioning clustering web document categorization 
decision support systems 
brand brand oliver pentland 

coupled hidden markov models complex action recognition 
proc 
ieee conf 
computer vision pattern recognition pages 
cadez cadez smyth 

general probabilistic framework clustering individuals objects 
proc 
sixth kdd conference pages 
chakrabarti mehrotra chakrabarti mehrotra 

hybrid tree index structure high dimensional feature spaces 
proc 
icde pages 
chang ghosh chang ghosh 

unified model probabilistic principal surfaces 
ieee trans 
pami 
cover thomas cover thomas 

elements information theory 
wiley 
craven craven dipasquo freitag mccallum mitchell nigam slattery 

learning extract symbolic knowledge world wide web 
aaai pages 
cutting cutting karger pedersen tukey 

scatter gather approach browsing large document collection 
proc 
th intl 
acm sigir conf 
research development information retrieval pages 
dasgupta dasgupta 

experiments random projection 
proc 
th conf 
uncertainty ai stanford ca june july pages 
dempster dempster laird rubin 

maximum likelihood incomplete data em algorithm 
royal statistical society 
series methodological 
dhillon dhillon modha 

visualizing class structure multidimensional data 
editor proc 
th symposium interface computing science statistics minneapolis mn may 
dhillon dhillon 

clustering documents words bipartite spectral graph partitioning 
technical report department computer science university texas austin 
dhillon modha dhillon modha 

data clustering algorithm distributed memory multiprocessors 
proc 
large scale parallel kdd systems workshop acm sigkdd 
duda duda hart stork 

pattern classification nd ed 
wiley new york 
eisen eisen spellman brown botstein 

cluster analysis display genome wide expression patterns 
proc 
natl 
acad 
sci 
usa 
faloutsos lin faloutsos lin 

fastmap fast algorithm indexing data mining visualization traditional multimedia datasets 
proc 
acm sigmod int 
conf 
management data san jose ca pages 
acm 
lewis elkan 

scalability clustering algorithms revisited 
sigkdd explorations 
fayyad fayyad reina bradley 

initialization iterative refinement clustering algorithms 
proc 
th intl 
conf 
machine learning icml pages 
frakes frakes 

stemming algorithms 
frakes baeza yates editors information retrieval data structures algorithms pages 
prentice hall new jersey 
friedman friedman 

overview predictive learning function approximation 
cherkassky friedman wechsler editors statistics neural networks proc 
nato asi workshop pages 
springer verlag 
ganti ganti gehrke ramakrishnan 

cactus clustering categorical data summaries 
proc 
fifth int conference knowledge discovery data mining pages 
gluck gluck 

information uncertainty utility categories 
proceedings seventh annual conference cognitive science society pages irvine ca 
lawrence erlbaum associates 
gupta ghosh gupta ghosh 

detecting seasonal trends cluster motion visualization high dimensional transactional data 
proc 
siam conf 
data mining sdm pages 
han han karypis kumar mobasher 

hypergraph clustering high dimensional data sets summary results 
data engineering bulletin 
han kamber han kamber 

data mining concepts techniques 
morgan kaufmann 
hartigan hartigan 

clustering algorithms 
wiley new york 
indyk indyk 

sublinear time approximation scheme clustering metric spaces 
proceedings th symposium foundations computer science pages 
jain dubes jain dubes 

algorithms clustering data 
prentice hall new jersey 
jain jain murty flynn 

data clustering review 
acm computing surveys 
karypis karypis han kumar 

chameleon hierarchical clustering dynamic modeling 
ieee computer 
karypis kumar karypis kumar 

fast high quality multilevel scheme partitioning irregular graphs 
siam journal scientific computing 
kohonen kohonen 

self organizing maps 
springer berlin heidelberg 
second extended edition 
kumar ghosh kumar ghosh 

generalized framework associative modular learning systems 
proceedings applications science computational intelligence ii pages orlando florida 
kumar kumar ghosh crawford 

feature extraction algorithms classification hyperspectral data 
ieee trans 
geoscience remote sensing 
lang lang 

newsweeder learning filter netnews 
international conference machine learning pages 
leighton rao leighton rao 

maxflow min cut theorems designing approximation algorithms 
journal acm 
levenshtein levenshtein 

binary codes capable correcting deletions insertions reversals 
problems information transmission 
li biswas li biswas 

improving hmm clustering bayesian model selection 
ieee international conference systems man cybernetics nashville 
mannila mannila 

similarity event sequences 
proc 
th intl 
workshop temporal representation reasoning pages 
mao jain mao jain 

artificial neural networks feature extraction multivariate data projection 
ieee transactions neural networks 
rastogi shim rastogi shim 

scalable algorithms mining large databases 
han editor kdd tutorial notes 
acm 
salton salton 

automatic text processing transformation analysis retrieval information computer 
addison wesley reading ma 
smyth smyth 

clustering sequences hidden markov models 
mozer petsche editors advances neural information processing systems pages 
mit press 
spearman spearman 

footrule measuring correlations 
british journal psychology 
steinbach steinbach karypis kumar 

comparison document clustering techniques 
kdd workshop text mining 
strehl ghosh strehl ghosh 

scalable approach balanced high dimensional clustering market baskets 
proc 
bangalore volume lncs pages 
springer 
strehl ghosh strehl ghosh 

clustering visualization high dimensional data mining 
informs journal computing 
press 
strehl strehl ghosh mooney 

impact similarity measures web page clustering 
proceedings seventeenth national conference artificial intelligence workshop artificial intelligence web search july austin texas usa pages 
aaai 
torgerson torgerson 

multidimensional scaling theory method 
psychometrika 
watanabe watanabe 

knowing guessing formal study 
john wiley sons zhang zhang ramakrishnan livny 

birch new data clustering algorithm applications 
data mining knowledge discovery 
zhao karypis zhao karypis 

criterion functions document clustering experiments analysis 
technical report university minnesota 
zhong ghosh zhong ghosh 

hmms coupled hmms multi channel eeg classification 
proc 
ijcnn honolulu pages 
grouping objects represented dimensional space numeric features clusters 
similarity dendogram obtained single link agglomerative clustering objects described 
visualizing partitioning customers clusters purchase products means customer balanced sum opossum revenue balancing 
mi gp gp gp km km km hgp som gp km bal gp gp gp gp hgp km km km som km comparison cluster quality terms mutual information balance average data sets trials samples 
error bars indicate standard deviation 
graph partitioning significantly better terms mutual information balance 
euclidean distance approaches perform better random clustering 
table list descriptive top discriminative products bottom dominant value balanced clusters obtained data see 
item average amount spent cluster corresponding lift 
top product lift sec 
product lift third product lift bath gift packs hair growth island smoking tp canning item blood pressure tp coffee maker hea games items facial tp wine jug ite alkaline appliances item appliances appl christmas light appliances hair tp toaster oven christmas food christmas cards cold girl toys boy toys items everyday girls christmas christmas home christmas food christmas christmas light pers cd player tp laundry soap facial hand body film cameras planners bloc tools binders items drawing american paperback items op american christmas cards basket candy tp seasonal boo american box item group tp seasonal boo bag basket candy cold cold items hair clr american cls face cls face hair clr headache top product lift sec 
product lift third product lift action items tp video comedy family items smoking blood pressure pnts nut hea miscellaneous tp items tp exercise ite dental appliances item peg tp items multiples packs christmas light tv items sleep aids item items tp beer super tp items tp metal tp furniture tp art craft tp family plan pers cd player tp plumbing ite adult cat child pro treatment items ca items mop broom lint cards tools dental repair tp lawn seed tp telephones gift boxes item hearing aid bat american economy tp seasonal boo girls socks ite tp wine va group items tp med oint tp tp bath hair clr imple tp power tools cls face telephones cord 
