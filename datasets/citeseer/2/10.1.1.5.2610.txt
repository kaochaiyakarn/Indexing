cheng soon ong alexander smola robert williamson research school information sciences engineering australian national university canberra act australia ong alex smola bob anu edu au consider problem choosing kernel suitable estimation gaussian process estimator support vector machine 
novel solution involves defining reproducing kernel hilbert space space kernels 
utilizing analog classical representer theorem problem choosing kernel parameterized family kernels varying width reduced statistical estimation problem akin problem minimizing regularized risk functional 
various classical settings model kernel selection special cases framework 
choosing suitable kernel functions estimation gaussian processes support vector machines important step inference process 
date systematic techniques assist choice 
restricted problem choosing width parameterized family kernels gaussian simple elegant solution 
development solves problem restricted sense involves semidefinite programming learn arbitrary positive semidefinite matrix subject minimization criteria kernel target alignment maximum posterior probability minimization learning theoretical bound subject cross validation settings 
restriction mentioned methods kernel matrix kernel 
furthermore whilst demonstrably improving performance estimators degree require clever parameterization design method particular situations 
general principles guide choice family kernels choose efficient parameterizations space suitable penalty terms combat overfitting 
point particularly issue large set semidefinite matrices disposal 
whilst providing complete solution problems presents framework allows optimization parameterized family relatively simply crucially intrinsically captures tradeoff size family kernels sample size available 
furthermore solution optimizing kernels kernel matrix 
approaches learning kernel include boosting bounding rademacher complexity 
outline show section kernel learning methods exists functional quality functional plays similar role empirical risk functional subsequently section kernel kernels called conjunction regularization reproducing kernel hilbert space formed kernels leads systematic way parameterizing function classes whilst managing overfitting 
give examples section show section practically 
due space constraints consider support vector classification 
quality functionals train fx xm denote set training data train fy ym set corresponding labels jointly drawn iid probability distribution furthermore test test denote corresponding test sets drawn 
train test train test introduce new class functionals data call quality functionals 
purpose indicate kernel training data train train suitable kernel explaining training data 
definition empirical quality functional kernel data define empirical quality functional depends exists function kernel matrix 
basic idea adapt manner minimized single dataset sufficiently rich class kernels general possible find kernel attains arbitrarily small values train train training set 
test test similarly small general 
analogously standard methods statistical learning theory aim minimize expected quality functional definition expected quality functional suppose empirical quality functional 
ex expected quality functional expectation taken respect note similarity empirical risk estimator remp suitable loss function cases compute value functional depends sample drawn function cases ex ex remp known expected risk 
examples quality functionals derive exact minimizers possible 
example kernel target alignment quality functional introduced assess alignment kernel training labels 
defined alignment emp train train ky kyk kkk denotes vector elements train kyk denotes norm kkk frobenius norm kkk ij note definition looks somewhat different algebraically identical 
mean badness minimizing functional 
decomposing eigensystem see minimized yy case alignment emp train train yy kyk kyk kyk kyk clear expect alignment emp train train data set chosen determine example regularized risk functional reproducing kernel hilbert space rkhs associated kernel regularized risk functionals form reg train train kfk kfk rkhs norm virtue representer theorem see know minimizer written kernel expansion :10.1.1.11.2062
loss leads quality functional emp train train min minimizer difficult find carry double minimization note yy kyk emp train train sufficiently large emp train train arbitrarily close 
disallow setting zero setting trk determine minimum follows 
set kzk zz kzk choosing argmin yields minimum respect proof global minimizer quality functional omitted brevity 
example negative log posterior gaussian processes functional similar reg train train includes regularization term negative log prior loss term negative log likelihood 
addition includes log determinant measures size space spanned quality functional emp train train min log jx log jkj note full rank send cases need excluded 
fix jkj exclude case set kyk yy kyk yy leads jkj 
assumption minimum log respect attained see leads minimum emp train train 
examples cross validation leave estimators luckiness framework radius margin bound empirical quality functionals arbitrarily minimized 
examples illustrate existing methods assessing quality kernel fit quality functional framework 
saw rich class kernels optimization result kernel useless prediction purposes 
example danger optimizing free lunch 
hyper reproducing kernel hilbert space introduce method optimizing quality functionals effective way 
method propose involves reproducing kernel hilbert space kernel hyper rkhs 
basic properties rkhs see def thm citations details :10.1.1.11.2062
definition reproducing kernel hilbert space nonempty set called index set denote hilbert space functions called reproducing kernel hilbert space endowed dot product 
norm kfk hf fi exists function satisfying 
reproducing property hf 
particular hk 



spans 
jx xg completion advantage optimization rkhs certain conditions optimal solutions linear combination finite number basis functions regardless dimensionality space seen theorem 
theorem representer theorem denote strictly monotonic increasing function set arbitrary loss function 
minimizer regularized risk ym xm admits representation form 
definition allows define rkhs kernels simply introducing treating functions definition hyper reproducing kernel hilbert space nonempty set compounded index set 
hilbert space functions endowed dot product 
norm kkk hk ki called hyper reproducing kernel hilbert space exists properties 
reproducing property hk 
particular hk 



spans 
jx 
fixed kernel second argument fixed function kernel 
distinguishes normal rkhs particular form index set additional condition kernel second argument fixed argument 
condition somewhat limits choice possible kernels 
hand allows simple optimization algorithms consider kernels convex cone analogously definition regularized risk functional define regularized quality functional reg kkk regularization constant kkk denotes rkhs norm minimization reg prone overfitting minimizing regularization term kkk effectively controls complexity class kernels consideration 
regularizers kkk possible 
question arising immediately minimize regularized quality functional efficiently 
show minimum linear combination 
corollary representer theorem hyper rkhs hyper rkhs denote strictly monotonic increasing function set arbitrary quality functional 
minimizer regularized quality functional kkk admits representation form ij 
proof need rewrite satisfies conditions theorem 
ij 
properties loss function depends values ij furthermore kkk rkhs regularizer representer theorem applies expansion follows 
result shows optimizing entire potentially infinite dimensional hilbert space kernels able find optimal solution choosing finite dimensional subspace 
dimension required surprisingly significantly larger number kernels required kernel function expansion direct approach possible small problems 
sparse expansion techniques problem tractable practice :10.1.1.11.2062
examples having introduced theoretical basis hyper rkhs need answer question practically useful exist satisfy conditions definition 
address question giving set general recipes building kernels 
example power series construction denote positive semidefinite kernel function positive taylor expansion coefficients convergence radius fixed sum kernel functions kernel kernel 
show kernel note 
example harmonic special case harmonic denote kernel rbf kernels satisfy property set 
example gaussian harmonic exp kx exp kx kx converges expression kkk converges frobenius norm power series expansion exp 

sinh 


cosh 

ln table examples find simply consulting tables power series functions 
table contains list suitable expansions 
recall expansions mainly chosen computational convenience particular clear particular class kernels useful expansion 
example explicit construction know reasonable guess kernels potentially relevant range scales kernel width polynomial degrees may set candidate kernels say 
kn define clearly kn 
application minimization regularized risk recall case regularized risk functional regularized quality optimization problem takes form minimize kfk kkk second term kfk linear function convex loss function regularized quality functional convex corresponding regularized quality functional reg emp kkk fixed problem formulated constrained minimization problem subsequently expressed terms lagrange multipliers minimum depends efficient minimization compute derivatives respect lemma tells extension result omit proof brevity lemma denote convex functions parameterized 
minimum optimization problem denote minimizer minimize subject denotes derivative respect second argument minimizer written kernel expansion representer theorem hyper rkhs optimal regularized quality functional written soft margin loss reg max pq pq ij pq minimization achieved alternating minimization fixed quadratic optimization problem subsequently minimization ij ensure positivity kernel matrix fixed low rank approximation finite number parameters despite optimization possibly infinite dimensional hilbert spaces presents formidable optimization problem practice coefficients 
explicit expansion type optimize expansion coefficients directly means simply quality functional penalty expansion coefficients 
approach recommended terms 
general case resort low rank approximation described :10.1.1.11.2062
means pick 
small fraction terms approximate sufficiently 
experimental results summary experimental setup test claims kernel adaptation regularized quality functionals performed preliminary tests datasets uci repository pima ionosphere wisconsin diagnostic breast cancer usps database handwritten digits vs 
datasets split training data test data usps data provided split 
experiments repeated random splits 
deliberately attempt tune parameters choices uniformly sets kernel width set dimensionality data 
deliberately chose large value comparison usual rules thumb avoid default kernels 
adjusted vapnik style parameterization svms 
commonly reported yield results 
gaussian harmonic chosen giving adequate coverage various kernel widths small focus exclusively wide kernels close treat widths equally 
regularization set compared results performance generic support vector machine values chosen hand tuned cross validation 
results despite fact try tune parameters able achieve highly competitive results shown table 
worth noticing number required low rank decomposition matrix contained typically rendering optimization problem costly standard support vector machine high quality approximation optimization typically 
dramatically reduced computational burden 
non optimized parameters different data sets achieved results comparable classification boosting optimized svms kernel target alignment note smaller part data training reg reg best tuned data size train test train test svm pima usps na table training test error percent :10.1.1.133.1040
results reg comparable hand tuned svms right column ionosphere data 
suspect due small training sample 
summary outlook regularized quality functional allows systematic solution problems associated choice kernel 
quality criteria include target alignment regularized risk log posterior 
regularization implicit approach allows control overfitting occurs optimizes large choice kernels 
promising aspect current opens way theoretical analyses price pays optimizing larger set kernels 
current research devoted working analysis subsequently developing methods design 
supported australian research council 
authors grace wahba helpful comments suggestions 
lanckriet cristianini bartlett el ghaoui jordan 
learning kernel matrix semidefinite programming 
icml 
morgan kaufmann 
williams 
prediction gaussian processes linear regression linear prediction 
jordan editor learning inference graphical models 
kluwer academic 
chapelle vapnik bousquet mukherjee 
choosing kernel parameters support vector machines 
machine learning 
forthcoming 
wahba 
spline models observational data volume cbms nsf regional conference series applied mathematics 
siam philadelphia 
crammer keshet singer 
kernel design boosting 
advances neural information processing systems 
press 
bousquet herrmann 
complexity learning kernel matrix 
advances neural information processing systems 
press 
cristianini elisseeff shawe taylor 
optimizing kernel alignment 
technical report nc tr neurocolt www neurocolt com 
scholkopf smola 
learning kernels 
mit press 
fine 
efficient svm training low rank kernel representation 
technical report ibm watson research center new york 
freund schapire 
experiments new boosting algorithm 
icml pages 
morgan kaufmann publishers 
ratsch onoda muller 
soft margins adaboost 
machine learning 
