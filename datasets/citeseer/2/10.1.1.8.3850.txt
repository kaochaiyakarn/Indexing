reinforcement learning approach job shop scheduling wei zhang department computer science oregon state university corvallis oregon thomas dietterich department computer science oregon state university corvallis oregon apply reinforcement learning methods learn domain specific heuristics job shop scheduling 
repair scheduler starts critical path schedule incrementally repairs constraint violations goal finding short conflict free schedule 
temporal difference algorithm td applied train neural network learn heuristic evaluation function states 
evaluation function step lookahead search procedure find solutions new scheduling problems 
evaluate approach synthetic problems problems nasa space shuttle payload processing task 
evaluation function trained problems involving small number jobs tested larger problems 
td scheduler performs better best known existing algorithm task zweben iterative repair method simulated annealing 
results suggest reinforcement learning provide new method constructing high performance scheduling systems 
problems commercial interest including job shop scheduling instances np complete problems 
little hope finding generalpurpose solutions problems 
particular application setting usually domainspecific constraints regularities exploited construct fast domain specific heuristic algorithms 
domain specific heuristics engineered hand process expensive time consuming 
goal research described explore possibility applying reinforcement learning algorithms discover domain specific heuristics automatically 
reinforcement learning algorithms learn policies state space problem solving tasks 
state policy specifies action performed 
learning learning system receives reinforcement signal called reward action 
goal learning system find policy maximizes expected reinforcement actions 
context job shop scheduling policy tells scheduling action order maximize measure quality final schedule 
focus application domain space shuttle payload processing nasa 
goal schedule set tasks satisfy set temporal resource constraints seeking minimize total duration makespan schedule 
particular interest nasa scheduling methods repair schedule unforeseen difficulty arises 
previous task zweben colleagues zweben developed iterative repair scheduling procedure combines set heuristics simulated annealing search procedure 
resulting scheduling system provides efficient flexible facility scheduling space shuttle ground operations 
regular kennedy space center challenge learning approach discover scheduling heuristics match exceed quality efficiency iterative repair method 
remainder describe scheduling task greater detail 
briefly describe zweben iterative repair scheduler 
review reinforcement learning method known td describe scheduling task formulated td applied 
describe experiments simulated problem sets discuss results 
results indicate reinforcement learning outperform iterative repair scheduler realistic scheduling tasks 
furthermore knowledge learned reinforcement learning applied scheduling problems larger complex ones studied training 
initial results suggest reinforcement learning important role play developing high performance ai scheduling systems 
nasa domain iterative repair method nasa space shuttle payload processing domain requires scheduling various tasks performed install test payloads placed cargo bay space shuttle 
jobshop scheduling terminology shuttle mission job 
job consists partially ordered set tasks performed 
task duration list resource requirements 
example task mission sequence test duration requires quality control officers technicians ate hits 
different types resources 
may units resource available 
example quality control officers available technicians 
available resources may split resource pools example quality control officers subdivided pools size 
task requires quality control officers drawn pool 
resource pools model multiple shifts multiple physical locations 
complete schedule specify start time task resource pool resource requirement task satisfied 
typical problem involves simultaneous scheduling shuttle missions mission involves tasks 
domain requires solving scheduling problems containing tasks 
tasks performed prior launch take place shuttle landed 
shuttle mission fixed launch date starting date date 
tasks prior launch deadlines ready times tasks landing ready times deadlines 
key goal scheduling system minimize total duration schedule 
challenging simply finding feasible schedule 
zweben developed iterative repair method solving scheduling problem 
critical path schedule constructed working backward forward launch landing dates 
task prior launch scheduled late temporal partial order permit task landing scheduled early temporal partial order permit 
resource constraints ignored resource requests randomly assigned resource pools 
critical path schedule constructed efficiently provides starting state scheduling problem space 
state problem space possible operators applied 
operator changes pool assignment resource requirements task 
applied pool reassignment allow resource requirement successfully satisfied 
move operator moves task different time temporal dependents task critical path method leaving resource pool assignments dependents unchanged 
move operator applied move task earlier time violated resource requirement satisfied 
operators applied iterative repair method follows 
step earliest constraint violation resource pool allocated identified 
reassign pool operator applied reduce allocation applied 
move operator applied move offending tasks earlier time 
different pool possible chosen random 
earlier move possible chosen random 
tasks involved resource violation chosen random heuristic prefers move task requires amount resource nearly equal amount allocated temporal dependents needs moved short distance satisfy resource request 
control structure algorithm applies simulated annealing minimize number resource pool violations 
operator applied number violations resulting schedule computed 
decreased resulting schedule accepted current schedule 
increased resulting schedule accepted probability exp gamma deltav deltav change number violations current temperature 
temperature gradually decreased 
search proceeds constraints violated 
obtain short schedule algorithm run times shortest resulting schedule selected 
reinforcement learning temporal difference learning scheduling reinforcement learning methods learn policy selecting actions problem space 
policy tells state action performed state 
action chosen applied state problem space shifts state learning system receives reinforcement 
view scheduling problem reinforcement learning problem describe problem space reinforcement function employ problem space zweben starting state critical path schedule discussed 
define reinforcement function give reinforcement gamma schedule contains constraint violations 
assesses small penalty scheduling action reassign pool move intended encourage reinforcement learning prefer actions quickly find schedule 
schedule free violations reinforcement negative resource dilation factor 
rdf attempts provide measure length schedule final reinforcement intended encourage reinforcement learning find short final schedules 
reinforcement function depends resulting state write 
rdf defined follows 
capacity fixed capacity resource type combined capacity resource pools resource type time schedule current utilization resources type capacity resource type time matter assign resource requests resource pools type 
define resource utilization index rui resource type time rui max ae capacity oe resource allocated rui fraction 
total resource index schedule length sum resource utilization index taken resources times rui definitions resource dilation factor defined rdf understand rationale formula note final schedule just times length schedule 
final schedule resource rui 
theta negative value reinforcement function reinforcement learning easier reinforcement function independent difficulty scheduling problem 
difficult problem jobs simultaneous deadlines require long schedule simple problem require shorter schedule 
total resource utilization index initial schedule measures amount resources initial state provides crude measure difficulty scheduling problem 
normalize final schedule length produce resource dilation factor 
specified view repair scheduling reinforcement learning problem turn attention learning algorithm 
suppose point learning process developed policy says state best action select 
define associated function called value function tells cumulative reward receive follow policy state onward 
formally number steps conflict free schedule 
reinforcement learning attempt learn value function optimal policy denoted directly learning learned optimal value function transform optimal policy simple step lookahead search choose best action state compute state result applying possible action state action compute value resulting state choose action maximizes value 
note approach requires know effects operators certainly true repair scheduling operators 
learn value function apply method temporal difference learning known td developed sutton 
td value function represented feed forward neural network vector weights network 
policy fixed td applied learn value function pi follows 
sequence states visited policy associated reinforcements sn 
step compute temporal difference error step gamma td computes smoothed gradient gamma updates weights network deltaw ffj smoothing parameter combines previous gradients current gradient ff learning rate 
td algorithm designed learn value function stationary markov random process result fixed policy 
reinforcement learning want apply learn value function optimal policy starting initial random policy 
employ form value iteration 
td applied online sequences states reinforcements result choosing actions current estimated value function state learning conduct onestep lookahead search current estimated value function evaluate states resulting applying possible operator 
select action maximizes predicted value resulting state applying action receiving reward update estimate reflect difference value informed value 
employ slightly complex procedure described 
means policy continually changing learning process 
fortunately td converge conditions sutton modifications algorithm preliminary experiments 
reinforcement learning algorithm critical perform kind exploration discover new better ways getting start state goal 
employed simple exploration strategy 
state probability fi choose random action action recommended current value function policy 
initially fi set 
action fi decreased amount deltafi reaches final value 
values deltafi 
second perform weight updates neural network action 
remember sequence states visited path starting state final conflict free schedule 
update network starting final action working backward start action sequence 
experimentally works better simple online training values backed date 
third employ lin experience replay method 
learning best sequence moves start goal remembered training sequences update network best training sequence 
improved learning performance significantly 
fourth employ full step lookahead search select actions branching factor problem space typically costly compute value successor states 
employ random sample greedy search generates random subset possible operators evaluates resulting states 
best operators chosen 
size random sample determined incrementally 
initial sample actions chosen 
resulting computed values permitted amount error ffl desired confidence gamma ffi compute probability value best sampled action ffl best possible action 
continue sampling possible actions probability exceeds gamma ffi set ffl ffi 
random sample greedy search employed training execution 
final change learning algorithm actual states scheduling process input neural network 
neural network accept fixed vector feature values describing state current schedule 
schedules hand variable length objects 
necessary define set useful features extract important aspects current schedule neural network predict value state 
defined features modest amount experimentation mean standard deviation free pool capacity bottleneck pools simple experiments showed technician logistics electrical engineer mechanical engineer quality control officer resource types major bottleneck resources 
bottleneck pool number unallocated units free capacity measured schedule period mean standard deviation quantity provide features pool 
mean standard deviation slacks slack time task temporal prerequisites difference time prerequisite task scheduled start time task 
measure minimum slack task temporal prerequisites average slack task 
mean standard deviation quantities taken tasks provide features 
modified rdf slightly modified version resource dilation factor current schedule 
numerator modified rdf computed capacity allocation individual resource pools resource types 
allocation index total number units allocated resources current schedule divided total number units allocated resources starting schedule 
percentage windows violation window defined maximal period time set currently scheduled tasks change 
schedule segmented sequence windows 
compute percentage windows contain constraint violation 
find earliest window contains constraint violation compute percentage windows violations 
percentage windows violation resolved pool reassignment fraction windows having constraint violations total amount resources assigned total capacity resources subdivided pools resource requirements met 
percentage time units violation measured schedule period 
violated window index normalized index earliest window violation 
total number windows 
feature gamma violations repaired value decreases zero 
window violation set features developed studying small scheduling problems find quantities ability predict rdf 
believe features improved substantially goal ongoing research 
consequence features full state learned policy may enter infinite loops 
taken steps detect prevent loops 
randomness introduced random sample greedy procedure random exploration process tends avoid loops state revisited action may chosen 
second states visited solving particular problem recorded checked detect loops 
loop detected apply learned value function compute second best action choose 
loop detected state backtrack preceeding state take second best action 
create loop continue backtracking earlier states 
methods briefly describe methods applied generate training test problems network architecture parameters employed learning algorithm 
problem sets constructed problem sets artificial problem set problem set specifications nasa problem 
artificial problems generated follows 
generated pool jobs 
constructed scheduling problems choosing random subsets jobs 
intended model nasa setting limited number possible shuttle cargo bay configurations jobs scheduling problem unique combination shuttle missions 
generally models job shop new scheduling interval requires scheduling unique mix standard jobs 
generate synthetic job choose number tasks randomly range 
set temporal constraints tasks randomly generated approximately possible pairwise precedence constraints asserted 
resource requirements determined task 
types resources 
resource pools pool capacity units capacity units 
resource requirements randomly assigned task uniformly range units resource type 
pool jobs generated way training problems test problems constructed 
generate problem choose number jobs problem equal probability desired number jobs selected randomly replacement job pool 
job assigned completion deadline deadlines randomly separated time units 
sixteen input features computed represent schedules problems pool capacity features pools slack features features describing modified rdf percentage windows time units violation percentage violated windows violation resolved pool reassignment 
training training problems held validation set determine halt training 
remaining problems repeatedly processed train value function networks 
addition test problems generated second test set larger problems evaluate ability learned value functions scale larger scheduling problems 
larger problems generated way smaller problems number jobs chosen uniformly 
space shuttle payload processing task problem consists set shuttle missions launch dates months apart 
mission payloads 
considered kinds payloads long module lm mission peculiar equipment support structure pallet pallet 
tasks respectively 
types resources major bottleneck resources 
randomly generated training set problems test set problems 
training problems contained shuttle missions 
training problems held validation determine training 
test problems contained shuttle missions 
test problems assess ability learned policy scale larger problems 
shuttle problems input features features pool capacity slack features modified rdf features describing windows violation percentage time units violation index violated window index 
network architecture training procedure represent value function trained feed forward networks having sigmoidal hidden units sigmoidal output units 
output units encode predicted rdf technique overlapping gaussian ranges pomerleau follows 
output unit represents assigned rdf value 
artificial problems rdf values 
problems rdf values 
training target output activation output unit set target rdf gamma oe standard normal probability density function mean standard deviation oe 
testing predicted rdf value computed act delta act act actual output activation output unit problem trained different networks combinations parameters learning rate ff exploration schedule deltafi 
preliminary experiments showed perform 
training set problems processed round robin fashion 
problem solved networks obtain sequence states actions 
network updated backpropagation td processing state sequence working backward final state 
passes training set cross validation test conducted compute average rdf final schedules produced cross validation problems 
best network cross validation parameter sets retained 
network training continues cross validated rdf network worse previous measured values cross validated rdf 
networks chosen testing follows 
best networks cross validation retained corresponding final networks 
retain final networks compensate variance cross validation measurements 
simulated annealing component iterative repair method set starting temperature synthetic scheduling task task 
accepted repairs schedule temperature reduced results shows average cross validation rdf value function networks trained ff 
horizontal axis gives number training sequences processed 
shows performance trained networks improving cross validation problems 
plots number repair actions networks 
shows reduction number actions required convert starting schedule conflict free final schedule 
figures compares performance temporal difference td scheduling iterative repair ir number sequences trained average rdf cv problems number sequences trained average number repairs cv problems method zweben 
vertical axis rdf best conflict free schedule far 
horizontal axis machine independent proxy amount cpu time consumed method 
ir horizontal axis gives number restarts simulated annealing procedure vertical axis records rdf best conflict free schedule far 
longer ir run better performance 
td scheduler horizontal axis represents number neural network evaluation functions employed 
networks solve scheduling problem problem solved times network schedule having best rdf returned answer 
best networks determined cross validation 
curves networks 
care taken interpreting horizontal axis measure cpu time 
step td scheduler requires cpu time step ir scheduler td scheduler perform random sample lookahead search check loops 
average td spends times cpu time step ir 
hand td requires fewer steps find conflict free schedule 
average sequence length iteration td long average ir sequence 
net effect iteration td equivalent approximately iterations ir 
bearing mind key point notice curve td scheduler lies curve iterative repair 
means amount cpu time td finds better schedule lower rdf 
example networks td obtains rdf compared ir rdf delta iterations 
improvement schedule lasting year savings days number iterations td ir performance comparison td ir small scale problems number iterations td ir performance comparison td ir medium scale problems thousands dollars 
curve shows iterative repair requires time iterations vs find schedule quality matches rdf td 
shows similar comparison td ir larger test problems 
difference algorithms pronounced 
temporal difference scheduling scales better larger problems trained smaller problems 
shows analogous results temporal difference iterative repair test set problems 
horizontal axis log cpu time 
see td maintains constant factor advantage iterative repair 
temporal difference scheduling finds better schedules faster iterative repair 
note just gives average rdf test set 
random components algorithms hides considerable variation 
reveals variation 
say td wins particular problem rdf best schedule computed far better rdf best ir schedule computed amount cpu time 
algorithms said tie find schedules identical rdf values 
plots fraction td wins td wins ties function log cpu time 
see low cpu costs td wins problem 
eventually cpu time larger td wins ties slightly time 
discussion concluding remarks results show temporal difference td methods outperform best previous algorithm scheduling space shuttle payload processing jobs 
furthermore running time seconds td ir performance comparison td ir pps problems rdf running time seconds td win tie td win performance comparison td ir pps problems wins clearly ways td methods improved 
example current set features needs improved learning procedure capture domain specific knowledge 
evidence suggest training procedure improved 
authors bradtke thrun schwartz boyan moore schraudolph shown pitfalls associated neural networks function approximation schemes represent value functions reinforcement learning 
results notable success tesauro td backgammon system show situations pitfalls encountered 
important open question understand td works applications 
suspect success td methods domain results factors 
probably solutions scheduling problem 
certainly solution paths search space highly redundant 
second td essentially technique smoothing adjacent estimates final rdf 
smoothing remove local minima poor job predicting final rdf 
properties may permit simple greedy algorithm find schedules 
properties may explain iterative repair method simulated annealing succeeds domain 
simulated annealing stochastic method locally smoothing objective function 
applied domain simulated annealing run long find global optimum may able escape local minima find acceptable solution spite 
industrial scheduling problems abound generalpurpose solutions problems probably exist 
research shown reinforcement learning methods potential quickly finding highquality solutions scheduling problems 
goal research improve learning methods applied minimum domain specific engineering produce new costeffective scheduling technology 
authors rich sutton monte zweben helpful discussions 
authors gratefully acknowledge support nasa nag nasa ames research center 
additional support provided nsf cda iri 
boyan moore boyan moore 
generalization reinforcement learning safely approximating value function 
advances neural information processing systems san mateo ca 
morgan kaufmann 
bradtke bradtke 
reinforcement learning applied linear quadratic regulation 
advances neural information processing systems pages san mateo ca 
morgan kaufmann 
kautz carpenter zweben davis 
space shuttle ground processing scheduling system 
zweben fox editors intelligent scheduling chapter pages 
morgan kaufmann san francisco ca 
pomerleau pomerleau 
efficient training artificial neural networks autonomous navigation 
neural computation 
schraudolph schraudolph dayan sejnowski 
td learn evaluation function game go 
advances neural information processing systems san mateo ca 
morgan kaufmann 
sutton sutton 
learning predict methods temporal differences 
machine learning august 
tesauro tesauro 
practical issues temporal difference learning 
machine learning 
thrun schwartz thrun schwartz 
issues approximation reinforcement learning 
proceedings fourth connectionist models summer school hillsdale nj 
lawrence erlbaum publisher 
zweben zweben 
scheduling rescheduling iterative repair 
zweben fox editors intelligent scheduling chapter pages 
morgan kaufmann san francisco ca 
