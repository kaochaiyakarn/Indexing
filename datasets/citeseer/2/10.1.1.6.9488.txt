hierarchical hidden markov models information extraction cs wisc edu department computer sciences university wisconsin madison wisconsin information extraction defined task automatically extracting instances specified classes relations text 
consider case machine learning methods induce models extracting relation instances biomedical articles 
propose evaluate approach hierarchical hidden markov models represent grammatical structure sentences processed 
approach uses shallow parser construct multi level representation sentence processed 
train hierarchical hmms capture regularities parses positive negative sentences 
evaluate method inducing models extract binary relations biomedical domains 
experiments indicate approach results accurate models baseline hmm approaches 
application domains potential greatly increase utility line text sources automated methods mapping selected parts unstructured text structured representation 
example curators genome databases tools accurately extract information scientific literature entities genes proteins cells diseases reason interest developing methods task information extraction defined automatically recognizing extracting instances specific classes entities relationships entities text sources 
machine learning methods play key role systems difficult costly manually encode necessary extraction models 
hidden markov models hmms leek bikel freitag mccallum related probabilistic sequence models mc lafferty accurate methods learning information extractors 
learning hmms information extraction focused tasks semi structured text sources english grammar play key mark craven craven wisc edu ray cs wisc edu department biostatistics medical informatics university wisconsin madison wisconsin 
report identification integral membrane enzyme 
enzyme ubc localizes catalytic domain facing 
subcellular localization ubc example information extraction task 
top shows part document wish extract instances subcellular localization relation 
bottom shows extracted tuple 
role 
contrast task consider extracting information abstracts biological articles hirschman 
domain important learned models able represent regularities grammatical structure sentences 
approach hierarchical hidden markov models hhmms fine extract information scientific literature 
hierarchical hidden markov models multiple levels states describe input sequences different levels granularity 
models top level hmms represent sentences level phrases lower level hmms represent sentences level individual words 
approach involves computing shallow parse sentence processed 
training testing hierarchical hmms manipulate level description sentence parse just processing sentence words directly 
evaluate approach extracting instances binary relations abstracts scientific articles 
experiments show approach results accurate models baseline approaches hmms 
example binary relation consider experiments subcellular localization relation represents location particular protein cell 
refer domains relation protein location 
refer instance relation tuple 
provides illustration extraction task 
top shows sentences bottom shows instance target relation subcellular localization extract second sentence 
tuple asserts protein ubc subcellular compartment called 
order learn models perform task training examples consisting passages text annotated tuples extracted 
earlier ray craven approach incorporates grammatical information single level hmms 
approach described extends earlier hierarchical hmms provide richer description information available sentence parse 
hierarchical hmms originally developed fine 
application models information extraction novel approach incorporates extensions models tailor task 
bikel 
developed approach named entity recognition uses hmms multi level representation similar hierarchical hmm 
models top level represents classes interest person name bottom level represents words sentence processed 
approach differs theirs key respects input representation sentences processed hierarchical ii models represent shallow phrase structure sentences iii focus learning extract relations entities iv null models represent sentences describe relations interest discriminative training procedure 
miller 
developed information extraction approach uses lexicalized probabilistic context free grammar simultaneously syntactic parsing semantic information extraction 
genre text consider quite different news story corpus available trained 
clear intriguing approach transfer task 
sentence representation previous hmms natural language tasks passages text processed represented sequences tokens 
hypothesis underlying incorporating sentence structure learned models provide better extraction accuracy 
approach syntactic parses sentences processed 
particular sundance system riloff obtain shallow parse sentence 
representation incorporate information provided sundance parser 
representation provides partially flattened level description sundance parse tree 
top level represents sentence sequence phrase segments 
lower level represents individual tokens part speech pos tags 
positive training examples segment contains word words belong domain target tuple segment words interest annotated corresponding domain 
refer annotations labels 
test instances contain labels labels predicted learned model 
shows sentence containing instance enzyme ubc localizes catalytic domain facing np segment det unk enzyme np segment protein unk protein ubc vp segment localizes pp segment prep np segment location art location location pp segment prep np segment art catalytic unk domain vp segment facing np segment art input representation sentence contains subcellular localization tuple sentence segmented typed phrases phrase segmented words typed part speech tags 
phrase types labels shown column 
word part speech tags labels shown column 
words sentence shown column 
note grouping words phrases 
labels protein location training sentences 
subcellular localization relation annotated segments 
sentence segmented typed phrases phrase segmented words typed part speech tags 
example second phrase segment noun phrase np segment contains protein name ubc protein label 
note types constants pre defined representation sundance parses labels defined domains particular relation trying extract 
hierarchical hmms information extraction schematic hierarchical hmms shown 
top shows positive model trained represent sentences contain instances target relation 
bottom shows null model trained represent sentences contain relation instances topic sentences 
coarse level hierarchical hmms represent sentences sequences phrases 
think top level hmm states emit phrases 
refer hmm phrase hmm states phrase states 
fine level phrase represented sequence words 
achieved embedding hmm phrase state 
refer embedded hmms word hmms states word states 
phrase states depicted rounded rectangles word states depicted ovals 
explain sentence hmm follow transition start state phrase state word hmm emit phrase sentence transition phrase state emit start protein np segment np segment location np segment start 
pp segment np segment pp segment 
schematic architecture hierarchical hmm subcellular localization relation 
top part shows positive model bottom part null model 
phrase states depicted rounded rectangles word states ovals 
types labels phrase states shown rectangles bottom right state 
labels shown bold states associated non empty label sets depicted bold borders 
labels word states abbreviated compactness 
phrase word hmm moves state phrase hmm 
note word states direct emissions 
phrases input representation phrase state hmm type may labels 
phrase state constrained emit phrases type agrees state type 
refer states labels associated extraction states predict test sentences tuples extracted 
architectures word hmms shown 
different architectures depending labels associated phrase state word hmm embedded 
word hmms phrase states empty label sets consist single emitting state self transition 
extraction states phrase hmm word hmms specialized architecture different states domain instances words come domain instances figures 
states word hmms emit words type part speech 
untyped contrast typed phrase states 
word states annotated label sets trained emit words identical label sets 
example word embedded model non extraction state start start start embedded model extraction state domain label embedded model extraction state domain labels location protein location protein location architectures word hmms relation 
bold text states denotes domain labels 
states implicit empty labels italicized text parentheses denotes position state emissions relative domain words 
shows structure embedded hmms phrase states labels phrase states label phrase states labels 
hmm shown explain phrase transition start state state emitting word transitioning location state emitting words location label transitioning state 
order phrase state emit phrase input representation sequences words shorter longer phrase require embedded word hmm transition state exactly emitted words phrase 
word hmms emit sequences words constitute phrases transitions phrase states occur phrase boundaries 
standard dynamic programming algorithms learning inference hmms forward backward viterbi rabiner need slightly modified hierarchical hmms 
particular need handle multiple levels input representation enforcing constraint word hmms emit sequences words constitute phrases ii support typed phrase states enforcing agreement state phrase types 
forward algorithm hierarchical hmms defined recurrence relationships shown table 
equations recurrence relation provide phrase level description algorithm equations provide word level description 
notice third equation describes linkage phrase level th phrase sentence th word phrase th start states phrase hmm th start states word hmm phrase state starting start state ing state probability emitting sequence phrases probability emitting sequence words starting start state state probability word state emits word probability transition phrase state phrase state word state probability transition word state table left side table shows forward algorithm recurrence relation hierarchical hmms 
right side table defines notation recurrence relation 
word level 
backward viterbi algorithms require similar modifications show due space limitations 
illustrated training instance hmms consists sequence words segmented phrases associated sequence labels 
test instance trained model accurately predict sequence labels observable part sentence words phrases 
discriminative training algorithm krogh tries find model maximize conditional likelihood labels observable part sentences parameters sequence words phrases th instance sequence labels instance 
training algorithm converge local maximum objective function 
initialize parameters models doing standard generative training 
apply krogh algorithm involves iterative updates hmm parameters 
avoid overfitting training accuracy held aside tuning set maximized 
order algorithm able adjust parameters positive model response negative instances vice versa join positive null models shown 
combined model includes positive start positive model null model architecture combined model 
positive null models refer models 
null models shown submodels shared start states 
model trained viterbi algorithm predict tuples test sentences 
extract tuple sentence viterbi path goes states labels domains relation 
example subcellular localization relation viterbi path sentence pass state protein label state location label 
process illustrated 
hierarchical hmms context features section describe extension hierarchical hmms previous section enables represent additional information structure sentences phrases 
refer extended hmms context hierarchical hmms 
hierarchical hmms earlier partition sentence disjoint observations word represents sequence overlapping observations observation consists window words centered partof speech tags words 
formally vector part speech tag word note share located different positions vectors 
shows vectors emitted phrase word hmm 
features represent previous words allows models capture regularities pairs triplets words 
instance potentially able learn word membrane part subcellular location plasma membrane membrane 
furthermore features represent part speech words models able learn regularities groups words part speech addition regularities individual words 
features mas mas 
np segment np segment location protein 
vp segment pp segment phrase words word label sets phrase label sets mas mas protein protein protein location location extracted tuples subcellular localization mas subcellular localization mas example procedure extracting tuples subcellular localization relation sentence fragment mas mas 
top shows path explains sentence fragment 
bold transitions states denote path 
dashed lines connect state words emits 
table shows label sets assigned phrases words sentence 
extracted tuples shown bottom 
null null art art null null start location generation phrase word hmm 
bold arcs represent path generates phrase 
vector observations emitted state shown rectangles model connected dotted arcs emitting state 
word emitted state equivalent hhmm shown boldface 
advantages representation especially realized dealing vocabulary word case part speech tags neighboring words may quite informative meaning vocabulary word 
example vocabulary adjective rarely protein proteins usually nouns 
number possible observations word state large possible vectors representing sequences words pos tags model probability observation assume features conditionally independent state 
assumption probability obser vation emitted state defined probability word state emitting observation th feature note features employed representation equation clearly conditionally independent 
consecutive words independent certainly part speech tag word independent word 
argue discriminative training algorithm krogh compensate part violation independence assumption 
empirical evaluation section experiments testing hypothesis hierarchical hmms able provide accurate models hmms incorporate grammatical information 
particular empirically compare types hierarchical hmms baseline hmms 
context hhmms hierarchical hmms context features described previous section 
hhmms hierarchical hmms context features 
phrase hmms single level hmms states typed phrase level hhmm emit phrases 
hmms introduced ray craven 
hierarchical hmms states phrase hmms embedded hmms emit words 
state single multinomial distribution represent emissions emitted phrase treated bag words 
pos hmms single level hmms states emit words typed part speech tags state emit words single pos 
token hmms single level hmms untyped states emit words 
evaluate hypotheses data sets assembled biomedical literature 
data sets composed abstracts gathered medline database national library medicine 
set contains instances subcellular localization relation 
composed positive negative sentences 
positive sentences contain total tuple instances 
number actual tuples tuples occur multiple times sentence multiple sentences 
second refer disorder association data set characterizes binary relation genes disorders 
contains positive negative sentences 
positive sentences represent instances tuples 
third refer protein interaction data set characterizes physical interactions pairs proteins 
composed positive negative sentences 
contains instances tuples 
fold cross validation measure accuracy approach 
processing sentences obtain parses sundance stem words porter stemmer porter 
map numbers special number token words occur training set vocab token 
discard punctuation 
preprocessing done test sentences exception words encountered training set mapped vocab token 
vocabulary emitting states models parameters smoothed estimates cestnik 
train models discriminative training procedure referred section krogh 
evaluate models construct precision recall graphs 
precision defined fraction correct tuple instances instances extracted model 
recall defined fraction correct tuple instances extracted model total number tuple instances exist data set 
tuple extracted sentence calculate confidence measure refers state combined model probability path viterbi algorithm total probability sequence calculated forward algorithm 
construct precision recall curves varying threshold confidences 
figures show precision recall curves data sets 
shows curves types earlier versions data sets previous ray craven 
various aspects data sets cleaned versions somewhat different 
data sets available www wisc edu craven 
precision context hhmms hhmms phrase hmms pos hmms token hmms recall precision vs recall types hmms subcellular localization data set 
precision context hhmms hhmms phrase hmms pos hmms token hmms recall precision vs recall types hmms disorder association data set 
hmms described section 
show error bars context hhmm precision values subcellular localization protein interaction data sets 
data sets hierarchical hmm models clearly superior precision recall curves baseline models 
nearly level recall hierarchical hmms exhibit higher precision baselines 
additionally hhmms achieve higher endpoint recall values 
results definitive disorder association data set 
pos hmms token hmms achieve precision levels comparable cases slightly better context hhmms 
clear winner data set context hhmms competitive 
comparing context hhmms ordinary hhmms see results superior precision recall curves data sets 
result demonstrates clearly value including context features hierarchical hmms type task 
summary empirical results support hypothesis ability hierarchical hmm approach capture grammatical information sentences results accurate learned models 
precision context hhmms hhmms phrase hmms pos hmms token hmms recall precision vs recall types hmms protein interaction data set 
approach learning models information extraction hierarchical hmms represent grammatical structure sentences processed 
employ shallow parser obtain parse trees sentences trees construct input representation hierarchical hmms approach builds previous hierarchical hmms incorporating grammatical knowledge information extraction models 
application hhmms novel required modify hhmm learning algorithms operate hierarchical input representation 
particular methods take account phrases states matching types phrase states emit complete phrases 
introduced novel modification hhmms observations feature vectors 
respect previous incorporating grammatical knowledge models main contribution approach takes advantage grammatical information represented multiple scales 
appealing property approach generalizes additional levels description input text 
evaluated approach context learning models extract instances biomedical relations abstracts scientific articles 
experiments demonstrate incorporating hierarchical representation grammatical structure improves extraction accuracy hidden markov models 
acknowledgments research supported part nih lm nsf iis university wisconsin medical school howard hughes medical institute research resources program medical schools 
bikel bikel schwartz weischedel 
algorithm learns name 
machine learning 
cestnik cestnik 
estimating probabilities crucial task machine learning 
proc 
th european conf 
artificial intelligence pages stockholm sweden 
pitman 
fine fine singer tishby 
hierarchical hidden markov model analysis applications 
machine learning 
freitag mccallum freitag mccallum 
information extraction hmm structures learned stochastic optimization 
proc 
th national conf 
artificial intelligence 
aaai press 
hirschman hirschman park tsujii wong wu 
accomplishments challenges literature data mining biology 
bioinformatics 
krogh krogh 
hidden markov models labeled sequences 
proc 
th international conf 
pattern recognition pages jerusalem israel 
ieee computer society press 
lafferty lafferty mccallum pereira 
conditional random fields probabilistic models segmenting labeling sequence data 
proc 
th international conf 
machine learning pages ma 
morgan kaufmann 
leek leek 
information extraction hidden markov models 
thesis dept computer science engineering univ california san diego 
mccallum mccallum freitag pereira 
maximum entropy markov models information extraction segmentation 
proc 
th international conf 
machine learning pages stanford ca 
morgan kaufmann 
miller miller fox ramshaw weischedel 
novel statistical parsing extract information text 
proc 
th applied natural language processing conf pages seattle wa 
association computational linguistics 
national library medicine national library medicine 
medline database 
www ncbi nlm nih gov pubmed 
porter porter 
algorithm suffix stripping 
program 
rabiner rabiner 
tutorial hidden markov models selected applications speech recognition 
proc 
ieee 
ray craven ray craven 
representing sentence structure hidden markov models information extraction 
proc 
th international joint conf 
artificial intelligence pages seattle wa 
morgan kaufmann 
riloff riloff 
sundance sentence analyzer 
www cs utah edu projects nlp 
