combining active learning semi supervised learning gaussian fields harmonic functions zhu cs cmu edu john lafferty lafferty cs cmu edu zoubin ghahramani school computer science carnegie mellon university pittsburgh pa usa zoubin gatsby ucl ac uk gatsby computational neuroscience unit university college london london wc ar uk active semi supervised learning important techniques labeled data scarce 
combine gaussian random field model 
labeled unlabeled data represented vertices weighted graph edge weights encoding similarity instances 
semi supervised learning problem formulated terms gaussian random field graph mean characterized terms harmonic functions 
active learning performed top semisupervised learning scheme greedily selecting queries unlabeled data minimize estimated expected classification error risk case gaussian fields risk efficiently computed matrix methods 
experimental results synthetic data handwritten digit recognition text classification tasks 
active learning scheme requires smaller number queries achieve high accuracy compared random query selection 

semi supervised learning targets common situation labeled data scarce unlabeled data abundant 
suitable assumptions uses unlabeled data help supervised learning tasks 
various semi supervised learning methods proposed show promising results seeger gives survey 
methods typically assume labeled data set fixed 
practice may sense utilize active learning conjunction semi supervised learning 
allow learning algorithm pick set unlabeled instances labeled domain expert augment labeled data set 
words label instances semisupervised learning may attractive learning algorithm tell instances label selecting randomly 
limit range query selection unlabeled data set practice known active learning selective sampling 
great deal research active learning 
example tong koller select queries minimize version space size support vector machines cohn 
minimize variance component estimated generalization error freund 
employ committee classifiers query point committee members disagree 
active learning methods take advantage large amount unlabeled data queries selected 
exceptions include mccallum nigam em unlabeled data integrated active learning muslea 
semi supervised learning method training 
addition body machine learning community large literature closely related topic experimental design statistics give survey experimental design bayesian perspective 
zhu 
introduced semi supervised learning framework gaussian random fields harmonic functions 
demonstrate framework allows combination active learning semi supervised learning 
brief framework allows efficiently estimate expected generalization error querying point leads better query selection criterion naively selecting point maximum label ambiguity 
queries selected added labeled data set classifier trained labeled remaining unlabeled data 
results synthetic data text classifica proceedings icml workshop continuum labeled unlabeled data washington dc 
tion image classification tasks indicate combination techniques result highly effective learning schemes 

gaussian random fields harmonic energy minimizing functions briefly describing semi supervised learning framework zhu 

labeled points unlabeled points usually 
denote labeled unlabeled set total number points 
assume labels binary 
assume connected graph nodes corresponding data points nodes labeled corresponding edges represented weight matrix 
example radial basis function rbf nearby points euclidean space assigned large edge weights 
weightings possible course may appropriate discrete symbolic 
purposes matrix fully specifies data manifold structure 
note method learning scale parameter proposed zhu 
semi supervised algorithm relaxation requirement labels binary resulting simple tractable family algorithms 
allow continuous labels unlabeled nodes 
labels labeled data constrained 
denote constraint 
want unlabeled points nearby graph similar labels define energy low energy corresponds slowly varying function graph 
define diagonal matrix entries weighted degrees nodes 
combinatorial laplacian matrix rewrite energy function matrix form 
consider gaussian random field inverse temperature parameter partition function 
gaussian random field differs standard discrete markov random field field configurations continuous state space 
gaussian field joint probability distribution unlabeled nodes gaussian covariance matrix submatrix corresponding unlabeled data 
minimum energy function arg min gaussian field harmonic satisfies unlabeled data points equal labeled data points 
harmonic property means value unlabeled node average neighboring nodes consistent prior notion smoothness respect graph 
maximum principle harmonic functions doyle snell unique 
furthermore satisfies connected labeled nodes classes boundary usual case takes extremum 
definition mode gaussian random field joint distribution gaussian mean field 
harmonic energy minimizing function computed matrix methods 
partition laplacian matrix blocks labeled unlabeled nodes denotes mean values unlabeled data points 
solution hard show gaussian field conditioned labeled data multivariate normal carry classification gaussian field note harmonic energy minimizing function mean field 
bayes classification rule label node class case label node class 
harmonic function nice interpretations random walk view particularly relevant 
define transition matrix consider random walk graph particle 
starting unlabeled node moves node probability step 
walk continues particle reaches labeled node 
probability particle starting node reaches labeled node label 
labeled data absorbing boundary random walk 
semi supervised learning framework zhu 

active learning propose perform active learning gaussian random field model greedily selecting queries unlabeled data minimize risk harmonic energy minimization function 
risk estimated generalization error bayes classifier efficiently computed matrix methods 
define true risk bayes classifier harmonic function sgn sgn bayes decision rule slight abuse notation sgn sgn 
unknown true label distribution node labeled data 
computable 
order proceed necessary assumptions 
assuming estimate unknown distribution mean gaussian field model intuitively recalling probability reaching random walk graph assumption approximate distribution biased coin node probability heads 
assumption compute estimated risk sgn sgn perform active learning query unlabeled node receive answer 
adding point training set retraining gaussian field mean function course change 
denote new harmonic function estimated risk change know answer receive assume answer approximated expected estimated risk querying node input weight matrix labeled data required compute harmonic find best query query point receive answer add remove output classifier 

active learning algorithm active learning criterion greedy procedure choosing query minimizes expected estimated risk arg min carry procedure need compute harmonic function adding current labeled training set 
retraining problem computationally intensive general 
gaussian fields harmonic functions efficient way retrain 
recall harmonic function solution solution fix value node 
finding conditional distribution unlabeled nodes value 
noting multivariate normal distribution standard result derivation appendix gives conditional fix th column inverse laplacian unlabeled data th diagonal element matrix 
computed compute harmonic function 
linear computation carried efficiently 
summarize active learning algorithm shown 
time complexity find best query final word computational efficiency note adding query answer iteration need compute inverse laplacian unlabeled data row column removed 
naively inverse efficient algorithms compute derivation appendix 
experiments shows top left synthetic dataset labeled data marked unlabeled point risk active learning random query uncertain query labeled set size accuracy active learning random query uncertain query labeled set size 
synthetic data experiments 
top left synthetic data top right synthetic data 
bottom left risk synthetic data bottom right classification accuracy synthetic data 
standard errors shown dotted lines 
center cluster unlabeled points 
shifted right 
graph fully connected weight euclidean distance 
configuration uncertainty points certain labels 
risk minimization criterion picks upper center point marked star query 
fact estimated risk shows active learning algorithm simply picking uncertain point query thinks globally shows top right synthetic dataset true labels points form chess board pattern 
expect active learning discover pattern query small number representatives cluster 
hand expect larger number queries queries randomly selected 
fully connected graph weight perform random trials 
trial randomly select positive example negative example initial training set 
run active learning compare baselines random query randomly selecting query uncertain query selecting uncertain instance closest 
case run iterations queries 
iteration plot estimated risk selected query lower left classification accuracy lower right 
error bars standard deviation averaged random trials 
expected active learning reduce risk quickly random queries uncertain queries 
fact active learning queries plus initial random points learns correct concept nearly optimal clusters 
looking queries find active learning selects central points clusters 
report results document categorization experiments newsgroups dataset evaluate binary classification tasks rec sport baseball documents vs rec sport hockey comp sys ibm pc hardware vs comp sys mac hardware talk religion misc vs alt atheism 
represent easy moderate hard problems respectively 
document minimally processed tf idf vector applying header removal frequency cutoff stemming stopword list 
documents connected edge nearest neighbors nearest neighbors measured cosine similarity weight function edges perform trials randomly pick www ai mit edu people newsgroups risk accuracy active learning random query labeled set size active learning random query svm labeled set size risk accuracy active learning random query labeled set size active learning random query svm labeled set size risk accuracy active learning random query labeled set size active learning random query svm labeled set size 
risk top classification accuracy bottom newsgroups compared random queries baseball vs hockey left pc vs mac center religion vs atheism right 
initial training examples start trial 
rest documents treated unlabeled data 
trial answer queries 
compare active learning baselines 
compares random queries 
active learning reduces risk faster achieves high classification accuracy rapidly random queries 
easy datasets handful queries necessary active learning achieve accuracy 
observe active learning tends query small set documents different trials 
trained svm classifier random queries cosine similarity kernel best setting different kernels wide range values 
note svm utilize unlabeled data datasets semi supervised method outperforms svm 

compares uncertain queries 
uncertain query selects instance value closest query 
svm uncertain selects instance margin closest closest decision boundary 
svm utilize unlabeled data 
harmonic function classification worse uncertain queries random queries svm improves uncertain queries 
cases proposed active learning scheme clearly outperforms baselines newsgroups datasets 
evaluate active learning handwritten digits dataset originally cedar buffalo binary digits database hull 
digits preprocessed reduce size image grid sampling gaussian smoothing pixel values ranging le cun 
scaled averaging pixel bins 
image represented dimensional vector 
consider binary problem classifying digits vs images class 
create graph images edge images iff nearest neighbors euclidean distance vice versa 
weights edges pixel wise euclidean distance images 
trials randomly pick example class form initial training set 
compare active learning random queries uncertain queries iterations 
left shows risk 
active learning risk decreases faster baselines 
center shows classification accuracy active learning seen outperform baselines 
handful examples needed active learning reach high accuracy 
svm uses kernel best polynomial kernels order wide range values 
observe certain risk accuracy active learning uncertain query labeled set size active learning uncertain query svm uncertain labeled set size risk accuracy active learning uncertain query labeled set size active learning uncertain query svm uncertain labeled set size risk accuracy active learning uncertain query labeled set size active learning uncertain query svm uncertain labeled set size 
risk top classification accuracy bottom newsgroups compared uncertain queries baseball vs hockey left pc vs mac center religion vs atheism right 
images active learning frequently queries different trials 
right shows frequently queried images believe images representative variations dataset 
evaluate difficult binary problem classifying odd digits vs digits 
group classes 
images digit class 
difficult dataset target concept artificial hand dataset resembles synthetic data class internal clusters 
experimental setup run iterations 
shows results 
active learning superior baselines 
see odd vs harder concept takes active learning queries approximately learn 
digits shown frequently queried instances iterations trials 
instances dataset largest slowest 
naive matlab implementation calculations take roughly seconds iteration ghz linux machine 
contrast vs dataset requires seconds iteration newsgroups datasets take seconds iteration 
note train svm active queries chosen harmonic function risk minimization procedure accuracy worse proposed active learning method worse svm random queries 

summary proposed approach active learning tightly coupled semi supervised learning gaussian fields harmonic functions 
algorithm selects queries minimize approximation expected generalization error 
experiments text categorization handwritten digit recognition indicate active learning algorithm highly effective 


bayesian experimental design review 
statistical science 
cohn ghahramani jordan 

active learning statistical models 
journal artificial intelligence research 
doyle snell 

random walks electric networks 
mathematical assoc 
america 
freund seung shamir tishby 

selective sampling query committee algorithm 
machine learning 
hull 

database handwritten text recog nition research 
ieee transactions pattern analysis machine intelligence 
le cun boser denker henderson howard howard jackel 

handwritten digit recognition back propagation network 
advances neural information processing systems 
mccallum nigam 

employing em pool active learning text classification 
proceedings icml th international conference machine learning pp 

madison morgan kaufmann publishers san francisco 
muslea minton knoblock 

active semi supervised learning robust multi view learning 
proceedings icml th international conference machine learning pp 

seeger 

learning labeled unlabeled data technical report 
university edinburgh 
tong koller 

support vector machine active learning applications text classification 
proceedings icml th international conference machine learning pp 

stanford morgan kaufmann publishers san francisco 
zhu ghahramani lafferty 

semisupervised learning gaussian fields harmonic functions 
icml th international conference machine learning 
appendix harmonic function knowing label construct graph usual 
random walk solution 
unlabeled nodes 
ask question solution add node value graph connect new node unlabeled node weight new node attached node 
usage nodes useful handling noisy labels put observed labels infer hidden true labels nodes attached 
note effectively assign label node 
labeled node augmented graph column vector length position 
note matrix inversion lemma obtain shorthand green function th row th column element square matrix th column 
calculation gives unlabeled node original solution th column vector 
want pin unlabeled node value obtain appendix inverse matrix row column removed non singular matrix 
fast algorithm compute matrix obtained removing th row column 
perm matrix created moving th row front st row th column front st column 
perm note perm need consider special case removing row column matrix 
write transform block diagonal form steps 
interested step 
matrix inversion lemma sherman morrison woodbury formula applying matrix inversion lemma block diagonal know risk risk active learning random query labeled set size active learning uncertain query labeled set size accuracy accuracy active learning random query svm labeled set size active learning uncertain query svm uncertain labeled set size 
handwritten digits vs compared random queries top uncertain queries bottom risk left classification accuracy center frequently queried images right 
risk risk active learning random query labeled set size active learning uncertain query labeled set size accuracy accuracy active learning random query svm labeled set size active learning uncertain query svm uncertain labeled set size 
handwritten digits odd vs compared random queries top uncertain queries bottom risk left classification accuracy center frequently queried images right 
