topic segmentation algorithms applications jeffrey reynar dissertation computer information science faculties university pennsylvania partial fulfillment requirements degree doctor philosophy mitchell marcus adviser mark steedman graduate group chair copyright jeffrey reynar memory mom dad iii research described dissertation conducted isolation 
impossible way contributed equally impossible individuals contributions 
am indebted technical insights encouragement friendship members research community university pennsylvania privilege working summer institute linguistics ibm watson research center fuji xerox 
want advisor mitch marcus providing ideas inspiring helping refine develop 
mitch allowing freedom pursue muc lexis nexis projects spend summers working industry 
members thesis committee julia hirschberg aravind joshi mark liberman lyle ungar providing suggestions improved quality 
additional aravind mark mitch serving committee 
benefited conversations natural language processing tools developed members university pennsylvania muc team breck baldwin mike collins jason eisner adwait ratnaparkhi joseph anoop sarkar srinivas 
adwait deserves special permitting maximum entropy modeling tools 
doran beth ann hockey kim dan melamed tom morton michael niv martha palmer mike schultz iv matthew stone david yarowsky helpful discussions feedback 
time penn enjoyable friendship people 
greatly profited discussions researchers outside penn including ken church lance ramshaw salim roukos sibun larry mark wasson 
kathy mccoy de facto undergraduate advisor introducing computational linguistics 
betsy norman amy dunn regularly squeezing mitch schedule short notice conversation waiting see 
grateful mike help getting administrative hurdles faced graduate student staff cis business office helping navigate complexities university pennsylvania payroll billing systems 
owe gratitude parents encouraged pursue dreams strive best wife angela picking left 
topic segmentation algorithms applications jeffrey reynar supervisor mitchell marcus documents subject majority natural language processing algorithms information retrieval techniques implicitly assume document just topic 
described clues mark shifts new topics algorithms identifying topic boundaries uses boundaries identified 
number topic shift indicators proposed literature 
review features suggest new ones test implemented topic segmentation algorithms 
hints topic boundaries include repetitions character sequences patterns word word gram repetition word frequency presence cue words phrases synonyms 
algorithms cues singly combination identify topic shifts kinds documents 
algorithm tracks compression performance indicator topic shift self similarity topic segments greater segment similarity 
technique relies word repetition places boundaries minimizing word repetitions segment boundaries 
third method compares performance language model knowledge contents preceding sentences determine topic shift occurred 
output vi algorithm statistical model incorporates synonymy bigram repetition features topic segmentation 
benchmark algorithms compare algorithms literature concatenations documents perform evaluation techniques collection news broadcasts transcribed annotators speech recognition system 
test effectiveness algorithms identifying chapter boundaries works literature story boundaries spanish news broadcasts 
suggest ways improve information retrieval language modeling various natural language processing algorithms exploiting topic segmentation 
vii contents iv vi motivation 
segmentation cues 
noisy channel model 
goals 
topic segmentation discourse segmentation 
outline 
theories discourse structure halliday hasan 
grosz sidner 
ko 
structuring clues currently uncomputable features 
grimes 

summary 
currently computable features 
cue words phrases 
viii uses 
word repetition 
word gram repetition 
word frequency 
synonymy 
named entities 
pronoun usage 
character gram repetition 
additional indicators 
previous text segmentation methods word repetition 
morris hirst 
hearst 
richmond smith amitay 
yaari 
approaches 
features 
beeferman berger lafferty 
phillips 
youmans 
kozima 
ponte croft 
discussion 
algorithms desiderata 
text normalization 
tokenization 
conversion lowercase 
lemmatization 
ix identifying closed class words 
document concatenations 
performance algorithms literature 
texttiling 
vocabulary management profiles 
compression algorithm 
lempel ziv 
complicating factors 
evaluation 
optimization algorithm 

text segmentation 
algorithmic boundary identification 
minimization versus maximization 
formal description algorithm 
similarity vector space model 
evaluation 
word frequency algorithm 
model 
word frequency algorithm specification 
impact individual words 
effect perturbing parameters 
parameter estimation 
evaluation 
training corpus required 
maximum entropy model 
performance 
evaluation performance measures 
comparison human annotation 
broadcast news 
topic detection tracking corpus 
performance tdt corpus 
adapting algorithm wf tdt data 
inducing domain cues 
value different indicators 
recovering authorial structure 

applications information retrieval 
previous 
trec spoken document retrieval task 
language modeling 
text segmentation language modeling 
topic dependent language modeling 
improving nlp algorithms 
potential applications 
summarization 
hypertext 
information extraction 
topic detection tracking 
automated essay grading 
directions 
bibliography xi list tables cue words hirschberg litman 
domain cues identified hand documents hub broadcast news corpus 
list corporate designators named entity recognition 
types clues various text structuring algorithms 
features text structuring algorithms 
closed class words letter 
accuracy baseline segmentation algorithms pairs concatenated wsj articles 
rows average results iterations random selection algorithm 
texttiling applied document concatenations 
texttiling label boundaries concatenations short 
results implementation algorithms vector space model applied documents optional normalization 
algorithm tested concatenations pairs wall street journal articles 
row gray presents results settings texttiling 
results implementation algorithms vector space model applied normalized data 
algorithm tested concatenations pairs wall street journal articles 
row gray presents results settings texttiling 
xii results variants youmans technique applied data consisting concatenations pairs wall street journal articles 
sample lz compression 
results compression algorithm concatenations pairs wall street journal articles 
sample word repetition matrix 
application optimization algorithms topic segmentation sample text 
results variants optimization algorithm tested concatenations pairs wall street journal articles 
reduce words roots ignore frequent words 
results variants optimization technique tested concatenations pairs wall street journal articles 
data preprocessed reduce words roots ignore frequent words 
example distribution bursty words document 
conditional probabilities seeing particular number occurrences word block certain number observed block 
number occurrences blocks combined 
normalization constant discussed text 
effect increasing parameter values ratio 
range parameters model estimated wall street journal training corpus turing smoothing applied 
results word frequency algorithm guess location boundary 
data consisted concatenations pairs wall street journal articles 
results word frequency algorithm parameters unknown words 
data consisted concatenations pairs wall street journal articles 
pronouns indicators topic boundaries 
news programs hub broadcast news corpus 
xiii statistics hub corpus annotation 
reliability annotation hub corpus 
performance various algorithms files hub broadcast news corpus 
performance algorithms wf test portion tdt corpus 
best cue phrases induced tdt training corpus 
performance algorithm wf modified select best boundary neighboring hypothesized boundaries trained cues induced tdt training corpus 
performance algorithm trained training portion tdt corpus indicators text structure listed row 
accuracy algorithm wf works literature 
columns labeled acc 
indicate accuracy labeled prob 
show performance probabilistic metric beeferman 
ir performance spoken document retrieval corpus stemming smart built stemmer 
results language modeling experiment conducted tdt corpus 
number candidate antecedents singular pronouns referred people randomly selected broadcast news transcripts 
pronouns examined 
xiv list figures noisy channel model 
changes attentional intentional linguistic structure processing sample text 
ko text types 
example domain cue marking boundary topic segments fragment transcript episode national public radio show things considered 
phrase bold domain specific cue phrase form person 
gap separates different news stories 
features named entity recognizer determining label word apple example 
example large number uses words marking new segment 
fragment transcript episode national public radio show things considered 
words bold time 
gap news items 
example large number uses open class words marking new segment 
fragment transcript episode national public radio show things considered 
words bold time 
gap separates news stories 
example showing degree word repetition topic segments 
data national public radio program things considered 
xv example demonstrating quantity content word repetition topic segments 
text national public radio program things considered 
example indicating usefulness tracking repetition word bigrams topic segmentation 
data national public radio program things considered 
example showing usefulness tracking repetition content word bigrams topic segmentation 
text transcribed national public radio program things considered 
example indicating utility tracking synonyms identifying topic boundaries 
data national public radio program things considered 
example transcript indicating usefulness coreference topic segmentation 
text transcribed national public radio program things considered 
example showing usefulness limited coreference named entities 
text transcript national public radio program things considered 
example text region pronoun existence preceding segment boundary 
fragment transcript episode national public radio show things considered 
crucial pronoun shown bold 
topic boundary sentence said 
example showing utility tracking character gram repetition identifying topic boundaries 
transcript national public radio program things considered 
unsmoothed depth score graph concatenated wall street journal articles 
smoothed depth score graph concatenated wall street journal articles 
xvi sample dendogram type produced yaari hierarchical clustering algorithm 
axis sentence number axis level merge 
type token plot brown corpus file cd 
vocabulary management profile brown corpus file cd 
dotplot matrix table shows word repetitions phrase 
dotplot concatenated wall street journal articles 
graphical illustration working optimization algorithm 
graphical illustration working optimization algorithm boundary identified 
outside density plot concatenated wall street journal articles 
ways handle ignored words 
left dotplot ignored words may participate matches 
dotplot right ignored words eliminated 
performance best performing version algorithm 
perfect score exact matches distance graph 
example hub corpus showing annotation topic segments produced ldc 
vertical whitespace indicate changes speaker background recording condition 
precision recall curve algorithms wf tested hub news broadcast data 
lone point represents baseline performance 
precision recall curve algorithm wf data hub corpus words corpus treated unknown 
performance original wf algorithm shown comparison 
precision recall curve hub news broadcast data 
performance shown algorithm wf speech recognized data manual transcriptions data 
baseline performance shown 
xvii precision recall curve spanish news broadcast data hub corpus 
diagram hmms mittendorf schauble passage retrieval 
transition probabilities set 
example segment identified returned user ir system 
whitespace indicates additional boundary placed annotators 
sample completed templates information extraction task 
example text indicating topic segments useful information extraction 
management template filled sentence bold 
xviii chapter twentieth century people contend increasing quantity information various forms numerous sources 
radio television internet consumer electronic devices people daily information subjects ways 
information contained call documents lack better term 
document mean repository snippet natural language medium accessed frequently computer created 
recorded radio broadcasts television programs books papers stored computer handwritten notes scanned reports web pages voice mails faxes examples documents 
conversations example considered documents definition lack persistence 
number documents existence today staggering 
database service lexis nexis indexes collection grows week lexis nexis 
number pages world wide web www difficult count users popular search engine alta vista search order terabyte data alta vista index entire web digital equipment 
library congress collection numbers items 
items photographs documents definition undoubtedly largest collections documents assembled library congress 
documents generated far greater rate today 
despite claims impending office earlier lancaster increased pace due exclusively growth broadcast industries rise internet 
rate production documents growing evidenced fact world wide production doubled food agriculture organization united nations 
fact producing fewer documents today information technology producing easier high quality printers inexpensive word processing desktop publishing packages ubiquitous 
proliferation documents due primarily increased access information technology cease larger burden average consumer information technology access desired piece information catches technology create new documents fox 
stem tide documents time soon need better ways cope store index access convert form easily form forth 
needs partly responsible interest research areas seemingly diverse digital libraries optical character recognition ocr speech recognition information extraction information retrieval ir 
common thread running lines research access needed information 
obviously purpose information retrieval information extraction 
goal information retrieval permit document databases searched arbitrary ways 
information extraction focuses identifying predetermined types information new documents entered document collection 
digital libraries provide enhanced access collections making electronic documents accessible distance theory permitting documents stored single repository 
allow users search ways traditional libraries 
ocr speech recognition enhance access converting documents form difficult manipulate today access tools form text medium required natural language processing information retrieval information extraction systems 
motivation dissertation techniques improving access information improving underlying language technology helps provide access dividing lengthy documents topically coherent sections 
technology necessary people search information interested finding conveniently sized packages 
clearly unhelpful learn answer question local library matter internet dow jones database 
marginally useful discover answer lies section books call numbers range average community library looking titles reading book require time 
finding needed information particular book sufficiently specific information needed quickly event time available reading immediately relevant material book beneficial 
generally best possible solution request information give requester answer 
times strictly speaking answer 
queries posed ir systems open ended technology advanced today best response consist set documents 
consider query shot john kennedy 
answer lee harvey oswald suffice cases alternate theories abound 
ideally user ir system aware 
technology advanced provide simple answer lee harvey oswald queries 
answer particular query may database presidents information retrieval systems search large text databases extracting answers arbitrary natural language questions text open research area 
compromise state technology limit size documents information seekers just large satisfy information needs larger 
expected read warren commission report entirety learn oswald certainly 
potential approach satisfying demands information synthesize material selected passages document multiple documents creating new documents demand requested information summary form 
currently variety done area touch topic briefly 
see instance aone 
important constraints just large mean responding demand information 
crucial returned material interpretable 
unresolved pronominal sufficient context allow ambiguous words phrases understood information respond original query 
constraints unimportant arbitrary passages possibly mid sentence users 
ir systems index document fragments consisting contiguous sequences words return users response queries 
place storage burden systems index large collections ir performance improve technique 
see callan kaszkiel zobel 
modest improvement approach limit indexed segments sentence boundaries 
response user uninterpretable sentence fragment 
reduce amount storage space required ir system 
problems remain 
pronouns may resolvable may context understand meaning text fragment 
better solution divide documents sections section limited extent particular topic boundaries topic segments aligned sentence boundaries clarity 
result minimal increase storage space required ir system solve problems stemming lack context 
ideally pronominal entities outside section resolved sections intelligible possible 
difficulty making sections stand varies greatly depends type document partitioned 
types documents created partitions place 
newspapers divided articles article labeled title 
readers choose read read articles titles 
documents labels segment boundaries transcripts interviews telephone conversations spontaneous communications lie opposite continuum 
additional partitioning labeled documents necessary level subdivision rarely fine 
short newspaper articles may restricted single topic feature articles articles magazines journals may range number related topics 
documents media may segments marked implicitly recovering boundaries segments may difficult 
difficulty pertains segmenting television radio news broadcasts produced independence stories mind 
obvious viewers news programs boundaries stories occur 
challenging find point story ends begins transcript 
segmentation cues dissertation focus segmenting documents cues text transcribed speech 
additional sources cues segmenting video broadcasts applies equally audio data 
video portion programs rich source information transitions 
example black frames appear stories shifts location reports back anchor studio result dramatic alterations image content caused camera movement shifting focus attention anchor 
cues divide video documents scenes yeo liu 
second source cues speech stream 
linguistic cues speech intonation pauses generally transcribed 
hirschberg grosz studied relationship intonation discourse structure correlations intonational features annotator labeling discourse structure hirschberg grosz grosz hirschberg 
hirschberg nakatani showed annotators consistently segment discourse speech conjunction transcripts text annotators reliably segment spontaneous planned speech hirschberg nakatani 
litman studied consistency correlation linguistic cues segmentation 
proposed segmentation algorithms cue words referential noun phrases presence pauses 
performance algorithms encouraging human performance litman 
ultimately textual spoken video cues combined incorporated video demand system informedia digital video library permits video documents searched browsed ways information retrieval systems textual documents christel hauptmann witbrock 
leave combination textual cues sources 
noisy channel model assume canonical topic segmentation treat creation documents explicitly marked topic boundaries instance noisy channel model cover thomas 
model resulted advances statistical techniques speech recognition bahl natural language processing tasks brown brown 
production news broadcast example noisy channel model applies 
producer news program begins collection independent stories 
set stories intends convey corresponds original message 
content stories passes encoder journalists translate content stories words 
stage assuming story written independently boundaries stories clear 
consider written stories ultimately form news broadcast encoded message 
message passes noisy channel stochastic process blurs removes boundaries stories 
simple explanation production process 
goal making broadcast flow smoothly motivates journalists relate stories provide natural transitions 
story began set phrase story detecting boundaries stories trivial phrase news broadcasts currently 
encoder message channel noisy original corrupted message encoded message noisy channel model 
connections transitions obscure boundaries stories 
place broadcast final form corresponds corrupted message 
broadcast form topic boundaries may marked phrases coming intonational cues changes image content 
fact boundaries may detectable determining meaning program markings longer explicit stories passed noisy channel 
example dealt boundaries stories 
similar process obscure topic boundaries individual stories news broadcasts documents types 
fact authors focus making boundaries subtle readability partly function smoothness transitions topics 
said participants conversations focus structuring primarily monologues 
goals describe methods recognizing boundaries topic segments documents 
best methods applicable wide variety document types various media 
contents documents converted text techniques 
topic boundary annotated corpora available 
result earliest results simulations concatenated documents 
rest dissertation demonstrate effectiveness methods newspaper articles transcribed speech recognized television radio broadcasts works literature 
show methods apply languages english transcripts spanish television radio broadcasts 
evaluating segmentations produced text structuring algorithms presents problems 
discuss merits various performance measures order demonstrate utility algorithms describe number applications benefit topic segmented documents 
example performance various natural language processing tasks coreference resolution word sense disambiguation improved 
describe information retrieval experiments segmentations produced human annotators identified algorithmically 
topic segmentation discourse segmentation algorithms propose text segmentation intended useful language engineering applications 
result identify boundaries demarcate units text appropriately sized particular tasks 
segments boundaries delimit range length sentences paragraphs 
theories discourse structure grosz sidner grosz mann thompson focus relating smaller units discourse utterances 
result empirical discourse segmentation focussed primarily identifying relations units hirschberg grosz litman 
research regarding relationships utterances numerous implications language processing tasks outside scope 
coarse grained segmentation pursuing may subset detailed analysis goal discourse segmentation believe techniques propose applicable discourse segmentation 
features topic segmentation resolving power finer scale 
variety freely interchange phrases text structuring text segmentation topic segmentation 
reserve discourse segmentation predominantly hierarchical analyses finer grained produced algorithms 
outline chapter briefly summarize theories discourse structure relate text structuring 
knowledge theories necessary understand algorithms review chapter influenced design algorithms describe chapter 
discuss features useful identifying topic structure chapter 
clues previous approaches text structuring novel 
chapter reviews previous computational approaches chapter outline algorithms 
chapter describe methods evaluating text structuring techniques measure performance algorithms 
chapter describe applications benefit text structuring demonstrate utility text structuring techniques subset applications 
summarize discuss findings outline chapter 
chapter theories discourse structure large body literature structure discourse 
addresses wide range issues including large units relationships described theory discourse structure harris longacre stark 
nature relationships individual phrases written texts mann thompson 
relationship coherence attention selection referring expressions grosz sidner grosz 
discourse structured hierarchically linearly ko webber sibun 
relationships utterances multi party discourse hobbs walker whittaker 
large scale shifts narrative texts identified grimes youmans 
review entire discourse structure literature summarize articles pertinent 
algorithms chapter structure text theory neutral way 
depend implicitly particular answers questions 
algorithms propose divide texts sections range sentences paragraphs length 
literature discourse structure focuses relations utterances tangentially related analyses involving larger fragments discourse 
ample evidence suggesting discourse hierarchically structured grosz sidner webber algorithms propose partition text linearly 
segments methods identify contain utterances inter relations may best described hierarchical structure 
identifying structure provenance discourse segmentation research 
may segments algorithms locate fit naturally hierarchical structure 
segmenting documents linearly facilitates comparison algorithms literature topic segmentation permits evaluation available linearly segmented corpora 
corpora evaluate text segmentation algorithms contain brief passages dialogue 
news broadcasts hosted anchors occasionally discuss news 
majority documents monologue 
result survey dialogue reserve application techniques typical conversational text 
review halliday hasan theory types relations textual elements provide coherence document 
influenced design text segmentation algorithms including 
review grosz sidner theory discourse structure understanding algorithms review requires knowledge 
summarize ko theory text organization underpinned hearst dividing long documents subtopic sections 
halliday hasan book cohesion english halliday hassan describe texture property possessed text arbitrary collection sentences 
readers frequently tell series sentences exhibits texture 
sentences example exhibit example halliday hasan 
wash core cooking apples 
put dish 
wash core cooking apples 
prices computers drop regularly 
cohesion elements discourse contributes texture 
cohesion element text best interpreted light previous frequently element text 
halliday hasan identify cohesive relations contribute texture document 
pointers 
repeat phrase text writer speaker may pointer entity selected phrase 
halliday hasan distinguish main types 
entities world discourse portions text 
word example example 
john likes apples loves pears 
jolly fellow 
say 
substitution substitution similar differ substitution occurs prior semantic interpretation occurs interpretation 
substitute acts merely pointer region text refers entity world discourse refers directly entity mediation original referring phrase 
example substitutes phrase apples 
apples 
everybody 
ellipsis ellipsis similar substitution 
viewed substitution zero 
example bought replaced null phrase mary flowers 
john bought mary flowers 
conjunction conjunction difficult define previous relations 
holds elements text ordered temporally causes describe contrast elaborates 
examples cohesion english demonstrate relations 
sentences read immediately sentence example 
day climbed stopping 
fell sat rest 
temporal order night time valley far 
causation hardly aware tired 
contrast time met 
elaboration lexical cohesion lexical cohesion holds tokens text type semantically related particular way 
semantic relations constitute lexical cohesion 

identity occurs particular entity previously referred discourse referred 
john saw dog 
dog retriever 
example refers particular dog example refers dog 

identity occurs entire class entity previously referred discourse belongs 
john saw small retriever 
usually large 
example refers particular member set dogs identified example refers entire class 

means superordinate occurs superclass class previously mentioned entity belongs 
john saw retriever 
dogs favorite animals 
example refers retriever type dog example refers dogs general 

systematic semantic relation holds word group words clearly definable relationship previously word phrase 
example refer members set 
john likes 
doesn 
example refers example mentions subsets species dogs 
case relationship classified membership particular class 

nonsystematic semantic relation holds words phrases discourse pertain particular theme topic nature relationship difficult specify 
recognizing category computational system difficult recognizing categories 
john spent afternoon studying room 
loves attending college 
semantic connection exists word example college example hard classify relations preponderance knowledge source way synonymy relations identified thesaurus 
halliday hasan categories overlap degree 
example difficult distinguish instances substitution 
substitution subtly different relates words text semantic relation requires substituted phrase role phrase substitutes 
case 
halliday hasan acknowledge instances category applies equally 
halliday hasan explain texts frequently exhibit varying degrees cohesion different sections 
obviously start text cohesive preceding sections exhibit cohesion sections 
middle text quantity cohesion vary greatly 
authors halliday hasan suggest prefer alternate high low degrees cohesion 
texture frequently called coherence cohesion confused differ significantly 
cohesion relates elements text generally identified see hobbs hobbs extended discussion difference 
context 
texture property applies entire text 
difficult define recognized reading text entirety 
grosz sidner influential organization discourse grosz sidner grosz sidner tripartite theory 
theory addresses relationships attentional state linguistic structure intentional structure 
attentional state pertains conversants focus attention accessibility salience discourse entities 
grosz sidner track attentional state stack standard stack operators push pop 
elements stack data structures called focus spaces contain lists available entities discourse segment purpose element intentional structure discussed 
top element stack focus space added elements lower stack available hearer reader utterance 
aspects linguistic structure determine focus spaces added removed stack 
linguistic structure captures relationships successive utterances divides text discourse segments 
segments form hierarchical structure 
linguistic structure constrains changes attentional state 
focus space stack updated transitioning segment 
leaving segment entering sister segment stack popped prior pushing focus space associated new segment 
entering embedded segment focus information associated segment simply pushed stack 
intentional structure models goals subgoals 
discourse segment purpose dsp intention associated discourse segment 
grosz sidner call intention entire discourse discourse purpose dp 
classify relationships intentions identified linguistic structure dominance satisfaction precedence 
dominance means satisfying dominated intention contributes satisfaction dominating intention 
satisfaction precedence indicates intention satisfied satisfied 
relations mirror relations linguistic structure 
intention dominates intentional structure dominated intention corresponds discourse segment linguistic structure descendant discourse segment related dominating intention 
satisfaction precedence relationship intentions indicates corresponding discourse segments sisters linguistic structure 
example similar grosz sidner illustrate relationships components theory 
imagine text consisting sentences constitutes separate discourse segment 
sentence particular topic second third sentences support 
final sentence different subject 
shows linguistic structure terms discourse segments intentional structure encoded list dominance relations attentional structures represented stack containing focus spaces 
part represents state model sentences processed 
third sentence discourse processed structures updated correspond shown part 
final sentence processed structures shown part 
part sentence labeled discourse segment ds contributed bottom element focus space stack second sentence ds contributed top element 
dsp associated segment dsp dominates dsp associated second segment dsp 
third sentence processed hearer recognizes separate discourse segment ds sister ds 
fact reflected linguistic structure ds ds dominated ds indicated dominated segments 
model attentional state updated pushing focus space associated ds stack 
processing fourth sentence change dominance hierarchy affect attentional state 
focus spaces associated ds ds popped stack focus space associated ds pushed 
ds ds ds focus space focus space dsp dominates dsp discourse segments focus space stack dominance hierarchy focus space focus space dsp dominates dsp dsp dominates dsp ds ds focus space dsp dominates dsp dsp dominates dsp ds ds ds ds changes attentional intentional linguistic structure processing sample text 
ko ko discussed methods automatically generating abstracts documents 
outlined typology texts presence semantic relationships textual elements sentences paragraphs document suggested identified semantic network ko 
proposed types text chained neighboring elements strongly related 
relationships segments represented graph type text single cycle encompasses textual elements 
newspaper article leading summary explanatory text possess type structure segments related presumably neighboring segments document related 
chained monolithic piecewise monolithic ko text types 
monolithic segments text related 
graph text kind structure clique 
piecewise monolithic portions text monolithic connections monolithic portions 
contains diagrams representing types structure ko identified 
circles represent textual elements lines indicate presence semantic relations 
numbers associated textual elements indicate order text 
shows documents having chained structure easily segmented 
exhibiting structure segmented 
relationship final units ignored document structure transformed chained structure 
piecewise monolithic documents segmented straightforwardly 
fact texts decomposed segments individual topics monolithic nature 
monolithic texts pose problem text segmentation techniques elements related 
chapter structuring clues text structuring algorithms literature employ wide variety features 
chapter describe features algorithms 
describe features single system literature chapter conjunction algorithms employ 
summarize interesting clues suggested literature computational systems current natural language processing technology 
cues highlight fact people may different cues discover topic shifts computers currently capable 
chapter list additional indicators topic structure topic segmentation systems 
currently uncomputable features documents exhibit number features described strong indicators topic shifts 
features easily recognized current computational techniques 
survey interesting features 
grimes grimes suggested clues presence topic boundaries 
indicators narratives 
shifts new segments may marked changes 
scene 
participant orientation 
chronology 
theme indicators mark topic shifts particular boundary may multiple markers 
explanation indicators order 
change setting action frequently novels plays indicates topic shift 
changes participant orientation importance ranking characters story affect story structure 
instance new topic segment begins new character introduced focus attention shifts away previously important characters 
shift time period example evening day morning description intervening time indicates start new segment 
shifts theme conveyed dialogue frequently occur independent changes setting time period may signal new topic segment grimes 
identified types shift text segment narratives 

topic shifts 
shifts space time 
discontinuities ground 
changes narrative perspective devised heuristics segmenting narrative discourse difficult implement 
heuristics recognize types shift identified 
change topic may signaled anaphoric relations new segment focus segment obvious inferences relate new segment focus segment 
acknowledged vague inferences defined abilities particular computational system 
temporal discontinuities marked tense shifts earlier scenes 
shifts ground accompanied aspectual changes changes discourse focus temporal spatial information 
change narrative perspective may accompanied things shift story teller 
shifting narrator particular character relates action person example changing narrative perspective 
summary techniques proposed grimes straightforward people processing discourse 
currently difficult implement require level natural language understanding attained current technology 
example recognized 
features section require shallower processing 
tedious people manually identify topic boundaries implemented automatic text segmentation system 
currently computable features section describe features topic segmentation systems literature algorithms describe chapter 
examples features sample texts hub broadcast news corpus collected linguistic data consortium ldc spoken document retrieval portion trec text retrieval conference hub program committee 
boundaries topics examples generally indicated vertical white space 
text examples transcribed speech lacks proper capitalization punctuation sentence breaks paragraph boundaries 
produced figures words phrases linked sra discourse tagging tool 
basically essentially generally look ok right say second see similarly table cue words hirschberg litman 
cue words phrases grosz sidner explained words discourse indicate changes discourse structure convey information subject matter discussed 
example incidentally jane swims day 
improbable jane unintentionally happens swim daily interpretation incidentally conveys information relationship utterance jane swims day current discourse 
incidentally marks brief diversion main topic signals start new discourse segment dominated current segment grosz sidner 
researchers examined relationship particular words phrases discourse structure reichman dahlgren 
cue words play important role discourse segmentation hirschberg litman 
number cue words indicate changes discourse structure gleaned various sources hirschberg litman 
cue words shown table study correspondence intonation cue word usage speech 
cue words relatively domain independent may mark topic shifts genres 
domain cues cue words phrases intend differ hirschberg litman important ways 
highly domain specific 
domain specificity means new lists created documents new sources segmented 
drawback manually creating lists time consuming automatically creating requires annotated corpus 
advantage genre specific conventions reliable indicators topic shifts 
example domain news broadcasts cue phrases involving greetings evening night morning occur exclusively broadcasts brief segments show 
indicators shifts topic segment 
cue phrase evening highly domain specific 
penn wall street journal treebank marcus occur words text 
wall street journal articles guarantee mark topic shift 
second cue phrases contain sequences words particular types person place names 
presence word sequences particular types cue phrases dynamic 
example news broadcasts illustrate 
reporters conclude location reports phrase reporting followed name place country city example 
reports followed new topic segments type phrase indicator topic boundary 
employing mildly productive cue phrases reduces number 
example label instances followed reporter name identify instances treat cue phrases 
simpler non cue phrases way 
phrase late example cue phrase 
prevent confusion conventional notions cue phrase refer cue words phrases domain cues 
manually identified domain cues broadcast news domain separated categories 
categories new person cues greeting cues introductory cues pointers upcoming stories shifts returns commercials signing cues 
new person cues occur guests join reporters introduced 
greetings usually mark broadcast 
introductory cues frequently accompany news stories 
pointers upcoming stories keep audience interested suggested importance discussed broadcast 
ldc eliminated commercials corpus phrases indicating commercial just ended remained useful indicators topic shifts 
final category sign cues contains productive phrases signals transition location report back anchor studio 
table lists specific domain cues 
person shorthand person name place refers location station indicates call letters station network example example hub broadcast news corpus contains cue phrases 
divided domain cues categories cues indicate upcoming topic shifts 
greetings precede new topic segments introductory phrases frequently follow topic boundaries 
text segmentation algorithms category phrase individual phrases occurred rarely reliably statistical model 
training data explore appropriateness categories established test value individual cue phrases 
section show larger corpus induce list useful domain cues 
domain cue categories self explanatory 
greeting category contains phrases commonly regarded greetings 
phrases brought words transcript occur start broadcast rarely occur 
result behave members greeting category indicate upcoming topic shifts obviously greetings way morning computing domain cues identified television station network names regular expressions 
label words categories person place built maximum entropy model modeling tools designed implemented adwait ratnaparkhi part speech tagging parsing sentence detection things ratnaparkhi ratnaparkhi reynar ratnaparkhi 
see berger maximum entropy modeling natural language processing ratnaparkhi information ratnaparkhi implementation particular 
domain cue category joining new person night greeting evening greeting morning greeting hello greeting news greeting tomorrow greeting brought greeting transcript greeting top story introductory top stories introductory news introductory introductory just introductory introductory introductory stay pointer ahead pointer come back pointer ll right back pointer return pointer coming pointer ll come back pointer come pointer half hour pointer pointer moment pointer ll right back pointer continue pointer passing passing welcome back return commercial re back return commercial person sign person station sign station person sign reporting place sign person sign live place sign table domain cues identified hand documents hub broadcast news corpus 
says observers stay long west african west african states threatening pull force leaders violating year peace accord weeks chaos capital relative calm returned week peace troops fighters guns faction heads claimed officials warn sustain cease fire troops equipment need western aid security council friday urged member countries enforce nineteen arms officials complain constant violations human rights groups cite peace troops arms jennifer reporting witness david hale began serving month prison sentence today judge banker guilty years ago small business administration hale main witness related trial led governor jim guy tucker james susan hale initially said governor bill clinton pressured dollar loan susan nineteen example domain cue marking boundary topic segments fragment transcript episode national public radio show things considered 
phrase bold domain specific cue phrase form person 
gap separates different news stories 
trained model annotated training data muc named entity task identified subset types participants muc competition expected label chinchor 
subset contain simpler types identified regular expressions including monetary amounts percentages dates may 
maximum entropy model predicts label person place word document features word surrounding context 
model uses features ffl word list corporate designators shown 
ffl previous word list corporate designators shown 
ffl word list corporate designators shown 
ffl word list places taken muc gazetteer 
ffl identity preceding words 
ffl identity preceding word 
ffl identity words 
ffl identity word 
ffl probability word capitalized middle sentence treebank wall street journal corpus 
features final self explanatory 
rough indicator word part proper noun useful identifying person names place names gazetteer 
example model features shown predict label word apple example 
heavy selling widely held stocks oracle systems apple lotus development nasdaq composite index percent ab ae ag ag ao aps ay ba bm bsc bv ca cv cpt ec epe gmbh ges gbr gm gmbh gp gsk hf mij mig ver 
incorporated kb kg kk ks ky lda llc lp mij nl npl nv oe oy plc pn pp pt sa sac sacc saic sas sci scl scp send send 
sl sma snc spa srl sv tas upa vn table list corporate designators named entity recognition 
word apple systems oracle oracle systems features named entity recognizer determining label word apple example 
fact apple precedes corporate designator substantial probability capitalized training data sufficient enable model identify apple part name 
designed named entity recognizer speech recognized transcribed text lacks punctuation contains lowercase letters 
labeling accuracy percent muc named entity test corpus 
labeled token text person place named entity percent error 
simple baseline algorithm posited token named entity achieved percent accuracy 
muc named entity data came new york times 
order build model speech recognized data preprocessed training test data remove capitalization punctuation 
result directly compare performance model systems entries muc competition krupka nymble system bikel punctuation capitalization help identify named entities 
focus subset categories labeled muc competition complicated comparing technique literature 
uses youmans suggested uses words documents accompany topic shifts youmans 
new people places events discussed words preceding text new topic segment begins 
start document majority words time obviously higher proportion uses portions document 
preponderance uses start document partly due words occur independent topic 
word instance text subject probably sentences 
observation applies frequent function words 
long documents number uses associated new topics decline document progresses authors finite vocabularies 
despite complications unusually large number uses indicator start new topic 
presents example hub corpus shows number uses new topic segment 
note large number uses immediately vertical white space marks shift new topic 
twelve thirteen words segment occurred time document 
considering uses content words reduces severity bias uses documents 
ignoring uses function words beneficial usage dependent topic content words 
sample npr transcript shown time uses non function words highlighted 
word repetition halliday hasan lexical cohesion halliday hasan pointed repetition words phrases provides coherence text 
observed degree lexical cohesion topic segment greater topic boundary 
forms basis morris hirst lexical chaining algorithm morris hirst 
technique required hand annotation lexical cohesion relationships text reliably identified computationally 
hearst says observers stay long west african west african states threatening pull force leaders violating year peace accord weeks chaos capital relative calm returned week peace troops fighters guns faction heads claimed officials warn sustain cease fire troops equipment need western aid security council friday urged member countries enforce nineteen arms officials complain constant violations human rights groups cite peace troops arms jennifer reporting witness david hale began serving month prison sentence today judge banker guilty years ago small business administration hale main witness related trial led governor jim guy tucker james susan hale initially said governor bill clinton pressured dollar loan susan nineteen example large number uses words marking new segment 
fragment transcript episode national public radio show things considered 
words bold time 
gap news items 
says observers stay long west african west african states threatening pull force leaders violating year peace accord weeks chaos capital relative calm returned week peace troops fighters guns faction heads claimed officials warn sustain cease fire troops equipment need western aid security council friday urged member countries enforce nineteen arms officials complain constant violations human rights groups cite peace troops arms jennifer reporting witness david hale began serving month prison sentence today judge banker guilty years ago small business administration hale main witness related trial led governor jim guy tucker james susan hale initially said governor bill clinton pressured dollar loan susan nineteen example large number uses open class words marking new segment 
fragment transcript episode national public radio show things considered 
words bold time 
gap separates news stories 
vector space model hearst optimization algorithm reynar number algorithms literature approximate identification simple lexical cohesion relationships looking patterns word repetition 
shows number word repetitions excerpt npr program things considered 
number repetitions topic segment greater number cross topic boundary 
anecdotally demonstrates existence repetitions spanning potential topic boundary indicator boundary 
function words depend heavily topic discussed 
noted word appears documents 
function words account large percentage words documents 
penn treebank wall street journal corpus open class words account percent tokens 
reasons focusing word repetition open class words lemmas beneficial segmentation accuracy speed 
shows excerpt things considered previous repetitions open class words linked 
ignoring function words reduces number repetitions topic boundary number topic segment 
multiple occurrences identically spelled words necessarily contribute cohesion 
texts may contain homographs words spelled different meanings 
example lie mean 
meanings come different root words lie lay respectively 
cases roots meanings differ 
eliminates possibility relying morphology normalization 
goal word sense disambiguation algorithms identify intended meaning cases 
yarowsky part speech tagging sufficient determine appropriate sense cases sophisticated techniques needed 
ignore problems assume repetitions identical word forms contribute cohesion 
justification decision comes suggests generally meaning associated word type discourse gale 
example showing degree word repetition topic segments 
data national public radio program things considered 
example demonstrating quantity content word repetition topic segments 
text national public radio program things considered 
word gram repetition natural extension word repetition techniques previous section count repetitions word grams 
looking repetitions multi word phrases primary advantage 
phrases words occur independently unrelated topic segments 
reason phrases exhibit fewer sense ambiguities words 
number phrases incidentally overlap discourses different topics smaller number incidentally overlapping words making phrases better indicators topic segmentation words 
topic segmentation techniques literature directly word grams 
approach proposed beeferman uses grams language model explicitly track frequency multiple word phrases beeferman 
number repeated bigrams text smaller number repeated words number trigrams repeat documents smaller number bigrams 
trend applies strongly longer grams 
result algorithms repetitions bigrams trigrams provide little additional information 
penn treebank wall street journal corpus bigrams occur trigrams merely grams 
demonstrates usefulness tracking repetitions bigrams identifying shifts topic 
presence multiple instances particular bigram suggests regions containing bigrams topic segment 
case word repetition repeated phrases informative 
particular bigrams function words frequent little weight hints topic structure 
bigrams consisting content word function word example book convey little additional information repetition content word 
algorithms restrict bigrams contribute evidence segmentation containing content words 
statistics regarding gram frequency skewed grams content words 
treebank wall street journal corpus contains content bigram repetitions documents repetitions trigrams grams example indicating usefulness tracking repetition word bigrams topic segmentation 
data national public radio program things considered 
respectively 
shows portion document repetitions bigrams annotated 
sophisticated approach consider repetition terminology 
example terms modal dialog box junk bond new york alan greenspan american telephone telegraph 
example points advantage identifying terminology phrases containing function words permitted informative 
system identifies terminology justeson katz propose justeson katz identify interesting multiple example showing usefulness tracking repetition content word bigrams topic segmentation 
text transcribed national public radio program things considered 
word phrases 
track repetitions phrases bigrams 
approach limit applicability segmentation algorithms domains large training corpora exist 
prevent bigrams seen time test data contributing information segmentation 
result repetitions bigrams containing content words indicator topic shift 
word frequency word frequency topic segmentation differs models word bigram repetition word frequency assumes prior knowledge individual words occur corpus 
models predict frequency occurrence words called language models 
speech recognition instance lau applied optical character recognition hull wide variety problems natural language processing including author identification mosteller wallace language identification dunning part speech tagging church text compression suen spelling correction kukich 
advantage word frequency detect shifts topic merely counting number word repetitions repetitions words weighted differently depending probability 
simply word occurs neighboring sentences imply sentences topic segment 
fact hardly says likelihood topic segment 
word occurs neighboring sentences reading rest text guess high confidence sentences segment 
frequent occur discussion topic rare consequently discussing subjects 
address problem modify word repetition algorithm ignore frequent words content words continuum importance 
repetitions light verbs informative repetitions function words far helpful segmentation repetitions rare content words 
word frequency provides useful way weight words repetitions higher frequency words contribute information segmentation repetitions lower frequency ones 
closed class words simply ignored consequently paid little heed 
wall street journal occurs occurs 
repetitions weighted highly turn count repeat occurrences 
advantage tracking word frequency occurrences word type weighted differently depending probability 
example model word frequency assign occurrence word different probability second occurrence turn different probability additional occurrences 
word repetition algorithms hand tend treat occurrences identically 
see section discussion burstiness phenomenon content words repeat 
cost associated additional information exploited word frequency algorithm 
track word repetition statistical model language 
fact assuming preprocessing done segment text language highly agglutinative optimization algorithm reynar version hearst texttiling normalize term frequency hearst 
algorithms rely word frequency language modeling technique developed beeferman beeferman require knowing language text assumptions content 
assumptions necessary frequency occurrence words crucially depends subject matter discussed 
word quite frequent penn treebank wall street journal corpus 
occurs times words 
brown corpus occurs times words kucera francis 
brown corpus newspaper text times occur sample financial news sample corpus 
variation word frequency suggests utility technique word frequency depend accurate knowledge source text 
example indicating utility tracking synonyms identifying topic boundaries 
data national public radio program things considered 
synonymy subset halliday hasan lexical cohesion relationships captured identifying pairs words synonyms 
example npr things considered shows relations synonyms 
identifying synonyms computationally knowledge source encodes synonymy wordnet miller roget thesaurus roget problematic 
wordnet roget thesaurus designed broad coverage means contain synonymous relations words wide variety contexts 
advantageous human users causes spurious synonymous relations identified algorithmically 
example wordnet considers man synonymous operate words sense 
words legitimate synonyms contexts 
part speech tagging prevent words labeled synonyms nouns plant factory example synonymous 
part speech tagging word sense disambiguation techniques naive thesaurus result overgeneration synonymous relations words 
problem knowledge sources coverage 
synonyms restricted particular domains technical legal text 
instance word keyboard roget thesaurus 
thesaurus predates computer years language evolves new jargon constantly created evidenced wordnet lack entry trackball 
named entities indicators topic shift described previous sections address halliday hasan category lexical cohesion 
relations category pertain topic shifts 
referential links involve pronouns identified pronoun resolution techniques 
shows portion transcript episode npr things considered annotated links indicating phrases refer entities 
unfortunately pronoun resolution remains unsolved problem computational systems today address subset baldwin 
repetitions people names names companies organizations place names easily detected 
way identical word grams arise independently different topic segments repetitions proper names occur chance neighboring topic segments 
result indicators portions text topic segment 
shows transcript previous time coreference links involving proper names indicated 
identify coreference links involving proper names called named entities chinchor statistical model built identify proper nouns occur dynamic cue phrases 
attempt resolve pronouns identify involving names portions names 
instance identify example transcript indicating usefulness coreference topic segmentation 
text transcribed national public radio program things considered 
example showing usefulness limited coreference named entities 
text transcript national public radio program things considered 
relationship international business machines international business machines ibm 
repetitions named entities identify string matching techniques consequently subsumed approaches identify repetitions grams 
named entities merit special attention domain news broadcasts particularly informative clues 
news items particular people companies nations entities news stories 
result distinguishing repetitions entities word gram repetitions weighting highly improve topic segmentation performance 
pronoun usage dissertation levy described study impact type referring expressions location mentions people gestures speakers cohesiveness discourse levy 
strong correlation types referring expressions people particular explicit degree cohesiveness preceding context 
cohesive utterances generally contained explicit referring expressions definite noun phrases phrases consisting possessive followed noun cohesive utterances frequently contained zeroes pronouns 
converse levy observation pronouns gauge likelihood topic shift 
levy generally pronouns utterances exhibited high degree cohesion prior context investigate hypothesis pronoun presence topic boundary 
presence pronoun words immediately putative topic boundary provides evidence topic boundary exists 
presents example phenomenon hub broadcast news corpus 
example text topic boundary sentence said 
year old name todd hot springs montana upset winner javelin grew uh flat head indian reservation eighteen years came little uh called native american sports council john sports council got held religious todd uh night race credits sort giving certain extra spirit uh won event throw said biggest high school went hot springs high school graduating class senior year won state track field won meters hurdles course javelin wow said matched performance high school goodness give event just key event wednesday atlanta example text region pronoun existence preceding segment boundary 
fragment transcript episode national public radio show things considered 
crucial pronoun shown bold 
topic boundary sentence said 
general new segment pronoun 
course exceptions including instances uses pronouns metaphorical statements 
presence pronoun region text follows putative boundary indicator topic boundary 
character gram repetition complications word repetition word frequency track topic shift word types occur different inflected forms text 
desirable note relationship singular plural forms particular noun different inflected forms verbs 
identify relations morphological analyzer xtag morphology system karp convert words roots prior looking repetition 
unfortunately systems available languages 
part speech tagging text ambiguities 
take word said example 
xtag morphology software roots say 
lemma frequently correct second equally valid lemma noun form said alternate form arabic word meaning lord sir english 
mistakenly determine lemma verb form said relationship said say 
previous example demonstrates difficulties may arise relying naive discover relations words 
suggests sophisticated tools generally rely part speech tagging difficult standard cues capitalization punctuation sentence boundaries case speech recognized text 
text identifying word repetitions ignore lemmatization rely grams characters 
example morphology normalization words said say character sequence sa common 
shows character gram overlap transcript npr things considered 
highlights repetitions characters words common roots 
indicating instances multiple character repetition render text unreadable 
particularly interesting relation shown name misspelling occurred transcript story 
drawbacks character grams 
common words spelled character sequences frequently occur longer words 
instance common word genres substring 
algorithm identifies similarities regions text character grams recognize spurious similarities just readily observe legitimate ones involving instance singular plural forms noun 
removing function words data reduces severity problem eliminate 
instance open class word dent substring unrelated word identify 
unrelated words share features inflectional derivational morphology verb forms takes faxes share es common root 
unclear sort overlap help hindrance 
prove useful segmentation permitting recognition similarities verb tense writing style 
alternatively cause unhelpful substring repetitions 
example showing utility tracking character gram repetition identifying topic boundaries 
transcript national public radio program things considered 
character strings text processing applications past 
literature discourse structure mention tracking repeated character strings church parallel text alignment technique church text analysis depend character string repetition 
additional indicators algorithms employ indicators listed previous sections 
indicators identify topic shifts 
instance structural parallelism provide continuity texts 
conversely neighboring sentences similar structure topic boundary 
parallelism identified processing output parser xtag parser xtag group ratnaparkhi statistical parser ratnaparkhi 
authors number related points clarity 
enumerations generally signifies sentences topic segment 
straightforward identify sentences second forth regular expressions 
major shifts topic may accompanied changes complexity text 
text complexity measured called reading level indicators commonly word processing packages fog index 
reading level metric function word length sentence length 
suggestions left relevant small proportion documents 
parsing computationally expensive performance generally necessitates annotating training data domain parsed 
limits applicability parallelism technique 
importantly topic segmentation meant applied early stages language processing 
full parsing come especially topic segmentation improve preliminary processing steps word sense disambiguation 
chapter previous text segmentation methods publicly available topic segmented corpora exist 
result comparing performance topic segmentation algorithms literature difficult researchers evaluated techniques number different ways 
result simplest dimension compare various algorithms features 
table lists features algorithms described section 
similar table chapter algorithms 
word repetition dominates previous segmenting text topic 
half techniques described depend word repetition 
summarize techniques move methods features 
word repetition frequently indicator topic shift word repetition 
researchers word repetition structure text various ways conjunction features 
algorithm cue words phrases pronoun usage uses word occurrence word repetition word frequency word grams named entities synonymy character grams semantic similarity morris hirst 
hearst plaunt richmond yaari van der eijk nitta takeo 
beeferman phillips youmans kozima ponte croft texttiling tf delta idf weighting 
tf delta idf word frequency algorithm word repetition algorithm 
table types clues various text structuring algorithms 
morris hirst morris hirst described discourse segmentation algorithm morris hirst morris lexical cohesion relations halliday hasan 
goals provide support grosz sidner theory discourse structure grosz sidner algorithm divides texts segments form hierarchical structure 
step morris hirst algorithm link sequences related words document form lexical chains 
words initially form lexical chain related lexical cohesion 
additional word added existing lexical chain participate lexical cohesion relation word chain 
morris hirst roget thesaurus roget determine pair words satisfies relations 
forced identify lexical chains hand roget thesaurus available machine readable form 
thesaurus determine lexical cohesion relations words thought related topic text open class words occur overly frequently 
understanding algorithm requires knowledge organization knowledge source 
roget thesaurus divided categories index indicates categories words appear 
categories paired paired categories labeled words usually antonyms 
categories grouped semantically related sets 
categories may contain words cross categories 
roget thesaurus typically find synonyms particular word 
identifies synonyms locating word index identifying pertinent category word meaning context 
words category selects best synonym 
example looking text index finds meaning part writing relevant category number labeled part 
category contains number related words section article page 
morris hirst thesaurus differently identify lexical chains 
decided pairs words satisfied halliday hasan lexical cohesion relation checking index entries words 
technique words deemed related lexical chain true 
share common category 
word category contains pointer category containing second word 
word label category containing word 
word category containing common category 
words group categories examples relations clarify things 
text article category number 
placed lexical chain reason listed 
topic essence related form lexical chain reason 
category contains word topic pointer category contains word essence 
text topic lexical chain third reason topic label category text member 
document category contains pointer category 
record category points category 
document record placed lexical chain result common category 
text composition put lexical chain text category composition category categories group categories labeled 
addition excluding overly frequent words closed class words participating lexical chains morris hirst identify relations pairs words widely separated text 
handled relations distant words closer lexical chain permitting lexical chains related 
identified lexical chains document compared elements chains determine chains continuations earlier ones 
labeled chains related earlier ones chain returns revisited topic established chain 
morris hirst analyzed small number texts algorithm quantitatively evaluate performance 
compared output structure identified discourse grosz sidner theory 
morris hirst structures identified lexical chaining algorithm similar structures identified hand 
hearst automated lexical chaining process earlier version roget thesaurus roget 
automatic algorithm label boundaries morris hirst hand execution 
performance suffered version thesaurus inferior version automation lexical chaining introduced errors 
building lexical chains manually morris hirst missed relations hearst implementation algorithm 
relations algorithmically spurious arose word sense ambiguities hearst 
hearst hearst developed technique automatically divide long expository texts segments paragraphs length single subtopic 
chose linearly segment text partly ko structure texts difficulty eliciting hierarchical segmentations annotators litman 
hearst algorithm texttiling vector space model determines similarity texts assuming documents represented word vectors high dimensional space 
information retrieval applications texts compared user query document 
texttiling uses vector space model determine similarity neighboring groups sentences places subtopic boundaries dissimilar neighboring blocks 
vector space model simplest form vector space model treats documents vectors values correspond number occurrences words document 
example phrase give liberty give death represented vector 
case element vector number occurrences give second number repetitions forth 
number ways compute similarity documents vector space model 
frequently cosine distance determines angle vectors associated document 
vectors represent similar documents smaller angle represent dissimilar documents 
refer documents write similarity sim 
label word vector th document equation cosine distance measure sim cos delta jw jjw formula cosine distance written shown 
dimensionality word vectors represents number different word types 
cos ik jk ik jk second version formula summation word types clearer number common words number times appear document determines similarity score 
processing texttiling hearst describes detail processing steps texttiling hearst 
algorithm document removes common words closed class list 
words eliminated helpful identifying subtopic sections 
texttiling reduces remaining words morphological root wordnet 
divides root case issues style authorship believed affect structure document 
see mosteller wallace discussion role closed class words determining authorship biber biber discussion relationship function words register 
words groups called pseudo sentences 
groups pseudo sentences blocks 
computes similarity scores adjacent blocks cosine distance measure 
computing similarity scores blocks pseudo sentences paragraphs eliminates difficulties vector space model stem length variation 
similarity scores texttiling computes depth scores quantify similarity block blocks vicinity 
terms graph similarity scores depth score thought sum differences top peak immediately left right valley 
computation depth scores proceeds follows start particular gap blocks record similarity score associated blocks side gap 
check similarity score preceding gap 
higher continue examining similarity score previous gap 
continue way score lower score examined 
subtract similarity score initial gap maximum similarity score encountered 
repeat procedure gaps blocks gap 
sum differences computed 
value depth score gap examined 
depth scores need computed gaps local minima similarity function 
texttiling selects gaps highest depth scores sites subtopic boundaries 
algorithm adjusts identified locations ensure correspond paragraph boundaries 
discards boundaries lie close previously identified boundaries 
unsmoothed depth score graph shown 
shows depth scores data round smoothing smoothing algorithm 
hearst compared segmentation produced texttiling reader judgments locations topic boundaries thirteen magazine articles 
measured performance information retrieval metrics precision recall 
precision ratio number correct guesses total number guesses recall ratio number correct guesses total number answers scoring key 
values precision recall range indicating perfect performance 
texttiling score depth score word unsmoothed depth score graph concatenated wall street journal articles 
score depth score word smoothed depth score graph concatenated wall street journal articles 
scored precision recall hearst compared segmentation texttiling produced consensus labelings generated group human judges 
hearst evaluated automated version morris hirst lexical chaining algorithm 
algorithm performed precision recall 
performance algorithms better baseline techniques scored precision recall precision recall randomly guessing boundaries 
judges consistent algorithms compared collective judgments 
judges averaged precision recall compared consensus individual annotations hearst 
richmond smith amitay richmond smith amitay describe technique locating topic boundaries richmond 
method weights importance words frequency document distance repetitions 
determine similarity neighboring regions text summing weights words occur regions subtracting summed weights words occur segment 
normalize dividing number words section 
algorithm steps 
basic preprocessing done 
calculate weight word call significance 
compute values formula 
significance scores different instances word type may differ depending context 
significance delta arctan represents particular word token 
number word tokens document 
number occurrences words type word distance word th nearest repetition word 
number nearest neighbors deemed useful significance computation determined formula 
gamma delta gamma values range 
significance ranges initially normalized lie indicating minimum significance 
richmond significance values word compute similarity regions document 
determined optimal size regions compared fifteen sentences 
formula similarity regions call correspondence 
correspondence ja jaj jb jbj formula bag words contained region bag second region 
word type appears bags occurs associated region text 
portions respectively containing words types occur portions respectively contain words types occur notation jaj indicates summing significance scores words richmond smooth correspondence scores form weighted average smoothing 
place topic boundaries correspondence scores lowest 
applied algorithm text articles front page newspaper psychology 
results suggested algorithm performed perform systematic evaluation corpus 
yaari yaari proposed expository texts segmented hierarchical agglomerative clustering hac yaari 
hac initially places element set class recursively merges similar classes items class 
hac produce dendogram depicts relationships elements order merges classes 
yaari modified hac text segmentation sample dendogram type produced yaari hierarchical clustering algorithm 
axis sentence number axis level merge 
permit merges neighboring segments 
shows dendogram similar produced hac 
yaari removed closed class words documents porter algorithm reduce remaining words stems porter 
computed similarity paragraphs cosine measure inverse document frequency idf weighting weights rare words highly common ones 
hac similarity scores group similar neighboring segments 
hac clustered paragraphs yaari created dendogram showing order merges applied rules convert hierarchical clustering linear segmentation 
facilitate comparing hac output linear segmentation produced texttiling 
tested hac performance single article article discover magazine collection hearst evaluate texttiling 
algorithm better replicated annotation produced hearst judges texttiling 
approaches van der eijk texttiling compute similarity translations documents multiple languages van der eijk 
nitta extended texttiling japanese texts nitta 
takeo developed disambiguation algorithm perform text segmentation takeo 
nitta takeo algorithms semi automatic required hand annotation 
designed number techniques word repetition manually segmenting text 
features beeferman berger lafferty beeferman berger lafferty describe technique identifying document boundaries statistical techniques beeferman 
heart method statistical framework called feature induction random fields exponential models 
built statistical models framework incorporated number cues presence story boundaries 
hints included ffl particular words appear sentences prior potential boundary 
ffl particular cue words preceding sentences 
ffl trigger language model beeferman predict text compared static trigram language model 
feature provides measure topicality 
trigger language model boosts probability particular word presence words preceding context occured near word training corpus 
language model performs poorly relative static language model may preceding context topically dissimilar current text 
beeferman measured performance segmenting news feed containing concatenated wall street journal articles precision recall 
figures significantly higher achieved guessing randomly placing boundary possible point locating boundaries 
proposed probabilistic performance measure discuss chapter beeferman 
phillips phillips examined relationship word collocations topic described features semiautomatic system phillips 
system preprocessed text discard high low frequency words converted remaining words root forms 
counted collocations particular word words window extended words left right word 
example start address score years ago fathers brought phillips identify bigrams involving word years 
bigrams pair years words left words right 
phillips resulting collocational frequency statistics cluster analysis identify lexical networks chapters science textbooks 
showed clusters corresponded subtopic structure chapters identified book author 
proposed method identify global topic structure 
extracted nuclear words considered central text word clusters identify subtopic structure 
compared sets nuclear words different chapters sufficiently similar noted relations chapters 
phillips suggested relations hypertext linking 
youmans youmans described text analysis technique applications study literature youmans 
technique observation shifts topic accompanied changes word usage 
particular new topic introduced words related topic time 
quantify observation graphed number word types document function number word tokens 
presents sample type graph 
obviously document tokens instances type 
result slope portion type token graph greater slope portions graph start document tokens 
total number word types word position type token plot brown corpus file cd 
addition suggesting usefulness technique tracking topic shift youmans proposed type token curve useful manually estimating size author vocabulary authorship drawn comparing curves associated different works 
methods automating tasks 
vocabulary management profiles youmans subsequently improved type token curve technique youmans 
drawback technique topic boundaries difficult identify graphs changes topic resulted words 
address youmans suggested counting number uses fixed size window text usually defined words 
proposed number occurrences graphed function word position document 
type graph youmans called vocabulary management profile vmp represents approximation derivative type token curve 
vmp document brown corpus 
uses window word position vocabulary management profile brown corpus file cd 
youmans proposed discourse boundaries identified examining vmp sharp deep valleys 
defined discourse boundary common set boundaries identified theories described polanyi chafe longacre grimes 
goal identifying boundaries common theories place boundaries trained readers english literature perceive 
youmans describe techniques automatically identify discourse boundaries 
perform hand analyses concluded structures suggested vmp james joyce novel george essay corresponded structures perceived reading 
kozima kozima defined measure lexical cohesion called lexical cohesion profile lcp kozima kozima furugori 
lcp computed similarity metric derived spreading activation semantic network systematically constructed english dictionary 
lcp score particular word sum see kozima furugori details 
semantic similarity scores arise comparing word word window preceding words 
kozima postulated local minima similarity scores correspond positions topic boundaries text 
compared boundaries identified lcp segmentation identified subjects labeled text paragraph boundaries removed 
quantitatively evaluate algorithm performance state segmentation produced lcp resembled human annotators generated 
ponte croft ponte croft topic segmentation technique ponte croft models length topic segments uses local content analysis lca query expansion technique ir systems xu croft 
goal identify boundaries short topic segments news articles wall street journal contain brief summaries news items discussed greater detail newspaper 
summary sentences long average article contains summaries 
dynamic programming algorithm identify best partitioning article segments single news item 
lca short topic segments frequently contain repeated words suggested approaches word repetition little 
lca generally identify related passages document database ir 
ponte croft lca identify key concepts returned passages concepts surrogates sentences text 
computed similarity scores neighboring sentences counting number concepts common sets concepts identified sentence 
employed scores dynamic programming algorithm conjunction model length derived empirically training data 
glance results stellar recall precision test set 
words original sentences claimed little occurred achieved recall precision 
analysis contribution lca revealed performance length modeling reduced greatly recall precision 
suggests length modeling account performance improvement lca 
baseline performance task quite 
test corpus achieved results contained sentences topic segments 
naively assuming topic boundary sentence achieve perfect recall precision 
discussion permit comparisons algorithms literature chapter apply evaluation data 
hearst showed texttiling outperformed morris hirst lexical chaining technique 
richmond smith amitay evidence algorithm worked handful texts 
said yaari hierarchical clustering method tested single document 
van der eijk multilingual technique hearst offered new method segmenting documents 
algorithms attributable nitta takeo designed structure japanese text 
methods automatic 
result compare performance algorithms english data segmentations produced texttiling best word repetition algorithms english literature 
model described beeferman difficult replicate statistical modeling framework uses language model trained vast amount data 
compare performance algorithms performance tested tdt corpus evaluate algorithms chapter 
phillips algorithm kozima technique difficult replicate 
kozima method requires semantic network compute lcp scores publicly available 
phillips method semiautomatic quantitative results suggest performed 
ponte croft algorithm addressed different task 
task lies discourse segmentation text segmentation segment size smaller text segmentation techniques identify larger identified discourse segmentation algorithms 
leaves youmans technique collection methods structure text features word repetition 
compare algorithms algorithmic implementation youmans vmp method chapter 
chapter algorithms text segmentation algorithm possess certain attributes 
discuss section describe novel algorithms possess 
table shows features algorithms 
describing algorithms explain preprocessing perform documents discuss document segmentation simulation compare performance algorithms literature hearst texttiling algorithm hearst automated implementation vocabulary management profile youmans 
results novel algorithms vector space model similar texttiling 
desiderata ideal text segmentation algorithm properties 
fully automatic 
computationally efficient 
robust applicable variety document types algorithms propose satisfy criterion completely computational 
manual step required algorithms texttiling available ftp cs berkeley edu src 
algorithm cue words phrases pronoun usage uses word occurrence word repetition word frequency word grams named entities synonymy character grams semantic similarity compression optimization word frequency max 
ent 
model 
table features text structuring algorithms 
identification cue phrases discussed section 
discuss simple way automate step chapter 
algorithms described chapter automatic required human intervention 
efficiency important primary goal build tools address real nlp problems 
extremely accurate slow system theoretically interesting allow practical natural language processing improved 
real time performance may necessary nlp algorithms discuss chapter involve large potentially fast growing text collections 
robustness crucial text structuring algorithms applied wide variety domains 
algorithms rely word frequency statistics collected training corpus robust require statistics 
robustness algorithms propose literature diminished reliance language specific preprocessing 
text normalization steps shown constitute text normalization 
normalize documents performing steps prior segmenting 
discuss step detail normalization important natural language processing applications rarely receives attention 

tokenization 
conversion lower case 
lemmatization optional 
removal common words optional tokenization tokenization prevents word tokens neighboring punctuation marks jointly misidentified single token 
consider example 
tokenizing sentences produces text shown example 
tokenization nlp algorithms repetition word stocks instance followed comma 
depending source data tokenize text maximum entropy model trained wall street journal text simple set rules designed separate punctuation words 
today stocks closed higher heavy trading 
stocks despite early losses reached time 
today stocks closed higher heavy trading stocks despite early losses reached time built maximum entropy model tokenization ratnaparkhi software ratnaparkhi 
model determines space inserted neighboring characters document features similar named entity recognizer described earlier 
set features tokenization includes ffl identity characters preceding spot space added 
ffl identity preceding character 
ffl identity characters spot 
ffl identity character 
ffl rudimentary class information preceding characters 
measured performance hand corrected file subset penn wall street journal treebank percent precision percent recall 
considerably better hand built tokenizer university pennsylvania muc system scored percent precision recall data baldwin 
conversion lowercase second step text normalization convert letters lower case 
step eliminates problems caused sentence initial capitalization 
normalize text way stock stock treated different words example 
course process alters useful capitalization 
example occurrences international example differ part proper noun second 
distinction difficult observe capitalization eliminated 
stock prices edged higher today light trading stock markets closed tomorrow holiday 
international business machines announced worldwide today citing need reduce labor costs better compete increasingly international personal computer market lemmatization optional third component normalization replacing inflected forms roots 
perfect lemmatization allows useful regularities text identified 
example shows text example tokenization morphology normalization xtag morphology software karp 
aboard ah alas albeit alongside table closed class words letter 
today stock close high heavy trade stock despite early loss reach time high identifying closed class words flag frequent closed class words list doing improves text segmentation techniques 
function words generally contribute little information regarding topic segmentation ignoring increase processing speed improve performance 
table shows words letter list closed class words 
list available summer institute linguistics 
document concatenations artificial task refine benchmark text segmentation algorithms 
goal task identify boundaries pairs concatenated wall street journal articles drawn random penn treebank 
worthwhile briefly consider artificial task light analogy noisy channel model process obscuring topic boundaries chapter 
data actual topic segmentation performed document concatenations passed noisy channel 
simulation approximates task identifying boundaries obscured provide available ftp www sil org 
restriction placed articles 
permitted brief articles page wall street journal summarize sets articles appear 
words baseline method middle sentence middle paragraph random sentence random paragraph table accuracy baseline segmentation algorithms pairs concatenated wsj articles 
rows average results iterations random selection algorithm 
reader hearer appropriate level continuity topics 
easier actual topic segmentation serves useful benchmark 
task simpler topic segmentation part number segment boundaries identify known advance 
concatenated documents may address entirely different subjects 
boundaries pairs concatenated articles difficult identify domain wall street journal constrained 
task challenging article length varies widely 
short article may words long feature stories may words length 
words wall street journal occur articles independent topic 
repeat occurrences words may mislead algorithms word repetition 
admittedly task artificial evaluating algorithms advantages 
definitive correct answer concatenation 
human annotators need trained locate boundaries need identify consensus segmentation pool annotations produce 
treebank contains articles easily create new evaluation training corpora 
advantage measure baseline performance ways compare performance sophisticated algorithms baseline scores 
simple baseline randomly select single boundary 
trivial algorithm selects middle sentence paragraph concatenation 
table presents performance techniques test corpus consists pairs wall street journal articles 
concatenations wall street journal articles testing contained average paragraphs sentences words 
evaluated algorithms counting number exact matches guesses article boundaries number guesses lying words boundary 
counting exact matches overly harsh desirable algorithm score better places boundaries close actual location 
evaluating algorithms results achieved optional normalization steps described words removing words list 
theory topic boundary occur middle sentence 
simulation know boundaries documents occur paragraphs 
paragraph sentence boundaries labeled penn treebank 
result measure performance algorithms guessed boundaries constrained occur sentences paragraphs 
restricting guesses paragraph boundaries may boost performance fewer possible boundary sites choose 
texts annotated paragraph boundaries initially marked difficult recover bounds automatically 
domains sentence boundaries labeled systems exist accurately disambiguate potential sentence boundaries statistical techniques 
example reynar ratnaparkhi 
performance algorithms literature order permit comparisons algorithms literature tested hearst texttiling algorithm implementation youmans vmp technique concatenations wall street journal articles 
performance techniques 
describe evaluate novel text segmentation algorithms vector space model similar texttiling 
unit analysis normalization words sent para sent para table texttiling applied document concatenations 
texttiling label boundaries concatenations short 
texttiling tested texttiling document concatenations permitting guess location boundary documents concatenation 
table shows performance optional normalization steps described 
column indicates boundaries constrained lie sentences paragraphs 
remaining columns indicate boundaries specified number words actual boundary location 
column measures number exact matches 
show best entry column bold subsequent performance tables 
expected performance improved guesses restricted lie paragraphs 
morphological normalization boosted performance 
reimplemented texttiling prior discovering version freely available 
addition implemented segmentation algorithms vector space similar hearst texttiling technique 
implementing algorithms allowed explore validity decisions hearst regarding design texttiling data 
texttiling computed similarity fixed length blocks paragraphs eliminate length variations cause problems vector space model 
smoothed placed boundaries gaps large depth scores hearst 
vector space model text segmentation techniques implemented parameterized described 
parenthesized abbreviations concisely indicate parameter settings 
ffl smoothing performed 
ffl sentences sent paragraphs para pseudo sentences unit analysis 
pseudo sentences boundaries restricted lie sentences ps paragraphs pp 
ffl boundaries identified relative depths valleys hearst recommended rel identifying point similar adjacent blocks min 
testing variants text short allow block size pseudo sentences hearst recommended decreased block size boundary identified 
table shows performance variations document concatenations 
results optional normalization shown table 
columns tables indicate parameterization tested 
possible variants correspond combinations possible settings parameters 
setting min smooth location minimum score affected smoothing 
hypothesis borne preliminary experiments 
eliminated possible parameterizations leaving table 
reimplementation parameterization hearst texttiling table row parameter settings pp rel 
reimplementation performed better publicly available version texttiling due adjustment block size allowed boundaries identified short concatenations 
draw performance publicly available texttiling algorithm segmentation techniques vector space model implemented 
enhancement decreased block size boundary identified crucial test data concatenations articles shorter articles hearst evaluate texttiling 
fact publicly available smoothing unit analysis algorithm words ps min pp min sent min para min ps rel pp rel sent rel para rel ps rel pp rel sent rel para rel table results implementation algorithms vector space model applied documents optional normalization 
algorithm tested concatenations pairs wall street journal articles 
row gray presents results settings texttiling 
smoothing unit analysis algorithm words ps min pp min sent min para min ps rel pp rel sent rel para rel ps rel pp rel sent rel para rel table results implementation algorithms vector space model applied normalized data 
algorithm tested concatenations pairs wall street journal articles 
row gray presents results settings texttiling 
unit analysis normalization words sent para sent para table results variants youmans technique applied data consisting concatenations pairs wall street journal articles 
version texttiling failed guess boundary location wall street journal articles short 
contrary hearst suggested smoothing detrimental performance 
pseudo sentences hearst proposed improved performance concatenations 
test corpus algorithms depth scores texttiling fared worse simpler technique locating boundaries neighboring blocks similar 
fact simpler technique identified maximum number exact matches text segmentation algorithm vector space model 
vocabulary management profiles implemented tested youmans vmp technique 
window words youmans suggested 
automation vmp technique placed boundary immediately preceding point concatenation new words introduced 
evaluated vmp technique boundaries constrained lie sentence boundaries paragraph boundaries 
table presents results evaluations optional normalization 
youmans algorithm performed better restricted placing boundaries paragraphs 
performed slightly better applied normalized data 
accurate best algorithms vector space model 
compression algorithm algorithms relies repetitions character grams identify topic boundaries 
elegant approach compressing data capitalizes inherent self similarities text 
crucial assumption underlying compression algorithm similarity terms distributions characters topics greater topics 
exploit assumption identify topic boundaries observing compression performance measured ratio size original text size compressed text 
identify boundaries locating point compression ratio minimized point text compressed preceding text 
location maximum dissimilarity topic boundary 
lempel ziv performed text compression method developed ziv lempel commonly called lz ziv lempel 
lz widely implemented gzip popular compression packages 
important contribution ziv lempel showing self similarity exploited compress data 
earlier compression methods dictionaries relied assumptions frequency individual characters similar way frequency information determine encode letters dots dashes morse code 
lz algorithm incrementally compresses text 
location portion text compressed marked pointer text pointer stored lookahead buffer 
portion text immediately pointer retained fixed size window size usually integer power 
window compressed text perform additional compression identifying maximal string match text immediately pointer text window 
algorithm begins window empty text compressed 
compression proceeds follows 
set pointers precede character text 

set character move pointer character right 

match string fixed sized window go step 
follows character document go step 
concatenate character lookahead buffer 
move character right 
go step 
remove final character longer character 
move character left 

length encode literal 
encode pair containing distance pointer location window string identical begins length 

quit follows character document 

move right length go step 
example elucidate process 
assume text compressed string window size characters 
compression proceed shown table 
literal column pointer length columns empty row table iteration algorithm encodes text literal pair consisting pointer length 
sample text encoded sequence 
short example output compression algorithm longer original text 
generally lz reduces length texts percent 
performance usually improves text length 
lz algorithm modified ways suit various kinds data 
optimized reduce compression time modified perform additional text window lookahead buffer literal pointer length baba ab aba ab ba table sample lz compression 
compression portions resulting encoding 
perform topic segmentation implemented variant lz similar gzip 
variant differs original lz algorithm literals lengths compressed codebook pointers compressed second codebook 
window size characters 
complicating factors difficulty character sequences indicator topic segmentation spurious substring repetitions occur accidental word repetitions 
instance strings associated inflection suffix ing repeat topic segments 
morphology normalization crucial 
example normalization word making compressed previous context contained word 
reducing words root forms overlap strings degree compression reduced 
coincidental substring repetitions occur letter sequences quite frequent 
instance string th occurs times character portion penn treebank wall street journal corpus 
assume non topic repetitions similarly distributed topic segments result significantly hamper identifying segment boundaries 
see bell descriptions variations 
unit analysis normalization words sent para sent para table results compression algorithm concatenations pairs wall street journal articles 
evaluation table shows performance compression algorithm 
simple elegant algorithm performs poorly 
fact slightly better baseline algorithm guesses boundary concatenation middle paragraphs 
result test algorithm corpora described chapter 
concluded poor performance algorithm character sequences useful indicator text segmentation words 
segmentation performed data word boundaries character sequences useful 
informative features 
optimization algorithm second text structuring method lexical cohesion halliday hasan segments text optimization algorithm applied patterns word repetition 
initially motivated technique called 
dotplot visual aid viewing data matrix 
display large quantities similarity information permit information analyzed visually 
display data binary valued matrix example point placed coordinate dotplot value cell matrix 
table sample word repetition matrix 
dotplot matrix table shows word repetitions phrase 
text analysis aid software engineering 
analyzing repetitions character grams detected similar sections documents document collections 
identified modules contained similar code fragments database computer source code 
church align parallel translations pairs languages cognates church 
number words document sequentially build matrix cell contains words numbered identical 
build dotplot matrix 
example matrix shown table represents word repetitions phrase 
created dotplot shown matrix 
note clarity labeled axes graph words word numbers 
word position word position dotplot concatenated wall street journal articles 
text segmentation segment text generate matrix cells set word number word number root 
word appears word positions text set values cells 
type matrix cells value words identical 
evaluations describe condition hold preprocessed documents function words ignored 
shows dotplot matrix built way 
constructed word repetition matrix concatenated wall street journal articles 
boundaries documents located immediately words numbered 
due number repeated words documents individual document appears dark square region dotplot 
boundaries documents visually apparent 
algorithmic boundary identification fact boundaries identified visually suggests identified algorithmically processing dotplot word repetition matrix 
extent topic segment apparent dotplot segments correspond regions line darker regions 
darkness arises regions high density density measure number points unit area computed simply dividing number points region area region 
example region words words dotplot contained points density delta 
propose related algorithms identifying topic segments measuring density 
technique identifies boundaries maximize density segments lie diagonal dotplot 
second method locates boundaries minimize density regions diagonal intuitively algorithm finds topic segments maximizing self similarity second identifies minimizing similarity different segments 
algorithm identifies boundaries minimizing density proceeds follows 
posit boundary particular location 
compute density regions main diagonal boundary previously identified boundaries place 
record density location hypothesized boundary 
repeat steps putative boundaries 
select boundary results lowest density steps repeated find boundaries necessary 
maximization algorithm follows steps computes density regions main diagonal concludes selecting boundary corresponds maximum density score 
graphical example steps minimization algorithm clearer 
algorithm posited boundary step 
boundary divides dotplot shown points plotted simplicity regions 
regions potential topic segments lie main diagonal 
regions lie main diagonal potential topic segments 
step algorithm count number points regions region region region region position boundary graphical illustration working optimization algorithm 
shaded divide number combined area regions yield density score 
score location boundary recorded step 
process repeat remaining potential boundaries 
location gave rise minimum density score selected best site topic boundary 
shows situation boundary identified search second boundary way 
step algorithm sum number points plotted regions shaded compute density areas dividing sum combined area regions 
step density location putative boundary recorded 
minimization versus maximization minimization maximization algorithms similar yield different results seen simple example 
consider word text algorithm identifies best boundary maximizing self similarity posits boundary position boundary identified iteration position putative boundary graphical illustration working optimization algorithm boundary identified 
second algorithm places boundaries minimizing similarity regions predicts boundary final table shows density scores placing boundaries possible position algorithms 
symbol represents location hypothesized boundary 
density score associated best boundary algorithm shown bold 
putative segmentation minimization score maximization score table application optimization algorithms topic segmentation sample text formal description algorithm order formally specify algorithm minimizes outside density define variables describe compute values variables 
variables describe algorithm maximizes selfsimilarity 
give specification algorithm trivially different minimization algorithm 
define element vector index 
document segmented 
assume words long 
vector containing word tokens word second word concluding final word vector indices corresponding location topic boundaries 
sorted ascending order 
initially contains implicit boundary start document 
vector containing potential boundaries 
element index contains locations sentence paragraph boundaries 
vector containing number word tokens word type 

different vectors created needed compute values defined 
dimensional array th row array 
th element vector 
vector inserted sorted ascending order 
dimensionality jaj jbj 
rows vector iteration algorithm 
vector 
number elements number elements turn number rows jaj jp gamma delta gamma gamma gamma argmin 
minimum density achieved putative boundaries 
index index minimum density 
position boundary gives rise minimum density 
iteration algorithm finds best boundary determining value identifying value algorithm updates vector boundaries contain elements 
element removed vector algorithm rerun manner desired number boundaries located 
variables described remain running algorithm 
vector grows size new boundaries added vectors decrease size element iteration algorithm 
dimensionality changes accordance changes size numerator equation compute values dot product word vectors associated regions document denominator product number words regions upper bound numerator 
maximum value numerator occurs section contains tokens single type 
values densities numerator formula number points particular region denominator area region dotplot 
depicts density regions main diagonal boundary placed location axis 
data derived dotplot shown 
boundary identified data graph boundary position gives rise lowest density 
iteration graph updated reflect presence boundary 
similarity vector space model dot product formula formula reveals similarity optimization algorithm texttiling hearst 
crucial difference lies global nature approach 
hearst algorithm identifies boundaries comparing neighboring regions minimization technique compares region regions 
maximization algorithm similar texttiling essentially local 
density word position outside density plot concatenated wall street journal articles 
evaluation tested variants optimization technique corpus concatenated wall street journal articles 
parameterized follows ffl density regions main diagonal maximized max density points outside regions diagonal minimized min 
ffl boundaries constrained lie sentences sent paragraphs para 
ffl density computation performed sentences words measure area 
parameters self explanatory third requires description 
sentence length varies greatly wall street journal 
hearst addressed problem texttiling dividing documents equal length pseudo sentences 
handle length variation changing units denominator density formula words sentences 
forces sentences treated equally regardless number words contain 
example clarify difference settings 
assume region text contains sentences total words second region contains technique unit analysis area words max sent max para max sent max para min sent min para min sent min para table results variants optimization algorithm tested concatenations pairs wall street journal articles 
reduce words roots ignore frequent words 
sentences words 
method compute density points region calculating dot product word vectors region dividing value product words words 
method divide dot product product sentences sentences 
table presents results versions optimization algorithm task identifying single boundary corpus concatenations pairs wall street journal articles 
case hearst youmans techniques beneficial restrict boundaries lie paragraphs 
accounting variations sentence length hurt performance third fourth seventh eighth lines table show 
maximization algorithm performed better minimization algorithm 
maximization technique outperformed youmans vmp hearst technique measured number exactly correct boundaries identified number words correct location 
optional normalization ways optimization algorithms deal words meant ignored list 
intuitive simply disallow ignore ignore ignore ignore ignore ignore ways handle ignored words 
left dotplot ignored words may participate matches 
dotplot right ignored words eliminated 
matches ignored words 
case number words document affected ignoring closed class words 
second possibility remove ignored words reducing number words document 
sample method shown 
preliminary experiments removing ignored words entirely performed best 
result results method 
comparing scores table table reveals performance parameterizations dotplot technique improved words words list ignored 
constraining boundaries lie paragraph boundaries improved performance optional normalizations performed 
contrary results optional normalization minimization algorithm consistently outperformed maximization algorithm identical parameter settings 
minimization algorithm identified exact matches words correct location far texttiling vmp algorithm compression technique 
technique unit analysis area words max sent max para max sent max para min sent min para min sent min para table results variants optimization technique tested concatenations pairs wall street journal articles 
data preprocessed reduce words roots ignore frequent words 
results suggest things 
possible boundaries placed paragraphs 
second normalization crucial 
third computing density words unit area accurate 
able normalize text best minimization algorithm 
maximization algorithm appropriate 
word frequency algorithm goal language modeling algorithms estimate probability sequence words 
burstiness phenomenon language model take account 
words considered bursty appearance document indicator additional occurrences 
put way word bursty occur additional times document implied simply frequency collection documents church gale 
example word appear document contains time presumably crucial document topic 
similarly frequent topic word somewhat exhibits burstiness documents somewhat way particular general 
terms language modeling seeing instance bursty word boost probability seeing instance word 
determine topic boundary appears neighboring blocks text document language model 
refer block text prior putative boundary block text boundary block 
language model accounts burstiness determine blocks topic boundary separating 
language model compute probability seeing words block continuation block compute probability block conditioned block 
compute probability seeing words block new segment independent block 
perform computations language model 
difference language model preceding context plays role 
probability generating words block sufficiently greater conditioning words block conditioning words blocks text probably topic putative boundary 
blocks different topics proposed boundary 
probability conditioned block greater blocks topic generally additional instances bursty words occur bursty words occur chance block 
brief qualitative example clarify idea 
suppose document known topics bursty words distributed shown table 
scanning list paragraphs words paragraph conclude location boundary topics paragraphs vocabulary paragraph considerably different vocabulary paragraphs 
hope algorithm language model determine best location boundary lie paragraphs 
paragraph words table example distribution bursty words document 
model church gale document collection compare number documents containing particular words expected number documents words appear frequency approximated poisson church gale 
concluded useful independently measure document frequency poisson poorly predicts 
poisson single parameter model unable capture dependent relationships word frequency hidden variables topic 
may account discrepancy predicted observed number documents containing particular words 
church gale showed negative binomial katz mixture parameter models better predict document frequency poisson 
advantage mixture model negative binomial parameters easier estimate 
probability seeing instances word document mixture model gamma ff ffi ff fi fi fi ff fi subscripted indicate pertinence particular word sake readability omit subscripts 
ffi value equals 
katz proposed model improvement mixture model church gale described katz 
designed model predict number occurrences content words phrases documents demonstrated predicted number occurrences word phrases document collection accurately negative binomial mixture model 
model compute probability seeing words particular order predicts probability seeing particular bag words 
result context impact probability generating particular word language models 
trigram language models assume language second order markov process 
simplification permits probability word conditioned solely preceding words jw gamma gamma 
model different stronger assumption probability generating particular words completely independent surrounding words 
model assumes word probability depend document length 
katz defends stating number instances specific content word particular document explicitly depend document length function document concept expressed named word 
short document may contain instances content word names concept essential document longer document having occasional concept single instance word 
average longer documents containing particular content word usually instances word shorter documents 
katz katz divided words categories number times appeared particular document 
labeled word occurred assumed word central topic discussed occur 
words occured called topical words 
possible truly words may occur document unequivocally topical words may occur time model account phenomena 
view word repetition led katz propose model parameters word 
parameter intuitive explanation 
ff represents probability word appears document time 
fl probability word appears appears 
fl measures degree topicality word 
number times average word appears document appears 
parameters estimated easily corpus 
katz estimated collection technical documents varied widely length 
pw probability model seeing instances word document 
computation pw divided cases 
corresponds seeing instances word second represents probability seeing exactly instance third probability seeing occurrences values greater 
pw gamma ff pw ff gamma fl pw fffl gamma gamma gamma gamma word frequency algorithm specification write probability seeing instances occurrences word independent preceding context simply pw probability context written pw 
probability generating words block product probabilities generating appropriate number occurrences word type 
call probability generating words context previous text treat words blocks segment regard language model 
probability generating words block independent block called blocks different topic segments 
formulae shown 
pw pw experiments compute blocks text words long 
examining potential boundaries near document may sufficient text block contain words 
event reduce size blocks 
chose block size length average topic segment annotators identified primary evaluation corpus hub corpus 
compare determine topic boundary 
word probabilities small perform word probability calculations log domain compare differences log probabilities ratios probabilities 
normalize log probabilities dividing number words block 
identical th root product probabilities outside log domain yields average probability ratio word 
facilitates thresholds determine boundary block size 
compute pw pw formulae katz model described 
computing pw requires knowledge number words type block 
compute count number word tokens type block call compute pw formulae model 
compute product probabilities word type 
computing pw difficult requires define conditional probabilities 
divide conditional probabilities cases correspond combinations number appearances particular word block number appearances block 
table shows conditional probabilities cases 
symbol number table means occurrences 
practice need conditional probability listed table case occurrences block occurrences block 
ignore conditional probability need compute probabilities words appear blocks 
rows table simply probabilities seeing number occurrences block model 
having seen occurrences block provide information observed block conditional probability simply probability original model 
occurrences block occurrences block conditional probability gamma ff ff gamma fl ff gamma gamma gamma gamma gamma fl fl gamma gamma gamma gamma gamma gamma gamma gamma table conditional probabilities seeing particular number occurrences word block certain number observed block 
number occurrences blocks combined 
normalization constant discussed text 
conditional probabilities summed possible outcomes naturally total 
cases occurrences block sum 
proving demonstrate soundness probability model gamma ff ff gamma fl fffl gamma gamma gamma gamma gamma ff ff gamma fffl fffl gamma gamma gamma gamma changing variable summation gamma fffl fffl gamma gamma gamma identity gammaq gamma fffl fffl gamma gamma gamma gamma gamma fffl fffl gamma gamma gamma fffl fffl observed instance word knowledge total number possibilities additional instances additional occurrences 
conditional probability additional occurrences total occurrence gamma fl 
explained ways 
definition fl measures probability seeing word seen time 
gamma fl probability seeing word 
second explanation having seen occurrence impossible see zero occurrences total 
means terms regarding exactly occurrence occurrences formulae model relevant 
terms sum ff conditional probabilities sum 
dividing term ff normalizes probabilities sum 
term exactly occurrence model ff gamma fl gamma fl 
trick dividing ff determine conditional probability additional occurrences 
probability seeing occurrences fffl gamma gamma gamma gamma dividing ff yields fl gamma gamma gamma gamma conditional probability seeing instances 
conditional probability seeing exactly instances represented single term summation fl gamma gamma gamma gamma observed occurrences word conditional probability additional occurrences depends number observed somewhat differently previous cases 
final term formula model relevant computation conditional probabilities case 
assuming exactly repetitions occurred block normalize probability encountering total occurrences accounts aggregate probability equal fffl dividing fffl yield formula table 
additional normalization required observed occurrences block probability occurrences accounts fffl original probability mass account probability numbers repetitions number block 
case tally probability call compute conditional probabilities dividing fffl gamma gamma gamma gamma locate topic boundaries word frequency statistic compute ratio log domain difference log gamma log 
compare difference threshold 
natural threshold log domain difference log probabilities equally 
difference greater posit boundary word sequence generated context preceding segment 
propose boundary probable word sequence generated independent preceding text 
identify reliable boundaries decreasing threshold improving precision expense recall 
similarly raising threshold improve recall reduce precision 
impact individual words instructive consider contributions individual words ratio write probability single word need consider cases cases occurrences word occurred block equal remaining cases occurrence word block block occurrence word block block occurrences block block 
variables cases number occurrences word block 
number occurrences word block 
number occurrences word blocks 
case single occurrence particular word type block occurrences word type block 
table gamma fl probability independently generating region containing zero instances word simply gamma ff ratio probabilities gamma fl gamma ff case occurrence word block occurrence block 
table fl gamma gamma gamma gamma subcases correspond number occurrences block 
probability seeing exactly instance case independent segment ff gamma fl probability seeing instances case fffl gamma gamma gamma gamma result probability ratios 
case fl gamma gamma gamma gamma ff gamma fl fl ff gamma fl gamma gamma gamma gamma case fl gamma gamma gamma gamma fffl gamma gamma gamma gamma ff gamma gamma case instances word block 
probability seeing occurrences block continuation block gamma gamma gamma gamma gamma gamma gamma sub cases 
pertain instances block instance block instances block 
probability case instances block arising independently gamma ff probability instance occurred case ff gamma fl probability occurrences case fffl gamma gamma gamma gamma probability ratios sub cases 
case gamma gamma gamma gamma gamma ff gamma gamma ff gamma gamma gamma case gamma gamma gamma gamma ff gamma fl ff gamma fl gamma gamma gamma gamma case gamma gamma gamma gamma fffl gamma gamma gamma gamma fffl gamma gamma parameter case block block ff fl na na na table effect increasing parameter values ratio effect perturbing parameters table shows effect ratio altering parameter associated particular word holding constant 
table indicates likelihood topic boundary solely number occurrences increases parameter increased means topic boundary 
table shows probability occurrence ff increases likelihood boundary decreases assuming appears block block 
case particular word greater chance appear independently block 
cases correspond instances block increasing ff increases likelihood boundary occur provide weak evidence blocks segment 
increasing fl probability word topically effect cases 
case increasing improves chance blocks topic segment additional occurrences continuations topic 
cases increasing fl increases likelihood topic boundary multiple occurrences particular occur single segment making relatively new topic segment instances word occurred previous segment continued additional uses word 
altering value impact case 
word multiple occurrences segment 
explains increasing decreases likelihood boundary cases 
remaining cases instances word block increased probability boundary increases 
parameter estimation estimated parameters word frequency model corpus approximately words wall street journal text 
average document contained words 
statistical natural language processing algorithms unknown words words training corpus occur test data pose problem measure word frequency 
account unknown words applied simplified turing gt smoothing number documents word appeared gale sampson 
gt smoothing redistributes probability mass items number times appear sample 
words training corpus probability discounted gt smoothing probability mass reserved unseen words 
turing proposal accounting unseen events assumed number unseen words identical number observed word types distributed probability mass equally 
gt smoothing partially solves problem handling unseen events model parameters additional smoothing necessary 
gt smoothing addressed difficulties caused values ff parameter values parameter fl problematic 
value fl estimated training corpus value computable 
estimated values fl counting number times word type appeared times document 
smoothed counts averaging average counts randomly selected content words order eliminate problems estimating fl average counts determine parameter values unknown words 
table shows ranges parameters estimated training corpus smoothing 
dan melamed implementation turing smoothing 
parameter minimum maximum ff gamma fl gamma table range parameters model estimated wall street journal training corpus turing smoothing applied 
unit analysis words sent para table results word frequency algorithm guess location boundary 
data consisted concatenations pairs wall street journal articles 
evaluation tested word frequency algorithm corpus randomly selected concatenations pairs wall street journal articles 
table presents algorithm performance text normalized ignoring words list reducing words roots 
evaluate technique normalization model intended predict frequency open class words 
initially experimenting model data observed frequently guessing early late concatenation 
address problem allow boundaries placed close concatenations 
hypothesized boundaries restricted lie sentences algorithm permitted propose boundary third sentence placed final sentences 
similarly guesses forced lie paragraphs placed third paragraph final paragraphs 
restriction prevented guesses occurring early late occasionally caused legitimate boundaries discarded 
compression algorithm optimization algorithm algorithm requires training data 
result technique better model topic boundary detection tested data source 
evaluation test data source training data time period 
difference topics discussed changes writing style may responsible surprising fact algorithm performed slightly poorer optimization algorithm 
training corpus required drawback word frequency algorithm dependence training data 
method relatively insensitive training corpora drawback minor 
test sensitivity model quantity quality training text retrain different quantities data corpora containing text sources 
testing model robustness way chose answer radical question happens effectively training data 
order compute probability seeing word particular number times model uses values parameters word 
estimate sensible values parameters required smoothing 
smoothing method employed turing smoothing allowed estimate ff parameters unknown words 
additional ad hoc smoothing permitted estimate values fl final smoothing step averaged parameters associated randomly selected content words 
test performance effectively little training data discarded probabilities estimated words training corpus relied solely parameters estimated unknown words 
clear mean ignored identities individual words testing parameters word frequency model word type identical 
parameters word type estimated unknown words primarily smoothing 
table presents results evaluation version word frequency algorithm concatenations wall street journal articles 
algorithm parameters unknown words performed surprisingly 
scored percent fewer exact matches word frequency algorithm 
unit analysis words sent para table results word frequency algorithm parameters unknown words 
data consisted concatenations pairs wall street journal articles 
algorithm training data performs slightly poorer optimization algorithm number advantages 
training data take advantage 
useful segmenting documents domains word repetition indicator structure 
second iterative optimization algorithm identify number boundaries single pass 
third completely local costly compute minimization version optimization algorithm 
due advantages output algorithm statistical model section incorporates topic segmentation clues chapter 
maximum entropy model word frequency indicator topic shift 
chapter described number indicators including previously 
incorporated number features statistical model built ratnaparkhi maximum entropy modeling tools ratnaparkhi 
model predicts probability topic boundary particular location document features surrounding context 
uses features ffl word frequency algorithm suggest topic boundary 
ffl domain cues categories table 
ffl word bigrams occurred region region putative topic boundary 
theirs table pronouns indicators topic boundaries 
ffl named entities common regions putative topic boundary 
ffl words regions synonyms wordnet 
ffl percentage words region putative boundary time 
ffl pronouns list shown table words potential topic boundary 
ffl words previous segment 
trained model file subset hub broadcast news corpus chapter evaluation 
files contained total topic segments annotated linguistic data consortium 
training subset contain files relevant queries constituted spoken document retrieval task 
chose files subset files manually identified cue phrases randomly selecting documents broadcast sources 
train washington journal stories identifying cue phrases observed sources structured differently hub corpus 
documents contained topic shifts washington journal documents shifted topic frequently 
test model concatenations wall street journal articles features useful identifying topic boundaries corpus 
leave evaluation model chapter 
matches distance baseline texttiling vector space vmp compression optimization word frequency performance best performing version algorithm 
perfect score exact matches distance graph 
performance conclude chapter graph showing best performing version algorithm baseline performance document concatenation task 
note optimization algorithm word frequency model perform better baseline algorithm compression technique vmp texttiling best vector space models 
chapter evaluation evaluation described chapter somewhat unrealistic knew number topic segment boundaries advance meant program algorithms select set number boundaries 
result measured performance computing accuracy number guessed boundaries matched actual boundaries 
evaluating text structuring systems realistic data difficult 
challenge selecting appropriate performance measures 
number boundaries known advance algorithms determine posit 
consequently accuracy uninformative measure perfect accuracy achieved proposing boundary potential location 
challenge choosing appropriate corpora evaluation 
solutions problem 
available annotated corpora case reliability annotation determined new corpus annotated 
discuss evaluation metrics decisions regarding corpora 
sections evaluations algorithms various corpora evaluation metrics 
performance measures number researchers precision recall evaluate text discourse segmentation algorithms hearst litman metrics drawbacks 
overly strict hypothesized boundary close actual segment boundary equally detrimental performance far boundary 
address litman proposed allowing fuzzy boundaries annotators agreed boundary utterance range disagreed exact location litman 
suggested counting boundaries correct appeared fixed size window words actual boundary previous chapter reynar 
approach solves problem adequately 
approaches produce performance figures overly generous weight inexact matches exact ones 
precision recall frequently exchanged 
algorithms tuned increase recall exchange loss precision vice versa 
means full knowledge performance algorithm requires precision recall graph measure combines precision recall 
various combination methods proposed ir literature including precision recall product measure 
beeferman berger lafferty proposed new performance measure solves problems 
metric weights exact matches near misses yields single score beeferman 
suggested measuring performance determining probability randomly selected sentences units text located similarly algorithm segmentation segmentation 
similarly located means sentences topic segment segmentation segment hypothesized segmentation 
alternatively different segments answer key different segments hypothesized segmentation 
beeferman proposed formula compute probability corpus containing sentences 
ref hyp ijn ffi ref phi ffi hyp fl exponential distribution mean 
beeferman determined average topic segment length collection 
fl normalization constant normalizes probability distribution 
effectively weights comparisons closer sentences highly computation ffi hyp ffi ref binary valued functions sentences topic segment appropriate corpus 
symbol phi represents xnor function arguments equal 
beeferman observed computing metric requires knowledge collection value chosen 
presence constant dependent collection formula means performance figures different collections may comparable 
instance safe say algorithm scores collection better scores different collection 
second disadvantage metric exists designed evaluate segmentation algorithms single files containing hundreds concatenated stories 
straightforward compute aggregate precision recall segmenting collections documents different files obvious combine scores probabilistic metric 
files concatenated prior computing score distort evaluation algorithms credit guessing boundaries files known advance 
believe method evaluation useful proposed date appropriate corpora 
comparison human annotation difficult time consuming annotate corpora topic boundaries 
detailed instructions developed nakatani nakatani written annotators expected label consistently 
specific instructions annotators agree location topic boundaries samples text speech litman hirschberg grosz 
collection documents annotated problem remains determining annotations consistent useful 
carletta proposed abc abc world news abc world news tonight cnn early edition cnn early prime news cnn headline news cnn news cnn world today span washington journal npr things considered npr pri marketplace table news programs hub broadcast news corpus 
kappa statistic widely field content analysis measure agreement carletta 
kappa statistic defined gammap gammap fraction times annotators agree fraction expected agree chance 
content analysis kappa score greater indicates high reliability score indicates tentative reliability 
scores mean judgments inconsistent 
performance algorithms described previous chapter corpus annotated topic boundaries 
prior testing algorithms corpus measured reliability annotation kappa statistic 
broadcast news hub broadcast news corpus trec spoken document retrieval task composed radio television news broadcasts eleven sources 
sources encompass number different formats focus various aspects news coverage 
cnn headline news contains short summaries current news items depth analysis 
national public radio show marketplace reports exclusively financial news contains brief detailed stories 
abc covers issues hour long program 
sources exemplify amount variation average story length subject matter corpus 
table contains list programs corpus 
ldc recorded audio portion news broadcast produced types text transcript recording 
people manually transcribed speech broadcast 
second transcripts automatically generated speech recognizer 
speech recognized human transcribed versions lacked punctuation labeler potential boundary sites story segments filler segments unlabeled sites ldc reynar table statistics hub corpus annotation 
indications sentence paragraph boundaries normal capitalization 
text annotated boundaries topic segments annotators perceived 
annotators identified number segment types kinds segments significant evaluation 
annotators labeled sections broadcasts self contained limited single news item type story 
marked smaller segments contained information upcoming stories self contained subjects type filler 
ldc filtered segment types commercials traffic reports 
shows example labeling 
agreement original annotation performed linguistic data consortium guidelines annotate corpus relatively brief somewhat ambiguous relabeled corpus compared annotations 
note removed ldc annotation prior corpus 
annotating corpus examined annotated data write script remove segment markup 
identified domain cues corpus 
table presents statistics corpus annotations 
quite similar ldc annotation terms number segments type labeled total number segments identified 
largest difference figures percent 
kappa statistic determine reliability agreement annotations 
table presents statistics 
computed kappa statistic ways 
measured reliability agreement annotations story filler segments tentatively reliable kappa score ranges example hub corpus showing annotation topic segments produced ldc 
vertical whitespace indicate changes speaker background recording condition 
units kappa score separate story filler combined table reliability annotation hub corpus 
content analysis 
kappa score labeling story segments tentatively reliable 
kappa score annotating filler segments reliable 
kappa score labeling segments filler story categories fell tentatively reliable range 
score higher score measured treating filler story segments separately 
result differentiate segment types experiments describe 
structuring hub corpus hub corpus consists files 
files manually identify cue phrases files train maximum entropy model text segmentation 
segmented remaining files algorithms restricted posit topic boundaries particular sites 
addition annotating corpus topic boundaries ldc annotators divided corpus units criteria significant speech recognition 
annotation units meant facilitate experiments determine aspects broadcast presence background music speech spontaneous planned significantly impacted speech recognition ir performance 
text segmentation algorithms placed topic boundaries units paragraphs sentences labeled 
units designed speech recognition experiments varied greatly length 
contained words occasionally hundreds words long 
number units typical topic segment depended heavily source data varied widely 
result evaluate text segmentation morphology algorithm normalization precision recall random guess guess texttiling texttiling optimization optimization publicly available version hearst texttiling algorithm produced output long document 
document eliminated test set recall improved slightly precision remained 
file processed publicly available version texttiling 
performance excluding file precision recall 
table performance various algorithms files hub broadcast news corpus 
algorithms data precision recall metric beeferman proposed 
precision recall permits easily aggregate performance scores measured individual documents 
table presents performance text structuring algorithms test data hub corpus 
measured precision recall ldc annotation story filler boundaries merged single category indicating topic shift 
evaluated texttiling optimization algorithm ignoring words list converting words lemmas 
measured performance baseline algorithms 
algorithm randomly selected boundaries second posited possible boundaries scored perfectly terms recall 
hearst texttiling algorithm slightly better terms precision considerably better terms recall guessing randomly 
correctly identifies nearly twice boundaries normalized unnormalized text 
optimization algorithm outperformed guessing randomly texttiling 
test compression algorithm youmans vmp technique performed poorly document concatenation task 
precision recall wf random precision recall curve algorithms wf tested hub news broadcast data 
lone point represents baseline performance 
presents performance word frequency algorithm called wf maximum entropy model uses output wf additional cues 
precision recall graph shows performance algorithms normalization performed 
performance wf considerably better algorithms listed table 
point precision recall nearly equal 
performed better precision equal recall approximately 
performance training data algorithm wf outperformed optimization algorithm transcribed hub data 
disadvantage wf noted earlier requires training corpus estimate parameters associated word 
means apply wf text languages genres different word frequencies optimization algorithm 
address evaluated model words treated unknown words discussed previous chapter 
performance version wf hub corpus shown precision recall graph 
quite surprising method identifying topic boundaries performs 
outperform baseline algorithm outperforms texttiling optimization algorithm 
fact performs nearly wf word frequency statistics 
factors account performance version wf 
dotplot algorithm performs moderately corpus prior information word frequency conclude word frequency crucial correctly identifying boundaries 
factor garnered word frequency statistics wall street journal articles exhibit similar identical word distributions broadcast news sources 
word frequency statistics broadcast news corpus probably widen gap version wf version available statistics 
believe burstiness wf tracks accounts performance wf evaluated accurate statistics 
performance simply due word repetition texttiling optimization algorithm perform nearly 
speech recognized broadcast news access speech recognized transcripts hub corpus sdr task 
data sdr task available 
sdr corpus consisted subset files hub corpus 
crucial difference annotation versions 
segment corpus points background conditions important speech recognition changed annotators divided corpus segments speaker changes 
useful division speech recognition systems recognize speaker changes reliably 
recognizing changes related studied problem determining identity speaker sample speech 
note restricting topic boundaries lie speaker turn changes eliminate precision recall wf wf unknown baseline precision recall curve algorithm wf data hub corpus words corpus treated unknown 
performance original wf algorithm shown comparison 
possibility perfect performance single speaker may discuss different topics 
due change format version algorithm trained data retrain limited number speech recognized files available 
consequently presents performance algorithm wf parameters 
shows performance baseline algorithm randomly selects boundaries performance wf manual transcriptions documents 
wf outperforms baseline algorithm speech recognized data 
performance speech recognized transcripts poorer performance manually produced transcripts considerably better baseline 
precision recall baseline speech recognized transcripts precision recall curve hub news broadcast data 
performance shown algorithm wf speech recognized data manual transcriptions data 
baseline performance shown 
spanish news broadcasts evaluated usefulness wf broadcast news data language english 
portion hub corpus spanish broadcast news 
data transcribed divided units changes speaker annotated topic boundaries way english language data 
conflated filler story categories spanish data just english hub data distinguishing english data problematic 
shows performance wf corpus 
evaluate little data separate requisite training test portions 
case english data wf spanish language data performs better baseline 
segment data version wf rely word frequency statistics treated word types unique unknown words 
anticipate performance precision recall baseline wf precision recall curve spanish news broadcast data hub corpus 
improve corpus spanish news broadcasts learn word probabilities tools eliminate closed class words data 
topic detection tracking corpus topic detection tracking tdt evaluation contains task similar document boundary identification task chapter 
topic tracking process progression news stories time usually data news feed 
boundaries articles news feed generally annotated tdt pilot evaluation included document segmentation task 
goal identify boundaries articles relying markup 
task included evaluation eventually conducted speech recognized news broadcasts divided segments prior tracking topics 
algorithm probabilistic metric precision recall wf table performance algorithms wf test portion tdt corpus 
significant difference tdt corpus hub corpus discussed previous section composition 
tdt corpus contains broadcast material transcribed cnn text reuters newswire 
documents hub corpus stories sources accurate punctuation capitalization 
tdt corpus contains approximately words text 
divided corpus training test portions 
training corpus development contained approximately words test corpus contained roughly words 
corpus annotated paragraph boundaries facilitate comparison algorithm beeferman identified sentence boundaries statistical sentence boundary disambiguation algorithm prior testing algorithms reynar ratnaparkhi 
performance tdt corpus evaluation metric tdt pilot evaluation beeferman berger lafferty probabilistic metric 
table presents performance algorithms wf test corpus measured probabilistic metric precision recall 
wf performed metric beeferman proposed fared poorly terms precision recall 
performance algorithms compared favorably algorithm beeferman 
tested versions algorithm trained broadcast news data outside tdt corpus trained words tdt data 
algorithm scored metric tested words tdt data second scored 
note algorithm wf scored trained broadcast news data set value beeferman 
doug beeferman providing scoring software 
trained small quantity transcribed spoken data differed significantly tdt data 
adapting algorithm wf tdt data poor performance wf terms precision recall led examine performance training portion tdt corpus 
wf postulated number boundaries immediate vicinity actual boundaries 
happened testing tdt corpus hub corpus reasons 
wf posited boundaries units hub corpus twice long sentences algorithmically identified tdt corpus 
result changes vocabulary affected similarity scores potential boundaries tdt corpus 
hub corpus lacked punctuation contained greater proportion open class words tdt corpus 
open class words computation similarity scores result fewer indicators segmentation wf exploit tdt corpus hub corpus 
address modified algorithm wf reduce number boundaries guessed close proximity 
tried selecting highest scoring boundary contiguous hypothesized boundaries selecting boundaries local maxima imposing minimum separation guessed boundaries 
selecting highest scoring boundary neighboring boundaries improved performance training corpus 
result evaluated method test corpus 
table presents evaluation algorithm tdt test corpus 
updated algorithm performed better version beeferman algorithm trained tdt data quite version trained tdt data 
inducing domain cues tdt corpus exclusively contain broadcast material domain cues identified hand model solely drawn broadcasts 
address induced domain cues training corpus 
automatically determined word unigrams bigrams trigrams occurred frequently immediately barker susan reed susan king skip tokyo dallas chicago table best cue phrases induced tdt training corpus 
algorithm probabilistic metric precision recall wf table performance algorithm wf modified select best boundary neighboring hypothesized boundaries trained cues induced tdt training corpus 
boundaries 
domain cues identified occurred times training corpus times frequently immediately boundaries training corpus 
domain cues identified exclusively cnn portion tdt corpus 
despite drawback induced domain cues confirmed selected manually hub corpus appropriate types 
best induced cues occurred frequently context boundaries contained reporters names station identifiers places reporters broadcast 
table shows best domain cues 
table presents performance model trained training portion tdt corpus domain cues output wf indicators structure previously performance wf modified tdt data better model proposed beeferman slightly poorer second model trained large amount broadcast data 
model precise indicator probabilistic metric precision recall word frequency cue phrases named entities bigrams synonyms uses pronouns table performance algorithm trained training portion tdt corpus indicators text structure listed row 
models 
achieved precision best model scored 
value different indicators determine importance features model trained model features measured performance test portion tdt corpus 
table shows results tests 
precision recall scores beeferman reported beeferman may slight overestimates due glitch scoring software lafferty 
measured precision recall independent scorer 
computation probabilistic metric affected glitch 
performance suffers greatly output word frequency model cue phrases omitted 
named entities improved recall single percentage point precision remained constant 
removing indicators altered balance precision recall way precision rose recall declined 
performance probabilistic measure declined indicators removed model 
results conclude indicators useful data word frequency statistics cue phrases far away helpful 
findings particular tdt data demonstrate cues selected useful identifying topic boundaries 
recovering authorial structure authors endow types documents structure write 
may divide documents chapters chapters sections sections subsections forth 
exploit structures evaluate topic segmentation techniques comparing algorithmic determinations structure author original divisions 
method evaluation viable numerous documents available electronic form 
document concatenation task evaluating algorithms way labor intensive task annotating corpus 
tested algorithm wf randomly selected texts project gutenberg 
texts thomas common sense published volume decline fall roman empire edward gibbon book herman melville classic moby dick 
permitted algorithm guess boundaries paragraphs marked blank lines document 
assessing performance violated earlier recommendation set number boundaries guessed number authors identified 
result evaluation focuses solely algorithm ability rank candidate boundaries determining boundaries select 
evaluate wf random boundaries acc 
prob 
acc 
prob 
common sense decline fall roman empire moby dick combined na na table accuracy algorithm wf works literature 
columns labeled acc 
indicate accuracy labeled prob 
show performance probabilistic metric beeferman performance computed accuracy algorithm guesses compared chapter boundaries authors identified 
measured performance probabilistic metric beeferman proposed 
documents evaluation may contained legitimate topic boundaries correspond chapter boundaries scored guesses boundaries incorrect 
table presents results works 
algorithm performed better baseline algorithm randomly assigned boundaries documents common sense 
common sense small number chapters poor performance document significant poor performance longer works 
performance works significantly better chance ranged improvement factor accuracy baseline factor nearly lengthy decline fall roman empire 
results suggest algorithm wf recover authorial structure suggest authors divide documents segments write 
demonstrated word frequency algorithm wf useful segmenting number different types documents 
outperformed optimization algorithm texttiling hub broadcast news corpus 
important evaluations performed uses text segmentation pertain broadcast documents 
wf performed tdt corpus evaluation slightly interesting tdt corpus contains broadcast documents interspersed reuters data 
demonstrated algorithm utility independent training corpora spanish language data english data frequency statistics available 
showed wf useful recovering chapter divisions works literature 
algorithm performed better wf hub data greatly improved precision tdt corpus 
additional training data performance hub corpus improve 
tests conducted hub corpus trained documents various broadcast sources idiosyncratic hints structure 
knew source documents segmented build source specific models 
models cognizant names reporters frequently program particular stylistic conventions employed 
demonstrated algorithmically identify domain specific cues verified appropriateness domain cues identified hand learning cue words phrases tdt corpus 
chapter applications automatic techniques linearly structuring text potential uses 
discuss number including areas confirmed useful segmentations produced algorithms information retrieval coreference resolution language modeling 
information retrieval information retrieval task identifying documents collection satisfy user request information particular subject 
documents meeting criterion come known relevant documents despite discrepancy usual usage relevant meaning related implied documents specifically topic study harter 
general process information retrieval proceeds follows user formulates query usually natural language logical language uses boolean connectives 
ir system searches collection frequently precompiled index identify relevant documents 
system presents list documents user perusal 
returned documents may ranked order relevance retrieval technique employed computes similarity score 
generally boolean systems divide collection sets containing relevant irrelevant documents systems employ similarity metric rank documents metric 
common belief information retrieval performance improved documents especially long ones divided sections ir systems indexed sections separate documents 
belief arises word usage drives ir depends topic 
mentioned earlier number times word occurs document depends topic document 
short documents lengthy ones address topic 
documents pertain multiple topics sufficiently different facets single topic vocabulary change document topic shifts 
result word frequency statistics collected entire documents driving force ir algorithms may unrepresentative single topic section 
useful statistics taken individual topic segments 
example document hub broadcast news corpus short segment travels olympic torch 
query associated collection torch travel motorcycle 
query exhibit similarity entire document fact word motorcycle occurs words 
query similar section olympic torch 
section words long motorcycle occurs torch appears times appears entire document 
assuming function words eliminated content words common document query motorcycle torch similarity score query document cosine distance measure score relevant subsection greater 
difference scores demonstrates usefulness sectioning documents prior comparing queries vector space model 
previous attempts show ir systems perform better units text smaller documents indexed 
units varied sentences paragraphs fixed length blocks varying sizes subtopic segments 
stanfill waltz stanfill waltz demonstrated ir benefited indexing word segments 
tested information retrieval system ran massively parallel connection machine collection news articles precision recall product stanfill waltz 
hearst plaunt hearst plaunt texttiling form term weighting known term frequency inverse document frequency weighting tf delta idf structure texts prior ir 
tf delta idf weights words occur documents collection highly occur documents 
vector space model compute similarity values word vectors weights merely number times word appears document 
tf delta idf weighting weight document word ik depends number times word appears document referred tf ik number documents word appears collection consisting documents called equation determines weight associated word 
ik tf ik delta log tf ik delta log hearst plaunt segmented text texttiling indexed subtopic segment separately ir system 
measured performance improvements percent 
obtained similar improvements collection documents indexed paragraphs segments texttiling identified hearst plaunt 
salton allen buckley salton allen buckley improved performance ir system incorporating information passages salton 
method involved steps 
vector space model identify documents globally similar query 
vector space model compute similarity query individual sentences paragraphs text 
globally similar text contain relevant passages assumed erroneously identified step 
precision improved percent method 
recall improve technique performance measured relative set documents identified pass 
recall improve second pass need identify additional relevant documents 
callan callan studied effect indexing passages inquery information retrieval system callan probabilistic ir system identifies relevant documents bayesian networks callan 
suggested reasons units smaller documents 
portion text passage ranked interface quickly direct user relevant information document 
second long documents documents complex structure short documents summarizing subjects challenge algorithms distinguish document text matches query 
algorithm distinguish matches scattered document dense region matches may difficulty retrieving long documents newswire news summaries callan suggested types units exploited ir system discourse units units document paragraphs sections identified explicitly document author 
semantic units units document explicitly marked document author divide document non overlapping regions 
fixed size units units document content authorial annotation solely predetermined word window size 
windows may may permitted overlap 
callan tested ir performance units third types 
concluded passages overlapping word windows useful ir comprised paragraphs 
zobel zobel compared number passage retrieval techniques federal register collection kaszkiel zobel 
evaluated retrieval performance cosine distance measure tf delta idf documents paragraphs pages documents sections documents tiles produced texttiling fixed length variable length passages text windows sizes 
evaluated vector space model pivoted document length normalization singhal set units 
discuss pivoted document length normalization section 
drawn wide range experiments zobel conducted 
pivoted document length normalization generally improves performance simply tf delta idf 
second retrieving documents fixed length non overlapping windows words improves performance considerably percent terms average precision 
best performance consistently comes fixed length passages starting word document 
passages sizes words improved performance percent 
improvements come indexing fixed length passages considerably better come unit drawbacks 
goal identify relevant document case zobel experiments sole drawback terms indexing 
dividing documents fixed length passages word document greatly increases size index ir 
needless say put burden ir system index large collections especially documents collection long longer documents costly index short ones 
example compare storage costs indexing collections consisting words fixed length passage retrieval passage length set words 
suppose collection document words second collection documents exactly words length 
indexing collection require indexing gamma documents length total word index entries second require entries 
hmm hmm hmm diagram hmms mittendorf schauble passage retrieval 
transition probabilities set 
potentially significant disadvantage items retrieved sections documents entire documents 
fixed length passage retrieval user ir system shown passages middle topic middle paragraph middle sentence 
technique limited situations units retrieval entire documents crucial consider increased storage cost 
mittendorf schauble mittendorf schauble proposed method retrieving arbitrary length passages different hidden markov models hmm 
type hmm compute probability seeing particular stretch text independent query 
second hmm determined probability passage relevant query 
shows linkage types hmm 
mittendorf schauble determined probability retrieving particular passage computing probability seeing text prior passage null passage occurred document type hmm combining probability probability second hmm particular passage matched query 
viterbi algorithm determine highest probability path concatenated hmms path identified best ranked passage particular query mittendorf schauble 
mittendorf schauble technique advantage document collection need segmented prior ir 
disadvantage passages retrieves words query 
passages sentence boundaries paragraph topic boundaries 
mittendorf schauble tested technique collection 
performance variations passage retrieval model better performance vector space model 
performance figures graphically passage retrieval model appeared improve performance measured precision recall approximately equal vector space model 
summary salton allen buckley improved performance ir system comparing queries individual sentences paragraphs 
hearst plaunt showed subtopic segments identified texttiling improved retrieval performance improvement better achieved indexing paragraphs 
stanfill waltz callan zobel demonstrated indexing blocks text beneficial ir performance 
mittendorf schauble built hmm identified passages query words improved precision recall 
hearst plaunt demonstrated improvement performance motivated segments segments intended topically coherent 
advantage segments users response queries 
arbitrary passages middle sentence paragraph stanfill waltz callan mittendorf schauble zobel useful purpose 
topic segments show performance improvement ir corpus transcribed spoken data 
previous attempts improve ir performance segments conducted textual data reliable punctuation capitalization indications sentence paragraph boundaries 
data transcribed speech lack advantages conveyed features 
trec spoken document retrieval task measured utility best performing text segmentation algorithms chapter ir data trec spoken document retrieval sdr task 
task differs ir tasks significant ways 
conducted text derived speech 
ir performed speech signal 
scenarios 
ir performed transcripts produced human annotators second transcript created automatically speech recognition system 
able test ir performance manually produced transcripts access speech recognized data entire corpus 
second difference pertains method evaluation 
ir test sets sdr test set contains document relevant query 
fact queries written knowledge contents documents 
result measuring recall pointless query 
measuring precision equally uninformative maximum number relevant documents 
collection contained relatively small number documents ir systems able rank entire collection 
measuring performance precision recall systems scored determining average rank relevant document 
perfect system rank sole relevant document query assign relevant document average rank 
worst possible performance collection average rank achieved ranking relevant document query hub program committee 
evaluations performed ir sections documents eliminated highest ranked section document 
yielded ordering entire collection rank relevant section document 
ordering measure rank relevant document 
ir experiments conducted ir experiments publicly available smart system implements vector space model buckley 
smart tf delta idf weighting implementation pivoted document length normalization singhal accounts variations document length short documents retrieved overly 
normalization inspired observation positive correlation document length relevance trec collection harmon 
singhal buckley mitra showed normalization improves average precision disks trec corpus percent singhal 
cosine distance measure 
crucial difference traditional cosine distance length document vector denominator formula cosine distance altered bias retrieval longer documents 
length document vector jw computed scaled formula shown 
formula normalization required compensate tendency retrieve short documents frequently set value yielded results collections 
av average document vector length collection 
gamma delta jw av table presents results experiments conducted sdr data 
rows table represent different ways indexed sdr collection 
row labeled documents refers indexing entire documents 
success callan zobel indexing fixed length overlapping segments documents divided documents word overlapping passages indexed 
overlapping passages words 
tested usefulness segments labeled human annotators 
results test row labeled annotator segments 
rows labeled wf segments table results indexed segments identified text segmentation algorithms 
indexed collection treating small segments annotated enable tests speech recognition systems documents 
results test row labeled background segments table 
method average rank average rank documents annotator segments background segments word overlapping passages wf segments segments table ir performance spoken document retrieval corpus stemming smart built stemmer 
results table demonstrate usefulness text segmentation information retrieval 
segments annotators identified outperformed indexing documents 
indexing overlapping passages performed best probably improve indexed possible word segments 
feasible indexing segments words resulted megabyte index megabytes text 
treating background segments documents yielded worst performance pivoted document length normalization 
indexed segments identified algorithm wf performance marginally better achieved indexing documents indexed annotators segments 
observed second best performance indexed segments identified average relevant document ranked best performing method indexing word overlapping passages 
indexing changes background conditions yielded poor ir performance 
previous attempts show usefulness segmentation ir shown segments produced algorithms improve ir performance units documents 
data labeled sentence paragraph boundaries compare hearst plaunt ir performance indexing units 
example retrieved segment shows segment relevant query discussed travels olympic torch motorcycle 
ir system indexed segments identified algorithm relevant segments users text query 
segment contains words document containing words long 
result user ir system save substantial amount reading time section 
perfect segmentation technique replicated annotation produced ldc divided segment nearly equal sized segments saving user additional time 
language modeling language modeling number applications including playing crucial role speech recognition 
speech recognition notoriously difficult problem determining words encoded acoustic signal 
challenging number reasons 
different speakers pronounce words subtly different ways 
languages contain homonyms pairs words pronounced spelled differently 
speakers slur words speak different rates 
explicit boundaries words marked whitespace text 
continual improvement speech recognition technology past years speech recognition sufficiently accurate computationally inexpensive affordable commercial speech dictation systems available personal computers 
systems perfect improving accuracy active area research 
speech recognition performed form gram language model 
simple trigram model 
model probability word depends identity previous words 
gross simplification necessary model language limiting context language models prevents sparse data problems overwhelming 
word trigrams relatively officials new jersey waiting arrival olympic flame torch relay expected reach new jersey capitol clock tonight right torch new jersey schedule way philadelphia torch cross pennsylvania clock hit philadelphia travel motorcycle buck county route roosevelt boulevard going ridge avenue kelly drive kelly drive local torch philadelphia area carry torch west river drive final destination art museum big party take place eleven clock starts torch won get center city eleven clock torch leaves philadelphia starting tomorrow morning travel delaware counties going delaware final torch tomorrow baltimore torch started journey april seventh fifteen miles start olympics july nineteenth delaware residents electing new mayor year republican candidate brad yesterday released plan fight growing crime problem tells twelve tonight city neighborhoods need help fighting crime unusual police officers face bricks bottles certain neighborhood fact certain neighborhoods people deliver pizza mail afraid go parents live fear children caught cross fire say life certain parts starting dangerous life washington unacceptable incumbent term democratic mayor jim running year facing stiff challenge democratic primary coming september checking franklin institute forecast tonight clouds fog scattered showers area torch comes philadelphia eleven clock tomorrow ah scattered showers right degrees philadelphia re listening example segment identified returned user ir system 
whitespace indicates additional boundary placed annotators 
sparse 
system expected handle vocabulary words far fewer typical adults know possible trigrams words largest training corpus contains 
trigram modeling simplification described allows speech recognition performed time severe handicap 
longer distance dependencies language 
noun phrase consisting determiner adjectives head noun words long 
trigram model determiner adjective impact identifying head noun 
number attempts address handicap language modeling 
cache language models boost probability seen words see example lau 
trigger language models increase probability particular words observing sets words frequently occur rosenfeld huang 
text segmentation language modeling relevant topic segmentation done nyu conjunction bbn focused improving speech recognition accuracy sublanguage corpora sekine 
performing speech recognition nyu bbn system employs previously recognized sentences identify documents contents related sentences 
system words documents adapt word probability model speech recognition 
topic segments way entire documents 
advantages topic segments improve speech recognition context similar advantages ir 
long documents numerous topics documents relevant sections may contain completely unrelated sections 
words unrelated sections erroneously probability boosted way words relevant section 
example documents hub collection way recognizing document tobacco industry may edition npr marketplace enhance probabilities topic related words long section tobacco companies document sections german auto makers british newspapers 
words irrelevant sections mercedes newspaper probability boosted pertinent words smoke 
performing document matching step collection topic segments identify shorter passages text permitting faster execution 
application topic segments context language modeling build static topic dependent language models 
currently language models intended relatively broad coverage 
built wall street journal text 
able model language accurately topic account 
test hypothesis divide documents topic segments group segments topic clustering techniques 
build language models topic clusters 
determining language model appropriate difficult 
cases information outside speech stream may useful selecting best language model 
example speech recognition regularly performed news broadcast correlations story type reporters identified 
information identify topically related language model 
topic dependent language modeling conducted experiment tdt corpus test hypothesis topic segmentation useful language modeling 
segmented corpus algorithm identified segments contained word clinton 
identification segments simulated clustering articles topic 
divided segments groups 
reserved set articles containing approximately words test data built topic dependent language model approximately remaining words 
language model trigrams backed bigrams unigrams 
built separate language model words tdt corpus contain articles test corpus 
language model perplexity standard topic dependent table results language modeling experiment conducted tdt corpus 
measured perplexity determine model better predicted test data bahl 
perplexity measures difficulty predicting identity word text words precede 
low perplexity scores indicate language model representative text 
computing perplexity involves determining probability generating sequence words test corpus language model 
trigram language model assumes probability word dependent identity words preceding 
compute probability corpus formula 
number words jw gamma gamma computed perplexity perp text formula 
perp gamma log perplexity scores table indicate segmenting documents performing simple topic clustering improve language modeling 
difference perplexity modest trained language models words text simple techniques identify related segments 
improving nlp algorithms natural language processing tools rely word occurrence statistics collected corpora examine windows words preceding context particular word phrase 
algorithms type include word sense disambiguation yarowsky language modeling beeferman coreference resolution kehler identifying genre document losee kessler 
pronoun resolution techniques search preceding context candidate antecedents baldwin 
data arbitrary windows words yielded state art systems 
performance various tasks improve motivated segments 
words related topic erroneously counted occurring words neighboring topic segment 
relying example computing statistics arbitrarily sized window say words independent topic changed better location topic boundaries limit size window confines single topic 
example computed frequency statistics words occur near word industrial hub corpus erroneously identify word churches appearing twice words simply discussion dow jones industrial average topic segment immediately followed story burning churches southern document 
identify spurious connection industrial churches identified topic boundary segments containing words counted occurrences segments 
number nlp algorithms benefit topic boundary detection measured potential improvement pronoun resolution 
pronoun resolution important tasks summarization ir performed ranking candidate resolutions various heuristics choosing best 
size set candidate antecedents significantly impact running time coreference algorithms performance 
demonstrate usefulness topic segmentation coreference counted number candidate antecedents singular pronouns referred people mentioned randomly selected documents hub corpus 
limited set possible antecedents noun phrases referred people coreference systems incorporate property information 
table presents statistics regarding investigation 
statistics gold standard topic segmentation produced annotators output model source avg 
candidates previous context avg 
candidates current topic segment resolutions outside segment human annotation max 
ent 
model table number candidate antecedents singular pronouns referred people randomly selected broadcast news transcripts 
pronouns examined 
numbers indicate topic segmented text results approximately fold reduction number candidate antecedents search pronoun resolutions limited current topic segment 
reduction occur algorithm produced segmentation identical human annotators 
cost associated assuming resolutions current topic segment 
resolutions noun phrases current topic segment human annotation 
number increased automatically generated annotation cases candidate antecedents current topic segment 
result coreference resolution algorithm easily determine searching antecedents outside current context necessary mitigating effect pronouns pronoun resolution performance 
potential applications summarization presidential address acl sparck jones stressed importance summarization focus nlp research sparck jones 
suggested understanding discourse structure crucial producing summaries text structure deeply related text meaning course summaries attempt capture 
documents summarized information text structure algorithm extract important sentences 
text divided segments structuring algorithm representative important sentences segments selected concatenated produce summary 
difficult part determining sentences satisfy criteria 
possibility choose sentences frequently mentioned entity segment sparck jones suggested 
possibility word frequency information identify important terms segment earliest summarization extraction systems luhn 
paice observed summarization systems hard believe systems relying selection limited adjustment textual material succeed paice 
observation producing abstracts articles indexed ir systems quickly determine read printed document 
summary produced summarization system takes advantage text structure extract sentences may adequate task decide read electronic version document 
lower fidelity summarization techniques useful today cost accessing electronic document lower cost measured terms time effort money retrieving article distant library 
addition summaries summarized documents online hyperlinks established extracted sentences original document facilitate skimming sections identified important 
hypertext salton buckley allen describe system automatically linking related portions document portions separate documents salton buckley salton allan 
suggest links improve access large collections documents providing easy means directed random access highlighting relationships sections 
reasons desirable automate linking process 
document collection segments potential links 
consider manually number segments small 
second identifying links hand may require services domain expert 
consequently automation linking feasible large collections reduce cost technical collections 
obvious text structuring algorithms divide lengthy documents segments determine similarity segments vector space model similarity metric 
similar segments linked allow easier navigation www documents navigation segments documents identified ir engine 
information extraction information extraction task automatically filling templates particular facts document 
systems different templates report different types information 
example convey details new product introductions slots relevant pieces information contained template date product available 
frequently addressed task identifying management newswires muc program committee baldwin 
example sentences example system generate filled templates shown 
slots empty management change events specify subset types facts corresponding template slot 
bill smith today xyz computer citing conflicts board directors 
immediately hired chief financial officer newly formed 
built tools information extraction eagle nlp system part project conducted lexis nexis baldwin 
evaluate information extraction system algorithmically case message understanding conference evaluations perform quantitative qualitative evaluation 
quantitative performance encouraging qualitative evaluation relevant 
difficulties encountered templates xyz computer event type person bill smith position reason conflicts board directors event type hired person position chief financial officer reason sample completed templates information extraction task 
partially completed 
occurred number reasons including failure various processing components 
frequent causes relevant information localized distributed document 
text triggered pattern fire far fragment text needed fill slot template 
approach information extraction highly syntactic relied pattern matching language called mop doran access various types analysis including part speech tags parse trees 
way address problem needing distant information complete template system involve structuring texts prior performing information extraction 
filling templates system identify important fields filled local information look global information current topic segment 
track presence particular types entities segment information fill empty slots 
information extraction text segmentation unable examples data lexis nexis simple example wall street journal suffice 
number management text identified sixth time years continental airlines new senior executive 
gone joseph corr airline chairman chief executive president appointed december 
corr pursue business interests airline said 
reached comment 
succeeding chairman chief executive frank lorenzo chairman chief executive continental parent texas air lorenzo years old reclaiming job corr signed 
airline named president 
year veteran texas air texas international airlines predecessor 
executive vice president planning finance texas air 
top executives continental haven lasted long especially recruited outside 
corr tenure shorter 
year old corr hired largely credited returning trans world airlines profitability president 
executive manufacturing concern 
example text indicating topic segments useful information extraction 
management template filled sentence bold 
system 
sentence bold particular interest 
sentence refers reason joseph corr initially hired continental 
mention continental sentence 
processing sentence system leave field blank mistakenly fill trans world airlines mentioned sentence 
continental airlines best guess field frequently referred text 
text need perform topic segmentation order hypothesize continental hired corr 
information longer article gleaned transcript news broadcast lacked story boundaries advantageous know portion text search 
write pattern mop identify frequently mentioned current topic segment name entity fill empty slots type 
topic detection tracking increased interest today tracking evolution news stories time analyzing contents various news broadcasts newswires 
task main components computing topical similarity passages text identifying passages compared 
component addressed primarily ir algorithms 
second requires techniques topic segmentation proposed dissertation 
topic tracking done performing topic segmentation data sources may contain minimal markup 
news feeds may may indicate boundaries stories may beneficial segment lengthy stories 
speech recognized broadcast news programs virtually annotation regarding topic structure 
fact step news broadcasts may separating contents program commercials interspersed 
commercials removed topic segmentation algorithm divide remaining text segments topic 
segments fed topic tracking system 
automated essay grading standardized tests routinely administered assess people qualifications admitting educational institutions 
standardized tests consist primarily multiple choice questions easily graded machine 
essay questions test mastery subjects ways multiple choice tests time consuming expensive grade 
burstein describe system automatically assigning scores essays burstein 
designed system replace judges traditionally score essays order reduce grading time expense 
prior scoring system divides essays units intended contain single argument point burstein 
currently cue words natural extension word frequency information cues exploited algorithms 
jill burstein karen kukich educational testing service suggestion 
chapter proposed new indicators topic shift described algorithms dividing documents topic segments indicators previously clues 
proposed topic shifts accompanied changes distribution character grams tested hypothesis implementing algorithm tracked text compression performance 
despite intuitive appeal character grams relatively uninformative text structure 
devised text segmentation algorithm patterns word repetition detect topic shifts 
optimization algorithm performed better hearst texttiling youmans vmp topic segmentation simulation concatenated documents hub broadcast news corpus third fourth algorithms 
algorithm advantages 
easily applied documents variety domains various languages requires training data 
second conjunction visualization technique graphically displays similarity information 
developed language model algorithm performed simulation better actual data hub corpus 
method requires training corpus optimal performance showed performs nearly training data exploits burstiness content words locate topic segments 
model accurately segmented text produced speech recognition system surprisingly useful segmenting spanish text 
final technique employed statistical model built maximum entropy modeling software incorporated number features output word frequency algorithm uses words synonymy novel features including repetition named entities bigrams domain cue phrases incorporated named entities 
model performed extremely hub corpus trained small number articles exploited domain specific cue phrases induced portion tdt corpus segment tdt corpus high precision 
showed topic segmentation techniques useful language modeling demonstrating decrease perplexity topic language model trained topic segmented data 
demonstrated utility topic segments improving pronoun resolution algorithms decreasing average number candidate antecedents restricting resolutions topic segments 
demonstrated indexing topic segments identified text segmentation algorithms improves ir performance sdr collection 
showed sample segment returned ir system response queries sdr task 
segment contained relevant information collection reading take time reading entire document came 
provides anecdotal evidence text segmentation useful improving user interactions ir systems retrieval performance 
directions possible directions pointed previous chapters 
performance best algorithms state art obviously room improvement 
intend refine algorithms incorporate additional segmentation cues 
mentioned cues section parallelism enumerated points text complexity 
named entity detection system built speech recognized data performed domain system trained data reliable capitalization punctuation data tdt corpus helpful segmenting documents similar sources 
explored usefulness text structuring language modeling ir coreference resolution ample room additional areas 
interesting utilize structured text statistical coreference resolution system kehler kehler 
explore language modeling applications possibly context ocr 
discussed number potential applications text segmentation previous chapter 
incorporate systems address 
summarization area active research believe benefit greatly structured text 
hypertext generation obvious interesting application especially light growth world wide web 
revisit earlier information extraction order test usefulness structured text constraining possible fills template slots 
feel interesting exciting lies incorporating textual features audio video portions multimedia documents 
combine relationship intonation pauses discourse structure textual features produce better segmentation system 
intend video cues scene transitions fades black commonly occur news broadcasts additionally improve segmentation 
ultimately build integrated model encompassing video audio textual cues test context real world video demand application 
apply segmentation techniques transcripts dialogues switchboard corpus 
far focused exclusively types documents containing primarily planned speech 
segmenting dialogues provide additional set challenges speech disfluencies interruptions topics followed returns frequent defined topic shifts news broadcasts possess 
apply techniques spontaneous speech study information retrieval documents type 
bibliography aone aone larsen 

scalable summarization system robust nlp 
mani maybury editors proceedings workshop intelligent scalable text summarization pages madrid 
bahl bahl baker jelinek mercer 

perplexity measure difficulty speech recognition tasks 
th meeting acoustical society america journal acoustical society america volume page 
bahl bahl jelinek mercer 

maximum likelihood approach continuous speech recognition 
ieee transactions pattern analysis machine intelligence 
baldwin baldwin 

high precision coreference limited knowledge linguistic resources 
proceedings workshop operational factors practical robust anaphora resolution unrestricted texts pages madrid 
baldwin baldwin doran reynar niv srinivas wasson 

eagle extensible architecture general linguistic engineering 
proceedings riao pages montreal 
baldwin baldwin reynar collins eisner ratnaparkhi sarkar srinivas 

university pennsylvania description university pennsylvania system muc 
proceedings sixth message understanding conference pages san francisco 
defense advanced research projects agency morgan kaufmann 
beeferman beeferman berger lafferty 

model lexical attraction repulsion 
proceedings th annual meeting association computational linguistics pages madrid 
beeferman beeferman berger lafferty 

text segmentation exponential models 
proceedings second conference empirical methods natural language processing pages providence rhode island 
bell bell cleary witten 

text compression 
advanced series 
prentice hall englewood cliffs new jersey 


lexis annual reports text segmentation lexical threads 
technical report development international research english commerce technology 


lexis annual reports cluster triangle technique 
technical report development international research english commerce technology 
berger berger della pietra della pietra 

maximum entropy approach natural language processing 
computational linguistics 
biber biber 

typology english texts 
linguistics 
biber biber 

methodological issues regarding corpus analyses linguistic variation 
literary linguistic computing 
bikel bikel miller schwartz weischedel 

nymble high performance learning name finder 
proceedings fifth conference applied natural language processing pages washington brown brown cocke pietra pietra jelinek lafferty mercer 

statistical approach machine translation 
computational linguistics 
brown brown pietra desouza lai mercer 

class gram models natural language 
proceedings ibm natural language itl paris 
buckley buckley 

implementation smart information retrieval system 
technical report technical report cornell university 
burstein burstein 

personal communication 
burstein burstein wolff lu kaplan 

automatic scoring system advanced placement biology essays 
proceedings fifth conference applied natural language processing pages washington callan callan 

passage level evidence document retrieval 
proceedings sixteenth annual international acm sigir conference research development information retrieval pages dublin ireland 
association computing machinery 
callan callan croft harding 

inquery retrieval system 
proceedings third international conference database expert systems applications pages valencia spain 
springer verlag 
carletta carletta 

assessing agreement classification tasks kappa statistic 
computational linguistics 
chafe chafe 

language consciousness 
language 
chinchor chinchor 

muc named entity task definition dry run version version 
documentation seventh message understanding conference 
christel christel kanade mauldin reddy stevens wactlar 

informedia digital video library 
communications acm 
church church 

stochastic parts program noun phrase parser unrestricted text 
proceedings second conference applied natural language processing pages 
church church 

char align program aligning parallel texts character level 
proceedings st annual meeting association computational linguistics pages 
church gale church gale 

inverse document frequency idf measure deviations poisson 
yarowsky church editors proceedings third workshop large corpora pages 
association computational linguistics 
church gale church gale 

poisson mixtures 
journal natural language engineering 
cover thomas cover thomas 

elements information theory 
john wiley sons new york 
dahlgren dahlgren 

discourse coherence segmentation 
hovy scott editors computational conversational discourse burning issues interdisciplinary account chapter pages 
springer verlag berlin 
moore paolucci 

learning features predict cue usage 
proceedings th annual meeting association computational linguistics pages madrid 
digital equipment digital equipment 
alta vista 
www altavista digital com av content story htm 
doran doran niv baldwin reynar srinivas wasson 

mother perl multi tier pattern description language 
proceedings international workshop lexically driven information extraction italy 
dunning dunning 

statistical identification language 
technical report crl technical memo university new mexico 
food agriculture organization united nations food agriculture organization united nations 
www database available apps fao org 
fox fox furuta leggett 

digital libraries 
communications acm 
gale gale church yarowsky 

sense discourse 
proceedings fourth darpa speech natural language workshop pages 
gale sampson gale sampson 

turing smoothing tears 
journal quantitative linguistics 


population frequencies species estimation population parameters 
biometrika 
grimes grimes 

thread discourse 
mouton hague 
grosz hirschberg grosz hirschberg 

intonational characteristics discourse structure 
international conference spoken language processing pages 
grosz grosz joshi weinstein 

centering framework modeling local coherence discourse 
computational linguistics 
grosz sidner grosz sidner 

attention intentions structure discourse 
computational linguistics 


technique clear writing 
mcgraw hill new york 
halliday hasan halliday hasan 

cohesion english 
longman group new york 
harmon harmon 

overview text retrieval conference 
harmon editor proceedings trec text retrieval conference pages washington 
harris harris 

discourse analysis 
language 
harter harter 

psychological relevance information science 
journal american society information science 
hauptmann witbrock hauptmann witbrock 

informedia news demand multimedia information acquisition retrieval 
maybury editor intelligent multimedia information retrieval chapter pages 
aaai press mit press menlo park california 
hearst hearst 

texttiling quantitative approach discourse segmentation 
technical report university california berkeley 
hearst hearst 

context structure automated full text information access 
phd thesis university california berkeley 
hearst hearst 

multi paragraph segmentation expository text 
proceedings nd annual meeting association computational linguistics pages las cruces new mexico 
hearst plaunt hearst plaunt 

subtopic structuring full length document access 
proceedings special interest group information retrieval pages 


similarity patterns language 
ieee symposium visual languages 


organizational patterns discourse 
discourse analysis volume syntax semantics pages 
academic press new york 
hirschberg grosz hirschberg grosz 

intonational features local global discourse 
proceedings workshop spoken systems pages 
darpa 
hirschberg litman hirschberg litman 

empirical studies disambiguation cue phrases 
computational linguistics 
hirschberg nakatani hirschberg nakatani 

prosodic analysis discourse segments direction giving monologues 
proceedings th annual meeting association computational linguistics pages santa cruz california 
hobbs hobbs 

coherence coreference 
cognitive science 
hobbs hobbs 

discourse coherent 
neubauer editor coherence natural language texts papers pages 
hamburg 
hub program committee hub program committee 
hub annotation specification evaluation speech recognition broadcast news version 
hull hull 

hidden markov model language syntax text recognition 
proceedings eleventh conference pattern recognition volume pages 


functional theory discourse 
editor discourse processes advances research theory volume discourse production comprehension chapter pages 
ablex publishing norwood new jersey 
justeson katz justeson katz 

technical terminology linguistic properties algorithm identification text 
natural language engineering 
karp karp schabes 

freely available wide coverage morphological analyzer english 
proceedings th international conference computational linguistics nantes france 
kaszkiel zobel kaszkiel zobel 

passage retrieval revisited 
proceedings acm sigir conference research development information retrieval pages philadelphia 
acm 
katz katz 

distribution content words phrases text language modeling 
natural language engineering 
kehler kehler 

probabilistic coreference information extraction 
proceedings second conference empirical methods natural language processing providence rhode island 
kessler kessler nunberg schuetze 

automatic detection text genre 
proceedings th annual meeting association computational linguistics pages madrid 
kozima kozima 

text segmentation similarity words 
proceedings st annual meeting association computational linguistics student session pages 
kozima furugori kozima furugori 

similarity words computed spreading activation english dictionary 
proceedings european association computational linguistics pages 
kozima furugori kozima furugori 

segmenting narrative text coherent scenes 
literary linguistic computing 
krupka krupka 

sra description sra system muc 
proceedings sixth message understanding conference pages san francisco 
morgan kaufmann 
kukich kukich 

techniques automatically correcting words texts 
acm computing surveys 
kucera francis kucera francis 

computational analysis day american english 
brown university press providence 
lafferty lafferty 

personal communication 
lancaster lancaster 

information systems 
academic press new york 
lau lau rosenfeld roukos 

adaptive language modeling maximum entropy principle 
proceedings arpa human language technology workshop pages 
levy levy 

communicating thematic structure narrative discourse referring terms gestures 
phd thesis university chicago chicago illinois 
lexis nexis lexis nexis 
lexis nexis home page 
www 
com 
library congress library congress 
library congress acquisitions 
loc gov acq acquire html 
longacre longacre 

paragraph grammatical unit 
discourse analysis volume syntax semantics pages 
academic press new york 
longacre longacre 

grammar discourse 
topics language linguistics 
plenum press new york 
losee losee 

text windows phrases differing discipline location document syntactic structure 
information processing management 
luhn luhn 

automatic creation literature abstracts 
journal research development 
takeo takeo 

word sense disambiguation text segmentation lexical cohesion 
proceedings th international conference computational linguistics pages 
mann thompson mann thompson 

rhetorical structure theory theory text organization 
polanyi editor structure discourse 
ablex norwood available isi rr june 
marcus marcus santorini marcinkiewicz 

building large annotated corpus english penn treebank 
computational linguistics 
miller miller beckwith fellbaum gross miller 

papers wordnet 
technical report cognitive science laboratory princeton university 
mittendorf schauble mittendorf schauble 

document passage retrieval hidden markov models 
proceedings acm sigir conference research development information retrieval pages dublin ireland 
acm 
morris morris 

lexical cohesion thesaurus structure text 
technical report csri computer systems research institute university toronto 
morris hirst morris hirst 

lexical cohesion computed thesaural relations indicator structure text 
computational linguistics 
mosteller wallace mosteller wallace 

applied bayesian classical inference case federalist papers 
springer series statistics 
springer verlag new york 
muc program committee muc program committee 
information extraction task definition version 
proceedings sixth message understanding conference pages san francisco 
morgan kaufmann 
nakatani nakatani grosz ahn hirschberg 

instructions annotating discourses 
technical report tr center research computing technology harvard university cambridge ma 


aspect aspectual class temporal structure narrative 
computational linguistics 
nitta nitta 

statistical approach discourse partitioning 
proceedings coling pages kyoto japan 
paice paice 

constructing literature abstracts computer techniques prospects 
information processing management 
litman litman 

intention segmentation human reliability correlation linguistic cues 
proceedings st meeting association computational linguistics pages 
litman litman 

empirical analysis dimensions spoken discourse segmentation coherence linguistic devices 
hovy scott editors computational conversational discourse burning issues interdisciplinary account chapter pages 
springer verlag berlin 
phillips phillips 

aspects text structure investigation lexical organisation text 
north holland linguistic series 
north holland amsterdam 
polanyi polanyi 

formal model structure discourse 
journal pragmatics 
ponte croft ponte croft 

text segmentation topic 
european conference digital libraries pages pisa italy 
porter porter 

algorithm suffix stripping 
program 


new speech enhancement technique application speaker identification 
proc 
icassp pages adelaide 
ratnaparkhi ratnaparkhi 

maximum entropy model part ofspeech tagging 
proceedings conference empirical methods natural language processing pages university pennsylvania 
ratnaparkhi ratnaparkhi 

linear observed time statistical parser maximum entropy models 
proceedings second conference empirical methods natural language processing pages providence rhode island 
ratnaparkhi ratnaparkhi 

simple maximum entropy models natural language processing 
institute research cognitive science university pennsylvania philadelphia 
reichman reichman 

plain speaking theory grammar spontaneous discourse 
phd thesis harvard university department computer science 
reynar reynar 

automatic method finding topic boundaries 
proceedings nd annual meeting association computational linguistics student session pages las cruces new mexico 
reynar ratnaparkhi reynar ratnaparkhi 

maximum entropy approach identifying sentence boundaries 
proceedings fifth conference applied natural language processing pages washington richmond richmond smith amitay 

detecting subject boundaries text language independent statistical approach 
exploratory methods natural language processing pages providence rhode island 
roget roget 

roget international thesaurus 
new york edition 
roget roget 

roget international thesaurus 
harper row new york fourth edition 
rosenfeld huang rosenfeld huang 

improvements stochastic language modeling 
proceedings darpa speech human language technology workshop san mateo california 
morgan kaufmann 


clustering analysis subjective partitions text 
discourse processes 
salton allan salton allan 

selective text utilization text traversal 
hypertext new york 
salton salton allan buckley 

approaches passage retrieval full text information systems 
rasmussen willett editors proceedings sixteenth annual international acm sigir conference research development information retrieval pages pittsburgh pa association computing machinery 
salton buckley salton buckley 

automatic text structuring experiments 
jacobs editor text intelligent systems current research practice information extraction retrieval pages 
lawrence erlbaum associates hillsdale new jersey 
sekine sekine sterling grishman 

nyu bbn csr evaluation 
proceedings workshop spoken systems technology 
darpa 
sibun sibun 

generating text trees 
computational intelligence 
singhal singhal buckley mitra 

pivoted document length normalization 
proceedings acm sigir conference research development information retrieval pages zurich switzerland 
acm 
ko ko 

adaptive method automatic abstracting indexing 
information processing 
sparck jones sparck jones 

natural language processing needs old new borrowed blue 
association computational linguistics presidential address 
stanfill waltz stanfill waltz 

statistical methods artificial intelligence information retrieval 
jacobs editor text intelligent systems current research practice information extraction retrieval pages 
lawrence erlbaum associates hillsdale new jersey 
stark stark 

paragraph markings 
discourse processes 
suen suen 

gram statistics natural language understanding text processing 
ieee transactions pattern analysis machine intelligence 
van der eijk van der eijk 

comparative discourse analysis parallel texts 
proceedings second workshop large corpora kyoto 
walker whittaker walker whittaker 

mixed initiative dialogue investigation discourse segmentation 
proceedings th annual meeting association computational linguistics pages 
webber webber 

structure interpretation discourse deixis 
language cognitive processes 
xtag group xtag group 
lexicalized tree adjoining grammar english 
technical report ircs university pennsylvania 
xu croft xu croft 

query expansion local global document analysis 
proceedings nineteenth annual international acm sigir conference research development information retrieval pages zurich switzerland 
yaari yaari 

segmentation expository texts hierarchical agglomerative clustering 
proceedings advances natural language processing bulgaria 
yarowsky yarowsky 

word sense disambiguation statistical models roget categories trained large corpora 
proceedings coling pages nantes france 
yeo liu yeo 
liu 

rapid scene analysis compressed video 
ieee transactions circuits systems video technology 
youmans youmans 

measuring lexical style competence vocabulary curve 
style 
youmans youmans 

new tool discourse analysis vocabulary management profile 
language 
ziv lempel ziv lempel 

universal algorithm sequential data compression 
ieee transactions information theory 

