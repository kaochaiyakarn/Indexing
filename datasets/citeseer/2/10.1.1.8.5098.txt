survey dimension reduction techniques fodor center applied scientific computing lawrence livermore national laboratory box livermore ca fodor llnl gov june advances data collection storage capabilities past decades led information overload sciences 
researchers working domains diverse engineering astronomy biology remote sensing economics consumer transactions face larger larger observations simulations daily basis 
datasets contrast smaller traditional datasets studied extensively past new challenges data analysis 
traditional statistical methods break partly increase number observations increase number variables associated observation 
dimension data number variables measured observation 
high dimensional datasets mathematical challenges opportunities bound give rise new theoretical developments 
problems high dimensional datasets cases measured variables important understanding underlying phenomena interest 
certain computationally expensive novel methods construct predictive models high accuracy high dimensional data interest applications reduce dimension original data prior modeling data 
mathematical terms problem investigate stated follows dimensional random variable 
xp find lower dimensional representation 
sk captures content original data criterion 
components called hidden components 
different fields different names multivariate vectors term variable statistics feature attribute alternatives commonly computer science machine learning literature 
assume observations realization random variable 
xp mean 
covariance matrix denote observation matrix xi 
denote mean standard deviation ith random variable respectively standardize observations xi xi xi xi xi xi distinguish major types dimension reduction methods linear non linear 
linear techniques result components new variable linear combination original variables si wi 
wi 
wx wk linear transformation weight matrix 
expressing relationship ap note new variables called hidden latent variables 
terms observation matrix si wi 
wi 

indicates jth realization equivalently sk wk xp ap ksk 
linear techniques simpler easier implement methods considering non linear transforms 
review traditional current state art dimension reduction methods published statistics signal processing machine learning literature 
numerous books articles statistical literature techniques analyzing multivariate datasets 
advances computer science machine learning 
earlier survey papers 
reviews methods including principal components analysis projection pursuit principal curves self organizing maps provides neural network implementations reviewed statistical models :10.1.1.44.6279
surveys results independent component analysis context dimension reduction methods :10.1.1.107.3613
survey organized follows 
sections review principal component analysis factor analysis respectively widely linear dimension reduction methods second order statistics 
normal variables mean zero covariance matrix contains information data 
second order methods relatively simple code require classical matrix manipulations 
datasets interest realizations gaussian distributions 
cases higherorder dimension reduction methods information contained covariance matrix appropriate 
linear higher order method projection pursuit reviewed section 
section summarizes higher order linear method called independent component analysis 
nonlinear principal component analysis considered special case independent component analysis section reviewed separately section 
uses non linear objective functions determine optimal weights resulting components linear combinations original variables 
section explains method random projections 
section presents extensions non linear dimension reduction techniques 
principal component analysis principal component analysis pca best mean square error sense linear dimension reduction technique 
covariance matrix variables second order method 
various fields known singular value decomposition svd karhunen lo transform hotelling transform empirical orthogonal function eof method 
essence pca seeks reduce dimension data finding orthogonal linear combinations pcs original variables largest variance 
pc linear combination largest variance 
dimensional coefficient vector 
solves arg max var 
second pc linear combination second largest variance orthogonal pc 
pcs number original variables 
datasets pcs explain variance rest disregarded minimal loss information 
variance depends scale variables customary standardize variable mean zero standard deviation 
standardization original variables possibly different units measurement comparable units 
assuming standardized data empirical covariance matrix spectral decomposition theorem write diag 
diagonal matrix ordered eigenvalues 
orthogonal matrix containing eigenvectors 
shown pcs rows matrix 
comparing see weight matrix shown subspace spanned eigenvectors smallest mean square deviation subspaces dimension briefly indicated section pcs obtained neural networks specific architectures learning algorithms 
property eigenvalue decomposition total variation equal sum eigenvalues covariance matrix fraction var pci trace trace gives cumulative proportion variance explained pcs 
plotting cumulative proportions function select appropriate number pcs keep order explain percentage variation 
plots called diagram plots statistical literature 
number pcs keep determined fixing threshold keeping eigenvectors corresponding eigenvalues greater 
method preferable author suggested keeping variables 
interpretation pcs difficult times 
uncorrelated variables constructed linear combinations original variables desirable properties necessarily correspond meaningful physical quantities 
cases loss interpretability satisfactory domain scientists 
alternative way reduce dimension dataset pca suggested 
pcs new variables method uses information pcs find important variables original dataset 
calculates pcs studies plot determine number important variables keep 
considers eigenvector corresponding smallest eigenvalue important pc discards variable largest absolute value coefficient vector 
considers eigenvector corresponding second smallest eigenvalue discards variable contributing largest absolute value coefficient eigenvector variables discarded earlier 
process repeated variables remain 
factor analysis section follows 
pca factor analysis fa linear method second order data summaries 
suggested psychologists fa assumes measured variables depend unknown common factors 
typical examples include variables defined various test scores individuals scores thought related common intelligence factor 
goal fa uncover relations reduce dimension datasets factor model 
zero mean dimensional random vector xp covariance matrix satisfies factor model matrix constants fk random common factors specific factors respectively 
addition factors uncorrelated common factors standardized variance var cov ui uj cov 
assumptions diagonal covariance matrix written cov diag pp 
data covariance matrix decomposed shown factor model holds 
xi written variance may decomposed xi ui 
ii ij ii part ij called represents variance xi common variables second part ii called specific unique variance contribution variability xi due specific ui part shared variables 
term ij measures magnitude dependence xi common factor fj 
variables xi high loadings ij factor fj implication variables measure unobservable quantity redundant 
pca factor model depend scale variables 
factor model holds orthogonal rotations factors 
orthogonal matrix model new model holds new factors corresponding loadings factors generally rotated satisfy additional constraints diagonal diagonal diag 
pp diagonal elements decreasing order 
techniques method rotate factors obtain parsimonious representation significantly non zero loadings sparse matrix 
explained ica see section thought factor rotation method goal find rotations maximize certain independence criteria 
cases order factor model provides better explanation data alternative full covariance model var 
cases possible derive parameter estimates 
denote sample mean covariance matrix correlation matrix respectively observed data matrix starting ii sii 
obtain ii ij ii 
different possibilities derive estimates model parameters detailed sections 
principal factor analysis suppose data standardized covariance matrix equal correlation matrix 
obtain estimates standardized variables estimate 
common estimates include square multiple correlation coefficients ith variable variables largest correlation coefficient ith variable variables 
form reduced correlation matrix diagonal elements replaced elements ii 
decompose reduced correlation matrix terms eigenvalues 
ap orthonormal eigenvectors 
ai eigenvalues positive estimate ith column equivalently 


diag 
ak 
eigenvectors orthogonal constraint holds 
specific variance estimates updated ii ij 

factor model permissible terms non negative 
practice number factors may determined looking eigenvalues ai reduced correlation matrix choosing index sharp drop eigenvalue magnitudes 
name suggests principal factor analysis pfa related principal component analysis 
specific variances zero comparing equations section indicates pfa equivalent pca 
maximum likelihood factor analysis addition factor model specified assume factors distributed multivariate normal variables parameters model estimated maximizing likelihood 
cases test hypothesis factor model describes data accurately unconstrained variance model 
log likelihood function written log tr goal maximize respect parameters subject constraint 
factor model 
optimization carried noting function tr log linear function log likelihood maximum corresponding minimum terms arithmetic mean geometric mean eigenvalues 
minimizing proceeds stages minimization fixed analytical solution minimization carried numerically 
projection pursuit projection pursuit pp linear method pca fa incorporate higher secondorder information useful non gaussian datasets 
computationally intensive second order methods 
projection index defines interestingness direction pp looks directions optimize index 
gaussian distribution interesting distribution having structure projection indices usually measure aspect non gaussianity 
uses second order maximum variance subject projections orthogonal projection index pp yields familiar pca 
writing optimization criterion var direction pc solves arg max corresponding pc 
commonly higher order projection index negative shannon entropy 
random variable probability distribution negative entropy defined 
gaussian distribution minimizes measure sense find directions maximize entropy projected data respect subject having constant variance projection indices include indices higher order cumulants fisher information :10.1.1.107.3613:10.1.1.44.6279
measures depend unknown probability distribution difficult estimate 
alternative indices approximations different measures non normality proposed literature :10.1.1.107.3613:10.1.1.107.3613
fastica algorithm independent components section find projection pursuit directions 
independent component analysis section survey independent component analysis ica :10.1.1.107.3613:10.1.1.107.3613
information software currently popular method various websites including 
books summarizing advances theory application ica include :10.1.1.107.3613
ica higher order method seeks linear projections necessarily orthogonal nearly statistically independent possible 
statistical independence stronger condition 
involves second order statistics depends higher order statistics 
formally random variables 
xp uncorrelated cov xi xj xi xj xixj xi xj 
contrast independence requires multivariate probability density function factorizes written 
xp 
fp xp 
independence implies vice versa general 
distribution 
xp multivariate normal equivalent 
gaussian distributions pcs independent components 
noise free ica model dimensional random vector seeks estimate components dimensional vector full column rank mixing matrix :10.1.1.107.3613:10.1.1.107.3613
xp ap 
sk components independent possible definition independence 
hidden independent components si non gaussian ensure identifiability model :10.1.1.107.3613:10.1.1.107.3613
noisy ica contains additive random noise component 
xp ap 
sk 
estimation models open research issue :10.1.1.107.3613:10.1.1.107.3613
survey consider noiseless model specified 
overcomplete versions ica number ics larger number original variables :10.1.1.107.3613:10.1.1.107.3613
assume independent components original variables contrast pca goal ica necessarily dimension reduction 
find independent components needs reduce dimension original data method pca 
problem stated order ics 
estimated ordered norms columns mixing matrix similar ordering pca non gaussianity measure similar ordering pp 
ica considered generalization pca pp concepts 
pca seeks uncorrelated variables ica seeks independent variables 
noise free ica special case pp independence interestingness projection pursuit index definition 
noisy ica model equivalent fa model assuming non gaussian data 
ica applied different problems including exploratory data analysis blind source separation blind deconvolution feature extraction 
feature extraction context columns matrix represent features data components si give coefficient ith feature data 
authors ica extract meaningful features natural images :10.1.1.107.3613:10.1.1.107.3613
estimation model consists steps specifying objective function called contrast loss function cost function algorithm optimize objective function 
objective functions categorized groups multi unit contrast functions estimate independent components unit contrast functions estimate single independent component time :10.1.1.107.3613:10.1.1.107.3613
detailed section section respectively 
section lists optimization algorithms 
multi unit objective functions different ways specify objective functions 
section lists possibilities 
shown despite different formulations closely related certain conditions equivalent :10.1.1.107.3613:10.1.1.107.3613
certain conditions distribution independent components known sufficient accuracy mutual information method essentially equivalent maximum likelihood principle non linear pca method 
conditions cumulant methods approximations mutual information 
cumulant general contrast methods non gaussian data knowing underlying distributions 
maximum likelihood network entropy method specifies likelihood noise free ica model uses maximum likelihood principle estimate parameters 
conditions equivalent infomax network entropy maximization concept neural network literature 
advantages method include asymptotic efficiency maximum likelihood estimates regularity conditions 
requires knowledge distribution independent components sensitive outliers computationally intensive undesirable practical situations 
mutual information kullback leibler divergence mutual information measures dependence random variables yi 
ym yi differential entropy 
mutual information non negative zero variables statistically independent 
sense find variables minimize mutual information components 
mutual information equal kullback leibler divergence log dy joint density factorized 
fm ym 
kullback leibler divergence measures closeness distributions 
components independent actual density factorizes just results zero divergence 
mutual information hard estimate imposing difficulties objective function 
summarized approximations polynomials higher order cumulants maximum entropy principle proposed :10.1.1.107.3613:10.1.1.107.3613
non linear cross correlations principle canceling non linear cross correlations form yi yj non linearities specified user 
assuming yi yj independent cross correlations zero 
oftentimes explicit objective functions associated chosen cross correlation implicitly specified 
non linear pca method indicates strong connection ica non linear pca 
introducing non linearities probability densities independent components pca objective function obtain ica model arg max var 
non linear cross correlation method exist explicit contrast functions 
higher order cumulant tensors ica model estimated solving eigenvectors corresponding linear operator defined fourth order cumulant kij cum xi xj xk xl 
linear operator maps space matrices eigenvalues corresponding 
procedure need know probability densities independent components suffers suboptimal statistical properties characteristic cumulant estimators 
unit objective functions unit contrast functions seek single vector linear combination equal independent components si 
desirable pcs needed iteratively find pcs tends result computationally simple solutions 
contrast functions section closely related 
cumulants general contrast functions approximate negentropy 
objective function kurtosis fourth order cumulant special case general contrast functions 
negentropy differential entropy invariant scale transformations 
negentropy negative normalized entropy differential entropy gaussian random vector covariance matrix linearly invariant version entropy 
non negative zero gaussian 
finding direction maximum negentropy equivalent finding representation minimum mutual information 
directions maximize negentropy differential entropy projection index pp 
negentropy difficult estimate 
approximations higher order cumulants explained ones general contrast functions 
higher order cumulants higher order cumulant measure non gaussianity fourth order cumulant called kurtosis 
definition kurtosis kurt random variable kurt 
kurtosis zero gaussian variable positive heavy tailed super gaussian distributions negative light tailed sub gaussian distributions 
independent components derived maximizing modulus kurtosis 
cumulant estimators poor terms robustness asymptotic variance 
consider tails distribution sensitive outliers 
general contrast functions contrast contrast functions introduced earlier general contrast functions formulated statistical properties requiring knowledge distributions allow simple interpretation algorithmic implementation 
contrast functions measure non gaussianity standardized random variable comparing standard gaussian variable smooth non quadratic function jg ey usually taken 
jg simply kurtosis 
suitable choices log cosh exp constants estimators optimizing generalized contrast functions superior statistical properties cumulant estimators 
log density super gaussian distribution related maximum likelihood estimation 
optimization algorithms optimization algorithms require data sphered converge better sphered data 
sphering linear transformation maps new variable unit covariance matrix terms ica model written qx vv 
bs 
assuming unit variance independent components vv ss bb orthogonal 
problem translates finding appropriate orthogonal matrix sphered independent components obtained 
algorithms proposed estimate independent components 
summarizes major types adaptive batch mode block algorithms :10.1.1.107.3613
adaptive methods stochastic gradient type algorithms 
likelihood multi unit contrast functions optimized gradient ascent objective function 
unit implementations straightforward stochastic gradient methods optimize negentropy approximations 
examples adaptive algorithms include jutten herault algorithm canceling non linear cross correlations converges harsh restrictions algorithms nonlinear stable computationally tractable jutten herault method algorithms maximum likelihood estimation non linear pca algorithms neural unit learning rules exploratory projection pursuit algorithms 
batch mode algorithms computationally efficient adaptive algorithms desirable practical situations need adaptation 
fastica algorithm fixed point iteration 
introduced kurtosis subsequently extended general contrast functions 
matlab implementation available 
projection pursuit analysis described section 
non linear principal component analysis non linear pca introduces non linearity objective function resulting components linear combinations original variables 
method thought special case independent component analysis section 
indicated different formulations non linear pca 
non linear pca criterion data vector 
xp searches components 
sp form minimizing wg respect weight matrix denotes component wise application non linear function elements vector commonly non linear functions odd functions tanh optimization carried stochastic gradient descent algorithm learning parameter update matrix wg approximate recursive squares rls algorithm 
rls method converges faster corresponding gradient descent method final accuracy slightly higher computational load 
applying algorithms data needs pre whitened vx vv denoting vx bx optimization written wg convergence contains sought vector 
assuming components variance equal final estimates standardized yy resulting matrix orthogonal yy vv 
condition shown wg yi yi 
indicated section proposed neural network architecture non linear activation functions hidden layers estimate non linear pcas 
random projections method random projections simple powerful dimension reduction technique uses random projection matrices project data lower dimensional spaces 
original data transformed lower dimensional rx columns realizations independent identically distributed zero mean normal variables scaled unit length 
method proposed context clustering text documents initial dimension order final dimension relatively large order 
circumstances pca simplest alternative linear dimension reduction technique computationally expensive 
random projections applied data pre processing step resulting lower dimensional data clustered 
shown empirically results random projection method comparable results obtained pca take fraction time pca requires :10.1.1.54.4679
reduce computational burden random projection method slight loss accuracy random normal projection matrix may replaced thresholding values matrices rows fixed number random locations rest 
similarity vectors measured inner product giving cosine angle unit length vectors showed dimension reduced space large random projection matrices preserve similarity measure average distortion inner products zero variance inverse twice non linear methods extensions non linear independent component analysis non linear methods principal curves self organizing maps topographic maps principle incorporated ica 
dimensional zero mean non gaussian variable non linear ica model replaces linear transformation 
xp 
sk unknown real valued component vector function 
general identifiability properties general non linear ica model estimation difficult 
publications considering special cases mentioned :10.1.1.107.3613:10.1.1.107.3613
overview problem maximum likelihood bayesian ensemble learning method estimation 
principal curves principal curves smooth curves pass middle multidimensional data sets :10.1.1.44.6279:10.1.1.42.1317
linear principal curves fact principal components non linear principal curves generalize concept 
dimensional random vector 
yp density smooth curve 
fp parameterized real valued define projection index value corresponding point closest euclidean distance set principal curves defined curves intersect self consistent respect data 
definition curve self consistent point mean points support projected 

shown curve principal curve solves minf yi yi yi ith instance dimensional vector composition functions yi gives dimensional coordinates projection yi curve estimate proposed iterative algorithm 
starts eigenvector covariance matrix 
iterates steps 
fixed minimize setting fj yj 
fix set change threshold 
alternative formulation principal curves method generalized em algorithm estimation gaussian distribution 
general model smooth necessarily principal curve 
special cases known general type distributions principal curves exist principal curves properties :10.1.1.44.6279
concept principal curves extended higher dimensional principle surfaces estimation procedure gets complicated 
multidimensional scaling items dimensional space matrix proximity measures items multidimensional scaling mds produces dimensional representation items distances points new space reflect proximities data 
proximity measures dis similarities items general distance measure similar items smaller distance popular distance measures include euclidean distance norm manhattan distance absolute norm maximum norm 
results mds indeterminate respect translation rotation reflection 
mds typically transform data dimensions visualizing result uncover hidden structure data 
rule thumb determine maximum number ensure twice pairs items number parameters estimated resulting :10.1.1.44.6279
items xi rp symmetric distance matrix ij 
result dimensional mds set points yi rk distances dij yi yj close possible function corresponding proximities ij 
mds methods incorporate distances ij calculations called metric methods ones rank ordering distances called non metric methods 
contrast states depending linear non linear mds called metric non metric correspondingly :10.1.1.44.6279
steps general estimation procedure follows :10.1.1.44.6279
define stress objective function minimized ij dij scale factor scale factor usually ij ij xi find minimizes stress minf determine optimal stress minx stress 
special case euclidean distance identity leads principal coordinates dimensions solution equivalent principal components re scaling correlations 
alternative mds fastmap computationally efficient algorithm maps high dimensional data lower dimensional spaces preserving distances objects 
topologically continuous maps methods finding continuous map transform high dimensional data lower dimensional lattice latent space fixed dimension :10.1.1.44.6279
techniques called self organizing maps name associated particular method kohonen selforganizing maps 
avoid confusion follow review refer methods collectively methods topologically continuous maps :10.1.1.44.6279
kohonen self organizing maps data vector tn kohonen self organizing maps learn unsupervised way map data space dimensional lattice 
method extended dimensional topological arrangements 
dl dd denote distances typically euclidean lattice data space respectively define neighborhood function hij lattice space symmetric values interval hii node lattice node lattice smaller hij neighborhood node consists nodes hij greater threshold :10.1.1.107.3613
typical neighborhood function hij exp specifies range neighborhood 
kohonen rule uses initially random set vectors data space rp updates iteratively data distribution final vector dense regions data common 
kohonen rule iterates procedure data points convergence occurs 
data tn find closest vector lattice space arg max tn iteration learning rate update vector moving distance new old tn old old tn :10.1.1.107.3613
useful applications drawbacks implicit criteria try optimize rules optimally select proof converge general 
density networks density networks assume probability distribution data parameters prior distributions parameters apply bayesian learning techniques model data terms latent variables :10.1.1.44.6279
generative topographic mapping gtm special density network constrained gaussian mixtures uses expectation maximization em algorithm estimate parameters maximizing likelihood function 
introduced section provides rigorous treatment soms certain assumptions 
neural networks neural networks nns model set output variables yj terms input variables xi yj yj functions yj specify network architecture weights determined training learning nn set known examples error function 
traditional linear non linear dimension reduction techniques implemented neural networks different architectures learning algorithms :10.1.1.42.1317
simplest nn layers input layer hidden bottleneck layer output layer 
obtain data node hidden layer inputs xi combined weights wih threshold bias term passed corresponding activation function second step output obtained similar way data hidden nodes weights threshold possibly different output function 
yj part network reduces input data lower dimensional space second decodes reduced data original domain 
frequently activation output functions include linear identity function sigmoidal shaped functions logistic function heaviside thresholding function 
nns single hidden layer networks threshold activation function called perceptrons 
networks try learn identity mapping outputs yj identical inputs xi called auto associative auto encoders bottlenecks networks 
hetero associative neural nets different number input output layers example classification 
summarized types nn architectures extract principal components :10.1.1.44.6279
complete details 
example linear hidden layer auto associative perceptron input units hidden units output units trained back propagation find basis subspace spanned pcs error metric minimum squared sum differences input output units 
networks oja rule various de correlating devices find principal components 
adding hidden layers nonlinear activation functions input bottleneck bottleneck output layer pca network generalized obtain non linear principal components 
starting feed forward neural network implementation pca extended idea include non linear activation functions hidden layers :10.1.1.42.1317
framework non linear pca network thought auto associative neural network layers input hidden bottleneck hidden output 
denotes function modeled layers denotes function modeled layers shown weights non linear pca network determined minf sf xi xi xi denotes ith instance dimensional vector note close connection principal surfaces section 
lead pca case linear sf thesis compares principle component analysis vector quantization layer neural networks reducing dimension images 
provides software package called implementing methods 
vector quantization explained introduced hybrid non linear dimension reduction technique combining vector quantization clustering data applying local pca resulting voronoi cell clusters 
image data set non linear techniques vector quantization vq nonlinear pca layer neural network implementation nlpca outperformed linear pca 
non linear techniques vq achieved better results nlpca 
genetic evolutionary algorithms genetic evolutionary algorithms geas optimization techniques darwinian theory evolution natural selection genetics find best solution members competing population 
describing geas dimension reduction 
essence set candidate solutions objective function evaluate fitness candidates values parameters chosen algorithm geas search candidate space member optimal fitness 
example gas combination nearest neighbor knn classifier reduce dimension feature set starting population random transformation matrices wk gas find transformation wk knn classifier new features sk wk classifies training data accurately 
regression regression methods dimension reduction goal model response variable terms set xi variables 
regression function linear non linear 
traditionally xi variables called independent explanatory variables statistics response dependent variable 
regression context generally assumed xis carefully selected uncorrelated relevant explaining variation current data mining applications assumptions rarely hold 
variable selection dimension reduction needed cases 
known statistical variable selection method step wise regression different models fit different subsets explanatory variables 
results compared calculating various goodness fit measures subset best measure chosen explanatory variables reduced dimension 
similar approach selecting relevant features evaluating random subsets original features called wrapper method machine learning community 
dimension reduction methods related regression include projection pursuit regression generalized linear additive models neural network models sliced inverse regression principal hessian directions 
summary described dimension reduction methods 
acknowledgments id 
performed auspices department energy university california lawrence livermore national laboratory contract 
eng 
hyv rinen karhunen oja 
independent component analysis 
series adaptive learning systems signal processing communications control 
wiley 
bishop 
neural networks pattern recognition 
oxford university press 
bishop williams 
em optimization latent variable density models 
touretzky mozer hasselmo editors advances neural information processing sytems volume 
mit press cambridge ma 
breiman 
random forests 
technical report department statistics university california 
breiman friedman olshen stone 
classification regression trees 
wadsworth belmont california 

cardoso 
ica website 
cardoso 
www tsi enst fr cardoso 
carreira :10.1.1.44.6279
review dimension reduction techniques 
technical report cs department computer science university sheffield 
cox cox 
multidimensional scaling 
chapman hall second edition 

kung 
principal component neural networks 
theory 
john wiley sons new york sydney 
dobson 
generalized linear models 
chapman hall 
donoho 
high dimensional data analysis dimensionality 
lecture delivered mathematical challenges st century conference american math 
society los angeles august 
stanford edu donoho lectures ams ams html 
faloutsos 
lin 
fastmap fast algorithm indexing data mining visualization traditional multimedia datasets 
michael carey donovan schneider editors proceedings acm sigmod international conference management data pages san jose california 
friedman hastie tibshirani 
elements statistical learning prediction inference data mining 
springer 
fukunaga 
statistical pattern recognition 
san diego academic press nd edition edition 
girolami editor 
advances independent component analysis 
perspectives neural computing 
springer 
goldberg 
genetic algorithms search optimization machine learning 
addison wesley reading ma 
hand 
discrimination classification 
new york john wiley 
hastie stuetzle 
principal curves 
am 
stat 
assoc 
hastie tibshirani 
generalized additive models volume monographs statistics applied probability 
chapman hall 
huber 
projection pursuit 
ann 
stat 
hyv rinen 
fast robust fixed point algorithms independent component analysis 
ieee transactions neural networks 
hyv rinen :10.1.1.107.3613
survey independent component analysis 
neural computing surveys 
citeseer nj nec com hyv survey html 
hyv rinen oja 
fast fixed point algorithm independent component analysis 
neural computation 
rinen ica website helsinki university technology 
www cis hut fi 
jackson 
user guide principal components 
new york john wiley sons 
jolliffe 
discarding variables principal component analysis artificial data 
appl 
statist 
jolliffe 
discarding variables principal component analysis ii real data 
appl 
statist 
jolliffe 
principal component analysis 
springer verlag 
leen 
fast non linear dimension reduction 
advances neural information processing systems pages 
morgan kaufmann publishers 
karhunen 
nonlinear independent component analysis 
roberts editors independent component principles practice 
cambridge university press cambridge uk 
karhunen pajunen oja 
nonlinear pca criterion blind source separation relations approaches 
neurocomputing 
kaski 
data exploration self organizing maps 
phd thesis helsinki university technology finland 
kaski 
dimensionality reduction random mapping fast similarity computation clustering 
proc 
ieee international joint conference neural networks 
kohavi john 
wrapper approach 
liu motoda editors feature extraction construction selection data mining perspective 
springer verlag 
kohonen self organization massive document collection 
ieee transactions neural networks may 
kohonen 
self organizing map 
proc 
ieee 
kramer 
non linear principal component analysis autoassociative neural networks 
aiche journal 

lee 
independent component analysis theory applications 
kluwer publishers 

li 
high dimensional data analysis sir phd approach 
www stat ucla edu april 
lecture notes progress 

theoretical results nonlinear principal component analysis 
citeseer nj nec com html 
mardia kent bibby 
multivariate analysis 
probability mathematical statistics 
academic press 
mccullagh nelder 
generalized linear models 
chapman hall 
quinlan 
induction decision trees 
machine learning 
quinlan 
programs machine learning 
morgan kaufman 
dimensionality reduction genetic algorithms 
ieee transactions evolutionary computation july 
ripley 
pattern recognition neural networks 
cambridge university press 
ritter kohonen 
self organizing semantic maps 
biological cybernetics 
roberts editors 
independent component principles practice 
cambridge university press cambridge uk 
sejnowski ica website salk institute 
www cnl salk edu ica cnl html 
siedlecki sklansky 
automatic feature selection 
international journal pattern recognition artificial intelligence 

dimension reduction images neural networks 
master thesis leiden university 
tibshirani 
principal curves revisited 
statistics computing 
ripley 
modern applied statistics plus 
springer 

