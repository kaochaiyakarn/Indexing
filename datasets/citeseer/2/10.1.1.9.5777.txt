exploiting dictionaries named entity extraction combining semi markov extraction processes data integration methods william cohen center automated learning discovery carnegie mellon university pittsburgh pa cs cmu edu consider problem improving named entity recognition ner systems external dictionaries specifically problem extending state art ner systems incorporating information similarity extracted entities entities external dictionary 
difficult high performance named entity recognition systems operate sequentially classifying words participate entity name useful similarity measures score entire candidate names 
correct mismatch formalize semi markov extraction process sequentially classifying segments adjacent words single words 
addition allowing natural way coupling high performance ner methods highperformance similarity functions formalism allows direct useful entity level features provides natural formulation ner problem sequential word classification 
experiments multiple domains show new model substantially improve extraction performance previous methods external dictionaries ner 
categories subject descriptors information storage retrieval content analysis indexing dictionaries artificial intelligence learning general terms algorithms experimentation 
keywords learning information extraction named entity recognition data integration sequential learning 

named entity recognition ner finding names entities unstructured text 
studied cases ner authors contributed equally research 
permission digital hard copies part personal classroom granted fee provided copies distributed profit commercial advantage copies bear notice full citation page 
copy republish post servers redistribute lists requires prior specific permission fee 
kdd august seattle washington usa 
copyright acm 
sunita sarawagi iit bombay mumbai india sunita iitb ac identifying personal names names newswire text identifying gene protein names biomedical publications identifying titles authors line publications :10.1.1.14.8357
named entity recognition important step deriving structured database records text 
cases ultimate goal information extraction process answer queries combine information structured unstructured sources 
example biologist want look publications proteins particular superfamily superfamily defined structured database biomedical information business analyst want find articles concerning companies particular industry sector intelligence analyst wish look documents link persons previously known engaged suspicious activity 
applications ner successful extent finds entity names matched pre existing database 
ner methods step query process natural want optimize perform best important entities entities appear external databases structured queries 
reasonable expect large collection names known entities collection associated type structured database improve ner performance 
investigates problem task improving ner systems external dictionaries 
problem surprisingly subtle 
naively expect large dictionary simply looking exact matches dictionary entry reasonable ner method 
fact seldom case 
surface form name free text vary substantially dictionary version leading issues analogous arise linking de heterogeneous database records 
problem compounded extracting text informal prone noise errors email corpus address corpus consider experiments 
external dictionary transforming useful ner system difficult 
conversely state art ner system incorporating information possible linkage external dictionary non trivial 
primary issue high performance ner systems operate sequentially classifying words participate entity name record linkage systems operate scoring entire candidate names similarity existing dictionary entry 
fundamental mismatch representation means incorporating dictionary information awkward best 
discuss new formalism ner corrects mismatch 
describe semi markov extraction process relaxes usual markov assumptions ner systems 
process sequentially classifying segments adjacent words single words 
addition allowing natural way linking ner high performance record linkage methods formalism allows direct useful features length entity 
arguably natural formulation ner problem sequential word classification eliminates certain decisions problem encoding 
new model describe learning algorithm 
experimental results new algorithm discuss related conclude 

algorithms name finding name finding word tagging named entity recognition ner process annotating sections document correspond entities people places times amounts 
example output ner email document fred please office afternoon 
fred person please office loc afternoon time common approach ner convert name finding tagging task 
document encoded sequence tokens 
xn tagger associates parallel sequence tags 
yn yi tag set tags appropriately defined name segments derived 
instance associate tag entity type add special tag words part entity name tagged version sentence fred please office afternoon person oth oth oth loc loc time time common way constructing tagging system learn mapping data :10.1.1.14.8357
typically data form annotated documents readily converted pairs 
methods learning taggers exploit way sequential nature classification process 
general tag depends tags instance person names usually tokens long yi tagged person probability yi person increased probability yi person decreased 
common learning approaches ner learn sequential model data generally variant hidden markov model hmm 
convenient describe framework context hmm variants conditional distribution defined yi yi assume distinguished start tag begins observation 
formalism maximum entropy taggers variously called maximum entropy markov model memm conditional markov model cmm 
inference model performed variant viterbi algorithm hmms 
training data form pairs local conditional distribution yi yi learned derived triples yi yi example maximum entropy methods 
semi markovian ner relax model assuming tags yi change position tags change certain selected positions tag change number tokens observed 
semi markov decision processes call conditional semi markov model 
notation 
sm segmentation segment sj consists start position tj index position uj label segmentation valid tj uj 
consider valid segmentations 
conceptually segmentation means tag xi tj uj inclusive alternatively means tags yt 
yu corresponding xt 
xu equal formally index tj uj define tag sequence derived 

instance segmentation sample sentence person oth loc time written fred person please oth office loc afternoon time defined distribution pairs form sj tj generally term semi markov model smm 
model sj depends label associated sj independent sj 
discussion issues need addressed inference learning algorithms 
inference version viterbi finds probable time nl length upper bound segment length uj tj 
inference procedure polynomial 
experiments sufficient limit small values 
learning inspection equation shows training form pairs learning local distribution sj tj complex cmm 
conditionally structured models cmm ideal model ner systems better performance obtained algorithms learn single global model 
extension global learning algorithm semi markov distribution 
emphasize smm segment length bounded quite different order cmm order cmm label depends previous labels corresponding tokens 
smm different cmm uses window previous tokens predict yi smm single labeling decision segment making series interacting decisions incrementally 
section experimentally compare smm high order cmms 
discriminative training perceptron training learning algorithm training derived collins perceptron algorithm discriminatively training hmms summarized follows 
assume local feature function maps pair index vector features 
define weight vector components inference need compute viterbi decoding argmax completeness outline computed 
viterbi search tractable restrict limited simplify discussion assume strictly markovian component yi yi fixed denote vector 
viterbi inference defined recurrence designated start state vx maxy vx max vx 
goal learning find leads globally best performance 
best repeatedly updating improve quality viterbi decoding particular example xt yt 
specifically collin algorithm starts 
th example xt yt viterbi sequence yt wt xt computed 
yt yt wt set wt wt additional complexity learn predict tag type position uj segment equivalently length 
perceptron smm learning feature vector representation segment sj 
score 
example xt st 
modified version equation find segmentations sk highest score xt wt si 

wt wt 

score xt wt si greater score xt wt st update wt follows wt wt xt st xt si final output learning return average wt segment equation find best segmentation 
discriminative training smm replaced wt wt xt yt xt yt training takes final learned weight vector average value wt time steps simple algorithm performed number important sequential learning tasks including ner :10.1.1.7.8542
proved converge certain plausible assumptions 
natural extension algorithm smm assumes training data form pairs segmentation 
assume feature vector representation computed segment sj proposed segmentation assume function 
defining apply collins method immediately long possible perform viterbi search find best segmentation input smm viterbi search need restrict form tj uj vector implement viterbi recurrence vx maxy vx conceptually score best segmentation tokens concludes segment sj uj refinements learning algorithm smm viterbi search efficient segment size bounded number case replace max term equation experimentally evaluated number variants collins method obtained somewhat better performance extension 
described algorithm finds single top scoring label sequence updates score greater score correct sequence score 
extension modified viterbi method find top sequences yk update yi score higher times score complete algorithm shown 
technique learn hmms replacing equation equation 
collins algorithm method works best passes data 
parameters method number epochs iterations examples 
features semi markov learner features longer apply individual words applied hypothesized entity names 
somewhat natural define new features providing context 
notation recall assumed smm feature function written tj uj function tj uj sequence typically compute property proposed segment xt 
xu possibly tokens indicator function couples property label concrete examples possible table 
features applied word segments ordinary tokens hmm word tagging ner system 
features powerful applied multi word segments 
instance pattern capitalized words sequence indicative person name pattern 
example length feature informative segments 
distance features longer classifying tokens classifying segments correspond complete entity names straightforward similarity words external dictionary feature 
dictionary entity names distance metric entity names 
define gd minimum distance entity name gd min instance contains strings frederick barney jaro winkler distance metric gd fred gd fred please fred frederick fred please frederick 
feature form gd trivially added smm representation pair problem distance features relatively expensive compute particularly large dictionary 
experiments pre processed dictionary building inverted index character grams appearing dictionary entries discarding frequent grams appear dictionary entries 
approximate gd finding minimum dictionary entries share common non frequent gram 
experimental results baseline algorithms evaluate proposed method learning compared hmm version algorithm 
strong baseline 
previous experimental studies collins method proved superior maximum entropy cmm tagging methods ner shallow parsing close competitor conditional random fields pos tagging shallow parsing 
extension method performs better ner tasks usually gives comparable improvements smm hmm version algorithm see section 
experiments 
features th token history length plus lower cased value token letter cases letter case patterns illustrated tokens window size centered th token 
additional dictionary features described 
experimented ways encoding ner word tagging problem 
simplest method hmm vp predicts labels label tokens inside entity label tokens outside entity 
second encoding scheme due borthwick 
tags associated entity type corresponding token entity token multi token entity word entity token multi token entity 
fifth tag indicating tokens part entity 
example locations tagged labels tagged example fred person please fourth floor meeting room loc encoded omitting brevity tags fred person unique please fourth loc floor meeting loc continue room loc call scheme hmm vp 
add dictionary information hmm vp simply add additional binary feature fd true token appears dictionary token xi fd xi xi matches word dictionary fd xi 
feature treated binary feature training procedure assigns appropriate weighting relative features 
add dictionary information hmm vp follow borthwick proposed set features fd unique fd fd fd continue 
features analogous entity labels token xi binary dictionary features denote respectively match word dictionary entry match word multi word entry match word multi word entry match word entry 
example token xi feature values fd unique xi fd xi fd continue xi fd xi dictionary table 
function explanation examples xt 
xu value segment fred please lowercase xt 
xu lower cased value fred please length segment xt value left window size fred xu xu value right window size please office translate za xx xt 
xu letter cases segment xxxx xxxxxx xxxx xx za xx xt 
xu letter case pattern segment gd jaro winkler distance dictionary examples fred please office afternoon frederick barney additional baseline ner method evaluated rote matching dictionary extracting phrases exactly match dictionary entry 
approach low recall dictionary incomplete handle variations way names appear text dictionary misspellings abbreviations 
results provide indication quality dictionary 
note cases better performance obtained carefully normalizing dictionary entries 
simple normalization scheme eliminate case punctuation complex ones ner 
just record linkage problems normalization desirable name chemical proper normalization certainly problem dependent 
experiments normalize dictionary entries making match case insensitive 
final baseline dictionary information hmm vp hmm vp extended distance features described tokens distance compute feature token xi minimum distance xi entity dictionary features natural tokens segments turned surprisingly useful weak partial matches entity names informative 
knowledge features sort previously ner tasks 
dictionaries described distance functions open source software package jaccard jaro winkler 
briefly jaccard distance sets applied strings treating sets words 
jaro winkler distance character distance word distance number characters appear approximately position strings 
tfidf word measure 
jaccard distance tfidf scores number words com table possible feature functions mon strings rare words weighted heavily common words 
hybrid measure modifies tfidf considering words small jaro winkler distance common strings 
semi markov learner smm vp denote implementation algorithm 
parameters set hmm vp hmm vp 
hmm vp smm vp predicts label values corresponding segments inside outside entity 
limit length entity segments limit length non entity segments 
value set separately dataset value observed entity lengths 
baseline set features hmm vp hmm vp 
additionally feature hmm vp indicator function true iff token segment feature indicator function true iff token segment feature indicator function true iff token segment feature 
instance suppose baseline indicator function features hmm vp office office true iff xi value office yt smm vp function office true xi value office function office true xt value office analogous office state output encoding features enable smm vp model token distributions different different parts entity 
alternative distance features described section provided binary dictionary information smm vp introducing binary feature true segment iff exactly matches dictionary entity 
corresponds class code 
dataset instances words entity entities words entity dictionary words entry text text entries dictionary email person jobs title address state city table description data tags dictionary experiments 
datasets evaluated systems information extraction problems derived different datasets 
address data 
address dataset consists home addresses students major university india 
addresses set regular addresses extracting relatively structured fields city names challenging 
external dictionaries list cities india list state names india defined corresponding extraction tasks identify city names identify state names 
email data 
dataset consists email messages email corpus contains approximately email messages collected management game conducted carnegie mellon university 
game mba students organized approximately teams members ran simulated companies different market scenarios week period 
messages sent day time period manually tagged person names 
person names email headers regular names email bodies reduce effect testing header fields field subject field 
dictionary list students participated game 
jobs data 
set computer related job postings posted austin jobs newsgroup 
postings manually annotated various entities researchers university texas 
annotated entities names job titles 
construct dictionaries entities manually extracted names job titles current hi tech job listings austin area large job listing board 
table give summary extraction tasks listing number instances entities dictionary entries average number words entity average number words dictionary entries 
indication difference dictionary entries entities extracted seen difference number tokens entity cases 
results discussion results initial experiments shown table 
ner tasks learned regardless feature set data table learners trained available data remainder testing 
show results training set sizes 
results reported averaged random selections disjoint training test examples measure accuracy terms correctness entire extracted entity partial extraction gets credit 
compared ner methods different tasks external dictionary column external dictionary binary features second column external external dictionary distance features third column 
report recall precision values observations concerning results table 
generally speaking smm vp best performing method hmm vp worst 
hmm vp outperforms equals hmm vp cases considered ner problems dictionary binary dictionary features distance features 
exceptions address state extraction dictionary features 
likewise smm vp outperforms hmm vp cases 
binary dictionary features helpful dictionary features helpful 
addition binary dictionary features improves learners problems 
replacing binary dictionary features distance features improves performance cases 
expected exact matches external dictionary generally give low recall 
precision surprisingly poor job title task 
dictionary lookup smm vp distance features 
concise view results table summarizes impact performance novel techniques proposed distance dictionary features semi markov extraction methods compares baseline method hmm vp binary dictionary features take representative previous state art dictionary features ner 
improved ner tasks baseline modified distance features binary features line labeled binary distance smm vp hmm vp line labeled hmm smm 
smm vp distance features improves scores baseline average 
additional experiments perform detailed comparison smm vp hmm vp various conditions 
focus comparing performance smm vp hmm vp distance features 
detailed comparisons running times methods implementation optimized running defined precision recall precision recall 
dictionary dictionary binary features distance features recall prec 
recall prec 
recall prec 
address state lookup hmm vp hmm vp smm vp address city lookup hmm vp hmm vp smm vp email person lookup hmm vp hmm vp smm vp job lookup hmm vp hmm vp smm vp job title lookup hmm vp hmm vp smm vp table performance ner methods tasks conditions external dictionary external dictionary binary features external dictionary distance features 
span accuracy address city hmm vp smm vp fraction available training data span accuracy email person hmm vp smm vp fraction available training data span accuracy job title hmm vp smm vp fraction available training data comparing smm vp hmm vp changing training set size tasks domains 
axis fraction examples training axis field level 
address email job state city person title baseline method binary distance hmm smm changes table summary improvements measure baseline method hmm vp binary dictionary features 
time 
implemented smm vp method times slower hmm vp expensive distance features expanded viterbi search 
effect extensions collins method table compares performance implementation collins original method labeled collins variant labeled 
compare natural semi address email job state city person title hmm vp collins smm vp collins table performance voted perceptron variant considered vs method described collins 
markov extension collins method smm vp 
cases collins method performs better problems worse remaining 
changes performance associated extension affect markovian semi markovian versions algorithm similarly 
tasks change collins original method variant change relative order methods 
history recall prec 
address state hmm vp smm vp address city hmm vp smm vp email person hmm vp smm vp table effect increasing history size hmm vp performance compared performance smm vp 
effect training set size show impact increasing training set size hmm vp smm vp representative ner tasks 
training size small smm vp better hmm vp training size increases gap methods narrows 
suggests semi markov features important large amounts training data available 
amount data needed methods converge varies substantially illustrated curves address city email person 
effect history size straightforward extend algorithms hmm vp construct features rely predicted classes class 
table show result increasing history size hmm vp 
find performance hmm vp improve increasing history size particular increasing history size change relative ordering hmm vp smm vp 
result supports claim section smm vp segment length bounded quite different order hmm 
alternative dictionaries re ran extraction problems alternative dictionaries study sensitivity dictionary quality 
emails dictionary student names obtained students universities country part project 
job title extraction task obtained dictionary job titles california software jobs website recall original email dictionary contained names people sent emails original dictionary austin area job postings jobs austin area 
table shows result hmm vp smm vp distance features 
methods fairly robust www com dictionaries related entities 
quality extractions lowered methods cases performance changes large 

related methods described section integrating dictionary ner systems number techniques proposed dictionary information extraction 
method incorporating external dictionary generative models hmms proposed 
dictionary treated collection training examples emissions state recognizes corresponding entity instance dictionary person names treated example emissions person name state 
method suffers number drawbacks obvious way apply conditional setting highly sensitive misspellings token dictionary large different training text may degrade performance 
concurrent authors scheme proposed compiling dictionary large hmm emission transition probabilities highly constrained hmm free parameters 
approach suffers limitations described may useful training data limited 
describe edit distance scheme finding partial matches dictionary entries text 
scheme uses blast high performance tool designed dna protein sequence comparisons edit distance computations 
obvious way combining edit distance information informative features model 
experimental studies pure edit distance metrics best performers matching names suggests may advantageous ner able exploit types distance metrics edit distance 
early ner systems sliding windows approach extraction word grams classified entities non entities bounded size 
systems easily extended dictionary features 
prior experimental comparisons sliding window ner system usually proved inferior hmm ner systems 
sliding window approaches disadvantage may extract entities overlap 
mechanism exploiting dictionary bootstrap search extraction patterns unlabeled data 
systems dictionary entries matched unlabeled instances provide seed positive examples learn extraction patterns provide additional entries dictionary 
extraction systems rule exceptions appear assume relatively clean set extracted entities 
contrast focus probabilistic models incorporation large noisy dictionaries 
knowledge semi markov models previously information extraction domains 
knowledge smm dictionaries method combine arbitrary similarity measures multi word segments markovian hmm extraction learning algorithm 
plan explore adapting recall prec 
recall prec 
recall prec 
email person dictionary original dictionary student names dictionary lookup hmm vp smm vp job title dictionary original austin job titles california job titles dictionary lookup hmm vp smm vp semi markov ner learning algorithms discussed conditional random fields 

cases ultimate goal information extraction process answer queries combine information structured unstructured sources 
applications ner successful extent finds entity names matched pre existing database 
extending state art ner systems incorporating external dictionary difficult 
particular incorporating information similarity extracted entities dictionary entries awkward best ner systems operate sequentially classifying words participate entity name best similarity measures score entire candidate names 
correct mismatch relax usual markov assumptions formalize semi markov extraction process 
process sequentially classifying segments adjacent words single words 
addition allowing way coupling high performance ner methods high performance record linkage metrics formalism allows direct useful features length entity 
provides arguably natural formulation ner problem sequential word classification 
instance usual formulation design new set output tags corresponding change tag entity decoding scheme account distributional differences words entity words entity 
semi markov formulation merely adds new features entity words 
compared proposed algorithm strong baseline ner uses collins perceptron algorithm training hmm state art multi label encoding dictionary information 
new algorithm surprisingly effective datasets outperforms previous baseline dramatically 
acknowledgments authors reviewers number substantive comments greatly improved final version 
preparation supported part funded information processing technology office defense advanced research projects agency darpa national science foundation 
eia national institute statistical sciences contract army table results changing dictionary 
research office center computer communications security carnegie mellon university 

agichtein gravano 
snowball extracting relations large plaintext collections 
proceedings th acm international conference digital libraries 
altun tsochantaridis hofmann 
hidden markov support vector machines 
proceedings th international conference machine learning icml 
bikel schwartz weischedel 
algorithm learns name 
machine learning 
borkar deshmukh sarawagi 
automatic text segmentation extracting structured records 
proc 
acm sigmod international conf 
management data santa usa 
borthwick sterling agichtein grishman 
exploiting diverse knowledge sources maximum entropy named entity recognition 
sixth workshop large corpora new brunswick new jersey 
association computational linguistics 
ge kate marcotte mooney wong 
learning extract proteins interactions medline abstracts 
available www cs utexas edu users ml publication html 
ge mooney marcotte 
extracting gene protein names biomedical abstracts 
unpublished technical note available www cs utexas edu users ml publication html 
califf mooney 
bottom relational learning pattern matching rules information extraction 
journal machine learning research 
cohen 
open source java toolkit approximate string matching techniques 
project web page sourceforge net 
cohen fienberg 
comparison string distance metrics name matching tasks 
proceedings ijcai workshop information integration web 
collins 
discriminative training methods hidden markov models theory experiments perceptron algorithms 
empirical methods natural language processing emnlp 
collins singer 
unsupervised models named entity classification 
proceedings joint sigdat conference empirical methods natural language processing large corpora emnlp college park md 
crammer singer 
ultraconservative online algorithms multiclass problems 
mach 
learn 
res 
craven 
constructing biological knowledge bases extracting information text sources 
proceedings th international conference intelligent systems molecular biology ismb pages 
aaai press 
eddy krogh mitchison 
biological sequence analysis probabilistic models proteins nucleic acids 
cambridge university press cambridge 
freitag 
multistrategy learning information extraction 
proceedings fifteenth international conference machine learning 
morgan kaufmann 
freund schapire 
large margin classification perceptron algorithm 
computational theory pages 
ge 
segmental semi markov models applications sequence analysis 
phd thesis university california irvine december 
zimmer 
playing biology name game identifying protein names scientific text 
pac symp pages 
humphreys gaizauskas 
applications information extraction biological science journal articles enzyme interactions protein structures 
proceedings pacific symposium biocomputing psb pages 
klein manning 
conditional structure versus conditional estimation nlp models 
workshop empirical methods natural language processing emnlp 
kraut lerch espinosa 
coordination teams evi dence simulated management game 
appear journal organizational behavior 
friedman 
blast identifying gene protein names journal articles 
gene 
lafferty mccallum pereira 
conditional random fields probabilistic models segmenting labeling sequence data 
proceedings international conference machine learning icml williams ma 
lawrence giles bollacker 
digital libraries autonomous citation indexing 
ieee computer 
littlestone 
learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
malouf 
markov models language independent named entity recognition 
proceedings sixth conference natural language learning conll 
mccallum freitag pereira 
maximum entropy markov models information extraction segmentation 
proceedings international conference machine learning icml pages palo alto ca 
mccallum nigam rennie seymore 
automating construction internet portals machine learning 
information retrieval journal 
ratnaparkhi 
learning parse natural language maximum entropy models 
machine learning 
riloff jones 
learning dictionaries information extraction multi level boot strapping 
proceedings sixteenth national conference artificial intelligence pages 
sarawagi 
interactive deduplication active learning 
proceedings eighth acm sigkdd international conference knowledge discovery data mining july edmonton alberta canada 
acm 
seymore mccallum rosenfeld 
learning hidden markov model structure information extraction 
papers aaai workshop machine learning information extraction pages 
sha pereira 
shallow parsing conditional random fields 
proceedings hlt naacl 
sutton 
integrated architectures learning planning reacting approximating dynamic programming 
proceedings seventh international conference machine learning austin texas 
morgan kaufmann 
sweeney 
finding lists people web 
technical report cmu cs cmu carnegie mellon university school computer science 
available privacy cs cmu edu projects 
winkler 
matching record linkage 
business survey methods 
wiley 
winston lin grishman 
bootstrapped learning semantic classes positive negative examples 
proceedings icml workshop continuum labeled unlabeled data washington august 
