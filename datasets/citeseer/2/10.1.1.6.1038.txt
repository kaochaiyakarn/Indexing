machine learning kluwer academic publishers 
manufactured netherlands 
training invariant support vector machines dennis decoste decoste aig jpl nasa gov jet propulsion laboratory ms oak grove drive pasadena ca usa california institute technology bernhard sch lkopf bs conclu de max planck institut fuer kybernetik 
bingen germany editor nello cristianini 
practical experience shown order obtain best possible performance prior knowledge invariances classification problem hand ought incorporated training procedure 
describe review known methods doing support vector machines provide experimental results discuss respective merits 
significant new results reported achievement lowest reported test error known mnist digit recognition benchmark task svm training times significantly faster previous svm methods 
keywords support vector machines invariance prior knowledge image classification pattern recognition 
lecun published pattern recognition performance comparison noting optimal margin classifier excellent accuracy remarkable high performance classifiers include priori knowledge problem 
fact classifier just image pixels permuted fixed mapping 
improvements expected technique relatively new things changed years statement 
optimal margin classifiers support vector machines svms boser guyon vapnik cortes vapnik vapnik turned mainstream method part standard machine learning toolkit :10.1.1.49.8736
second methods incorporating prior knowledge optimal margin classifiers part standard sv methodology 
things closely related 
initially svms considered theoretically elegant spin general allegedly largely useless vc theory statistical learning 
methods incorporating prior knowledge sch lkopf burges vapnik svms competitive state art handwritten digit classification benchmarks lecun popularized decoste sch lkopf machine learning community bell labs 
point practitioners interested theory results longer ignore svms 
sense methods described helped pave way svm widely machine learning tool 
tries provide snapshot state art methods incorporate prior knowledge invariances classification problems 
gathers material various sources 
partly reviews older material appeared sch lkopf burges vapnik journal partly presents new techniques applications experimental results 
organized follows 
section introduces concept prior knowledge discusses types prior knowledge pattern recognition 
focuses prior knowledge invariances section describes methods incorporating invariances svms 
section introduces specific way combines method invariant svms widely smo training algorithm 
section reports experimental results methods section discusses findings 

prior knowledge pattern recognition prior knowledge refer information learning task available addition training examples 
general form prior knowledge possible generalize training examples novel test examples 
instance classifiers incorporate general smoothness assumptions problem 
test patterns similar training examples tend assigned class 
svms shown kernel function amounts enforcing smoothness regularizer pf estimated function green function adjoint operator smola sch lkopf ller girosi cf 
poggio girosi 
bayesian posteriori setting corresponds smoothness prior exp pf kimeldorf wahba 
second method incorporating prior knowledge somewhat specific consists selecting features thought particularly informative reliable task hand 
instance handwritten character recognition correlations image pixels nearby tend reliable ones distant pixels 
intuitive reason variations writing style tends leave local structure handwritten digit fairly unchanged global structure usually quite variable 
case svms type prior knowledge readily incorporated designing polynomial kernels compute mainly products nearby pixels sch lkopf cf 
table 
example applies handwritten character recognition clear great deal problems similar local structure instance problems computational biology involving nucleotide sequences 
zien 
analogy engineer kernels outperform approaches problem recognizing translation initiation sites dna mrna sequences positions mark regions coding proteins 
haussler gone constructed convolutional kernels compute features targeted problems bioinformatics 
training invariant support vector machines way look feature selection changes representation data different method incorporating prior knowledge svms attracted attention 
method assumed knowledge probabilistic models generating data 
specifically generative model characterizes probability pattern underlying parameter 
possible construct class kernels invariant respect loosely speaking property similarity subject assumption stem generative model 
kernels called fisher kernels jaakkola haussler 
shown induce regularizers turn take account underlying model oliver sch lkopf smola 
different approach designing kernels probabilistic models watkins 
get type prior knowledge deal prior knowledge invariances 
instance image classification tasks exist transformations leave class membership invariant translations 

incorporating invariances svms section describe methods incorporating invariances svms 
distinguish types methods addressed subsections engineer kernel functions lead invariant svms generate artificially transformed examples training set subsets thereof set svs combine approaches making transformation examples part kernel definition mainly focus methods 
completeness include material method presently describe 

invariant kernel functions section briefly reviews method engineering kernel functions leading invariant svms 
sch lkopf 
consider linear decision functions sgn vi bx 
get local invariance transformations forming lie group lt minimize regularizer 
decoste sch lkopf show equivalent performing usual svm training preprocessing data 
words modification kernel function equivalent whitening data respect covariance matrix vectors 
get clearer understanding means diagonalize matrix containing eigenvalues sd 
preprocessing data consists projecting eigenvectors dividing square roots eigenvalues 
words directions main variance directions affected transformation scaled back 
note leading omitted unitary symmetric affect value dot product 
practice advisable way balance goals maximizing margin getting invariant decision function parameter 
sch lkopf 
obtained performance improvements applying technique linear case 
improvements nearly large ones get methods described section put main emphasis study 
nonlinear case method applies 
feature map corresponding kernel chosen usually nonlinear map satisfying 
compute projections mapped data eigenvectors needs essentially carry kernel pca cf 
sch lkopf chapelle sch lkopf 
material methods construct invariant kernel functions cf 
vapnik burges 

virtual examples support vectors way classifiers invariant generate artificial training examples virtual examples transforming training examples accordingly baird poggio vetter 
hoped learning machine extract invariances artificially enlarged training data 
simard 
report surprisingly training neural network artificially enlarged data set significantly slower due correlations artificial data training invariant support vector machines increase training set size 
size training set multiplied number desired invariances generating corresponding number artificial examples training pattern resulting training sets get large ones drucker schapire simard 
method generating virtual examples advantage readily implemented kinds learning machines discrete ones reflections form lie groups 
desirable construct method advantages virtual examples approach computational cost 
virtual sv method sch lkopf burges vapnik described section retains flexibility simplicity virtual examples approaches cutting computational cost significantly 
sch lkopf burges vapnik vapnik observed sv set contains information necessary solve classification task 
particular possible train different types sv machines solely sv set extracted machine test performance worse training full database sv sets fairly robust characteristics task hand 
led conjecture sufficient generate virtual examples support vectors 
hope add information generate virtual examples patterns close boundary 
high dimensional cases care exercised regarding validity intuitive picture 
experimental tests high dimensional real world problems imperative confirmed method works sch lkopf burges vapnik 
proceeds follows cf 

train support vector machine extract support vector set 
generate artificial examples termed virtual support vectors applying desired invariance transformations support vectors 
train support vector machine generated examples 
desired invariances incorporated curves obtained applying lie symmetry transformations points decision surface tangents parallel simard 
small lie group transformations generate virtual examples implies virtual support vectors approximately close decision surface original support vectors 
fairly support vectors second training run sense add considerable amount information training set 
noted method applicable invariance transformations form lie groups 
instance performance improvements object recognition bilaterally symmetric objects obtained reflections sch lkopf 

kernel jittering alternative vsv approach pre expanding training set applying various transformations perform transformations inside kernel function decoste burl simard lecun denker 
decoste sch lkopf 
suppose prior knowledge indicating decision function invariant respect horizontal translations 
true decision boundary drawn dotted line top left just limited training sample different separating hyperplanes conceivable top right 
sv algorithm finds unique separating hyperplane maximal margin bottom left case quite different true boundary 
instance lead wrong classification ambiguous point indicated question mark 
making prior knowledge generating virtual support vectors support vectors training run retraining yields accurate decision boundary bottom right 
note considered example sufficient train sv machine virtual examples generated support vectors sch lkopf 
particular explored notion jittering kernels examples explicitly jittered space discrete invariance distortions closest match 
kernel xi kij suitable traditional svm consider jittering kernel form xi ij defined procedurally follows training invariant support vector machines 
consider jittered forms example xi including turn select xq closest xj specifically select xq minimize metric distance xq space induced kernel 
distance 
ij 
kjj 
multiple invariances translations rotations considered considering cross products valid combinations generation jittered forms xi step 
investigated practicality multiple invariance kernel jittering empirical date 
kernels radial basis functions rbfs simply selecting maximum value value ij suffices jj terms constant case 
similarly holds translation long sufficient padding exists image pixels fall image translations 
general jittering kernel may consider jittering examples 
symmetric invariances translation suffices jitter just 
refer jittering kernels jsv approach 
major motivation considering jsv vsv approaches scale quadratically number considered favorable conditions jittering kernel approaches scale linearly discussed 
jittering kernels times expensive compute non jittering kernels ij computation involves finding minimum computations 
potential benefit training set times smaller corresponding vsv method vsv approach expand training set factor known svm training methods scale quadratically number training examples 
potential net gain jsv training may scale linearly quadratically vsv 
furthermore comprehensive kernel caching common modern practical svm implementations factor slow kernel computations jittering kernels may largely amortized away multiple different regularization values training simply kernel value requested times training 
long distance metric conditions satisfied jittering non negative symmetry inequality kernel matrix defined ij positive definite suitable traditional svm training satisfy mercer conditions set training examples 
practice conditions satisfied rarely detect symptoms non positive definite kernel matrix svm training jittering kernels 
smo method tolerate minor degrees non positivity discussed platt somewhat reduces convergence speed 
rarity particularly true simple translation invariances focused date empirical 
exception triangular inequality decoste sch lkopf violated 
example imagine simple images consisting single row binary pixels 
minimal jittered distances pixel translation 
distance positive linear kernel 
triangular inequality violated example 
note sufficiently large jittering set including pixel pixel translations example inequality violated 
suspect reasonably complete jittering sets odds inequality violations small 
limited preliminary experiments kernel jittering date unclear impact violations typically generalization performance practice 
jittering kernels potential disadvantage compared vsv approaches kernels continue jitter test time 
contrast vsv approach effectively compiles relevant jittering final set svs produces 
cases final jsv sv set size smaller final vsv sv set size jsv approach fast faster test time 
observed result frequent relatively small tests tried date 
know common case large scale applications 

efficient invariance training developments decomposition methods svm training demonstrated significant improvements space time costs training svms tasks 
particular smo platt svm light joachims implementations popular practical methods large scale svm training 
section discuss improvements style svm training represented approaches smo svm light improvements lead significantly faster training times cases order magnitude 
specific implementation tests enhancements variant smo described keerthi 

ideas applicable original smo specification svm light simplicity cast discussion terms known smo specification platt 
sake rest section assume svm training task consists quadratic programming qp dual formulation maximize subject yi yi xi training invariant support vector machines number training examples yi label positive example negative ith training example xi xi denotes value svm kernel function ith th examples 
output prediction svm example sign yi xi scalar bias vector alphas length variables determined qp optimization problem 

key efficiency issue maximizing cache reuse employing general qp solvers svm optimization problem typically involves space time complexities 
smo avoid space cost avoiding need explicit full kernel matrix computes recomputes elements needed 
evaluating current svm outputs training examples involves summations current support vectors 
smo approach similarly checks kkt conditions training necessarily suffers time complexity final number support vectors require final full pass examples simply check kkt conditions satisfied 
practice computing elements kernel matrix dominates training time 
element typically required times tens hundreds passes training set may required convergence final solution 
modern smo implementations try cache kernel computations possible 
caching computed kernels generally possible key reasons 
significant fraction space required avoid kernel recomputations prohibitively 

intermediate working set candidate support vectors typically larger final size full passes examples required smo working set candidate support vectors locally optimal kernel cache implemented simply frequently accessed rows implicit full kernel matrix 
instance training examples mnist data set discussed section megabytes computer memory reserved kernel cache ieee single precision byte representations kernel values rows cached time half support vectors mnist binary recognition tasks 
particular concern smo training final set support vectors known turns common examples eventually support vectors grabbing hoarding cache rows requiring kernel decoste sch lkopf rows including final support vectors continually recomputed examples cease support vectors release cache memory 
refer problem intermediate support vector bulge 
problem particularly critical vsv methods greatly enlarge number effective training examples 
addressing key issue subject section 

reducing intermediate sv bulge smo alternates full inbounds iterations examples just examples alphas considered respectively 
inbounds stage optimization continues alpha changes occur 
full stage example optimized heuristically selected example see part working inbounds set triggers new inbounds stage 
course smo full iterations uncommon number working candidate support vectors orders magnitude larger number result inbounds iteration 
kernel cache sufficiently large near sv bulge behavior critically problematic 
case computation svm outputs example quite excessive time cost computing output linear number current support vectors 
generally useful keep size support vector set closer minimal 
key problem arises size working candidate support vector set exceeded case additional support vectors able cache kernel computations 
address key problem extended smo algorithm include concept call 
basic idea jump full smo iterations early working candidate support vector set grows large amount 
switches smo inbounds iteration fully digests working candidate sv set reducing locally minimal size switching back full iteration returning example previously jumped 
allows better control size intermediate sv bulge best tradeoff cost overflowing kernel cache cost doing inbounds iterations standard smo 
similar non smo chunking methods performs full optimizations various large subsets data 
differs significantly full optimizations triggered heuristics maximizing reuse kernel cache predetermined fixed chunk size 
suspect useful distinct sparse kernel caches hash tables current implementation caches entire rows kernel matrix published implementations svm light 
current approach involves user defined settings reflect tradeoff heuristics 
expect provide meta learning methods identifying best values settings tuned particular nature specific domains costs kernel computation course multiple training sessions 
settings choosen training invariant support vector machines particularly reasonable training set sizes order kernel cache megabytes case mnist experiments see section 
heuristics highest priority 
performed wait number default new currently uncached kernel values computed 

current full iteration required default new kernel computations force 

wait long factor default times time spent previous inbounds iteration 
idea inbounds iterations expensive tolerate longer full iterations 

kernel cache full working candidate sv set size digest soon net number new working candidate svs grows past threshold default 
idea kernel values computed new sv cached critical free rows kernel cache possible 

digest net number new svs grows past threshold default 
intuition simply output computations involve summations candidate svs working set 
worthwhile periodically sure set times larger 
easily imagine promising alternative heuristics ones working candidate sv set grows percentage working candidate sv size start full iteration 
key point heuristics induce proven useful significantly reducing complexity smo training particularly difficult large scale problems 
example observed speedups times training svms mnist examples section heuristics default settings 

experiments 
handwritten digit recognition start reporting results widely known handwritten digit recognition benchmarks usps set mnist set 
described databases 
available www 
kernel machines org data html 
postal service usps database see contains handwritten digits training testing collected mail envelopes buffalo lecun 
digit image represented dimensional vector entries 
preprocessing consisted smoothing gaussian kernel width 
known usps test set difficult human error rate bromley 
discussion see simard lecun denker 
note results reported literature usps set obtained enhanced training set 
instance drucker schapire simard decoste sch lkopf 
usps training images class labels 
enlarged training set size containing additional digits note improves accuracy test set 
similarly bottou vapnik training set size 
digits test set commonly size addition distorts original learning problem situation results somewhat hard interpret 
experiments original training examples disposal 
mnist database contains handwritten digits equally divided training test set 
database modified version nist special database nist test data 
training test set consist patterns generated different writers 
images size normalized fit pixel box centered image lecun :10.1.1.138.1115
training invariant support vector machines 
mnist training images class labels 
test results mnist database literature 
lecun lecun commonly full mnist test set characters 
subset characters consisting test set patterns 
obtain results compared literature test set larger preferable point view obtaining reliable test error estimates 
mnist benchmark data available www research att com yann exdb mnist index html 

virtual sv method usps database 
set experiments conducted usps database 
database extensively literature lenet convolutional network achieving test error rate lecun 
decoste sch lkopf table 
comparison support vector sets performance training original database training generated virtual support vectors 
training runs polynomial classifier degree 
trained size av 
svs test error full training set sv set virtual sv set virtual patterns full db virtual support vectors generated simply shifting images pixel principal directions 
adding unchanged support vectors leads training set second classifier times size classifier support vector set union support vector sets binary classifiers size note due overlap smaller sum support set sizes 
note training virtual patterns generated training examples lead better results han virtual sv case training set case larger hardly leads svs 

different invariance transformations case handwritten digit recognition 
cases central pattern taken mnist database original transformed virtual examples marked grey frames class membership applying small transformations sch lkopf 
regularization constant 
value taken earlier sch lkopf burges vapnik additional model selection performed 
virtual support vectors generated set different support vectors classifiers 
alternatively carry procedure separately binary classifiers dealing smaller training sets training second machine 
table shows incorporating translational invariance improves performance significantly error rate 
types invariances improvements albeit smaller ones generating virtual support training invariant support vector machines table 
summary results usps set 
classifier train set test err nearest neighbor usps simard lenet usps lecun optimal margin classifier usps boser svm usps sch lkopf linear hyperplane kpca features usps sch lkopf local learning usps bottou vapnik virtual svm usps sch lkopf virtual svm local kernel usps sch lkopf boosted neural nets usps drucker tangent distance usps simard human error rate bromley note variants database literature denoted usps enhanced set machine printed characters improve test error 
note virtual sv systems perform best systems trained original usps set 
vectors rotation line thickness transformation drucker schapire simard constructed polynomial classifiers error rate cases 
purposes comparison summarized main results usps database table 
note generating virtual examples full database just sv sets improve accuracy enlarge sv set final classifier substantially 
finding confirmed gaussian rbf kernels sch lkopf case similar table generating virtual examples full database led identical performance slightly increased sv set size 
conclude considered recognition task sufficient generate virtual examples svs virtual examples generated patterns add useful information 

virtual sv method mnist database 
larger database information invariances decision function contained differences patterns class 
show possible improve classification accuracies technique applied method mnist database handwritten digits 
database standard performance comparisons bell labs 
virtual support vectors generated pixel translations improved degree polynomial sv classifier error rate element test set 
case applied technique separately support vector sets binary classifiers union order avoid having deal large training sets retraining stage 
note mnist database decoste sch lkopf table 
summary results mnist set 
rounding system described section performs best 
test err 
test err 
classifier nearest neighbor lecun layer mlp lecun svm sch lkopf tangent distance simard lecun lenet lecun lenet local learning lecun virtual svm sch lkopf lenet lecun dual channel vision model loe boosted lenet lecun virtual svm pixel translation see section compare results vsv technique generating virtual examples database computationally exceedingly expensive entails training large training set :10.1.1.138.1115
retraining number svs doubled 
training sets second set binary classifiers substantially smaller original database virtual svs sv times size original sv sets case amounting concluded amount data region interest close decision boundary doubled 
reasoned possible complex decision function second stage note typical vc risk bounds depends ratio vc dimension training set size vapnik 
degree polynomial led error rate close record performance 
main results mnist set summarized table 
prior best system mnist set boosted ensemble lenet neural networks trained huge database artificially generated virtual examples 
note difference error rates ist set may considered significant lecun :10.1.1.138.1115
noted slower training lenet ensemble advantage faster runtime speed 
especially number svs large svms tend slower runtime neural networks comparable capacity 
particularly virtual sv systems increasing number sv 
runtime speed issue systems sped 
burges sch lkopf shown reduce number svs original number minor losses accuracy 
study mnist set started vsv system performing test error rate sped factor accuracy training invariant support vector machines degrading slightly 
note neural nets faster cf lecun 


smo vsv experiments 
section summarize newest results mnist data set 
section polynomial kernel degree 
normalized dot products giving values yield kernel values specifically ensures kernel values sort canonical meaning holds radial basis function rbf kernels 
kernel value corresponds minimum distance identical examples kernel induced metric distance corresponds maximum distance 
ensured dot product normalizing example norm scalar value example dot gives value 
normalization form brightness amount ink equalization default sort normalization routinely nasa space image detection applications 
appears normalization mnist domain 
polynomial kernel value normalization gives kernel value special significance suspected svm regularization parameter setting probably low 
determined trying large value training binary recognizer digit training example reached alpha value 
looking histogram alpha values case realized handful examples digit classes alpha values 
assumption training examples class particularly noisy digit harder digits recognize simplicity training svm 
attempted find better values simplistic choice relatively suboptimal 
determine effect careful selection cross validation approximations generalization error upper bound estimates 
experiment pixel translations tables summarize vsv results mnist data sets pixel translations directions 
experiments employed new smo methods described section including technique 
see faster training times put tried vsv method invariance practical previous experiments 
specifically box jitter image centers corresponds translation image distance pixel directions horizontal vertical 
total training time obtaining binary recognizers decoste sch lkopf table 
errors mnist test examples vsv pixel translation 
misclassifications digit misclassifications digit class test error rate sv vsv table 
binary recognition errors mnist test examples vsv pixel translation 
errors binary recognizer svm digit sv vsv false negatives false positives false negatives false positives table 
number support vectors mnist training examples vsv pixel translation 
number support vectors binary recognizer digit sv vsv base sv vsv stages days hours sun ultra workstation mhz processor gigabytes ram allowing mb kernel cache 
training time compares favorably published results learning just single binary recognizer vsv training digit took hours platt 
vsv stage significantly expensive base sv stage averaging hours versus hour majority examples vsv training typically support vectors 
training invariant support vector machines experiment images additional translations previous mnist data achieved best results image training testing 
performed computing image principal axis horizontally translating row pixels best approximate process rotating image principal axis vertical 
example tilted digits similar close vertical 
tables summarize newest results versions mnist training test sets 
data training conditions previous experiments kernel box jitter vsv 
interestingly images improve test error rates experiments 
fact test error rate reached version data set sv errors vsv errors specific test examples errors varied somewhat 
result provide evidence polynomial kernel doing job capturing rotational invariance training table 
errors mnist test examples vsv pixel translation 
misclassifications digit misclassifications digit class test error rate sv vsv vsv table 
binary recognition errors mnist test examples vsv pixel translation 
errors binary recognizer svm digit sv vsv false negatives false positives false negatives false positives vsv false negatives false positives decoste sch lkopf table 
number support vectors mnist training examples vsv pixel translation 
number support vectors binary recognizer digit sv vsv vsv set 
comparing table shows data lead significantly fewer svs particularly high percentage reduction digit expect digits especially similar 
investigate lead better results worse ones tried vsv method box jitter combined additional translations pixels horizontally vertically training conditions experiments 
enlarged number training examples original sv require approximately times training time due large number resulted 
results experiment labelled vsv tables 
shows misclassified test examples resulted 
interesting note largest number third size full training set despite large number translation explored case 
distortions scale rotation line thickness reported help significantly domain 
clear remaining misclassifications due failure fully account invariances 

smo jittering kernels experiments 
table summarizes initial experiments comparing vsv jsv methods small subset mnist training set 
specifically digit examples digit examples training set digits example test set 
run comparisons entire data set 
polynomial kernel degree 
experiments illustrate typical relative behaviors jsv test times faster worst case times slower vsv test times jsv jitter test time due jsv having fewer final svs vsv 
furthermore jsv vsv typically beat standard svms invariance 
training invariant support vector machines 
errors mnist test data vsv 
number lower left corner image box indicates test example number 
number upper right corner indicates predicted digit label second indicates true label 
typically beat query jitter test examples jittered inside kernels svm output computations 
query jitter effectively uses jittering kernels test time svm specifically trained jittering kernel 
tested case simply control experiments verify training svm actual jittering kernel test time important 
relative test errors vsv jsv vary vsv substantially better box jitter example somewhat worse 
results date preliminary vsv methods substantially robust jsv ones terms variance generalization errors 

nasa recognition currently applying invariance approaches difficult nasa object detection tasks 
particular focusing improving known results decoste sch lkopf table 
vs results mnist test examples training set 
method test errors sv task time secs box jitter base train test query jitter test vsv train test jsv train test box jitter preprocessing principal axis vertical base train test query jitter test vsv train test jsv train test detection burl enabling useful initial results crater detection application burl 
nasa applications fact motivated efficient training invariant svms 
section preliminary results subset nasa data specifically currently publicly available uci kdd archive volcanoes data burl 
shows examples data set 
described burl 
examples determined focus attention foa matched filter designed quickly scan large images select pixel pixel subimages relatively high false alarm rates low rates 
previous best results data achieved quadratic classifier modeling classes gaussian class burl 
overcome high dimensionality pixel example vectors pca performed find principal axes positive examples examples projected axes 
mean covariance matrices computed dimensions class 
figures compare roc curves initial svms explicit invariance versus best previous gaussian models 
experiments varying homogeneous collections volcanoes regions venus heterogeneous collections examples planet various fold partitionings training test sets experiment 
roc curves represent leave fold cross validation performances experiment 
training invariant support vector machines 
positive left negative right test examples data set hom partition burl 
number example images vary thousands tens thousands experiments 
due greater number negative examples positive examples experiment factors ranging trained svms sv employing implemented ability regularization parameters single done reflect importance high detection rate despite relatively scarcity positive training examples 
specifically polynomial kernel solely manual tuning data set hom experiment 
burl 
similarly hom determine settings remaining experiments 
interesting note hom experiments svm results similar theirs 
adopted normalization bit pixel values original experiments mean pixel value example image standard deviation 
extract roc curves trained svm added sliding offset value svm output offset giving trained response 
interesting svms beat previous best gaussian model experiments offset operating point svm specifically trained 
suggests svm derived roc curves higher svm explicitly trained multiple operating points just 
interesting svm approach sort feature extraction effort pca previous required 
disadvantage svm approach training time experiments somewhat slower minutes versus minutes gaussian model machine mhz sun ultra 
nasa detection problem somewhat challenging vsv techniques progress potential invariances normalized away 
example sun light source angle intensity example considered rotation transformations useful 
furthermore focus attention mechanism centers example quite 
examples indicate slight variance location decoste sch lkopf 
roc curves experiments described burl 

solid curve roc svm dashed curve roc best gaussian model burl 

mark svm curve indicates operating point trained svm offset 
dashed line near top plot box indicates best performance possible detection rate due focus attention mechanism having non zero rate 
characteristic bright middle investigating vsv translations help 
due relatively large number negative examples explored performing vsv transformations positive examples suffice 
inspired previous succeeding pca limited capturing variance positive examples 
furthermore possible large set negative examples contain considerable translation variation 
lead significantly worse performance vsv 
standard vsv approach transforming negative positive svs critical new positive instances free claim large areas kernel feature space incorrectly appears negative instances occur 
training invariant support vector machines 
roc curves fifth experiment 
shows comparison roc curves baseline svm vsv svm pixel translations horizontal vertical svs baseline svm training conditions 
mnist domain domain replacing pixel moves away edge simple shifting constant background zero pixel 
simplicity tried replicating shifted row column presumably conflicted higher order correlations modeled nonlinear kernel 
image apply translation raw bit pixel image concern shift values shrink images ignore borders normalize example 
shrinking done baseline svm apparently influence roc curve significantly comparison hom figures indicates 
histograms svm outputs suggest key reason improvements roc curve vsv svm significantly confident outputs larger magnitude especially negative examples 
comparison tried translating examples just svs 
earlier results essentially identical slower train 
vsv data preliminary focussing hom data remaining blind vsv performance 
date observed vsv hurt roc performance long decoste sch lkopf 
dashed roc curve baseline svm mark shows threshold operating point 
solid roc curve svm including vsv pixel translations mark shows threshold operating point 
warnings helps 
suspect significant employing scale transforms starting larger images shrink translations comprehensive model selection significantly better results achieved domain 

described variety methods incorporating prior knowledge invariances svms 
experimental demonstrated techniques allow state art performance terms generalization error terms svm training time 
mentioned promising lines 
include experiments wider assortment distortions rotations scale line thickness multiple domains nasa space applications addition traditional digit recognition tasks 
interesting issue conditions applying additional distortions training data leads significantly worse test errors careful cross validation training 
expect obviously excessive distortions rotating digits training invariant support vector machines far confuse digits easily detected counter productive cross validations 
open issue extent distortions reflecting known invariances safely applied training computationally feasible extent controlling generalization error difficult 
demonstrated success training invariant svms generalize key issue lowering test time costs 
example best neural networks 
lecun appear faster test time best svms due large number virtual support examples compared test example kernel computations 
reducing number comparisons test example minimal increase test errors lines reduced sets burges sch lkopf particularly useful point 
acknowledgments chris burges michael burl olivier chapelle sebastian mika patrice simard alex smola vladimir vapnik useful discussions 
parts research carried jet propulsion laboratory california institute technology contract national aeronautics space administration 
notes 
reader familiar concept may think group transformations element labelled set continuously variable parameters cf 
simard simard lecun denker 
group may considered manifold parameters coordinates 
lie groups required group operations smooth maps 
follows instance compute derivatives respect parameters 
examples lie groups translation group rotation group lorentz group details see 


think study focused linear case 
nonlinear case studied chapelle sch lkopf 

clearly scheme iterated care exercised iteration local invariances lead global ones desirable cf 
example rotating simard lecun denker 

example shows jittered forms particular example digit 

corresponds euclidian distance feature space corresponding kernel definition norm zi zi zi zi 

incidentally expect large scale problems jsv approaches may able effectively amortize away extra jittering kernel costs times slower vsv standard kernels 
jsv working set times smaller corresponding vsv reuse limited cache effectively 

comparison subset mnist database utilizing training example 
degree polynomial classifier improved error virtual sv method increase average sv set sizes 
generating virtual examples full training set retraining obtained system slightly svs unchanged error rate 

note study done vsv systems higher accuracy obtained 
decoste sch lkopf 
difference cpu sparc ultra ii mhz versus platt intel pentium ii mhz significant imagine 
experiments cpu indicate sparc effectively faster pentium dot product computations dominate training time experiments 

burl 
free response roc curves showing probability detection versus number false alarms unit area 

data uci kdd archive burl 
slightly different data burl 
due changes foa mechanism 
author experiments burl reran roc plots involve data 
accounts slight differences roc curves shown burl 

baird 

document image defect models 
proceedings iapr workshop syntactic structural pattern recognition pp 

murray hill nj 
boser guyon vapnik 

training algorithm optimal margin classifiers 
haussler ed proceedings th annual acm workshop computational learning theory pp 

pittsburgh pa acm press 
bottou vapnik 

local learning algorithms 
neural computation 
bromley 

neural network nearest neighbor classifiers 
technical report tm burges 

geometry invariance kernel methods 
sch lkopf burges smola eds 
advances kernel methods support vector learning pp 

cambridge ma mit press 
burges sch lkopf 

improving accuracy speed support vector learning machines 
mozer jordan petsche eds 
advances neural information processing systems pp 

cambridge ma mit press 
burl 

nasa data set uci kdd archive 
see kdd ics uci edu databases volcanoes volcanoes html 
burl 

mining large image collections architecture algorithms 
grossman kamath kumar eds 
data mining scientific engineering applications 
series massive computing 
cambridge ma kluwer academic publishers 
burl asker smyth fayyad perona 

learning recognize volcanoes venus 
machine learning 
chapelle sch lkopf 

incorporating invariances nonlinear svms 
nips workshop learning kernels 
cortes vapnik 

support vector networks 
machine learning 


applicable differential geometry 
cambridge uk cambridge university press 
decoste burl 

distortion invariant recognition jittered queries 
computer vision pattern recognition cvpr 
decoste wagstaff 

alpha seeding support vector machines 
international conference knowledge discovery data mining kdd 
drucker schapire simard 

boosting performance neural networks 
international journal pattern recognition artificial intelligence 
girosi 

equivalence sparse approximation support vector machines 
neural computation 
haussler 

convolutional kernels discrete structures 
technical report ucsc crl computer science department university california santa cruz 
jaakkola haussler 

exploiting generative models discriminative classifiers 
kearns solla cohn eds 
advances neural information processing systems 
cambridge ma mit press 
training invariant support vector machines joachims 

making large scale support vector machine learning practical 
advances kernel methods support vector machines sch lkopf 
keerthi bhattacharyya murthy 

improvements platt smo algorithm svm classifier design 
technical report cd dept mechanical production engineering national university singapore 
kimeldorf wahba 

correspondence bayesian estimation stochastic processes smoothing splines 
annals mathematical statistics 
lecun boser denker henderson howard hubbard jackel 

backpropagation applied handwritten zip code recognition 
neural computation 
lecun bottou bengio haffner 

gradient learning applied document recognition 
proceedings ieee 
lecun jackel bottou cortes denker drucker guyon ller simard vapnik 

comparison learning algorithms handwritten digit recognition 
fogelman gallinari eds 
proceedings icann international conference artificial neural networks vol 
ii pp 

france ec 
oliver sch lkopf smola 

natural regularization svms 
smola bartlett sch lkopf schuurmans eds 
advances large margin classifiers pp 

cambridge ma mit press 
platt 

fast training support vector machines sequential minimal optimization 
sch lkopf burges smola eds 
advances kernel methods support vector learning pp 

cambridge ma mit press 
poggio girosi 

theory networks approximation learning 
technical report aim artificial intelligence laboratory massachusetts institute technology mit cambridge massachusetts 
poggio vetter 

recognition structure model view observations prototypes object classes symmetries 
memo artificial intelligence laboratory massachusetts institute technology 
sch lkopf 

support vector learning 
oldenbourg verlag nchen 
tu berlin 
download www kernel machines org 
sch lkopf burges smola 

advances kernel methods support vector machines 
cambridge ma mit press 
sch lkopf burges vapnik 

extracting support data task 
fayyad uthurusamy eds 
proceedings international conference knowledge discovery data mining menlo park aaai press 
sch lkopf burges vapnik 

incorporating invariances support vector learning machines 
von der malsburg von seelen sendhoff eds 
artificial neural networks icann pp 

berlin springer 
lecture notes computer science vol 

sch lkopf simard smola vapnik 

prior knowledge support vector kernels 
jordan kearns solla eds 
advances neural information processing systems pp 

cambridge ma mit press 
sch lkopf smola ller 

nonlinear component analysis kernel eigenvalue problem 
neural computation 
simard lecun denker 

efficient pattern recognition new transformation distance 
hanson cowan giles eds 
advances neural information processing systems 
proceedings conference pp 

san mateo ca morgan kaufmann 
simard lecun denker 

tangent prop formalism specifying selected invariances adaptive network 
moody hanson lippmann eds 
advances neural information processing systems san mateo ca morgan kaufmann 
smola sch lkopf ller 

connection regularization operators support vector kernels 
neural networks 

loe 

handwritten digit recognition novel vision model extracts linearly separable features 
computer vision pattern recognition cvpr 
vapnik 

nature statistical learning theory 
ny springer 
vapnik 

statistical learning theory 
ny wiley 
decoste sch lkopf watkins 

dynamic alignment kernels 
smola bartlett sch lkopf schuurmans eds 
advances large margin classifiers pp 

cambridge ma mit press 
zien tsch mika sch lkopf lengauer ller 

engineering support vector machine kernels recognize translation initiation sites 
bioinformatics 
received march revised march accepted march final manuscript march 
