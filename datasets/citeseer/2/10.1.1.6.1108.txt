finite time analysis multi armed bandit problem peter auer university technology graz graz austria igi tu graz ac nicol cesa bianchi dsi university milan milano italy dsi unimi paul fischer lehrstuhl informatik ii universit dortmund dortmund germany neurocolt technical report series nc tr november produced part esprit working group neural computational learning ii neurocolt information see neurocolt website www neurocolt com email neurocolt neurocolt com reinforcement learning policies face exploration versus exploitation dilemma search balance exploring environment nd pro table actions empirically best action possible 
popular measure policy success addressing dilemma regret loss due fact globally optimal policy followed times 
simplest examples exploration exploitation dilemma multi armed bandit problem 
lai robbins rst ones show regret problem grow logarithmically number plays 
policies asymptotically achieve regret devised lai robbins 
show optimal logarithmic regret achievable uniformly time simple ecient policies reward distributions bounded support 
keywords bandit problems adaptive allocation rules nite horizon regret 
exploration versus exploitation dilemma described search balance exploring environment nd pro table actions empirically best action possible 
simplest instance dilemma multi armed bandit problem extensively studied statistics turned fundamental di erent areas arti cial intelligence reinforcement learning evolutionary programming 
basic formulation armed bandit problem de ned random variables index gambling machine arm bandit 
successive plays machine yield rewards independent identically distributed unknown law unknown expectation independence holds rewards machines independent usually identically distributed 
policy allocation strategy algorithm chooses machine play sequence past plays obtained rewards 
number times machine played rst plays 
regret plays de ned def max 
denotes expectation 
regret expected loss due fact policy play best machine 
classical lai robbins speci families reward distributions indexed single real parameter policies satisfying kp ln main results kp def ln kullback leibler divergence reward density machine reward density machine highest reward expectation policies optimal machine played exponentially machine asymptotically 
lai robbins proved regret best possible 
allocation strategy suboptimal machine ln kp asymptotically provided reward distributions satisfy mild assumptions 
policies associating quantity called upper con dence index machine 
computation index generally hard 
fact relies entire sequence rewards obtained far machine 
index machine computed policy uses estimate corresponding reward expectation picking play machine current highest index 
agrawal introduced family policies index expressed simple function total reward obtained far machine 
policies easier compute lai robbins regret retains optimal logarithmic behavior larger leading constant cases 
strengthen previous results showing policies achieve logarithmic regret uniformly time asymptotically 
policies simple implement computationally ecient 
theorem show simple variant agrawal index policy regret logarithmically bounded arbitrary sets reward distributions bounded support regret better constants proven theorem complicated version policy 
similar result shown theorem variant known randomized greedy heuristic 
theorem show index policy logarithmically bounded nite time regret natural case reward distributions normally distributed unknown means variances 
distributions rewards machine understood context de ne 
def recall reward expectation machine minimal element set main results rst result shows exists allocation strategy ucb achieving logarithmic regret uniformly preliminary knowledge similar extensions lai robbins results obtained lowe 
main results deterministic policy ucb 
initialization play machine 
loop play machine maximizes ln average reward obtained machine number times machine played far number plays done far 
sketch deterministic policy ucb see theorem 
reward distributions apart fact support 
policy ucb sketched derived index policy agrawal 
index policy sum terms 
rst term simply current average reward 
second term related size cherno hoe ding bounds see fact con dence interval average reward true expected reward falls overwhelming probability 
theorem policy ucb run machines having arbitrary reward distributions pk support expected regret number plays ln 

expected values pk prove theorem show suboptimal machine ln plus small constant 
leading constant 
worse corresponding constant kp lai robbins result 
fact show kp 
constant best possible 
slightly complicated policy call ucb see bring main constant arbitrarily close 

policy ucb works follows 
plays divided rounds 
new round machine picked played times exponential function number rounds played machine far 
machine picked new round maximizing main results policy ucb 
parameters 
initialization set play machine 
loop 
select machine maximizing average reward obtained machine de ned number plays done far 

play machine exactly times 

set 
sketch deterministic policy ucb see theorem 
current number plays current average reward machine ln en 
result state bound regret ucb 
constant left unspeci ed de ned appendix theorem proven 
theorem policy ucb run input machines having arbitrary reward distributions pk support expected regret number max 
plays ln 


expected values pk 
choosing small constant leading term sum gets arbitrarily close 

terms sum traded letting slowly decreasing number plays 
simple known policy bandit problem called main results randomized policy greedy 
parameters 
initialization de ne sequence def min ck loop machine highest current average reward 
probability pull probability pull random arm 
sketch randomized policy greedy see theorem 
greedy rule see 
policy prescribes play probability machine highest average reward probability randomly chosen machine 
clearly constant exploration probability causes linear logarithmic growth regret 
obvious go zero certain rate exploration probability decreases estimates reward expectations accurate 
turns rate usual index current play allows prove logarithmic bound regret 
resulting policy greedy shown 
theorem reward distributions pk support policy greedy run input min 
probability number ck plays greedy chooses suboptimal machine ln ck ck ck 
large bound order 
note result stronger theorems establishes bound instantaneous regret 
theorems need know lower bound di erence reward expectations best second best machine 
result concerns special case bandit problem normally distributed rewards 
surprisingly nd literature regret bounds asymptotical case mean variance reward distributions unknown 
show index policy called ucb normal see achieves logarithmic regret uniformly knowing means variances reward distributions 
proofs deterministic policy ucb normal 
loop machine played log ne times play machine 
play machine maximizes 
ln average reward obtained machine sum squared rewards obtained machine number times machine played far 
update obtained reward sketch deterministic policy ucb normal see theorem 
theorem policy ucb normal run machines having normal reward distributions pk expected regret number plays log 
log 
means variances distributions pk nal section note theorems hold rewards independent machines dependent furthermore need rewards single arm weaker assumption proofs recall max xed policy number times machine played rst plays 
course de ne denotes machine played time proofs de ne call optimal machine index follows put superscript quantity refers optimal machine 
example write index optimal machine 
predicate de ne true 
note regret plays written 
bound regret simply bounding 
standard exponential inequalities bounded random variables 
fact cherno hoe ding bound random variables common range jx 
pfs ag pfs ag fact bernstein inequality random variables range pfs ag exp proof theorem 
ln machine upper bound sequence plays follows fi ig ln ln 
ln ln 
proofs ln min max ln 
ln ln 
observe implies bound probability events cherno hoe ding bound ln ln probability event zero 
fact ln 
ln 
get ln ln 
ln ln concludes proof 
proof theorem 
recall ck ck 
probability machine chosen time pfi jg proofs 

analysis terms right hand side 
number plays machine chosen random rst plays 







cherno hoe ding bound bx 
bx 

bx 
bx 
bx 
bernstein inequality get remains lower bound ck ln ln nd ck experiments lower bound obtain bx ln ck ck ck concludes proof 
experiments practical purposes bound theorem tuned nely 
def ln upper bound variance machine replace upper con dence bound ln policy ucb ln minf factor upper bound variance bernoulli random variable variant call ucb tuned performs substantially better ucb essentially experiments 
compared empirical behaviour policies ucb tuned ucb greedy bernoulli reward distributions di erent parameters shown table 
rows de ne reward distributions armed bandit problem rows de ne reward distributions armed bandit problem 
entries row denote reward expectations probabilities getting reward bernoulli distributions machines indexed columns 
note distributions easy reward optimal machine low variance di erences experiments plays best machine played ucb ucb ucb ucb plays regret ucb ucb ucb ucb search best value parameter policy ucb 
large distributions hard reward optimal machine high variance di erences small 
experiments test di erent policies policy di erent input parameters distributions listed 
experiment tracked performance measures percentage plays optimal machine actual regret di erence reward optimal machine reward machine played 
plot experiment shows semi logarithmic scale behaviour quantities plays averaged di erent runs 
ran rst round experiments distribution nd values parameters policies 
parameter chosen small regret grows linearly exponentially semi logarithmic plot parameter chosen large regret grows logarithmically large leading constant corresponding steep line semi logarithmic plot 
policy ucb relatively insensitive choice parameter long kept relatively small see 
xed value remaining experiments 
hand choice policy greedy dicult value works reasonably distributions considered 
roughly searched best value distribution 
plots show performance greedy values empirically best value 
shows performance degrades rapidly parameter appropriately tuned 
experiment parameter greedy set max comparison policies summarize comparison policies distributions follows 
optimally tuned greedy performs best 
signi cant exceptions distributions greedy explores 
plays best machine played ucb ucb tuned greedy greedy greedy plays regret ucb ucb tuned greedy greedy greedy comparison distribution machines parameters 
plays best machine played ucb ucb tuned greedy greedy greedy plays regret ucb ucb tuned greedy greedy greedy comparison distribution machines parameters 
uniformly machines policy hurt nonoptimal machines especially reward expectations di er lot 
furthermore greedy tuned performance degrades rapidly distribution greedy performs wide range values parameter 
cases ucb tuned performs comparably tuned greedy 
furthermore ucb tuned sensitive variance machines performs similarly distributions distributions 
policy ucb performs similarly ucb tuned slightly worse 

shown simple ecient policies bandit problem set reward distributions known bounded support exhibit uniform logarithmic regret 
policies deterministic upper con dence bounds exception greedy randomized allocation rule 
plays best machine played ucb ucb tuned greedy greedy greedy plays regret ucb ucb tuned greedy greedy greedy comparison distribution machines parameters 
plays best machine played ucb ucb tuned greedy greedy greedy plays regret ucb ucb tuned greedy greedy greedy comparison distribution machines parameters 
plays best machine played ucb ucb tuned greedy greedy greedy plays regret ucb ucb tuned greedy greedy greedy comparison distribution machines parameters 
dynamic variant greedy heuristic 
policies robust respect moderate dependencies reward processes 
extended ways 
general version 
plays best machine played ucb ucb tuned greedy greedy greedy plays regret ucb ucb tuned greedy greedy greedy comparison distribution machines parameters 
plays best machine played ucb ucb tuned greedy greedy greedy plays regret ucb ucb tuned greedy greedy greedy comparison distribution machines parameters 
bandit problem obtained removing stationarity assumption reward expectations see extensions basic bandit problem 
example suppose stochastic reward process fx associated machine playing machine time yields reward causes current state change states machines remain frozen 
studied problem setup maximization total expected reward sequence plays 
methods gittins allocation indices allow nd optimal machine play time considering reward process independently globally optimal solution depends processes 
computation gittins indices requires preliminary knowledge reward processes 
overcome requirement learn gittins indices proposed case nite state markovian reward processes 
nite time regret bounds shown solution 
moment know techniques see application gittins indices average undiscounted reward criterion 
extended general bandit problems 
acknowledgments support esprit working group ep neural computational learning ii neurocolt ii gratefully acknowledged 
agrawal 
sample mean index policies log regret multi armed bandit problem 
advances applied probability 
berry 
bandit problems 
chapman hall 

optimal adaptive policies sequential allocation problems 
advances applied mathematics 
du learning bandit problems 
proceedings th international conference machine learning pages 
morgan kaufmann 
gittins 
multi armed bandit allocation indices 
wiley interscience series systems optimization 
john wiley sons 
holland 
adaptation natural arti cial systems 
bradford books 
varaiya 
multi armed bandit problem revisited 
journal optimization theory applications 
lai robbins 
asymptotically ecient adaptive allocation rules 
advances applied mathematics 
sutton barto 
reinforcement learning 
mit press bradford books cambridge 
lowe 
nonparametric bandit methods 
annals operations research 
proof theorem note assume 
largest integer er ln en 

proof theorem note 
nishes th er er nishes th consider chain implications nishes th 



implication hold increasing er er 
er 
assumption 
implies ln en 


start bounding rst sum 
cherno hoe ding bound get 


exp 
exp 
expf 

get 
er 
proof theorem er expf 
cg dx 

manipulation yields exp dx dy change variable dy exp dx change variable 
continue bounding second sum 
get er 
er exp 
ln expf 
er exp ln expf 
er expf 
dx expf 
dy change variable expf 
dx proof theorem change variable expf 
bound series formula integral expf 
expf 
dx ln exp 
dz change variable ln dx set 


upper bound bracketed formula consider function dx derivatives dx dx interval seen zero 
interval unique maximum nd 
dx 
piecing upper bound er nd er 

ln ln en 

ln concludes proof 
proof theorem proof theorem fix machine set 
sx ln corresponding quantity best machine 
upper bound get hn sx student distributed degrees freedom nd sx ln ln probability fx bounded analogously 
sx distributed degrees freedom get pf sx ln sx max ln setting max ln completes proof theorem 

