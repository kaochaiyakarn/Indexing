proceedings siam international conference data mining sdm pp 
lake vista fl april active semi supervision pairwise constrained clustering basu computer sciences univ texas austin austin tx cs utexas edu banerjee electrical computer eng univ texas austin austin tx ece utexas edu raymond mooney computer sciences univ texas austin austin tx mooney cs utexas edu semi supervised clustering uses small amount supervised data aid unsupervised learning 
typical approach specifies limited number link constraints pairs examples 
presents pairwise constrained clustering framework new method actively selecting informative pairwise constraints get improved clustering performance 
clustering active learning methods easily scalable large datasets handle high dimensional data 
experimental theoretical results confirm active querying pairwise constraints significantly improves accuracy clustering relatively small amount supervision 
data mining machine learning tasks large supply unlabeled data limited labeled data labeled data expensive generate 
consequently semi supervised learning learning combination labeled unlabeled data topic significant interest 
specifically semisupervised clustering class labels pairwise constraints examples aid unsupervised clustering focus projects :10.1.1.11.5360:10.1.1.7.9416:10.1.1.20.7363
semi supervised clustering setting focus clustering large amounts unlabeled data presence small amount supervised data 
setting consider framework pairwise link constraints points dataset associated cost violating constraint addition having distances points 
constraints specify examples cluster link different clusters link :10.1.1.20.7363
real world unsupervised learning tasks clustering speaker identification conversation visual correspondence multiview image processing clustering multi spectral information mars images considering supervision form constraints generally practical providing class labels true labels may unknown priori easier specify pairs points belong cluster different clusters :10.1.1.118.3857
propose cost function pairwise constrained clustering pcc shown configuration energy hidden markov random field hmrf data defined potential function noise model 
pairwise constrained clustering problem equivalent finding hmrf configuration highest posterior probability minimizing energy 
algorithm solving problem 
order maximize utility limited supervised data available semi supervised setting supervised training examples actively selected maximally informative ones chosen random possible 
case fewer constraints required significantly improve clustering accuracy 
new method actively selecting pairwise constraints semi supervised clustering 
active learning pairwise constrained clustering algorithms linear size data easily scalable large datasets 
formulation handle high dimensional data experiments text datasets demonstrate 
section outlines pairwise constrained clustering framework section presents refinement kmeans clustering called pckmeans utilizes pairwise constraints 
section method actively picking constraints asking queries form examples different classes 
experimental results clustering high dimensional text data uci data demonstrate pckmeans clustering constraints performs considerably better unconstrained kmeans clustering active pckmeans achieves significantly steeper learning curves compared pckmeans random pairwise queries 
pairwise constrained clustering centroid partitional clustering algorithms kmeans find disjoint partitioning fx partition having centroid dataset fx total distance points cluster centroids locally minimized 
introduce framework pairwise constrained clustering pcc pairwise link link constraints subset points dataset cost violating constraint addition distances points :10.1.1.20.7363
centroid clustering handle pairwise constraints explicitly formulate goal clustering pcc framework minimizing combined objective function sum total distance points cluster centroids cost violating pairwise constraints 
pcc framework link constraints set link pairs implies assigned cluster set link pairs implies assigned different clusters 
note consider tuples order independent fw ij fw ij sets give weights corresponding link constraints link constraints respectively 
cluster assignment point fhg cost violating link constraints typically quantified metrics 
restrict attention uniform metric known generalized potts metric cost violating link ij linked points assigned different clusters incurred cost ij similarly cost violating link ij linked points assigned cluster incurred cost ij note indicator function true false 
model problem pcc link link constraints formulated minimizing objective function point assigned partition centroid kx ij ij minimizing objective function shown equivalent maximizing posterior configuration probability hidden markov random field details appendix 
mathematical formulation framework motivated metric labeling problem generalized potts model kleinberg proposed approximation algorithm 
formulation considers set link constraints extended pcc framework adding set link constraints 
proposed pairwise constrained kmeans pckmeans algorithm greedily optimizes kmeans type iteration modified cluster assignment step 
experiments text documents variant kmeans called spherical kmeans spkmeans computational advantages sparse high dimensional text data vectors 
algorithm motivation kmeans section easily extended spkmeans 
domains considering text clustering different costs different pairwise constraints available general simplicity assuming elements constant value 
detailed study effect choice section 
note kmeans running time number data points number dimensions number clusters 
spkmeans running time lk number non zero entries input data matrix 
linear size input making pckmeans algorithm pcc framework quite efficient 
pckmeans handle sparse high dimensional data text gene micro array computational advantage spkmeans domains 
clustering algorithm set data points set link constraints set link constraints weight constraints number clusters form propose algorithm pckmeans finds disjoint partitioning fx partition having centroid locally minimized 
previous observed proper initialization centroid algorithms kmeans provided semi supervision form labeled points greatly improves clustering performance 
labeled points supervision form constraints pairs points case goal initialization step get estimates cluster centroids pairwise constraints 
initialization step pckmeans take transitive closure link constraints augment set adding entailed constraints :10.1.1.20.7363
note current model considers consistent non noisy constraints extended handle inconsistent noisy constraints discussed section 
number connected components augmented set create neighborhood sets fn note complexity transitive closure constraint augmentation takes jmj jcj operations 
pair neighborhoods link add link constraints pair points augment link set entailed constraints assuming consistency constraints 
refer augmented link link sets respectively 
note neighborhood sets contain neighborhood information inferred link constraints unchanged iterations algorithm different partition sets contain cluster partitioning information get updated iteration algorithm 
preprocessing step get neighborhood sets fn initialize cluster centroids 
required number clusters select neighborhood sets largest size initialize cluster centers centroids sets 
initialize cluster centers centroids neighborhood sets 
look point connected links neighborhood set 
point exists initialize th cluster 
cluster centroids left uninitialized initialize random perturbations global centroid :10.1.1.44.4060
algorithm pckmeans input set data points fx set link constraints set link constraints number clusters weight constraints output disjoint partitioning fx objective function locally minimized 
method 
initialize clusters 
create neighborhoods fn 
sort indices decreasing size 
initialize centroids fn initialize centroids fn point linked neighborhoods fn initialize initialize remaining clusters random 
repeat convergence 
assign cluster assign data point cluster set arg min kx 
estimate means jx xg 
pckmeans algorithm algorithm pckmeans alternates cluster assignment centroid estimation steps see 
cluster assignment step pckmeans point assigned cluster minimizes sum distance cluster centroid cost constraint violations incurred cluster assignment equivalently satisfying links links possible assignment 
note cluster assignment step order dependent subsets associated cluster may change assignment point 
experiments consider random ordering points assignment step 
centroid re estimation step kmeans cluster centroid calculated mean points assigned cluster 
lemma 
algorithm pckmeans converges local minimum proof 
analyzing cluster assignment step consider 
point moves new cluster component kx contributed point decreases 
points new assignment decrease remain 
analyzing centroid re estimation step consider equivalent form xh kx ij ij cluster centroid re estimated mean points partition minimizes component xh kx contributed partition pairwise constraints take part step constraints explicit function centroid 
result term xh kx minimized step 
objective function decreases cluster assignment centroid re estimation step till convergence implying pckmeans algorithm converge local minimum active learning algorithm semi supervised setting getting labels data point pairs may expensive 
section discuss active learning scheme pcc setting order improve clustering performance queries possible 
formally scheme access noiseless oracle assign link link label pair pose constant number queries oracle 
order get pairwise constraints informative random pcc model developed active learning scheme selecting pairwise constraints farthest traversal scheme 
basic idea farthest traversal set points find points far 
farthest traversal select starting point random choose point farthest add traversed set pick point farthest traversed set standard notion distance set min 
farthest traversal gives efficient approximation center problem construct hierarchical clusterings performance guarantees level hierarchy 
data model see appendix prove interesting property farthest traversal see appendix justifies active learning 
observed initializing kmeans centroids estimated set labeled examples cluster gives significant performance improvements 
certain generative model assumptions connect mixture gaussians model kmeans model :10.1.1.48.3989
direct calculation chernoff bounds shows particular cluster underlying gaussian model true centroid seeded points drawn independently random corresponding gaussian distribution estimated centroid pr 
deviation centroid estimates falls exponentially number seeds seeding results initial centroids 
initial centroids critical success greedy algorithms kmeans follow principle pairwise case try get points proportional actual cluster size possible cluster asking pairwise queries pckmeans initialized set centroids 
proposed active learning scheme phases 
phase explores data get pairwise disjoint non null neighborhoods belonging different cluster underlying clustering data fast possible 
note point neighborhood neighborhood structure defines correct skeleton underlying cluster 
phase propose algorithm explore essentially uses farthest scheme form appropriate queries getting required pairwise disjoint neighborhoods 
oracle give don know response query case response ignored pair considered constraint query posed 
explore point obtained cluster 
remaining queries consolidate structure 
cluster skeleton obtained explore initialize pairwise disjoint nonnull neighborhoods fn point existing neighborhoods ask queries pairing member disjoint neighborhoods find neighborhood belongs 
principle forms second phase active learning algorithm call algorithm phase consolidate 
phase able get correct cluster label asking queries 
pairwise labels equivalent single pointwise label worst case 
details algorithms performing exploration consolidation 
algorithm explore input set data points fx access oracle answers pairwise queries number clusters total number queries output disjoint neighborhoods fn corresponding true clustering point neighborhood 
method 
initialize set neighborhoods fn null 
pick rst point random add 
queries allowed point farthest existing neighborhoods fn querying linked existing neighborhoods start new neighborhood add neighborhood linked algorithm explore explore exploration phase interesting property farthest traversal 
set disjoint balls unequal size metric space show farthest scheme sure get point balls reasonably small number attempts see appendix 
algorithm explore see uses farthest traversal getting skeleton structure neighborhoods 
explore queries allowed pairwise disjoint neighborhoods point farthest existing neighborhoods chosen candidate starting new neighborhood 
queries posed pairing random point existing neighborhoods 
linked existing neighborhoods new neighborhood started link obtained particular neighborhood added neighborhood 
continues till algorithm runs queries pairwise disjoint neighborhoods 
case active learning scheme enters consolidation phase 
algorithm consolidate input set data points fx access oracle answers pairwise queries number clusters total number queries disjoint neighborhoods corresponding true clustering point neighborhood 
output disjoint neighborhoods corresponding true clustering higher number points neighborhood 
method 
estimate centroids neighborhoods 
queries allowed 
randomly pick point existing neighborhoods 
sort indices increasing distances kx 
query neighborhoods sorted order till link obtained add neighborhood algorithm consolidate consolidation consolidation phase starts point obtained clusters 
basic idea consolidation phase points clusters proper neighborhood random point determined maximum queries 
queries formed point neighborhoods turn asking label pair till link obtained 
get link reply queries get link replies queries neighborhoods infer point linked remaining neighborhood 
note practical sort neighborhoods increasing order distance centroids correct link neighborhood encountered sooner querying process 
outline algorithm consolidate 
explore consolidate add points clusters rate 
shown result appendix explore phase gets point underlying clusters maximum queries 
active scheme finished running explore running consolidate shown generalization coupon collector problem appendix high probability get new point cluster approximately log queries 
consolidate adds points clusters faster rate explore factor log validated experiments section 
note analysis balanced clusters similar analysis unbalanced clusters gives improvement factor 
briefly address case right number clusters known clustering algorithm unknown active learning scheme 
case explore queries allowed 
explore keep discovering new clusters fast 
obtained clusters way knowing 
point onwards farthest draws dataset find neighborhood linked 
discovering clusters explore essentially consolidate clusters 
known sense invoke consolidate adds points clusters faster rate explore picks random samples underlying data distribution certain nice properties terms estimating centroids chernoff bounds centroid estimates shown samples obtained farthest traversal need 
experiments section outline details experiments 
datasets experiments high dimensional text documents datasets created newsgroups collection 
messages collected different usenet newsgroups messages newsgroup 
original dataset reduced dataset news created random subsample documents newsgroups subsample difficult dataset cluster original newsgroups cluster fewer documents 
news points dimensions 
selecting categories reduced dataset news datasets created news sim consists newsgroups similar topics comp graphics comp os ms windows comp windows significant cluster overlap news diff consists newsgroups different topics alt atheism rec sport baseball sci space separated clusters 
news sim points dimensions news diff points dimensions 
dataset experiments subset classic containing documents cranfield documents aeronautical system papers medline documents medical journals cisi documents information retrieval papers 
classic subset dataset specifically designed create clusters unequal size points dimensions 
similarities data points text datasets computed cosine similarity spkmeans 
spkmeans maximizes average cosine similarity points cluster centroids objective function monotonically increases iteration till convergence 
text datasets preprocessed methodology dhillon 
experiments low dimensional data selected uci dataset iris points dimen www ai mit edu people newsgroups sions 
euclidean metric computing distances points dataset kmeans 
case objective function average squared euclidean distance points cluster centroids decreases iteration till convergence 
iris dataset pre processed way 
clustering evaluation metrics cluster evaluation normalized mutual information nmi pairwise measure objective function 
normalized mutual information nmi determines amount statistical information shared random variables representing cluster assignments class assignments data points 
computed nmi methodology strehl 
nmi measures closely clustering algorithm reconstruct underlying label distribution data 
random variable denoting cluster assignments points random variable denoting underlying class labels points nmi measure defined nmi jy mutual information random variables shannon entropy jy conditional entropy pairwise measure defined harmonic mean pairwise precision recall traditional information retrieval measures adapted evaluating clustering considering pairs points pair points explicit constraints decision cluster pair different clusters considered correct matches underlying class labeling available points :10.1.1.140.2184
pairwise measure defined recision recall measure recision recall recision recall measures modified rand index frequently evaluation clustering pcc framework similar pairwise measure 
nmi popular clustering evaluation metric 
results evaluation measures observe results strongly correlated 
show results objective function number pairwise constraints normalized mutual information pckmeans seeded kmeans constrained kmeans active pckmeans active seeded kmeans active constrained kmeans comparison nmi values news sim number pairwise constraints pairwise measure pckmeans seeded kmeans constrained kmeans active pckmeans active seeded kmeans active constrained kmeans comparison pairwise measure values news sim experimental methodology algorithms dataset generated learning curves fold cross validation axis represents number pairwise constraints input algorithms 
non active pckmeans pairwise constraints selected random active pckmeans pairwise constraints selected active learning scheme 
studying effect pairwise constraints active learning dataset set aside test set particular fold 
training sets different points learning curve pairwise constraints obtained remaining data increasing number pairwise constraints input clustering learning curve 
clustering algorithm run dataset corresponding objective function reported 
methodology basu nmi pairwise measure calculated test set constraints supplied 
results point learning curve obtained averaging folds 
continue learning curve queries news general nature results evident range 
practical active learning applications unrealistic expect user answer queries 
results results experiments shown figures 
standard deviations nmi pairwise measure objective function values plots small datasets shown plots reduce clutter 
choice experimented different values constraint weight parameter set algorithm initialized neighborhoods derived constraints normal kmeans iterations run till convergence 
similar algorithm outlined labeled data seeds initialize kmeans algorithm steps algorithm 
set high value algorithm initialized neighborhoods derived constraints constraints hard constraints constraint cost violation component objective function far supersedes distance component 
similar algorithm outlined 
algorithm seeds initialize kmeans algorithm 
subsequent steps cluster labels seed data kept unchanged labels non seed data re estimated 
set intermediate value algorithm gives tradeoff minimizing total distance points cluster centroids cost violating constraints 
result plots figures pckmeans refers running algorithm intermediate value parameter chosen user degree confidence constraints chosen constant order average similarity spherical kmeans distance euclidean kmeans pairs points dataset 
set text datasets iris dataset 
parameter acts tuning knob giving continuum algorithm extreme guarantee constraint satisfaction clustering algorithm extreme clustering process forced respect constraints 
note selectively guarantee particular constraint satisfied clustering iterations selecting high corresponding cost constraint violation 
comparative results active non active algo number pairwise constraints normalized mutual information pckmeans active pckmeans comparison nmi values news diff number pairwise constraints pckmeans active pckmeans comparison objective function news diff number pairwise constraints normalized mutual information pckmeans active pckmeans comparison nmi values news number pairwise constraints pckmeans active pckmeans comparison nmi values classic number pairwise constraints pckmeans active pckmeans comparison nmi values iris number pairwise constraints objective function pckmeans active pckmeans comparison objective function iris rithms obtained different values similar datasets considered see figures 
leads conclude proper initialization active learning constraints gives benefit satisfying constraints algorithm validates observation proper initialization crucial performance centroid clustering algorithms 
point explained detail discussion 
figures results intermediate value clarity plots 
curves nmi pairwise measure similar datasets considered see figures presenting nmi results 
objective function results show representative objective function plot text dataset clustered spkmeans objective function increases learning curve 
objective function decreasing learning curve simple kmeans euclidean distance dataset 
note objective function plot active non active schemes number constraints sets point learning curve actual constraints may different 
active non active schemes allowed choose sets constraints objective function value running pckmeans clustering depends choice 
active pckmeans constraints chooses give better initialization discussed detail resulting better value objective function running clustering algorithm 
non active schemes shown appendix number random pairwise constraints low probability largest neighborhoods fact different clusters low 
point learning curve neighborhoods initialize pckmeans belong cluster may get representatives clusters 
gives poor initialization pckmeans may cause algorithm converge bad local minima 
consequently clustering produced pckmeans unstable resulting varying pairwise measure nmi values test set 
initial jitter observed figures 
point learning curve non active pckmeans initialized points cluster 
initial jitter performance non active pckmeans improves steadily learning curve respect objective function nmi pairwise measure showing benefit incorporating supervised data constraints clustering process 
active schemes active algorithms consistently get significant improvements non active algorithms datasets considered 
firstly see jitter early part learning curve 
explore phase creates neighborhood cluster continues pairwise disjoint neighborhoods creating neighborhoods small number queries see appendix 
jitter early learning curve observed plots 
jitter disappears queries 
explore phase active selection algorithm guarantees pairwise disjoint neighborhoods inferred constraints belong different clusters actual underlying clustering neighborhoods give initializations clustering algorithm 
consolidate phase grows pairwise disjoint neighborhoods created active learning scheme runs queries pckmeans initialized centroids constructed neighborhoods 
improvement active scheme pronounced difficult high dimensional text datasets 
results conclude semi supervised clustering constraints performs considerably better unsupervised clustering datasets considered note unsupervised clustering corresponds semi supervised clustering constraints 
active non active algorithms clustering evaluation measures nmi pairwise measure objective function improve increasing number pairwise constraints provided learning curve 
active selection pairwise constraints twophase active learning algorithm significantly outperforms random selection constraints 
number pairwise constraints pckmeans active pckmeans explore consolidate active pckmeans explore comparison explore consolidate phases nmi news diff explore vs consolidate ran ablation experiments comparing performance active pckmeans scheme explore consolidate active pckmeans explore 
ran ablation experiment news diff dataset 
nmi result shown see running explore active learning phase gives improvement random choice constraints running explore consolidate gives better results 
explore consolidate useful phases active learning algorithm 
related cop kmeans algorithm pairwise constrained clustering model handle constraints violated associated violation cost pckmeans :10.1.1.20.7363
algorithm scop kmeans proposed performance interesting compare pckmeans 
bansal proposed theoretical model performed clustering pairwise constraints different model consider constraints underlying metric points clustering 
pairwise constrained clustering model includes learning distance metrics clustering pairwise constraints :10.1.1.11.5360
domain cohn 
proposed iterative acquire constraints active learning algorithm 
active learning classification framework problem different principles query selection studied reduction version space size reduction uncertainty predicted label maximizing margin training data finding high variance data points density weighted pool sampling active learning techniques classification applicable clustering framework basic underlying concept reduction classification error variance distribution examples defined clustering 
unsupervised setting hofmann consider model active learning different incomplete pairwise similarities points active learning goal select new data expected value information estimated existing data risk making wrong estimates true underlying clustering existing incomplete data minimized 
contrast model assumes complete similarity information pairs points pairwise constraints violation cost component objective function active learning goal select pairwise constraints informative underlying clustering 
klein consider active learning semi supervised clustering making queries cluster level queries ask user clusters merged :10.1.1.11.5360
answering example level queries cluster level queries easier task user making model practical real world active learning setting 
pairwise constrained clustering framework new theoretically motivated method actively selecting pairwise constraints semi supervised clustering 
experiments text uci data show pckmeans constraints performs considerably better unconstrained kmeans active learning scheme performs quite giving significantly steeper learning curves compared random pairwise queries 
active learning pairwise constrained clustering algorithms linear suitable realworld clustering applications easily scaled large datasets handle high dimensional data 
explore stage active learning scheme currently sensitive outliers data 
note sensitive outliers baseline algorithm kmeans 
outlier sensitivity handled point selection explore modified farthest traversal selects distant points dense regions data space 
formulation active learning robust outliers outlier robust clustering algorithms 
current clustering model assumes constraints consistent noise constraints 
working incorporating noise model pcc framework able handle noisy constraints 
involve changes algorithms adding inferred constraints neighborhoods initialization step pckmeans selectively rejecting points noise model explore stage active learning algorithm clustering model assumes right number clusters input want select number clusters automatically incorporating model selection criterion pcc objective function 
cluster assignment step pckmeans algorithm incremental step dependent order points assigned cluster 
currently random ordering cluster assignment experiments 
plan investigate cluster assignment schemes look online version pckmeans 
theoretical side want explore correlation nmi measure empirically observed experiments come better guarantees improvement pcc actively selected constraints pcc randomly selected constraints 
want apply active learning algorithm pcc framework domains especially gene micro array data analysis bioinformatics 
acknowledgments research supported part ibm phd fellowship intel nsf iis 
appendix hidden markov random field pckmeans objective function tries minimize total distance points cluster centroids minimum number specified constraints points violated 
mathematical formulation motivated considering markov random field mrf defined field set ff random variables take values fl fhg :10.1.1.118.3857
configuration denote joint event ff lg ff restricting model mrfs clique potentials involve pairwise points prior probability configuration exp ij ij assume identity covariance gaussian noise model observed data von mises fisher distribution considered noise model high dimensional text data assume observed noisy data points drawn independently model 
denote true representatives corresponding labels fhg conditional probability observation configuration jl exp kx 
mrf defined hidden true labels fl observed points fx model called hidden markov random field hmrf direct generalization hidden markov model 
posterior probability configuration jl pcc objective function proportional negative logarithm posterior probability specified hmrf 
finding map configuration hmrf equivalent minimizing objective function 
model assumptions formal model dataset analysis done 
data assumed coming disjoint uniform density balls unequal size metric space 
framework shown hold arbitrary exponential noise models 
balls defined terms metric 
data points inside particular ball assumed cluster points different balls assumed different clusters 
oracle assumed know model 
total number points consideration 
probabilities drawing point randomly th ball loss generality assume 
number points dataset volume 
number possible links fh number links mh fh mh analysis random initialization pckmeans initialization done largest sized neighborhoods 
argue small number queries probability getting point neighborhood cluster low 
pairs random average link pairs 
total link pairs expected behavior 
th cluster link pairs average 
focus particular cluster pairs selected random 
size cluster get point neighborhood points gets drawn random pair sampling 
sampling pairs replaced sampling vertices probability getting vertex twice increased 
probability getting point neighborhood lower bounded probability getting vertex twice vertex sampling setting 
rh 
mh rh 
rh close small values probability getting point neighborhoods low 
initialization essentially done random draws set approximately point neighborhoods 
setting probability getting exactly neighborhood cluster 

am gm inequality stirling formula 
clearly probability quite low 
results significant variance initializing neighborhoods explains initial jitter non active algorithms low values analysis explore shall refer points cluster having color 
probability drawing points different colors 
extension coupon collector problem show points colors drawn high probability ln draws 
claim farthest scheme gets points colors attempts probability 
worst case disjoint balls placed adversary adversary try place balls getting point ball difficult 
show optimum strategy adversary smallest ball difficult reach 
packing argument show irrespective placement balls farthest traversal avoid particular ball long 
consider balls probabilities radii balls vb volumes balls 
denote packing number balls maximum number disjoint balls packed inside ball just balls universe farthest traversal starts points obtained entering pairwise distances centers traversal picked farthest point got distance traversal stay farthest jumps exactly points inside distance 
packing number vb ratio probabilities 
argument extended general case balls 
general case number times farthest traversal continue entering ball clearly number largest smallest ball maximum number farthest jumps reaching jump farthest gets point farthest traversal find points colors attempts 
note significant ln factor improvement random scheme 
abe 
query learning strategies boosting bagging 
proc 
th intl 
conf 
machine learning icml pages 
banerjee dhillon ghosh sra :10.1.1.140.2184
generative model clustering directional data 
proc 
th acm sigkdd intl 
conf 
knowledge discovery data mining kdd 
nikhil bansal avrim blum chawla 
correlation clustering 
ieee symp 
foundations comp 
sci 
basu banerjee mooney 
semi supervised clustering seeding 
proc 
th intl 
conf 
machine learning icml 
blake merz 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
avrim blum tom mitchell 
combining labeled unlabeled data training 
proc 
th annual conf 
computational learning theory 
yuri boykov olga veksler ramin zabih :10.1.1.118.3857
markov random fields efficient approximations 
ieee computer vision pattern recognition conf 
david cohn rich caruana andrew mccallum 
semisupervised clustering user feedback 
technical report tr cornell university 
david cohn zoubin ghahramani michael jordan 
active learning statistical models 
journal artificial intelligence research 
sanjoy dasgupta 
performance guarantees hierarchical clustering 
conf 
computational learning theory pages 
dhillon modha 
concept decompositions large sparse text data clustering 
machine learning 
byron dom 
information theoretic external measure 
research report rj ibm 
richard duda david stork peter hart 
pattern classification nd ed 
wiley new york ny 
usama fayyad cory reina bradley :10.1.1.44.4060
initialization iterative refinement clustering algorithms 
proc 
th intl 
conf 
knowledge discovery data mining kdd pages 
fern carla brodley 
random projection high dimensional data clustering cluster ensemble approach 
proc 
th intl 
conf 
machine learning icml 
yoav freund sebastian seung eli shamir naftali tishby 
selective sampling query committee algorithm 
machine learning 
bar hillel hertz noam weinshall 
learning distance functions equivalence relations 
proc 
th intl 
conf 
machine learning icml 
hochbaum shmoys 
best possible heuristic center problem 
mathematics operations research 
thomas hofmann joachim buhmann 
active data clustering 
advances neural information processing systems 
thorsten joachims 
transductive inference text classification support vector machines 
proc 
th intl 
conf 
machine learning icml 
michael kearns mansour andrew ng :10.1.1.48.3989
information theoretic analysis hard soft assignment methods clustering 
proc 
th conf 
uncertainty artificial intelligence uai pages 
dan klein kamvar christopher manning :10.1.1.11.5360
instance level constraints space level constraints making prior knowledge data clustering 
proc 
th intl 
conf 
machine learning icml 
jon kleinberg eva tardos 
approximation algorithms classification problems pairwise relationships metric labeling markov random fields 
ieee symp 
foundations comp 
sci 
david lewis william gale 
sequential algorithm training text classifiers 
proc 
th intl 
acm sigir conf 
research development information retrieval 
macqueen 
methods classification analysis multivariate observations 
proc 
th berkeley symp 
mathematical statistics probability pages 
mardia 
directional statistics 
john wiley sons nd edition 
andrew mccallum kamal nigam 
employing em pool active learning text classification 
proc 
th intl 
conf 
machine learning icml madison wi 
morgan kaufmann 
francis editors 
discrete location theory 
wiley new york ny 
motwani raghavan 
randomized algorithms 
cambridge university press 
nigam mccallum thrun mitchell 
text classification labeled unlabeled documents em 
machine learning 
alexander strehl joydeep ghosh raymond mooney 
impact similarity measures web page clustering 
workshop artificial intelligence web search aaai pages july 
wagstaff 
intelligent clustering instance level constraints 
phd thesis cornell university 
wagstaff claire cardie seth rogers stefan :10.1.1.20.7363
constrained means clustering background knowledge 
proc 
th intl 
conf 
machine learning icml 
eric xing andrew ng michael jordan stuart russell 
distance metric learning application clustering side information 
advances neural information processing systems 
mit press 
zhang smith brady 
hidden markov random field model segmentation brain images 
technical report tr yz oxford center john radcliffe hospital 
