hidden markov model approach sequential data clustering dipartimento di informatica university ca le italy sci 
clustering sequential temporal data challenging traditional clustering dynamic observations processed static measures 
proposes hidden markov model hmm technique suitable clustering data sequences 
main aspect probabilistic model approach hmm derive new proximity distances likelihood sense sequences 
novel partitional clustering algorithm designed alleviates computational burden characterizing traditional hierarchical agglomerative approaches 
experimental results show approach provides accurate clustering partition devised distance measures achieve performance rates 
method demonstrated real world data sequences eeg signals due temporal complexity growing interest emerging field brain computer interfaces 
key words hidden markov model sequence clustering eeg signal autoregressive hmm analysis sequential data doubts interesting application area real processes show dynamic behavior 
examples reported analysis dna strings classification genes protein family modeling sequence alignment 
problem unsupervised classification temporal data tackled technique hidden markov models hmms 
hmms viewed stochastic generalizations finite state automata transitions states generation output symbols governed probability distributions 
basic theory hmms developed late decade extensively applied large number problems speech recognition handwritten character recognition dna protein modeling gesture recognition behavior analysis synthesis general computer vision problems 
related sequence clustering hmms extensively papers literature 
early works proposed related speech recognition 
interesting approach directly linked speech issues smyth clustering faced devising distance measure sequences hmms 
assuming model structure known algorithm trains hmm sequence log likelihood ll model sequence computed 
information build ll distance matrix cluster sequences groups hierarchical algorithm 
subsequent li biswas address clustering problem focusing model selection issue search hmm topology best representing data clustering structure issue finding number clusters 
issue addressed standard approach bayesian information criterion extending continuous case bayesian model merging approach 
regarding issue sequence hmm likelihood measure enforce similarity criterion 
optimal number clusters determined maximizing partition mutual information pmi measure inter cluster distances 
second problems addressed terms bayesian model selection bayesian information criterion bic stutz cs approximation 
justified heuristics introduced alleviate computational burden making problem tractable despite remaining elevate complexity 
model clustering method proposed hmms cluster prototypes rival penalized competitive learning state merging adopted find hmms modeling data 
approaches interesting theoretical point view tested real data 
computationally expensive 
idea smyth extended defining new metric measure distance likelihood sense sequences 
clustering algorithms proposed hierarchical agglomerative approach second partitional method variation means strategy 
particular care posed hmm training initialization utilizing kalman filtering clustering method mixture gaussians 
important proposed algorithm tested real data sequences electroencephalographic eeg signals 
analysis kind signals important years due growing interest field brain computer interface bci 
choose signals temporal complexity suitable hmm modeling 
rest organized follows 
sect 
hmm introduced 
section describes eeg signal modeled specific initialization phase proposed approach 
core algorithm sect 
definition distances clustering algorithms detailed 
subsequently experimental results sect 
drawn sect 

hidden markov models discrete hmm formally defined elements set sn hidden states 
state transition probability distribution called transition matrix aij representing probability go state si state sj 
aij qt sj qt si aij aij set vm observation symbols 
observation symbol probability distribution called emission matrix bj indicating probability emission symbol vk system state sj 
bj vk time qt sj bi bj 
initial state probability distribution representing probabilities initial states 
si convenience denote hmm triplet 
discussion considered case observation characterized sequence discrete symbols chosen finite alphabet 
application observations continuous signals 
possible quantize continuous signals codebooks advantageous able hmms continuous observation densities 
case emission probability distribution bj jm jm observation vector modeled mixture coefficient mth mixture state log concave symmetric density gaussian density 
adaption reestimation formulas baum welch procedure continuous case straightforward 
general formulation continuous density hmms applicable wide range problems interesting class hmms particularly suitable eeg signals autoregressive hmms 
case observation vectors drawn autoregression process 
section explained models applied eeg modeling 
eeg signal modeling electroencephalographic eeg signals represent brain activity subject give objective mode recording brain stimulation 
useful tool understanding aspects brain diseases detection sleep analysis potential analysis 
system model eeg signal largely penny roberts key idea approach train autoregressive hmm directly eeg signal intermediate ar representation 
hmm state associated different dynamic regime signal determined kalman filter approach 
kalman filter preliminary segment signal different dynamic regimes estimates fine tuned hmm model 
approach briefly resumed rest section 
hidden markov ar models type models differs defined sect 
definition observation symbol probability distribution 
case defined yt qt si yt ft ft yt yt yt column vector ar coefficients ith state estimated observation noise th state estimated method 
prediction ith state ft order ar model hmm training procedure fundamentally gradient descent approach sensitive initial parameters estimate 
overcome problem kalman filter ar model passed data obtaining sequence ar coefficients 
coefficients corresponding low evidence discarded 
gaussian mixture models 
center gaussian cluster initialize ar coefficients state hmm ar model 
number clusters number hmm states order autoregressive model decided performing preliminary analysis classification accuracy 
varying number states varying order autoregressive model best configuration 
classification accuracy obtained superior obtained neural network data showing hidden markov models effective modeling eeg signals 
initialize transition matrix prior knowledge problem domain average state duration densities 
equation aii hmm remain state samples 
number computed knowing eeg data stationary period order half second 
proposed method approach inspired depicted algorithm 
train states hmm sequence si dataset hmm identified initialized kalman filter ar model described sect 

model evaluate probability generate sequence sj obtaining measure matrix lij sj 
apply suitable clustering algorithm matrix obtaining clusters data set method aims exploits measure defined naturally expresses similarity observation sequences 
hidden markov models able describe sequence simple scalar number transform difficult task clustering sequences easier clustering points 
step apply clustering algorithms need matrix result step really distance matrix 
define ij lij kind hmm measure applied remind kullback leibler information number defines distance lkl hmm symmetrized version ij kl lii ln lii lij ln lij ij kls ij kl kl introduced measure called bp metric defined ij lij lii bp lii motivated considerations measure defines similarity measure sequences si sj likelihood sequence si respect model trained sj really account sequence sj 
words kind measure assumes sequences modeled quality considering sequence sj modeled hmm true 
proposed distance considers modeling goodness evaluating relative normalized difference sequence training likelihoods 
step investigated clustering algorithms complete link agglomerative hierarchical clustering class algorithms produces sequence clustering decreasing number clusters step 
clustering produced step results previous merging clusters 
partitional clustering methods obtains single partition data clustering structure dendogram produced hierarchical technique 
partitional method advantages application involving large data sets construction dendogram computationally prohibitive 
context developed ad hoc partitional method described section henceforth called 
partitional clustering algorithm proposed algorithm shares ideas known means techniques 
method finds optimal partition evaluating iteration distance item cluster descriptor assigning nearest class 
step descriptor cluster reevaluated averaging cluster items 
simple variation method partition medoid pam determines cluster representative choosing point nearest centroid 
context evaluate centroid cluster item distances values 
address problem novel algorithm proposed 
method able determine cluster descriptors pam paradigm item distances values 
choice initial descriptors affect algorithm performances 
overcome problem adopted multiple initialization procedure best resulting partition determined sort davies criterion 
fixed number tested initializations number sequences number clusters proximity matrix characterized previously defined distances resulting algorithm initial cluster representatives randomly chosen 


repeat partition evaluation step compute cluster sequence si 
belongs si lies cluster distance si 

minimum 
parameters upgrade compute sum distance element cluster cj element jth cluster determine index element cj sum minimal index new descriptor cluster cj representatives values successive iterations don change 
rt 
ck compute davies index defined dbl max sc cr sl cs sc intra cluster measure defined cr cr cr endfor final solution best clustering rp minimum davies index viz arg mint dbl experiments order validate exposed modeling technique worked primarily eeg data recorded zak purdue university 
dataset contains signal recorded different subjects asked perform mental tasks baseline task subjects asked relax possible math task subjects nontrivial multiplications problems asked solve making physical movements letter task subjects instructed mentally compose letter friend geometric rotation subjects asked visualize particular block rotated axis visual counting task subjects asked image blackboard visualize numbers written board sequentially 
applied method segment segment basis signals sampled hz drawn dataset cardinality varying mental states sequences mental states removed segments biased signal spikes arising human artifact ocular blinks 
proposed hmm clustering algorithm applied mental states baseline math task extend trials available data 
accuracies computed comparing clustering results real segment labels percentage merely ratio correct assigned label respect total number segments 
applied hierarchical complete link technique varying proximity measure results shown table number mental states growing 
note accuracies quite satisfactory 
method experimented considered best measures effective 
applied partitional algorithm datasets setting number initializations experiments 
results table case bp distance slightly better experimented measures 
final comparison bp kl sm natural clusters natural clusters natural clusters natural clusters bp kl sm table 
results hierarchical complete link partitional clustering varying distances defined bp kl sm 
partitional agglomerative hierarchical algorithms underlines remarkable differences proposed approaches 
clearly partitional approaches alleviates computational burden preferred dealing complex signals clustering eeg 
comparison clustering classification results obtained earlier works shown just slightly better 
strengthen quality proposed method considering unsupervised classification inherently difficult task 
addressed problem unsupervised classification sequences hmm approach 
models suitable modeling sequential data characterize similarity sequences different ways 
extend ideas exposed defining new metric likelihood sense data sequences applying distance matrices clustering algorithms traditional hierarchical agglomerative method novel partitional technique 
partitional algorithms generally computational demanding hierarchical applied context proper adaptations proposed 
tested approach real data complex temporal signals eeg increasing importance due interest brain computer interface 
results shown proposed method able infer natural partitions patterns characterizing complex noisy signal eeg ones 

rabiner tutorial hidden markov models selected applications speech recognition 
proc 
ieee 

hu brown turin hmm line handwriting recognition 
ieee trans 
pattern analysis machine intelligence 

hughey krogh hidden markov model sequence analysis extension analysis basic method 
comp 
appl 
biosciences 

hidden markov model online gesture recognition 
proc 
int 
conf 
pattern recognition icpr 

jebara pentland action reaction learning automatic visual analysis synthesis interactive behavior 
st intl 
conf 
computer vision systems 

rabiner lee juang hmm clustering connected word recognition 
proceedings ieee icassp 

lee context dependent phonetic hidden markov models speaker independent continuous speech recognition 
ieee transactions acoustics speech signal processing 

smyth clustering sequences hmm advances neural information processing mozer jordan petsche eds 
mit press 

li biswas clustering sequence data hidden markov model representation spie conference data mining knowledge discovery theory tools technology 

li biswas bayesian approach temporal data clustering hidden markov models 
intl 
conference machine learning 

schwarz estimating dimension model 
annals statistics 

stolcke omohundro hidden markov model induction bayesian model merging 
hanson cowan giles eds 
advances neural information processing systems 

cheeseman stutz bayesian classification autoclass theory results 
advances knowledge discovery data mining 

law kwok rival penalized competitive learning model sequence proceedings intl conf 
pattern recognition icpr 

penny roberts curran stokes eeg communication pr approach 
ieee trans 
rehabilitation engineering 

juang levinson sondhi maximum likelihood estimation multivariate mixture observations markov chain 
ieee trans 

theory 

juang rabiner mixture autoregressive hidden markov models speech signals 
ieee trans 
acoust 
speech signal proc 


penny roberts dynamic models nonstationary signal segmentation 
computers biomedical research 

kalman new approach linear filtering prediction problems 
transaction asme journal basic engineering 

adaptive filtering 
automatica 

theodoridis pattern recognition 
academic press 

anderson multivariate autoregressive models classification spontaneous mental tasks 
ieee transactions biomedical engineering 

neocortical dynamics human eeg rhythms 
oxford university press 

kaufman findings groups data cluster analysis 
john wiley sons new york 

alternative modes communication man machine 
master thesis 
purdue university 
