progressive attractor selection latent attractor networks complex adaptive systems laboratory department university cincinnati cincinnati oh ali complex adaptive systems laboratory department university cincinnati cincinnati oh latent attractor networks recurrent neural networks weak embedded attractors 
attractors bias network response external inputs fully manifested 
latent attractor networks model context dependent spatial representations hippocampus encode context dependent stimuli neural networks 
current model selection biasing attractor occurred response initial triggering stimulus indicating context signal 
example sign door may set context representation room 
realistic situations context set set cues single cue cues typically seen sequentially particular order 
problem addressed latent attractor network progressively select attractor response sequence context patterns 
situations meaning identical inputs depends context speci ed past 
depending far past context information relative current time distinguish types context dependent problems 
rst may called near term context dependence refers situations output time depends inputs immediately preceding interval time 
examples problems appear dynamical systems speech processing word recognition embodied autoregressive models nite state machines learned recurrent neural networks 
second case termed episodic context dependence refers problems information context transiently particular time past 
examples context social situations recognition spatial environments task speci cations context typically speci ed episode continues force entire duration episode lasts long time 
general arti cial neural networks diculties representing inputs contextdependent way 
problems involving near term context dependence usually solved recurrent neural networks past states fed back input 
episodic context dependence di cult address way information context speci ed xed time past remote 
requires system latch information dicult recurrent networks 
concrete example consider situation set identical rooms hotel distinguished number door 
looking number sets context subsequent episode enters room 
experiments rodents suggest hippocampal region brain critical spatial tasks episodic memory construct distinct representations similar identical environments depending episodic context prime motivation 
proposed class networks called latent attractor networks paradigm dealing episodic context dependent situations line external biasing 
latent attractor networks model contextdependent spatial representations hippocampus encode context dependent mappings stimuli neural networks 
recurrent neural networks competitive ring embed patterns activity attractors associative learning 
recurrent connections strong stabilize patterns autonomously 
original formulation attractor associated speci external stimulus pattern called context pattern 
context pattern network tends disproportionately activate neurons active set associated attractor 
sets stable bias set recurrent connections result subsequent external stimuli explicitly associated particular attractor produce response patterns activity lies largely active set chosen attractor 
system produces responses conditioned original context pattern long context pattern gone 
situation lasts external stimulus associated context attractor network 
paradigm described context patterns represent stimuli set context episode looking number door 
typical situations context set single stimulus conjunction stimuli 
example entering environment may recognize set characteristic objects presence certain stimuli 
furthermore possible stimulus part combination indicating contexts combination denotes speci context 
cognitive system scans context setting stimuli unique context emerge gradually instantaneously 
unique context identi ed system response may compatible choices 
studies latent attractor networks far focused latent attractors activated gradually combination stimulus patterns just pattern instantaneously 
aim investigate issue 
important point note problem somewhat di erent standard sequence recognition stimuli particular order recognized di erentiated 
case context setting order stimuli necessarily important stimuli encountered sequentially possibly di erent order time 
question addressed latent attractor network slowly settle activity attractors context inputs sequentially recognized network 
problem encoding episodes case contexts set inputs sequentially neural system relevant situations 
example case storing information hierarchically neural network pieces information lower level de ne single entity higher level 
higher concept example title movie recalled immediately sequential presentation recall lower level concepts topic actors type slowly direct recollection higher category 
example case speech recognition words represented sequences phonemes 
system grossberg myers recognizes spoken words phonemic entities activated order turn stimulate word entities module 
word representations compete activation stimulate active phonemes components seen network module representing recognized word active 
case differs general described earlier order stimuli important 
neural networks models developed encoding recovering hierarchical information 
modular networks lower level concepts stored di erent modules higher level categories represented activation component modules 
hierarchical networks learn patterns multiple levels representing specially designed patterns common part retained high level concepts 
problem de nition system task progressively recognize set context inputs sequentially random order 
network di erent external stimulus sequences discrete time 
sequence consists context patterns followed regular patterns 
context patterns drawn context set fc regular patterns drawn set fr input space dimension set context patterns fc sequence selected randomly repetition context set network input arbitrary order 
context pattern sets di erent episodes may share patterns 
regular stimulus patterns episode chosen randomly may overlap episodes 
episode context patterns network response con ned gradually state associated particular context pattern set 
sequence network state overlaps candidate states activated current context input 
competition simultaneously stimulated network states subsequent evidence form context patterns neurons active correct attractor ring ones continuously reinforced increase activity level 
context pattern sequence network activity concentrated completely active set speci ed attractor 
method approach uses layer latent attractor network 
stimulus layer ls ns neurons project input patterns response layer lr response layer nr neurons receives recurrent connection intermediary layer lh nh neurons 
connections ls lr set randomly probability ps connection 
ks neurons input layer active time 
latent attractors stored recurrent connections lr lh layers 
attractors comprising binary patterns layer lr layer lh patterns sparse gr gh active neurons respectively lr lh sets gr gh neurons active attractor fully manifested termed active sets attractor 
connections lr lh layers chosen randomly probability connections pr lh lr ph lr lh 
attractors embedded connections setting weights clipped binary hebbian rule rst proposed willshaw connections neurons active patterns attractor set high values rest set low values 
way pairs patterns set attractors xed points layer network 
attractors called latent allowed fully active time 
activity network determined way 
excitation layer lr neuron time ls ij sum lh jz ij denote connection weights jth bit external stimulus patterns time output neuron layer lh nd ble recurrent gain neuron excitation layer lh neuron lr jz output lr excitation lr lh neurons determined ring layers competitive output kr kh excited neurons lr lh time set rest neurons output 
corresponds winner take competitive ring rule 
values kr kh respectively smaller gr gh respective sizes attractors active sets layer 
latent attractors associated stimulus sequences follows connections ls lr layers modi ed context patterns sequence stimulate neurons active set corresponding attractor lr layer 
recurrent gain parameter important controls stability attractors determining strength recurrent projection lr relative external stimulus 
latent attractor persistent neurons active set minimum value 
regular patterns stimulus sequence input system stable regime network state vary smoothly current input time stay neurons attractor activated initially context patterns 
requirement smooth dependence regular stimuli corresponding responses important representations generated network lose similarity information inherent set 
example stimuli sensory representations moving animal similarity stimuli indicate spatial proximity information lost 
shown previously achieving twin apparently incompatible objectives preserving stimulus information sustaining latent attractors requires careful critical management recurrent gain layer recurrent networks better able support layer networks 
key requirement correct context selection context patterns stimulus sequence network state settle received context patterns uniquely identify sequence 
achieved system strategy incremental competitive positive feedback 
stability attractor network ected decisively recurrent gains small relative strength external stimulus attractor dynamics dominated impact feed forward stimulus 
stimulus selectively associated particular sets neurons re total activity constrained kr neurons lr large recurrent path dominates network forced choose attractors due competitive ring lr lh system set small value episode leaving choice activity lr initial context stimuli 
attractors associated early context patterns activated bit due feed forward association 
presentation context patterns proceeds neurons belong active sets attractors current activity increased gradually priming attractors possible persistence reinforced subsequent context stimuli 
stage activity distributed attractors consistent context stimuli received far 
new context stimulus hypothesized candidate attractors reinforced expense nally left 
equation governing modulation recurrent gain kr rate parameter index attractor active set total number currently ring neurons lr active set attractor belongs active set attractors largest value chosen attractors 
modulation recurrent gain individual neurons motivated biological considerations 
extensive evidence projections neurons cortical regions segregated dendritic tree making selective modulation gain projections individual sources quite feasible 
modulation suggested basis ecient associative learning 

known hippocampal region basis model animals especially attentive episode indicated change eeg theta rhythm 
leads example greater spike synchronization lower ring latency ad phenomena 

granule cells gyrus hypothesize corresponds roughly layer lr anatomical physiological evidence intricate highly speci system modulation motivational attentional circumstances 
model represents simple rst cut attempt understand mechanisms may help support gradual context selection hippocampus arti cial neural networks motivated hippocampus 
simulation results simulation done layer latent attractor network parameters ns ks ps nr gr kr pr nh gh kh ph 
attractors embedded connections lr lh layers 
modulation rate recurrent gain 
context set distinct patterns context sets selected 
consists distinct patterns picked randomly context patterns di erent mutually exclusive 
set context patterns associated randomly chosen attractor connections ls lr layers line patterns activate neurons active set associated attractor 
patterns context set random order network input layer 
sequence recurrent gain attractors set low value 
depending context groups simultaneously stimulated incoming context patterns activity latent attractor layer distributed excited attractors 
recurrent gain attractors goes rest attractors lr activity level selected attractors lr layer respect time 
time steps di erent context sequence starts 
activity normalized kr decreased 
context sequence attractor consistently stimulated current context patterns 
modulation recurrent gain results continuous increase amount activity selected attractor active set 
shows result single network simulation context sequences input 
sequences order time steps 
graph represents normalized activity active set attractor pattern lr layer 
seen context sequence time steps activity attractors goes steadily 
attractors activity increase time steps nally shuts 
shows results simulation repeated contexts context patterns di erent order time 
seen real ect performance 
shows repeat process time regular stimulus patterns context set 
clear activity remains con ned chosen attractor regular patterns association attractor 
note incorrect attractor chosen second episode 
happen occasionally small networks limited capacity 
activity context attractors averaged di erent networks network repeats run context pattern sets di erent pattern order set time 
random presentations context patterns set 
context sequence activity level low high variance due di erence number attractors simultaneously excited 
activity progressively con ned attractors context patterns seen reaching single attractor probability close step 
proposed mechanism attractor latent attractor network activated progressively set inputs sequentially random order single cue 
system able slowly select activate right attractor individual input pattern associated attractors 
importantly response invariant order context patterns set 
process associating set inputs single episode strong implications recognition spatial environments formation hierarchical memory systems 
bengio simard frasconi learning long term dependencies gradient descent di lr repeat run regular patterns presentation context set 
cult ieee trans 
neural networks vol 
vol 
pp 

carpenter grossberg 
store working memory networks storage ad recall arbitrary temporal sequences 
biological cybernetics 
best 
generating smooth context dependent representations 
proc 
ijcnn best 
latent attractors model context selection system 
neurocomputing 
best 
latent attractors model context dependence place representations hippocampus 
neural computation 

network capacity network attractor computation 
proc 
ijcnn 
best comparison context dependent hippocampal place codes layer layer recurrent networks neurocomputing 

information capacity hierarchical neural network 
phys 
rev 

hierarchical model memory 
physica 
elman finding structure time cognitive science vol 
pp 

mean activity level context patterns respect di erent networks di erent presentation orders context patterns 
time axis corresponds size context set 
frasconi gori computational capabilities local feedback recurrent networks acting nite state machines ieee trans 
neural networks vol 
vol 
pp 

giles miller chen chen sun lee learning extracting nite state automata second order recurrent neural networks neural computation vol 
pp 

grossberg myers 
resonant dynamics speech perception interword integration duration dependent backward ects 
psychological review press 

han somogyi high degree spatial selectivity axonal dendritic domains physiologically identi ed local circuit neurons gyrus rat hippocampus 
eur 
neurosci 

hasselmo 
dynamics learning recall excitatory recurrent synapses modulation hippocampal region ca 
neurosci 
jackson positive feedback cells granule cells gyrus revealed voltage sensitive dye recording 
neurophysiol 

best 
encoding spatial context hypothesis function system 
proc 
ijcnn 
