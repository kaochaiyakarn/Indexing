tutorial support vector machines pai chen chih jen lin bernhard sch lkopf department computer science information engineering national taiwan university taipei taiwan max planck institute biological cybernetics bingen germany bernhard tuebingen mpg de 
briefly describe main ideas statistical learning theory support vector machines svms kernel feature spaces 
place particular emphasis description called svm including details algorithm implementation theoretical results practical applications 
introductory example suppose empirical data 
xm ym 
domain nonempty set patterns xi taken yi called labels targets 
stated indices understood run training set 
note assumptions domain set 
order study problem learning need additional structure 
learning want able generalize unseen data points 
case pattern recognition means new pattern want predict corresponding 
mean loosely speaking choose sense similar training examples 
need similarity measures 
easy target values identical different 
require similarity measure function examples returns real number characterizing similarity 
reasons clear function called kernel 
type similarity measure particular mathematical appeal dot products 
instance vectors canonical dot product defined parts article 

denotes ith entry geometrical interpretation dot product computes cosine angle vectors provided normalized length 
allows computation length vector distance vectors length difference vector 
able compute dot products amounts able carry geometrical constructions formulated terms angles lengths distances 
note assumption patterns live dot product space 
order able dot product similarity measure need transform dot product space need identical map 
space called feature space 
summarize benefits transform data 
lets define similarity measure dot product 

allows deal patterns geometrically lets study learning algorithm linear algebra analytic geometry 

freedom choose mapping enable design large variety learning algorithms 
instance consider situation inputs live dot product space 
case directly define similarity measure dot product 
choose apply nonlinear map change representation suitable problem learning algorithm 
position describe pattern recognition learning algorithm arguable simplest possible 
basic idea compute means classes feature space yi yi xi xi number examples positive negative labels respectively see 
assign new point class mean fig 

simple geometric classification algorithm classes points depicted compute means assign test pattern mean closer 
done looking dot product changes sign enclosed angle passes 
note corresponding decision boundary hyperplane dotted line orthogonal 
closer 
geometrical construction formulated terms dot products 
half way lies point 
compute class checking vector connecting encloses angle smaller vector connecting class means words defined offset sgn sgn sgn 

proved instructive rewrite expression terms patterns xi input domain note dot product similarity measure cf 

need rewrite terms kernel evaluated input patterns 
substitute get decision function sgn sgn yi yi xi xi yi yi xi xi 
similarly offset yi yj xi xj yi yj xi xj 
consider known special case type classifier 
assume class means distance origin viewed density positive integral dx 
order state assumption require define integral holds true corresponds called bayes decision boundary separating classes subject assumption classes generated probability distributions correctly estimated parzen windows estimators classes yi yi xi xi 
point label simply computed checking larger directly leads 
note decision best prior information probabilities classes 
details see 
classifier quite close types learning machines interested 
linear feature space input domain represented kernel expansion terms training points 
example sense kernels centered training examples arguments kernels training example 
main points sophisticated techniques discussed deviate selection examples kernels centered weights put individual data decision function 
longer case training examples appear kernel expansion weights kernels expansion longer uniform 
feature space representation statement corresponds saying study normal vectors decision hyperplanes represented linear combinations training examples 
instance want remove influence patterns far away decision boundary expect improve generalization error decision function reduce computational cost evaluating decision function cf 

hyperplane depend subset training examples called support vectors 
learning pattern recognition examples example mind consider problem pattern recognition formal setting 
class pattern recognition seek estimate function input output training data 
assume data generated independently unknown fixed probability distribution 
goal learn function correctly classify unseen examples want examples generated 
put restriction class functions choose estimate function training data satisfying xi yi 
need generalize unseen examples 
see note function test set 
satisfying 

xm exists function xi xi 
xi xi 
training data means selecting functions completely different sets test label predictions preferable 
minimizing training error empirical risk remp xi yi imply small test error called risk averaged test examples drawn underlying distribution dp 
statistical learning theory vc vapnik chervonenkis theory shows imperative restrict class functions chosen capacity suitable amount available training data 
vc theory provides bounds test error 
minimization bounds depend empirical risk capacity function class leads principle structural risk minimization 
best known capacity concept vc theory vc dimension defined largest number points separated possible ways functions class 
example vc bound vc dimension class functions learning machine implement functions class probability bound log remp holds confidence term defined log log log 
tighter bounds formulated terms concepts annealed vc entropy growth function 
usually considered harder evaluate play fundamental role conceptual part vc theory 
alternative capacity concepts formulate bounds include fat shattering dimension 
bound deserves explanatory remarks 
suppose wanted learn dependency pattern contains information label uniform 
training sample fixed size surely come learning machine achieves zero training error provided examples contradicting 
order reproduce random labelling machine necessarily require large vc dimension confidence term increasing monotonically large bound support possible hopes due small training error expect small test error 
understandable hold independent assumptions underlying distribution holds provided nontrivial prediction bound error rate void larger maximum error rate 
order get nontrivial predictions function space restricted capacity vc dimension small relation available amount data 
hyperplane classifiers section shall describe hyperplane learning algorithm performed dot product space feature space introduced previously 
described previous section design learning algorithms needs come class functions capacity computed 
considered class hyperplanes corresponding decision functions sgn proposed learning algorithm separable problems termed generalized portrait constructing empirical data 
facts 
hyperplanes separating data exists unique yielding maximum margin separation classes max min xi 

second capacity decreases increasing margin 
construct optimal hyperplane cf 
solves optimization problem minimize subject yi xi 

note 
fig 

binary classification toy problem separate balls diamonds 
optimal hyperplane orthogonal shortest line connecting convex hulls classes dotted intersects half way classes 
problem separable exists weight vector threshold yi xi 

rescaling point closest hyperplane satisfy xi obtain canonical form hyperplane satisfying yi xi 
note case margin measured perpendicularly hyperplane equals 
seen considering points opposite sides margin projecting hyperplane normal vector 
way solve lagrangian dual max min yi xi 
lagrangian minimized respect primal variables maximized respect dual variables nonlinear problem called primal problem closely related problems lagrangian dual important 
certain conditions primal dual problems optimal objective values 
solve dual may easier problem primal 
particular see section working feature spaces solving dual may way train svm 
try get intuition primal dual relation 
assume optimal solution primal optimal objective value satisfies yi xi 

yi xi 
provide rigorous proof details example 
note general convex programming result requires additional conditions constraints satisfied simple linear inequalities 
implies hand max min max min 
min max 
inequality equality 
property strong duality primal dual optimal objective value 
addition putting yi xi yi xi 
usually called complementarity condition 
simplify dual convex fixed leads iyi 
may wonder means 
definition lagrangian iyi decrease iyi want 
substituting dual problem written max xi xj iyi iyi 
definitely maximal objective value dual dual optimal solution happen iyi 
dual problem simplified finding multipliers maximize xi xj subject 
iyi 
dual svm problem usually refer 
note called karush kuhn tucker kkt optimality conditions primal problem 
abnormal situation optimal zero computed 
discussion implies consider different form dual problem maximize subject 
called wolfe dual convex optimization early duality 
convex differentiable problems equivalent lagrangian dual derivation lagrangian dual easily shows strong duality results 
notes duals example section 
discussion hyperplane decision function written sgn yi xi 
solution vector expansion terms subset training patterns patterns non zero called support vectors 
support vectors lie margin cf 

remaining examples training set irrelevant constraint play role optimization appear expansion 
nicely captures intuition problem hyperplane cf 
completely determined patterns closest solution depend examples 
structure optimization problem closely resembles typically arise lagrange formulation mechanics 
subset constraints active 
instance keep ball box typically roll corners 
constraints corresponding walls touched ball irrelevant walls just removed 
seen light surprising possible give mechanical interpretation optimal margin hyperplanes assume support vector xi exerts perpendicular force size sign yi solid plane sheet lying hyperplane solution satisfies requirements mechanical stability 
constraint states forces sheet sum zero implies torques sum zero xi yi 
theoretical arguments supporting generalization performance optimal hyperplane 
addition computationally attractive constructed solving quadratic programming problem 
optimal margin support vector classifiers tools describe support vector machines 
section formulated dot product space 
think space feature space described section 
express formulas terms input patterns living need employ expresses dot product bold face feature vectors terms kernel evaluated input patterns 
done feature vectors occurred dot products 
weight vector cf 
expansion feature space typically longer correspond image single vector input space 
obtain decision functions general form cf 
sgn sgn quadratic program cf 
maximize yi xi yi xi xi xj subject 
iyi 
working feature space somewhat forces solve dual problem primal 
dual problem number variables number training data 
primal problem may lot infinite variables depending dimensionality feature space length 
derivation dual problem section considers problems finite dimensional spaces directly extended problems hilbert spaces 
constitutes special case called representer theorem states fairly general conditions minimizers objective functions contain terms norm feature space kernel expansions 
fig 

example support vector classifier radial basis function kernel exp 
coordinate axes range 
circles disks classes training examples middle line decision surface outer lines precisely meet constraint 
note support vectors algorithm marked extra circles centers clusters examples critical classifica tion task 
grey values code modulus argument pm yi xi decision function 
kernels take closer look issue similarity measure kernel section think subset vector space endowed canonical dot product 
product features suppose patterns information contained dth order products monomials entries jd 
jd 

case prefer extract product features feature space products entries 
visual recognition problems images represented vectors amount extracting features products individual pixels 
instance collect monomial feature extractors degree nonlinear map 
approach works fine small toy examples fails realistically sized problems dimensional input patterns exist nh 

different monomials comprising feature space dimensionality nh 
instance pixel input images monomial degree yield dimensionality certain cases described exists way computing dot products high dimensional feature spaces explicitly mapping means kernels nonlinear input space subsequent processing carried dot products exclusively able deal high dimensionality 
polynomial feature spaces induced kernels order compute dot products form employ kernel representations form allow compute value dot product having carry map 
method boser extend generalized portrait hyperplane classifier nonlinear support vector machines 
aizerman called linearization space context potential function classification method express dot product elements terms elements input space 
look case polynomial features 
start giving example 
map dot products take form desired kernel simply square dot product input space 
note possible modify maps space monomials degree defining 
examples kernels considering feature maps possible look things way start kernel 
kernel function satisfying mathematical condition termed positive definiteness possible construct feature space kernel computes dot product feature space 
brought attention machine learning community 
functional analysis issue studied heading reproducing kernel hilbert space rkhs 
popular choice kernel gaussian radial basis function exp 
illustration 
overview kernels see 
soft margin support vector classifiers practice separating hyperplane may exist high noise level causes large overlap classes 
allow possibility examples violating introduces slack variables order relax constraints :10.1.1.2.6040:10.1.1.15.9362
yi xi 

classifier generalizes controlling classifier capacity sum slacks done shown provide upper bound number training errors leads convex optimization problem 
possible realization called svc soft margin classifier minimizing objective function subject constraints value constant determining trade 
boldface greek letters shorthand corresponding vectors 

incorporating kernels rewriting terms lagrange multipliers leads problem maximizing subject constraints 
iyi 
difference separable case upper bound lagrange multipliers way influence individual patterns outliers gets limited 
solution takes form 
possible realization called svc soft margin variant optimal hyperplane uses parameterization 
parameter replaced parameter lower upper bound number examples support vectors lie wrong side hyperplane respectively 
primal problem approach termed sv classifier consider minimize subject yi xi 

note constant appears formulation parameter additional variable optimized 
understand role note constraint simply states classes separated margin 
explain significance introduce term margin error denote training points 
points errors lie margin 
formally fraction margin errors emp xi 
denote argument sign decision function sgn position state result explains significance 
proposition 
suppose run svc kernel function data result 
upper bound fraction margin errors fraction training errors 
ii lower bound fraction svs 
iii suppose data 
xm ym generated iid distribution pr pr pr pr pr contains discrete component 
suppose kernel analytic non constant 
probability asymptotically equals fraction svs fraction margin errors 
get technical details dual derivation take look toy example illustrating influence 
corresponding fractions svs margin errors listed table 
derive dual sv classification algorithm 
consider lagrangian yi xi fig 

toy problem task separate circles disks solved sv classification parameter values ranging top left bottom right 
larger points allowed lie inside margin depicted dotted lines 
results shown gaussian kernel exp 
table 
fractions errors svs margins class separation toy example 
note upper bounds fraction errors lower bounds fraction svs increasing allowing errors increases margin 
fraction errors fraction svs margin multipliers 
function minimized respect primal variables maximized respect dual variables 
derivation compute corresponding partial derivatives set obtaining conditions iyi 
sv expansion non zero correspond constraint precisely met 
substituting incorporating kernels dot products leaves quadratic optimization problem sv classification maximize xi xj subject iyi 
resulting decision function shown take form sgn xi 
compared svc dual differences 
additional constraint 
second linear term longer appears objective function 
interesting consequence quadratically homogeneous 
straightforward verify decision function obtained start primal function 
computation threshold margin parameter discussed section 
connection standard sv classification somewhat surprising interpretation regularization parameter described result proposition connection svc svc 
sv classification leads sv classification set priori leads decision function 
details connection svms svms see 
considering optimal function parameters complete account follows proposition detailed connection svc svc 
cm svm defined decreasing function define lim cm min lim cm max 

max min 
max dual svm infeasible 
set feasible points empty 
min max optimal solution set dual svm svm form interval 
addition optimal objective value svm strictly positive 
min dual svm feasible zero optimal objective value 

kernel matrix positive definite min 
problem kernel interval min max admissible values min max 
illustration relation 
log fig 

relation rbf kernel problem australian statlog collection noted svms interesting interpretation terms reduced convex hulls 
show separable problems obtain optimal margin separating hyperplane forming convex hulls classes finding shortest connection convex hulls problem separable disjoint putting hyperplane halfway connection orthogonal 
problem non separable convex hulls classes longer disjoint 
longer sense search shortest line connecting 
situation natural reduce convex hulls size limiting size coefficients ci convex sets yi ci ci 
yi value 
intuitively amounts limiting influence individual points 
possible show svm formulation solves problem finding hyperplane orthogonal closest line connecting reduced convex hulls 
move aspect soft margin classification 
introduced slack variables attempt justify fact objective function 
instance yield exactly counts number margin errors 
unfortunately leads combinatorial optimization problem 
yielding optimization problems particularly convenient hand obtained 
default possesses additional property statistically attractive 
proposition shows linearity target function slack variables leads certain outlier resistance estimator 
shorthand xi xi 
proposition resistance sv classification 
suppose expressed terms svs bound coefficients dual solution 
local movements margin error xj parallel change hyperplane 
result stability classifiers 
results shown general leads fewer support vectors 
results support case seen 
proposition shows possesses intuitive meaning unclear choose learning task 
proves close upper bound expected optimal bayes risk asymptotically estimate optimal value ixi proposition 
expected risk defined rp inf kernel svm universal rp exists constant 
xm ym rp cm 
quite popular kernels gaussian universal 
definition universal kernel seen 
decision function obtained training svm data set note perturbation point carried feature space 
precisely corresponds input space depends specific kernel chosen 
upper bound rp decision function respect surely achieves risk larger rp rp 
selection kernel parameters done estimating performance support vector binary classifiers data observed 
performance estimate leave error unbiased estimate generalization performance 
compute performance metric single point excluded training set classifier trained remaining points 
determined new classifier correctly labels point excluded 
process repeated entire training set 
theoretically attractive estimate obviously entails large computational cost 
estimates leave error sv learning algorithm 
estimates general sv bound upper bound leave error restricted sv estimate approximation assumes sets margin errors support vectors margin constant maximized target estimate approximation assumes sets margin errors non support vectors decrease 
derivation general sv bound takes form similar upper bound described sv classifier restricted sv estimate similar sv estimate proposed estimates geometrical concept span roughly speaking measure easily particular point training sample replaced points define classification function 
analogous method exists sv case maximized target estimate 
implementation sv classifiers change dual form sv classifiers minimization problem minimize rm xi xj subject iyi 
proves optimal solution satisfies 
sufficient solve simpler problem equality constraint 
similar svc difficulty solving xi xj general zero 
large data sets hessian second derivative matrix objective function stored computer memory traditional optimization methods newton quasi newton directly 
currently decomposition method approach conquer difficulty 
implementation modifies procedure svc 
decomposition method decomposition method iterative process 
step index set variables partitioned sets working set 
iteration variables corresponding fixed sub problem variables corresponding minimized 
procedure follows algorithm decomposition method 
number size working set 
find initial feasible solution 
set 
optimal solution 
find working set 
size define 
subvectors corresponding respectively 

solve sub problem variable minimize xi xj xi xj subject iyi yi 

set optimal solution goto step 
set note updated iteration 
simplify notation simply working set selection important issue decomposition method selection working set consider approach violation kkt condition 
similar putting kkt conditions yi jk xi xj 

rewritten xi xj xi xj 
optimal dual problem feasible satisfies 
property yi representing xi xj written max min low max min low yi low yi yi low yi 
call low low yi yj satisfying violating pair satisfied 
optimal violating pair included optimal objective value small decomposition procedure objective value strictly decreasing iteration 
natural choice select pairs violate 
precise set integer sequentially select pairs 
iq jq low yi yj yi yj 
working set selection merely extension svc 
main difference svm inequality due similarity believe convergence analysis svc adapted detailed proofs written published 
considers working set selection 
derivation svc obtained concept feasible directions constrained optimization 
feel derivation violation kkt condition intuitive 
smo type implementation sequential minimal optimization smo algorithm extreme decomposition method svc working set restricted elements 
main advantage variable sub problem analytically solved numerical optimization software needed 
method elements required working set 
equality constraint low iyi yj leads fixed optimal objective value sub problem 
decomposition procedure stays point 
dual svc possesses inequalities may think elements needed working set 
elements case svc 
note rewritten yi iyi yi yi yi iyi yi yi 
selected working set selection yi yj reduces equality variables 
sub problem guaranteed smaller comparison shows connection proposition equivalent stopping condition performance smo type implementation described svm svm comparable 
calculation stopping criteria optimal solution yi low 
similarly yj solving equalities gives 
practice average avoid numerical errors yi yi components number interval formed 
common way select middle point solves linear equations stopping condition decomposition method easily follow new form optimality condition max min low max min low max chosen stopping tolerance 
multi class sv classifiers svm originally designed class problems approaches developed extend svm multi class data sets 
section discuss extension approach multi class svm 
approaches multi class svm decompose data set binary problems 
example approach trains binary svm classes data obtains decision function 
class problem decision functions 
prediction stage voting strategy testing point designated class maximum number votes 
experimentally shown general problems sv classifier various multi class approaches give similar accuracy :10.1.1.129.6281
method efficient training 
focus extending svm 
multi class methods considered parameter selection strategies 
search appropriate kernel parameters constructing better model 
restrict discussion gaussian radius xi xj basis function kernel xi xj kernel parameter 
parameter selection considered ways implement method classes data parameter selection conducted best 
best model selected decision function 
experiments parameter selection binary svm fold cross validation 
second way evaluation criterion cross validation combining method estimating performance model 
sequence pre selected tried select best model 
model decision functions share 
clear implementations better 
hand single parameter set may uniformly decision functions 
hand accuracy final consideration parameter set decision function may lead fitting 
compare approaches svm preliminary results show give similar accuracy 
svm binary svm data ith jth classes admissible interval ij min ij max ij max min mi mj mi mj proposition 
mi mj number data points ith jth classes respectively 
decision functions share admissible interval max ij min min ij max 
set non empty kernel matrix positive definite 
reason proposition implies ij min mini ij max 
svm large valid range svm worry admissible interval may small 
example data set highly unbalanced mini ij min small 
redo comparison svm 
results table 
consider multi class problems tested statlog collection 
data sets dna shuttle letter satimage usps test sets available separate problem training testing 
cross validation conducted training data 
settings data scaling 
experiments conducted libsvm solves svm svm 
results table show significant difference implementations 
note problems shuttle highly unbalanced admissible interval small 
surprisingly intervals find suitable leads model 
preliminary experiment indicates general approach multi class svm viable 
table 
test accuracy percentage multi class data sets svm svm 
columns common different common different testing accuracy different decision functions 
validation conducted points 


svm range validate point discretization interval ij min ij max depending decision functions share parameters 
small problems number training data cross validation times average testing accuracy 
data set class 
training testing common different common different vehicle glass iris dna segment shuttle letter vowel satimage wine usps contours svm svm approach decision functions share 
contour svm xaxis axis log log respectively 
svm axis interval 
clearly region svm smaller 
confirms concern earlier motivated conduct experiments section 
fortunately points smaller region lead models competitive svm 
ways enlarge admissible interval 
extend algorithm case small values allowing negative margins 
upper bound proposition classes balanced upper bound 
leads idea modify algorithm adjusting cost function classes balanced terms cost terms mere numbers training examples 
earlier discussion formulations 
example consider formulation minimize subject yi xi 
yi yi satimage log log gamma satimage nu log gamma fig 

fold cross validation accuracy data set satimage 
left svm right svm dual maximize xi xj subject yi yi iyi 
clearly equals corresponding upper bound feasible solution possibility minimize min subject yi xi 
dual maximize xi xj subject min iyi 
largest admissible 
slight modification implementation section formulations 
applications sv classifiers researchers applied svm different applications 
feel easier intuitive deal 
briefly summarize libsvm solve svm 
researchers hp labs discuss topics personal email agent 
data classification important component authors svm think parameter intuitive parameter applies machine learning methods detect localize boundaries natural images 
classifiers tested svm authors considered svm 
appealing features kernel algorithms solid foundation provided statistical learning theory functional analysis 
kernel methods interpret design learning algorithms geometrically feature spaces nonlinearly related input space combine statistics geometry promising way 
kernels provide elegant framework studying fundamental issues machine learning similarity measures kernel viewed nonlinear similarity measure ideally incorporate prior knowledge problem hand data representation described kernels induce representations data linear space function class due representer theorem kernel implicitly determines function class learning 
support vector machines major kernel methods data classification 
original form requires parameter controls trade classifier capacity training errors 
parameterization parameter replaced parameter 
tutorial derivation possible advantages support vector classifier 
acknowledgments authors ingo arthur helpful comments 

aizerman 
braverman theoretical foundations potential function method pattern recognition learning 
automation remote control 

alon ben david cesa bianchi haussler 
scale sensitive dimensions uniform convergence learnability 
journal acm 


nonlinear programming 
prentice hall new jersey 

bartlett shawe taylor 
generalization performance support vector machines pattern classifiers 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 

sherali shetty 
nonlinear programming theory algorithms 
wiley second edition 

bennett bredensteiner 
duality geometry svm classifiers 
langley editor proceedings th international conference machine learning pages san francisco california 
morgan kaufmann 

bergman griss staelin 
personal email assistant 
technical report hpl hp laboratories palo alto ca 

boser guyon vapnik 
training algorithm optimal margin classifiers 
haussler editor proceedings th annual acm workshop computational learning theory pages pittsburgh pa july 
acm press 

burges sch lkopf 
improving accuracy speed support vector learning machines 
mozer jordan petsche editors advances neural information processing systems pages cambridge ma 
mit press 


chang 
lin 
libsvm library support vector machines 
software available athttp www csie ntu edu tw cjlin libsvm 


chang 
lin 
training support vector classifiers theory algorithms 
neural computation 


chew 
lim 
dual support vector machine error rate training size biasing 
proceedings icassp pages 

chew lim 
implementation training dual nu support vector machines 
qi teo yang editors optimization control applications 
kluwer 


chung 
kao 
sun 
lin 
decomposition methods linear support vector machines 
technical report department computer science information engineering national taiwan university 

cortes vapnik 
support vector networks 
machine learning 

crisp burges 
geometric interpretation svm classifiers 
solla leen 
ller editors advances neural information processing systems 
mit press 

herbrich chapelle sch lkopf rayner 
estimating leave error classification learning svms 
technical report cued infeng tr cambridge university engineering department 


hsu 
lin 
comparison methods multi class support vector machines 
ieee transactions neural networks 

joachims 
making large scale svm learning practical 
sch lkopf burges smola editors advances kernel methods support vector learning pages cambridge ma 
mit press 


lin 
formulations support vector machines note optimization point view 
neural computation 


lin 
convergence decomposition method support vector machines 
ieee transactions neural networks 

estimation characters obtained statistical procedure recognition 


martin fowlkes malik 
learning detect natural image boundaries brightness texture 
advances neural information processing systems volume 

mercer 
functions positive negative type connection theory integral equations 
philosophical transactions royal society london 

michie spiegelhalter taylor 
machine learning neural statistical classification 
prentice hall englewood cliffs 
data available www ncc pt ml statlog datasets html 

opper winther 
gaussian processes svm mean field leave estimator 
smola bartlett sch lkopf schuurmans editors advances large margin classifiers cambridge ma 
mit press 

perez cruz weston herrmann sch lkopf 
extension svm range classification 
suykens basu micchelli vandewalle editors advances learning theory methods models applications pages amsterdam 
ios press 

platt 
fast training support vector machines sequential minimal optimization 
sch lkopf burges smola editors advances kernel methods support vector learning cambridge ma 
mit press 

sch lkopf 
support vector learning 
oldenbourg verlag nchen 
technische universit berlin 
available www tuebingen mpg de bs 

sch lkopf burges smola 
advances kernel methods support vector learning 
mit press cambridge ma 

sch lkopf smola 
learning kernels 
mit press cambridge ma 

sch lkopf smola williamson bartlett 
new support vector algorithms 
neural computation 

smola bartlett sch lkopf schuurmans 
advances large margin classifiers 
mit press cambridge ma 


support vector machines universally consistent 
journal complexity 


optimal parameter choice support vector machines 
ieee transactions pattern analysis machine intelligence 
appear 


sparseness support vector machines 
technical report 

vapnik 
estimation dependences empirical data russian 
nauka moscow 
english translation springer verlag new york 

vapnik 
nature statistical learning theory 
springer verlag new york 

vapnik 
statistical learning theory 
wiley ny 

vapnik chapelle 
bounds error expectation support vector machines 
neural computation 

vapnik chervonenkis 
theory pattern recognition russian 
nauka moscow 
german translation theorie der akademie verlag berlin 

vapnik lerner 
pattern recognition generalized portrait method 
automation remote control 

wahba 
spline models observational data volume cbms nsf regional conference series applied mathematics 
society industrial applied mathematics philadelphia pennsylvania 

williamson smola sch lkopf 
generalization performance regularization networks support vector machines entropy numbers compact operators 
ieee transactions information theory 

wolfe 
duality theorem non linear programming 
applied mathematics 
