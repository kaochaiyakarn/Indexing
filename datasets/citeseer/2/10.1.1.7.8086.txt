learning distance functions equivalence relations bar hillel cs huji ac il hertz cs huji ac il noam cs huji ac il weinshall cs huji ac il school computer science engineering center neural computation hebrew university jerusalem jerusalem israel address problem learning distance metrics side information form groups similar points 
propose rca algorithm simple cient algorithm learning full ranked mahalanobis metric 
show rca obtains solution interesting optimization problem founded information theoretic basis 
mahalanobis matrix allowed singular show fisher linear discriminant followed rca optimal dimensionality reduction algorithm criterion 
show optimization problem related criterion optimized algorithm metric learning xing uses kind side information 
empirically demonstrate learning distance metric rca algorithm significantly improves clustering performance similarly alternative algorithm 
rca algorithm cient cost effective alternative uses closed form expressions data preferable choice learning full rank mahalanobis distances 
keywords learning partial knowledge semisupervised learning feature selection clustering 
learning algorithms distance function input space principal tool performance critically depends quality metric 
learning metric examples may key successful application algorithms 
cases choosing right metric may important specific algorithm 
choosing right metric especially important unsupervised setting clustering tasks clustering algorithms means graph methods 
supervised classification techniques distance 
kernel machines inner product functions closely related euclidean distance metric 
wide variety algorithms problem finding metric equivalent problem finding representation function transferring data representation discuss problems interchangeably 
main goal design simple method learning metric order improve subsequent performance unsupervised learning techniques 
accomplished side information form equivalence relations 
equivalence relations provide small groups data points known similar dissimilar key observation unsupervised learning tasks groups similar points may extracted data minimal ort possibly automatically need labels 
occurs data originates natural sequence modeled markovian process 
consider example task movie segmentation objective find frames actor appears 
due continuous nature movies faces extracted successive frames roughly location assumed come person 
true long scene change automatically robustly detected rowe 
analogous example speaker segmentation recognition conversation proceedings twentieth international conference machine learning icml washington dc 
speakers needs segmented clustered speaker identity 
may possible automatically identify small segments speech contain data points single unknown speaker 
discuss problem learning linear representation functions equivalently optimal mahalanobis distance data points equivalence relations 
specifically focus relevant component analysis rca algorithm introduced algorithm reviewed section 
section new analysis novel information theoretic optimality criterion 
rca shown optimal learning procedure sense 
show fisher linear discriminant function followed rca optimizes criterion dimensionality reduction allowed 
section show rca optimal solution problem minimizing inner class distances 
viewed way rca directly compared approach proposed xing algorithm metric learning side information 
comparison shows optimality criteria algorithms similar arbitrary aspects criterion xing exist rca 
empirical study shows results algorithms comparable empirically tested rca algorithm number databases uci repository showing significant improvement clustering performance similar better improvement reported xing 
major di erence algorithms computational rca robust cient uses closed form expressions data algorithm described xing hand uses iterative methods sensitive parameter tuning demanding computationally 
related learning representations distance functions supervised learning setting just briefly mention examples 
hastie tibshirani jaakkola haussler labeled data learn metrics classification 
thrun distance function representation function learned classification leaning learn paradigm 
setting related classification tasks learned labeled data sets algorithms proposed learn representations distance functions way allows transfer knowledge tasks 
tishby joint distribution random variables assumed known problem reduced learning compact representation bears high relevance developed chechik tishby viewed supervised representation learning 
information theoretic criteria unsupervised learning neural networks suggested linsker tasks neural network literature bell sejnowski 
years done equivalence relations side information 
equivalence relations introduced means clustering algorithm 
positive similar negative dissimilar relations 
problem finding better mahalanobis metric equivalence relations addressed xing conjunction constrained means algorithm 
compare algorithm current section compare empirical results results algorithms section 
developed way introduce positive negative equivalence relations em algorithm estimation mixture gaussian models hertz 

relevant component analysis relevant component analysis rca method seeks identify scale global unwanted variability data 
method changes feature space data representation global linear transformation assigns large weights relevant dimensions low weights irrelevant dimensions cf 
tenenbaum freeman 
relevant dimensions estimated 
define subset points known belong unknown class obtained equivalence relations applying transitive closure 
rca transformation intended reduce clutter new feature space inherent structure data easily 
method preprocessing step unsupervised clustering data nearest neighbor classification 
specifically rca see illustration fig 

subtract mean 
illustrative example rca algorithm applied synthetic gaussian data 
fully labeled data set classes 
data unlabeled clearly classes structure evident 
set provided rca algorithm points share color marker type form 
centered empirical covariance 
whitening transformation applied 
original data applying rca transformation 
points contains fig 


compute covariance matrix centered data points fig 

assume total points consists points ji mean rca computes matrix ji ji 
compute whitening transformation associated covariance matrix fig 
apply original data points xnew wx fig 

alternatively inverse mahalanobis distance 
ect whitening transformation assigns lower weight directions original feature space directions data variability mainly due class variability irrelevant task classification 

information maximization constraints section suggest information theoretic formulation problem hand 
problem formulated constrained search representation function possible state problem general families transformations treat linear case 
section discuss problem formulation 
show rca solves problem linear invertible transformations considered 
section extend family functions considered include non invertible linear transformations leads dimensionality reduction 
show data gaussian solution fisher linear discriminant followed rca 

information theoretic perspective linsker information theoretic criterion states input transformed new representation seek maximize mutual information suitable constraints 
general deterministic case set data points transformed set points wish find function maximizes family allowed transformation functions hypotheses family 
case set data points ji repre sentation function required keep close 
may pose problem max ji denotes mean points transformation total number points constant 
mutual information di erential mutual information continuous variables depends respective densities 
note asses densities provided sample data points 
case deterministic maximization achieved maximizing entropy 
see recall deterministic uncertainty concerning known 
lowest possible value 
noted bell sejnowski depend quantization scale 
finite quantization space term constant 
maximizing respect done considering term 
noted increased simply stretching data space choosing 
constraint keeps certain points close required order prevent trivial scaling solution 
family representation functions carefully chosen avoid trivial solutions 

rca information theoretic perspective look problem posed family invertible linear functions 
invertible function connection densities expressed px jacobian transformation 
noting dy dx relate follows log dy non intuitive divergence result generalization information theory continuous variables specifically result ignoring discretization constant definition di erential entropy 
log dx log linear function ax jacobian constant equals term depends transformation problem max ji denote mahalanobis distance matrix positive definite log log 
rewritten max ji writing solving lagrangian get solution average covariance matrix dimension data space 
solution identical mahalanobis matrix proposed rca scale factor 
rca solution 

dimensionality reduction section analyze problem posed section case general linear transformations ax simplify analysis assume multivariate gaussian 
saw earlier maximizing equivalent maximizing respect assumed gaussian gaussian entropy log log log log max log ji target dimension solution problem fisher linear discriminant followed applying rca reduced dimensional space 
sketch proof appendix scale constant important classification tasks relative distances 

rca minimizes inner class distances order gain intuition solution provided information maximization criterion formalized eq 
look optimization problem obtained reversing roles maximization term constraint term min ji mahalanobis distance sought minimizes sum inner squared distances 
demanding amounts demand minimizing distances achieved shrinking entire space 
kuhn tucker theorem reduce min ji log log di erentiating lagrangian shows minimum average covariance matrix 
solution identical mahalanobis matrix proposed rca scale factor 
interesting respect compare rca method proposed xing 
consider problem learning mahalanobis distance side information form pairwise similarities 
assume knowledge set pairs points known similar set pairs points known dissimilar 
sets pose optimization problem 
min 
problem solved gradient ascent iterative projection methods 
allow clearer comparison rca eq 
cast minimization inner pairwise distances 
point ji ji ji jk ji jk size considered 
problem rewritten min ji jk size case studied xing problem reduces min clearly minimization terms problems identical constant 
di erence problems lies constraint term 
constraint proposed xing tries information concerning pairs dissimilar points constraint rca formulation interpreted pure scale constraint allow volume mahalanobis neighborhood shrink 
constraint xing appears take consideration information closer look shows somewhat arbitrary 
usage squared distance minimization term root square distance constraint term arbitrary symmetric 
importantly noted unsupervised applications dissimilar pairs explicitly available 
case xing recommends take pairs points problematic choice reasons practical scenarios pairs points necessarily dissimilar 
addition definition usually yields large set substantially slows algorithm running time 
contrast rca distance computation simple fast requiring single matrix inversion need iterative procedure 
order justify constraint suggested problem proceed suggest probabilistic interpretation rca algorithm 

rca maximum likelihood analyze case data consists normally distributed classes share covariance matrix 
assumption sampled points sampled likelihood distribution written exp ji ji easy see rca mahalanobis matrix maximizes possible choices maximum likelihood estimator setting 
order gain insight constraint chosen take log likelihood equation drop constant terms denote obtain arg min ji log denotes total number points 
equation closely related lagrangian lagrange multiplier replaced constant gaussian assumptions solution problem probabilistic justification 
ect size gaussian assumptions define unbiased version rca estimator 
assume simplicity constrained data points divided size 
unbiased rca estimator written follows denotes data point denotes empirical mean empirical mean covariance estimators produced 
shown variance estimator matrix elements ij bounded ar ij ar ij nk ij nk estimator nk points known belong class forming best estimate possible points 
proof see hertz 
bound shows variance rca estimator small rapidly converges variance best estimator 

experimental results application clustering noted main goal method side information form equivalence relations improve performance unsupervised learning techniques 
order test proposed rca algorithm compare xing data sets uc irvine repository xing 
xing set pairwise similarity constraints size 
clustering algorithms 
means default euclidean metric side information 

constrained means means subject points assigned cluster 

constrained means metric proposed xing constrained means distance metric proposed xing learned 
constrained means rca constrained kmeans rca distance metric learned 
em expectation maximization gaussian mixture model side information 

constrained em em side information form equivalence constraints hertz rca distance metric initial metric 
xing normalized accuracy score evaluate partitions obtained di erent clustering algorithms 
formally case cluster data accuracy measure written indicator function rue alse cluster point assigned clustering algorithm correct desired assignment 
score equivalent computing probability algorithm assignment randomly drawn points agrees true assignment allow fair comparison xing repeated exact experimental setup criteria 
noted xing score needs normalization number clusters larger 
normalization achieved sampling pairs cluster determined probability di erent clusters probability matches mismatches weight 
wine normalized rand score balance normalized rand score ionosphere normalized rand score soybean normalized rand score boston normalized rand score iris normalized rand score 
clustering accuracy uci datasets 
panel bars left correspond experiment little side information bars right correspond side information 
left right bars respectively means original feature space side information 
constrained means original feature space 
constrained means feature space suggested xing 
constrained means feature space created rca 
em original feature space side information 
constrained em feature space created rca 
shown number points number classes dimension feature space kc mean number connected components see footnote 
results averaged realizations side information 
xing tested method conditions little side information side information 
xing experiments means multiple restarts 
fig 
shows results algorithms described conditions little side information 
clearly rca distance measure significantly improves results original means algorithm 
comparing results results reported xing see rca achieves similar results 
respect noted rca metric computation single step cient computation method xing requires gradient descent iterative projections 
generated choosing random subset pairs points sharing class case little side information size subset chosen resulting number connected components kc transitive closure pairs roughly size original dataset 
case side information changed 

discussion concluding remarks algorithm side information form equivalence relations learn mahalanobis metric 
shown method optimal criteria showed considerable improvement clustering standard datasets 
rca techniques developed equivalence relations enhance unsupervised learning 
related technique introduced constraints em formulation gaussian mixture model hertz 
enhances power rca ways possible incorporate negative constraints 
second allows improvement rca metric may seen fig 

bell sejnowski 

approach blind separation blind deconvolution 
neural computation 
rowe 

comparison video shot boundary detection techniques 
spie storage retrieval images video databases iv 
chechik tishby 

extracting relevant structures side information 
nips 
fukunaga 

statistical pattern recognition 
san diego academic press 
nd edition 
hastie tibshirani 

discriminant adaptive nearest neighbor classification regression 
advances neural information processing systems pp 

mit press 
hertz bar hillel weinshall 

enhancing image video retrieval learning equivalence constraints 
leibniz cs huji ac il 
jaakkola haussler 

exploiting generative models discriminative classifiers 
linsker 

application principle maximum information preservation linear systems 
nips pp 

morgan kaufmann 
hertz bar weinshall 

computing gaussian mixture models em equivalence constraints 
hertz weinshall pavel 

adjustment learning relevant component analysis 
computer vision eccv 
tenenbaum freeman 

separating style content bilinear models 
neural computation 
thrun 

learning th thing easier learning 
advances neural information processing systems pp 

mit press 
tishby pereira bialek 

information bottleneck method 
proc 
th annual allerton conference communication control computing pp 

cardie rogers 

constrained means clustering background knowledge 
proc 
th international conf 
machine learning pp 

morgan kaufmann san francisco ca 
xing ng jordan russell 

distance metric application clustering side information 
advances neural information processing systems 
mit press 
appendix information maximization case non invertible linear transformation briefly sketch proof claim section 
denote average covariance matrix 
rewrite constrained expression ji ji tr ac tr ca lagrangian may written log tr aca di erentiating lagrangian leads ca multiplying rearranging get ca 
equation give information concerning subspace optimal takes 
data respect covariance subspace similarly rca 
follows inequality constraint equality find 
tr aca tr aca solution space aca log aca log holds points 
modify maximization argument follows log log aca log optimization argument familiar form 
known fukunaga maximizing determinant ratio done projecting space span eigenvectors denote solution matrix unconstrained problem 
order enforce constraints define matrix claim solution constrained problem 
notice value maximization argument change switch product full ranked matrix 
shown satisfies constraints solution problem eq 

