appear adv 
neural information processing systems kearns solla cohn eds 
mit press 
learning estimate scenes images william freeman egon pasztor merl mitsubishi electric research laboratory broadway cambridge ma freeman merl com pasztor merl com seek scene interpretation best explains image data 
example infer projected velocities scene best explain consecutive image frames image 
synthetic data model relationship image scene patches scene patch neighboring scene patches 
new image propagate likelihoods markov network ignoring ect loops infer underlying scene 
yields cient method form low level scene interpretations 
demonstrate technique motion analysis estimating high resolution images low resolution ones 
interest studying statistical properties visual world 
olshausen field bell sejnowski derived receptive elds ensembles images simoncelli schwartz account contrast normalization ects redundancy reduction 
li atick explain retinal color coding information processing arguments 
various research groups developed realistic texture synthesis methods studying response statistics multi scale oriented receptive elds 
methods help understand early stages image representation processing brain 
unfortunately don address visual system interpret images estimate underlying scene 
study statistical properties labelled visual world images scenes order infer scenes images 
image data single multiple frames scene quantities estimated projected object velocities surface shapes re patterns colors 
ask visual system correctly interpret visual scene models probability local scene patch generated local image probability local scene neighbor 
rst probabilities allow making scene estimates local image data second allow local estimates propagate 
leads bayesian method low level vision problems constrained markov assumptions 
describe method show working low level vision problems 
markov networks scene estimation synthetically generate images underlying scene representations computer graphics 
synthetic world typify visual world algorithm operate 
example motion estimation problem sect 
training images irregularly shaped blobs occlude moving randomized directions speeds pixels frame 
contrast values blobs background randomized 
image data concatenated image intensities successive frames image sequence 
scene data velocities visible objects pixel frames 
second place image scene data markov network 
break images scenes localized patches image patches connect underlying scene patches scene patches connect neighboring scene patches 
neighbor relationship regard position scale orientation motion problem represented images velocities level gaussian pyramids ciently communicate space 
scene patch additionally connects patches neighboring resolution levels 
shows multiresolution representation time frame images scenes 
third propagate probabilities 
weiss showed advantage belief propagation regularization methods problems apply related methods problems 
ith jth image scene patches yi xj respectively 
map estimate scene data want nd argmax xn xn jy ym number scene image patches 
joint probability simpler compute nd equivalently argmax xn xn ym 
conditional independence assumptions markov network factorize desired joint probability quantities involving local measurements calculations 
consider patch system fig 

factorize maintain desired conditional independence relationships appended image data scenes 
provided scene elements image contrast information lack 
related arguments follow mmse estimators 
steps jx elementary probability jx jx jx conditional independence jx jx jx elementary probability markov assumption 
estimate just node argmax maxx slides constants giving terms involving local computations node argmax maxx argmax jx maxx jx jx factorization generalizes network structure loops 
di erent factorization scene node turn initial joint probability conditional factoring node prior xj proceeding analogously example 
resulting factorized computations give local propagation rules similar node receives message neighbor accumulated likelihood function yk yz jxj yk yz image nodes lie scene node relative scene node iteration image nodes enter likelihood function 
iteration map estimate node argmax xj xj yj jxj runs scene node neighbors node calculate xk jxj yk jxk previous iteration 
initial 
markov network nodes example 
factorization rules described verify local computations compute argmax xn xn jy ym desired 
learn network parameters measure xj yj jxj xk jxj directly synthetic training data 
network contains loops factorization hold 
learning inference require computationally intensive methods 
alternatively multi resolution quad tree networks factorization rules apply propagate information spatially 
gives results artifacts quad tree boundaries statistical boundaries model real problem 
results including loop causing connections adjacent nodes tree level applying factorized propagation rules anyway 
obtained results approach inference weiss provides theoretical arguments works certain cases 
discrete probability representation motion example applied training method propagation rules motion estimation vector code representation images scenes 
wrote treestructured vector quantizer code pixel frame blocks image data pyramid level codes level 
coded scene patches codes 
training approximately examples irregularly shaped moving blobs overlapping contrast background randomized values 
histograms measured statistical relationships embody algorithm yjx xn jx scene xn neighboring scene shows input test image vector quantization 
true underlying scene desired output shown vector quantization 
shows iterations algorithm eq 
converges estimate underlying scene velocities 
local probabilities learned yjx xn jx lead gure ground segmentation aperture problem constraint propagation lling see 
frames image data gaussian pyramid vector quantized 
optical ow scene information vector quantized 
large arrow added show small vectors orientation 
density representation super resolution example super resolution input image high frequency components sharpest details sub sampled image 
scene estimated high frequency components full resolution image fig 

improved method second problem 
faithful image representation requires vector codes infeasible measure prior statistics note fig 

hand discrete representation allows fast propagation 
developed hybrid method allows tting fast propagation 
describe image scene patches vectors continuous space rst modelled probability densities xn gaussian mixtures 
reduced dimensionality principal components analysis 
evaluated prior conditional distributions eq 
discrete set scene values di erent node 
sample approach relates 
scenes sampling scenes render image node 
focusses computation locally feasible scene interpretations 
xk jxj eq 
ratios gaussian mixtures xk xj xj evaluated scene samples nodes respectively 
yk jxk isp yk xk xk evaluated scene samples node select scene samples condition mixture node sample resulting mixture gaussians 
obtained somewhat better results scenes training set probable scene code fig 
rst iterations bayesian belief propagation 
note initial motion estimates occur edges 
due aperture problem initial estimates agree 
filling motion estimate occurs 
cues gure ground determination may include edge curvature information lower resolution levels 
included implicitly learned probabilities 
ground undetermined region low edge curvature 
velocities lled agree 
velocities lled agree correct velocity direction shown fig 

images closely matched image observed node avoiding gaussian mixture modeling step 
scene samples node setting xk jxj matrix link took minutes pixel images 
scene high resolution patch size image low resolution patch size 
didn feel long range scene propagation critical pyramid node structure 
matrices computed iterations eq 
completed seconds 
shows results 
training images random shaded painted blobs test image shown 
iterations synthesized maximum likelihood estimate high resolution image visually close actual high frequency image top row 
including gave results suspect due errors modeling highly peaked distribution 
dominant structures approximately correct position 
may enable high quality zooming low resolution images attempted limited success 
discussion related applications markov random elds vision researchers typically relatively simple heuristically derived expressions learned likelihood function yjx spatial relationships prior term scenes superresolution example 
top row input desired output contrast normalized orientations vertical 
bottom row algorithm output comparison image estimated high vertical frequencies 

researchers applied related learning approaches low level vision problems restricted linear models 
learning constraint propagation approaches motion analysis see 
summary wehave developed principled practical learning method low level vision problems 
markov assumptions lead factorizing posterior probability 
parameters markov random eld probabilities speci ed training data 
examples programmed matlab respectively training take hours running takes minutes 
scene estimation markov networks may useful low level vision problems extracting intrinsic images line drawings photographs 
adelson tenenbaum viola weiss helpful discussions 
atick li redlich 
understanding retinal color coding rst principles 
neural computation 
bell 
independent components natural scenes edge lters 
vision research 
berger 
statistical decision theory bayesian analysis 
springer 
bishop 
neural networks pattern recognition 
oxford 
black anandan 
framework robust estimation optical ow 
proc 
th intl 
conf 
computer vision pages 
ieee 
burt adelson 
laplacian pyramid compact image code 
ieee trans 
comm 
viola 
texture recognition non parametric multi scale statistical model 
proc 
ieee computer vision pattern recognition 
frey 
bayesian networks pattern classi cation 
mit press 
geiger girosi 
parallel deterministic algorithms mrf surface reconstruction 
ieee pattern analysis machine intelligence may 
geman geman 
stochastic relaxation gibbs distribution bayesian restoration images 
ieee pattern analysis machine intelligence 
gray 
incorporating visual factors vector quantizers image compression 
watson editor digital images human vision 
mit press 
heeger bergen 
pyramid texture analysis synthesis 
acm siggraph pages 
computer graphics proceedings annual conference series 
poggio 
synthesizing color algorithm examples 
science 
isard blake 
contour tracking stochastic propagation conditional density 
proc 
european conf 
computer vision pages 
jordan editor 
learning graphical models 
mit press 
ju black jepson 
skin bones multi layer locally ne optical ow regularization transparency 
proc 
ieee computer vision pattern recognition pages 
kersten 
cooperative computation scene attributes 
landy movshon editors computational models visual processing chapter 
mit press cambridge ma 
kersten toole knill anderson 
associative learning scene parameters images 
applied optics 
knill richards editors 
perception bayesian inference 
cambridge univ press 
karl willsky 
cient multiscale regularization applications computation optical ow 
ieee trans 
image processing 
mackay neal 
error correcting codes sparse matrices 
cryptography coding lncs 
nowlan 
selection model motion processing area mt primates 
neuroscience 
olshausen field 
emergence simple cell receptive eld properties learning sparse code natural images 
nature 
pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
pentland horowitz 
practical approach fractal image compression 
watson editor digital images human vision 
mit press 
poggio torre koch 
computational vision regularization theory 
nature 
saund 
perceptual organization occluding contours opaque surfaces 
cvpr workshop perceptual organization santa barbara ca 
schultz stevenson 
bayesian approach image expansion improved de nition 
ieee trans 
image processing 
simoncelli 
statistical models images compression restoration synthesis 
st asilomar conf 
sig sys 
computers paci grove ca 
simoncelli schwartz 
modeling surround suppression neurons statistically derived normalization model 
adv 
neural information processing systems volume 
weiss 
interpreting images propagating bayesian beliefs 
adv 
neural information processing systems volume pages 
weiss 
belief propagation revision networks loops 
technical report ai lab memo mit cambridge ma 
zhu mumford 
prior learning gibbs reaction di usion 
ieee pattern analysis machine intelligence 
