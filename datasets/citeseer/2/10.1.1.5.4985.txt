feature selection classifier ensembles study hyperspectral remote sensing data en ensembles een op tot het van de van doctor de aan de universitaire antwerpen te door yu prof dr paul prof dr dirk van dyck 
antwerpen feature selection classifier ensembles study hyperspectral remote sensing data yu dissertation submitted partial fulfillment requirements degree doctor philosophy physics university antwerp antwerp yu rights reserved 
parents cao yu ii says completing requirements doctoral degree just running 
time final goal long race acknowledge people way influenced supported course 
dissertation advisor prof dr paul enthusiastic supervision continuous encouragement great patience effort polishing english writings 
go giving complete time freedom develop define research direction 
proven somewhat tough appeared just lost way wandering go research come appreciate wisdom way trains student research 
benefited guidance 
addition wish extend appreciation prof dr dirk van dyck director dissertation advisor providing large quiet office enjoyed time stay 
am indebted members doctoral committee carefully reading manuscript dissertation 
am grateful physics department secretary kind help 
due members 
particular werner translating summary dissertation dutch 
iii finish dedicating dissertation important people life mother father 
eclipse true love unconditional dedication consistent support shown lows years 
achieved dreams 

sisters invaluable support continual encouragement thousands miles away especially care parents years duty 
antwerp jan yu iv summary feature selection classifier ensembles study hyperspectral remote sensing data yu pattern recognition research area studies operation design systems recognize patterns data 
dissertation important aspects pattern recognition classifier ensemble problem feature selection problem studied hyperspectral remote sensing data 
rapid advances sensor technology possible collect hyperspectral remote sensing data spans typically spectral bands 
high dimensionality hand provides potential discrimination power classification tasks 
hand classification performance improves limited point additional features added deteriorates due limited number training samples 
shows importance feature reduction critical pre processing step 
feature reduction includes feature selection feature extraction 
part due difficulty interpreting transformed features feature extraction feature selection emphasized 
problem classification high dimensionality tion classes difficult due fact number training samples catch increase dimensionality 
fore important topic data analysis hyperspectral remote sensing improvement classification performance 
dissertation achieved studying idea fusing existing classification schemes improve clas performance 
attractive advantages nearest neighbor classifier corresponding ensembles focused dissertation 
roadmap dissertation 
study classifier ble methods 
initiate idea learning means evaluate merit feature subsets selection stage feature selection 
roadmap general overviews classifier ensembles chapter ii feature selection chapter respectively categorization scheme 
frame classifier ensembles comprehensive empirical study nearest neighbor ensembles carried account scheme bias plus variance decomposition error rate chapter iv chapter vii 
goal improving classification performance decreasing storage requirements simultaneously method called cnn ecoc utilizes con nearest neighbors cnn algorithm conjunction technique error correcting output codes ecoc section 
vari ant called knn ecoc rs takes advantage randomly selected subspaces conjunction ecoc method improve performance nearest vi neighbors suggested product section 
frame feature selection undertake study genetic algorithms learning scheme 
performance gains learning demonstrated experiments chapter vii 
mean time categorization scheme existing genetic feature selection methods conducted chapter vi 
contributions dissertation summarized follows 
categorization scheme classifier ensemble methods 
categorization scheme feature selection methods 
taxonomy classification methods nearest neighbor learning algorithm 
categorization scheme genetic feature selectors 
cnn ecoc methods takes advantage condensed nearest neighbor cnn algorithm conjunction technique error correct ing output codes ecoc 
seen major contribution dissertation 
knn ecoc rs method takes advantage randomly selected sub spaces feature space conjunction ecoc technique 
method suggested product 
initiate advocate idea learning conjunction genetic algorithms perform feature selection 
seen second major contribution dissertation 
performance comparison various nearest ensembles takes account paradigm bias plus variance decomposition error vii rate 
classification algorithm small error rate ideally time small bias small variance usually favored better choice 
performance comparison genetic search sequential float ing forward search takes account number subset evaluations provides pragmatic view time complexity search methods hyperspectral data 
viii table contents dedication 
ii 
iii summary 
notation 
xi chapter 
background hyperspectral remote sensing 
statement problem 
organization dissertation 
ii 
classifier ensembles overview 

hypothesis classifier ensembles 
categorization scheme classifier ensembles 
voting classifier ensembles 
classifier ensembles manipulating training samples 
classifier ensembles different input feature subsets 
heterogeneous classifier ensembles 
homogeneous classifier ensembles 
recursive partition classifier ensembles 
summary 
iii 
nearest neighbor learning algorithm revisited 

nearest neighbor classifier 
fuzzy nearest neighbor classifier 
iv 
ensemble methods nearest neighbor learning algorithm 

error correcting output codes ecoc 
ensemble methods nearest neighbor classifiers 
nearest neighbor classification multiple feature subsets mfs voting multiple condensed nearest neighbors cnn 
nearest neighbor classifiers small prototype sets skalak nn architecture 
ix nearest neighbor classifier error correcting output codes feature selection knn ecoc fs 
condensed nearest neighbors cnn error correcting output codes cnn ecoc 
bias plus variance decomposition error rate 
feature selection methods overview 

relevance concept weak strong relevance 
general characteristics feature selection methods 
categorization scheme feature selection methods 
filter selection model 
wrapper selection model 
embedded selection model 
vi 
genetic feature selectors 

genetic algorithms 
categorization scheme genetic feature selectors 
aggregation selection parameter variation 
pareto selection 
vii 
experiments discussion 

experiments nearest neighbor classifier ensembles 
experiment 
experiment 
experiment 
summary remarks nearest neighbor classifier ensembles 
experiments genetic feature selector 
experiment 
experiment 
summary remarks genetic feature selector 
viii 



samenvatting 
list publications 
bibliography 
notation notation dissertation follows general conventions mathematics statistics 
natural constant xn transposed dimensional vector argmax fi returns largest value fi index distance multi dimensional vectors notation classification schemes considered dissertation follows training dataset labeled examples xi space coming yi predefined classes space xi yi 
samples xi labels yi 
classifier hypothesis predicts corresponding values new values 
classifiers denoted contrast target function true corresponding relationship xi yi 
xi chapter background hyperspectral remote sensing earth observation remote sensing provided human global view earth 
remote sensing versatile tool exploring earth involves instruments sensors capture spectral spatial relations objects materials observable distance typically 
aerial photograph common example remotely sensed camera film product 
numerous real world applications typical monitoring forest tree species determining status growing crop defining urban patterns delineating extent flooding recognizing rock types pinpointing areas deforestation 
rapid advances sensor technology possible collect re mote sensing data multispectral data typically ranges bands today hyperspectral data spectral bands span bands meter spatial resolution bits dynamic range airborne visible infrared imaging spectrometer bands range nm spatial ground resolution meter airborne prism experiment apex 
iii imaging spec 
background hyperspectral remote sensing trw produce spectral band imagery spatial resolutions spanning meter meters spectral coverage nm 
spectral resolution nm visible near infrared nm nm short wave infrared nm 
number spectral bands tends continue increase rapid development sensor technology 
sensor currently developed span spectral bands 
hyperspectral image viewed image cube third dimension spectral domain represented hundreds narrow contiguous spectral bands corresponding spectral reflectance 
plot contiguous spectral reflectances compared laboratory produced ones identify example target area 
rapid development hyperspectral sensor technology greatly extends scope traditional remote sensing provides scientists environmental geoscience research communities power explore earth provides challenges data analysis tasks 
furthermore volume hyperspectral data produced staggering 
fast growing size hyperspectral database requests compression research 
particular compression algorithm take advantage spectral nature hyperspectral data 
shows hyperspectral image contains bands pixels downloadable groundtruth image containing classes shown 
dataset experimental comparison dissertation 
pixel image represented obstacle research hyperspectral remote sensing due lack data despite rapid advances sensor technology demands hyperspectral data general hyperspectral imagery markets ready commercially support industry years due number factors distinct requirements different market segments 
neural network techniques compression 

statement problem example band hyperspectral image simulated color infrared form 
dimensional vector 
classification task recognize new unknown objects predefined classes ground truth training classes 
views hyperspectral data image space spectral space feature space 
image space 
illustration spectral space classes shows corresponding dimensional feature space 
statement problem pattern recognition research area 
term pattern recognition meant broad sense generally speaking encompasses researchers pattern recognition community speak highly book agree researchers walks pattern recognition get book part due provides better balance theoretical practical treatment pattern recognition 
long list reseach monographs useful understanding pattern recognition reader referred tudelft pattern recognition group page 
date books simply input key words pattern recognition book search large online book stores amazon com bn com buy com find available date books topics relating pattern recognition 

statement problem groundtruth image containing predefined classes corn corn grass grass trees grass min till clean wheat woods bldg grass trees drives stone steel towers 
spectral space feature space 
statement problem major categories supervised learning unsupervised learning semi supervised learning partially supervised 
supervised learning kind machine learning learning algorithm provided set inputs gorithm corresponding correct outputs learning involves algorithm comparing current actual output correct target outputs knows error modify things accordingly 
contrast su learning unsupervised learning signifies mode machine learning system told right answer example trained pairs consisting input desired output 
system input patterns left find interesting patterns regularities clusterings 
semi supervised learning thought kind supervised system unlabeled samples incorporated 
shows typical supervised classification process hyperspectral data analysis 
typically classification performance improves limited point ad ditional features added deteriorates 
referred hughes phenomenon peaking phenomenon shown 
vertical axis mean recognition accuracy averaged possible classifiers 
plot ted function measurement complexity horizontal axis 
bands hyperspectral data greater measurement complexity 
parameter number training samples 
hughes phenomenon explained follows 
consider finite fixed number training samples 
ac statistics estimation decreases dimensionality increases leading opinion term curse dimensionality describe difficult problems general high dimensional multivariate analysis uses curse dimensionality describe difficulty visualization problem high dimensional data analysis follow researchers hughes phenomenon describe high dimensional data classification problem dissertation 

statement problem deterioration classification accuracy 
increasing number spectral bands potentially provides class separability positive effect diluted poor statistical parameter estimation 
consequently performance classifier fixed sample size may degrade increase number features illustrated 
logical infer large number features give discrim power high dimensional space fact empty modest number samples importance feature reduction 
feature reduction includes feature extraction feature selection 
feature ex traction refers process finding mapping reduces dimensionality patterns feature selection refers picking number features suboptimal feature subset 
part due difficulty inter mapped features feature extraction feature selection problem emphasized critical preprocessing step 
problem arises classification task high dimensionality discrimination classes difficult 
mentioned due fact assumption training samples available accurately estimate class statistics fail data gathering training samples practice difficult expensive 
improve classification performance remains impor tant task data analysis hyperspectral remote sensing 
achieved dissertation studying idea merely fusing existing classification schemes develop new sophisticated classification techniques 

statement problem hyperspectral input data feature reduction classifier design labeled samples training testing classification classification evaluation typical supervised classification procedure hyperspectral data analysis 
hughes phenomenon 

organization dissertation high dimensionality potentially provides better class separability 
peaking phenomenon results combination opposite effects shown 
accuracy statistics estimation decreases dimensionality increases 
conceptual explanation hughes phenomenon shown 
organization dissertation previous sections background motivation research con dissertation explained 
remaining dissertation diagram past 
past generalize primary frameworks research classifier ensembles feature selection laid numerous researchers field studies classifier ensembles nearest neighbor classifier ensembles feature selection genetic feature selectors propose possible immediate 
past diagram dissertation illustrated 

organization dissertation past chapter ii overview classifier ensembles chapter iii nearest neighbor learning algorithm chapter overview feature selection chapter iv nearest classifier ensembles chapter vi genetic feature selectors chapter vii experiments discussion chapter viii past diagram dissertation 
chapter ii overview classifier ensembles tion scheme 
nearest neighbor learning algorithm revisited chapter iii 
various methods combining nearest neighbor learning algorithms described chapter iv 
scheme bias plus variance decomposition error rate explained 
goal improving classification performance decreasing storage requirements time hybrid method utilizes error correcting output codes ecoc condensed nearest neighbors cnn 
variant takes advantages randomly selected features conjunction ecoc suggested 
chap ter methods feature selection reviewed described categorization scheme 
genetic feature selector genetic algorithms en learning scheme initiated chapter vi 
mean time 
organization dissertation tion scheme done existing genetic feature selection methods 
experimental results nearest neighbor ensembles genetic feature selector typical hyperspectral remote sensing data set corresponding discussion chapter vii 
dissertation possible chapter viii 
chapter ii classifier ensembles overview seminal hansen salamon recognized unstable nature certain neural networks helpful ensembles opened door theoretical study classifier ensembles 
classifier ensembles extensive theoretical em studies 
classifier ensembles frontiers pattern recognition fall large paradigm supervised learning 
literature design methodology supervised learning addressed various supervised learning algorithms reviewed categorization scheme performance systematically investigated 
hand research classical pattern recognition models including feature selection classifier ensembles challenged development novel approach called support vector machines svms 
argument advanced design methodology svms questions online classifier ensembles bibliography 
practice may feasible collect data single flat file due reasons storage cost bandwidth security privacy proprietary nature data 
ability deal data distributed manner draws increasing attention 
research distributed classification distributed clustering corresponding distributed ensemble methods pertains emerging active direction 
distributed scheme discussed dissertation 
semi supervised learning ensembles construct classification ensembles labeled unlabeled data adaptive semi supervised ensemble 

hypothesis classifier ensembles fusing multiple classifiers redundant date raised researchers field 
kittler lecture assured rationale continued need interests research classifier ensembles 
years seen rapid progress research topic svms traditional pattern recognition model date svms benefiting research classifier ensembles 
hypothesis classifier ensembles main discovery topic combining classifiers ensemble architecture gain better accuracy individual component 
ensembles machine learning new idea aggregating opinions committee experts obtain better accuracy new 
cite condorcet jury theorem states voter probability correct probability majority voters correct implies limit approaches number voters approaches infinity marquis condorcet proposed theorem 
due 
hypothesis classifier ensemble accurate individual component classifiers component classifiers accurate diverse introduced hansen salamon 
proved individual classifiers independent error rates error rate ensemble classifier decrease number individual classifiers 
key factors crucial accurate classifiers diversity 
hypothesis classifier ensembles simulation dietterich classifiers 
accurate classifier defined having classification accuracy better random guess 
diverse classifiers different predictions new data points different error rates 
simulation dietterich shown 
dietterich stated error rates classifiers equal values furthermore error rates assumed correlated probability majority vote wrong calculated area binomial distri bution half classifiers wrong 
dietterich simulated classifier ensemble individual classifiers having error rate 
area curve 
error rate individual classifier 
relationship error rate classifier ensemble error 
categorization scheme classifier ensembles rates individual classifiers shown tumer ghosh follows ensemble individual bayes number classifiers correlation classifier errors bayes error rate obtained bayes rule assuming conditional probabilities known 
ensemble individual represent error rate classifier ensemble error rate individual component classifier respectively 
means error ensemble decreases pro number component classifiers means error ensemble architecture equals error single component classifier 
categorization scheme classifier ensembles history research classifier ensembles decade area active 
literature exists little effort categorize classifier ensembles 
generally speaking classifier en divided parallel ensembles sequential ensembles 
categorization scheme done combining strategies diversity classifier ensemble 
dietterich describes ensemble meth ods point view machine learning 
jain duin mao list number popular ensemble methods review statistical pattern recognition 
sharkey points limiting factor reseach combin ing classifiers due lack awareness full range available modular structures 
reason little agreement means describing classifying types multiple classifier systems literature 

categorization scheme classifier ensembles presents categorization scheme types multiple neural network systems subdivisions involving competitive cooperative combination mechanisms ii combining ensemble modular hybrid components iii relying bottom top combination methods iiii bottom static fixed combination methods 
chapter aspire general comprehensive categorization scheme 
describe classifier ensembles categories section voting classifier ensembles section classifier ensembles manipulating training samples section classifier ensembles different input feature subsets section heterogeneous classifier ensembles section homogeneous classifier ensembles section recursive partition classifier ensembles 
voting classifier ensembles primary categories voting ensemble scheme 
simple voting simple voting called majority voting select majority sam considers component classifier equally weighted vote :10.1.1.33.353
classifier largest amount votes chosen final classification scheme 
weighted voting 
categorization scheme classifier ensembles weighted voting schemes vote receives weight usually propor tional estimated generalization performance corresponding compo nent classifier 
weighted voting schemes usually give better performance simple voting 
weighted majority weighted majority algorithm generalization ing algorithm 
similar weighted voting main difference weights generated 
predictions weighted vote pool classification algorithms learns altering weight associated prediction algorithm 
algorithm starts assigning weight classification algorithm considers training samples 
classification algorithm misclassifies new training sample weight decreased multiplying number 
number mistakes weighted majority algorithm bounded terms number mistakes best classification algorithm voting pool 
suppose set component classifica tion algorithms minimum number mistakes algorithm training set littlestone warmuth generalized showed bound number mistakes rlog log log 
categorization scheme classifier ensembles obtain bound log classifier ensembles manipulating training samples methods fall category characteristic learning algorithm run times time different partition training samples 
works unstable learning algorithms 
unstable mean learning algorithms output predictions changes response small change training samples 
methods boosting bagging successful representative methods developed date classifier ensembles 
researchers compared boosting bagging methods demonstrated superiority 
bagging bagging due breiman 
method run times training samples run produces replication original training samples sampling replacement size original training size 
training samples appear produced samples may 
training set called bootstrap replicate original training set technique called bootstrap aggregating name bagging stems 
training set examples probability example reproduced 
categorization scheme classifier ensembles input output learning algorithm integer bootstrap sample training set hi hf argmax hi table bagging algorithm 
large mathematically speaking approximates 
value 
bootstrap reproduces average original training samples 
individual classifiers classify example test set usually vote scheme taken 
pseudocode bagging algorithm shown table 
bootstrap replicates sufficient 
breiman states replicates required increasing number classes notes bagging dream procedure parallel computing simu lation experiment varying number bootstraps verified usually sufficient 
boosting boosting proposed schapire 
proven weak learning algorithm may boosted strong theoretical model known weak learning model pac 
pac learning model probably approximately correct assumes exist weak learning gorithms slightly better random guessing regardless 
categorization scheme classifier ensembles underlying probability distribution generating examples 
known boosting idea adaboost introduced freund schapire 
term adaboost stems adaptive boosting 
solved practical difficulties earlier boosting algorithms 
just bagging adaboost manipulates training examples generate diverse hypotheses 
pseudocode adaboost illustrated table 
main tains probability distribution pn training samples 
iteration draws training set size sampling replacement probability distribution pn 
learning algorithm applied produce classifier hn 
error rate en classifier training sam ples computed adjust probability distribution training samples 
adaboost developed originally class problems methods developed handling multi class problems 
kuncheva whitaker described variants adaboost sive boosting conservative boosting inverse boosting 
boosting schapire boosting active research direction date research trends boosting consult website 
cross validated committees cross validated committees construct training sets leaving disjoint subsets training data 
example training set randomly divided disjoint subsets 
overlapping sets constructed dropping different subsets 
procedure employed construct training sets fold cross validation 
method called cross validated committees 

categorization scheme classifier ensembles input learning algorithm integer initialize initial weights pn wn wn normalize weights hn pn en pn hn xi yi calculate error hn en goto en en wn wn hn xi yi output hf argmax log hn calculate new weights table adaboost algorithm 
value equals true 
classifier ensembles different input feature subsets general method different feature subsets taken passed different classifiers 
example trained neural network classifier ensemble consists component neural networks identify volcanoes venus 
neural networks trained different subsets available input features different network sizes 
input feature subsets selected manually group features different image processing opera tions 
doing able match performance human experts identifying volcanoes 
combining classifiers different features studied chen wang chi emphasis text independent speaker identification 
systematic investigation classified frameworks linear opinion pools winner take evidential reasoning 
framework linear opinion 
categorization scheme classifier ensembles pools combination schemes final decision linear combination predictions multiple classifiers 
framework winner take chooses best classifier viewed winner 
framework evidential reasoning input pattern output individual classifier regarded evidence event combination scheme final decision method evidential reasoning 
evidential reasoning methodology dempster shafer calculus evidence 
heterogeneous classifier ensembles stacked generalization meta learning representative methods fall category heterogeneous classifier ensembles different types classifiers ensemble architecture 
comparison study combining heterogeneous sets classifiers done navarro stacked generalization general framework stacked generalization addressed wolpert 
scheme minimizing generalization error rate gen 
stacked generalization layered architecture classifiers lever bottom layer receive original data input classifiers outputs prediction 
successive layers receive predictions immediate preceding layer input 
output passed layer 
works deal layer architectures 
layer level level architecture works follows train level classifiers leave cross validation 
example training set leave train 
categorization scheme classifier ensembles remaining samples 
training classify left examples 
form vector predictions level classifiers actual class example 
train level classifier collection vectors generated previous step training set 
number examples level data equal number examples training set 
step classifiers generated leave method 
fully explore training set level classifiers re trained entire training set 
generated models classify examples test set 
course stacked generalization constrained layers generalized multi layer architectures 
meta learning survey meta learning shows term meta learning ascribed different meanings different research groups 
methods meta learning introduced chan stolfo arbiter combiner 
schemes meta learn set meta classifiers training data predictions set base classifiers 
arbiter learned learning algorithm arbitrate predictions generated different base classifiers 
aim combiner strategy coalesce predictions base classifiers learning relationship predictions correct prediction 

categorization scheme classifier ensembles homogeneous classifier ensembles research continues general classifier ensemble algorithms efforts researchers combine specific classifier classifier ble 
zheng studied naive bayesian classifier ensembles 
various neural network ensembles addressed investigated systematically 
comprehensive study nearest neighbor classifier ensembles chapter iv ensembles contain type classifier nearest neighbor classifier 
recursive partition classifier ensembles recursive partitioning algorithms classifier ensembles divide conquer strategy partition space regions contain instances class 
utgoff brodley provide scheme examples recursive partition ensemble algo rithms 
utgoff perception tree algorithms combines univariate decision tree linear threshold units 
determines subspace linearly separable heuristic measure 
subspaces linearly separable linear threshold unit applied 
space divided informa tion theoretic measure 
brodley model class selection system creates recursive tree structured hybrid classifier combines decision trees linear discriminant functions instance classifiers 
opinion recursive partition method classifier ensembles getting attention classifier ensemble community idea recursive partition lot real world applications 
online bibliography research recursive partition limited classifier ensembles see 

summary summary chapter described methods classifier ensembles tion scheme 
basic criteria usually evaluate classifier ensemble accuracy efficiency diversity accuracy top priority consideration 
argue tradeoff error rate bias variance taken account practical applications 
considering specific classifier adopted strategy choose classification algorithm small error rate ideally time small bias small variance 
example classification algorithm low error rate high bias high variance cautious case high bias means algorithm high systematic error high variance indicates algorithm poor generalization 
details scheme bias plus variance decomposition error rate see chapter iv 
chapter vii conduct experiments various nearest neighbor ensembles account bias plus variance decomposition scheme 
chapter iii nearest neighbor learning algorithm revisited nearest neighbor learning paradigm central subject theoretical experimental studies half century 
oldest simplest methods performing non parametric classification class label input pattern assigned class labels represented closest neighbors training set 
despite simplicity advantages require knowledge statistical properties data may give competitive performance compared methods 
basic concept underlying nearest neighbor classifier introduced fix hodges 
cover hart formally defined nearest neighbor rule applied pattern recognition problem 
nearest neighbor algorithm extensive study 
easy effective way calculate classification error rate leave method 
time complete training set left training sample testing 
doing training sample separately classification error rate evaluated 

nearest neighbor classifier class class class assign class neighbors classes geometric illustration crisp knn classification rule 
label vector represents absence presence label 
nearest neighbor classifier suppose unlabeled vector xi ith labeled vector number nearest neighbors find neighborhood exists dis tance measure xi geometric illustration crisp knn classification rule give 
algorithmic parameters associated knn rule value choice distance measure distance weighting measure weighted method counting votes 
distance metrics proposed literature example chi square metric metric cosine similarity metric quadratic metric modified value difference metric similarity cosine angle dimensional vectors cos similar objects closer value cos approaches angle close 
hand spectral angle mapper sam somewhat popular classifier laboratory spectra determine similarity spectra calculating spectral angle 
algorithm implemented envi software package shares idea cosine similarity metric name sam remote sensing community fact similar nearest neighbor rule nn cosine similarity metric distance measure 
probably better tunable parameter sam algorithm choice needs 

fuzzy nearest neighbor classifier euclidean distance commonly 
select distance metrics studied barker 
euclidean distance assumes variables uncorrelated justified hyperspectral data dissertation commonly 
variants knn milestone articles knn rule variants 
machine learning community knn called instance learning memory learning responses computed interpolating table stored patterns 
drawbacks nonparametric methods require large amount computation time 
case nearest neighbor classification due fact compute time distance input pattern training sample patterns order find nearest neighbors 
methods proposed reduce computation time literature 
fuzzy nearest neighbor classifier difficulties knn labeled samples equal importance deciding class memberships patterns classified re 
hand problem arises classification high dimensionality discrimination classes terms instance memory learning just synonymous names lazy learning 
lazy learning subsumes family algorithms store complete set examples delay requests classifying unseen instances received 
synonymous names lazy learning include exemplar cased experience 
opposed lazy learners eager learners artificial neural networks algorithms training examples complied model training time available runtime 
field computational geometry referred nearest neighbor search similarity search database points multidimensional space construct data structure query point finds database point closest 
practice dimensionality higher higher computing exact nearest neighbor difficult task 
algorithms significantly better brute force computation distances 
case approximate nearest neighbor sought exact nearest neighbor trade accuracy time complexity 
nearest neighbor search problem key issue computational geometry great importance areas computer science including pattern recognition databases vector compression computational statistics data mining 

fuzzy nearest neighbor classifier difficult 
due fact number training samples needed catch increasing dimensionality grows overwhelmingly 
pattern recognition crisp classification replaced fuzzy classification 
techniques decision classify data point delayed long possible memberships 
membership values assigned point function point distance nearest neighbors neighbors memberships possible classes 
techniques proven outperform crisp clas techniques especially clusters tend overlap 
theory fuzzy sets introduced nearest neighbor classification 
fuzzy knn rule 
fuzzy knn classifier designed keller class memberships assigned sample function sample distance nearest neighboring training samples 
fuzzy knn procedure described 
store training data partitions 

choose number neighbors find 

choose distance metric 
vector xi compute rank order distances xi dk dk 
calculate ui uij xj xj ui assigned membership vector uij membership ith class jth vector labeled sample set scaling parameter 
fuzzy nearest neighbor classifier 
euclidean distance xj xj memberships training samples uij defined ways 
way give complete membership class classes 
fuzzy alternative assign training samples memberships distance class mean 
calculating memberships test sample assigned class highest membership 
experiments second approach leads best results 
chapter iv ensemble methods nearest neighbor learning algorithm integration predictions number classifiers shown effective way achieve accurate classification component classifiers promising results real world applications handwritten character recognition protein structure prediction calculation fat content ground meat 
general algo rithms combining classifiers bagging boosting 
area important directions machine learning research different names classifier fusion classifier ensembles aggregation 
contrast huge amount research active area little done combining specific classifier nearest neighbor classifier knn 
knn rule studied improved ensemble methods knn classifiers limited literature 
chapter part chapter vii generalized 
problem combining preference arises applications combining results different search engines 

error correcting output codes ecoc purpose chapter fold comprehensive description nearest neighbor classifier ensembles performed experimental study hyperspectral remote sensing data chapter vii 
ii method cnn ecoc combines condensed nearest neighbors cnn conjunction error correcting output codes ecoc proposed section 
variant method knn ecoc rs utilizes randomly selected features error correcting output codes suggested product section 
chapter organized follows section technique error correcting output codes ecoc briefly explained 
section different kinds nearest neighbor classifier ensemble methods extended methods 
taxonomy classification methods nn algorithm 
section scheme bias plus variance analysis error rate explained 
error correcting output codes ecoc error correcting output codes ecoc kind distributed output rep proposed multi class classification tasks dietterich bakiri seminal classifiers combined multi class problems decomposing multiple class distribution classifiers 
class assigned binary code word component classifier assigned task learning bit position code word 
resulting predictions component classifiers form vector representing separation classes disjoint subsets 
hamming distance measure compute closest codebook vector vector predictions 
table shows example typically pre defined ecoc codes class 
ensemble methods nearest neighbor classifiers code word class corn min grass grass trees clean wheat table example typical bit ecoc codes class problem 
problem 
suppose class classification problem description classes table chapter vii 
classify new object bits evaluated obtain bit binary string say 
hamming distance counts number bits differ string pre defined codewords calculated 
nearest codeword corresponds class corn min 
new object assigned corn min class 
ensemble methods nearest neighbor classifiers section describe existing methods com nearest neighbor classifiers extended methods 
ensemble methods belong category homogeneous classifier ensembles described section contain nearest neighbor classifiers component classifiers 
taxonomy classification methods chapter nn algorithm illustrated 
divide methods nn algo rithm ensembles nn component classifier non ensembles single nn variant 
divided nn en different categories voting ensembles ensembles different input feature subsets ensembles manipulating training samples 
voting mul tiple condensed nn section belongs voting ensembles 
ensembles 
ensemble methods nearest neighbor classifiers methods nn nn ensembles nn non ensembles nn fuzzy nn condensed nn edited nn ensembles manipulating training samples ensembles different input feature subsets voting ensembles nn small prototype sets voting multiple condensed nn section skalak nn architecture cnn ecoc section section nn classification knn ecoc fs multiple feature subset knn ecoc rs section section taxonomy classification methods nn algorithm 

ensemble methods nearest neighbor classifiers different input feature subsets consist methods nn classification multiple feature subsets section knn ecoc selection method knn ecoc fs knn ecoc randomly selected feature method knn ecoc rs section 
skalak nn architecture section condensed nearest neighbors cnn error correcting output codes cnn ecoc section methods belong ensembles manipulating training samples 
nearest neighbor classification multiple feature subsets mfs bay proposed algorithm nearest neighbor classification multiple fea ture subsets mfs 
uses simple voting scheme just takes output highest accuracy output number component nn classifiers 
component nn classifiers number features feature subsets chosen sampling original feature space 
sam pling methods sampling replacement sampling replacement 
voting multiple condensed nearest neighbors cnn number training patterns large need store patterns requires large memory 
hart condensing algorithm solved problem storing subset full training set 
algorithm works follows 
starts subsets subset store subset 
store subset empty subset contains training set 
store subset initialized sample taken randomly 
iteration randomly take sample correctly classified current samples store place subset store throw back subset 
procedure referred condensing 

ensemble methods nearest neighbor classifiers method local search resulting subset store depends order samples stored 
shuffles training samples may get different results 
may want procedure iterations order speed process example stored patterns iteration say training set 
final patterns stored store subset condensed patterns training patterns 
classification rule procedure nn nn rule computational cost higher 
alpaydin proposed train multiple condensed nearest neighbor subsets take vote 
voting schemes simple voting voters equal weights weighted voting weights depend classifiers confidence predictions 
simple method calculate weights nearest neighbors say closest pattern second closest weight class label distance measure 
way union method combines multiple cnn applying nn union subsets obtained multiple cnn nearest neighbor classifiers small prototype sets skalak nn architecture follow skalak discussion composite nearest neighbor classifiers 
architectures classifier combinations till primary archi combining classification algorithms 
stacked generalization local search strategy ib grow learn gal learning scheme 

ensemble methods nearest neighbor classifiers nearest neighbor component classifiers hi id nn vote combining prediction hn instance train class instance component predictions stacked nearest neighbor classifier architecture 
vote boosting architecture 
classifier taken additional component classifiers 

boosting 
recursive partitioning 
wolpert probably discuss idea stacked generalization full generality 
stacked generalization assumes set level compo nent learning algorithms level learning combining algorithm training set classified instances 
recursive layered structure com classifiers layer classifiers combine output classifiers just layer 
boosting due schapire 
goal boosting increase accuracy algorithm distribution training instances 
successively creates complementary component classifiers filtering training set 
recursive partitioning algorithms divide conquer strategy partition space regions contain instances class 

ensemble methods nearest neighbor classifiers example recursive partitioning architecture 
component classifiers applies particular region instance space 
composite architectures studied skalak 
layer architecture consisting level level classifiers 
level classifiers consist classifiers base classifier say full near est neighbor classifier uses instances prototypes complementary classifier say minimal nearest neighbor classifier storing prototype class 
complementary nearest neighbor classifier obtained procedure randomly sample sets instances replacement training set number classes exposed instance drawn class 
set prototype set construct nearest neighbor classifier 
classify instances classifiers 
choose classifier highest classification accuracy classifier 
level combining algorithm decision tree algorithm id 
original training instance class si level training instance 
ensemble methods nearest neighbor classifiers id base classifier instance instances stored prototypes level classifier complementary classifier level classifier instance prototype stored class composite architecture 
created si 
example level instance predicts class applied instance predicts class applied instance level feature representation 
entire level representation includes class say level representation 
set tuples class samples training set train level learning algorithm id 
id greedy algorithm grows tree top node selecting attribute best classifies local training examples 
procedure continues tree perfectly classifies training examples attributes 
id longer considered state art decision tree level combining algorithm 
level features symbolic implementation id uses feature selection metric described decision tree descendant id 
dietterich bakiri showed ecoc improve de cision trees neural networks 
motivated hypothesize ecoc combination structure shown probably improve classification gains 
experimental results explained section chapter vii 

ensemble methods nearest neighbor classifiers nearest neighbor classifier error correcting output codes feature selection knn ecoc fs bagging directly combining error correcting output codes ecoc knn improves classification performance aha bankert proposed combine ecoc feature selection output bit empirical study cloud data showed performance gains 
procedures follows create codewords specific problem 
step confusion matrix obtained ib classifier training set 
create output bits building set partitions output bit repeat set partitions distinguishes pair classes requested hamming distance 
knn ecoc computes set features predicting bit values output bit selection bit 
test sample knn ecoc predicts value output bit compares predicted output string codeword yielding simi lar codeword class prediction test sample class 
classification accuracy calculated 
probably interesting technique 
knn ecoc idea combined feature selection method improves classification gains feature selection bit 
considerable computational cost 
combining randomly chosen feature subset bit feature selection act way feature selection distances computed differently bit procedure save considerable computational time 
call variant knn ecoc rs dissertation 
random 
ensemble methods nearest neighbor classifiers sampling original feature space naturally chance get classification performance improved 
ecoc conjunction random sampling errors occurring chance bit ecoc codes smoothly alleviated consequence entire performance improved 
condensed nearest neighbors cnn error correcting output codes cnn ecoc nonlocal learning algorithms induce compact classifiers decision tree neural networks trained backpropagation benefit ecoc local ones generate pre information near query samples nearest neighbor algorithm 
reason local information bias errors different output bits correlated prevent ecoc reducing bias errors 
aha bankert solved problem combining ecoc feature selection technique 
hand method vot ing multiple condensed nearest neighbors cnn generates different cnn shuffling training samples run 
leads propose combine ecoc multiple cnn relies running cnn output bit similarity computed different resulting cnn bit 
fact cause different stored samples retrieved different output bits predictions correlated 
follows strategy seminal aha bankert depending local information classification predictions uses different local information output bit 
trying create designed codeword step section convenient large sets code 
bias plus variance decomposition error rate words ecoc designed dietterich group downloaded 
experiments chapter vii pre designed codes 
bias plus variance decomposition error rate unfair evaluate classification performance error rate considering bias variance effects classification algorithm 
example classification algorithm acceptable classification performance high variance cautious high vari ance suggests algorithm poor generalization 
bias plus variance decomposition powerful tool explaining changes potential gorithm affect resulting error rates 
researchers proposed number decomposition methods literature 
basic idea theoretical framework classification algorithm kinds errors systematic error due representation languages learning algorithm 
ii error resulting random variation noise training set random behavior learning algorithm error depends generated model training set 
follow kohavi wolpert definitions bias variance decomposition method avoids potentially negative variance 
values hypothesis target function associated zero loss function test sample mapping 
yf yh yf yh 
cost random variable defined loss random variable yf yh 
expected cost error rate expressed 
bias plus variance decomposition error rate yh yf equation rewritten yh yf yh yf yh yf yf yh rearranging terms yh yf yh yf yf yh yf yh yh yf assume yf yh conditionally independent yh yf yh yf term equation zero 
estimating expected cost fixed target averaging 
bias plus variance decomposition error rate training set size written bias bias yf yf yh yh bias difference learning algorithm average prediction target 
refers systematic error learning algorithm 
variance tells learning algorithm prediction bounces different training sets size 
results random variation noise training set random behavior learning algorithm 
intrinsic noise learning algorithm 
chapter vii take account bias plus variance decomposition error rate study performance various nearest neighbor classifier ensembles 
chapter feature selection methods overview feature selection attribute selection traditional research topic dat ing back early 
broad subject spans research disciplines statistics pattern recognition data mining machine learning neural networks fractals rough sets theory mathematical programming 
advantages feature selection reduces dimensionality feature space removes redundant irrelevant noisy data 
immediate effects data analysis tasks speeding running time learning algo rithms improving data quality increasing accuracy resulting model 
feature selection 
suppose original feature space cardinality selected feature space cardinality selection criterion selected feature space loss generality assume higher value indicates better feature space 
goal online feature subset selection bibliography 
dissertation feature selection attribute selection variable selection distinction 
refer book feature selection knowledge discovery data mining consult appendix easy machine learning ml data mining knowledge discovery kd resources see online 

relevance concept weak strong relevance maximize 
formally problem feature selection find sub space max exhaustive approach performed need consider pos sible combinations 
number combinations grows exponentially making exhaustive search unfeasible larger values moderate values performing exhaustive search impractical 
finding best feature subset usually intractable problems related feature selection shown np hard 
kinds feature selection strategies number features say task search algorithms decide features constitute sub optimal feature subset 
ii second strategy search smallest feature dimensionality tion performance exceeds specified value 
iii third search strategy selects sub optimal feature subset trade class ity classification error rate subset size number selected features 
relevance concept weak strong relevance determining features relevant learning task central issue machine learning inclusion irrelevant redundant features reduce performance different learning algorithms 
order determine features relevant need know concepts weak relevance strong relevance 
number different definitions machine learning literature means features relevant 
john kohavi pfleger 
general characteristics feature selection methods define notations relevance strong relevance attribute xi strongly relevant removal yields deterioration performance bayes optimum classifier 
weak relevance attribute xi weakly relevant strongly relevant exists subset variables performance xi better performance features strongly relevant weakly relevant ir relevant 
irrelevant features left 
general characteristics feature selection methods feature selection aims search relevant features feature space 
re searchers studied various aspects feature selection 
point view heuristic search blum langley argue issues affect nature search characterize feature selection method 

starting point feature space 
depending point start search direction vary 
search features successively add called forward selection 
con trast search features successively remove features called back ward selection 
third method combine forward backward search 

organization search procedure 
obviously number features large exhaustive search feature subspace prohibitive possible combinations features 
example heuristic search realistic exhaustive search doesn guarantee finding optimal solutions 

general characteristics feature selection methods 
evaluation strategy 
feature subsets evaluated important problem 
classifica tion ideal feature subset best separation data 
data separation usually computed inter class distance measure 
frequently discriminating measure lambda defined follows intra class matrix dispersion corresponding selected vari able set corresponding inter class matrix determinant matrix computed respectively nj nj number classes nj number samples class mean class global mean 
smaller value better discriminating power indicates 
dissertation classification accuracy evaluation fea ture subset 
classification accuracy defined percentage test examples correctly classified algorithm 
induction algorithms incorporate criterion information theory directly measure accuracy training set 

categorization scheme feature selection methods 
criterion stopping search 
process evaluation want search ob serving improvements classification accuracy 
categorization scheme feature selection methods plenty effort compare evaluate different feature selection meth ods attempts categorize feature selection methods literature 
siedlecki sklansky discussed evolution feature selection methods grouped methods past categories 
main focus branch bound method variants 
dash liu divided existing feature selection methods different groups major characteristics feature selection generation procedure complete heuristic random evaluation function distance information con classification error rate 
taxonomy feature selection algorithms broad categories jain zongker methods divided statistical pattern recognition spr classification tech niques artificial neural networks 
spr category divided sub categories 
categorization simply done monotonicity selection evaluation criteria monotonic versus non monotonic 
categorization time complexity feature selection algorithm time complexity floating search meth ods sequential backward sequential forward take somewhat pragmatic viewpoint time complexity forward backward floating search methods example 
suppose number selected features original feature space features say time complexity forward floating search backward floating search 
see clearly case time complexity backward floating search times forward floating search indicates forward version floating search preferred forward backward floating search methods applicable 

categorization scheme feature selection methods selection methods denotes tight estimate complexity denotes estimate complexity upper bound known 
feature selection methods categorized general groups classifier specific selection methods goodness evaluated criterion error rate certain classifier useful cases know classification performed selection classifier independent selection methods goodness evaluated methods criterion measures approximation class conditional probability density functions useful cases don know classification 
categorization schemes include simply dividing feature selection methods optimal exhaustive search vs non optimal suboptimal point view optimality resulting subset backward elimination vs forward selection point view starting point feature search space 
hand feature selection generally regarded optimization problem 
general optimization problem may optimization tree category divides optimization techniques discrete optimization continuous optimization divided sub categories 
optimization reader referred optimiza tion online 
describe typical model approaches filter selection model wrapper selection model embedded selection model 
need categorize selection methods major groups promising ones large dataset genetic feature selectors discussed chapter vi floating search methods effective large scale databases especially time information grows amazing speed promising ones weak power deal high dimensional data due reasons costly computation complexity problem mainly targeted small medium scale databases 

categorization scheme feature selection methods filter selection model filter selection model earliest approach feature selection 
utilizes independent search criterion find appropriate feature subset machine learning algorithm performed termed filter method john kohavi pfleger filters irrelevant attributes induction occurs search done independently induction algorithm 
procedure filter model shown 
advantage filter model need re run algorithm induction algorithm choosing run reduced feature dataset consequence filter approach generally computational efficient practical data sets high dimensionality 
number different representative filter algorithms literature 
focus algorithm designed almuallim dietterich originally boolean domain searches feature space looking feature isolation turn pairs features triples stops finds minimal combination features 
minimal feature subset divides training data pure classes instances class 
original training samples characterized resulting feature subset passed decision tree induction algorithm id 
representative filter approach relief algorithm due kira rendell 
relief algorithm follows general simple filter scheme evaluates individual feature evaluation criterion best features selected 
uses complex evaluation function 
training samples characterized selected features passed id 
extensions algorithm kononenko general data types treated 

categorization scheme feature selection methods focus relief decision tree induction algorithm feature selection naturally confined decision tree algorithms induction algorithms 
table shows list filter approaches feature selection literature 
filter approach take account learning bias introduced final induction algorithm may able select suitable subset final induction algorithm 
reason wrapper model proposed 
wrapper selection model strategy wrapper model induction algorithm estimate merit searched feature subset training data estimated accuracy resulting classifier metric wrapper approaches better results filter approaches tuned specific interaction induction algorithm training data 
chapter vi discuss typical wrapper selection approach called genetic feature selectors genetic algorithms search engine uses nearest neighbor classifiers induction algorithm 
way feature selection takes account biases final learning algorithm 
wrapper approaches supported study aha john 
wrapper selection procedure illustrated 
disadvantage wrapper model tractable prohibitive cost running classification algorithm times dimensionality considerably high 
table shows different wrapper approaches feature selection literature 

categorization scheme feature selection methods training set features feature selection procedure search algorithm candidate feature subset training set candidate feature subset measure discrimination power feature subset training set selected feature subset training set selected feature subset induction algorithm final model test set selected feature subset final performance estimation test set filter selection procedure 

categorization scheme feature selection methods author system starting point search control evaluation criterion learning algorithm aha bankert beam variants forward ib backward selection separability almuallim dietterich focus breadth id cardie greedy consistency nearest neighbor kira rendell relief ordering threshold id singh provan greedy information gain bayesian network koller sahami greedy threshold tree bayes liu setiono lvf random las consistency id kubat greedy consistency naive bayes schlimmer consistency table summary different filter approaches feature selection 

bas described section focus algorithm searches feature isolation turn pairs features triples finds minimal subset divides training data pure classes context say focus algorithm finds minimal subset consistent training data 
clas vegas algorithm uses randomness guide search random subset generated round search 
systematic search avoid revisiting search states 

categorization scheme feature selection methods training set features feature selection procedure search algorithm candidate feature subset performance estimation training set evaluation function training set candidate feature subset selected feature subset training set selected feature subset induction algorithm test set selected feature subset final model final performance estimation test set induction algorithm wrapper selection procedure 

categorization scheme feature selection methods author system starting point search control evaluation criterion learning algorithm aha bankert random beam variants forward leave ib backward selection cross validation moore lee comparison greedy better nn skalak random fold cross validation nn langley sage greedy fold cross validation nn langley sage greedy accuracy training set naive bayes singh provan greedy information theoretic measures bayesian network table summary different wrapper approaches feature selection 
binary vector represent feature subset search step mutate element bit chosen random point binary vector evaluate new vector 

categorization scheme feature selection methods embedded selection model contrast wrapper approach treats feature selection wrapper induction process embedded approach embeds selection basic induction algorithm 
examples model decision tree algo rithms id quinlan cart breiman 
decision tree algorithms recursive partitioning methods induction carry greedy search space decision trees 
stage evaluation function select attribute best ability discriminate classes 
partition training data attribute repeat process subset extending tree downwards discrimination possible 
approaches model called weighted model introduced feature weighting considered 
fuzzy set theory introduced decision trees researchers 
quinlan developed advanced version called see 
algorithm called generalization know cart algorithm 
difference id cart id aims knowledge comprehensibility symbolic domains cart naturally designed deal continuous domains lacks level comprehensibility 
chapter vi genetic feature selectors section introduce background evolutionary algorithms situate genetic algorithms frame evolutionary algorithms 
evolutionary algorithms eas broad class different randomized search heuristics currently include evolution strategies ess evolution ary programming ep genetic programming gp genetic algorithms gas 
stem modeling natural evolution pro cesses 
evolutionary principles employs particular chromosomal representation set genetic operators selection replacement scheme 
evolutionary computation countless real world applications applied field hyperspectral image analysis remote sensing 
hand evolutionary algorithms proposed optimization research continues 
genetic algorithms successful soft computing technique solving optimiza chapter part chapter vii generalized 
don distinction evolutionary algorithms evolutionary computation 
evolutionary computation refer page international society genetic computation check menu books members 
cooperative coevolutionary computation type evolutionary computation promises advantages traditional evolutionary algorithms terms corresponding adaptability potential open 

genetic algorithms tion problems applied problem feature selection 
gas efficient 
chapter discuss problem genetic algorithms search engine perform feature selection high dimensionality limited training data 
outline chapter follows sections describe basic concepts standard genetic algorithms address categorization scheme existing feature selection methods genetic algorithms feature selector genetic algorithms conjunction learning scheme 
genetic algorithms genetic algorithms gas invented holland general purpose search algorithms utilize principles inspired natural population genetics evolve solutions problems 
different variants genetic algorithms vary aspects share prototypical procedure shown 
gas iterative optimization process set operators crossover mutation applied 
solution represented finite sequence called chromosome 
chromosomes allowed crossover parental chromosomes simple way choose randomly point called crossover point point copies parent crossover point copies second parent 
way parental chromosomes exchange parts crossover point create new child chromosomes 
chromosomes allowed mutate small change flipping bit see online genetic algorithm archive 
finland compiled series indexed bibliography genetic algorithms applications areas 
series bibliography downloaded 

genetic algorithms criterion met define genetic representation problem randomly create initial population compute individual fitness current population choose parents reproduction individual fitness crossover mutation outline standard gas 
chromosome 
optimization process carried generations time population new chromosomes generated 
population size finite best chromosomes allowed survive 
fitness function defined allows calculate fitness score chromosomes 
due inherent parallel nature genetic parallel versions genetic algorithms proposed literature 
theory fuzzy set introduced genetic algorithms 
study various aspects genetic algorithms reader referred standard introductory material 

categorization scheme genetic feature selectors categorization scheme genetic feature selectors genetic feature selectors series feature selection methods genetic algorithms guide selection procedure 
basically speaking genetic feature selectors fall category wrapper model described section chapter gas may general classification scheme categorized groups combine single learning scheme non learning scheme combine learning scheme ensemble nearest neighbor learning algorithms proposed evaluate merit features selected selection process 
chapter vii experiments learning scheme demonstrate superiority single learning scheme genetic feature selection 
genetic search type search properties stochastic search ii multi point search iii direct search iiii parallel search 
due reasons genetic algorithms applied problem feature selection 
genetic feature originally inspired seminal siedlecki sklansky 
designed genetic feature selection algorithm efficient high dimensionalities 
kelly davis punch expanded approach gas feature extraction 
extended genetic feature selection simultaneous optimization feature weights selection key features including masking vector ga chromosome 
followed procedure siedlecki sklansky seminal mainly initiated idea learning scheme means 
categorization scheme genetic feature selectors evaluate intermediate subsets selection stage 
classical optimization procedures genetic feature selector op single solution modifies population solutions time 
guarantees suboptimal optimization 
problem feature selection chromosome length total number features 
stands selected feature stands rejected feature 
ways optimize binary string 
way minimize classification error rate 
necessarily minimize number selected features 
better optimize classification performance number selected features simultaneously 
categorize genetic feature selection methods fitness function chosen 
optimization problems evolutionary algo rithms broadly divided single objective optimization multi objective optimization 
real world direct single objective optimization objective fitness function identical rare 
ing objectives simultaneously common 
way fitness assignment selection performed existing feature selection methods genetic algorithms classified categories tion selection parameter variation section pareto selection section 
aggregation selection parameter variation category different objectives combined aggregated scalar fitness function 
weighted sum approach popular category adds different objective functions different weighting coefficients com fitness function 
practice defining suitable trade different 
categorization scheme genetic feature selectors objectives needs knowledge domain concerned consequence general non trivial task 
advantages approach simplest efficient interaction decision maker needed 
optimization done multiple directions members population evaluated different objective function 
siedlecki sklansky idea define threshold error rate find binary string lowest number selected features leads error rate lower fitness function defined follows ai aj ai ai mean standard deviation population small positive constant assures minf ai fit chromosome chance reproduce 
score string length number string penalty function obtained error rate threshold error rate negative grows larger grows rapidly exp exp small scaling parameter 
penalty function approach constrained optimization problems literature decades 
penalty methods constrained genetic optimization addressed 
smith categorize penalty functions groups static penalty functions dynamic penalty functions adaptive penalty functions 
yao show applying different penalty function methods evolutionary optimization equivalent different selection schemes 

categorization scheme genetic feature selectors yang honavar genetic feature selector combines neural network clas standard genetic algorithm 
defined fitness function com different criteria classification accuracy neural network cost classification fitness accuracy cost accuracy feature subset fitness fitness 
accuracy classification accuracy neural network classifier subset estimated calculating percentage patterns test set 
cost cost classification number different measures cost measuring value specific feature needed classification risk involved upper bound costs candidate solutions 
ishibuchi nakashima similar kuncheva jain optimize competing objectives ously minimize training set minimize classification error rate minimize number selected features 
pareto selection method aggregation selection parameter variation commonly disadvantages 
difficulty artificially designed composite fitness aware behavior objective function 
secondly trade model accuracy complexity difficult explore 
thirdly weights proven problematic 
final ga solution usually sensitive small changes penalty function coefficients weighting factors 

categorization scheme genetic feature selectors pareto selection approach proposed 
solution said dominant performance superior respect criteria 
solution said pareto optimal dominated solution 
category aware multi objective genetic feature selector uses multiobjective genetic algorithms aiming producing pareto optimal feature subsets 
main idea variant niched pareto genetic algorithm npga feature selection 
essence genetic feature selection falls scope multi objective opti mization optimize objectives error rate number selected features simultaneously 
multi objective optimization active rich research area 
different methods multi objective optimization evolutionary algorithms studied compared 
literature study refer reader remarkable surveys 
excellent self contained topic multi objective optimization recommend dissertation van veldhuizen 
comprehensive multi objective optimization evolutionary algorithm see unique online evolutionary multi objective optimiza tion repository information serve gateway interested area 
chapter vii experiments discussion dataset empirical study airborne visible infrared imaging spectrometer dataset shown section chapter chapter experiments carried nearest neighbor classifier en genetic feature selection respectively 
due lack accessing hy data conduct experiments freely available dataset described validate experiments ways conduct experiments bands data ii bands data take values 
course alternative validate data randomly shuffling spectral bands take bands randomly shuffling spectral bands take bands leads question design optimal experiment limited hyperspectral dataset scope dissertation interesting 

experiments nearest neighbor classifier ensembles class name corn min grass grass trees clean wheat 
samples table description classes data set 
experiments nearest neighbor classifier ensembles experiments carried class problem described table implemented java machine learning package weka university waikato open source library colt cern 
random number generator implementation mersenne twister algorithm matsumoto nishimura strongest uniform pseudo random number generators known far time quick 
pseudo random number generators reader referred 
conducted experiments experiments done training set test set 
aha bankert setting scheme training set test set time randomly splitting original data set training set testing set 
calculation bias equation variance equation experiments base section 
experiment run times average result obtained 
experiment experiment methods utilize dynamic nature con nearest neighbor cnn learning algorithm compared 
particular study performance proposed cnn ecoc method 
methods cnn voting cnn simple weighted union cnn cnn ecoc nn 
nn integrated development environment ide java free package community edition forte java called sun studio sun microsystems 
better alternative free package eclipse 

experiments nearest neighbor classifier ensembles error rate function number voters spectral band data 
base comparison 
see section details voting cnn union cnn section cnn ecoc method 
pointed chapter iv cnn suited classification tasks demand reasonably small memory footprint tolerate acceptable perfor mance deterioration 
error rate bias variance plotted respectively function spectral bands starting bands full bands bands bands full bands 
figures show respectively comparison error rate bias variance nearest neighbor classifier nn condensed nearest neighbor cnn simple voting cnn weighted voting cnn union voting cnn ecoc 
voting method performs better increase number voters see doesn show higher performance gains number voters small 
number stored patterns method increases number voters linear relation shown 
experiment set number voters 
plots obtain 
experiments nearest neighbor classifier ensembles number stored patterns function number voters spectral band data 
weighted voting cnn regarded perform better simple voting cnn observe method weighted voting cnn doesn show advantages method simple voting cnn performances coincide completely dataset 
due largely setup weighting coefficients nearest neighbors taken account 
author reported performance union cnn computational cost promising datasets appear dataset 
see performance union cnn points worse cnn voting cnn outperforms cnn spectral bands 
subset union cnn obtained applying nn union subsets chosen multiple cnn resulting union cnn considerable number training samples 
results highlight voting process number training samples contributes performance gains 
see proposed method cnn ecoc achieves system 
experiments nearest neighbor classifier ensembles error rate spectral bands nn cnn union cnn simple voting cnn weighted voting cnn ecoc cnn comparison error rate nn cnn union cnn simple voting cnn weighted voting cnn cnn ecoc codes 
results shown respectively bands bands full bands 
error rate spectral bands nn cnn union cnn simple voting cnn weighted voting cnn ecoc cnn comparison error rate nn cnn union cnn simple voting cnn weighted voting cnn cnn ecoc codes 
results shown respectively bands bands full bands 

experiments nearest neighbor classifier ensembles bias spectral bands nn cnn union cnn simple voting cnn weighted voting cnn ecoc cnn comparison bias nn cnn union cnn simple voting cnn weighted voting cnn cnn ecoc codes 
results shown respectively bands bands full bands 
bias spectral bands nn cnn union cnn simple voting cnn weighted voting cnn ecoc cnn comparison bias nn cnn union cnn simple voting cnn weighted voting cnn cnn ecoc codes 
results shown respectively bands bands full bands 

experiments nearest neighbor classifier ensembles variance spectral bands nn cnn union cnn simple voting cnn weighted voting cnn ecoc cnn comparison variance nn cnn union cnn simple voting cnn weighted voting cnn cnn ecoc codes 
results shown respectively bands bands full bands 
variance spectral bands nn cnn union cnn simple voting cnn weighted voting cnn ecoc cnn comparison variance nn cnn union cnn simple voting cnn weighted voting cnn cnn ecoc codes 
results shown respectively bands bands full bands 

experiments nearest neighbor classifier ensembles classification performance improvement cnn improvement comparable nn better nn keeping lower memory demand training set nn 
ecoc decreases bias variance classifier combines claim verified 
particular cnn ecoc smaller variance cnn 
shows classification performance cnn depends specific dataset involved part due nature local search cnn uses tendency classification performance cnn ecoc varies dataset dataset mitigated ecoc technique 
decomposing multi class problem class problems ecoc technique able diminish errors occurred series distributed bits ecoc codes 
results better generalization proposed cnn ecoc method 
experiment purpose second experiment show advantage selected sub spaces feature space conjunction ecoc improve clas performance 
ways choose sub space nal feature space feature sub space selected feature selection method knn ecoc fs section feature sub space randomly taken knn ecoc rs section multiple feature subsets mfs including mfs replacement mfs replacement section 
nearest neighbor classifier nn base comparison 
comparison error rate bias variance shown figures func tion number spectral bands 
main observations 
experiments nearest neighbor classifier ensembles figures explained 
effectiveness multiple feature subsets mfs shown exper results 
parameters set mfs algorithm size feature subset number classifiers combine 
experiment bay setting value subset size subset size set basis cross validation accuracy estimates evenly spaced intervals size original feature set evaluated 
bay sets number classifiers choose parameter order save computation time 
plot observe difference mfs replacement mfs replacement terms error rate bias variance dataset 
method knn ecoc rs rs means randomly selected features ways fix number features selected bit ecoc code 
experiments choose parameter half features 
performance knn ecoc fs fs means selection method aha bankert depends largely feature selection method embedded 
experiment choose wrapper feature selection method section 
simple sequential forward selection method sfs nn classifier induction algorithm sfs 
sfs algorithm selects successive features respect current set features 
error rate knn ecoc fs larger knn ecoc rs 
mean time knn ecoc fs high bias especially high variance especially 
high bias indi cates knn ecoc fs mainly due adoption sfs feature 
experiments nearest neighbor classifier ensembles error rate spectral bands nn mfs replacement mfs replacement knn ecoc fs knn ecoc rs comparison error rate nn mfs replacement mfs replacement knn ecoc fs knn ecoc rs 
results shown respectively bands bands full bands 

error rate spectral bands nn mfs replacement mfs replacement knn ecoc fs knn ecoc rs comparison error rate nn mfs replacement mfs replacement knn ecoc fs knn ecoc rs 
results shown respectively bands bands full bands 


experiments nearest neighbor classifier ensembles bias spectral bands nn mfs replacement mfs replacement knn ecoc fs knn ecoc rs comparison bias nn mfs replacement mfs replacement knn ecoc fs knn ecoc rs 
results shown respectively bands bands full bands 

bias spectral bands nn mfs replacement mfs replacement knn ecoc fs knn ecoc rs comparison bias nn mfs replacement mfs replacement knn ecoc fs knn ecoc rs 
results shown respectively bands bands full bands 


experiments nearest neighbor classifier ensembles variance spectral bands nn mfs replacement mfs replacement knn ecoc fs knn ecoc rs comparison variance nn mfs replacement mfs replacement knn ecoc fs knn ecoc rs 
results shown respectively bands bands bands 

variance spectral bands nn mfs replacement mfs replacement knn ecoc fs knn ecoc rs comparison variance nn mfs replacement mfs replacement knn ecoc fs knn ecoc rs 
results shown respectively bands bands bands 


experiments nearest neighbor classifier ensembles selection method combined high systematic error high variance suggests poor generalization 
feature included sfs feature set mechanism feature set stage features added feature superfluous 
words feature sets nested 
example best features chosen sfs necessarily best 
remedy sfs adopt mechanism delete feature finding irrelevant redundant 
example adopting corresponding floating version sequential forward search sffs knn ecoc fs method certainly perform corre sponding computational cost expected computational cost sffs pretty high see section 
poor generalization knn ecoc fs sfs causes performance uncertain actual performance knn ecoc fs sfs depends individual dataset concerned 
seen challenged simply adopting random sampling original feature space 
course important contribution phenomenon due power ecoc technique 
simple random sampling feature space chance get classification performance improved 
ecoc conjunction random sampling errors occurring chance series distributed bits ecoc codes details ecoc see section smoothly alleviated consequently entire performance improved 
computational cost increases number nearest neighbors chose experiment sake computational efficiency 

experiments nearest neighbor classifier ensembles experiment methods mainly skalak composite nn architecture section evaluated third experiment 
experiment compares nearest neighbor classifier knn fuzzy nearest neighbor classifier fuzzy knn skalak composite nn architecture skalak composite nn architecture ecoc decision tree naive bayes nb 
error rate bias variance plotted figures function number spectral bands starting bands full spectral bands bands bands bands full bands 
nearest neighbor classifier base comparison parameter chosen cross validation 
justification comparing fact level combination algorithm chapter iv id extension id 
major concerning experiment summarized follows naive bayes classifier performs poorly dataset 
rationale choosing include naive bayes researchers noted accuracy simple nb classifier superior real world datasets 
take advantage experiment verify observation holds dataset 
unfortunately conclude plots observation didn show dataset 
see classification performance fuzzy knn slightly performs counter part crisp knn low spectral bands classification performances coincide dimensionality increases 
bias variance fuzzy knn little smaller crisp 
experiments nearest neighbor classifier ensembles knn 
figures concluded skalak composite nn architecture gives best prediction rate 
particular low bias indicates architecture small systematic error low variance suggests better generalization henceforth tendency classification perfor mance depends largely individual dataset concerned 
hypothesized ecoc technique probably improves classification performance skalak composite nn architecture see paragraph section show plots 
fact skalak composite nn architecture ecoc completely coincide 
confirms dietterich arguments representation feature input decision tree id ambiguous id difficulty finding decision tree ecoc able overcome problem 
purpose skalak composite architecture improve performance base classifier example knn rule architecture base classifier 
may hypothesize fuzzy version knn rule base classifier performance improved hypothesis confirmed 
may question architecture improve learning non single learning knn 
interesting open topic 

experiments nearest neighbor classifier ensembles error rate spectral bands nb knn fuzzy knn skalak knn skalak knn ecoc comparison error rate naive bayes nb decision tree knn fuzzy knn skalak knn skalak knn ecoc codes 
results shown respectively bands bands full bands 

error rate spectral bands nb knn fuzzy knn skalak knn skalak knn ecoc comparison error rate naive bayes nb decision tree knn fuzzy knn skalak knn skalak knn ecoc codes 
results shown respectively bands bands full bands 


experiments nearest neighbor classifier ensembles bias spectral bands nb knn fuzzy knn skalak knn skalak knn ecoc comparison bias naive bayes nb decision tree knn fuzzy knn skalak knn skalak knn ecoc codes 
results shown respectively bands bands full bands 

bias spectral bands nb knn fuzzy knn skalak knn skalak knn ecoc comparison bias naive bayes nb decision tree knn fuzzy knn skalak knn skalak knn ecoc codes 
results shown respectively bands bands full bands 


experiments nearest neighbor classifier ensembles variance spectral bands nb knn fuzzy knn skalak knn skalak knn ecoc comparison variance naive bayes nb decision tree knn fuzzy knn skalak knn skalak knn ecoc codes 
results shown respectively bands bands full bands 

bias spectral bands nb knn fuzzy knn skalak knn skalak knn ecoc comparison variance naive bayes nb decision tree knn fuzzy knn skalak knn skalak knn ecoc codes 
results shown respectively bands bands full bands 


summary remarks nearest neighbor classifier ensembles summary remarks nearest neighbor classifier ensembles section chapter iv ensemble methods nearest neighbor classifiers reviewed studied hyperspectral remote sensing data 
presenting complicated classifier classification architecture general study 
example complicated classifiers fuzzy idea plus condensed idea may give rise called condensed fuzzy nearest neighbor classifier 
complicated classification architecture layer fig condensed fuzzy nearest neighbor classifier applied base classifier complementary classifier producing different kinds combinations 
voting cnn voting condensed nearest neigh bor classifiers multiple feature subsets mfs idea applied condensed component nearest neighbor classifiers 
dynamic voting multiple condensed nearest neighbors feature sub set number feature subset dynamic 
advanced sampling technique weighted random sampling applied mfs method improve performance 
simple voting weighted voting ensemble methods weighted majority described section chapter ii applied 
order decrease number design patterns edit algorithm condensing design patterns probably expense performance deterioration 
pointed modifications series research 
nearest neighbor ensembles exist various kinds combinations 
experiments genetic feature selector individual component classifiers combination best studied dissertation 
study orthogonal experimental design oed method orthogonal arrays oas factor analysis suggested find simultaneously changing factors affect classification performance entire classification architecture 
experiments genetic feature selector due problem long running times java code implementation feature selection hyperspectral data turned genetic algorithms library mit 
random shuffle implementa tion borrowed repository free peer reviewed libraries boost 
genetic feature selection tested experiments belongs type aggregation selection parameter variation section scheme seminal siedlecki sklansky 
experiment experiment genetic feature selection technique evaluated 
minimal number obtained features plotted function number generations 
parameters see section detail set classification error 
experimental results displayed class problem classes data points class 
experiments starting different numbers bands conducted 
plot numbers bands respectively bands bands bands dataset full band data 
population size 
crossover rate usually assumes high values close integrated development environment ide open edition borland 
versatile development tool powerful borland component library cross platform 

experiments genetic feature selector number obtained features versus number generations genetic feature selection bands respectively 
equal mutation rate typically small 
crossover rate high allow produce offspring optimal parents 
crossover rate disrupt solution 
experiment crossover mutation rates set respectively 
classification fuzzy nn algorithm applied 
plot observe number obtained features decreases number generations 
classification errors constant curves 
reduction fast generations convergence slow 
experiment somewhat controversial reports performance comparison genetic selection method floating search method literature 
ferri points ga sequential floating forward search sffs comparable moderate size performance ga 
experiments genetic feature selector deteriorates dimensionality increases 
jain zongker report ga approach tendency premature convergence 
contrast reports kudo sklansky suggests sffs suitable problems small medium dimensionality 
high dimensionality tend favor ga approach 
purpose experiment fold undertake study uses learning scheme induction algorithm selec tion process genetic algorithms demonstrate superiority non learning counterpart genetic feature selection 
ii pragmatic view genetic search sequential floating forward search hyperspectral remote sensing data 
due costly time complexity backward version sequential floating search see footnote chapter choose forward version sequential floating search compare 
genetic algorithms difficult task set param eters gas 
unfortunately unified guidance 
sake simplicity experiment experience set population size generation size crossover probability mutation probability 
comparison scheme done follows 
run genetic search get number selected features 
set sequential floating forward search get number selected features genetic search 
comparison done error rate number evaluations respectively 
order demonstrate superiority learning algorithm hybridized feature selection algorithms genetic search sequential float ing forward search learning cnn ecoc corre 
experiments genetic feature selector selected bands spectral bands ga cnn ga cnn ecoc number obtained spectral bands running ga cnn ga cnn ecoc respectively 
results shown respectively bands bands full bands 
sponding non counterpart cnn algorithm respectively 
shows numbers selected features run ga cnn ga cnn ecoc respectively 
observation number obtained features ga cnn ecoc generally speaking smaller ga cnn 
suggests potential advantage learning smaller number selected features simplified model may result 
classification performance different search methods plotted re spectively function numbers spectral bands 
observe systematic classification performance improvement cnn ecoc cnn conjunction genetic algorithms 
low spectral bands difference classification performance small 
experiments genetic feature selector dimensionality exceeds difference tends grow 
demonstrates superiority learning non learning genetic algorithms 
suggest specific classifier genetic algorithms pre processing extend choice classifiers account corresponding counterpart classification algorithms 
glance may see classification performance sffs cnn ecoc slightly dominates sffs cnn spectral bands difference significant difference 
difference classification performance genetic search sffs spectral bands cnn ecoc gas reduces difference 
example dimensionality tends higher difference smaller 
remedy optimal parameter settings ga addressed dissertation 
didn compare directly execution time complete search task genetic search sequential floating search compared number intermediate subset evaluations execution time usually varies implementation 
example highly optimized library people write implementation code sequential floating search optimized manner 
higher number subset evaluations need running time highly optimized poorly optimized implementation sequential floating search smaller number subset evaluations directly comparing exe cution time fair 
number intermediate subset evaluations reflects 
summary remarks genetic feature selector classification performance spectral bands ga cnn ga cnn ecoc sffs cnn sffs cnn ecoc classification performance ga cnn ga cnn ecoc sfs cnn sfs cnn ecoc 
results shown respectively bands bands full bands 
time complexity larger number evaluations higher time complexity search algorithm 
see number evaluations genetic search grows slowly dimensionality sequential floating search grows quickly dimensionality 
time complexity float ing search times genetic search case current parameter settings gas 
dimensionality exceeds num ber evaluations sffs cnn ecoc larger sffs cnn 
summary remarks genetic feature selector section chapter vi learning genetic algorithms perform feature selection initiated applied high 
summary remarks genetic feature selector number evaluations ga cnn ga cnn ecoc sffs cnn sffs cnn ecoc spectral bands comparison number evaluations ga cnn ga cnn ecoc sfs cnn sfs cnn ecoc 
results shown respectively bands bands full bands 
dimensional remote sensing data 
demonstrated reduces number selected features improves classification performance 
comparing genetic feature selection sequential floating forward selection conclude sequential floating forward feature selection suited small medium sized databases larger databases definitely take account gas terms classification performance especially running time 
due inherent parallel nature genetic algorithms parallel genetic algorithms ready parallel version floating search time 
didn experiments parallel gas believe improve computation efficiency especially dimensionality increases 
terms time complexity floating search method longer 
summary remarks genetic feature selector rival genetic search 
low dimensionality merit floating search straightforward needs small number subset evaluations finish search 
point view experimental results support claim kudo sklansky ga approach suited time critical tasks 
order rival parallel ga approach extent floating forward search method needs parallel version especially case high dimensional data 
practical advantage genetic search floating forward search robust sense parameters easily tunable order meet computational needs 
chapter viii dissertation important aspects pattern recognition studied hyperspectral remote sensing data classifier ensemble problems particular nearest neighbor classifier ensembles feature selection problems particular genetic feature selectors 
fact research feature selection dates back demonstrates topic traditional fundamental problem sug remains despite decades research efforts numerous researchers challenging task 
idea feature selection old application hyperspectral remote sensing data band selection new mainly due advances sensor technology 
tion feature selection hyperspectral data hope alleviates called hughes phenomenon classification performance hyper spectral data improves limited point additional features added deteriorates 
pointed chapter ii idea aggregating opinions committee experts improve accuracy new real 
world applications reflect direct connections daily life 
idea applying aggregation classification algorithms new 
current research classifier ensembles focuses general principles methodologies dissertation specific nearest neighbor classifier ensembles focused 
summary roadmap research conducted dissertation studied classifier ensembles general 
method called cnn ecoc main contribution takes advantage dy namic nature condensed nearest neighbors cnn algorithm conjunction technique error correcting output codes ecoc 
ecoc tech nique distributed scheme decomposes multi class classification problem series distributed class problems 
adopting ecoc cnn demonstrated method improves classification performance time decreases storage requirements 
variant called knn ecoc rs takes advantage dynamic nature randomly selected features conjunction ecoc suggested product 
second main contribution initiated advocated idea en learning conjunction genetic algorithms perform feature selection 
example demonstrated superiority proposed learning cnn ecoc genetic algorithms non counterpart cnn 


part thesis deals feature selection problem remote sensing data idea band selection criticism leading scientist remote sensing community arguing pick subset bands completely ignore rest may ignore useful diagnostic 
research needed investigate feature extraction useful feature selection hyperspectral data 

evolutionary feature selector multi layer neural network feature se uses scheme eda estimation distribution algorithm search engine multi layer neural network models representing probability distribution set candidate solutions suggested topic research 

population crossover function optimization investigated theoretically 
theoretical study population uses crossover mutation may considered interesting subject 
samenvatting dutch summary 
translated appear final print version 
list publications yu steve de backer paul genetic feature selection com fuzzy knn hyperspectral satellite imagery 
proc 
ieee ternational geoscience remote sensing symposium july honolulu hawaii vol 
pp 

yu steve de backer paul 
genetic feature selection com composite fuzzy nearest neighbor classifiers high dimensional remote sensing data 
proc 
ieee conference systems man cybernetics october nashville tn usa pp 

yu paul 
fuzzy markov chain approach feature selection high dimensional remote sensing data proc 
ieee interna tional geoscience remote sensing symposium july sydney australia 
vol 
pp 

yu paul 
feature selection high dimensional remote sensing data maximum entropy optimization proc 
ieee inter national geoscience remote sensing symposium july sydney australia 
vol 
pp 

yu steven de backer paul 
genetic feature selection combined wth composite fuzzy nearest neighbor classifiers hyperspectral 
satellite imagery 
pattern recognition letters 
yu paul 
combining nearest neighbor classifiers empirical study hyperspectral remote sensing data 
submitted pattern recognition letters 
bibliography bibliography bibliography ftp ftp cs orst edu put tgd programs ecoc codes tar gz 
ftp ftp fi cs report 
dynamo ecn purdue edu documentation html 
dynamo ecn purdue edu publications html 
dcs napier ac uk 
european network excellence evolutionary computing 
home net feature selection html 
online bibliography feature selection 
iris usc edu vision notes bibliography contents html 
annotated computer vision bibliography check multiple classifiers combining classifiers combinations 
iris usc edu vision notes bibliography contents html 
annotated computer vision bibliography check feature selection pattern recognition clustering 
lancet mit edu ga 
genetic algorithm library 
jpl nasa gov html 
main homepage jpl nasa 
surf de uu net encore www 
hitch guide evolutionary computation 
tilde hoschek home cern ch hoschek colt index htm 
colt java package high performance computing 
www fp mcs anl gov otc guide index html 
www aaai org 
aaai american association artificial intelligence 
www aic nrl navy mil 
genetic algorithm archive 
www boosting org 
online resources boosting research 
www boost org 
boost repository free peer reviewed libraries 
www borland com 
www cs waikato ac nz ml weka 
weka java machine learning package 
www eclipse org 
www genetic programming org 
genetic programming online 
www inf fu berlin de papers nn node html 
bibliography www org 
international society genetic evolutionary computation 
www org 
evolutionary optimization international journal internet 
www kernel machines org 
primary resources kernel machines related research 
www mx 
repository evolutionary multiobjective optimization mirror sites www org emo delta cs mx 
www org 
machine learning network online information service 
www ncsa uiuc edu apps cmp rng www rng html 
random numbers web 
www optimization online org 
optimization online eprint site optimization community 
www ph tn tudelft nl books html 
list research monographs pattern recognition 
www public asu edu html 
www recursive partitioning com 
online bibliography recursive partitioning 
www com envi 
envi environment visualizing images remote sensing software research systems www com see info html 
sun com software 
www trw com 
www vtt fi tte research tte tte virtual 
remote www virtual library 
pattern recognition group delft university technology 
personal communication 
hunt 
hyperspectral image compression predictive trellis coded quantization 
ieee ip apr 
aha 
comparative evaluation sequential feature selection algorithms 
proc 
th international workshop artificial intelligence statistics pages menlo park ca 
aaai 
david aha 
lazy learning 
kluwer academic publishers dordrecht jun 
aha bankert 
cloud classification error correcting output codes 
technical report aic 
aha kibler albert 
instance learning algorithms 
machine learning 
bibliography almuallim dietterich 
learning irrelevant features 
proc 
th national conference artificial intelligence pages san jose ca 
aaai press 
alpaydin 
neural models incremental supervised unsupervised learning 
phd thesis department informatique ecole rale de lausanne lausanne switzerland 

alpaydin 
voting multiple condensed nearest neighbors 
artificial intelligence review 
back 
evolutionary algorithms theory practice 
oxford university press 
back 
schwefel 
survey evolution strategies 
belew booker editors proc 
th international conference genetic algorithms pages 
san mateo ca morgan kaufmann 
thomas back editor 
handbook evolutionary computation 
iop publishing oxford university press 
dennis laura navarro 
combining heterogeneous sets classifiers theoretical experimental comparison methods 
allen barker 
selection distance metrics feature subsets nearest neighbor classifiers 
phd thesis dept computer science university virginia may 
bauer kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning 

improving accuracy artificial neural network multiple differently trained networks 
neural computation 
bay 
nearest neighbor classification multiple feature subsets 
intelligent data analysis 
bellman 
adaptive control processes 
princeton university press 
bengio bengio 
curse dimensionality joint distributions neural networks 
ieee trans 
neural networks may 
bennett demiriz maclin 
exploiting unlabeled data ensemble methods 

published proc 
kdd www rpi edu assemble ps gz 
bezdek 
pattern recognition fuzzy objective function algorithms 
plenum press new york 
james bezdek 
fuzzy logic neural network handbook chapter 
mcgraw hill companies 
blum langley 
selection relevant features examples machine learning 
artificial intelligence 
blum rivest 
training node neural network np complete 
neural networks 
boyce weischedel 
optimal subset selection 
springer verlag berlin germany 
bibliography bradley fayyad mangasarian 
mathematical programming data mining formulations challenges 
informs journal computing 
breiman 
bagging predictors 
machine learning 
breiman 
bias variance classifiers 
technical report statistics department university california berkeley 
breiman 
stacked regressions 
machine learning 
breiman friedman olshen stone 
classification regression trees 
belmont ca 
bremermann 
global properties evolution processes 
editor natural automata useful simulations pages 

brodley 
dynamic automatic model selection 
technical report department computer science university massachusetts amherst ma 
theiler perkins harvey szymanski 
genetic programming approach extracting features remotely sensed imagery 
proc 
fusion 
traina jr traina wu christos faloutsos 
fast feature selection fractal dimension 
xv brazilian symposium databases 

dendrite method cluster analysis 
communications statistics 
cantu paz 
efficient accurate parallel genetic algorithms 
volume genetic algorithms evolutionary computation 
kluwer academic publishers 
cardie 
decision trees improve case learning 
proc 
th international conference machine learning pages amherst ma 
morgan kaufmann 
cestnik 
estimating probabilities crucial task machine learning 
proc 
european conference artificial intelligence pages stockholm sweden 
sung cha srihari 
fast nearest neighbor search algorithm filtration 
pattern recognition feb 
philip chan salvatore stolfo 
comparative evaluation voting meta learning partitioned data 
proc 
th international conference machine learning pages 
ke chen lan wang chi 
methods combining multiple classifiers different features applications text independent speaker identification 
international journal pattern recognition artificial intelligence 
chen han yu 
data mining overview database perspective 
ieee trans 
knowledge data engineering 

human expert level performance scientific image analysis task system combined artificial neural networks 
chan editor working notes aaai workshop integrating multiple learned models pages 

bibliography chung micheli 
classifiers overview 
micheli editor supervised unsupervised pattern recognition feature extraction computational intelligence industrial electronics series pages 
crc press boca raton fl 

constructing predictions trees data approach 
computational aspects model choice pages 
physica verlag 

chang hogg mckinney 
recursive partition versatile method exploratory data analysis biostatistics 
editors biostatistics pages 
reidel dordrecht 
carlos coello coello 
comprehensive survey evolutionary multiobjective optimization techniques 
knowledge information systems 
carlos coello coello 
updated survey ga multiobjective optimization techniques 
acm computing surveys 
david alice smith david tate 
adaptive penalty methods genetic optimization constrained combinatorial problems 
informs journal computing 
marquis condorcet 
sur les elections par scrutiny 
de des sciences pages 
cost salzberg 
weighted nearest neighbor algorithm learning symbolic features 
machine learning 
cover hart 
nearest neighbor pattern classification 
ieee trans 
information theory 
joao manuel da gama 
combining classification algorithms 
phd thesis universidade porto 
dasarathy 
nearest neighbor nn norms nn pattern classification techniques 
ieee computer society press lost alamitos ca 
dash liu 
feature selection classification 
intelligent data analysis international journal 

nearest neighbor classification rule dempster shafer theory 
ieee trans 
systems man cybernetics 
raghavan sever 
feature selection effective classifiers 
journal asis 
devijver kittler 
pattern recognition statistical approach 
prentice hall london 
luc devroye szl gy rfi bor lugosi 
probabilistic theory pattern recognition 
applications mathematics 
springer verlag 
dietterich 
machine learning research current directions 
ai magazine 
available www aaai org html machine html 
dietterich 
ensemble methods machine learning 
proc 
mcs lecture notes computer science pages 
new york springer verlag 
dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning 
bibliography dietterich bakiri 
solving multiclass learning problems error correcting output codes 
journal artificial intelligence research 
dimitrios andreas 
multi svm classification system 
proc 
nd international workshop multiple classifier systems mcs cambridge uk 

evaluation feature selection methods application computer security 
technical report cse department computer science engineering university 
pier luigi giovanni poggi arturo 
compression multispectral images dimensional algorithm 
ieee trans 
geoscience remote sensing jan 
duda hart 
pattern classification scene analysis 
new york wiley 
petersen de ridder 
image processing neural networks review 
pattern recognition 
appear 
christos andrew hunter john macintyre chris cox 
selecting features modelling multiobjective genetic algorithms 
proc 
icann th international conference artificial neural networks volume pages edinburgh uk sep 
everitt 
cluster analysis 
new york press 
ferri kittler 
comparative study techniques large scale feature selection 
gelsema kanal editors pattern recognition practice volume iv pages 
elsevier science 
fix hodges jr discriminatory analysis nonparametric discrimination consistency properties 
technical report usaf school aviation medicine 
project report 
fogel 
system identification simulated evolution machine learning approach modelling 
needham ma press 
carlos fonseca peter fleming 
multiobjective optimization multiple constraint handling evolutionary algorithms part unified formulation 
ieee trans 
systems man cybernetics part systems humans 
fonseca fleming 
overview evolutionary algorithms multiobjective optimization 
evolutionary computation 
fox 
applied regression analysis 
sage publications 
yoav freund robert schapire 
experiments new boosting algorithm 
proc 
th international conference machine learning pages 
friedman 
exploratory projection pursuit 
journal american statistical association 
fukunaga 
optimal global nearest neighbor metric 
ieee trans 
pattern analysis machine intelligence 
fukunaga 
statistical pattern recognition 
academic press san diego california 
bibliography fukunaga narendra 
branch bound algorithm computing nearest neighbors 
ieee trans 
computers 
mohamed keller 
fusion handwritten word classifiers 
pattern recognition letters 
geman bienenstock doursat 
neural networks bias variance dilemma 
neural computation 
joydeep ghosh 
systems back 
fabio roli kittler editors proc 
rd international workshop multiple classifier systems mcs volume lecture notes computer science pages italy jun 
springer 
goldberg 
genetic algorithms search optimization machine learning 
addison wesley reading ma 
guyon 
welcome problem feature variable selection 
nips workshop feature variable selection 
mark hall 
correlation feature selection machine learning 
phd thesis department computer science university waikato hamilton new zealand apr 
hansen salamon 
neural network ensembles 
ieee trans 
pattern analysis machine intelligence 
hart 
condensed nearest neighbor rule 
ieee trans 
information theory 
harvey porter perkins theiler young szymanski bloch 
parallel evolution image tools multispectral imagery 
proc 
imaging spectrometry iv volume spie pages 
intl 
soc 
opt 
eng 
kazuo takahashi 
new edited nearest neighbor rule pattern recognition problem 
pattern recognition mar 
heath kasif 
committees decision trees 

herrera lozano 
fuzzy genetic algorithms issues models 
technical report computer science artificial intelligence university granada granada spain 
hertz krogh palmer 
theory neural computation 
addison wesley redwood city ca 
alexander hinneburg aggarwal daniel keim 
nearest neighbor high dimensional spaces 
proc 
th international conference large data bases vldb cairo 
downloadable site www acm org sigmod 
comprehensive site special interest group management data sig mod acm 
ho liu liu 
design optimal nearest neighbor classifier intelligent genetic algorithm 
pattern recognition letters 
ho hull 
decision combination multiple classifier systems 
ieee trans 
pattern analysis machine intelligence 
bibliography holland 
adaptation natural artificial systems 
university michigan press ann arbor 
holland 
adaptation natural artificial systems introductory analysis applications biology control artificial 
mit press cambridge ma 
nd edition 
horn nafpliotis goldberg 
niched pareto genetic algorithm multiobjective optimization 
proc 
st ieee conference evolutionary computation pages 
hughes 
mean accuracy statistical pattern recognizers 
ieee trans 
information theory 
neuro fuzzy id 
fuzzy sets systems 
jack sklansky 
optimum feature selection zero integer programming 
ieee trans 
systems man cybernetics smc 
piotr indyk 
high dimensional computational geometry 
phd thesis dept computer science stanford university 
piotr indyk 
approximate nearest neighbor algorithms distance product metrics 
proc 
symposium computational geometry 
ishibuchi nakashima 
multi objective pattern feature selection genetic algorithm 
proc 
genetic evolutionary computation conference pages las vegas nevada july 
raj iyer jr efficient boosting algorithm combining preferences 
technical report mit lcs tr 
william 
statistical graphics visualizing multivariate data 
sage publications 
jain zongker 
feature selection evaluation application small sample performance 
ieee trans 
pattern analysis machine intelligence 
jain chandrasekaran dimensionality sample size consideration pattern recognition practice 
kanal editors handbook statistics volume ii pages 
north holland amsterdam netherlands 
anil jain robert duin mao 
statistical pattern recognition review 
ieee trans 
pattern analysis machine intelligence 
james hastie 
generalizations bias variance decomposition prediction error 
stat stanford edu gareth 

fuzzy decision trees issues methods 
ieee trans 
systems man cybernetics 
thomas jansen 
utility populations 
technical report ci department computer science university dortmund 
film fuzzy inductive learning method automated knowledge acquisition 
decision support systems 
david landgrebe 
partially supervised classification weighted unsupervised clustering 
ieee trans 
geoscience remote sensing march 
bibliography george john 
enhancements data mining process 
phd thesis department computer science stanford university mar 
john kohavi pfleger 
irrelevant features subset selection problem 
proceedings eleventh international conference machine learning pages new brunswick nj 
morgan kaufmann 

learning scheme fuzzy nn rule 
pattern recognition letters july 
jussi 
wavelet compression multispectral images 
proc 
iasted international conference computer graphics imaging 
kargupta huang krishnamoorthy johnson 
distributed clustering collective principal component analysis 
knowledge information systems journal 
yi ke guo 
distributed classification knowledge probing new framework distributed data mining 
kargupta philip chan editors advances distributed parallel knowledge discovery 
mit aaai press sep 
keller gray jr givens 
fuzzy nearest neighbor algorithm 
ieee trans 
systems man cybernetics 
kelly davis 
genetic algorithm nearest neighbors classification algorithm 
proc 
th international conference genetic algorithms applications icga pages 
kira rendell 
practical approach feature selection 
proc 
th international conference machine learning pages aberdeen scotland 
morgan kaufmann 
kittler 
framework classifier fusion needed 
pierre devijver award lecture 
available www ph tn tudelft nl organisation tc pda pda html 
kittler 
feature set search algorithm 
chen editor pattern recognition signal processing pages 
aan den netherlands 
kittler 
feature selection extraction 
young king sun fu editors handbook pattern recognition image processing pages 
academic press 
kittler duin matas 
combining classifiers 
ieee trans 
pattern analysis machine intelligence 
kohavi john 
wrappers feature subset selection 
artificial intelligence 
kohavi wolpert 
bias plus variance decomposition zero loss functions 
proc 
th international conference machine learning 
koller sahami 
optimal feature selection 
proc 
th international conference machine learning pages bari italy 
morgan kaufmann 
kong dietterich 
error correcting output coding corrects bias variance 
proc 
th national conference artificial intelligence 
kononenko 
estimating attributes analysis extensions relief 
proc 
th european conference machine learning 
bibliography john koza 
genetic programming 
james williams allen kent editors encyclopedia computer science technology volume pages 
marcel dekker 
krogh vedelsby 
neural network ensembles cross validation active learning 
tesauro editors advances neural information processing systems volume pages 
cambridge ma mit press 
kruse boardman shapiro goetz 
spectral image processing system sips interactive visualization analysis imaging spectrometer data 
remote sensing environment 
pfurtscheller 
discovering patterns eeg signals comparative study methods 
proc 
th european conference machine learning pages heidelberg 
springer verlag 
kudo sklansky 
comparison classifier specific feature selection algorithm 
ferri amin editors proc 
joint iapr international workshops spr volume advances pattern recognition lecture notes computer science pages spain aug sep 
springer 
kudo jack sklansky 
comparison algorithms select features pattern classifiers 
pattern recognition 
kuncheva jain 
nearest neighbor classifier simultaneous editing feature selection 
pattern recognition letters 
kuncheva bezdek duin 
decision template multiple classifier fusion experimental comparison 
pattern recognition 
kuncheva christopher whitaker 
diversity variants boosting aggressive conservative inverse 
proc 
mcs lecture notes computer science 
springer verlag 
appear 
lam suen 
optimal combination pattern classifiers 
pattern recognition letters 
david 
personal communication 
landgrebe 
information extraction principles methods multispectral hyperspectral data 
chen editor information processing remote sensing 
world scientific usa 
langley sage 
induction selective bayesian classifiers 
proc 
th conference uncertainty artificial intelligence pages seattle wa 
morgan kaufmann 
langley sage 
oblivious decision trees cases 
working notes aaai workshop case reasoning pages seattle wa 
aaai press 
lao wong 
hyperspectral imagery market forecast 
technical report economic market analysis center systems engineering division engineering technology group december 
available www aero org pk vl pdf 
bibliography lim sei yin loh yu shan shih 
comparison prediction accuracy complexity training time old new classification algorithms 
machine learning 
preprint available www recursive partitioning com mach pdf appendix containing complete tables error rates ranks training times www recursive partitioning com appendix pdf 
littlestone 
learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
littlestone warmuth 
weighted majority algorithm 
technical report ucsc crl department computer engineering information sciences university california santa cruz 
littlestone warmuth 
weighted majority algorithm 
information computation 
liu setiono 
probabilistic approach feature selection filter solution 
proc 
th international conference machine learning pages bari italy 
morgan kaufmann 
huan liu hiroshi 
feature selection knowledge discovery data mining 
kluwer academic publishers 
isbn 
nicholas short sr rst gsfc nasa gov front html 
prasanna lee tan huan liu 
sampling databases trees 
cikm pages 

john reif 
multispectral image compression algorithms 
james storer martin cohn editors dcc data compression conference pages apr 
matsumoto nishimura 
mersenne twister dimensionally uniform pseudo random number generator 
acm trans 
modeling computer simulation jan 

joint classification compression hyperspectral images 
proc 
international geoscience remote sensing symposium volume pages piscataway nj 
ieee service center 
michalewicz 
genetic algorithms data structure evolutionary programs 
springer verlag 
miller 
subset selection regression 
chapman hall 
miller rao rose gersho 
global optimization technique statistical classifier design 
ieee trans 
signal processing 
mitchell 
genetic algorithms 
cambridge ma mit press 

comparison techniques choosing subsets pattern recognition 
ieee trans 
computers 
moore lee 
efficient algorithms minimizing cross validation error 
proc 
th international conference machine learning pages san francisco ca 
morgan kaufmann 
mori 
taguchi techniques image pattern developing technology 
new jersey prentice hall 
bibliography muhlenbein 
recombination genes estimation distributions 
binary parameters 
voigt editor lecture notes computer science parallel problem solving nature ppsn iv pages 

narendra fukunaga 
branch bound algorithm feature subset selection 
ieee trans 
computer 

collective decision making 
cambridge university press 
olken rotem 
random sampling databases survey 
statistics computing mar 
opitz maclin 
popular ensemble methods empirical study 
journal artificial intelligence research 
opitz shavlik 
generating accurate diverse members neural network ensemble classifiers 
tesauro editors advances neural information processing system volume pages 
cambridge ma mit press 
munro doyle 
improving diagnosis resampling techniques 
touretzky mozer editors advances neural information processing volume pages 
mit press cambridge ma 
terry payne 
dimension reduction representation nearest neighbor learning 
phd thesis department computing science university aberdeen 
terry payne edwards 
survey feature selection 
draft copy 
carlos andres pena reyes 
coevolutionary fuzzy modeling 
phd thesis swiss federal institute technology epfl lausanne 
potter 
design analysis computational model cooperative coevolution 
phd thesis george mason university 
kittler 
floating search methods feature selection 
pattern recognition letters 
punch goodman min pei lai chia shun 
research feature selection classification genetic algorithms 
proc 
international conference genetic algorithms applications icga pages 
fleming 
multi objective genetic algorithm applied benchmark problems analysis 
technical report department automatic control systems engineering university sheffield sheffield jd aug 

qian williams 
data compression hyperspectral imagery vector quantization multiple codebooks 
proc 

quinlan 
induction decision trees 
machine learning 
quinlan 
programs machine learning 
morgan kaufmann san ca 
quinlan 
bagging boosting 
proc 
th american association artificial intelligence 
aaai press 
bibliography paliwal 
fast nearest neighbor search algorithms approximation elimination search 
pattern recognition sep 
rauss daida 
classification spectral imagery genetic programming 
whitley editor proc 
gecco pages 
morgan kaufmann 
punch goodman kuhn jain 
dimensionality reduction genetic algorithms 
ieee trans 
evolutionary computation 
michael william punch erik goodman paul leslie kuhn 
simultaneous feature selection masking genetic algorithm 
technical report algorithms research applications group department computer science michigan state university east lansing mi 
ricci aha 
extending local learners error correcting output codes 
technical report aic navy center applied research artificial intelligence 
richardson palmer liepins 
guidelines genetic algorithms penalty functions 
proc 
rd international conference genetic algorithms pages 
morgan kauffman 
roger 
lossless compression imaging data 
technical report department electrical engineering australian defence force academy canberra australia 
report may obtained anonymous ftp evans ee oz au directory pub reports 

combining results neural network classifiers 
neural networks 
thomas philip xin yao 
constrained evolutionary optimization penalty function approach 
yao editors evolutionary optimization chapter pages 
kluwer academic publishers usa 
ryan arnold 
lossless compression images vector quantization 
ieee trans 
geoscience remote sensing may 
salton mcgill 
modern information retrieval 
new york mcgraw hill 
hanan samet 
similarity searching indexing nearest neighbor finding dimensionality reduction embedding methods applications multimedia databases 
icpr 
tutorial 
michael walter 
www apex esa org 
schapire 
strength weak learnability 
machine learning 
schlesinger 
lectures statistical structural pattern recognition 
kluwer academic publishers 
schlimmer 
efficiently inducing determinations complete efficient search algorithm uses optimal pruning 
proc 
th international conference machine learning pages amherst ma 
morgan kaufmann 
bibliography stephen scott 
feature vector selection 
lecture note pattern 
department computer science engineering university nebraska lincoln nebraska 
available www cse unl edu 
rudy setiono 
neural network feature selector 
ieee trans 
neural networks 
shafer 
mathematical theory evidence 
princeton university press princeton nj 
amanda sharkey editor 
combining artificial neural nets ensemble modular multi net systems perspectives neural computing 
springer verlag apr 
amanda sharkey 
types multinet system 
proc 
mcs 
appear 
siedlecki 
automatic feature selection 
international journal pattern recognition artificial intelligence 
siedlecki sklansky 
note genetic algorithms large scale feature selection 
pattern recognition letters 
siedlecki sklansky 
constrained genetic optimization dynamic reward penalty balancing pattern recognition 
chen pau wang editors handbook pattern recognition computer vision pages 
world scientific publishing pte 
box road singapore 
singh provan 
comparison induction algorithms selective bayesian classifiers 
proc 
th international conference machine learning pages lake tahoe ca 
morgan kaufmann 
singh provan 
efficient learning selective bayesian network classifiers 
proc 
th international conference machine learning pages bari italy 
morgan kaufmann 
singh john 
nearest neighbor classifiers natural scene analysis 
pattern recognition aug 
skalak 
prototype selection composite nearest neighbor classifiers 
phd thesis university massachusetts amherst ma 
alice smith david 
penalty function 
thomas david michalewicz editors handbook evolutionary computation 
joint publication oxford university press institute physics publishing 

adaptive floating search methods feature selection 
pattern recognition letters 
stanfill 
memory reasoning 
communications acm 
stearns 
selecting features pattern classifiers 
proceedings rd international conference pattern recognition pages ca 
tamaki kita kobayashi 
multi objective genetic algorithms review 
fukuda editors proc 
international conference evolutionary computation pages nagoya japan 
bibliography tan lee 
evolutionary algorithms multi objective optimizations performance assessments comparisons 
artificial intelligence review jun 
stephen tate 
band ordering lossless compression multispectral images 
proc 
data compression conference snowbird utah pages 
theodoridis 
pattern recognition 
academic press san diego ca jan 

review bayesian neural networks application near infrared spectroscopy 
technical report danish meat research institute 
tibshirani 
bias variance prediction error classification rules 
technical report department statistics university toronto 
ting witten 
stacked generalization 
proc 
international joint conference artificial intelligence 
memon bouman 
multispectral image coding 
alan bovik editor image video processing handbook 
academic press 
appear 
tumer joydeep ghosh 
classifier combining analytical results implications 
national conference artificial intelligence 
portland august 
tumer joydeep ghosh 
error correlation error reduction ensemble classifiers 
connection science december 
special issue combining artificial neural networks ensemble approaches 
tumer joydeep ghosh 
theoretical foundations linear order statistics combiners neural pattern classifiers 
technical report tr 
fuzzy decision trees fuzzy id application diagnosis systems 
proc 
ieee international conference fuzzy systems pages jun 
utgoff 
perception trees case study hybrid concept representations 
connection science 
valiant 
theory learnable 
communications acm 
vapnik 
nature statistical learning theory 
john wiley new york 
david van veldhuizen 
multiobjective evolutionary algorithms classifications analyses new innovations 
phd thesis air force institute technology jun 
david van veldhuizen gary lamont 
multiobjective evolutionary algorithm research history analysis 
technical report tr air force institute technology wright paterson afb oct 
david van veldhuizen gary lamont 
multiobjective evolutionary algorithms analyzing state art 
evolutionary computation 
ricardo youssef 
perspective view survey meta learning 
artificial intelligence review 

multi criteria optimization fuzzy genetic algorithm 
journal heuristic 
wilks 
mathematical statistics 
wiley new york 
bibliography wilson martinez 
improved heterogeneous distance functions 
journal artificial intelligence research 
journal artificial intelligence reseach jair online journal freely accessible www jair org 
wolpert 
stacked generalization 
technical report la ur complex systems group theoretical division center non linear studies ms lanl los alamos nm 
woods kegelmeyer bowyer 
combination multiple classifiers local accuracy estimates 
ieee trans 
pattern analysis machine intelligence 
wu govindaraju 
improved nearest neighbor classification 
pattern recognition 
xu suen 
methods combining multiple classifiers application handwriting recognition 
ieee trans 
systems man cybernetics 
yang honavar 
feature subset selection genetic algorithm 
proc 
second conference genetic programming 
mary yang 
hyperspectral image compression client sever software 
available www com proposal pdf 
yu steven de backer paul 
genetic feature selection combined composite fuzzy nearest neighbor classifiers hyperspectral satellite imagery 
pattern recognition letters 
yu paul 
combining nearest neighbor classifiers empirical study hyperspectral remote sensing data 
pattern recognition letters 
submitted 
zhang mesirov waltz 
hybrid system protein secondary structure prediction 
journal molecular biology 
ye zhang desai 
hyperspectral image compression adaptive recursive prediction jpeg 
pattern recognition nov 
zheng 
naive bayesian classifier committees 
proc 
ecml pages 
springer verlag 
zitzler 
evolutionary algorithms multiobjective optimization methods applications 
phd thesis swiss federal institute technology zurich 
tik nr 

eckart zitzler kalyanmoy deb lothar thiele 
comparison multiobjective evolutionary algorithms empirical results 
technical report tik report computer engineering networks laboratory tik swiss federal institute technology zurich dec 
revised version 

