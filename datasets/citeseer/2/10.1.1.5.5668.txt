head driven statistical models natural language parsing michael collins mit computer science artificial intelligence laboratory article describes statistical models natural language parsing 
models extend methods probabilistic context free grammars lexicalized grammars leading approaches parse tree represented sequence decisions corresponding head centered top derivation tree 
independence assumptions lead parameters encode bar schema subcategorization ordering complements placement adjuncts bigram lexical dependencies wh movement preferences close attachment 
preferences expressed probabilities conditioned lexical heads 
models evaluated penn wall street journal treebank showing accuracy competitive models literature 
gain better understanding models give results different constituent types breakdown precision recall results recovering various types dependencies 
analyze various characteristics models experiments parsing accuracy collecting frequencies various structures treebank linguistically motivated examples 
compare models applied parsing treebank aiming give explanation difference performance various models 

ambiguity central problem natural language parsing 
combinatorial effects mean relatively short sentences receive considerable number parses wide coverage grammar 
statistical parsing approaches tackle ambiguity problem assigning probability parse tree ranking competing trees order plausibility 
statistical models probability candidate tree calculated product terms term corresponding substructure tree 
choice parameterization essentially choice represent parse trees 
critical questions regarding parameterization parsing approach 
linguistic objects context free rules parse moves model parameters associated 
words features discriminate alternative parse trees 

choice instantiated sound probabilistic model 
article explore issues framework generative models precisely history models originally introduced parsing black mit computer science artificial intelligence laboratory massachusetts institute technology technology square cambridge ma 
mail ai mit edu 
association computational linguistics computational linguistics volume number 

history model parse tree represented sequence decisions decisions derivation tree 
decision associated probability product probabilities defines probability distribution possible derivations 
describe parsing models approach 
models originally introduced collins current article gives considerably detail models discusses greater depth 
model show approach extends methods probabilistic context free grammars pcfgs lexicalized grammars 
importantly model parameters corresponding dependencies pairs headwords 
show incorporate distance measure models generalizing model history approach 
distance measure allows model learn preference close attachment right branching structures 
model extend parser complement adjunct distinction important applications output parser 
model extended parameters corresponding directly probability distributions subcategorization frames headwords 
new parameters lead improvement accuracy 
model give probabilistic treatment wh movement loosely analysis wh movement generalized phrase structure grammar gpsg gazdar 
output parser enhanced show trace wh movement cases 
parameters model interesting correspond directly probability propagating gpsg style slash features parse trees potentially allowing model learn island constraints 
models parse tree represented sequence decisions corresponding head centered top derivation tree 
independence assumptions follow naturally leading parameters encode bar schema subcategorization ordering complements placement adjuncts lexical dependencies wh movement preferences close attachment 
preferences expressed probabilities conditioned lexical heads 
reason refer models head driven statistical models 
describe evaluation models penn wall street journal treebank marcus santorini marcinkiewicz 
model achieves constituent precision recall sentences words length section treebank models give improvements constituent precision constituent recall 
results competitive models applied parsing penn treebank 
models produce trees information wh movement subcategorization 
nlp applications need information extract predicate argument structure parse trees 
rest article structured follows 
section gives background material probabilistic context free grammars describes rules lexicalized addition headwords parse trees 
section introduces probabilistic models 
section describes various models 
section discusses issues parameter estimation treatment unknown words parsing algorithm 
section gives results evaluating performance models penn wall street journal treebank marcus santorini marcinkiewicz 
section investigates various aspects models detail 
give article edited version chapters collins 
collins head driven statistical models nl parsing detailed analysis parser performance treebank data including results different constituent types 
give breakdown precision recall results recovering various types dependencies 
intention give better idea strengths weaknesses parsing models 
section goes discuss distance features models implicit assumptions models treebank annotation style way context free rules original treebank broken allowing models generalize producing new rules test data examples 
analyze phenomena experiments parsing accuracy collecting frequencies various structures treebank linguistically motivated examples 
section gives discussion comparing models applied parsing treebank 
aim give explanation differences performance various models 

background probabilistic context free grammars probabilistic context free grammars starting point models article 
reason briefly recap theory pcfgs moving lexicalized case 
hopcroft ullman define context free grammar tuple set nonterminal symbols alphabet distinguished start symbol finite set rules rule form grammar defines set possible strings language defines set possible leftmost derivations grammar 
derivation corresponds tree sentence pair formed grammar 
probabilistic context free grammar simple modification context free grammar rule grammar associated probability 
interpreted conditional probability expanded rule opposed possibilities expanding listed grammar 
probability derivation product terms term corresponding rule application derivation 
probability tree sentence pair derived applications context free rules lhs stands left hand side rhs right hand side pcfg booth thompson specify conditions pcfg fact define distribution possible derivations trees generated underlying grammar 
condition rule probabilities define conditional distributions nonterminal grammar expand 
second technical condition guarantees stochastic process generating trees terminates finite number steps probability 
central problem pcfgs define conditional probability rule grammar 
simple way take counts treebank maximum likelihood estimates count count computational linguistics volume number treebank generated probabilistic context free grammar rules nonterminals model limit training sample size approaches infinity probability distribution implied estimates converge distribution underlying grammar 
model trained model defines sentence tree pair grammar 
output new test sentence tree model arg max arg max arg max parser algorithm searches tree maximizes 
case pcfgs accomplished variant cky algorithm applied weighted grammars providing pcfg converted equivalent pcfg chomsky normal form see example manning sch tze 
model probabilities true distribution generating training test examples returning tree optimal terms minimizing expected error rate number incorrect trees newly drawn test examples 
data generated pcfg training examples maximum likelihood estimates converge true values parsing method optimal 
practice assumptions verified arguably quite strong limitations prevented generative models successfully applied nlp speech tasks 
see collins discussion ways conceptualizing parsing problem 
penn treebank marcus santorini marcinkiewicz source data experiments rules internal tree lhs nonterminal rhs string nonterminals lexical lhs part speech tag rhs word 
see example 
lexicalized pcfgs pcfg lexicalized associating word part speech pos tag nonterminal tree 
see example tree 
pcfg model applied lexicalized rules trees exactly way 
nonterminals simple example np extended include word part speech tag example bought vbd np ibm nnp 
write nonterminal constituent label 
formally changed just vastly increased number nonterminals grammar factor point subtle appears anonymous reviewers pointing unable find proofs property literature pcfgs 
rule probabilities nonterminal appears probability greater zero parse derivations converge underlying values usual properties maximum likelihood estimation multinomial distributions 
assuming underlying pcfg generating training examples meet criteria booth thompson shown convergence rule probabilities implies distribution trees converge underlying pcfg kullback liebler divergence infinity norm taken measure distance distributions 
tommi jaakkola nathan discussions topic 
find lexical heads penn treebank data rules described appendix collins 
rules modified version head table provided david magerman parser described magerman 
collins head driven statistical models nl parsing parse tree list rules contains 
internal rules lexical rules top jj np np vp nn week np jj nn nnp ibm np nnp vbd bought vp vbd np nnp lotus np nnp internal rules top bought vbd bought vbd np week nn np ibm nnp vp bought vbd np week nn jj jj nn week nn np ibm nnp nnp ibm nnp vp bought vbd vbd bought vbd np lotus nnp np lotus nnp nnp lotus nnp lexical rules jj jj nn week nn week nnp ibm nnp ibm vbd bought vbd bought nnp lotus nn lotus lexicalized parse tree list rules contains 
number words vocabulary number part ofspeech tags 
changed formal point view practical consequences expanding number nonterminals quickly apparent attempting define method parameter estimation 
simplest solution maximum likelihood estimate equation example computational linguistics volume number estimating probability associated bought vbd np week nn np ibm nnp vp bought vbd np week nn np ibm nnp vp bought vbd bought vbd count bought vbd np week nn np ibm nnp vp bought vbd count bought vbd addition lexical items statistics estimate sparse count denominator relatively low number outcomes possible lexicalized rhss huge meaning numerator zero 
predicting lexicalized rule go big step 
way overcome sparse data problems break generation rhs rule sequence smaller steps independence assumptions reduce number parameters model 
decomposition rules aim meet criteria 
steps small parameter estimation problem feasible terms having sufficient training data train model providing smoothing techniques mitigate remaining sparse data problems 
second independence assumptions linguistically plausible 
sections describe statistical parsing models increasing degree linguistic sophistication 
model uses decomposition parameters corresponding lexical dependencies natural result 
model incorporates preference right branching structures conditioning distance features 
model extends decomposition include step subcategorization frames chosen probabilistically 
model handles wh movement adding parameters corresponding slash categories passed parent rule children discharged trace 

probabilistic models parsing model section describes generation rhs rule broken sequence smaller steps model 
thing note internal rule lexicalized pcfg form ln ln rm rm head child rule inherits headword tag pair parent ln ln rm rm left right modifiers may zero unary rules 
shows tree example article 
extend left right sequences include terminating symbol allowing markov process model left right sequences 
ln rm 
example bought vbd np week nn np ibm nnp vp bought vbd vp np np bought vbd ibm nnp week nn exception top rule tree form top 
collins head driven statistical models nl parsing note lexical rules contrast internal rules completely deterministic 
take form part speech tag word tag pair rule rewrites just word 
see examples lexical rules 
formally take lexicalized nonterminal expand deterministically probability way part speech symbol 
parsing models require nonterminal labels partitioned sets part speech symbols nonterminals 
internal rules lhs part speech symbol 
lexicalized rules deterministic discussed remainder article modeling choices concern internal rules 
probability internal rule rewritten exactly chain rule probabilities ln ln rm rm ph pl li li li li pr rj rj ln ln rj rj subscripts denote head left modifier right modifier parameter types respectively 
assumption modifiers generated independently pl li li li li pl li li pr rj rj ln ln rj rj pr rj rj summary generation rhs rule lhs decomposed steps 
generate head constituent label phrase probability ph 

generate modifiers left head probability pl li li ln ln 
symbol added vocabulary nonterminals model stops generating left modifiers symbol generated 

generate modifiers right head probability pr ri ri 
define rm rm 
example probability rule bought np week np ibm vp bought estimated ph vp bought pl np ibm vp bought pl np week vp bought pl vp bought pr vp bought exception rule tree top probability top computational linguistics volume number example examples rest article brevity omit part speech tags associated words writing example bought bought vbd 
emphasize models article word paired part speech word generated word conditioned 
adding distance model 
section describe model extended history show extension utilized incorporating distance features model 
black 
originally introduced history models parsing 
equations current article independence assumption modifier generated independently modifiers generated independently 
general probability generating modifier depend function previous modifiers head parent category headword 
top derivation order fully specified probability generating modifier conditioned structure previously generated 
remainder article assumes derivation order depth modifier recursively generates subtree modifier generated 
gives example illustrates 
models collins showed distance words standing head modifier relationships important particular important capture preference right branching structures translates preference dependencies adjacent words preference dependencies cross verb 
section describe information incorporated model 
section describe experiments evaluate effect features parsing accuracy 
partially completed tree derived depth 
marks position modifier generated nonterminal headword head tag triple symbol 
distribution possible symbols position conditioned previously generated structure structure appearing 
collins head driven statistical models nl parsing child generated probability 
distance function surface string previous modifiers 
principle model condition structure dominated orr matter structure previously generated tree 
distance incorporated model modifying independence assumptions modifier limited dependence previous modifiers pl li li li li pl li li pr ri ri ri ri pr ri ri functions surface string previous modifiers 
see illustration 
distance measure similar collins vector elements string zero length 
string contain verb 
feature allows model learn preference right branching structures 
second feature allows model learn preference modification verb 
model complement adjunct distinction subcategorization tree depicted illustrates importance complement adjunct distinction 
useful identify ibm subject week adjunct temporal modifier distinction tree nps position sisters vp node 
identify complements attaching suffix nonterminals 
shows tree added complement markings 
postprocessing stage add detail parser output couple reasons making distinction parsing 
identifying complements complex warrant probabilistic treatment 
lexical information needed example knowledge week temporal modifier 
knowledge subcategorization preferences example verb takes exactly subject required 
example week subject week model balance preference having subject note feature means dynamic programming parsing algorithms model keep track constituent verb string right left head 
see collins full description parsing algorithms 
models described collins third question concerning punctuation string contain commas 
comma tagged 
model described article cleaner incorporation punctuation generative process described section 
ibm closer vp note ibm subject ibm week bought lotus 
term complement broad sense includes complements specifiers terminology government binding 
computational linguistics volume number tree suffix identify complements 
ibm lotus subject object position respectively 
week adjunct 
examples assumption modifiers generated independently leads errors 
probability generating dreyfus fund subjects np dreyfus vp np fund vp unreasonably high 
similar np bill vp funding vp vb np bill vp vb vp funding vp vb bad independence assumption 
relative week headword subject 
problems restricted nps compare said sbar asbestos dangerous bonds beat short term investments sbar market sbar headed complement sbar headed adjunct 
second reason incorporating complement adjunct distinction parsing model may help parsing accuracy 
assumption complements generated independently leads incorrect parses 
see examples 
identifying complements adjuncts penn treebank 
add suffix nonterminals training data satisfy conditions 
nonterminal np sbar ors parent np sbar parent vp parent sbar 
collins head driven statistical models nl parsing 
nonterminal semantic tags adv voc bnf dir ext loc mnr tmp clr prp 
see marcus 
explanation tags signify 
example np week tmp temporal tag sbar sbar market adv adverbial tag 

nonterminal rhs coordinated phrase 
example rule cc child ss marked complements 
addition child head prepositional phrase marked complement 
probabilities subcategorization frames 
model retrained training data enhanced set nonterminals learn lexical properties distinguish complements adjuncts ibm vs week vs 
suffer bad independence assumptions illustrated 
solve kinds problems generative process extended include probabilistic choice left right subcategorization frames 
choose head probability ph 

choose left right subcategorization frames lc rc probabilities plc lc prc rc 
subcategorization frame multiset specifying complements head requires left right modifiers 

generate left right modifiers probabilities pl li li lc pr ri ri rc respectively 
subcategorization requirements added conditioning context 
complements generated removed appropriate subcategorization multiset 
importantly probability generating symbol zero subcategorization frame non empty probability generating particular complement zero complement subcategorization frame required complements generated 
probability phrase bought np week np ibm vp bought ph vp bought plc np vp bought prc vp bought pl np ibm vp bought np pl np week vp bought pl vp bought pr vp bought head initially decides take single np subject left complements right 
np ibm immediately generated required subject np removed lc leaving empty modifier np week generated 
incorrect structures low probability plc np np vp prc np vp vp vb small 
multiset bag set may contain duplicate nonterminal labels 
computational linguistics volume number model traces wh movement obstacle extracting predicate argument structure parse trees 
section describes probabilistic treatment extraction relative clauses 
noun phrases extracted subject position object position pps store sbar trace bought lotus store sbar ibm bought trace store sbar ibm bought lotus trace possible write rule patterns identify traces parse tree 
argue task best integrated parser task complex warrant probabilistic treatment integration may help parsing accuracy 
couple complexities modification sbar involve extraction fact sbar played ball bat uncommon extraction occur constituents changes sbar said government prepared trace 
hope integrated treatment traces improve parameterization model 
particular subcategorization probabilities smeared extraction 
examples bought transitive verb knowledge traces example training data contribute probability bought intransitive verb 
formalisms similar gpsg gazdar handle wh movement adding gap feature nonterminal tree propagating gaps tree discharged trace complement see 
extraction cases penn treebank annotation trace whnp head sbar straightforward add information trees training data 
np np sbar gap sbar gap whnp gap gap np vp gap vp gap vb trace np gap feature added nonterminals describe wh movement 
top level np initially generates sbar modifier specifies contain np trace adding gap feature 
gap passed tree discharged trace complement right bought 
collins head driven statistical models nl parsing lhs rule gap ways gap passed rhs head gap passed head phrase rule 
left right gap passed recursively left right modifiers head discharged trace argument left right head 
rule passed right modifier complement 
rule trace generated right head vb 
specify parameter type pg head left right 
generative process extended choose cases generating head phrase 
rest phrase generated different ways depending gap propagated 
head case left right modifiers generated normal 
left right cases gap requirement added left right subcat variable 
requirement fulfilled removed subcategorization list trace modifier nonterminal gap feature generated 
example rule sbar gap whnp bought gap probability ph whnp sbar pg right sbar whnp plc sbar whnp prc sbar whnp pr bought gap sbar whnp gap pr sbar whnp pl sbar whnp rule vp bought gap vb bought trace np week probability ph vb vp bought pg right vp bought vb plc vp bought vb prc np vp bought vb pr trace vp bought vb np gap pr np week vp bought vb pl vp bought vb pr vp bought vb rule right chosen gap requirement added rc 
generation bought gap fulfills gap requirements rc 
rule right chosen 
note generation trace satisfies np gap subcategorization requirements 

special cases linguistically motivated refinements models sections described basic framework parsing models article 
section describe linguistic phenomena nonrecursive nps coordination example clearly violate independence assumptions general models 
describe number special cases instance arguing phenomenon violates independence assumptions describing model refined deal problem 
nonrecursive nps define nonrecursive nps referred base nps labeled npb np directly dominate np dominated np possessive np directly dominates pos tag pos 
gives examples 
base nps deserve special treatment reasons boundaries base nps strongly marked 
particular start points base nps marked determiner computational linguistics volume number examples structures base nps 
distinctive item adjective 
probability generating symbol greatly increased previous modifier example determiner 
stand independence assumptions models lose information 
probability npb dog dt nn dog estimated ph nn npb dog pl dt npb nn dog pl npb nn dog pr npb nn dog making independence assumption pl dt npb nn dog pl npb nn dog model fail learn symbol follow determiner 
result model assign unreasonably high probabilities nps np yesterday dog sentences yesterday dog 
annotation standard treebank leaves internal structure base nps underspecified 
example pet food volume pet modifies food food modifies volume vanilla ice cream vanilla ice modify cream structure npb nn nn nn 
reason believe modifiers dependent head previous modifier 
fact happened majority phrases pet food volume conditioning previous modifier head preferable 
general important particular distance measure effective different nonterminal labels effectively different bar levels 
see section discussion 
reasons modifications models nonterminal label base nps changed np npb 
consistency np seen pre postmodifiers npb level added 
example np dog vp transformed np npb dog vp 
extra removed scoring output parser treebank 
simplicity give probability terms model distance variables probability terms distance variables models similar addition various pieces conditioning information 
collins head driven statistical models nl parsing independence assumptions different parent nonterminal npb 
specifically equations modified pl li li li li pl li li li li pr ri ri ri ri pr ri ri ri ri modifier previous modifier nonterminals adjacent distance variable constant omitted 
purposes model defined 
probability previous example ph nn npb dog pl dt npb nn dog pl npb dt pr npb nn dog presumably pl npb dt close 
coordination coordination constructions example independence assumptions basic models fail badly current annotation method treebank 
shows coordination annotated treebank 
example illustrate problems take rule np man np man cc np dog probability ph np np man pl np np man pr cc np np man pr np dog np np man pr np np man independence assumptions mean model fails learn exactly phrase coordinator cc 
basic probability models give high probabilities phrases np np cc np np cc np np 
reason alter generative process allow generation coordinator phrase step just generating nonterminal step nonterminal binary valued coord flag generated 
coord coordination relationship 
generative process generation coord flag modifier triggers additional step generative generic way annotating coordination treebank 
show specific examples base nps added described section 
note item conjunct taken head phrase 
see appendix collins description head rules treat phrases involving coordination 
computational linguistics volume number process generation coordinator tag word pair parameterized pcc parameter 
preceding example give probability ph np np man pl np np man pr np dog coord np np man pr np np man pcc cc np np np man dog note new type parameter pcc generation coordinator word pos tag 
generation coord np dog example implicitly requires generation coordinator tag word pair pcc parameter 
generation tag word pair conditioned words coordination dependency man dog example label relationship np np np example representing np coordination 
coord flag implicitly zero normal nonterminals generated example phrase bought np week np ibm vp bought probability ph vp bought pl np ibm coord vp bought pl np week coord vp bought pl vp bought pr vp bought punctuation section describes treatment punctuation model punctuation refer words tagged comma colon 
previous generative models described collins earlier version models described collins conditioned punctuation surface features string treating quite differently lexical items 
particular model collins failed generate punctuation deficiency model 
section describes punctuation integrated generative models 
step raise punctuation high parse trees possible 
punctuation sentences removed training test data altogether 
punctuation items apart tagged comma colon items quotation marks periods tagged removed altogether 
transformations mean punctuation appears nonterminals opposed appearing phrase 
see example 
parse tree punctuation transformations 
anonymous reviewers article pointed choice discarding sentence final punctuation may optimal final punctuation mark may carry useful information sentence structure 
collins head driven statistical models nl parsing punctuation treated similar way coordination intuition strong dependency punctuation mark modifier generated 
punctuation generated phrase punc flag similar coord flag binary valued feature equal punctuation mark generated phrase 
model np npb adjp old probability ph npb np pl np npb pr adjp old coord punc np npb pr np npb bought pp np npb adjp old pp new parameter type generation punctuation tag word pairs 
generation punc adjp old example implicitly requires generation punctuation tag word pair pp parameter 
generation tag word pair conditioned words punctuation dependency old example label relationship np npb adjp example 
sentences empty pro subjects sentences treebank occur frequently pro subjects may may controlled treebank annotation currently stands nonterminal sentence overt subject 
problem subcategorization probabilities models probability having zero subjects plc vp verb fairly high 
addition sentences subjects appear quite different syntactic environments 
reasons modify nonterminal sentences subjects sg see 
resulting model cleaner division subcategorization plc np vp verb plc np sg vp verb 
model learn probabilistically environments sg appear 
punctuation constraint final step rule concerning punctuation introduced collins impose constraint follows 
constituent chart children separated comma word directly followed comma word sentence 
training data commas follow rule 
rule benefit improving efficiency reducing number constituents chart 
preferable develop probabilistic analog rule leave research 
treebank annotates sentences empty subjects empty element subject position training evaluation null element removed models sentences subjects changed nonterminal sg 
computational linguistics volume number table conditioning variables level back 
example ph estimation interpolates ph ph ph 
distance measure 
back ph 
pg 
pl li lti 
pl 
level plc lc 
pr ri rti 
pr rwi 
prc rc 
lc li lti lc lc li lti lc lc lti 
practical issues parameter estimation table shows various levels back type parameter model 
note decompose pl li lti lc lti word pos tag generated nonterminal li coord punc flags associated nonterminal distance measure product pl li lti lc pl li lti lc probabilities smoothed separately 
eisner originally pos tags smooth generative model way 
case final estimate maximum likelihood estimates context levels table smoothing parameters 
smoothing method described bikel 
derived method described witten bell 
say specific estimate value denominator count relative frequency estimate 
second define number distinct outcomes seen events training data 
variable take value inclusive 
set analogous definitions lead coefficient chosen maximize accuracy development set section treebank practice value range gave similar level performance 
unknown words part speech tagging words occurring times training data words test data seen training replaced unknown token 
allows model handle robustly statistics rare new words 
words test data seen training deterministically assigned pos tag assigned tagger described ratnaparkhi 
preprocessing step collins erroneously stated words occuring times training data classified unknown dan bikel pointing error 
collins head driven statistical models nl parsing tagger decode test data sentence 
words tagged parsing output ratnaparkhi tagger ignored 
pos tags allowed word limited seen training data word tag word pairs seen training give estimate zero pl pr distributions 
model fully integrated part speech tags statistically generated words models parser statistical decision tag known word sentence 
parsing algorithm parsing algorithm models dynamic programming algorithm similar standard chart parsing algorithms probabilistic weighted grammars 
algorithm complexity number words string 
practice pruning strategies methods discard lower probability constituents chart improve efficiency great deal 
appendices collins give precise description parsing algorithms analysis computational complexity description pruning methods employed 
see eisner satta algorithm lexicalized grammars applied models 
eisner satta describe algorithm restricted class lexicalized grammars open question restricted class includes models article 

results parser trained sections wall street journal portion penn treebank marcus santorini marcinkiewicz approximately sentences tested section sentences 
parseval measures black compare performance labeled precision number correct constituents proposed parse number constituents proposed parse number correct constituents proposed parse labeled recall number constituents treebank parse crossing brackets number constituents violate constituent boundaries constituent treebank parse constituent correct span set words ignoring punctuation tokens tagged commas colons quotation marks label constituent treebank parse 
table shows results models variety models literature 
models collins charniak outperform models section treebank 
collins uses technique boosting algorithms machine learning best output model article 
charniak describes series enhancements earlier model charniak 
precision recall traces model respectively cases section treebank criteria met trace correct argument correct headword correct position relation headword preceding magerman collapses advp prt label comparison removed distinction calculating scores 
computational linguistics volume number table results section wsj treebank 
lr lp labeled recall precision 
cbs average number crossing brackets sentence 
cbs cbs percentage sentences crossing brackets respectively 
results table models trained tested data evaluation metric 
note results show slight improvement collins main model changes improved treatment punctuation section addition pp pcc parameters 
model words sentences lr lp cbs cbs cbs magerman collins goodman charniak model model model charniak collins model words sentences lr lp cbs cbs cbs magerman collins charniak ratnaparkhi model model model charniak collins dominated correct nonterminal label 
example trace argument bought follows dominated vp 
cases string vacuous extraction subject position recovered precision recall longer distance cases recovered precision recall 

discussion section discusses aspects models detail 
section gives detailed analysis parsers performance 
section examine exclude infinitival relative clauses figures example called trace fix sink coindexed trace subject infinitival 
algorithm scored precision recall cases section infinitival relatives extremely difficult human annotators distinguish purpose clauses case infinitival purpose clause modifying called ann taylor personal communication 
collins head driven statistical models nl parsing distance features model 
section examine model interacts penn treebank style annotation 
section discuss need break context free rules treebank way model generalize give nonzero probability rules seen training 
case methods analysis 
consider various aspects model affect parsing performance accuracy measurements treebank 
second look frequency different constructions treebank 
third consider linguistically motivated examples way justifying various modeling choices 
closer look results section look closely parser evaluating performance specific constituents constructions 
intention get better idea parser strengths weaknesses 
table breakdown precision recall constituent type 
somewhat useful understanding parser performance breakdown accuracy constituent type fails capture idea attachment accuracy 
reason evaluate parser precision recall recovering dependencies words 
gives better indication accuracy different kinds attachments 
dependency defined triple elements see example tree associated dependencies 
modifier index modifier word sentence 
table recall precision different constituent types section treebank model 
label nonterminal label proportion percentage constituents treebank section label count number constituents label 
proportion count label recall precision np vp pp sbar advp adjp whnp qp prn prt sinv nx whadvp nac frag ucp conjp sq sbarq rrc lst intj computational linguistics volume number raw dependencies normalized dependencies relation modifier head relation modifier head vp np vp np top top top top npb nn dt npb tag tag vp vb np vp tag np np npb pp np npb pp npb nn dt npb tag tag pp np pp tag np tree associated dependencies 
note normalizing dependencies pos tags replaced tag np parent fifth relation replaced np 

head index headword sentence 

relation parent head modifier direction tuple elements parent head modifier nonterminals involved dependency direction dependency left right 
example vp np indicate subject verb dependency 
coordination cases fifth element tuple cc 
example np np np cc instance np coordination 
addition relation normalized extent 
pos tags replaced token tag pos tagging errors lead errors dependencies 
second complement markings parent head nonterminal removed 
example np npb pp replaced np npb pp prevents parsing errors complement mistaken adjunct vice versa leading dependency error 
example np man telescope mistakenly identified adjunct normalization lead dependency errors pp dependency verb object relation incorrect 
normalization verb object relation incorrect 
justification estimated error rate hand assigned pos tags treebank ratnaparkhi didn want noise contribute dependency errors 
collins head driven statistical models nl parsing table dependency accuracy section treebank model 
labels means dependency needs correct relation may wrong complements means complement markings stripped comparing relations means complement markings retained modifying nonterminal 
evaluation precision recall labels complements definition gold standard parser output trees converted sets dependencies precision recall calculated dependencies 
dependency accuracies section treebank table 
table gives breakdown accuracies dependency type 
table shows dependency accuracies subtypes dependency account dependencies 
complement verb recall precision subtype includes relations form vp complement vp tag complement vp auxiliary verb verb dependencies excluded 
frequent verb complements subject verb object verb recovered precision recall 

complements recall precision subtype includes dependencies modifier complement dependency fall complement verb category 

pp modification recall precision dependencies modifier pp 

coordination recall precision 

modification base nps recall precision subtype includes dependencies parent npb 

modification nps recall precision subtype includes dependencies parent np head npb modifier pp 

sentential head recall precision subtype includes dependencies involving headword entire sentence 

adjunct verb recall precision subtype includes dependencies parent vp head tag modifier pp parent head vp modifier pp 
draw accuracies parser doing recovering core structure sentences complements sentential heads base np relationships np chunks recovered accuracy 
main sources errors adjuncts 
coordination especially difficult parser computational linguistics volume number table accuracy frequent dependency types section treebank recovered model 
rank cumulative percentage count relation recall precision percentage npb tag tag pp tag np vp np np npb pp vp tag np vp tag vp vp tag pp top top vp tag sbar qp tag tag np npb np sbar tag np npb sbar vp tag advp npb tag npb vp tag tag vp tag sg np np np cc vp pp sbar whnp sg vp tag adjp vp advp np npb vp adjp tag tag npb tag tag vp tag np vp tag sbar vp vp vp cc npb tag adjp vp tag sg vp tag cc pp tag sg qp tag tag vp tag npb tag qp sinv vp np vp np np np adjp tag pp top top sinv advp tag tag sbar whadvp vp sbar vp tag advp sinv vp np npb sg vp prn nx tag tag np npb prn collins head driven statistical models nl parsing table accuracy various types subtypes dependency part 
subtypes occurring times shown 
type sub type description count recall precision complement verb vp np subject vp tag np object cases vp tag sbar vp tag sg vp tag vp vp sg 
total complements pp tag np vp tag vp cases sbar tag sbar whnp sg pp tag sg sbar whadvp pp tag pp sbar whnp sbar tag sg pp tag sbar adjp np pp tag sbar 
total pp modification np npb pp vp tag pp cases vp pp adjp tag pp advp tag pp np np pp pp pp pp nac tag pp 
total coordination np np np vp vp vp cases adjp tag tag vp tag tag nx nx nx sbar sbar sbar pp pp pp 
total computational linguistics volume number table cont 
type subtype description count recall precision modification npb tag tag base nps npb tag npb cases npb tag tag npb tag adjp npb tag qp npb tag nac npb nx tag npb qp tag 
total modification nps np npb np appositive np npb sbar relative clause cases np npb vp reduced relative np npb sg np npb prn np npb advp np npb adjp 
total sentential head top top top top sinv cases top top np top top sg 
total adjunct verb vp tag advp vp tag tag cases vp tag adjp vp advp vp tag np vp tag sbar vp tag sg vp tag vp sbar vp tag advp vp prn vp np vp sg vp tag prn vp tag 
total collins head driven statistical models nl parsing table results section wsj treebank 
column means adjacency conditions distance measure likewise column indicates verb conditions distance measure 
lr labeled recall lp labeled precision 
cbs average number crossing brackets sentence 
cbs cbs percentages sentences crossing brackets respectively 
model lr lp cbs cbs cbs model model model model model model involves dependency content words leading sparse statistics 
distance measure distance measure implementation described section deserves discussion motivation 
section consider perspectives influence parsing accuracy analysis distributions training data sensitive distance variables examples sentences distance measure useful discriminating competing analyses 
impact distance measure accuracy 
table shows results models adjacency verb distance measures 
clear distance measure improves models accuracy 
striking just badly model performs distance measure 
looking parser output reason poor performance adjacency condition distance measure approximating subcategorization information 
particular phrases pps lesser extent vps take exactly complement right head adjacency feature encodes parameters pp sbar adjacent pp sbar adjacent 
shows particularly bad structures returned model distance variables 
surprise subcategorization useful distance measure masked utility 
interpretation moving parameterized model model fully parameterized model model adjacency condition adds accuracy verb condition adds subcategorization adds mere 
interpretation subcategorization information isn useful original assumption order features originally added model 
interpretation subcategorization useful moving model model see improvement result subcategorization parameters adjacency adds improvement verb condition adds final improvement 
engineering point view choice add just distance subcategorization model distance preferable 
linguistically clear adjacency approximate subcategorization subcategorization computational linguistics volume number examples bad parses produced model distance subcategorization conditions model table 
pp complements sbar complements 
examples adjacency condition subcategorization parameters correct errors examples adjacency subcategorization variables overlap utility 
table distribution nonterminals generated postmodifiers np see tree left various distances head 
true means modifier adjacent head true means verb head modifier 
distributions calculated events cases sections treebank 
true false false false false true percentage percentage percentage pp pp pp sbar sbar sbar np np np vp vp vp sg sg sg adjp prn prn prn advp adjp advp adjp advp conjp ucp ucp jj rrc vbg vbn rb rb rrc frag cd correct sense 
free word order languages distance may approximate subcategorization complement may appear right left head confusing adjacency condition 
frequencies training data 
tables show effect distance distribution modifiers frequent syntactic environments np verb modification 
distribution varies great deal distance 
striking way probability increases increasing distance np case verb case 
modifier probability generally decreases distance 
example probability seeing pp modifier np decreases 
collins head driven statistical models nl parsing table distribution nonterminals generated postmodifiers verb vp see tree left various distances head 
true means modifier adjacent head true means verb head modifier 
distributions calculated events distributions sections 
auxiliary verbs verbs vp complement right excluded statistics 
true false false false false true percentage percentage percentage np pp pp pp sbar np sbar sg np sg advp sg advp sbar advp adjp sbar sbar np np rb sg prn np adjp prt prn vbn sbar prt vb sg ucp vb sq prn frag frag ucp adjp vbz vbn vbp vbd vb frag ucp rb vbg intj sbarq conjp vbp rbr intj dt distance features right branching structures 
adjacency verb components distance measure allow model learn preference structures 
consider adjacency condition 
shows examples right branching structures frequent 
statistics tables probability alternative structures calculated 
results 
right branching structures get higher probability lexical dependency probabilities multiplied prior preference right branching structures overruled lexical preferences 
distance variables conditioned product terms alternatives identical model preference structure 
probabilities alternative pp structures excluding probability terms constant structures means distance adjacent means adjacent follows computational linguistics volume number alternative structures surface sequence chunks npb pp pp case npb pp sbar second case adjacency condition distinguishes structures 
percentages taken sections treebank 
cases right branching structures frequent 
right branching non right branching pp np npb np npb pp np npb np npb pp np npb pp np npb np npb np npb probabilities sbar case assuming sbar contains verb means modification cross verb means follows right branching non right branching pp np npb sbar np npb np npb np npb pp np npb np npb sbar np npb np npb collins head driven statistical models nl parsing alternative structures surface sequence chunks verb condition distance measure distinguishes structures 
cases low attachment analyses get higher probability model low probability generating pp modifier involving dependency crosses verb 
stands nonterminal 
verb condition right branching structures 
shows examples verb condition important differentiating probability structures 
cases adjunct attach high low high attachment results dependency crossing verb lower probability 
alternative surface string feature predicate previous modifiers set nonterminals contain verb vp sbar 
allow model handle cases example correctly 
second example shows preferable condition surface string 
case verb invisible top level generated recursively np object 
structural versus semantic preferences 
hypothesis lexical statistics really important parsing arriving correct interpretation sentence simply matter finding semantically plausible analysis statistics related lexical dependencies approximate notion plausibility 
implicitly just better statistics calculated items predicate argument level structure 
distance preferences interpretation just way mitigating sparse data problems lexical statistics sparse falling back structural preference ideal better chance 
hypothesis suggested previous specific cases attachment ambiguity pp attachment see collins brooks showed models perform better lexical statistics straight structural preference merely fallback 
examples suggest case fact sentences equally semantically plausible analyses structural preferences computational linguistics volume number distinguish strongly 
take example pereira warren john believed shot bill 
surprisingly sentence analyses bill deep subject believed shot 
people strong preference bill doing shooting may second analysis 
see analysis semantically quite plausible consider bill believed john shot 
evidence structural preferences override semantic plausibility take example pinker flip said yesterday 
sentence garden path structural preference yesterday modify verb strong easy semantically plausible interpretation paraphrased flip said yesterday 
model correct predictions cases 
example statistics table show pp times attach low attach high verbs candidate attachment points chances seeing pp modifier columns table respectively 
example probability seeing np adjunct modifier nonadjacent non environment sections treebank cases contrast chance seeing np adjunct modifying said verb cases 
probabilities differ factor 
importance choice tree representation figures show alternative styles syntactic annotation 
penn treebank annotation style tends leave trees quite flat typically level structure bar level extreme completely binary branching representations 
annotation styles sense equivalent easy define mapping 
crucially different annotation styles may lead quite different parsing accuracies model representations equivalent mapping 
parsing model need tied annotation style treebank trained 
procedure transform trees training test data new representation alternative annotation styles sentence verb head left modifiers right modifiers penn treebank style analysis level structure bar level alternative equivalent binary branching representation 
collins head driven statistical models nl parsing alternative annotation styles noun phrase noun head left modifiers right modifiers penn treebank style analysis level structure bar level note nonrecursive recursive noun phrases labeled np alternative equivalent binary branching representation modification penn treebank style differentiate recursive nonrecursive nps sense npb bar structure np bar structure 

transform training data trees new representation train model 

recover parse trees new representation running model test data sentences 

convert test output back treebank representation scoring purposes 
long mapping treebank new representation lost making transformation 
goodman johnson suggest strategy 
goodman converts treebank binary branching trees 
johnson considers conversion number different representations discusses influences accuracy pcfgs 
models developed article tacitly assumed penn treebank style annotation perform badly representations example binary branching trees 
section point explicit describing exactly annotation style suitable models showing annotation styles cause problems 
dependence penn treebank style annotations imply models inappropriate treebank annotated different style case simply recommend transforming trees flat bar level trees training model step procedure outlined 
models literature sensitive annotation style 
charniak models perform quite differently trees example current models learn rules vp sg pp rare binary branching structures context sensitivity lost 
models magerman ratnaparkhi contextual predicates need modified different annotation style 
goodman models exception specifies treebank transformed chosen representation binary branching trees 
representation affects structural lexical preferences 
alternative representations figures lexical dependencies providing binary branching structures centered head phrase examples 
difference representations involves structural preferences right branching preferences encoded distance measure 
applying models article treebank analyses type head centered computational linguistics volume number bb binary branching structures flat penn treebank style annotations 
case binary branching annotation style prevents model learning structures receive low probability long distance dependency associated final pp boldface 
binary branching tree result distance measure incorrectly encodes preference right branching structures 
see consider examples 
binary branching example generation final modifying pp blind distance head modifies 
top level tree apparently adjacent head crucially closer modifier sg pp hidden lower tree structure 
model unable differentiate generation pp adjacent versus nonadjacent non verb crossing versus verb crossing environments structures assigned unreasonably high probabilities 
mean distance preferences encoded pcfg 
goodman achieves adding distance features nonterminals 
spirit implementation top level rules vp vp pp np np pp modified vp vp pp np np pp respectively means phrase head verb right modifiers means phrase right modifier head 
model learn training data vp vp pp vp vp vp pp vp prepositional phrase modification cross verb 
importance differentiating nonrecursive recursive nps 
shows modification penn treebank annotation relabel base nps npb 
illustrates problem arises distinction structures assigned high probabilities way penn treebank annotates nps 
modification annotation differentiate recursive np nonrecursive npb noun phrases 
structure seen training data receive high probability model trained trees style 
collins head driven statistical models nl parsing examples phrases penn treebank nonrecursive recursive phrases differentiated 
seen training data 
johnson notes structure higher probability correct flat structure counts taken treebank standard pcfg 
model fooled binary branching style modeling pps adjacent head noun phrase assigned high probability 
problem apply nps types phrases adjectival phrases adverbial phrases nonrecursive bar recursive bar levels differentiated penn treebank 
see examples 
ideally cases differentiated implement change difference accuracy relative cases excluding coordination cases looking instances sections penn treebank parent head nonterminal np case cases coordination punctuation mark coordinator similar 
summary 
summarize models article assume 
tree representations flat level bar level 

different bar levels different labels particular nonrecursive recursive levels differentiated frequent case nps 
need break rules parsing approaches described concentrate breaking context free rules treebank smaller components 
lexicalized rules initially broken bare bones markov processes increased dependency previously generated modifiers built back distance measure subcategorization 
additional context models able recover rules test data seen training data 
alternative proposed charniak limit parsing contextfree rules seen training data 
lexicalized rule predicted steps 
context free rule generated 
second lexical items filled 
probability rule estimated ln ln rm rm ln hr rm pl li li pr rj rj example john eats apples mary eats bananas 
charniak model conditions parent nonterminal expanded omit brevity 
computational linguistics volume number estimation technique charniak cf rule probabilities interpolates estimates lowest ln hr rm 
rules seen training data assigned zero probability model 
parse trees test data limited include rules seen training 
problem approach coverage 
shown section test data sentences require rules seen training 
gives motivation breaking rules smaller components 
section motivates need break rules perspectives 
discuss penn treebank annotation style leads large number grammar rules 
second assess extent coverage problem looking rule frequencies training data 
third conduct experiments assess impact coverage problem accuracy 
fourth discuss breaking rules may improve estimation coverage 
penn treebank annotation style leads rules 
flatness penn treebank annotation style discussed section 
flatness trees leads large constantly growing number rules primarily number adjuncts head potentially unlimited example number pp adjuncts head verb 
binary branching chomsky adjunction grammar generate unlimited number adjuncts rules 
example grammar generates sequence vp np pp vp np vp vp pp contrast penn treebank style create new rule number pps seen training data 
grammar vp np vp np pp vp np pp pp vp np pp pp pp adverbial adjuncts adverbial phrases adverbial modify verb times different types adjuncts seen rule 
result combinatorial explosion number rules 
give flavor random sample rules format vp vb modifier occurred sections penn treebank vp vb np np np prn vp vb np sbar pp sg advp vp vb np advp advp pp pp vp vb rb vp vb np pp np sbar vp vb np pp sbar pp verb phrases cause kind combinatorial explosion phrases particular nonrecursive noun phrases contribute huge number rules 
section considers distributional properties rules detail 
collins head driven statistical models nl parsing note motivation penn treebank decision represent rules way rules expressing chomsky adjunction schema complements adjuncts separated rule types vp vb complement vp vp adjunct 
allows argument adjunct distinction pp modifiers verbs left undefined distinction difficult annotators 
second surface ordering opposed deep structure adjuncts closer head complements yielding structures fall outside chomsky adjunction schema 
example rule vp vb np pp sbar frequently penn treebank sbar complements nearly adjuncts 
quantifying coverage problem 
quantify coverage problem rules collected sections penn treebank 
punctuation raised high possible tree rules complement markings distinction base nps recursive nps 
conditions rule tokens collected distinct rule types 
collected count rule 
table shows statistics rules 
majority rules grammar occur 
rules account rules token 
rule tokens sections treebank drawn random chance instance rule tokens 
hand rule drawn random rules grammar induced sections chance rule having occurred 
percentage token count rules indication coverage problem 
estimate rules rules required test data seen training 
sentences rule occurred just 
gives estimate roughly sentences test data covered grammar induced sentences treebank 
complement markings added nonterminals base np nonrecursive np distinction coverage problem worse 
table gives statistics case 
counts sentences sentences contain count rule 
table statistics rules taken sections treebank complement markings included nonterminals 
rule count number rules percentage number rules percentage rules type type token token computational linguistics volume number table statistics rules taken sections treebank complement markings included nonterminals 
rule count number rules percentage rules number rules percentage rules type type token token table results section treebank 
label restricted means model restricted recovering rules seen training data 
lr labeled recall 
lp labeled precision 
cbs average number crossing brackets sentence 
cbs cbs percentages sentences crossing brackets respectively 
model accuracy lr lp cbs cbs cbs model model restricted model model restricted impact coverage accuracy 
parsing experiments assess impact coverage problem parsing accuracy 
section treebank parsed models parse trees restricted include rules seen training data 
table shows results 
restricting rules leads decrease recall decrease precision model decrease recall decrease precision model 
breaking rules improves estimation 
coverage problems motivation breaking rules 
method may improve estimation 
see consider rules headed told counts shown table 
estimating probability rule vp told charniak method interpolate maximum likelihood estimates pml rule vp told pml rule vp estimation interpolates specific lexically sensitive distribution table estimate just parent nonterminal vp 
different rules specific distribution different rule types tokens told vp head count rules cases 
statistics relatively low 
high chance new rule told required test data reasonable amount collins head driven statistical models nl parsing table distribution rules told head sections treebank distribution subcategorization frames told head 
count rule vp told vbd np sbar vp told vbd np vp told vbd np sg vp told vbd np np sbar vp told vbd np vp told vbd np pp sbar vp told vbd np pp vp told vbd np np vp told vbd np pp np sbar vp told vbd np pp pp vp told vbd np np pp vp told vbd np sbar vp told vbd np vp told vbd vp told advp vbd np sbar vp told vbd np sg sbar vp told vbd np sbar pp vp told vbd np sbar pp vp told vbd np pp sg vp told vbd np pp np vp told vbd np pp vp told vbd np np vp told vbd np advp sbar vp told vbd np advp pp np vp told vbd np advp vp told vbd np prn sbar total count subcategorization frame np sbar np np np sg total probability mass left backed estimate pml rule vp 
estimation method missing crucial generalization spite different rules distribution subcategorization frames sharper 
told seen subcategorization frames training data large number rules entirely due adjuncts punctuation appearing complements 
estimation method model effectively estimates probability rule plc lc vp told prc rc vp told rule vp told lc rc left right subcategorization frames lc rc chosen 
entire rule generated markov processes 
armed plc prc parameters model ability learn generalization told appears quite limited sharp distribution subcategorization frames 
say parameters estimated interpolation example pml lc vp told pml lc vp case quite high 
subcategorization frames opposed rule types seen cases 
lexically specific distribution computational linguistics volume number pml lc vp told quite highly trusted 
relatively little probability mass left backed estimate 
summary distributions table model quite uncertain rules told appear 
relatively certain subcategorization frame 
introducing subcategorization parameters allows model generalize important way rules 
carefully isolated core rules subcategorization frame model certain 
note charniak method certainly advantages estimation capture statistical properties rules independence assumptions lose distribution number pp adjuncts seen particular head 

related unfortunately space limitations possible give complete review previous article 
sections give detailed comparison models article lexicalized pcfg model charniak history models jelinek 
magerman ratnaparkhi 
discussion additional related chapter collins attempts give comprehensive review statistical parsing 
particular relevance parsing penn wsj treebank jelinek magerman eisner collins charniak goodman ratnaparkhi chelba jelinek roark 
eisner describes dependency models closely related models article 
collins describes dependency model applied treebank parsing 
goodman describes probabilistic feature grammars application parsing treebank 
chelba jelinek describe incremental history parsing approach applied language modeling speech recognition 
history approaches introduced parsing black 

roark describes generative probabilistic model incremental parser results terms parse accuracy treebank perplexity scores language modeling 
earlier particular relevance considered importance relations lexical heads disambiguation parsing 
see hindle rooth earliest pieces research topic context prepositional phrase attachment ambiguity 
uses lexical relations parse disambiguation promising results see sekine 
jones eisner alshawi carter 
statistical models lexicalized grammatical formalisms lead models parameters corresponding lexical dependencies 
see resnik schabes schabes waters stochastic tree adjoining grammars 
joshi srinivas describe alternative supertagging model tree adjoining grammars 
see alshawi stochastic head automata lafferty sleator temperley stochastic version link grammar 
de marcken considers stochastic lexicalized pcfgs specific em methods unsupervised training 
seneff describes markov models rule generation closely related markov style rules models current article 
note machine learning methods parsing probabilistic 
see brill mooney rule learning systems 
chiang shown models current article implemented unchanged stochastic tree adjoining grammar 
bikel collins head driven statistical models nl parsing developed generative statistical models integrate word sense information parsing process 
eisner develops sophisticated generative model lexicalized context free rules making probabilistic model lexicalized transformations rules 
charniak describe methods recovery semantic tags penn treebank annotations significant step forward complement adjunct distinction recovered model current article 
charniak gives measurements perplexity lexicalized pcfg 
gildea reports experiments investigating utility different features bigram lexical dependency models parsing 
miller 
develop generative lexicalized models information extraction relations 
approach enhances nonterminals parse trees carry semantic labels develops probabilistic model takes labels account 
collins 
describe models current article applied parsing czech 
charniak describes parsing model uses markov processes generate rules 
model takes account additional context previously generated modifiers nonterminals higher parse trees maximum entropy inspired model 
additional features gives clear improvements performance 
collins shows similar improvements quite different model boosting approaches reranking freund 
initial model fact model described current article generate best output 
reranking approach attempts rerank best lists additional features initial model 
intention approach allow greater flexibility features included model 
bod describes different approach dop approach parsing gives excellent results treebank parsing comparable results charniak collins 
comparison model charniak give detailed comparison models article parser charniak 
model described charniak types parameters 
lexical dependency parameters 
charniak dependency parameters similar parameters section 
parameters pl li lti lc charniak parameters notation pl li example dependency parameter np headed profits subject verb rose profits np rose 

rule parameters 
second type parameters associated context free rules tree 
example take node tree computational linguistics volume number nonterminal expand rules grammar 
rule probability defined rose vp 
rule probability depends nonterminal expanded headword parent 
sections give explanation differences charniak models models article 
additional features charniak model 
notable additional features charniak model 
rule probabilities conditioned parent nonterminal expanded 
models include information distinguishing recursive nonrecursive nps considered reduced form information 
see section discussion distinction arguments section motivation charniak choice conditioning parent 
second charniak uses word class information smooth probabilities reports improvement feature 
charniak uses words text unsupervised training 
parser trained treebank parse text statistics collected machine parsed text merged treebank statistics train second model 
gives improvement performance 
dependency parameters charniak model 
similar charniak dependency parameters conditioned information 
noted previously parameters pl li lti lc charniak parameters notation pl li 
additional information included models follows head nonterminal label vp previous profits rose example 
glance redundant example usually take vp head 
cases head label vary example take head coordination cases 
lti pos tags head modifier words 
inclusion tags allows models pos tags word class information 
charniak model may missing important generalization respect 
charniak shows pos tags word class information model important parsing accuracy 
coordination flag 
distinguishes example coordination cases appositives charniak model parameter modifier head np np cases 
lc rc punctuation distance subcategorization variables 
difficult tell empirical tests features important 
collins head driven statistical models nl parsing rule parameters charniak model 
rule parameters charniak model effectively decomposed parameters section head parameters models subcategorization gap parameters 
decomposition allows model assign probability rules seen training data see section extensive discussion 
right branching structures charniak model 
models distance features encode preferences right branching structures 
charniak model represent information explicitly learns implicitly rule probabilities 
example np pp pp sequence preference right branching structure encoded higher probability rule np np pp rule np np pp pp 
note conditioning rule parent needed disallow structure np np pp pp see johnson discussion 
strategy encode information distance measure 
distance measure effectively penalizes rules np npb np pp middle np contains verb case pp modification results dependency crosses verb 
charniak model unable distinguish cases middle np contains verb pp modification crosses verb 
comparison models jelinek 
magerman ratnaparkhi detailed comparison models history models ratnaparkhi jelinek 
magerman 
strength models undoubtedly powerful estimation techniques maximum entropy modeling ratnaparkhi decision trees jelinek magerman 
weakness argue section method associating parameters transitions taken bottom shift reduce style parsers 
give examples method leads parameters unnecessarily fragmenting training data cases ignoring important context cases 
similar observations context tagging problems maximum entropy models lafferty mccallum pereira klein manning 
analyze model magerman common examples ambiguity pp attachment coordination appositives 
case word sequence competing structures associated decision sequences dn em respectively 
probability structures written di di ei ei useful isolate decision structures single probability term 
value minimum value di ei 
rewrite probabilities follows di di dj dj di di ei ei ej ej ei ei computational linguistics volume number thing note di di ei ei probability terms irrelevant decision structures 
additional assumption di di ei ei justified examples section jth decision decisions practically deterministic 
equivalently assuming little probability mass lost trees 
equalities isolated decision structures parameters dj dj ej ej 
shows case pp attachment 
thing note pp attachment decision pp built 
decision linked np preceding preposition arc np go left right 
thing note important feature verb falls outside conditioning context 
model considers information constituents preceding location decision 
repaired considering additional context fixed bound far verb decision point 
note cases method fragments data unnecessary ways 
cases verb directly precedes np place farther left treated separately 
shows similar example np coordination ambiguity 
pivotal decision somewhat counterintuitive location np preceding coordinator 
point np coordinator built head noun contextual window 
shows appositive example head noun appositive np contextual window decision 
examples extended illustrate problem 
np conjunct comma subject clause 
example candidate structures sequence words 
shows decision labeled structures differ 
arc np go left verb attachment pp right noun attachment pp 
collins head driven statistical models nl parsing candidate structures sequence words 
shows decision labeled structures differ 
arc np go left high attachment coordinated phrase right low attachment coordinated phrase 
candidate structures sequence words 
shows decision labeled structures differ 
arc np go left high attachment appositive phrase right noun attachment appositive phrase 
john likes mary bill loves jill decision coordinate mary bill just np mary built 
point verb loves outside contextual window model way telling bill subject clause 
model assigning probability mass globally implausible structures result points local ambiguity parsing process 
problems repaired changing derivation order conditioning context 
ratnaparkhi additional chunking stage means head noun fall contextual window coordination appositive cases 

models article incorporate parameters track number linguistic phenomena bigram lexical dependencies subcategorization frames propagation slash categories 
models generative models parse trees decomposed number steps top derivation tree computational linguistics volume number decisions derivation modeled conditional probabilities 
careful choice derivation independence assumptions resulting model parameters corresponding desired linguistic phenomena 
addition introducing parsing models evaluating performance penn wall street journal treebank aimed discussion sections give insight models strengths weaknesses effect various features parsing accuracy relationship models statistical parsing 
highlight points section showed analysis accuracy different types dependencies adjuncts main sources error parsing models 
contrast dependencies forming core structure sentences example dependencies involving complements sentential heads np chunks recovered precision recall 
section evaluated effect distance measure parsing accuracy 
model adjacency distance feature subcategorization parameters performs poorly precision recall suggesting adjacency feature capturing subcategorization information model parser 
results table show subcategorization adjacency verb crossing features contribute significantly model implication model performance 
section described models suited penn treebank style annotation certain phenomena particularly distance features may fail modeled correctly treebanks different annotation styles 
may important point bear mind applying models treebanks languages 
particular may important perform transformations structures treebanks different annotation styles 
section gave evidence showing importance models ability break context free rules treebank generalizing produce new rules test examples 
table shows precision section treebank decreases recall decreases model restricted produce context free rules seen training data 
section discussed relationships generative model charniak history conditional models ratnaparkhi jelinek 
magerman 
certainly similar charniak model models article significant differences identified section 
important difference ability models generalize produce context free rules seen training data described section section showed parsing models ratnaparkhi jelinek 
magerman suffer similar problems label bias observation bias problem observed tagging models described lafferty mccallum pereira klein manning 
collins head driven statistical models nl parsing acknowledgments ph thesis basis article mitch marcus excellent ph thesis adviser contributing ways research 
members thesis committee aravind joshi mark liberman fernando pereira mark steedman remarkable breadth depth feedback 
benefited greatly discussions jason eisner dan melamed adwait ratnaparkhi paola merlo 
dimitrios giving feedback portions 
discussions people ircs university contributed quite directly research breck baldwin srinivas bangalore dan bikel james brooks david chiang doran kyle hart kim tony robert macintyre max mintz tom morton martha palmer jeff reynar joseph anoop sarkar matthew stone ann taylor john trueswell bonnie webber fei xia david yarowsky 
crucial input sources outside penn summer worked bbn technologies discussions scott miller richard schwartz ralph weischedel deep influence research 
rayner david carter sri cambridge supervised master thesis cambridge university technical supervision research 
anonymous reviewers comments 
alshawi 

head automata bilingual tiling translation minimal representations 
proceedings th annual meeting association computational linguistics pages 
alshawi david carter 

training scaling preference functions disambiguation 
computational linguistics 
bikel dan 

statistical model parsing word sense disambiguation 
proceedings student research workshop acl 
bikel dan scott miller richard schwartz ralph weischedel 

nymble high performance learning name finder 
proceedings fifth conference applied natural language processing pages 
black steven abney dan flickinger claudia ralph grishman philip harrison donald hindle robert ingria frederick jelinek judith klavans mark liberman mitch marcus salim roukos beatrice santorini tomek strzalkowski 

procedure quantitatively comparing syntactic coverage english grammars 
proceedings february darpa speech natural language workshop 
black frederick jelinek john lafferty david magerman robert mercer salim roukos 

history grammars richer models probabilistic parsing 
proceedings fifth darpa speech natural language workshop ny 
don eugene charniak 

assigning function tags parsed text 
proceedings annual meeting north american chapter association computational linguistics pages seattle 
bod 

minimal set fragments achieves maximal parse accuracy 
proceedings acl 
booth taylor richard thompson 

applying probability measures languages 
ieee transactions computers 
brill eric 

automatic grammar induction parsing free text transformation approach 
proceedings st annual meeting association computational linguistics 
charniak eugene 

statistical parsing context free grammar word statistics 
proceedings fourteenth national conference artificial intelligence aaai press mit press menlo park ca 
charniak eugene 

maximum entropy inspired parser 
proceedings naacl 
charniak eugene 

immediate head parsing language models 
proceedings th annual meeting association computational linguistics 
chelba frederick jelinek 

exploiting syntactic structure language modeling 
proceedings coling acl montreal 
chiang david 

statistical parsing automatically extracted tree adjoining grammar 
proceedings acl hong kong pages 
collins michael james brooks 

prepositional phrase attachment backed model 
proceedings third computational linguistics volume number workshop large corpora pages 
collins michael 

new statistical parser bigram lexical dependencies 
proceedings th annual meeting association computational linguistics pages 
collins michael 

generative lexicalised models statistical parsing 
proceedings th annual meeting association computational linguistics th conference european chapter association computational linguistics pages 
collins michael 

head driven statistical models natural language parsing 
ph thesis university pennsylvania philadelphia 
collins michael jan lance ramshaw christoph tillmann 

statistical parser czech 
proceedings th annual meeting acl college park maryland 
collins michael 

discriminative reranking natural language parsing 
proceedings seventeenth international conference machine learning icml 
collins michael 

parameter estimation statistical parsing models theory practice distribution free methods 
appear book chapter 
de marcken carl 

unsupervised induction phrase structure grammars 
proceedings third workshop large corpora 
eisner jason 

new probabilistic models dependency parsing exploration 
proceedings coling pages 
eisner jason 

empirical comparison probability models dependency grammar 
technical report ircs institute research cognitive science university pennsylvania philadelphia 
eisner jason 

transformational priors grammars 
proceedings conference empirical methods natural language processing emnlp philadelphia 
eisner jason giorgio satta 

efficient parsing bilexical context free grammars head automaton grammars 
proceedings th annual meeting acl 
freund yoav raj iyer robert schapire yoram singer 

efficient boosting algorithm combining preferences 
machine learning proceedings fifteenth international conference 
morgan kaufmann 
gazdar gerald klein ivan sag 

generalized phrase structure grammar 
harvard university press cambridge ma 
gildea daniel 

corpus variation parser performance 
proceedings conference empirical methods natural language processing emnlp pittsburgh pa goodman joshua 

probabilistic feature grammars 
proceedings fourth international workshop parsing technologies 
ulf ray mooney 

learning parse translation decisions examples rich context 
proceedings th annual meeting association computational linguistics th conference european chapter association computational linguistics pages 
hindle don mats rooth 

structural ambiguity lexical relations 
proceedings th annual meeting association computational linguistics 
hopcroft john ullman 

automata theory languages computation 
addison wesley reading ma 
jelinek frederick john lafferty david magerman robert mercer adwait ratnaparkhi salim roukos 

decision tree parsing hidden derivation model 
proceedings human language technology workshop pages 
johnson mark 

effect alternative tree representations tree bank grammars 
proceedings 
jones mark jason eisner 

probabilistic parser applied software testing documents 
proceedings national conference artificial intelligence aaai pages san jose ca 
jones mark jason eisner 

probabilistic parser application 
proceedings aaai workshop statistically natural language processing techniques san jose ca 
joshi aravind bangalore srinivas 

disambiguation super parts speech supertags parsing 
international conference computational linguistics coling kyoto university japan august 
klein dan christopher manning 

conditional structure versus conditional estimation nlp models 
proceedings collins head driven statistical models nl parsing conference empirical methods natural language processing emnlp philadelphia 
lafferty john andrew mccallum fernando pereira 

conditional random fields probabilistic models segmenting labeling sequence data 
proceedings icml 
lafferty john daniel sleator david temperley 

grammatical trigrams probabilistic model link grammar 
proceedings aaai fall symposium probabilistic approaches natural language 
magerman david 

statistical decision tree models parsing 
proceedings rd annual meeting association computational linguistics pages 
manning christopher hinrich sch tze 

foundations statistical natural language processing 
mit press cambridge ma 
marcus mitchell grace kim mary ann marcinkiewicz robert macintyre ann mark ferguson karen katz 

penn treebank annotating predicate argument structure 
proceedings human language technology workshop pages 
marcus mitchell beatrice santorini marcinkiewicz 

building large annotated corpus english penn treebank 
computational linguistics 
miller scott fox lance ramshaw ralph weischedel 

novel statistical parsing extract information text 
proceedings st meeting north american chapter association computational linguistics naacl pages 
pereira fernando david warren 

definite clause grammars language analysis survey formalism comparison augmented transition networks 
artificial intelligence 
pinker stephen 

language instinct 
william morrow new york 
ratnaparkhi adwait 

maximum entropy model part speech tagging 
proceedings conference empirical methods natural language processing may ratnaparkhi adwait 

linear observed time statistical parser maximum entropy models 
proceedings second conference empirical methods natural language processing brown university providence ri 
resnik philip 

probabilistic tree adjoining grammar framework statistical natural language processing 
proceedings coling vol 
pages 
roark brian 

probabilistic top parsing language modeling 
computational linguistics 
schabes yves 

stochastic lexicalized tree adjoining grammars 
proceedings coling vol 
pages 
schabes yves richard waters 

stochastic lexicalized context free grammar 
proceedings third international workshop parsing technologies 
sekine satoshi john carroll ananiadou tsujii 

automatic learning semantic collocation 
proceedings third conference applied natural language processing 
seneff stephanie 

tina natural language system spoken language applications 
computational linguistics 
witten ian timothy bell 

zero frequency problem estimating probabilities novel events adaptive text compression 
ieee transactions information theory 

