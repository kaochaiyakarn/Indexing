supervised locally linear embedding dick de ridder olga oleg inen robert duin pattern recognition group department imaging science technology delft university technology cj delft netherlands dick bob ph tn tudelft nl www ph tn tudelft nl machine vision group oulu department electrical information engineering box fin university oulu finland oleg mkp ee oulu fi www ee oulu fi php 
locally linear embedding lle proposed method unsupervised nonlinear dimensionality reduction 
number attractive features require iterative algorithm just parameters need set 
extensions lle supervised feature extraction independently proposed authors 
methods unified common framework applied number benchmark data sets 
results show perform high dimensional data exhibits manifold structure 
real world classification problems high dimensional data sets collected sensors 
ideal decision boundary different classes sets highly nonlinear 
classifier degrees freedom consequently large number parameters 
result training classifier data sets quite complicated large number parameters estimated limited number samples 
known curse dimensionality 
overcome problem mapping data highdimensional space classes approximately linearly separable 
kernel techniques support vector machines svms typical examples approach 
alternative lower data dimensionality increase 
information lost reduction number parameters needs estimate result better performance 
linear methods performing dimensionality reduction principal component analysis pca linear discriminant analysis lda established literature 
nonlinear dimensionality reduction method called locally linear embedding lle considered :10.1.1.111.3313:10.1.1.111.3313
main assumption lle financial support oulu graduate school gratefully acknowledged 
partly sponsored dutch foundation applied sciences stw project number 
data set sampled possibly nonlinear manifold embedded highdimensional space 
lle unsupervised non iterative method avoids local minima problems competing methods em algorithm 
advantages lle parameters need set selecting optimal values discussed local geometry high dimensional data preserved embedded space 
extend concept lle multiple manifolds representing data specific class supervised variants lle independently proposed 
framework unifying unsupervised supervised methods 
supervised lle applied feature extractor number benchmark data sets shown useful high dimensional data clear manifold structure 
lle framework lle input lle takes set dimensional vectors assembled matrix size output set dimensional vectors assembled matrix size th column vector corresponds th column vector squared distance matrix samples constructed 
sample xi 
nearest neighbours sought indices stored matrix ij index nearest neighbour sample xi 
step sample xi approximated weighted linear combination nearest neighbours making assumption neighbouring samples lie locally linear patch nonlinear manifold 
find reconstruction weight matrix ij contains weight neighbour reconstruction sample xi expression minimised xi wi ij ij subject constraint wi ij :10.1.1.111.3313
easy show weight calculated individually :10.1.1.111.3313
sample xi construct matrix ij im ij im 
ri suitably cho sen regularisation constant see 
wi ij rpq 
second final step weights stored kept fixed embedding ir minimising ii yi wi ij ij 
minimisation problem solved introducing constraint embedded data unit covariance minimise 
result minimised carrying eigen decomposition matrix :10.1.1.111.3313:10.1.1.111.3313
eigenvectors corresponding nd st smallest eigenvalues form final embedding eigenvector corresponding smallest eigenvalue corresponds mean embedded data discarded obtain embedding centered origin 
embedding new sample mapped quickly calculating weights reconstructing nearest neighbours training set step lle 
embedding weighted combination embeddings neighbours 
lle shown useful analysis high dimensional data sets 
typical example visualisation sequence images showing person face slowly rotating left right 
data sets lle finds embeddings individual axes correspond roughly small number degrees freedom data 
supervised lle supervised lle slle introduced deal data sets containing multiple disjoint manifolds corresponding classes 
fully disjoint manifolds local neighbourhood sample xi class composed samples belonging class 
achieved artificially increasing pre calculated distances samples belonging different classes leaving unchanged samples class max max maximum entry ij xi xj belong class 
obtains unsupervised lle result fully supervised lle introduced called slle 
varying gives partially supervised lle slle 
slle distances samples different classes large maximum distance entire data set 
means neighbours sample class picked class 
practice compute just select nearest neighbours certain sample class 
slle supervised lle 
contrast slle introduces additional parameter controls amount supervision 
mapping preserves manifold structure introduces separation classes 
allows supervised data analysis may lead better generalisation slle previously unseen samples 
feature extraction distance matrix represents fully disconnected classes mapped fairly lle 
added degrees freedom original lle slle slle fig 

third fourth feature iris set 
lle embeddings trained nearest mean classifier 
second step separate classes eigenvectors corresponding nd cth smallest eigenvalues just discard mean data 
mapped classes separated due constraint ny samples certain class mapped single point ir optimal embedding dimensionality 
slle necessarily optimal method trade lle slle 
automatic setting demanding locally average variance retained remaining dimensions 
local intrinsic dimensionality estimate denoted ml 
feature extraction process illustrated classes iris data set mapped single points slle 
slle retains class structure reduces class dispersion compared lle 
clearly slle suitable feature extraction step prior classification 
internal structure class partially lost mapping class overlap easily visualised dimensional space 
idea slle related spectral clustering 
affinity matrix samples calculated 
clusters matrix block diagonal structure 
eigen decomposition normalised affinity matrix gives embedding small number dimensions clusters clearly separated original space 
slle uses class label information construct artificial diagonal block matrix applies change distance matrix basis lle 
resulting matrix sparse containing non zero entries row changed block diagonal matrix 
result mixture unsupervised lle supervised spectral clustering obtained 
experiments setup verify feature extraction capabilities slle applied number data sets varying number samples dimensions classes sets experimental results tables 
comparison number dimensions needed global pca retain variance global intrinsic dimensionality mg shown tables 
sets obtained uci repository earlier 
chromosomes set contains gray values sampled chromosome banding profiles 
textures sets contain pixel gray value image patches natural unstructured structured regular brodatz textures 
nist digits set consists pixel gray value images pre processed handwritten digits taken nist database 
set contains multiresolution local binary patterns calculated images different types 
experiments set follows set randomly split times training set test set 
classifiers nmc nearest mean classifier ldc qdc bayes plug linear quadratic classifiers knnc nn classifier optimised leave procedure training set 
repeated data mapped lle slle ml dimensions see section slle dimensions 
mappings calculated range values neighbourhood size parameter applicable 
compare lle methods traditional feature extraction techniques classifiers trained data mapped ml dimensions principal component analysis pca linear discriminant analysis lda multidimensional scaling mds see overview methods 
results tables average errors test set random set splits standard deviation brackets 
best result shown bold underlined results significantly worse paired test shown bold 
slle slle best result range values shown 
ideally optimal values independent validation set size data sets permit setting aside samples 
results confirm slle generally leads better classification performance lle usually mapping technique 
expected slle extract nonlinear manifolds supervised way general feature extraction methods 
number interesting observations slle low dimensional data 
low dimensional sets shown table bayes plug classifiers ldc qdc original data number parameters need estimated reasonably low 
cases slle improve classification point better original data 
slle works high dimensional data 
cases performance nearly original data significantly original pca fisher mds lle slle slle iris mg ml nmc ldc qdc knnc wine mg ml nmc ldc qdc knnc diabetes mg ml nmc ldc qdc knnc glass mg ml nmc ldc qdc knnc vehicle mg ml nmc ldc qdc knnc hepatitis mg ml nmc ldc qdc knnc chromosomes mg ml nmc ldc qdc knnc table 
test error low dimensional data sets 
better ionosphere sonar textures sets 
splice set exception quadratic classifier qdc performs surprisingly improved 
slle works nn works original data 
slle neighbourhood method nn classifier 
fact slle seen generalised nn method neighbours labels play role distances neighbours 
slle vs slle consistently outperforms 
slle expected generalise better extra parameters estimated embedding dimensionality ml 
performance slle especially sensitive 
bayes plug original pca fisher mds lle slle slle ionosphere mg ml nmc ldc qdc knnc splice mg ml nmc ldc qdc knnc sonar mg ml nmc ldc qdc knnc optdigits mg ml nmc ldc qdc knnc natural textures mg ml nmc ldc qdc knnc structured textures mg ml nmc ldc qdc knnc nist digits mg ml nmc ldc qdc knnc mg ml nmc ldc qdc knnc table 
test error high dimensional data sets 
classifiers trained slle mapped data perform poorly samples mapped single point covariance structure left 
nearest mean classifier performs 
slle maps data nonlinearly simple classifier 
analogous svms kernel function performs desired nonlinear mapping simple linear classifier suffices 
local vs global intrinsic dimensionality 
priori expect slle sets contain curved manifolds 
sets exhibit high global intrinsic dimensionality mg coupled low local ml 
sets slle performs splice ml mg 
observation means simple test applicability slle quickly estimate ml subset samples compare mg 
significant difference indicates performance slle highly 
common framework unsupervised supervised lle proposed 
experiments number benchmark data sets demonstrated slle powerful feature extraction method coupled simple classifiers yield promising recognition results 
slle mainly applicable high dimensional data sets clearly exhibit manifold structure 
research address problems choosing ml founded way slle 
computational storage complexity concern data set needs retained mapping new data application large data sets infeasible 
subset selection may alleviate problem 

blake merz 
uci repository machine learning databases 

de ridder 
adaptive methods image processing 
phd thesis delft university technology delft 

de ridder duin 
locally linear embedding classification 
technical report ph pattern recognition group dept imaging science technology delft university technology delft netherlands 

duda hart stork 
pattern classification 
john wiley sons new york ny nd edition 

duin 
pattern recognition toolbox matlab 
download www ph tn tudelft nl duin 

marcos inen 
locally linear embedding algorithm 
technical report machine vision group university oulu finland 

inen 
selection optimal parameter value locally linear embedding algorithm 
proc 
st int 
conf 
fuzzy systems knowledge discovery singapore pages 

inen 
multiresolution gray scale rotation invariant texture classification local binary patterns 
ieee transactions pattern analysis machine intelligence 

roweis saul 
nonlinear dimensionality reduction locally linear embedding 
science 

saul roweis 
think globally fit locally unsupervised learning nonlinear manifolds 
technical report ms cis univ pennsylvania 

weiss 
segmentation eigenvectors unifying view 
proc 
ieee int 
conf 
computer vision pages 
