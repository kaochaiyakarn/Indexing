importance maintaining behavioural link parents offspring xin yao computational intelligence group school computer science university college university new south wales australian defence force academy canberra act australia email xin cs oz au study evolutionary artificial neural networks eanns argued partial training process architectural mutation plays important role maintaining behavioural link parents offspring beneficial simulated evolution 
investigates issue number experiments 
experimental results show closer behavioural link parents offspring due partial training process lead better performance evolved anns generalise better 
results illustrate fixed amount time optimal balance time evolution training learning 
evolution introduced anns different levels evolution connection weights evolution architectures evolution learning rules 
different approaches evolving ann architectures 
evolve ann architectures weights 
evolve ann architectures connection weights simultaneously 
pointed suffers noisy fitness evaluation permutation problems 
better choice practice 
methods evolve ann architectures connection weights simultaneously include kind architectural mutations modify existing ann architectures generate new ones 
architectural mutations hidden node connection deletion addition ann carried follow training weights mutated ann 
deletion addition hidden node connection dramatic change ann weights learned parent architecture may meaningful offspring architecture 
behavioural difference parent offspring may large 
useful information accumulated previous generations may lost large disruption ann behaviour 
necessary maintain close behavioural link parents offspring evolution 
examines importance behaviour link parents offspring set experiments 
worth pointing aim come best solution particular classification recognition problem investigate general issue simulated evolution 
rest organised follows 
section introduces eann system experimental studies points partial training process comes simulated evolution 
section describes experimental setup actual experiments presents experimental results remarks 
section concludes brief summary results 
evolving artificial neural networks epnet automatic system evolving ann architectures connection weights simultaneously 
applied benchmark problems success 
major components epnet described 
epnet uses evolutionary programming ep algorithm evolve anns 
uses selection mutations 
selection nonlinear ranking scheme 
major mutations ep hybrid training hidden node deletion connection deletion connection addition hidden node addition 
mutations attempted sequentially aforementioned order order encourage parsimony anns 
generation mutations successfully 
mutation regarded unsuccessful offspring improve performance parent certain degree certain threshold 
hybrid training addition connection node hidden node deletion random initialisation anns initial partial training rank selection obtain new generation 
successful 
successful 
successful 
mutations training deletion main components epnet 
mutations hybrid training weight adjusting mutation 
architecture modification mutations 
hybrid training consists backpropagation bp algorithm adaptive learning rates simplified simulated annealing sa algorithm 
sa algorithm help bp escape local minima 
key difference architectural mutations ones partial training process bp successful mutation 
idea additional training process narrow behavioural gap caused architectural mutation 
training partial fixed number epochs allowed 
experiments partial training process divided steps 
step consists fixed number epochs 
offspring ann improve reduce error certain margin step partial training process finishes 
maximum number epochs partial training process determined number steps epochs step 
partial training process may finish earlier 
improve generalisation ability evolved anns validation set set experiments measure fitness individual 
evolution finished evolved networks trained hybrid algorithm combined training set set 
second validation set set training 
ann smallest error rate set final result epnet system 
maintaining behavioural link parents offspring order investigate impact partial training epnet set experiments different number steps epochs partial training process carried 
australian credit card problem example 
problem assess applications credit cards number attributes 
cases total 
output classes 
attributes include numeric values discrete ones having possible values 
data set obtained uci machine learning repository 
input attributes eanns rescaled linear function 
data set randomly partitioned training cases testing data cases 
training data partitioned subsets training subset cases set cases set cases 
experiments population size 
experiment repeated runs 
initial population generated random 
order conduct fair comparison smaller number steps epochs larger number generations allowed 
tables show average results epnet respectively number epochs step number steps 
worth noting runs small number steps large number generations computation time opportunities finish partial training process early 
clear tables number epochs steps increases result epnet improves 
phenomenon illustrates architectural mutations cause disruption ann behaviours 
useful information anns learned previously may lost due disruption 
partial training process help reduce impact disruption narrowing behavioural gap parents offspring 
emphasise behavioural evolution epnet rooted ep simulates natural evolution behavioural genetic level 
important maintain behavioural link parents offspring ep evolutionary systems 
results tables appears suggest longer partial training time better 
case 
preliminary experiments number epochs shown testing errors increased 
closer look results reveal anns start overfitting validation sets especially set error rate set substantially lower tables 
complete training unnecessary may negative impact table best ann evolved epnet 
results averaged runs 
number epochs step fixed 
mean sd min max represent mean standard deviation minimum maximum values runs respectively 
testing error rate number steps number generations mean sd min max table best ann evolved epnet 
results averaged runs 
number steps fixed 
mean sd min max represent mean standard deviation minimum maximum values runs respectively 
testing error rate number epochs number generations mean sd min max simulated evolution 
partial training better time consuming method 
larger number epochs number generations simulated evolution reduced 
maximum number architectural mutations reduced 
epnet explore number possible architectures reply weight learning 
epnet advantage evolving architectures weights simultaneously fully exploited 
fixed amount computation time balance time evolution learning 
extreme evolution take time large number generations little time learning generation 
extreme learning dominant force means long learning time generation generations evolution 
extreme ideal problems 
best choice optimal balance time evolution learning 
unfortunately optimal choice highly problem dependent 
universal rules applied problems 
initialisation experiments initial individuals anns generated random 
preprocessing training applied random anns 
order investigate impact initialisation behavioural evolution 
set experimental runs carried training random anns epochs 
tables give experimental results 
tables show partial training architectural mutations beneficial evolution general 
case table increased number steps produced worse result 
case anns trained introducing extra epochs 
comparing results initial training apparent initial training bring benefit 
contrary initial training worsened final results 
indicate initial training unnecessary 
better random population 
behavioural link baldwin effect relationship evolution learning studied umbrella baldwin effect 
emphasis behavioural link parents offspring somewhat related different baldwin effect 
baldwin effect purely darwinian behavioural evolution emphasised lamarkian 
connection weights learned generation inherited offspring 
genetic assimilation studies epnet simulate evolution genetic level 
cost considered learning 
spite fundamental difference baldwin effect experimental results section show learning help table best ann evolved epnet 
initial population trained epochs evolution 
results averaged runs 
number epochs step fixed 
mean sd min max represent mean standard deviation minimum maximum values runs respectively 
testing error rate number steps number generations mean sd min max table best ann evolved epnet 
initial population trained epochs evolution 
results averaged runs 
number steps fixed 
mean sd min max represent mean standard deviation minimum maximum values runs respectively 
testing error rate number epochs number generations mean sd min max evolution 
behavioural link search importance behavioural link seen point view searching space possible anns 
essence epnet automatic ann design systems search infinitely large space possible anns different architectures connection weights 
weight training local learning corresponds series small search steps space 
trained ann regarded near local optimum small neighbourhood anns architecture 
architectural mutation ann large jump space move new point offspring far away current parent 
partial training offspring quite poor comparison parent 
goal partial training architectural mutation bring offspring local optimum neighbourhood 
words evolutionary search samples near local optima space 
noted anns similar behaviours necessarily similar architectures weights 
hidden node deletion addition may lead change ann architecture weights partial training evolution 
maintaining behavioural link parents offspring mean loss structural diversity population 
emphasised importance behavioural link parents offspring simulated evolution 
preliminary experiments carried demonstrate closer behavioural link parents offspring help evolution 
excessive learning training unnecessary harmful 
discussed relationship behavioural link issues evolutionary computation baldwin effect 
step study behavioural evolution framework eanns 
issues touched 
example experiments conducted data set 
data sets runs necessary order draw stronger 
furthermore investigation automatic mechanism balance evolution learning dynamically pursued 
partially supported australian research council small scheme 
author grateful jason running experiments 
yao liu new evolutionary system evolving artificial neural networks ieee transactions neural networks vol 

may 
yao review evolutionary artificial neural networks international journal intelligent systems vol 
pp 

yao evolutionary artificial neural networks encyclopedia computer science technology kent williams eds vol 
pp 
new york ny marcel dekker 
fogel owens walsh artificial intelligence simulated evolution 
new york ny john wiley sons 
fogel evolutionary computation new philosophy machine intelligence 
new york ny ieee press 
yao empirical study genetic operators genetic algorithms microprogramming vol 
pp 

michie spiegelhalter taylor machine learning neural statistical classification 
london ellis horwood limited 
belew mitchell eds adaptive individuals evolving populations models algorithms 
massachusetts addison wesley 
turney whitley anderson special issue baldwin effect evolutionary computation vol 

