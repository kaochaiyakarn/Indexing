maxq method hierarchical reinforcement learning thomas dietterich department computer science oregon state university corvallis oregon tgd cs orst edu presents new approach hierarchical reinforcement learning maxq decomposition value function 
maxq decomposition procedural semantics subroutine hierarchy declarative semantics representation value function hierarchical policy 
maxq unifies extends previous hierarchical reinforcement learning singh kaelbling dayan hinton 
conditions maxq decomposition represent optimal value function derived 
defines hierarchical learning algorithm proves convergence shows experimentally learn faster ordinary flat learning 
discusses interesting issues arise hierarchical reinforcement learning including hierarchical credit assignment problem non hierarchical execution maxq hierarchy 
hierarchical approaches reinforcement learning rl problems promise benefits improved exploration exploration take big steps high levels abstraction learning fewer trials fewer parameters learned subtasks ignore irrelevant features full state faster learning new problems subtasks learned previous problems re 
research explored general approaches reaching goals 
approach introduced dean lin exploits hierarchical decomposition primarily computational device accelerate computation optimal policy 
second approach introduced parr russell relies programmer design hierarchy machines constrains possible policies considered 
method computes policy optimal subject hierarchical constraints effectively flattening hierarchy 
call kind policy hierarchically optimal best policy consistent imposed hierarchy 
third approach pioneered singh kaelbling dayan hinton relies programmer designed hierarchy 
hierarchy subtask defined terms goal states termination conditions 
subtask hierarchy corresponds markov decision problem mdp methods seek compute policy locally optimal subtask 
call policies recursively optimal 
precup sutton singh studies aspects third approaches 
extend research recursively optimal policies introducing maxq method hierarchical reinforcement learning 
methods introduced singh kaelbling dayan hinton specific particular tasks 
feudal learning method dayan hinton suffers problem non primitive levels feudal hierarchy learning task non markovian difficult solve 
contrast maxq method general purpose 
level hierarchy task markovian solved standard rl methods 
cases state abstractions introduced destroying optimality learned policy 
kaelbling maxq supports non hierarchical execution learned policy permits behave optimal policy violates structure hierarchy 
organized follows 
introduce maxq hierarchy example define procedural declarative semantics 
introduce theo taxi domain describe conditions maxq hierarchy successfully represent value function fixed hierarchical policy 
section introduces learning algorithm training maxq hierarchy shows experimentally theoretically works 
shows non hierarchical policy computed executed maxq hierarchy 
maxq hierarchy introduce maxq method simple taxi problem shown 
taxi inhabits grid world 
specially designated locations world marked ed lue 
taxi problem episodic 
episode taxi starts randomly chosen state randomly chosen amount fuel ranging units 
passenger locations chosen randomly passenger wishes transported locations chosen randomly 
taxi go passenger location source pick passenger go destination location destination put passenger 
keep things uniform taxi pick drop passenger located destination 
episode ends passenger deposited destination location 
primitive actions domain navigation actions move taxi square north south east west consumes unit fuel pickup action putdown action action executed taxi location 
action deterministic 
reward gamma action additional reward successfully delivering passenger 
reward gamma taxi attempts execute putdown pickup actions illegally 
navigation action cause taxi hit wall action op usual reward gamma 
episode ends reward gamma fuel level falls zero 
seek policy maximizes average reward step 
domain equivalent maximizing total reward episode 
optimal policy non trivial implement hand attains average reward step computed trials 
possible states squares locations passenger counting starting locations taxi destinations fuel levels 
task simple hierarchical structure sub tasks get passenger taxi deliver passenger 
subtask involves navigating locations performing pickup putdown action 
taxi navigating location location relevant 
capture hierarchical structure take advantage learning performance 
shows maxq graph problem 
graph contains kinds nodes max nodes indicated triangles nodes indicated ovals 
max nodes children denote primitive actions domain max nodes children represent subtasks 
simple problem subtasks navigate move taxi target location get move passenger location pick passenger put move passenger destination put passenger move root perform task picking delivering passenger 
notice navigate task shared get put tasks 
immediate children max node nodes 
node represents action performed achieve parent subtask 
example node child represents action navigating current state passenger location 
distinction max nodes nodes critical ensuring subtasks shared reused 
max node learn context independent expected cumulative reward performing subtask 
example estimate expected cumulative reward navigating state target locations node learn context dependent expected cumulative reward performing subtask 
example learn expected cumulative reward navigating location completing get task 
hand learn expected cumulative reward navigating location completing put task 
nodes ask cost get location help compute values 
value function computed context independent putdown pickup north east south west maxq graph taxi domain shared parent nodes 
rest say max node child max node node parent child define semantics maxq graph formally suppose task solve markov decision problem mdp defined set states actions reward function js reward received entering state performing action state transition probability function js probability entering state result performing 
assume defines undiscounted stochastic shortest path problem 
results extended infinite horizon discounted case 
max node corresponds separate subtask children max node actions subtask divides set states disjoint subsets set set terminal states subtask terminate environment enters states subset terminal states goal states discuss details defining reward function encourage terminate goal states 
define arbitrary policy subtask policy attempts get state goal states hierarchical policy maxq graph set policies fp max node indicate max node choose actions 
hierarchical policy executed way subroutines executed ordinary programming languages 
root policy chooses child actions perform say get 
get policy chooses child actions say pickup 
pickup action executed primitive 
max node policy executed max node enters terminating state point control returns parent max node 
view maxq graph subroutine call graph 
subroutines max nodes parameterized 
graph takes parameter specifies locations target 
way graph different ordinary program children max node unordered 
called order max node execute children multiple times completes subtask 
maxq graph kind incompletely specified non deterministic program 
result learning determine policy max node tells invoke children 
maxq graph completely specified deterministic program interacting non deterministic environment 
far formulation maxq method essentially feudal learning method dayan hinton 
important improvement feudal learning ability interpret maxq graph representation value function hierarchical policy 
consider max node define expected cumulative reward hierarchical policy starting state enter state fixed hierarchical policy subtask welldefined transition probability function js probability environment move state state executes action probability defined child executing fixed policy descendants 
node treat action atomic action 
immediate reward node executing expected reward node moving current state terminal state policy denoted 
write js 
gives recursive decomposition value function value function root node value function entire mdp subtask separate mdp 
recursive expression useful switch action value representation value function 
define expected cumulative reward mdp performing action state hierarchical policy 
define second term right hand side eq 
call completion function 
expected cumulative reward completing mdp policy executing action state definitions rewrite eq 
ae composite js js primitive js completely define value function semantics maxq hierarchy 
node parent child stores information state max node returns value child chosen compute value hierarchical policy state node compute 
requires ask child node value 
child recursively asks child value leaf node reached 
path traversed maxq graph 
leaf node returns parent adds gamma recursively 
value returned gamma shows sequence rewards received primitive actions decomposed hierarchically sum terms 
representation theorems conditions hierarchy represent value function fixed hierarchical policy 
say maxq graph full state graph separate values stored state applications including desirable introduce abstraction function provide set features essential information state 
node store function value distinct state 
full state graphs easy prove theorem expanding equations theorem fp ng hierarchical policy defined full state maxq graph root node graph 
exist values internal max nodes primitive leaf max nodes expected cumulative reward policy state important difficult question understand conditions state maxq graph exactly represent value function hierarchical policy 
theorem establishes condition theorem max nodes actions result fs js set states result applying action state node hierarchical policy 
ae ae ae ae xxxxxx 
gamma gamma maxq decomposition denote sequence rewards received primitive actions times 
condition holds maxq graph abstraction functions represent value function policy value function represented maxq graph abstraction functions max nodes actions states distinct states result case words abstraction function treats pair result states identical un abstracted values equal 
value function properly represented 
children satisfy condition 
expected reward completing action depends current location taxi target location amount fuel remaining 
navigating example expected reward depend source destination locations 
abstractions create hierarchical credit assignment problem 
example implementation taxi location target location represent functions 
wanted nodes learn navigation policy independent fuel remained 
means fuel exhausted gamma penalty received nodes represent reason penalty 
hierarchical credit assignment problem determine node responsible reward received 
solution designer maxq hierarchy decompose reward function 
reward generated marker attached indicates nodes potentially responsible reward 
gamma empty fuel penalty nodes held responsible parent compare values decide avoid penalty 
functions able represent rewards 
requires change decomposition equations 
js portion reward assigned node write js js ae composite js js primitive domains believe easy designer hierarchy decompose reward function 
interesting problem research develop algorithms autonomously solving hierarchical credit assignment problem 
learning algorithm preceding section shown hierarchy correctly represent value function hierarchical policy full state employed represent function node apply parr russell ham algorithm learn best hierarchical policy 
committed employing state abstractions chosen develop reinforcement learning algorithm finding recursively optimal policy 
turns general different recursively optimal policies achieve better expected rewards 
problem subtask may policies locally optimal useful task 
example suppose changed taxi domain taxi hits wall trial terminated reward gamma 
target location steps away locally optimal policy hit wall 
part hierarchically optimal policy 
dayan hinton faced problem solved providing penalty points subtask entering undesired terminal state state 
proper effect maxq hierarchy causes value function computed entire hierarchy incorrect incorporates non zero probability receiving terminal state penalties 
better method define max node mdp parallel markov decision problem states actions transition probabilities second reward function zero undesired terminal states provides large penalty 
penalty gamma points 
learning algorithm seek locally optimal policy compute value function executing original mdp value passed maxq hierarchy 
specifically learning algorithm maxq variant learning performs 
composite max node maintain tables 
algorithm chooses action perform current exploration policy 
executes observes resulting state reward js computes argmax gamma delta js gamma delta js best action current values 
updated leaf node update slightly different gamma js quantity learning rate node time step order prove convergence algorithm assumptions 
assume deterministic policies mdp proper terminate probability 
second assume locally optimal policies give transition probability distribution js 
ensures locally optimal policies node give rise mdp node parent 
consequence assumption recursively optimal policies value function 
third assume jv jc bounded times easy enforce 
fourth exploration policy executed node learning glie greedy limit infinite exploration policy policy executes action infinitely state visited infinitely greedy respect probability 
learning rates satisfy usual conditions lim 
lim 

theorem assumptions listed probability maxq converge recursively optimal policy mdp consistent maxq hierarchy proof sketch proof employs stochastic approximation argument similar introduced prove convergence learning sarsa jaakkola jordan singh bertsekas tsitsiklis singh jaakkola littman 
proof induction levels tree starting max nodes children primitive leaf nodes 
level max nodes standard results learning applied prove values converge probability optimal value function 
furthermore node executing glie exploration policy policy nodes converge probability locally optimal policy 
consider max node children primitive nodes level max nodes 
define js transition probabilities observed parent node invokes child node state time learning process 
level max nodes executing glie policies js converge probability state transitions js produced locally optimal policies node assumption locally optimally policies give state transition probabilities 
enables prove node converges probability optimal values locally optimal policy 
key decompose error particular backup terms 
term corresponding difference sample backup observed state transition full bellman backup js expected value zero 
term corresponding difference doing full bellman backup current transition probabilities js doing full bellman backup final transition probabilities js converges zero probability 
applying stochastic approximation result proposition bertsekas tsitsiklis prove node converge locally optimal policy 
induction prove entire hierarchy converges recursively optimal policy 
proof sketch 
interesting method employed accelerate learning higher nodes graph 
action chosen max node state execution move environment series states best action choose best action choose node states equations applied states 
reflects important difference standard subroutine calls maxq hierarchy 
standard subroutines set preconditions true start subroutine 
partially executed subroutine preconditions false possible interrupt subroutine call re establishing preconditions 
maxq hierarchy max node invoked state complete execution task state onward 
means execution max node interrupted restarted change hierarchy 
applied algorithm maxq taxi task tabular representation functions 
employed state abstraction follows 
nodes function ignores passenger source destination locations amount fuel 
function ignores passenger destination fuel know source location taxi location order predict effects illegal pickup actions 
similarly ignores passenger source location fuel ignores source destination locations fuel 
represent function single value successful navigate pickup remains complete get action 
true 
hierarchical credit assignment need see entire state ignore state information succeeds task completed 
abstractions mean set element functions values flat learning maxq hierarchy requires values represent reward step trial flat hierarchical mean optimal performance online performance flat hierarchical learning taxi task 
curve smoothed trial moving average 
horizontal line shows average performance optimal policy 
functions 
compares online performance flat hierarchical learning 
flat learning employed boltzmann exploration initial temperature 
decreased factor successful trial 
experimented different cooling schedules unable get flat learning converge optimal policy trials 
fastest cooling schedule able attain briefly optimal expected reward 
hierarchical learning employed separate temperature max node 
starting temperature nodes 
node decreased temperature successfully reached goal terminal state 
cooled factor second level max nodes 
cases learning rate employed actions rewards deterministic 
cooling rates chosen lower max nodes graph reasonably competent subtasks nodes higher graph try learn 
care taken max node may conclude subtask expensive subtask learned policy sets value low 
combined boltzmann exploration result subtask may tried 
performed update node node completed subtask average absolute bellman error step 
parameter tuned 
shows hierarchical method able learn task faster achieve higher level performance flat learning 
course methods improved employing techniques accelerating learning eligibility traces peng williams 
non hierarchical execution shown maxq hierarchy learn optimal policy mdp policy recursively optimal hierarchical 
situations optimal policy quite hierarchical 
example consider modified taxi task fickle taxi problem soon taxi picks passenger moves square passenger randomly change destination probability 
change comes hierarchical policy committed executing original destination 
result subtask take taxi old destination 
control return invoke move taxi new destination 
hierarchical mdp raise question way convert recursively optimal hierarchical policy optimal non hierarchical policy 
answer question implemented fickle taxi domain 
removed aspects fuel domain optimal policy 
compares performance flat learning hierarchical learning modified task 
optimal policy achieve average reward step best hierarchical policy compatible maxq graph achieve 
hierarchical learning maxq able attain level rapidly 
flat learning approaches optimum reach trials 
tuned algorithm optimize performance 
employed learning rate decayed initial temperature factor flat hierarchical goal terminal state reached 
alternative hierarchical execution maxq graph polling execution suggested kaelbling hierarchical distance goal method 
polling approach maxq action chosen starting computing path root leaf highest value 
primitive action path executed process repeated 
equivalent computing step greedy lookahead policy current value function 
hierarchical policy optimal step greedy policy closer optimal policy corresponds step policy improvement policy iteration algorithm bertsekas 
informally proves theorem states value policy computed polling execution maxq hierarchy value policy computed hierarchical execution 
polling execution maxq graph produce non hierarchical policy better hierarchical policy represented graph 
tested fickle taxi task training maxq hierarchy maxq trials continuing training polling execution 
shows initial loss performance switch polling execution 
hierarchical training nodes graph learned values states frequently executed 
polling executed states rapidly learn correct values performance able reach level optimal non hierarchical policy 
domain polling execution best hierarchical policy produce optimal policy 
concluding remarks defined maxq value function decomposition hierarchical reinforcement learning 
shown maxq graph represent value function hierarchical policy implemented graph 
learning algorithm learning introduced proved converge shown experimentally perform better ordinary non hierarchical learning 
important aspect maxq method separation context independent policy value function represented max nodes contextdependent value function represented nodes 
permits value functions subtasks learned independent context enhances reusability subtasks easier employ state abstraction subtasks 
optimality learned policy lost general hierarchical problems may introduced 
fortunately ability maxq hierarchy represent value function hierarchical policy permits non hierarchical execution step greedy policy better hierarchical policy 

author eric chown helpful discussions bayer reward step trial optimal non hierarchical optimal hierarchical hierarchical flat online performance flat hierarchical learning fickle taxi task 
curve average runs returns run smoothed trial moving average 
trial optimal polling execution online performance fickle taxi task 
trials trained hierarchically 
remaining trials trained polling 
william langford wesley helpful comments earlier draft 
support onr nsf iri gratefully acknowledged 
bertsekas 

dynamic programming optimal control 
athena scientific belmont ma 
bertsekas tsitsiklis 

neuro dynamic programming 
athena scientific belmont ma 
dayan hinton 

feudal reinforcement learning 
nips pp 

morgan kaufmann san francisco ca 
dean lin 

decomposition techniques planning stochastic domains 
tech 
rep cs dept computer science brown university providence rhode island 
jaakkola jordan singh 

convergence stochastic iterative dynamic programming algorithms 

comp 
kaelbling 

hierarchical reinforcement learning preliminary results 
icml pp 
san francisco ca 
morgan kaufmann 
parr russell 

reinforcement learning hierarchies machines 
nips vol 
cambridge ma 
mit press 
peng williams 

incremental multi step qlearning 
mach 
learn 
singh jaakkola littman 

convergence results single step policy reinforcement learning algorithms 
tech 
rep university colorado dept comp 
sci 
singh 

transfer learning composing solutions elemental sequential tasks 
mach 
learn 
sutton precup singh 

mdps semi mdps learning planning representing knowledge multiple temporal scales 
tech 
rep university mass dept comp 
inf 
sci amherst ma 
