fast monte carlo algorithms matrices ii computing low rank approximation matrix petros drineas ravi kannan yz michael mahoney technical report yaleu dcs tr february 
applications data consist may naturally formulated matrix interest nd low rank approximation approximation matrix rank greater speci ed rank smaller methods singular value decomposition svd may nd approximation best de ned sense 
methods require memory time superlinear applications data sets large prohibitive 
simple intuitive algorithms matrix compute description low rank approximation qualitatively faster svd 
algorithms provable bounds error matrix matrix kxk kxk denote frobenius norm spectral norm respectively 
rst algorithm columns randomly chosen 
matrix consists columns appropriate rescaling shown approximations top singular values corresponding singular vectors may computed 
computed singular vectors description matrix may computed rank ka min rank ka dk poly kak holds high probability algorithm may implemented storing matrix random access memory ram provided passes matrix stored external memory additional ram memory 
second algorithm similar approximates matrix randomly sampling rows form matrix additional error implemented passes matrix constant additional ram memory 
achieve additional error best rank approximation kak algorithms take time polynomial log failure probability rst takes time linear max second takes time independent bounds improve previously published results respect rank parameter frobenius spectral norms 
addition proofs error bounds novel method important matrix perturbation theory 
probability distribution columns rescaling crucial features algorithms chosen judiciously 
department computer science rensselaer polytechnic institute troy new york cs rpi edu department computer science yale university new haven connecticut usa kannan cs yale edu supported part nsf 
department mathematics yale university new haven connecticut usa mahoney cs yale edu interested developing analyzing fast monte carlo algorithms performing useful computations large matrices 
consider singular value decomposition svd related papers consider matrix multiplication new method computing compressed approximate decomposition large matrix 
computations generally require time superlinear number nonzero elements matrix expect algorithms useful applications data sets modeled matrices extremely large 
cases assume input matrices prohibitively large store random access memory ram external memory storage possible 
algorithms allowed read matrices times keep small randomly chosen rapidly computable sketch matrices ram computations performed sketch 
framework pass ecient computational model scarce computational resources number passes data additional ram space required additional time required 
applications data consist may naturally formulated matrix low rank approximated low rank matrix :10.1.1.117.3373:10.1.1.12.7580:10.1.1.131.5084:10.1.1.120.3875:10.1.1.108.8490
application areas latent semantic indexing dna microarray analysis facial object recognition web search models data may consist points matrix columns methods dealing high dimensional data svd related principle component analysis multidimensional scaling 
interest nd low rank approximation approximation rank greater speci ed rank matrix smaller example rank reduction applications linear algebra statistics image processing lossy data compression text analysis cryptography 
svd may nd approximation best de ned sense requires superlinear polynomial time dependence prohibitive applications data sets large 
method attracted interest traditional random projection method projects problem randomly chosen low dimensional subspace 
dimensional reduction requires performing operation amounts premultiplying matrix matrix takes time dependent superlinear manner simple intuitive algorithms matrix compute description low rank approximation qualitatively faster svd 
algorithms provable bounds error matrix matrix kxk kxk denote frobenius norm spectral norm de ned section respectively 
rst algorithm algorithm section columns randomly chosen 
matrix consists columns appropriate rescaling shown approximations top singular values corresponding singular vectors may computed 
computed singular vectors description matrix may computed rank ka min rank ka dk poly kak holds high probability algorithm may implemented storing matrix ram provided passes matrix stored external memory additional ram memory 
second algorithm constant addl 
error ref 
ka ka summary sampling complexity algorithm section similar approximates matrix randomly sampling rows form matrix additional error implemented passes matrix constant additional ram memory 
achieve additional error kak algorithms take time polynomial log failure probability see summary dependence sampling complexity 
rst algorithm takes time linear max takes time independent bounds improve previously published results respect rank parameter frobenius spectral norms 
addition proofs error bounds novel method important matrix perturbation theory 
probability distribution columns rescaling crucial features algorithms chosen judiciously 
worth emphasizing ts computing low rank matrix approximations 
original frieze kannan vempala see shown working randomly chosen constant sized submatrix obtain bounds form frobenius norm indirectly spectral norm 
achieve additional error kak size submatrix constant respect depended polynomially submatrix constant sized construction particular construction sampling probabilities required space time linear modify algorithm construction computation constant sized submatrix requires constant additional space time ts framework cient model data streaming computation 
addition provide di erent proof main result frobenius norm improve polynomial dependence proof method quite di erent relies heavily approximate matrix multiplication result uses ho man inequality 
addition provide proof direct signi cantly improved bound respect spectral norm 
results technically quite complex corresponding proofs norms linear additional space time framework 
results context clustering applications included completeness provide motivation clarity complex constant time results :10.1.1.5.2580:10.1.1.37.9724
provides summary results linear constant time models shows number rows columns sampled sucient ensure high probability additional error kak see section discussion 
related achlioptas mcsherry computed low rank approximations somewhat di erent sampling techniques 
primary focus introducing methods accelerate orthogonal iteration lanczos iteration commonly methods computing low rank approximations matrix 
included comparison methods results :10.1.1.5.2580:10.1.1.37.9724
algorithms come mathematically rigorous guarantees running time quality approximation produced 
far know called incremental svd algorithms bring data possible memory compute svd update svd incremental fashion remaining data come guarantees 
section applications areas deal large matrices discussed section provide review relevant linear algebra pass ecient model approximate matrix multiplication result extensively 
section linear additional space time approximation algorithm algorithm analyzed section constant additional space time approximation algorithm algorithm analyzed 
section discussion 
applications numerous applications data approximated low rank matrix 
section discuss applications provide motivation algorithms 
latent semantic indexing latent semantic indexing general technique analyzing collection documents assumed related :10.1.1.117.3373:10.1.1.131.5084:10.1.1.108.8490
approaches retrieving textual information databases depend lexical match words query words document inaccurate users want retrieve information basis conceptual content individual words general provide reliable evidence conceptual topic document 
latent semantic indexing lsi alternative matching method attempts overcome problems associated lexical matching assuming underlying latent semantic structure partially obscured variability word choice techniques svd remove noise estimate latent structure 
suppose documents terms occur documents 
latent semantic structure analysis starts term document matrix matrix ij frequency th term th document 
topic modeled vector non negative reals summing th component topic vector interpreted frequency th term occurs discussion topic 
assumption number topics documents small relative number unique terms argued nding set topics best describe documents corresponds keeping top singular vectors important underlying structure association terms documents kept noise variability word usage removed 
dna microarray data dna microarray technology study variety biological processes permits monitoring expression levels thousands genes range experimental conditions 
depending particular technology absolute relative expression levels genes model organisms may constitute nearly entire genome probed simultaneously single microarray 
series arrays probe genome wide expression levels di erent samples di erent experimental conditions 
data microarray experiments may represented matrix ij represents expression level gene experimental condition matrix relative expression level th gene condition relative expression level gene th condition may easily extracted 
matrix low rank small number corresponding left right singular vectors sucient capture gene expression information 
removing rest correspond noise experimental artifacts enables meaningful comparison expression levels di erent genes 
processing modeling genome wide expression data svd low rank approximation provides framework mathematical variables operations suggest assigned biological meaning terms cellular regulatory processes cellular states may hidden original data due experimental noise hidden dependencies 
expression data inference tasks identify genes expression predict regulatory elements reverse engineer transcription networks inference dicult noise dependencies 
eigenfaces facial recognition applications svd low rank approximations computer vision include pattern estimation image compression restoration facial object recognition concept eigenfaces useful :10.1.1.12.7580
goal facial recognition recognize certain face database photographs human faces variations lighting conditions pose viewpoints 
common approach represent database matrix rows matrix images represented vectors 
images size matrix represents database images ij th pixel value th image 
typically singular vectors needed accurate reconstruction image singular vectors needed extract major appearance characteristics image 
right singular vectors matrix known eigenfaces principal components eigenvectors associated correlation matrix set face images 
eigenfaces computed project database photographs lower dimensional space spans signi cant variations known facial images 
new image projected low dimensional space position compared images database 
web search model problem extract information network structure hyperlinked environment world wide web considered kleinberg :10.1.1.120.3875
interest example wants nd web pages relevant query keyword web search program obvious endogenous measure authoritative page favor text ranking system 
starting set pages returned text search engine document de ned authority documents returned search point hypertext link 
document de ned hub points documents 
generally suppose documents returned search engine 
matrix de ned ij depending th document points th document 
kleinberg attempts nd vectors hub weight document authority weight document argues desirable nd max jxj jyj ay 
denotes euclidean length maximizing expects hub weights authority weights mutually consistent 
simply problem nding singular vectors large judiciously chooses submatrix computes singular vectors 
case key word multiple meanings top singular vectors large singular values interesting 
interest nd largest singular vectors form small problem considering nd singular vectors submatrix randomly chosen 
review relevant background section contains review linear algebra useful detail see 
section contains review pass ecient model data streaming computation provides framework svd results may viewed matrix multiplication result extensively proofs see details 
review linear algebra vector denote th element jxj jx 
matrix denote th column column vector denote th row row vector ij denotes th element ij 
range range fy ax span rank rank dimension range equal number linearly independent columns equal rank equals number linearly independent rows null space null fx ax matrix denote matrix norms kak subscripts distinguish various norms 
particular interest frobenius norm de ned kak ij tr matrix trace sum diagonal elements kak tr tr aa interest spectral norm de ned kak sup jxj norms invariant related kak kak kak norms provide measure size matrix note exists jxj ax kak fx basis kak ax exist orthogonal matrices av diag minfm ng 
equivalently matrices constitute singular value decomposition svd singular values vectors th left th right singular vectors respectively 
columns satisfy relations av symmetric matrices left right singular vectors 
singular values non negative square roots eigenvalues aa furthermore columns left singular vectors eigenvectors aa columns right singular vectors eigenvectors svd reveal important information structure matrix 
de ne rank null span range span 
denote matrix consisting rst columns denote matrix consisting rst columns denote principal sub matrix note dyadic decomposition property provides canonical description matrix sum rank matrices decreasing importance 
de ne av projection space spanned top singular vectors furthermore distance measured 

rank approximation minimized min rank ka dk ka min rank ka dk ka constructed largest singular triplets optimal rank approximation respect 

generally show kak kak perturbation theory matrices known size di erence matrices bound di erence singular value spectrum matrices 
particular max kek kek inequality known ho man inequality 
review pass ecient model pass ecient model data streaming computation computational model motivated observation modern computers amount disk storage sequential access memory increased rapidly ram computing speeds increased substantially slower pace 
pass ecient model scarce computational resources number passes data additional ram space additional time required algorithm 
data assumed stored disk consist elements size bounded constant algorithm read tape 
see details 
review matrix multiplication algorithm approximate product matrices analyzed 
algorithm input matrices probability distribution fp number returns output matrices cr ab matrix columns randomly chosen columns suitably rescaled matrix rows corresponding rows suitably rescaled 
important aspect algorithm probability distribution fp choose column row pairs 
uniform distribution superior results obtained probabilities chosen judiciously 
particular set sampling probabilities fp nearly optimal probabilities form optimal probabilities respect approximating product ab form 
prove theorem 
theorem suppose fp positive constant construct algorithm cr approximation ab 
kab kak kbk furthermore log 
probability kab kak kbk shown pass matrices nearly optimal probabilities constructed 
particularly interested case case select algorithm random samples drawn nearly optimal probabilities additional space time 
linear time svd approximation algorithm algorithm matrix wish approximate top singular values corresponding singular vectors constant number passes data additional space time algorithm input fp 
output pick pr set cp compute singular value decomposition say compute cy return algorithm fv fu uc fh vc fy diagram algorithm 
strategy algorithm pick columns matrix rescale appropriate factor form matrix compute singular values corresponding left singular vectors matrix approximations singular values left singular vectors sense precise 
calculated performing svd matrix compute right singular vectors calculating left singular vectors algorithm described takes input matrix returns output approximation top left singular values corresponding singular vectors 
note construction svd diagram illustrating action algorithm 
transformation represented matrix shown svd transformation represented matrix shown svd 
shown probabilities fp chosen judiciously left singular vectors high probability approximations left singular vectors section show algorithm takes linear additional space time section prove correctness algorithm 
analysis implementation running time assuming nearly optimal sampling probabilities de ned section algorithm sampling probabilities select columns sampled pass additional space time select algorithm 
elements sampled matrix constructed additional pass requires additional space time mc 
computing requires mc additional space time computing svd requires additional space time 
computing requires matrix vector multiplications total mck additional space time 
assumed constant additional space time required algorithm 
note description solution computable allotted additional space time explicit approximation top singular values corresponding left singular vectors 
analysis sampling step approximating incurs error equal ka ka optimal rank approximation respect 

show addition error matrix error depends aa cc results theorem show additional error depends kak rst consider obtaining bound respect frobenius norm 
theorem suppose constructed algorithm 
ka aa cc proof recall matrices kxk tr tr tr tr may express tr tr tr tr kak may relate aa cc aa cc rst inequality follows applying cauchy schwartz inequality inequality follows writing aa cc respect basis containing fh applying cauchy schwartz inequality noting xx matrix applying ho man inequality may relate 
cc aa 
cc aa 
cc aa combining results allows relate aa cc combining yields theorem 
prove similar result spectral norm note factor 
theorem suppose constructed algorithm 
ka aa cc proof range span hm orthogonal complement hm max jxj max jyj jzj max jyj max hm jzj max hm jzj follows follows hm bound cc aa cc 
aa cc aa cc ka aa cc follows max hm jz cj occurs st left singular vector maximum possible hm subspace 
follows cc aa aa cc follows ka 
theorem follows combining 
theorem theorem hold regardless sampling probabilities fp ka property matrix choice sampling probabilities enters error ka ak term involving additional error optimal rank approximation term cc additional error theorem depends aa cc note aa cc aa cc bound quantity bound 
note additional error 


theorem specialize sampling probabilities nearly optimal choosing columns error approximation svd arbitrarily small 
theorem suppose constructed algorithm sampling columns probabilities fp kak positive log 

ka kak probability ka kak addition ka kak probability ka kak proof combining theorems theorem ka kak ka kak probability ka kak ka kak theorem follows appropriate value note alternatively sample rows columns matrix case modi ed version algorithm leads results analogous theorem theorem 
constant time svd approximation algorithm algorithm matrix wish approximate top singular values corresponding singular vectors constant number passes data additional space time independent strategy algorithm pick columns matrix rescale appropriate factor form matrix compute approximations singular values left singular vectors matrix approximations singular values left singular vectors algorithm section left singular vectors matrix computed exactly analysis section showed computation takes additional space time 
algorithm order additional space time sampling performed drawing rows construct matrix svd computed singular values corresponding singular vectors obtained high probability approximations singular values singular vectors singular values right singular vectors algorithm described takes input matrix returns output description approximation top left singular values corresponding singular vectors 
description approximations left singular vectors may expense additional pass linear additional space time converted explicit approximation left singular vectors compute columns approximations left singular vectors note algorithm introduced bound small singular values may perturbed second level sampling indicated particular value chosen depends norm bound desired 
note probabilities fq algorithm optimal sense section probabilities fp enter theorem 
diagram illustrating action algorithm 
transformation represented matrix represented svd transformation represented matrix shown note svd shown 
transformation represented matrix constructed second level sampling shown svd 
addition approximations right singular vectors left singular vectors calculated shown 
section show algorithm takes additional space time 
section state theorem establish correctness algorithm theorem main result section analogue theorem 
section prove theorem 
analysis implementation running time assuming optimal sampling probabilities de ned section algorithm sampling probabilities select columns sampled pass additional space time select algorithm 
columns sampled explicitly construct matrix perform second level sampling select rows probabilities fq described algorithm order construct matrix performing algorithm input min fp 
output description pick pr save cg 
set cp 
note explicitly constructed ram 
choose fq kck pick pr set wq compute singular value decomposition 
say 
bound desired set 
bound desired set 
minfk kwk gg 
return singular values corresponding singular vectors algorithm fv fu vw fz uw diagram algorithm second pass additional space time select algorithm 
third pass explicitly construct requires additional space time cw 
computing requires cw additional space time computing svd requires additional space time 
singular values corresponding singular vectors computed returned description solution total time algorithm assumed constant 
explicitly compute require matrix vector multiplications require pass data mck additional space time 
statement theorem subsection provide analysis algorithm similar analysis algorithm section 
recall section interested bounding 
case orthonormal projection rank approximation constant time model access columns cz form orthonormal set 
lemma section constructed sampling optimal probabilities high probability columns approximately orthonormal approximately orthonormal projection 
applying get low rank approximation 
note dealing original proof contained small error corrected journal version 
section notation 
recall svd de ne matrix columns th th singular vectors cz diagonal matrix elements tt 
addition svd de ne matrix 

tz see 
measure degree columns orthonormal 
theorem constant time analogue theorem main result section 
note results sampling second step sampling matrix form matrix depend samples chosen rst sampling step state results expectation state high probability 
theorem suppose description constructed algorithm sampling columns probabilities fp rows probabilities fq kak kck log 
frobenius norm bound desired algorithm run choosing columns rows probability ka kak spectral norm bound desired algorithm run choosing columns rows probability ka kak proof see section 
recall section rst proved theorems provided bound respectively arbitrary probabilities proved theorem nearly optimal probabilities 
similar presentation strategy adopted section interests simplicity due technically complicated proofs constant time model immediately restrict theorem case optimal sampling probabilities defer proofs supporting lemmas section 
proof theorem section prove theorem 
start section lemmas common frobenius spectral norms 
section provide proof 
section provide proof 
general lemmas section prove lemmas proofs frobenius spectral norm results 
relate plus error term columns orthonormal allow bound arguments similar bound theorems 
lemma kak proof subadditivity kak lemma follows 
second vectors cz general form orthonormal set expect construction matrix close matrix high probability approximately orthonormal 
lemma establishes 
de ned characterizes far having orthonormal columns shows error introduced due bounded simple function error introduced second level sampling 
lemma written basis respect 
furthermore 
kwk proof recall cz th column tt tt kronecker delta function 
note cz 
tt note cz 
tt 
tt kwk tt follows 
kwk similarly 
krk kz kwk lemma follows 
third consider second term lemma show related 
lemma 
proof fourth lemma considers special case probabilities fp entered algorithm optimal case theorem 
lemma constructed algorithm sampling columns probabilities fp rows probabilities fq pr kak pr kck kwk kck kak proof kak kck ja cp kak similarly kck kwk jc wq kck lemma follows 
lemmas frobenius norm proof section prove 
rst proving lemmas sucient bound combined lemmas section obtain bound bound depends error optimal rank approximation ka additional errors depend quality sampling approximations aa cc analogue theorem applied constant additional space time model 
result associated proof similar structure theorem complicated due vectors involve additional error terms levels approximation involved 
prove lemmas provide bound rst term lemma applied frobenius norm 
rst rewrite term lemma 
note lemma constant time analogue 
lemma kak proof tr 
tr want provide lower bound terms singular values steps 
relate note assumption 
theorem optimal probabilities suciently columns rows drawn assumption dropped bounds form theorem may obtained slightly worse sampling complexity 
lemma 

proof rst provide upper bound terms note max max largest singular value max max follows ki 
provide lower bound terms tr tr 
tr tr 

denoting th column row matrix respectively follows tr 




combining 



lemma follows 
second relate lemma 
aa cc proof tr aa tr cc tr aa cc aa cc inequality follows tr aa cc aa cc aa cc lemma follows 


tt 
third relate 
lemma proof cz wz 
kwk second inequality uses kxk matrix matrix orthonormal columns 
multiplying right hand side ignoring terms reinforce inequality lemma follows kwk 
combining lemmas desired bound terms singular values matrix perturbation theory relate 
lemma aa cc kwk proof recalling ho man inequality see 
cc aa 
aa cc similarly 
ww cc 
combining see aa cc kwk kwk combining allows relate establishing lemma 
combine results order prove 
aa aa cc establish lower bound combining lemmas dropping terms reinforce inequality 

ke aa combining lemmas dropping terms reinforce inequality ke aa ke 

ke aa kwk lemma immediately leads upper bound ka ke aa ke 

ke aa kwk lemmas 
kak recall kak 
ke kwk lemma kwk kck kak lemma follows combining sampling probabilities indicated statement theorem choosing 
lemmas spectral norm proof section prove 
rst proving lemmas sucient bound combined lemmas section obtain bound bound depends error optimal rank approximation ka additional errors depend quality sampling approximations aa cc analogue theorem applied constant additional space time model 
result associated proof similar structure theorem complicated due vectors involve additional error terms levels approximation involved 
prove lemmas provide bound rst term lemma applied spectral norm 
rst rewrite term lemma 
lemma cz cz aa cc proof order bound project subspace spanned orthogonal complement manner analogous proof theorem 
range bm orthogonal complement bm 
max jxj max jyj jzj max jyj max bm jzj max bm jzj follows follows bm bound bm jzj aa 
cc 
aa cc 
cc cz 
cz 
aa cc 
cz 
cz 
aa cc 
follows zz cz cz implies cz bm combining cz cz aa cc lemma follows xx matrix bound cz term lemma note matrix perturbation theory 
lemma cz ka aa cc proof note cz wz kz wz follows cz double application see aa cc lemma follows combining ka 
bound cz term lemma note unnecessary 
lemma cz kwk proof note cz wz kz wz follows cz lemma follows kwk combine results order prove 
recall aa aa cc combining lemmas ka ke aa ke kwk lemmas 
kak recall 


ke kwk lemma follows combining sampling probabilities indicated statement theorem choosing 
discussion algorithms compute approximations svd matrix require stored ram additional space time required addition constant number passes matrix linear constant independent proven error bounds algorithms respect frobenius spectral norms 
section presents summary dependence sampling complexity 
algorithm additional error optimal rank approximation spectral norm bound kak sampling columns additional error frobenius norm kak sampling columns 
likewise algorithm additional error spectral norm kak sampling columns rows additional error frobenius norm kak sampling columns rows 
results require columns rows frobenius spectral norm bound 
focused developing new techniques proving lower bounds number queries sampling algorithm required perform order approximate function accurately low probability error 
methods applied low rank matrix approximation problem de ned approximating svd respect frobenius norm matrix reconstruction problem 
shown sampling algorithm high probability nds low rank approximation requires queries 
addition shown algorithm exact weight distribution columns matrix require column queries approximate algorithm see original optimal respect frobenius norm bounds rank parameter algorithm see original optimal respect frobenius norm bounds polynomial factors :10.1.1.5.2580:10.1.1.5.2580:10.1.1.37.9724
acknowledgments individuals comments fruitful discussions dimitris achlioptas alan frieze mauro frank mcsherry santosh vempala 
dimitris achlioptas frank mcsherry providing preprint journal version provides useful comparison results 
national science foundation partial support 
achlioptas mcsherry 
fast computation low rank matrix approximations 
submitted 
achlioptas mcsherry 
fast computation low rank matrix approximations 
proceedings rd annual acm symposium theory computing pages 
alter brown botstein 
singular value decomposition genome wide expression data processing modeling 
proceedings national academy sciences 
bar yossef 
complexity massive data set computations 
phd thesis university california berkeley 
bar yossef 
sampling lower bounds information theory 
proceedings th annual acm symposium theory computing pages 
berry 
matrices vector spaces information retrieval 
siam review 
berry dumais brian 
linear algebra intelligent information retrieval 
siam review 
bhatia 
matrix analysis 
springer verlag new york 
deerwester dumais furnas landauer harshman 
indexing latent semantic analysis 
journal american society information science 
drineas frieze kannan vempala vinay :10.1.1.37.9724
clustering large graphs matrices 
submitted 
drineas frieze kannan vempala vinay :10.1.1.5.2580:10.1.1.5.2580:10.1.1.37.9724
clustering large graphs matrices 
proceedings th annual acm siam symposium discrete algorithms pages 
drineas kannan 
fast monte carlo algorithms approximate matrix multiplication 
proceedings nd annual ieee symposium foundations computer science pages 
drineas kannan 
pass ecient algorithms approximating large matrices 
proceedings th annual acm siam symposium discrete algorithms pages 
drineas kannan mahoney 
unpublished results 
drineas kannan mahoney 
fast monte carlo algorithms matrices approximating matrix multiplication 
technical report yaleu dcs tr yale university department computer science new haven ct february 
drineas kannan mahoney :10.1.1.126.5994
fast monte carlo algorithms matrices iii computing compressed approximate matrix decomposition 
technical report yaleu dcs tr yale university department computer science new haven ct february 
frieze kannan vempala 
fast monte carlo algorithms nding low rank approximations 
submitted 
frieze kannan vempala 
fast monte carlo algorithms nding low rank approximations 
proceedings th annual ieee symposium foundations computer science pages 
golub van loan 
matrix computations 
johns hopkins university press baltimore 
horn johnson 
matrix analysis 
cambridge university press new york 
indyk 
stable distributions pseudorandom generators embeddings data stream computation 
proceedings st annual ieee symposium foundations computer science pages 
kleinberg 
algorithms nearest neighbor search high dimensions 
proceedings th annual acm symposium theory computing pages 
kleinberg :10.1.1.120.3875
authoritative sources hyperlinked environment 
proceedings th annual acm siam symposium discrete algorithms pages 
mardia kent bibby 
multivariate analysis 
academic press london 
murase nayar 
visual learning recognition objects appearance 
international journal computer vision 
papadimitriou raghavan tamaki vempala 
latent semantic indexing probabilistic analysis 
proceedings th acm sigact sigmod sigart symposium principles database systems pages 
stuart altman 
principal component analysis summarize microarray experiments application time series 
paci symposium biocomputing pages 
stewart sun 
matrix perturbation theory 
academic press new york 
cantor sherlock brown hastie tibshirani botstein altman 
missing value estimation methods dna microarrays 
bioinformatics 
turk pentland 
eigenfaces recognition 
journal cognitive neuroscience 
vempala 
random projection new approach vlsi layout 
proceedings th annual ieee symposium foundations computer science pages 

