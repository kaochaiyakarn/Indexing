word sense discrimination clustering contexts vector similarity spaces ted pedersen department computer science university minnesota mn usa umn edu sourceforge net systematically compares unsupervised word sense discrimination techniques cluster instances target word occur raw text vector similarity spaces 
context instance represented vector high dimensional feature space 
discrimination achieved clustering context vectors directly vector space finding pairwise similarities vectors clustering similarity space 
employ different representations context target word occurs 
order context vectors represent context instance target word vector features occur context 
second order context vectors indirect representation context average vectors represent words occur context 
evaluate discriminated clusters carrying experiments sense tagged instances senseval words known line hard serve sense tagged corpora 
words natural language multiple possible meanings determined considering context occur 
target word number different contexts word sense discrimination process grouping instances target word determining contexts similar 
motivated miller charles hypothesize words similar meanings similar contexts 
word sense discrimination reduces problem finding classes similar contexts class represents single word sense 
put way contexts grouped class represent particular word sense 
previous sense discrimination sch tze pedersen bruce pedersen bruce sch tze fukumoto suzuki comparison devoted word sense disambiguation process assigning meaning word predefined set possibilities :10.1.1.126.7186
solutions disambiguation usually require availability external knowledge source manually created sense tagged training data 
knowledge intensive methods difficult adapt new domains 
contrast word sense discrimination unsupervised clustering problem 
attractive methodology knowledge lean approach evidence simple raw text 
manually sense tagged text required specific knowledge rich resources dictionaries ontologies 
instances clustered mutual contextual similarities completely computed text 
presents systematic comparison discrimination techniques suggested pedersen bruce pedersen bruce pedersen bruce sch tze sch tze sch tze :10.1.1.126.7186
proposes evaluates extensions techniques 
summary previous discussion features types context vectors 
summarize techniques clustering vector versus similarity spaces experimental methodology including discussion data experiments 
describe approach evaluation unsupervised word sense discrimination 
analysis experimental results conclude directions 
previous pedersen bruce pedersen bruce propose dis similarity discrimination approach computes dis similarity pair instances target word :10.1.1.126.7186
information recorded dis similarity matrix rows columns represent instances target word discriminated 
cell entries matrix show degree pair instances represented corresponding row column dis similar 
dis similarity computed order context vectors instances show instance vector features directly occur near target word instance 
sch tze introduces second order context vectors represent instance averaging feature vectors content words occur context target word instance 
second order context vectors input clustering algorithm clusters contexts vector space building similarity matrix structure 
significant differences approaches suggested pedersen bruce sch tze 
systematic study determine set techniques results better sense discrimination 
sections follow highlight differences approaches 
context representation pedersen bruce represent context test instance vector features directly occur near target word instance 
refer representation order context vector 
sch tze contrast uses second order context representation averages order context vectors individual features occur near target word instance 
sch tze represents feature vector words occur context computes context target word adding feature vectors significant content words occur near target word context 
features pedersen bruce small number local features include occurrence part speech information near target word 
select features test data discriminated common practice clustering general 
sch tze represents contexts high dimensional feature space created separate large corpus referred training corpus 
selects features frequency counts log likelihood ratios corpus 
adopt sch tze approach select features separate corpus training data part number test instances may relatively small may suitable selecting feature set 
addition possible explore variations training data maintaining consistent test set 
training data unsupervised clustering need sense tagged plan develop methods collecting large amounts raw corpora web online sources extract features 
sch tze represents feature vector words occur feature training data 
feature vectors fact order context vectors feature words target word 
words occur feature words form dimensions feature space 
sch tze reduces dimensionality feature space singular value decomposition svd employed related techniques latent semantic indexing deerwester latent semantic analysis landauer 
svd effect converting word level feature space concept level semantic space smoothes fine distinctions features represent similar concepts 
clustering space pedersen bruce represent instances dis similarity space instance seen point distance points function mutual dis similarities 
dis similarity matrix showing pair wise dis similarities instances input agglomerative clustering algorithm 
context group discrimination method sch tze hand operates vector representations instances works vector space 
employs hybrid clustering approach uses agglomerative estimation maximization em algorithm 
order context vectors order context vectors directly indicate features context 
experiments context target word limited surrounding content words side 
true selecting features set training data converting test instances vectors clustering 
particular features interested bigrams occurrences 
occurrences words occur positions target word intervening words allowed 
bigrams ordered pairs words occur positions 
occurrences unordered word pairs include target word bigrams ordered pairs may may include target 
occurrences bigrams occur instances training data words log likelihood ratio excess effect removing occurrences bigrams chance independent target word 
selecting set occurrences bigrams corpus training data order context representation created test instance 
shows times feature occurs context target word positions target word instance 
second order context vectors test instance represented second order context vector finding average order context vectors associated words occur near target word 
second order context representation relies order context vectors feature words 
second order experiments different types features occurrences bigrams defined order experiments 
occurrence identified training data assigned unique index occupies corresponding row column word occurrence matrix 
constructed occurrence pairs symmetric adjacency matrix cell values show loglikelihood ratio pair words representing corresponding row column 
row occurrence matrix seen order context vector word represented row 
set words forming rows columns occurrence matrix treated feature words 
bigram features lead bigram matrix selected bigram represents single row say th row represents single column say th column bigram matrix 
value cell indicates log likelihood ratio words bigram 
row bigram matrix seen bigram vector shows scores bigrams word represented row occurs word 
words representing rows bigram matrix feature set words representing columns form dimensions feature space 
clustering objective clustering take set instances represented similarity matrix context vectors cluster instances instances belong clusters 
clustering algorithms classified main categories hierarchical partitional hybrid methods incorporate ideas 
algorithm acts search strategy dictates proceed instances 
actual choice clusters split merge decided criteria function 
section describes clustering algorithms criteria functions employed experiments 
hierarchical hierarchical algorithms agglomerative divisive 
proceed iteratively merge divide clusters step 
agglomerative algorithms start instance separate cluster merge pair clusters iteration single cluster remaining 
divisive methods start instances cluster split cluster iteration instances cluster 
widely known criteria functions hierarchical agglomerative algorithms single link complete link average link known upgma 
sch tze points single link clustering tends place instances single elongated cluster pedersen bruce show hierarchical agglomerative clustering average link method fares 
chosen average link upgma criteria function agglomerative experiments 
similarity space instance viewed node weighted graph 
weights edges joining nodes indicate pairwise similarity measured cosine context vectors represent pair instances 
agglomerative clustering starts node cluster considered centroid cluster 
iteration average link selects pair clusters centroids similar merges single cluster 
example suppose clusters merged single cluster ij 
weights edges connect existing nodes new node ij revised 
suppose node 
new weight graph computed averaging weight edge nodes edge words ij vector space average link starts assigning vector single cluster 
centroid cluster calculating average context vectors cluster 
iteration average link selects pair clusters centroids closest respect cosines 
selected pair clusters merged centroid computed newly created cluster 
partitional partitional algorithms divide entire set instances predetermined number clusters going series pairwise comparisons 
methods somewhat faster hierarchical algorithms 
example known means algorithm partitional 
vector space instance represented context vector 
means initially selects random vectors serve centroids initial clusters 
assigns vector clusters centroid closest vector 
vectors assigned recomputes cluster centroids averaging vectors assigned cluster 
repeats convergence vector changes cluster iterations centroids stabilize 
similarity space instance viewed node fully connected weighted graph edges indicate similarity instances connect 
means select random nodes represent centroids initial clusters 
assign node clusters edge joining centroid cluster maximum weight edges joining centroids 
hybrid methods generally believed quality clustering partitional algorithms inferior agglomerative methods 
study zhao karypis suggested experiments conducted smaller data sets larger data sets partitional algorithms faster lead better results 
particular zhao karypis recommend hybrid approach known repeated bisections 
overcomes main weakness partitional approaches instability clustering solutions due choice initial random centroids 
repeated bisections starts instances single cluster 
iteration selects cluster bisection optimizes chosen criteria function 
cluster bisected standard means method criteria function maximizes similarity instance centroid cluster assigned 
hybrid method combines hierarchical divisive approach partitioning 
experimental data words senseval sense tagged corpus line hard serve sense tagged corpora 
corpora instances consist sentences include single target word manually assigned sense tag 
ignore sense tags times evaluation 
point sense tags enter clustering feature selection processes 
clear believe unsupervised word sense discrimination needs carried relative pre existing set senses 
fact great advantages unsupervised technique doesn need manually annotated text 
employ sense tagged text order evaluate clusters discover 
senseval data divided training test sets splits retained experiments 
senseval data relatively small word approximately training test instances 
data particularly challenging unsupervised algorithms due large number fine grained senses generally word 
small volume data combined large number possible senses leads small set examples senses 
result prior clustering filter training test data independently instance uses sense occurs available instances word removed 
eliminate words training instances filtering 
process leaves set senseval words includes nouns adjectives verbs shown table 
creating evaluation standard assume instance assigned single cluster 
instance multiple correct senses associated treat frequent desired tag ignore possible correct answers test data 
line hard serve corpora standard training test split randomly divided training test splits 
due large number training test instances words filtered instances associated sense occurred training test instances 
randomly selected pairs words senseval data mixed instances retaining training test distinction existed data 
mixing data filtered sense training test data new mixed sample removed total number instances mixed pairs sum individual words 
mix words created order provide data included fine grained coarse grained distinctions 
table shows words experiments parts speech 
show number training trn test instances tst remain filtering number senses test data 
show percentage majority sense test data maj 
particularly useful accuracy attained baseline clustering algorithm puts test instances single cluster 
evaluation technique cluster test instances specify upper limit number clusters discovered 
experiments value 
reflects fact know priori number possible senses word 
allows verify hypothesis clustering approach automatically discover approximately number clusters senses word extra clusters actual senses contain instances 
seen column table words senses average 
clusters created algorithm detect significant clusters ignoring throwing clusters contain total instances 
instances discarded clusters counted unclustered instances subtracted total number instances 
basic strategy evaluation assign available sense tags discovered clusters assignment leads maximally accurate mapping senses clusters 
problem assigning senses clusters reordering columns confusion matrix shows senses clusters align diagonal sum maximized 
corresponds known problems assignment problem operations research determining maximal matching bipartite graph graph theory 
evaluation assign sense cluster vice versa 
number discovered clusters number senses mapping 
number clusters greater number actual senses clusters left unassigned 
number senses greater number clusters senses assigned cluster 
reason assigning single sense multiple clusters multiple senses cluster assuming sense instance sense cluster 
measure precision recall maximally accurate assignment sense tags clusters 
precision defined number instances clustered correctly divided number instances clustered recall number instances clustered correctly total number instances 
compute measure times precision recall divided sum precision recall 
experimental results discrimination results configurations features context representations clustering algorithms 
run target words mixed words 
follows concise description configuration 
pb order context vectors occurrence features clustered similarity space upgma technique 
pb pb order context vectors clustered vector space repeated bisections 
pb pb order context vectors bigram features occurrences 
pb experiments order context representations correspond approach suggested pedersen bruce 
sc second order context vectors instances clustered vector space repeated bisections technique 
context vectors created word occurrence matrix dimensions reduced svd 
sc sc second order context vectors converted similarity matrix clustered upgma method 
sc sc second order context vectors created bigram matrix 
sc experiments second order context vectors follow approach suggested sch tze 
experiment pb clusters pedersen bruce style order context vectors sch tze clustering scheme sc tries see effect pedersen bruce style clustering method sch tze style second order context vectors 
motivation experiments pb sc try bigram features pb sc style context vectors 
measure associated discrimination word shown table 
score significantly greater majority sense paired test shown bold face 
analysis discussion employ different types data experiments 
senseval words relatively small number training test instances 
line hard serve data larger word pos trn tst pb sc pb sc pb sc maj art authority bar channel child church circuit day facility feeling grip material mouth post blind cool fine free natural simple leave live train line hard serve cool train fine cool fine grip leave post post grip contains training test instances combined 
mixed word unique combined instances multiple target words larger number senses discriminate 
type data brings unique characteristics sheds light different aspects experiments 
senseval data table compares pb pb sc sc methods discriminate senseval words 
objective study effect bigram features occurrences pb second sc order context vectors relatively small amounts training data word 
note pb sc occurrence features pb sc rely bigram features 
table measures table shows number nouns adjectives verbs bigrams effective occurrences bigram occur effective bigram occur effect bigram cooccur 
table shows clear advantage bigrams occurrence features order context vectors pb 
bigram features show clear improvement results second order context vectors sc 
hypothesis order context vectors pb represent small set bigram features selected relatively small senseval words 
features sparse instances share common features instances making order clustering difficult 
bigram occur pb bigram occur bigram occur bigram occur sc bigram occur bigram occur table bigrams vs occurrences pb rbr upgma rbr upgma rbr upgma sc rbr upgma rbr upgma rbr upgma table repeated bisections vs upgma second order context vectors indirectly represent bigram features require exact match vectors order establish similarity 
poor performance bigrams case order context vectors suggests dealing small amounts data need boost enrich bigram feature set larger training source corpus drawn web 
table shows results repeated bisections algorithm vector space pb upgma method similarity space 
table shows number nouns adjectives verbs senseval words performed better rbr upgma worse rbr upgma equal rbr upgma repeated bisections clustering versus upgma technique pb second sc order vectors 
short table compares pb pb sc sc 
observe order second order context vectors repeated bisections effective upgma 
suggests better suited deal small amounts sparse data 
table summarizes performance experiments compared majority class 
table shows number words experiment performed better majority class broken part speech 
note sc sc better majority class followed closely pb sc 
suggests second order context vectors sc advantage order vectors small training data senseval words 
believe second order methods better total sc maj sc maj pb maj sc maj pb maj pb maj table vs majority class smaller amounts data feature spaces quite small able support degree exact matching features instances order vectors require 
second order context vectors succeed cases find indirect second order occurrences feature words describe context extensively order representations 
smaller quantities data possibility finding instances exactly set words 
semantically related instances words conceptually lexically 
second order context vectors designed identify relationships exact matching required words occur similar contexts similar vectors 
line hard serve data comparatively performance pb pb case line hard serve data see table suggests order context vectors clustered upgma perform relatively larger samples data 
sc experiments data performance sc relatively high 
suggests upgma performs better repeated bisections larger amounts training data 
observations correspond hypothesis drawn senseval results 
large amount training data lead larger feature space greater chance matching features directly context test instances 
order context vectors rely immediate context target word succeed contexts similar sets words turn selected large feature collection 
mix word results nearly experiments carried different methods perform better majority sense case mix words 
partially due fact words large number senses low majority classifiers 
addition recall data created mixing instances distinct target words leads subset coarse grained distinct senses data easier discover senses single word 
table shows top experiments mixed words second order vectors sc 
believe due sparsity feature spaces data 
different senses number order features required correctly discriminate high leading better results second order vectors 
directions plan conduct experiments compare effect large amounts training data versus smaller amounts instance includes target word case 
draw large corpora variety sources including british national corpus english corpus web 
motivation larger corpora provide generic occurrence information words regard particular target word 
data specific target word capture word usages immediate context target word 
test hypothesis smaller sample data instance includes target word effective sense discrimination general corpus training data 
planning automatically attach descriptive labels discovered clusters capture underlying word sense 
labels created characteristic features instances belonging cluster 
comparing descriptive features cluster words occur actual dictionary definitions target word plan carry fully automated word sense disambiguation rely manually annotated text 
extensive comparative analysis word sense discrimination techniques order second order context vectors employed similarity vector space 
conclude larger amounts homogeneous data line hard serve data order context vector representation upgma clustering algorithm effective word sense discrimination 
believe case large sample data features occur training data occur test data making possible represent test instances fairly rich feature sets 
smaller amounts data senseval second order context vectors hybrid clustering method repeated bisections perform better 
occurs small sparse data direct order features seldom observed training test data 
indirect second order occurrence relationships captured methods provide sufficient information discrimination proceed 
acknowledgments research supported national science foundation faculty early career development award 
experiments carried version package freely available url shown title page 
deerwester dumais furnas landauer harshman 

indexing latent semantic analysis 
journal american society information science 
fukumoto suzuki 

word sense disambiguation untagged text term weight learning 
proceedings ninth conference european chapter association computational linguistics pages bergen 
landauer foltz laham 

latent semantic analysis 
discourse processes 
miller charles 

contextual correlates semantic similarity 
language cognitive processes 
pedersen bruce 

distinguishing word senses untagged text 
proceedings second conference empirical methods natural language processing pages providence ri august 
pedersen bruce 

knowledge lean word sense disambiguation 
proceedings fifteenth national conference artificial intelligence pages madison wi july 


discriminating word senses similarity analysis 
proceedings hlt naacl student research workshop pages edmonton alberta canada may sch tze 

dimensions meaning 
proceedings supercomputing pages minneapolis mn 
sch tze 

automatic word sense discrimination 
computational linguistics 
zhao karypis 

evaluation hierarchical clustering algorithms document datasets 
proceedings th conference information knowledge management cikm pages 
