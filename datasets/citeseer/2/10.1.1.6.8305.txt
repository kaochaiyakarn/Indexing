robust computer vision kernel density estimation chen peter meer electrical computer engineering department rutgers university piscataway nj usa meer caip rutgers edu 
new techniques nonparametric estimation probability densities introduced improve performance equivalent robust methods currently employed computer vision 
technique draws projection pursuit paradigm statistics carries regression weak dependence accuracy scale estimate 
second technique exploits properties multivariate adaptive mean shift accomplishes fusion uncertain measurements arising unknown number sources 
example techniques extensively algorithm recovery multiple structures heavily corrupted data 
visual data complex measurements obey parametric model 
satisfactory performance robust estimators tolerating presence outliers data 
estimators popular vision community see representative sample applications 
robust techniques estimators median squares lmeds imported statistics hough transform ransac innate developed initially solve specific vision problems 
shown robust methods widely computer vision regarded members family estimators auxiliary scale estimators smooth loss function see section preferred 
propose novel approach computing estimators 
new approach combines desirable characteristics different robust techniques computer vision suited processing complex visual data 
large class computer vision problems modeled linear variables model 
model measurements corrupted independently zero mean noise yi yi subscript denotes unknown true value 
sequel consider simplest case noise covariance ip results easily extended arbitrary covariance matrices 
linear constraint defined io 


eds eccv lncs pp 

springer verlag berlin heidelberg robust computer vision kernel density estimation constraint written form shown euclidean distance measurement yi yi orthogonal projection hyperplane defined linear constraint yi yi geometric distance algebraic distance 
optimal estimator model total squares tls technique 
presence outliers corresponding robust estimator defined argmin yi yi subject scale parameter nonnegative symmetric loss function nondecreasing having unique minimum 
class estimators considered assume 
frequently estimator loss function employed 
success estimation procedure accurate estimation model parameters rejection outliers contingent having satisfactory scale parameter 
issue breakdown point estimators lesser relevance context seen 
shown robust techniques imported statistics differ developed vision community way scale obtained 
scale estimated data value set priori 
implicitly assumed scale estimate known 
statistical techniques simultaneous estimation scale available handle complex data type considered 
similarly vision application reliable scale estimate obtained underlying physical properties 
example data containing multiple structures instances model different sets parameters structure may measured different uncertainty 
typical vision task generating data recovery motion parameters moving objects presence camera egomotion 
case single global scale value involved robust estimation procedures may satisfactory 
performance robust type regression technique proposed weak dependence accuracy scale estimate 
technique provides satisfactory inlier outlier dichotomy wider range traditional estimators 
performance improvement achieved recasting estimation kernel density estimation problem 
kernel density estimation known technique statistics pattern recognition 
see books statistical treatment sec pattern chen meer recognition description 
xi 
scalar measurements drawn arbitrary probability distribution 
kernel density estimate distribution called parzen window estimate pattern recognition obtained kernel function bandwidth nh xi 
kernel functions considered satisfy properties 
conditions kernel function density estimated satisfied practice 
symmetry kernel function allows define profile ck normalization constant determined 
importance profile revealed case multivariate kernel density estimation discussed section 
quality density estimate assessed asymptotic mean integrated error integrated mean square error true density estimate slower rate 
optimal bandwidth depends second derivative unknown density 
approximation bandwidth obtained employing simple plug rule purposes bandwidth depending kernel function raw scale estimate sec suffices med xj med xi du du data taken consideration median absolute deviations mad scale estimate 
proportionality constant chosen avoid estimated density 
section connection regression estimator univariate density estimation established exploited computational advantage 
section kernel density estimation reformulated general multivariate form provide tool solve difficult problem 

set measurements uncertainty available covariance matrices cj 
large subset measurements related robust computer vision kernel density estimation different data sources rest completely erroneous 
value known 
find best statistical sense estimates vectors covariances characterizing sources 
fundamental feature space analysis problem approach extension variable bandwidth mean shift provides simple robust solution 
section new techniques main building blocks algorithm analyzing data containing multiple structures 
success algorithm illustrated examples 
section potential proposed techniques solve difficult vision tasks remaining open issues discussed 
estimators projection pursuit projection pursuit nonparametric technique introduced statistics exploratory data analysis soon applied nonlinear regression density estimation problems 
projection pursuit seeks interesting low dimensional dimensional projections multidimensional data 
informative value projection measured projection index 
projection index scalar functional univariate probability distribution estimated data projected chosen direction 
best direction obtained numerical optimization corresponds extremum projection index 
step iterative procedure solution 
current solution updated new search initiated 
example projection pursuit regression iteration shape smooth univariate nonlinear function minimizes sum squared residuals determined 
subsequent iteration function incorporated current model 
convergence declared squared error falls threshold 
papers offer excellent reviews projection pursuit paradigm contain extensive discussions researchers working related topics 
strong connection robust linear regression estimators median squares projection pursuit procedure sec 
relationship investigated statistics theoretical considerations case traditional regression case measured variables corrupted noise 
apply projection pursuit paradigm design technique general errors variables model better suited vision tasks 
prove connection regression estimation kernel density estimation definition rewritten argmax yi argmax yi called kernel function normalization constant making proper kernel 
note symmetry loss function allows removal absolute values plugged 
figs 
loss function corresponding kernel function shown 
compare kernel weight function chen meer fig 

different functions associated estimator 
loss function 
kernel function 
weight function 
known iterative reweighted squares implementation estimators fig 

functions look similar different expressions 
unit vector defining line origin rp projections data points yi line intrinsic coordinates xi 
kernel bandwidth estimated density sequence dependence bandwidth direction projection scale estimate projected points explicit 
mode density estimate defined argmax easily computed 
comparing taken kernel function chosen close true parameter linear model satisfactory substitute scale mode provide reasonable estimate intercept 
observation linear model estimation problem reformulated argmax max projection pursuit definition estimator projection index quantity inside brackets 
equivalence perfectly rigorous scale substituted bandwidth 
advantage role scale diminished bandwidth satisfactory long secures reliable recovery mode 
bandwidth proportional mad scale estimate unreliable distribution multimodal median biased estimator nonsymmetric data 
similarly small measurement noise small introduce artifacts bandwidth bounded downward 
robust computer vision kernel density estimation fig 

projection pursuit principle parameter estimates sought examining projections data points arbitrary directions 
fig 

processing data fig 

estimated density projection direction 
detected mode marked 
estimated density projection direction maximized projection index 
points projecting inside interval marked vertical bars selected 
projection pursuit line estimate 
dashed lines bound region delineated robust postprocessing 
hough transform line estimate 
chen meer projection pursuit approach estimation clear geometric interpretation 
direction regarded unit normal candidate hyperplane fitted dimensional data yi 
bandwidth defines band centered plane 
band translated maximize points band weighted average orthogonal distances hyperplane 
estimate corresponds densest band largest weighted average 
note similarity known interpretation lmeds estimator parameter estimates correspond narrowest band containing half data points 
approach important advantage 
optimization criterion dependent preset percentage data points inliers yielding better behavior presence severely contaminated data shown example 
data points rectangle fig 
belong classes 
measurements line segment corrupted normal noise covariance 
second structure 
measurements line segment corrupted normal noise covariance indistinguishable background 
background points uniformly distributed rectangle bounded definition lmeds estimator handle data 
similarly global maximum hough accumulator built pairs points yields erroneous fits angle side bins exceeds degrees 
example shown fig 

projections data points directions shown fig 

direction computed bandwidth 
mode detected value fig 

projection index maximized direction 
resulting bandwidth mode value fig 

basin attraction mode delineated significant local minimum left right marked vertical bars fig 

define parallel lines bound region containing structure interest fig 

outliers relative structure may included robust postprocessing required 
postprocessing allows lower accuracy projection pursuit search best necessary condition searches higher dimensional spaces see section 
estimator robust postprocessing 
scale structure parameters estimated simultaneously 
inlier outlier dichotomy established robust covariance parameter estimate computed 
example final line estimate remarkable close true values spite severe contamination fig 

improvement due postprocessing small role increased projection pursuit estimator employed computational module section 
robust data fusion robust computer vision kernel density estimation problem appears forms computer vision tasks 
measurements available uncertainty described covariance matrices cj 
account uncertainties classify measurements groups number clusters data 
value known 
problem regarded data fusion task available evidence reduced minimum number plausible representations 
consider trivial case case measurements belong single group 
satisfactory estimate center underlying cluster obtained minimizing sum mahalanobis distances argmin covariances assumed full rank 
expected solution covariance weighted average measurements 
uncertain measurement inverse covariance smaller norm contributes result fusion 
compute covariance matrix uncertainty associated covariances cj approximated cj ajc 
common covariance structure positive proportionality factors aj determined minimization arg min aj cj ajc trace squared frobenius norm matrix differentiating aj matrix gradient relations connecting unknown quantities obtained jcj trace 
trace relations evaluated iteratively starting average covariance 
refined value retained 
return kernel density estimation 
radially symmetric dimensional multivariate kernel built profile ck pk ck corresponding normalization constant properties easily extended general case bandwidth replaced symmetric positive definite bandwidth matrix chen meer data points xi 
multivariate density estimate computed kernel bandwidth matrix sec kh xi kh det ck det 
note ip reduces known traditional multivariate kernel density estimation expression 
practice single bandwidth satisfactory available data points spread uniformly region existence unknown density 
sample point kernel density estimator defined khi xi data point xi considered computations bandwidth matrix hi sample point estimator superior performance relative kernel density estimators variable bandwidth associated center kernel sec 
account obtain ck det hi xi xi 
solve robust data fusion problem compute sample point density estimate measurements multivariate epanechnikov kernels built profile bandwidth matrices covariances cj employed 
covariance matrices scaled chi square value degrees freedom level confidence implementation 
cj 
kernel associated measurement nonzero region confidence measurement having coverage probability 
density estimate ck det cj 
account obtained solving minimization problem equivalent finding maximum density estimate mode 
apparent differences scalar normalization factors covariances 
robust computer vision kernel density estimation ready proceed proposed problem measurements come unknown number sources characterize sources clusters delineated shown equivalent finding significant modes density arg max 
note value determined automatically data 
mode corresponds zero gradient ck det cj function defines new profile case cj indicator function selecting data points inside region confidence defining matrix wj det cj cj expression gradient rewritten ck presence indicator function assures robustness computations 
zeros gradient expression similar computations restricted local regions long clusters reasonably separated computing centers appropriate data points 
choosing kernel epanechnikov beta family binary indicator function additional weighting introduced 
modes definition located high density regions versatile robust mode detector mean shift property introduced pattern recognition popular computer vision large variety tasks 
variable bandwidth version mean shift procedure developed 
mean shift procedure recursively evaluates second term 
procedure starts new value computed data chen meer fig 

example data fusion 
measurements regions confidence 
result multivariate variable bandwidth mean shift 
note smaller scale 
points yield nonzero values indicator function 
process repeated obtained kernels shifted result previous step 
convergence achieved shift threshold 
see details mean shift procedure 
mean shift procedure applied measurements associating measurements theirs point convergence arbitrarily shaped basins attraction defined 
note outliers isolated erroneous measurements taken account fail evolve 
points convergence characterized applying data points basin attraction 
pairs squared mahalanobis distance metrics merged 
resulting modes output robust fusion procedure 
example shown fig 

large confidence regions fig 
correspond erroneous measurements hide majority data 
robust fusion modes detected associated small uncertainty fig 

fusion technique introduced provide robust component traditional approaches combining classifiers machine learning algorithms improve performance resampling bagging 
robust regression data multiple structures data containing multiple structures characterized presence instances model case defined different set parameters 
need reliable processing data distinguishes estimation problems computer vision applied statistics 
assumption sought model carried absolute majority data points embedded robust estimators statistics 
vision tasks structure motion scene representation assumption violated information object acquired simultaneously 
main classes robust techniques employed vision see section hough transform robust computer vision kernel density estimation capability handle complex multiple structured data 
example section shown performance hough transform contingent having access correct scale estimate accumulator bin size practice possible 
see detailed discussion difficulties traditional robust techniques handling multiple structured data 
main processing steps distinguished implementation robust estimators nondifferentiable optimization criterion lmeds ransac hough transform 
small random subsets data points samples selected 
sample parameter estimate candidate computed 
third step quality candidates assessed data points candidate yielding best quality measure retained 
data classified inliers outliers relation model parameter estimates 
steps intertwined refined case hough transform disguised provide general processing principle 
principle obeyed techniques introduced employed computational modules algorithm analyzing data multiple structures 

definition random samples 
data quantized rp defining dimensional bin bandwidths computed uniform kernel separately coordinate 
bins ranked number points inside random chosen upper half ranking 
starting bin sample generated probabilistic region growing 
bin boundary current region selects neighbor region probability equal normalized number points neighbor 
normalization total number points neighbors 
region growing stops sample reaches upper bound allowed bins examples nonempty bins growing possible 

computation parameter estimate candidates 
samples experiments projection pursuit procedure discussed section applied 
sample candidate vector covariance scale estimate sl obtained 
display purposes points declared inliers delineated bounding box 

selection best candidates 
estimates covariances robust fusion procedure discussed section applied 
number structures data determined characteristics computed 

classification data 
refine relation structures data points declared inliers samples sample structure association receives vote 
points votes retained structure 
starting structure having largest number points recursively removed data 
data classification starts reliable basis sophisticated application specific procedures 
experiments synthetic data containing structures 
data set figures contains planar regions arrangement 
region contains points corrupted normal noise having chen meer fig 

example data analysis containing multiple structures 
views data 
bounding boxes employed samples 
delineated structures 
covariance 
background data points scattered uniformly cube incorporating structures 
bounding boxes resulting estimation procedures shown feature space result robust fusion 
output algorithm structures delineated final bounding boxes shown 
second data set figures characteristics planar regions arranged type configuration 
spite intersecting regions algorithm succeeded distinguish structures 
examples estimated parameters close true values planes 
discussion computer vision problems recast framework robust analysis regression data containing multiple structures 
example costeira kanade algorithm structure motion factorization multiple objects reformulated kanatani finding tracked object dimensional linear subspace space having dimension twice number image frames 
similarly build image sequence scene representation visual environment mosaic multiple layer plane parallax representation robust computer vision kernel density estimation fig 

example data analysis containing multiple structures 
views data 
bounding boxes employed samples 
delineated structures 
general model detecting independently moving objects 
algorithm proposed offers tool simultaneously extract significant model instances usually employed recursive approach dominant feature detected 
vision tasks require processing high dimensional spaces 
efficient search strategy employed projection index maximized estimation 
examples described directions distributed uniformly followed refinement best direction previous step 
examples processed matlab minute 
parametrization takes account unit vector currently developing computationally feasible search strategy higher dimensions 
ideally search take account priori information specific vision task solved 
techniques extensive nonparametric statistics tools sensitive parametric methods require supporting data points yield reliable results 
see example discussion projection pursuit small sample sizes 
new data analysis algorithm tolerates bad data better robust techniques employed computer vision 
chen meer acknowledgment 
support nsf iri gratefully acknowledged 

breiman 
bagging predictors 
machine learning 

chen meer tyler 
robust regression data multiple structures 
ieee conference computer vision pattern recognition volume pages kauai hi december 

comaniciu meer 
mean shift robust approach feature space analysis 
ieee trans 
pattern anal 
machine intell may 

comaniciu ramesh meer 
variable bandwidth mean shift data driven scale selection 
th international conference computer vision volume pages vancouver canada july 

costeira kanade 
multiple factorization method motion analysis 
international computer vision 

donoho johnstone rousseeuw 
discussion projection pursuit 
annals statistics 

duda hart stork 
pattern classification 
wiley second edition 

fukunaga 
estimation gradient density function applications pattern recognition 
ieee trans 
information theory 

huber 
projection pursuit discussion 
annals statistics 

irani anandan 
video indexing mosaic representations 
proceedings ieee 

jones sibson 
projection pursuit 
discussion 
royal stat 
soc 
series 

kanatani 
motion segmentation subspace separation model selection 
th international conference computer vision volume ii pages vancouver canada july 

li 
robust regression 
mosteller tukey editors exploring data tables trends shapes pages 
john wiley sons 

rousseeuw leroy 
robust regression outlier detection 
wiley 

sawhney guo kumar 
independent motion detection scenes 
ieee trans 
pattern anal 
machine intell 

silverman 
density estimation statistics data analysis 
chapman hall 

special issue 
robust statistical techniques image understanding 
computer vision image understanding april 

tax van duin kittler 
combining multiple classifiers averaging multiplying 
pattern recog 

wand jones 
kernel smoothing 
chapman hall 


robust estimation errors variables model 
biometrika 
