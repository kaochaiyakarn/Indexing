learning formation low dimensional representation spaces shimon edelman center biological computational learning mit cambridge ma usa edelman ai mit edu nathan intrator school mathematical sciences tel aviv university tel aviv israel nin math tau ac il psychophysical findings accumulated past decades indicate perceptual tasks similarity judgment tend performed low dimensional representation 
low dimensionality especially important learning number examples required attaining level performance grows exponentially dimensionality underlying representation space 
curse dimensionality shape categorization high initial dimensionality sensory data reduced nontrivial computational process ideally capture intrinsic low dimensional nature families visual shapes 
connectionist class labels learn representation fulfills requirement facilitating shape categorization 
results indicate low dimensional representations best extracted learning task combines discrimination generalization constraints 
sophisticated behavior biological cognitive systems widely assumed stem ability learn environment leads formation internal representation information pertinent task 
learned representations employed shaping behavior similar potentially related situations shepard similarity nosofsky medin representation edelman central concerns cognitive science 
note consider core property representation similarities objects goal efficient learning generalization low dimensionality 
link issues similarity low dimensional representation ldr readily apparent visual psychophysics concentrate area potential applications cognition ranging vision language reasoning 
psychophysics definition involves relationship physical characteristics stimulus perceptual event evokes 
specific discrimination tasks natural framework physical description various relationships similarities different possible stimuli low dimensional metric space 
cases reasonable expect representational system reflect dimensional structure topology metrics stimulus space 
start examining extent expectation fulfilled typical perceptual task color perception 
case study color space central feature problem computing reflectance surface patch measurements performed retinal image expected solution reflectance function surface resides principle infinite dimensional space potentially different value reflectance may specified infinite number wavelengths incident light zmura iverson 
computationally recovery surface reflectance face possible variations illumination nominally infinite dimensional quantity difficult need apart multiplicatively combined functions reflectance illumination 
infinite dimensionality functions suggest finite set measurements suffice support recovery surface reflectance 
human vision exhibits color constancy wide range conditions beck despite small dimensionality neural color coding space de valois de valois psychological perceived color space small boynton 
fact color spaces dimensional 
low dimensional physiological color space 
human vision kinds different retinal cone types addition rods spectral selectivity resembles cones 
question arises possible recover potentially spectral quantities measurement mechanism 
solution paradox lies low dimensionality space actual surface reflectances daylight illumination spectra 
finding cohen judd helps understand small number independent color selective channels suffice represent internally rich world color 
reason simple internal representation space low dimensional distal space happens low dimensional 
additional dimension cases luminance 
color constancy requires simultaneous processing spatial locations making effective dimensionality task somewhat higher 
variance chip reflectance functions accounted basis functions corresponding roughly variations intensity color opponent gamma gamma channels cohen 
likewise variance daylight spectra accounted principal components judd 
low dimensional psychological color space 
dimensionality physiological coding space color matches universe stimuli geared respond surprising representation space fed color pathway equally low dimensional 
data processing tool proved exceptionally useful characterization internal representation spaces multidimensional scaling mds 
technique derived observation knowledge distances points constrains possible locations points relative sufficient degree allow recovery locations coordinates points numerical procedure see shepard review 
processing color perception data configuration derived invariably approximately circular placing violet close red reside dimensions corresponds hue saturation color boynton 
exploration metric dimensional structure psychological spaces boosted improvement metric scaling techniques development non metric multidimensional scaling shepard kruskal 
general pattern emerging large variety perceptual scaling experiments subject performance tasks involving similarity judgment perception accounted substantial degree postulating perceived similarity reflects metric structure underlying perceptual space various stimuli represented points shepard 
low dimensional shape representation space series psychophysical studies originating shepard suggest low dimensional similarity space framework extended representation basic perceptual qualities colors complex shapes 
studies low dimensional similarity patterns imposed families stimuli exerting parametric control shape object 
low dimensional similarity space recovered experiment applying mds response data human subjects shepard edelman edelman 
locations stimuli mds derived space closely reflected arrangement parametrically defined pattern imposed experiment 
properties internal shape representation space low dimensionality preservation distal similarity relationships indicate human visual system routinely solves formidable computational problem massive dimensionality reduction 
constraints dimensionality reduction empirical evidence low dimensionality psychological representation spaces accumulating steadily decades widespread tendency psychology overlook computational problem qualifications view discussed 
particular metric model modified krumhansl edelman account asymmetry lack transitivity similarity judgments tasks tversky tversky gati 
derivation low dimensional representations perceptual data 
main reason mistaken assumption raw data available cognitive system reside immediately accessible low dimensional space 
example textbooks typically describe visual perception extraction information dimensional retinal image completely ignoring fact immediate successor retinal space processing hierarchy primates dimensional space spanned activities individual axons optic nerve 
assuming information required system raw measurement space may wonder human visual system reduce dimensionality 
crucial theoretical consideration learning 
specifically learning examples computationally infeasible rely high dimensional representations 
reason known curse dimensionality number examples necessary reliable generalization grows exponentially number dimensions bellman stone 
learnability necessitates dimensionality reduction 
choice computational approach dimensionality reduction guided considerations 
scale problem shape representation human vision requires reduction tens hundreds thousands just dimensions 
second consideration preservation certain order points corresponding different objects mapped high dimensional measurement space low dimensional representation space objects geometrically similar mapped nearby locations edelman 
intuitively process preserve set stimuli measurement space points pertinent task hand 
topology preserving methods especially useful representing data known reside intrinsically lowdimensional space embedded high dimensional measurement space 
instance color stimuli natural low dimensional pattern similarities preserved pink represented closer red green furthermore objective distal reflectance illuminant spaces low dimensional seen 
likewise shape representation relevant distal spaces low dimensional smooth measurement space views object undergoing transformation rotation deformation morphing object form low dimensional manifolds 
cases objects represented may visualized points drawn sheet rubber high dimensional ball illustrated 
objective dimensionality reducing mapping unfold sheet low dimensional structure explicit 
sheet torn process mapping topology preserving rubber stretched compressed mapping preserves metric structure original space original configuration similarity pattern points 
requirement mapping isometry restrictive hold globally mapping linear 
local approximate isometry smooth regular map dimension discrimination dimensions ignored dimension generalized schematic illustration problem space efficient representation requires nonlinear dimensionality reduction 
instances classes lowdimensional manifold embedded measurement space dimensionality may run tens thousands 
discriminant analysis dimensions crucial distinguishing categories dimensions collapsed 
context object recognition may dimensions object identity object orientation 
standard discriminant analysis methods multidimensional spaces plagued presence irrelevant dimensions show training combined objective discrimination labeled categories known reside manifold explicit collapse dimensions discrimination generalized leads reliable recovery target manifold significantly curved problem highly nonlinear embedded measurement space nearly dimensions 
ping sufficient 
near linearity smoothness necessary topology preservation 
news far learnability mapping concerned smooth mapping implies small number parameters learned 
turn reduces likelihood overfitting poor generalization plague learning algorithms high dimensional spaces 
oldest nonlinear method topology preserving dimensionality reduction multidimensional scaling mds originally developed method recovery coordinates set points measurements pairwise distances points 
mds serve reduce dimensionality points embedded space fewer dimensions original space interpoint distances measured 
main problem mds considered method massive dimensionality reduction exploration experimental data applied sciences poor scaling dimensionality intrator edelman 
problem arises various attempts extend popular tool linear dimensionality reduction principal component analysis pca handle nonlinear spaces 
number learning methods topology preserving di discussion context shape representation edelman duvdevani bar 
example approach clustering followed local pca leen 
reduction derived idea self supervised auto associative network elman zipser demers cottrell 
methods unsupervised extract representations irrelevant dimensions input space 
result methods find target manifold intrator edelman defined large extent measurement space directions orthogonal see 
supervised approaches joint optimization discriminability topology preservation described fukunaga webb methods resemble mds suffer poor scaling dimensionality 
scheme extraction low dimensional representations proceed show training combined objective discrimination labeled objects known reside target manifold explicit collapse dimensions orthogonal manifold discrimination generalized leads reliable recovery target manifold 
solving problem chose address learning recognize visual objects examples requires ability find meaningful patterns series images words spaces high dimensionality 
mentioned dimensionality reduction task greatly assisted realization low dimensional solution fact exists 
mere knowledge existence automatically provide method computing lowdimensional solution 
learning system biased solutions possess desirable properties task highly nontrivial high dimensional space curse dimensionality 
method dimensionality reduction effectively biases learning system combining multiple constraints extensive set class labels 
multiple class labels steers low dimensional representation invariant directions variation input space irrelevant classification done merely making class labels independent directions 
extraction low dimensional representation bottleneck approaches cottrell leen forced classifier learn set class labels input objects constraining dimensionality representation number hidden units layer network classifier 
standard methods classifier produce labels reconstruct input patterns 
approach constitutes compromise completely unsupervised totally supervised methods uses label data item require information regarding relationship different items experimentation various architectures including multilayer perceptrons radial basis function networks yielded similarly encouraging results see intrator edelman details 
images faces data set 
top heads obtained placing theta grid space leading principal components original heads 
bottom views rightmost head top row views differ ffi steps rotation depth total difference ffi prior classification images originally size theta reduced dimensions cropping background correlation theta bank filters exact spatial profile filters turned unimportant gaussian filters just opponent center surround ones 
complete reconstruction data bottleneck autoencoder systems 
ability method discover simple structure embedded high dimensional measurement space demonstrated face data set extraction ldr low dimensional representation requires highly nonlinear transformation measurement space 
basis data set lies dimensional parametric representation space classes faces placed regular theta grid additional parametric dimension orthogonal models class variation see 
impose distinctive low dimensional structure set faces followed simple approach common parameterization principal component analysis pca 
done starting set laser scans human heads embedding theta grid space spanned leading obtained data pca 
heads derived pca original scanned head data piped graphics program rendered head viewpoints obtained stepping simulated camera ffi rotation steps axis 
results application label method led recovery relevant low dimensional description faces data set see 
performance method recovering row column parametric structure tested method data set consisting parameterized fractal images intrator edelman 
classes especially amazing 
structure apparent ldr produced network trained second face class faces tested full data set see right 
difficulty ldr extraction case demonstrated comparison results obtained conventional neural network methods 
asked self supervised layer autoencoder aims best reconstruction inputs reveal correct low dimensional structure case 
linear case networks quite essentially extracting principal components data elman zipser performance faces data poor network consistently converged mean data presumably due nonlinearity introduced imaging step 
second experimented layer nonlinear bottleneck autoencoder leen likewise performed poorly 
outcome experiment showed self supervised dimensionality reduction recover ldr case illustrating importance guidance provided class labels 
third tested modified version method classifier trained ignore direction orthogonal target manifold cf 
done training face view labels face identity labels 
ldr poor importance guidance provided explicit specification dimension collapsed 
discussion research program led results outlined motivated notion representation visual world foremost low dimensional representation 
described family methods map high dimensional data set low dimensional space topologically approximation nonlinear manifold original measurements 
property topology preservation shared methods considered appears due smooth nature mapping realize intrator edelman edelman duvdevani bar 
property insufficient ensure extraction correct manifold control experiments indicate importance class labels help define tangent space manifold illustrated stipulation generalization set stimuli defines normal manifold 
separate definition normal class specification variation faces important recovery nonlinear manifolds direction normal changes point point 
assess quality ldr defined dichotomy task faces labeled attributed class faces class 
task recovered method consistently supported better performance raw images 
ldr bottleneck procrustes faces data set dimensionality reduction bottleneck multilayer perceptron mlp plots show locations theta test stimuli space spanned activities units residing hidden layer faces times test orientations face 
left results obtained layer mlp units middle hidden layer trained epochs way classification task 
low dimensional representation proved substrate solving classification tasks system trained error rate nonlinear dichotomy involving classes compared obtained system trained specifically dichotomy raw multidimensional representation see intrator edelman details 
middle results layer bottleneck mlp hidden units middle hidden layer trained way classification task 
test dichotomy error rate compared raw data 
right results obtained layer mlp network hidden units trained second classes comprise problem space classes training omitted classes formed checkerboard pattern 
note classes familiar ones seen system topology preserving formation 
representation error rate nonlinear dichotomy involving classes compared obtained system trained specifically dichotomy raw multidimensional representation 
left right plots multidimensional scaling visualize representation spaces nominally dimensional respectively middle plot dimensional hidden unit space layer network plotted directly 
implications computational feasibility learning representation low dimensional similarity preserving may taken support attempts similarity central explanatory concept psychology 
attempt described shepard appeared tri anniversary publication newton principia 
shepard proposed law generalization tied likelihood stimuli evoking response proximity stimuli psychological representation space space persistently turned low dimensional dozens experiments surveyed shepard shepard works shape representation edelman edelman 
significance having similarity preserving lowdimensional space substrate representation twofold 
notion similarity space puts novel stimuli equal footing familiar ones point corresponding novel stimulus located representation space characterize location respect familiar points 
visual system literally encounters stimulus twice variations viewing conditions illumination objects look different different viewpoints articulated flexible objects change shape 
mere memory past stimuli faithful extensive may poor guide behavior 
contrast suitable similarity preserving representation space help system deal objects memory traces available cf 
right 
space proximity reliable guide generalization 
second important trait representation space common range stimuli task low dimensionality gradually clear emergence formal approaches quantification complexity learning problems 
perceptual tasks color vision low dimensionality representation stems naturally corresponding low dimensionality stimulus space tasks notably object shape recognition computationally convenient basis low dimensional shape representation developed 
useful common low dimensional parameterization shapes belonging certain categories achieved principal component analysis done human heads cf 
atick 
summary conclude propose development systems capable representing world governed unifying principle various aspects world repre shepard shows validity proximity basis generalization universal derived principles 
sented successfully insofar expressed lowdimensional space 
specifically suggest possibility effective representation stems low dimensional nature real world classification tasks intelligent system merely reflecting low dimensional distal space internally 
undertaking straightforward sounds 
relevant dimensions distal stimulus variation known advance immediately available internally perceptual front sophisticated representational system start high dimensional measurement stage task mainly assure relevant dimensions stimulus variation lost process encoding 
ultimate performance system depends capability reduce dimensionality measurement space back acceptable level par original presumably low dimensional distal stimulus space 
acknowledgments dayan tenenbaum useful suggestions goldstone hochberg schyns comments project 
atick griffin redlich 

vocabulary shape principal shapes probing perception neural response 
network 
beck 

surface color perception 
cornell university press ithaca ny 
bellman 

adaptive control processes 
princeton university press princeton nj 
boynton 

color hue wavelength 
friedman editors handbook perception volume pp 
academic press new york ny 
cohen 

dependency spectral reflectance curves color chips 
psychonomic sciences 


perceptual similarity shapes generated fourier descriptors 
journal experimental psychology human perception performance 
cottrell munro zipser 

learning internal representations gray scale images example extensional programming 
ninth annual conference cognitive science society pp hillsdale 
erlbaum 
edelman 

faithful representation similarities dimensional shapes human vision 
proceedings national academy science 
de valois de valois 

neural coding color 
friedman editors handbook perception volume pp 
academic press new york ny 
herault 

curvilinear component analysis self organizing neural network non linear mapping data sets 
submitted ieee transaction neural networks 
demers cottrell 

nonlinear dimensionality reduction 
hanson cowan giles editors advances neural information pp 
morgan kaufmann 
zmura iverson 

formal approach color constancy recovery surface light source spectral properties bilinear models 
dowling roberts editors progress mathematical psychology 
erlbaum hillsdale nj 
edelman 

representation similarity object discrimination 
neural computation 
edelman 

representation representation similarity 
behavioral brain sciences appear 
edelman duvdevani bar 

similarity shapes basis shape representation 
cottrell editor proceedingsof th annual conf 
cognitive science society pp san diego ca 
edelman duvdevani bar 

similarity connectionism problem representation vision 
neural computation 
elman zipser 

learning hidden structure speech 
journal acoustical society america 


similarity 
academic press new york 
intrator 

low dimensional representation suitable diverse tasks 
connection science 
intrator edelman 

learning low dimensional representations visual objects extensive prior knowledge 
press 
judd 

spectral distribution typical daylight function correlated color temperature 
journal optical society america 
fukunaga 

nonlinear feature extraction algorithm distance information 
ieee trans 
comput 
krumhansl 

concerning applicability geometric models similarity data interrelationship similarity spatial density 
psychological review 
kruskal 

multidimensional scaling optimizing goodness fit nonmetric hypothesis 
psychometrika 
leen 

fast non linear dimension reduction 
cowan tesauro alspector editors advances neural information processing systems volume pp 
morgan kauffman san francisco ca 
medin goldstone gentner 

respects similarity 
psychological review 
nosofsky 

similarity scaling cognitive process models 
annual review psychology 
shepard 

metric structures ordinal data 
math 
psychology 
shepard 

multidimensional scaling tree fitting clustering 
science 
shepard 

universal law generalization psychological science 
science 
shepard 

perceptual cognitive explorations toroidal set free form stimuli 
cognitive psychology 
stone 

optimal global rates convergence nonparametric regression 
annals statistics 
tversky 

features similarity 
psychological review 
tversky gati 

concerning applicability geometric models similarity data interrelationship similarity spatial density 
psychological review 
webb 

multidimensional scaling iterative majorization radial basis functions 
pattern recognition 
