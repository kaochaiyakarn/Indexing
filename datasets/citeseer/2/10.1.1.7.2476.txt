collaborative filtering gaussian probabilistic latent semantic analysis collaborative filtering aims learning predictive models user preferences interests behavior community data database available user preferences 
describe new model algorithm designed task generalization probabilistic latent semantic analysis continuous valued response variables 
specifically assume observed user ratings modeled mixture user communities interest groups users may participate probabilistically groups 
community characterized gaussian distribution normalized ratings item 
normalization ratings performed user specific manner account variations absolute shift variance ratings 
experiments movie data set show proposed approach compares favorably collaborative filtering techniques 
categories subject descriptors information storage retrieval information search retrieval information filtering pattern recognition clustering algorithms general terms algorithms experimentation theory keywords collaborative filtering recommender systems machine learning data mining 
content filtering retrieval build fundamental assumption users able formulate queries express interests information needs term intrinsic features items sought 
cases may difficult identify suitable descriptors author version 
posted permission acm personal 
redistribution 
definitive version published sigir doi acm org sigir july august toronto canada 
copyright acm 
thomas hofmann department computer science brown university providence ri usa th cs brown edu keywords topics genres accurately describe interests 
cases example electronic commerce users may simply unaware interests 
cases predict user preferences recommend items interest requiring user explicitly formulate query 
collaborative filtering technology complementary content filtering aims learning predictive models user preferences interests behavior community data database available user preferences 
dominant paradigm performing collaborative filtering recommender systems nearest neighbor regression called memorybased techniques 
systems general step approach 
users identified similar active user recommendation 
predictions recommendations computed preferences judgments similar users 
recommender systems memory technology include grouplens movielens project ringo number commercial systems notably systems deployed amazon com cdnow com 
memory methods reached level popularity simple intuitive conceptual level avoiding complications potentially expensive model building stage 
time deemed sufficiently accurate real world problems 
number severe shortcomings point accuracy obtained memory methods may suboptimal 
experiments indicate significant accuracy gains achievable 
ii explicit statistical model constructed really learned form available user profiles little general insight gained 
memory methods limited example data mining tools 
iii memory methods scale terms resource requirements memory computing time approximations subsampling 
iv difficult systematically tailor memory algorithms maximize objective associated specific task 
actual user profiles kept prediction potentially raising privacy issues 
deals model approach addresses shortcomings achieves higher prediction accuracies ii compresses data compact model automatically identifies user communities interest groups iii enables computing preference predictions constant time iv gives system designer flexibility specifying objectives application require keep original user profiles 
model techniques investigated notably bayesian non bayesian clustering techniques bayesian networks dependency networks :10.1.1.21.4665:10.1.1.21.4665
approach proposed generalization statistical technique called probabilistic latent semantic analysis plsa originally developed context information retrieval 
bears similarity clustering methods latent variables user communities introduced communities overlapping users partitioned probabilistically 
fact probabilistic latent semantic models ways closely related dimension reduction methods matrix decomposition techniques singular value decomposition applied context recommender systems 
main difference respect bayesian networks dependency networks fact learn dependency structures directly observables approach latent cause model introduces notion user communities groups items 
main advantage pca svd dimension reduction methods approach offers probabilistic semantics build statistical techniques inference model selection 

model collaborative filtering implicit explicit ratings domains consider consist set persons users 
un set items 
ym set possible ratings assume observations available person object pairs basic case observation just occurrence representing events person buys product person clicks link called implicit preference data 
cases users may provide explicit rating part observation 
main focus numerical ratings commonly quantized small number response levels 
example star rating scale commonly movie recommendation systems eachmovie movielens 
prediction problems consider type prediction problems 
setting call forced prediction involves predicting preference value particular item identity user learn mapping generally may interested conditional probability distribution user rate item may define deterministic prediction function dv denotes condi tional probability density function 
second setting call free prediction item selection process part predictive model goal learn probabilities order predict selected item associated rating virtue chain rule rewritten decomposing problem prediction selected item irrespective rating prediction rating conditioned hypothetically selected item 
forced prediction case user particular item provides explicit rating 
selection item user vote response solicited part experimental design 
single item user time user response recorded 
contrast free prediction case user control item selection interested predicting user select optionally rate item 
loss risk functions pursuing model approach collaborative filtering assume availability adequate loss function 
loss function function quantifies bad prediction model compared true outcome 
denote parameterized model space generic parameter refer particular model propose utilize negative logarithmic loss function forms basis popular maximum likelihood estimation approach statistics log log 
logarithmic loss define empirical risk function negative conditional log likelihood 
maximum likelihood estimation amounts minimizing angular brackets summation symbol shorthand notation refer observation triplets denotes total number observed triplets 

gaussian probabilistic latent semantic model occurrence model discuss simple model occurrence data known probabilistic latent semantic analysis plsa 
thought special case collaborative filtering implicit preference data 
data consists set user item pairs assumed generated independently 
key idea approach introduce hidden variables states user item pair user item rendered conditionally independent 
possible set states assumed finite size resulting model mixture model written way sums run possible states 
applying bayes rule alternatively equivalent parameterizations 
typical situation collaborative filtering personalized recommendations mainly conditional model 
parameter vector summarizes probabilities described independent parameters requires independent parameters 
notice model simply assumes selection item depend identity user resulting non personalized predictions 
user identity item identity assumed marginally independent case 
number hidden states increases set representable joint distributions user item pairs constrained saturated model obtained represent joint probability user item pairs 
practice chosen way adjusts model complexity light amount sparseness available data 
standard model selection techniques crossvalidation purpose 
associated priori meaning states hidden variables hope recover interesting structure data user communities groups related items 
intuitively state hidden variable associated observation supposed model hidden cause fact person selects item intended offer hypothetical explanation implicit rating directly observable 
number possible states typically smaller number items users model encourages grouping users user communities items groups related items 
models numerical ratings applications collaborative filtering involve explicit user ratings plsa model needs generalized appropriately 
discussed number ways extend dependency structure cooccurrence model include additional response variable 
extension proven useful empirical evaluations introduce direct dependencies response variable item question mediate dependency user latent variable 
supposed capture user communities interest groups 
formally results mixture model 
addition specifying dependency structure needs define parametric form conditional probability density 
previous models dealt multinomial sampling models appropriate categorical ratings binary votes investigate gaussian model 
propose introduce location parameters mean rating scale parameters spread ratings 
effectively defines gaussian mixture model user specific mixing weights exp notice expected response easily computed dv 
user normalization gaussian model far assumes users express vote common scale 
known different users may associate subjectively different meanings ratings 
instance votes star rating may mean different things different people 
memory methods taken account similarity measures pearson spearman correlation coefficient effectively normalize ratings user mean rating spread 
gaussian plsa numerical response variables accommodated model approach 
extent propose normalized model denotes user specific mean rating standard variation 
assumption normalized ratings user community normally distributed item normalization step performs simple transformation observed ratings users small number ratings careful perform appropriate smoothing estimating user specific parameters particular true standard deviations empirical estimates may unreliable due sampling noise 
scheme smooth estimates means variances nu nu denotes mean vote denotes variance ratings 
nu number ratings available user free parameter controlling smoothing strength set experiments 
notice shrink empirical maximum likelihood estimate pooled estimates common scheme employed instance hierarchical bayesian modeling 
expectation maximization algorithm maximum likelihood approach statistical inference propose fit model parameters maximizing conditional log likelihood equivalently minimizing risk function eq 

derivations free prediction case analogous equations forced prediction case essentially obtained replacing 
expectation maximization em algorithm standard methods statistical inference approximately maximize log likelihood mixture models plsa 
step deriving em algorithm specify complete data model 
complete data model treats hidden variables observed case amounts assumption observed triplet fact observe tuple 
complete data model corresponding negative log likelihood function written log log 
states latent variables known introduce called variational probability distribution triplets 
define family risk functions risk function choice log log exploiting concavity logarithm jensen inequality shown defines upper bound constant depends log log refers entropy probability distribution em algorithm consists steps performed alternation computing tightest bound parameters optimizing eq 
respect variational distribution called step amounts computing posterior probabilities hidden variable 
formal derivation technique lagrange multipliers straightforward 
hat indicates quantities parameterized 
obviously posterior probabilities need computed triplets observed 
averaging respect posterior distribution calculated eq 
yields log log needs optimized respect parameters constitutes maximization step 
maximization problem readily solved introducing lagrange multipliers normalization constraint solving resulting constrained optimization problem leads set equations mixing proportions 
similarly obtains equations parameters gaussian distributions 
notice ratings triplets refer ratings eq 

eqs 
essentially standard step equations gaussian mixture model 
fact mixing proportions user specific enters computation posterior probabilities step 
complete em algorithm proceeds alternating step eq 
step eqs 

regularization learning statistical models parameters limited amount data bears risk overfitting 
regularization technique address problem 
investigated modification em called tempered em conceptually simpler computationally expensive approach proven sufficient regularizing gaussian plsa models 
retaining fraction data training heldout data perform early stopping terminate em iterations accuracy held data deteriorates 
perform iteration em full training data 

experiments data set data set experiments movie data 
data collected digital equipment research center 
items movies data set user profiles total ratings 
rating scale discrete values star stars 
highest rating lowest rating 
average rating observed votes rating variance 
original ratings multiplied factor 
abs communities rms communities predictive performance terms absolute left rms error right normalized gaussian ranking model function number user communities 
eachmovie data set knowledge largest publicly available data set collaborative filtering possesses advantage offering explicit user ratings 
fact allows study item selection rating prediction 
evaluation metric thorough empirical analysis collaborative filtering algorithms adapted proposed evaluation metrics :10.1.1.21.4665
effectiveness collaborative filtering techniques measured various ways dependent recommender system results user 
setting investigated assumes goal system predict user ratings 
assume item user goal predict rating 
loss functions measure deviation predicted rating observed rating absolute deviation squared error empirical risks squared loss summarized rooted mean square rms error square root empirical average loss 
addition measured zero loss case predictions quantized rounding closest valid integer 
second setting goal predict selected item corresponding rating 
score ranked lists proposed :10.1.1.21.4665
denote permutation items rank item respect 
top ranked item second item forth 
ratings items training included ranking 
rank score max denoting mean vote 
rationale score ranked list items users sift list starting top find relevant item simply give 
probability user take notice item rank modeled exponential distribution half life constant set experiments 
total score population users measured cf 
max :10.1.1.21.4665
normalizes sum achieved score optimally achieved user relevant items appear top ranked list 
evaluation protocols leave protocol evaluate obtained prediction accuracies 
means randomly leave exactly rating user possessing observed ratings average loss function set users obtain estimate risk 
protocol called :10.1.1.21.4665
eliminated vote user training set trained models reduced set 
notice uses somewhat data required allows single model evaluate leave performance averaged users 
point protocols proposed :10.1.1.21.4665
reason afford perform complete model retraining evaluating system performance single user 
hand creating training data set keeping maximally observations user data considerably provide useful approximation 
baseline methods implemented baseline methods calibrate achieved results 
simple baseline largely non personalized prediction ranking function simply computes average normalized rating movie user population 
notice take user specific mean votes variances account equivalent normalized gaussian plsa model 
methods called baseline tables 
addition implemented standard memorybased method computes similarities user profiles pearson correlation coefficient 
implementation include possible improvements inverse user frequency case amplification 
corresponding results tables labeled pearson 
results prediction scenario table summarizes results including comparison standard memory method pearson correlation coefficient results published various methods bayesian clustering bc bayesian networks bn correlation cr :10.1.1.21.4665
baseline simply defined mean vote item 
seen proposed gaussian plsa model outperforms memory method achieves better results ones published :10.1.1.21.4665
respect results acknowledge somewhat different setup lead better performance results experiments 
approximate comparison possible identifying implementation correlation filtering method implemented :10.1.1.21.4665
seen accuracy gaussian method absolute error relative improvement abs rms loss abs rms loss baseline pearson gaussian gauss normalized cr bc bn table prediction accuracy various methods forced prediction mode :10.1.1.21.4665
method rank gain rel :10.1.1.21.4665
improv 
abs rms baseline pearson gaussian gaussian normalized table performance different methods free prediction mode ranking criterion 
model quite poor user specific normalization crucial ingredient proposed model 
quite remarkable result obtained model involving relatively small number communities 
conclude assumption user specific rating scales encodes useful prior knowledge 
shows prediction accuracy gaussian plsa model varies model complexity cardinality state space latent variable corresponding number latent communities searched 
graph shows clear optimum communities performance slowly degrades model size 
behavior strikingly different results obtained multinomial sampling model obtained accuracy constantly improves number communities levels 
results ranking scenario second scenario experimentally investigated free prediction mode 
prediction accuracy predicting votes longer adequate ranking loss eq 
benchmark different algorithms 
leave protocol 
results summarized table 
best ranking scores obtained normalized gaussian plsa model 
relative performance gain respect popularity baseline higher forced prediction mode relative improvement achieved 
notice actual prediction error models higher models trained forced mode 
fact absolute error slightly higher memorybased approach 
mining user communities decomposition user ratings may lead discovery interesting patterns regularities describe user interests 
extent visualized items latent variable state sorting popularity interest group measured irrespective actual vote 
mean vote displayed rectangular brackets movie title 
display interest groups extracted normalized gaussian model 
computational complexity emphasize fact computational complexity computing predictions active user scales number communities number users data base 
optimal accuracy achieved model compares favorably naive memory approaches scale linearly total number users case eachmovie 
notice complexity adding new user database scales long community parameters gaussian parameters held fixed 
achieved performing operation called fold 
model representation achieves significant compression terms space 
learning phase obviously requires resources 
notice complexity single em iteration scales times number observed ratings 
typical number em iterations performed experiments 
running time training gaussian plsa model reasonable practical purposes approximately minutes standard computer system powered pentium iii ghz cpu 

powerful method collaborative filtering mining user data novel statistical latent class model gaussian plsa 
method achieves competitive recommendation prediction accuracies highly scalable extremely flexible 
predictions recommendations computed constant time access data base original user profiles 
decomposition user preferences feature clearly distinguishes approach traditional memory approaches 
user communities extracted eachmovie data set gaussian plsa model 
numbers brackets denote means 
acknowledgments research sponsored nsf itr award number iis 
eachmovie preference data courtesy digital equipment generously provided paul mcjones 

goldberg nichols oki terry 
collaborative filtering weave information tapestry 
communications acm 
resnik iacovou suchak bergstrom riedl 
grouplens open architecture collaborative filtering netnews 
proceedings acm conference computer supported cooperative pages 
konstan miller maltz herlocker gordon riedl 
grouplens applying collaborative filtering usenet news 
communications acm 
shardanand maes 
social information filtering algorithms automating word mouth 
proceedings acm chi conference human factors computing systems volume pages 
breese heckerman :10.1.1.21.4665
empirical analysis predictive algorithms collaborative filtering 
proceedings th conference uncertainty artificial intelligence pages 
ungar foster 
clustering methods collaborative filtering 
proceedings workshop recommendation systems 
aaai press 
basu hirsh cohen 
recommendation classification social content information recommendation 
recommender system workshop pages 

chien george 
bayesian model user communities extracted eachmovie data set gaussian plsa model 
numbers brackets denote means collaborative filtering 
online proceedings seventh international workshop artificial intelligence statistics 
heckerman chickering meek kadie 
dependency networks inference collaborative filtering data visualization 
journal machine learning research 
hofmann 
unsupervised learning probabilistic latent semantic analysis 
machine learning journal 
hofmann 
probabilistic latent semantic indexing 
proceedings nd acm sigir international conference research development information retrieval pages 
sarwar karypis konstan riedl 
application dimensionality reduction recommender system case study 
acm webkdd web mining commerce workshop 
canny 
collaborative filtering privacy 
ieee symposium security privacy pages 
hofmann puzicha 
latent class models collaborative filtering 
proceedings international joint conference artificial intelligence 
herlocker konstan borchers riedl 
algorithmic framework performing collaborative filtering 
proceedings nd acm sigir international conference research development information retrieval 
hofmann 
people don want 
european conference machine learning ecml 
