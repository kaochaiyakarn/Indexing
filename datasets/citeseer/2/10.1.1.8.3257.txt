estimating continuous distributions bayesian classifiers george john computer science dept stanford university stanford ca cs stanford edu robotics stanford edu pat langley robotics laboratory stanford university stanford ca langley cs stanford edu robotics stanford edu langley modeling probability distribution bayesian network faced problem handle continuous variables 
previous solved problem discretizing assumed data generated single gaussian 
abandon normality assumption statistical methods nonparametric density estimation 
naive bayesian classifier experimental results variety natural artificial domains comparing methods density estimation assuming normality modeling conditional distribution single gaussian nonparametric kernel density estimation 
observe large reductions error natural artificial data sets suggests kernel estimation useful tool learning bayesian models 
proceedings eleventh conference uncertainty artificial intelligence morgan kaufmann publishers san mateo years methods inducing probabilistic descriptions training data emerged major alternative established approaches machine learning decision tree induction neural networks 
example cooper herskovits describe greedy algorithm determines structure bayesian inference network data heckerman geiger chickering provan singh report advances basic approach 
bayesian networks provide promising representation machine learning reasons useful performance tasks diagnosis deal explicitly issues uncertainty noise central problems induction task 
impressive results date come simpler older approach probabilistic induction known naive bayesian classifier despite simplifying assumptions underlie naive bayesian classifier experiments real world data repeatedly shown competitive sophisticated induction algorithms 
example clark niblett report naive bayes producing accuracies comparable rule induction methods medical domains langley iba thompson outperformed algorithm decision tree induction domains 
impressive results motivated researchers explore extensions naive bayes lessen dependence assumptions retain inherent simplicity clear probabilistic semantics 
langley sage describe variation mitigates independence assumption eliminating predictive features correlated 
kononenko pazzani propose alternative response assumption selectively introducing combinations attributes modeling process 
similar approaches represent important line research machine learning goal discover learning methods real world data clear semantics 
means study modern systems give bayesian interpretation research agenda begins understood methods attempts improve removing assumptions hinder performance 
take approach naive bayesian classifier traditionally assumption numeric attributes generated single gaussian distribution 
gaussian may provide reasonable approximation real world distributions certainly best approximation 
suggests direction profitably extend improve approach investigating general methods density estimation 
pages follow review naive bayes naive bayesian classifier describe flexible bayes extension single gaus naive bayesian classifier depicted bayesian network predictive attributes conditionally independent class attribute 
sian assumption favor kernel density estimation retains independence assumption 
discuss important properties kernel estimation flexible bayes inherits 
hypotheses new method behavior followed experiments natural artificial domains designed test hypotheses 
closing review related suggest directions research 
naive bayesian classifier noted naive bayesian classifier provides simple approach clear semantics representing learning knowledge 
method designed supervised induction tasks performance goal accurately predict class test instances training instances include class information 
view classifier specialized form bayesian network termed naive relies important simplifying assumptions 
particular assumes predictive attributes conditionally independent class posits hidden latent attributes influence prediction process 
depicted graphically naive bayesian classifier form shown arcs directed class attribute observable predictive attributes buntine 
assumptions support efficient algorithms classification learning 
see random variable denoting class instance vector random variables denoting observed attribute values 
represent particular class label represent particular observed attribute value vector 
test case classify simply uses bayes rule compute probability class vector observed values predictive attributes cjx xjc predicts probable class 
represents event event simply conjunction attribute value assignments attributes assumed conditionally independent obtains xjc jc jc simple compute test cases estimate training data 
generally directly estimate distribution denominator equation just normalizing factor ignores denominator normalizes sum cjx classes 
naive bayes treats discrete numeric attributes somewhat differently 
discrete attribute xjc modeled single real number represents probability attribute take particular value class contrast numeric attribute modeled continuous probability distribution range attribute values 
common assumption intrinsic naive bayesian approach class values numeric attributes normally distributed 
represent distribution terms mean standard deviation efficiently compute probability observed value estimates 
continuous attributes write xjc oe oe oe gamma gamma oe probability density function normal gaussian distribution 
model leaves small set parameters estimate training data 
class nominal attribute estimate probability attribute take value domain class 
class continuous attribute estimate mean standard deviation attribute class 
maximum likelihood estimation parameters straightforward 
estimated probability nominal random variable takes certain value equal sample frequency number times value equation strictly correct probability real valued random variable exactly equals value zero 
speak variable lying interval delta delta oe dx 
definition derivative lim delta delta delta oe 
small constant delta oe theta delta 
factor delta appears numerator equation class 
cancel perform normalization may equation 

single gaussian kernel density estimation effect single gaussian versus kernel method estimate density continuous variable 
observed divided total number observations 
maximum likelihood estimates mean standard deviation normal distribution sample average sample standard deviation 
clarify estimation process consider small data set classes gamma nominal attribute takes values continuous attribute training cases gamma gamma naive bayes obtains probability estimates ajc bjc xjc positive class analogous estimates negative class 
solid curve shows density function estimated numeric attribute somewhat larger data set 
bayesian estimation methods commonly assume dirichlet prior estimate model parameters 
despite similar names bayesian estimation common naive bayesian classifiers usually data parameters typically weak priors quickly overwhelmed 
summary naive bayes provides simple efficient approach problem induction 
typically relies assumption numeric attributes obey gaussian distribution may hold domains 
suggests explore methods estimating continuous distributions 
table algorithmic complexity naive bayes flexible bayes training cases features 
naive bayes flex bayes operation time space time space train cases nk nk nk test cases mk mnk flexible naive bayes introduce flexible bayes learning algorithm exactly naive bayes respects method density estimation continuous attributes 
single gaussian common technique handling continuous variables certainly 
researchers explored variety nonparametric density estimation methods 
chose investigate kernel density estimation 
recall naive bayes estimate density continuous attribute xjc oe 
kernel estimation gaussian kernels kernel functions looks estimated density averaged large set kernels xjc oe ranges training points attribute class dashed line shows kernel density estimation sampled points 
readers familiar kernel methods note equation equivalent standard kernel density formula xjc nh gamma gamma oe 
naive bayes estimate oe storing sum observed sum squares sufficient statistics normal distribution flexible bayes store continuous attribute value sees training 
sufficient statistic list list 
nominal attributes distributions learned storing single number value represents sample frequency naive bayes 
computing xjc continuous attribute classify unseen test instance naive bayes evaluate flexible bayes perform evaluations observed value class leads increase storage computational complexity summarized table 
addressed important issue kernel density estimation setting width parameter oe 
shall see section kernel estimation nice theoretical properties oe shrinks zero number instances goes infinity 
statistical literatures reports various rules thumb setting kernel width heuristic implicit explicit assumptions density function true distributions 
set oe number training instances observed class flexible bayes observes training points density estimates increasingly local 
intuition flexible bayes kernel estimation method perform domains violate normality assumption little cost domains holds 
understand claim review theoretical underpinnings kernel methods 
asymptotic properties flexible bayes general density estimation involves approximating probability density function continuous random variable 
bayesian classifier encounters problem estimate xjc continuous attribute general problem statistics variety methods available solving ripley silverman 
section discuss theoretical properties kernel density estimation implications flexible bayes algorithm 
statisticians principally concerned consistency density estimate 
definition strong pointwise consistency probability density function fn estimate examples strongly pointwise consistent surely ffl lim gamma ffl strongest asymptotic result hope regarding flexible bayes provided independence assumption holds estimate cjx strongly pointwise consistent 
imply limit flexible bayes estimate cjx classification produces bayes optimal error rate 
prove strong consistency flexible bayes steps proving method provides strongly consistent estimate xjc nominal proving property continuous proving estimate cjx strongly consistent 
theorem strong consistency nominals xn independent sample multinomial distribution values probability drawing value xi number samples value strongly consistent estimator proof direct instantiation strong law large numbers casella berger 
theorem strong consistency reals due devroye 
kernel density estimate strongly consistent ffl kernel function bona fide density estimate nonnegative integrate 
ffl hn 
recall standard notation equivalent oe 
ffl 
conditions satisfied gaussian kernels hn xjc density estimate flexible bayes strongly consistent 
lemma consistency products functions strongly consistent estimates density functions strongly consistent estimator proof prove strongly consistent estimator lemma follows induction 
definition strong consistency ffl ffl lim gamma ffl similarly strongly pointwise consistent lim gamma ffl hold true ffl ffl ffl ffl ffl simple algebraic manipulation finite ffl ffl may arbitrarily small bound ffl hold giving desired result strongly consistent 
theorem consistency flexible bayes true conditional distribution class attributes cjx jc 
actual conditional distribution estimate 
flexible bayes estimate cjx strongly consistent estimator cjx 
proof theorems flexible bayes estimates xjc strongly consistent lemma related lemma regarding quotient strongly consistent estimates flexible bayes estimate cjx strongly consistent 
table natural data set characteristics fold cross validation results 
characteristics set size number classes number nominal continuous attributes 
results mean standard deviations cross validation runs naive bayes flexible bayes significance level paired test method accurate 
accuracies shown provide context 
data set size class cont nom naive flex flex better 
breast cancer wisc sigma sigma cleveland heart disease sigma sigma theta credit card application sigma sigma glass sigma sigma glass float non sigma sigma horse colic sigma sigma theta iris sigma sigma labor negotiation sigma sigma meta learning sigma sigma pima diabetes sigma sigma vehicle silhouette sigma sigma experimental studies convincing arguments incorporating kernel estimation bayesian classifier finite sample behavior method ultimately empirical question 
machine learning standard experimental method kibler langley involves running learning algorithm set training data induced model predictions separate test cases measuring accuracy 
evaluate behavior flexible bayesian classifier designed carried number experimental studies lines 
experiments natural data determine relevance approach realworld problems selected databases uci machine learning repository murphy aha 
table summarizes number instances number classes number nominal numeric attributes data set 
sizeable fraction domain features numeric candidates contrasting behavior flexible bayes naive bayes 
domain fold cross validation evaluate generalization accuracy induction algorithms 
randomly partitioned data disjoint sets provided algorithm sets training data remaining set test cases 
repeated process times different possible test sets averaged resulting accuracies 
carried procedure quinlan known algorithm decision tree induction provide point comparison 
table shows results runs natural domains including mean accuracy standard deviation method significantly better paired test significance level test 
table indicates flexible bayes significantly accurate naive bayes domains accurate domains significantly different cases 
domains glass meta learning naive scheme significantly worse flexible method significantly better 
curiously domains naive bayes outperformed flexible bayes medical domains 
possibly doctors tend define diseases important continuous features roughly normal patient disease 
conditional densities truly gaussian naive bayes resting blood pressure systolic single gaussian kernel density estimation systematic measurement errors cleveland heart disease database 
accuracy training set size naive bayes flexible bayes learning curves testing hypothesis behavior normality assumption holds 
perform better flexible bayes small sample scenario normality assumption correct 
investigate plotted histogram estimates density continuous attribute cleveland heart disease dataset conditioned class 
visual inspection find distributions roughly normal 
quite surprised discover systematic measurement errors 
shows surprising phenomenon working flexible bayes algorithm blood pressure patients rounded nearest note peaks 
medical reason suspect human blood pressure multiple standard mercury apparatus measuring blood pressure difficult get significant figures rounding common personal communication 
smooth curve shows gaussian density estimate rough curve shows kernel density estimate confused rounding errors recording blood pressure patients 
hypothesized attribute removed flexible bayes naive bayes perform comparably 
ran fold cross validation experiment blood pressure attribute removed methods achieved accuracy rate 
artificial domains positive results natural domains give confidence usefulness flexible bayes tool machine learning real domains poorly understood scientifically draw hard 
expectations directly tested natural databases 
expectations accuracy training set size flexible bayes naive bayes learning curves second hypothesis behavior normality violated 

normality assumption holds xjc normal flexible bayes naive bayes exhibit asymptotic performance naive bayes reach asymptote faster smaller training sets 

independence assumption holds normality assumption flexible bayes reach bayes optimal error rate naive bayes 
test hypotheses defined appropriate joint probability distributions independently sampled varying sized training sets distributions order plot learning curves algorithms 
constructed test set containing instances sampled distribution evaluate models induced training sets 
training set size independent samples constructed report average standard deviation runs 
examine hypothesis defined joint probability distribution continuous attribute binary class conditional distribution attribute class gaussian 
learning curves support hypothesis flexible bayes performs worse naive bayes size training set small performance approaches naive bayes training set grows 
test second hypothesis defined joint probability distribution continuous attribute binary class conditional distribution attribute class mixture gaussians 
learning curves support hypothesis 
naive bayes learning overly parsimonious model flexible bayes flexibility lets fit distribution 
training set instances flexible bayes accuracy 
numerical integration estimated bayes optimal error rate domain 
apparent impossibility situation flexible bayes outperforming bayes optimal error rate explained realizing estimate algorithm performance comes test set instances 
expect order magnitude test instances algorithm error match bayes optimal error 
summary compared flexible bayes algorithm natural benchmark naive bayes variety natural artificial domains finding encouragement evidence hypotheses 
discussion approach bayesian induction novel bear similarities research machine learning statistics 
density estimation figures prominently learning algorithms 
specht latest series papers kernel estimation guise probabilistic neural networks 
contrast approach method handles continuous features independence assumptions single dimensional density estimation done class dimensional estimations 
novel feature conjugate gradient algorithm optimize cross validation estimate error space smoothing parameters parameter dimension 
gave tremendous improvement domains expect adaptive kernel width improve results 
histogram oldest simplest methods density estimation 
kononenko reports experts discretize continuous features 
contrast dougherty kohavi sahami study methods automatically discretizing continuous features statistical information theoretic metrics 
report naive bayesian classifier combined discretization gives higher accuracy averaged domains 
algorithms explored samples large space possible algorithms framework suggests 
gives perspective discussed showing naive bayes flexible bayes relate previous 
representational power flexible naive bayesian classifier quite similar generalized additive models hastie tibshirani regression 
method predicts value continuous variable various nominal numeric input variables naive bayes single multivariate gaussian independence assumption gaussian single general density estimation arbitrary dependencies probabilistic neural networks naive bayes discretization standard naive bayes flexible naive bayes space encompassing algorithms discussed 
arbitrary possibly nonlinear functions 
logarithms equation may transformed generalized additive model 
approaches close method glance close inspection important differences 
geiger heckerman method learning gaussian networks bayes nets continuous variables estimated single gaussian distribution contrast approach densities estimated kernel estimation 
kononenko uses discretization fuzzy modification 
value continuous feature test instance 
assigning single interval kononenko gaussian assigns probabilistically intervals 
gaussian similar kernels obtain smooth density estimate training data 
research hope extend flexible bayes set kernel width adaptively 
crossvalidation resampling schemes relatively cheap context 
implementation discussed complexity cross validation wrapper method setting oe similar reported john kohavi pfleger may employed small medium sized databases 
possibility borrow cheeseman bayesian approach density estimation gaussian mixtures em algorithm dempster laird rubin 
reviewed naive bayesian classifier assumptions relies including common single gaussian distribution predictive attribute 
argued assumption violated domains proposed kernel estimation method approximate complex distributions 
experiments natural domains showed number cases flexible bayesian classifier generalizes better version assumes single gaussian 
experimental studies artificial data suggest approach behaves expected important scenarios 
remains done results date indicate flexible bayesian classifier constitutes promising addition repertoire probabilistic induction algorithms 
acknowledgments authors nils nilsson wray buntine helpful comments pointers relevant literature 
george john supported nsf graduate research fellowship pat langley supported part 
office naval research 
buntine 
operations learning graphical models journal artificial intelligence research 
casella berger 
statistical inference wadsworth brooks cole 
cheeseman kelly self stutz taylor freeman 
autoclass bayesian classification system machine learning proceedings fifth international workshop morgan kaufmann pp 

clark niblett 
cn induction algorithm machine learning 
cooper herskovits 
bayesian method induction probabilistic networks data machine learning 
dempster laird rubin 
maximum likelihood incomplete data em algorithm journal royal statistical society 
devroye 
equivalence weak strong complete convergence kernel density estimates annals statistics 
dougherty kohavi sahami 
supervised unsupervised discretization continuous features machine learning proceedings twelfth international conference morgan kaufmann 
geiger heckerman 
learning gaussian networks proceedings tenth conference uncertainty artificial intelligence pp 

hastie tibshirani 
generalized additive models chapman hall 
heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data proceedings tenth conference uncertainty artificial intelligence pp 


developments nonparametric density estimation journal am 
stat 
assoc 

john kohavi pfleger 
irrelevant features subset selection problem machine learning proceedings eleventh international conference morgan kaufmann pp 

kibler langley 
machine learning experimental science proceedings third european working session learning pitman publishing london uk pp 

kononenko 
semi naive bayesian classifier proceedings sixth european working session learning pittman porto portugal pp 

kononenko 
inductive bayesian learning medical diagnosis applied artificial intelligence 
langley sage 
induction selective bayesian classifiers proceedings tenth conference uncertainty artificial intelligence morgan kaufmann seattle wa pp 

langley iba thompson 
analysis bayesian classifiers proceedings tenth national conference artificial intelligence pp 

murphy aha 
uci repository machine learning databases available anonymous ftp ics uci edu pub machine learning databases directory 
pazzani 
searching attribute dependencies bayesian classifiers fifth international workshop artificial intelligence statistics pp 

provan singh 
learning bayesian networks feature selection fifth international workshop artificial intelligence statistics pp 

quinlan 
programs machine learning morgan kaufmann 

pattern recognition statistical structural neural approaches wiley 
silverman 
density estimation statistics data analysis chapman hall 
specht 
experience adaptive neural networks adaptive general regression neural networks ieee international conference neural networks orlando fl 
ripley 
modern applied statistics plus springer verlag 
