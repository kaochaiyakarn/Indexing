discriminative generative imitative learning tony jebara eng electrical engineering mcgill university sc media arts sciences mit submitted program media arts sciences school architecture planning partial fulfillment requirements degree doctor philosophy media arts sciences massachusetts institute technology february institute technology rights reserved signature author program media arts sciences december certified alex pentland toshiba professor media arts sciences program media arts sciences thesis supervisor accepted andrew lippman chair departmental committee graduate students program media arts sciences discriminative generative imitative learning tony jebara submitted program media arts sciences school architecture planning partial fulfillment requirements degree doctor philosophy media arts sciences propose common framework combines different paradigms machine learning generative discriminative imitative learning 
generative probabilistic distribution principled way model machine learning machine perception problems 
provides domain specific knowledge terms structure parameter priors joint space variables 
bayesian networks bayesian statistics provide rich flexible language specifying knowledge subsequently refining data observations 
final result distribution generator novel exemplars 
conversely discriminative algorithms adjust possibly non distributional model data optimizing specific task classification prediction 
typically leads superior performance compromises flexibility generative modeling 
maximum entropy discrimination med framework combine discriminative estimation generative probability densities 
calculations involve distributions parameters margins priors provably uniquely solvable exponential family 
extensions include regression feature selection transduction 
svms naturally subsumed augmented example feature selection obtain substantial improvements 
extend mixtures exponential families derive discriminative variant expectationmaximization em algorithm latent discriminative learning latent med 
em jensen lower bound log likelihood dual upper bound possible novel reverse jensen inequality 
variational upper bound latent log likelihood form em bounds computable efficiently globally guaranteed 
permits powerful discriminative learning wide range contemporary probabilistic mixture models mixtures gaussians mixtures multinomials hidden markov models 
provide empirical results standardized data sets demonstrate viability hybrid discriminative generative approaches med reverse jensen bounds state art discriminative techniques generative approaches 
subsequently imitative learning variation generative modeling learns exemplars observed data source 
distinction generative model agent interacting complex surrounding external world 
efficient model aggregate space generative setting 
demonstrate imitative learning appropriate conditions adequately addressed discriminative prediction task outperforms usual generative approach 
discriminative imitative learning approach applied generative perceptual system synthesize real time agent learns engage social interactive behavior 
thesis supervisor alex pentland title toshiba professor media arts sciences mit media lab discriminative generative imitative learning tony jebara thesis committee advisor alex pentland toshiba professor media arts sciences mit media laboratory advisor tommi jaakkola assistant professor electrical engineering computer science mit artificial intelligence laboratory reader david hogg professor computing pro vice school computer studies university leeds reader tomaso poggio helen whitaker professor mit brain sciences department mit artificial intelligence lab acknowledgments extend warm alex pentland sharing wealth brilliant creative ideas ground breaking visions computer human collaboration ability combine strengths different fields applications 
extend warm tommi jaakkola sharing knowledge machine learning pioneering ideas estimation excellent statistical mathematical abilities 
extend warm david hogg sharing tackle great challenging problems visionary ideas behavior learning ability span panorama perception learning behavior 
extend warm tomaso poggio sharing extensive understanding aspects intelligence biological psychological statistical mathematical computational enthusiasm science general 
members committee profoundly shaped ideas thesis helped formalize document 
group great team karen navarro elizabeth farley choudhury brian clarkson sumit basu yuri ivanov nitin sawhney vikram kumar ali steve schwartz rich dave berger josh weaver nathan eagle 
trg great source readings brainstorming yu jason rennie adrian master martin szummer romer rosales chen hsiang 
media lab folks deb roy joe paradiso bruce blumberg picard irene claudia yuan qi fernandez push singh bill mike johnson bill tomlinson sharing bold ideas deep thoughts 
great media lab friends moved places profound influence moghaddam bernt schiele oliver ali azarbayejani thad starner kris popat chris wren jim davis tom minka francois andy wilson nuno vasconcelos janet cahn lee campbell marina bers martin wagner ken russell 
folks outside lab mukherjee marina meila yann lecun michael jordan andrew mccallum andrew ng thomas hoffman john weng great conversations valuable insight 
family father mother sister 
possible sacrifice effort phd supported endeavor 
contents learning generative modeling learning generative models ai learning generative models perception learning generative models temporal behavior probability 
generative versus discriminative learning imitative learning objective scope organization generative vs discriminative learning schools thought generative probabilistic models discriminative classifiers regressors generative learning bayesian inference maximum likelihood conditional learning conditional bayesian inference maximum conditional likelihood logistic regression discriminative learning empirical risk minimization structural risk minimization large margin estimation contents bayes point machines joint generative discriminative learning maximum entropy discrimination regularization theory support vector machines solvability support vector machines kernels med distribution solutions med augmented distributions information theoretic geometric interpretation computing partition function margin priors bias priors gaussian bias priors non informative bias priors support vector machines single axis svm optimization kernels generative models exponential family models empirical bayes priors full covariance gaussians multinomials generalization guarantees vc dimension sparsity pac bayes bounds summary extensions extensions maximum entropy discrimination med regression svm regression generative model regression feature selection structure learning feature selection classification contents feature selection regression feature selection generative models transduction transductive classification transductive regression extensions mixture models latent variables latent discrimination cem exponential family mixtures mixtures exponential family mixtures product logarithmic space expectation maximization divide conquer latency conditional discriminative criteria bounding mixture models jensen bounds reverse jensen bounds mixing proportions bounds cem algorithm deterministic annealing latent maximum entropy discrimination experiments simple mixtures structured mixture models hidden markov models mixture mixtures jensen bounds reverse jensen bounds reverse jensen inequality hidden markov models bounding hmm gaussians bounding hmm multinomials conditional hidden markov models experiments discriminative hidden markov models contents latent bayesian networks data set bounds applications reverse jensen inequality background inequalities reversals derivation reverse jensen inequality mapping important condition family applying mapping invoking constraints virtual data simple loose bound tighter bound bounding dimensional ray scaling multidimensional formulation analytically avoiding lookup tables final appeal convex duality imitative learning imitation framework simple example limitations long term behavior data collection data processing task generative modeling perception generative model images generative model spectrograms hidden markov models temporal signals conditional hidden markov models imitation experiments training testing likelihoods resynthesis results resynthesis training data resynthesis test data contents discussion contributions theoretical applied appendix optimization med framework constrained gradient ascent axis parallel optimization learning axis transitions note convex duality numerical procedures reverse jensen inequality list figures examples generative models 
probabilistic perception systems 
example discriminative classifier 
imitative learning discrimination probabilistic perception 
aim mapping action perception 
scales discrimination integration learning 
directed graphical models 
graphical models conditioned bayesian inference vs conditional bayesian inference 
convex cost functions convex constraints 
med convex program problem formulation 
med information projection operation 
margin prior distributions associated potential functions 
discriminative generative models 
information projection operation bayesian generative estimate 
classification visualization gaussian discrimination 
formulating extensions med 
various extensions binary classification 
margin prior distributions associated potential functions 
med approximation sinc function 
roc curves splice site problem feature selection 
cumulative distribution functions resulting effective linear coefficients 
roc curves corresponding quadratic expansion features feature selection varying regularization feature selection levels protein classification 
list figures sparsification linear model 
cumulative distribution functions linear regression coefficients 
transductive regression vs labeled regression illustration 
transductive regression vs labeled regression flight control 
tree structure estimation 
graph standard mixture model expectation maximization iterated bound maximization maximum likelihood versus maximum conditional likelihood 
dual sided bounding latent likelihood jensen reverse jensen bounds log sum 
cem data weighting translation 
cem vs em performance gaussian mixture model 
gradient ascent conditional joint likelihood 
cem vs em performance yeast uci dataset 
iterated latent med projection jensen reverse jensen bounds 
graph structured mixture model jensen reverse jensen bounds mixture mixtures gaussian case 
jensen reverse jensen bounds mixture mixtures poisson case 
jensen reverse jensen bounds mixture mixtures exponential case 
conditional hmm estimation training data san francisco data set 
test predictions san francisco data set 
convexity preserving map lemma bound gaussian mean 
lemma bound gaussian covariance 
lemma bound multinomial distribution 
lemma bound gamma distribution 
lemma bound poisson distribution 
lemma bound exponential distribution 
dimensional log gibbs partition functions 
intermediate bound log gibbs partition functions 
fl 
list figures max fl 
linear upper bounds max fl 
quadratic bound log gibbs partition functions 
bounding separate components bounding numerical fl analytic fl 
dialog interaction analysis window gestural imitation learning framework 
online interaction learned synthetic agent 
channel audio visual imitation learning platform 
wearable interaction data 
audio visual prediction task 
bayesian network description pca 
bayesian network representing pca variant collections vectors 
reconstruction facial images pca variant 
reconstruction error images self world 
reconstruction error spectrograms self world 
audio video prediction hmms conditional hmm estimation training log likelihoods audio prediction hmm 
training log likelihoods video prediction hmm 
hmm resynthesis training data 
hmm resynthesis training data continued 
hmm resynthesis testing data 
constrained gradient ascent optimization med framework 
axis parallel optimization med framework 
approximating decay rate change objective function 
axis parallel med maximization learned axis transitions parameters linear upper bounds fl function 
list tables margin prior distributions associated potential functions 
crabs cancer classification prediction test results boston housing data 
prediction test results gene expression level data 
sample exponential family distributions 
jensen reverse jensen bound parameters 
cem em performance yeast data set 
em latent med performance pima indians data set 
svm polynomial kernel performance pima indians data set 
cem em performance hmm precipitation prediction 
testing log likelihoods audio prediction hmm 
testing log likelihoods video prediction hmm 
parameters linear upper bounds fl function 
chapter knowledge act learning greatest enjoyment 
karl friedrich gauss 
objective thesis propose common framework combines different paradigms machine learning generative discriminative imitative learning 
resulting mathematically principled framework suggests combined hybrid approaches provide superior performance flexibility individual learning schemes isolation 
generative learning heart approaches pattern recognition artificial intelligence perception provides rich framework imposing structure prior knowledge problem 
progress discriminative learning demonstrated superior performance obtained avoiding generative modeling focusing task 
powerful connection proposed generative discriminative learning combine complementary strengths schools thought 
subsequently propose connection discriminative learning imitative learning 
imitative learning automatic method learning behavior interactive autonomous agent 
process cast augmented discriminative learning formalism 
chapter discussing motivation machine learning general 
draw applied examples pattern recognition various ai domains machine perception 
communities identified various generative models specifically designed reflect prior knowledge respective domains 
generative models discarded considers discriminative approach ironically provides superior performance despite naive models 
motivates need find common formalisms synergistically combine different schools thought community 
describe ambitious instance machine learning imitative learning attempts learn autonomous interactive agents directly data 
form agent learning benefit cast discriminative generative paradigm 
chapter summary objectives scope provide brief overview rest chapters document 
process constructing framework expose important mathematical tools valuable right 
include powerful reversal celebrated jensen inequality projection approaches learning 
chapter 
learning generative modeling science provide exact deterministic models phenomena domains newtonian physics mathematical relationships governing complex systems partially specifiable 
furthermore aspects model may uncertainty incomplete information 
machine learning statistics provide formal approach manipulating nondeterministic models describing estimating probability density variables question 
generative density specify priori partial knowledge refine partially specified model empirical observations data 
system variables xt system specified joint probability distribution significant variables xt 
known generative model probability distribution generate samples various configurations system 
furthermore full generative model straightforward condition marginalize joint density inferences predictions 
domains greater sophistication ambitious tasks problems intricate complete models theories quantitative approaches difficult construct manually 
combined greater availability data computational power encourage domains migrate away rule manually specified models probabilistic data driven models 
partial amounts domain knowledge available seed generative model 
developments machine learning bayesian statistics provided rigorous formalism representing prior knowledge combining observed data 
progress graphical generative models bayesian networks permitted prior knowledge specified structurally identifying conditional independencies variables parametrically providing prior distributions :10.1.1.114.4996
partial domain knowledge combined observed data resulting precise posterior generative model 
examples generative models 
see different examples generative models 
hidden markov model depicted directed graph identifies high level conditional independence properties :10.1.1.131.2084:10.1.1.114.4996
specify markov structure states depend predecessors outputs depend current state 
hierarchical mixture experts portrayed :10.1.1.136.9119
similarly mixture model shown seen graphical model parent node selects possible gaussian emission distributions 
details formalism underlying generative models chapter 
provide background motivation examples multiple caveat addressed sections warns partially specified aspects model inaccurate suspect 
chapter 
applied fields generative models increasingly popular 
area natural language processing instance traditional rule boolean logic systems dialog lexis nexis giving way statistical approaches markov models establish independencies chain successive events 
medical diagnostics quick medical knowledge base initially heuristic expert system reasoning diseases symptoms augmented statistical decision theoretic formulation 
new formulation structures diagnostics problem layer bipartite graph diseases parents symptoms 
success generative probabilistic models lies genomics bioinformatics 
traditional approaches modeling genetic regulatory networks boolean approaches differential equation dynamic models challenged statistical graphical models 
model selection criterion identifies best graph structure matches training data 
addition visualization interdependencies variables graphical models principled formalism proven superior heuristic counterparts 
learning generative models ai artificial intelligence ai area general see similar migration rule expert systems probabilistic generative models 
example robotics traditional dynamics control systems path planning potential functions navigation models complemented probabilistic models localization mapping control 
multi robot control demonstrated probabilistic reinforcement learning approach 
autonomous agents virtual interactive characters example ai systems 
early days interaction gaming simple rule schemes weizenbaum eliza program natural language rules emulate therapy session 
similarly graphical virtual worlds characters generated rules cognitive models physical simulation kinematics dynamics 
traditional approaches currently combined statistical machine learning techniques 
learning generative models perception machine perception generative models machine learning prominent tools particular complexity domain sensors 
speech recognition hidden markov models method choice due probabilistic treatment acoustic coefficients markov assumptions necessary time varying signals :10.1.1.131.2084
auditory scene analysis sound texture modeling cast probabilistic learning framework independent component analysis 
word distributions modeled bigrams trigrams markov models topic modeling uses multinomial distributions 
topic spotting system shown tracks conversation multiple speakers displays related material users read 
similar emergence generative models computer vision domain 
techniques physics modeling structure motion epipolar geometry approaches complemented probabilistic models kalman filters prevent instability provide robustness sensor noise 
depicts system probabilistically fuses motion estimates extended kalman filter obtain rigid face structure just small collection examples generative models various fields means complete survey 
chapter 
pose estimate 
multiple hypothesis filtering tracking vision generative model markov chain monte carlo condensation algorithm 
sophisticated probabilistic formulations computer vision include markov random fields loopy belief networks perform super resolution 
generative models structured latent mixtures compute transformations invariants face tracking applications 
distributions eigenspaces mixture models skin color modeling image manifold modeling 
instance eigenspace photos face scans formed generative model spaces regress face shapes images real time 
color modeling done mixture gaussian models permit tracking augmented reality applications 
object recognition feature extraction benefited greatly probabilistic interpretation 
example histograms convolution operations images provide reliable recognition 
real time augmented reality 
probabilistic perception systems 
learning generative models temporal behavior simultaneously evolution proceeding field vision techniques transition low level static image analysis dynamic high level video interpretation 
temporal aspects vision domains relied extensively generative models dynamic bayesian networks particular 
temporal tracking benefited generative models extended kalman filters 
tracking applications hidden markov models frequently recognize gesture spatiotemporal activity 
richness graphical models permit chapter 
straightforward combinations hidden markov models kalman filters switching linear dynamical systems modeling gaits driving maneuvers 
variations graphical models include coupled hidden markov models appropriate modeling interacting processes vehicle pedestrian traffic 
bayesian networks multi person interaction modeling 
classifying football plays 
forecasting temporal activity reviewed santa fe competition various approaches including hidden markov models compared 
probability 
efficient create probability distribution variables generative way 
previous systems distinction roles different variables merely trying model phenomenon 
inefficient simply trying learn particular tasks need solved interested characterizing behavior complete system 
additional caveat generative density estimation formally ill posed problem 
density estimation circumstances cumbersome intermediate step forming mapping variables input output 
dilemma explicated chapter 
issue difficulty density estimation terms sample complexity large amount data may necessary obtain generative model system may need small sample learn required input output sub task discriminatively 
furthermore ai perceptual systems structures priors representations invariants background knowledge designed system domain expert 
just due learning power estimation algorithms seeded right framework priori structures learning 
alleviate amount manual effort process take human knowledge engineering loop 
way require accurate generative model designed require domain expertise front 
discriminative learning algorithms powerful learning robust errors design process remain effective despite incorrect modeling assumptions 
generative versus discriminative learning previous applications described compelling evidence strong arguments generative models joint distribution estimated variables 
ironically flexible models outperformed cases relatively simpler models estimated discriminative algorithms 
generative modeling approach modeling tools available combining structure priors invariants latent variables data form joint density tailored domain hand discriminative algorithms directly optimize relatively domain specific model classification regression task hand 
example support vector machines directly maximize margin linear separator sets points euclidean space 
model simple linear maximum margin criterion appropriate maximum likelihood generative model criteria 
domain image digit recognition support vector machines svms produced chapter 
state art classification performance 
regression time series prediction svms improved generative approaches maximum likelihood logistic regression :10.1.1.127.1519:10.1.1.127.1519
text classification information retrieval support vector machines transductive support vector machines surpassed popular naive bayes generative text models 
computer vision person detection recognition gender classification dominated svm frameworks surpass maximum likelihood generative models approach human performance 
genomics bioinformatics discriminative systems play crucial role 
furthermore speech recognition discriminative variants hidden markov models demonstrated superior large corpus classification performance 
despite ambiguous models systems discriminative estimation process yields improvements sophisticated models tailored domain generative frameworks 
examples discriminative classifier 
deeply complementary advantages generative discriminative approaches algorithmically directly compatible 
community go far say exist somewhat disconnected camps generative modelers discriminative estimators :10.1.1.46.6208:10.1.1.44.7709
quickly discuss general aspects schools thought chapter details approaches previous efforts bridge machine learning literature 
generative models provide user ability seed learning algorithm knowledge problem hand 
terms structured models independence graphs markov assumptions prior distributions latent variables probabilistic reasoning 
focus generative models describe phenomenon try generate configurations 
context building classifiers predictors regressors task driven systems density estimation variables full generative description system inefficient intermediate goal 
clearly estimation frameworks probabilistic generative models optimize parameters specific task 
models generic optimization criteria maximum likelihood oblivious particular classification prediction regression task hand 
discriminative techniques support vector machines little offer terms structure modeling power achieve superb performance test cases 
due inherent direct optimization task related criterion 
example shows appropriate criterion binary classification largest margin separation boundary rd order polynomial model 
focus classification opposed generation properly allocating computational resources directly task required 
previously mentioned fundamental differences approaches term sophisticated refer extra tailoring generative model traditionally obtains user incorporate domain specific knowledge problem hand terms priors structures 
claim relative mathematical sophistication generative discriminative models 
chapter 
making awkward combine strengths principled way 
considerable value propose elegant framework subsume unite schools thought challenge undertaken thesis 
imitative learning far provided motivation multiple instantiations machine learning generative models applied domains perception temporal modeling autonomous agents 
due common probabilistic platform threading domains natural consider combinations couplings systems 
platforms investigate combining synergistic approaches learning imitative learning 
imitative learning learn autonomous agent exhibits interactive behavior 
imitative learning provides easy approach learning agent behavior providing real examples agents interacting world learned generalized 
components process passively perceiving real world behavior learning portrayed 
basic notion generative model perceptual level able regenerate virtual characters keeping discriminative model temporal learning focus resources prediction task necessary action selection 
conceptual loop implementation elaborated chapter 
briefly situate imitative learning context agent learning approaches motivate background related 
discriminative learning prediction imitation generative perception synthesis imitative learning discrimination probabilistic perception 
various approaches proposed learning autonomous agents domains robotics interactive graphics 
utilize rule discriminative generative models distinguish manner learning process cast agent behavior model 
example data acquired data organized labeled task agent generalize forth 
traditional rule systems agent modeling require cumbersome enumeration decisions 
simpler alternative supervised learning teacher provides exemplars optimal behavior context learning algorithm generalizes samples 
supervision required reinforcement learning 
remains popular approaches agent learning part due strong ethological cognitive science roots 
agent explores action space rewarded performing says types learning reflection second imitation easiest third experience 
chapter 
appropriate actions appropriate context 
supervision minimized due simplicity providing reward signal supervision process done active online setting 
imitative learning sense combination supervised unsupervised teacher learning 
collect data real people interacting real world shown exemplars appropriate behavior response current context 
data labeled needs teaching effort supervision 
course assumes perceptual techniques record represent natural real world activity automatically 
incarnation imitative learning involves data collection avoids manual supervision 
reason investigate implement discriminative generative models propose 
ideally agent utilize mixture learning methodologies multiple learning scenarios introspect bootstrap metalearning approach undertaking ambitious 
leave learning learn aspect agent behavior interesting topic research 
point quickly motivate background imitation learning spans multiple fields cognitive sciences philosophy psychology neuroscience robotics full survey scope thesis 
early research behavior cognitive sciences exhibited strong interest role imitative learning 
ground breaking works thorndike piaget followed lull area movement imitation 
part due presumption imitation mimicry entity necessarily sign higher intelligence critical development 
prejudice slowly faded arrival studies moore indicated infants ability perform facial manual gesture imitation ages days old cases hour old 
imitative learning began seen innate mechanism help development humans certain species 
furthermore demonstrated absent lower order animals limited 
addition discoveries mirror neurons action perception pathways functional magnetic resonance imaging results neural basis imitative learning hypothesized certain experiments indicated consistent firings mirror neuron action performed subject individual perceived performing action 
addition imitation suggested possible basis language learning 
results spurred applied efforts imitative approaches robotics mataric brooks imitation gained visibility complemented reinforcement learning 
arguments imitation learning include improved acquisition complex visual gestures human subjects 
domains predominantly focused uncovering direct mappings action perception 
mapping imitation learning problem translated direct supervised learning 
complex mapping certain extent achilles heel imitation learning 
effort humanoid robot imitation rests resolving moore active mapping aim problem 
creation mapping visual perception teacher movement high level representations matched high level representations learner action space proprioceptive senses see 
effectively aim problem change coordinates task intermediate representations allow mapping learner action space various perceived situations various teachers 
key challenge driven substantial effort area humanoid robotics 
various simplifications aim problem permit implementation faster learning robot behavior result simplification action perception spaces discovery imitation neurons generated extreme enthusiasm psychology predicted provide field leap forward equivalent progress biology obtained dna 
suggested mirror neurons may explain certain phenomena autism 
chapter 
consequently generate uninteresting robot behavior 
example billard simplifies action spaces perception spaces low dimensional discrete representation simple learning mechanism rote learner resolve mapping 
representation higher order representation perceptual space action space higher order mapping aim mapping action perception 
alternative approach away aim problem altogether providing teacher perceptual data terms action space learner considering virtual characters action space perceptual space 
example weng describes human pushing robot hallway robot collects images context 
actuators robot cameras measure human displacement imitate human displacement values need appropriate visual context 
hogg alternatively describes vision system obtains perceptual measurements needs behavior visual space generate action virtually 
methods cleverly avoid direct mapping perception teacher activity learner action space 
clearly lack higher order representation perception action requires extra care 
changes coordinate system abstracted away dealt front 
actuators weng robot replaced totally different motor system vision system hogg virtual characters pointed radically new scene 
way side step lack abstraction layer lock perceptual action coordinate system 
may difficult learner acquire lessons multiple teachers multiple contexts prevents types long term training developmental data scenarios 
furthermore long term training scenario important weed irrelevant data outliers behavioral data focus resources discriminatively defining exemplars training data 
second challenge thesis implement imitative learning show cast discriminative generative formalisms described previously 
extends theoretical combination discriminative generative learning practical applied task imitation learning 
objective pose main challenges 
seek combined discriminative generative framework extends powerful generative models popular machine learning community discriminative frameworks support vector machines 
seek imitative learning approach casts interactive behavior acquisition discriminative generative framework propose 
features subgoals chapter 
large contributions strive 
list enumerates detail 
ffl combined generative discriminative learning ideally combination generative discriminative learning done formal level easily extensible related domains approaches 
provide discriminative classification framework retains formalism bayesian generative modeling appeal maximum entropy connections bayesian approaches 
formalism powerful connections regularization theory support vector machines important principled approaches discriminative school thought 
ffl formal generalization guarantees empirical validation support combined generative discriminative framework refer reader formal generalization guarantees different perspectives 
various arguments literature sparsity vc dimension pac bayes generalization bounds compatible framework 
ffl applicability spectrum bayesian generative models span wide variety generative models focus exponential family central statistics maximum likelihood estimation 
discriminative methods consistently applicable large family distributions 
ffl ability handle latent variables strength generative models lies ability handle latent variables mixture models ensure discriminative method span higher order multimodal distributions 
novel bounds extend classical jensen inequality permits generative modeling apply mixtures 
ffl analytic reversal jensen inequality analytic reversal jensen inequality useful statistics provides new mathematical tool 
reversal permit various important manipulations particular discriminative estimation latent generative models 
mathematical tool permits dual sided bounds probabilistic quantities single bound 
ffl computational efficiency development discriminative generative imitative learning procedures consistently discuss issues computational efficiency implementation 
frameworks shown viable large data scenarios computationally tractable traditional counterparts 
ffl extensibility extensions demonstrated hybrid generative discriminative approach justify usefulness 
include ability handle regression multiclass classification transduction feature selection structure learning exponential family models mixtures exponential family 
ffl casting imitative learning generative discriminative framework chapter 
bring imitative learning generative discriminative setting describing generative models perceptual domain temporal domain 
augmented discriminatively learning prediction model synthesize interactive behavior autonomous agent 
ffl combining perception learning behavior acquisition demonstrate real time system performs perception learning acquires synthesizes real time behavior 
closes loop proposed shows common discriminative probabilistic framework extended multiple facets large system 
scope thesis focuses computational statistical aspects machine learning machine perception 
discussions different types learning discriminative generative conditional imitative reinforcement supervised unsupervised refer primarily computational mathematical aspects terms 
connections large bodies cognitive sciences psychology neuroscience philosophy ethology forth invariably arise brought motivation implementation purposes necessarily taken direct challenge conventional fields 
organization thesis organized follows ffl chapter complementary advantages discriminative generative learning discussed 
formalize models methods inference generative conditional discriminative learning 
various advantages disadvantages enumerated motivation methods fusing 
ffl chapter maximum entropy discrimination formalism introduced method choice combining generative models discriminative estimation setting 
formalism extension regularization theory shown subsume support vector machines 
discussion margins bias model priors 
med framework extended handle generative models exponential family 
comparisons art support vector machines learning algorithms 
generalization guarantees med provided appealing results literature 
ffl chapter various extensions maximum entropy discrimination formalism proposed elaborated 
include multiclass classification regression feature selection 
furthermore transduction discussed optimization issues 
chapter motivates need latent models med mixtures exponential family 
comparisons art support vector machines learning algorithms 
chapter 
ffl chapter latent learning motivated discriminative setting reverse jensen bounds 
socalled conditional expectation maximization framework proposed latent conditional discriminative med problems 
bounds exponential family mixture models mixing coefficients 
comparisons state art expectationmaximization approaches 
ffl chapter consider case discriminative learning structured mixture models mixture flat additional complications generate intractable number latent configurations 
case bayesian networks generally prevents tractable computation reverse jensen bounds 
show reverse jensen bounds computed efficiently circumstances extend applicability latent discrimination cem structured mixture models hidden markov models mixture models aggregated data set 
ffl chapter chapter begins brief discussion mathematical inequalities community including prior reversals converses jensen inequality 
derivation inequality discriminative learning performed justify form give global guarantees bounds 
ffl chapter imitative learning cast discriminative temporal prediction problem agent action predicted previous state world state 
implementation details hardware perceptual systems discussed 
generative model audio images temporal structure provides representations discriminative prediction problem 
synthesized behavior demonstrated quantitative measurement performance 
ffl chapter advantage joint framework generative discriminative imitative learning reiterated 
various contributions thesis summarized 
extensions elaborations proposed 
ffl chapter appendix gives standard derivations called main body thesis 
includes brief discussion convex duality details numerical procedures mentioned chapter 
chapter generative vs discriminative learning models wrong useful george box chapter situate discriminative generative learning formally context estimation algorithms criteria optimize 
natural intermediate conditional learning helps visualize coarse continuum extremes 
describes panorama approaches go horizontally extreme generative criteria discriminative criteria 
similarly scale variation vertical seen estimation procedures go direct optimizations criteria training data regularized ones fully averaged ones attempt better approximate behavior criteria data overfitting 
chapter sample generative discriminative techniques explore entries detail 
generative models extreme attempt estimate distribution variables inputs outputs system 
inefficient need conditional distributions output input perform classification prediction motivates minimalist approach conditional modeling 
practical systems minimalist need single estimate conditional distribution 
conditional modeling may inefficient motivates discriminative learning considers input output mapping 
conclude hybrid frameworks combining generative discriminative models point limitations 
risk box truly intended say robust statistics shall quote motivate combining usefulness generative models robustness practicality performance discriminative estimation 
chapter 
generative vs discriminative learning generative conditional discriminative local local prior model averaging maximum likelihood maximum posteriori bayesian inference maximum conditional likelihood maximum mutual information maximum conditional posteriori conditional bayesian inference empirical risk minimization support vector machines regularization theory maximum entropy discrimination bayes point machines scales discrimination integration learning 
schools thought called schools thought discriminative generative approaches 
alternative descriptions formalisms include discriminative versus informative approaches :10.1.1.46.6208:10.1.1.46.6208
generative approaches produce probability density model variables system manipulate compute classification regression functions 
discriminative approaches provide direct attempt compute input output mappings classification regression eschew modeling underlying distributions 
holistic picture generative models appealing completeness wasteful non robust 
furthermore box says models wrong useful 
graphical models prior structures enforce generative models may useful elements treated cautiously real world problems true distribution coincide constructed 
fact bayesian inference guarantee obtain correct posterior estimate class distributions consider contain true generator data observing 
show examples schools thought elaborate learning criteria section 
generative probabilistic models generative bayesian probabilistic models system input covariate features output response variables unobserved variables represented homogeneously joint probability distribution 
variables discrete continuous may multidimensional 
generative models define distribution variables classification regression standard marginalization conditioning operations :10.1.1.46.6208
generative models probability densities current typically span class exponential family distributions mixtures exponential family 
specifically popular models various domains include gaussians naive bayes mixtures multinomials mixtures gaussians mixtures experts hidden markov models sigmoidal belief networks bayesian networks markov random fields forth :10.1.1.131.2084:10.1.1.136.9119
variables form xn full joint distribution form xn 
joint distribution accurately captures possibly chapter 
generative vs discriminative learning istic relationships variables straightforward inference answer queries 
done straightforward manipulations basic axioms probability theory marginalizing conditioning bayes rule xn jx jx jx conditioning joint distribution easily form classifiers regressors predictors straightforward manner map input variables output variables 
instance may want obtain estimate output may discrete class label continuous regression output input conditional distribution jx 
bayesian argue appropriate answer conditional distribution jx practice settle approximation obtain example may randomly sample jx compute expectation jx find mode distribution argmax jx 
directed graphical models 
ways constrain joint distribution fewer degrees freedom directly estimate data 
way structurally identify conditional independencies variables 
depicted example directed graph bayesian network 
graph identifies joint distribution factorizes product conditional distributions variables parents parents variable node xn pi jx alternatively parametrically constrain distribution giving prior distributions variables hyper variables affect 
example may restrict variables jointly mixture gaussians unknown means covariance equal identity ffn gamma ff types restrictions exist example related sufficiency separability conditional distribution simplify mixture simpler conditionals jx ffp jx gamma ff jx chapter 
generative vs discriminative learning versatility inherent working joint distribution space insert knowledge relationships variables invariants independencies prior distributions forth 
includes variables system unobserved observed input output variables 
generative probability distributions flexible modeling tool 
unfortunately learning algorithms combine models observed data produce final posterior distribution inefficient 
finding ideal generator data combined prior knowledge intermediate goal settings 
practical applications wish generators ultimate tasks classification prediction regression 
optimizing intermediate generative goal sacrifice resources performance final discriminative tasks 
section discuss techniques learning data generative approaches 
discriminative classifiers regressors discriminative approaches explicit attempt model underlying distributions variables features system interested optimizing mapping inputs desired outputs say discrete class scalar prediction :10.1.1.46.6208
resulting classification boundary function approximation accuracy regression adjusted intermediate goal forming generator model variables system 
focuses model computational resources task provides better performance 
popular successful examples include logistic regression gaussian processes regularization networks support vector machines traditional neural networks :10.1.1.48.9258
robust discriminative classification regression methods successful areas ranging image document classification problems analysis time series prediction 
techniques support vector machines gaussian process models boosting algorithms standard related statistical methods logistic regression robust errors structural assumptions :10.1.1.32.8918
property arises precise match training objective criterion methods subsequently evaluated 
surrogate intermediate goal obtain generative model 
discriminative algorithms extend classifiers regressors arising generative models resulting parameter estimation hard :10.1.1.46.6208
models discriminative techniques parametric lack elegant probabilistic concepts priors structure uncertainty forth beneficial generative settings 
alternative notions penalty functions regularization kernels forth 
furthermore learning modeling focus discriminative approaches lack flexible modeling tools methods inserting prior knowledge 
discriminative techniques feel black boxes relationships variables explicit generative models 
furthermore discriminative approaches may inefficient train require simultaneous consideration data classes 
inefficiency arises discriminative techniques task discriminative inference engine needs solve requires different model new training training session 
various methods exist alleviate extra arising discriminative learning 
include online learning easily applied 
boosting procedures 
necessary construct possible discriminative mappings system variables require exponential number models 
frequent tasks canonical classification regression objectives targeted handful discriminative models generative model kept handling occasional missing labels unusual chapter 
generative vs discriminative learning types inference forth 
section discuss techniques learning data techniques discriminative approaches 
generative learning variations learning generative models data 
approaches priors model selection criteria include minimum description length bayesian information criterion akaike information criterion entropic priors survey scope thesis 
quickly discuss popular classical approaches include bayesian inference maximum posteriori maximum likelihood estimation 
seen ranging scale fully weighted averaging generative model hypothesis bayesian inference local computation simple regularization priors maximum posteriori simplest maximum likelihood estimator considers performance training data 
bayesian inference bayesian inference probability density function vector typically estimated training set vectors generally need vector correspond multiple observable variables continuous discrete :10.1.1.46.6208
assume vectors loss generality 
joint bayesian inference estimation process shown equation 
theta zj theta theta integrating theta essentially integrating pdf probability density function models possible 
involves varying families pdfs parameters 
impossible sub family selected parameterization theta varied 
theta parameterization pdf weighted likelihood training set 
having obtained compactly compute probability point continuous discrete probability space evaluating pdf manner necessarily ultimate objective 
components vector input learning system required estimate missing components output 
words broken sub vectors conditional pdf computed original joint pdf vector equation 
conditional pdf yjx superscript indicate obtained previous estimate joint density 
input specified conditional density density desired output system 
element may continuous vector discrete value sample probability space 
density required function learning system final output estimate need expectation arg max yjx 
yjx dy dy yj theta theta xj theta theta derivation deliberately expanded bayesian integral emphasize typical task unsupervised learning 
typical task supervised learning 
chapter 
generative vs discriminative learning permit differentiate joint bayesian inference technique conditional counterpart conditional bayesian inference 
maximum likelihood traditionally computing integral equation straightforward bayesian inference approximated maximum posteriori map maximum likelihood ml estimation equation 

zj theta theta ae arg maxp arg maxp zj theta theta map arg maxp zj theta ml iid independent identically distributed data conditions easier compute maximum logarithm quantities results arg max 
expand maximum likelihood case follows log zj theta log theta optimization joint likelihood additive sense data points contributes additive way logarithm facilitates optimization 
exponential family distributions 
maximum likelihood maximum posteriori seen approximations bayesian inference integral distribution models replaced mode 
posteriori solution allows prior regularize estimate maximum likelihood approach merely optimizes model training data may cause overfitting 
map permits user insert prior knowledge parameters model bias solutions generalize 
example may consider priors favor simpler models avoid overfitting entropic priors model 
maximum likelihood criterion considers training examples optimizes model specifically 
guaranteed converge model true distribution obtain samples overfit show poor generalization limited samples available 
ml local solution map tuned training data map tuned data prior approximates weighted averaging models bayesian inference solution closely 
furthermore important duality ml map approximations maximum entropy exists :10.1.1.114.4996
standard approaches solve distribution closest uniform kullback leibler divergence sense moments empirical distribution entropy projection moment constraints 
maximum likelihood map extensive asymptotic convergence properties efficient straightforward compute exponential family distributions latent variables complete data case 
case incomplete data mixture models sophisticated generative models maximum likelihood map estimates directly computable exist local maxima objective function 
situations standard iterative algorithms expectation maximization em variants 
em algorithm frequently utilized perform maximization likelihood mixture models due monotonic convergence properties ease implementation chapter 
generative vs discriminative learning 
expectation step consists computing bound log likelihood straightforward application jensen inequality 
maximization step usual maximum likelihood step complete data model 
furthermore various generalizations em proposed bring deep geometric concepts simplifying tools problem :10.1.1.37.8662
mixture models bayesian inference setting maximization appropriate 
jensen bounds em uses bound integration 
process known variational bayesian inference provides precise approximation bayesian inference making computation tractable latent models 
conditional learning generative learning seeks estimate probability distribution variables system including inputs outputs possible efficient task trying solve explicit 
know precisely conditional distributions appropriate directly optimize conditional distributions generative model 
probability model exclusively compute conditional outputs response variables inputs covariates directly optimize parameters fit model data task done optimally 
quite discriminative learning fitting probability density output distribution generative model outputs inputs yjx 
discriminative setting consider final estimate extract distribution winner take type scenario 
view conditional learning intermediate discriminative generative learning 
optimizing probability distribution ultimately classification regression purposes 
spirit minimalism away need learn joint generative model say focus conditional distribution yjx 
conditional bayesian inference obtaining conditional density unconditional joint probability density function equation equation roundabout shown suboptimal 
remained popular convenient partly availability powerful techniques joint density estimation em 
know priori need conditional density evident estimated directly training data 
direct bayesian conditional density estimation defined equation 
vector input covariate output response estimated 
training data course explicitly split corresponding vector sets 
note conditional density referred yjx distinguish expression equation 
yjx yjx theta jx theta yjx theta theta jx theta yjx theta theta jx theta theta parameterizes conditional density yjx 
theta exactly parameterization conditional density yjx results joint density parameterized theta 
initially intuitive expression yield exactly conditional density chapter 
generative vs discriminative learning 
natural yjx equal yjx theta just conditioned version theta 
words expression equation conditioned equation result equation identical 
conjecture wrong 
closer examination note important difference 
theta integrating equation theta equation 
direct conditional density estimate equation theta parameterizes conditional density yjx provides information density fact assume conditional density parameterized theta just function parameters 
essentially ignore relationship underlying joint density parameterized theta 
conditional model term theta jx equation behaves differently similar term equation 
illustrated manipulation involving bayes rule shown equation 
theta jx yj theta theta yj theta theta theta yj theta theta final line equation important manipulation noted theta replaced 
implies observing theta affect probability operation invalid joint density estimation case theta parameters determine density domain 
conditional density estimation observed theta independent way constrains provides information density merely conditional density yjx 
independence property hold strictly assuming parameterization theta conditional functional dependence parameters input variables marginal distribution induced theta 
graphical models depict difference joint density models conditional density models directed acyclic graph 
note theta model independent observed conditional density estimation scenario 
graphical terms theta joint parameterization parent children nodes conditional parameterization theta data parents child marginally independent 
equation illustrates directly estimated conditional density solution yjx joint density estimation conditional density estimation graphical models yjx yjx theta theta jx theta yjx theta yj theta theta theta yjx theta yj theta theta theta yjx chapter 
generative vs discriminative learning conditional density required appears superior perform conditional bayesian inference perform joint bayesian inference subsequently condition answer 
illustrated example 
example joint versus conditional bayesian inference specific example demonstrate difference argue favor conditional estimate yjx versus conditioned joint estimate yjx details appendix 
demonstrate simple component gaussian mixture model identity covariance equal mixing proportions shown 
likelihood data point conditioned joint density estimate conditional density estimate conditioned bayesian inference vs conditional bayesian inference 
zj theta 
prior theta parameters means theta wide zero mean spherical gaussian distribution large covariance oe 
infer standard joint bayesian distribution total training data points equation 
yj theta yj theta theta theta yj theta pi theta theta theta yj theta pi theta theta equation solved exactly expand products terms mixture model 
unfortunately grow exponentially fast possible assignments data points gaussians computed small data sets 
point data set compute joint bayesian inference plot shown 
conditioning gives conditional yjx superscript shows conditional came joint bayesian inference 
function yjx plotted value gamma 
proceed compute yjx directly integration conditional bayesian inference equation 
resulting function yjx different yjx plotted 
note conditional bayesian inference captures data lost regular bayesian inference 
yjx yjx theta yj theta theta theta yjx theta pi jx theta theta theta chapter 
generative vs discriminative learning yj theta yj theta dy pi theta yj theta dy theta theta maximum conditional likelihood bayesian inference integration conditional bayesian inference equation typically intractable evaluate closed form 
approximate average models simply pick model mode integral 
results corresponding maximum conditional posteriori map maximum conditional likelihood ml solutions shown equation 
posteriori solution allows prior regularize estimate conditional likelihood approach merely optimizes model training data may cause overfitting 
yjx yjx theta theta ae arg maxp yj theta theta map arg maxp yj theta ml typically find maximum logarithm quantities results arg max 
expand maximum conditional likelihood case follows log yj theta log jx theta log theta theta log theta gamma log theta dy optimization conditional likelihood similar maximum likelihood extra negative term referred background probability marginal input distribution 
trying maximize joint likelihood input output minimizing marginal likelihood input data 
sets interesting metaphor class conditional model attracted data fit joint likelihood background data belong model class 
criteria conditional likelihood disguise causes confusion 
example speech recognition literature conditional maximum likelihood referred maximum mutual information 
currently hidden markov models speech community trained conditional criteria obtain state art performance large corpus data sets 
maximum likelihood able handle incomplete latent models years em algorithm conditional likelihood traditionally difficult maximize especially mixture model scenario 
fact maximizing conditional likelihood non latent models give rise computational difficulties background probability involves log sum log integral outputs classes scalars break concavity 
approaches resort gradient descent :10.1.1.136.9119
variants gradient descent line search emerging statistics 
conditional version em algorithm proposed cem algorithm discussed chapter 
em conditional expectation maximization cem iterates bounding conditional likelihood solving resulting simpler complete data maximization 
converges iteratively chapter 
generative vs discriminative learning monotonically maximum conditional likelihood solution 
variational bayes similar cem bounds conditional posteriors conditional likelihoods prior integration result better tractable approximation conditional bayesian inference 
provide generative model optimized task hand relying fully bayesian formalism 
leave variational conditional bayesian inference interesting direction apply novel bounds 
logistic regression maximum conditional likelihood sense maximum likelihood closely related logistic regression popular technique statistics community 
logistic regression conditional distribution binary output variable input vector typically conditional model formula parameter vector jx exp gamma 
generates linear classifier varied parameter vector referred generalized linear model 
various ways augment framework computing higher order features vector 
include handling discrete values considering indicator features 
discriminative learning discriminative learning goes conditional learning perspective minimalist 
final mapping input output important final estimate produced considered :10.1.1.46.6208
estimation conditional distribution yjx viewed unnecessary intermediate step just previously argued estimation joint distribution deemed inefficient alternatively may consider quantities resulting classifier example margin distances decision boundary nearest exemplars 
discriminative techniques consider decision boundary regression function approximation evaluating parameters model 
learning algorithm closely matched final task system discriminative learning techniques resources intermediate goal generative modeling 
resulting performance classifier regressor improved 
longer consider distribution criterion bayesian likelihood learning techniques immediately applicable 
empirical risk minimization opposed previous sections started averaging solutions bayesian integration moved empirical local approximations maximum likelihood empirical approach optimizing discriminative classifier regressor show averaging regularization subsequently 
empirical risk minimization erm discriminative estimation criterion assumptions distribution input output 
erm typically loss function form theta measures penalty incurred data point input desired output assigning parameter theta model 
concern distinction conditional learning discriminative learning currently established convention field 
chapter 
generative vs discriminative learning empirical local solution minimize loss function training data set total training data points 
average loss called empirical risk remp theta theta meant coarse approximation true loss classifier unknown distribution samples known expected risk theta thetay theta dxdy limit infinite data loss functions equal theta value 
theta specifies mapping produce estimated input loss function measures level disagreement possible choices quadratic loss ky gamma binary winner take loss classification 
reinterpret loss functions corresponding likelihood specific choice conditional output distribution may awkward 
important aspect erm emphasis actual output resulting deterministic classification boundary formed 
example may choose compute classification result winner take sense case loss select class appropriately loss 
type hard classification fundamental discriminative estimation 
awkward represent conditional distribution logistic regressor possibility sharp version logistic function jx ae structural risk minimization large margin estimation erm locally attempting optimize model training data necessarily coincide true expected risk may exhibit generalization behavior data 
alternative consider augmenting local solution prior regularizer favors estimates agree data measure model capacity 
form regularized erm called structural risk minimization srm capacity measured terms called vapnik chervonenkis dimension 
srm perform training data erm generalize better data 
risk expected loss samples outside training set bounded empirical risk plus term depends size training set vc dimension classifier non negative integer quantity measures capacity classifier independent distribution data 
bound expected loss holds probability gamma ffi theta remp theta log gamma log ffi srm principle suggests minimize upper bound theta minimize expected risk 
seek minimize combination expected risk vc dimension chapter 
generative vs discriminative learning linear classifiers motivates classifier fits data large margin 
large margins mean try maximize minimum distance points decision boundary separates 
principles give rise support vector machine svm 
svms particularly important contemporary machine learning provided state art classification regression performance 
senses current discriminative estimation 
due fundamentally discriminative formalism don enjoy flexibility generative modeling priors invariants structure latent variables limits applicability 
bayes point machines alternative srm svms consider single solution fits data conjunction helpful regularizer prior consider weighted combination possible models 
bayesian inference better generalization properties consider discriminative averaging classifier 
instance may attempt average linear classifiers perfectly classify training data hard classification mainly discriminative concept 
problem cast framework bayesian inference specifically conditional bayesian inference problem 
popular approximation true bayesian inference called bayes point machine bpm 
tractability reasons bpm true result bayesian inference single point approximation 
summing effect linear classifier models bpm uses single model closest mean continuous space valid models linear classifiers perfect classification accuracy training 
bayesian way average linear models 
averaging soft probabilistic weighting done discriminative criterion binary decision count classifiers perfectly separates training data 
corresponds conditional distribution equation 
averaging models bring forth slightly better generalization properties bayes point machine bpm 
unfortunately practice performance exceed svms consistent manner 
furthermore bpm easily handle non separable data sets averaging multiple models perfect classification yield feasible solution whatsoever 
practical consideration bpm difficult compute requiring computational effort far surpasses generative modeling approaches svms implemented efficiently 
main concern bpm counterparts really designed handle linear models kernel nonlinearities 
easily computable classifiers arising large spectrum generative models 
instance exponential family mixtures exponential family easily estimated bpm framework 
don enjoy flexibility generative modeling priors non separability invariants structured models latent variables limits applicability 
discriminative averaging framework addresses limitations maximum entropy discrimination med introduced chapter 
joint generative discriminative learning having explored spectrum discriminative generative modeling see strong argument hybrid approach combines deeply complementary schools thought 
fusing versatility flexibility generative models power discriminative framework chapter 
generative vs discriminative learning focuses resources task extremely valuable 
furthermore argued averaging approach opposed local regularized local fit training data promises better generalization principled bayesian treatment 
approaches proposed combining generative discriminative methods 
include bayesian estimation special priors automatic relevance detection 
explored discriminative learning context simple linear kernel models show applicability large spectrum generative models 
alternative technique involves modular combination generative modeling subsequent svm classification fisher kernels :10.1.1.44.7709
technique readily applicable large spectrum generative models easily estimated maximum likelihood mapped features kernels training svm 
piece meal cascaded approach maximum likelihood followed large margin estimation fully take advantage power techniques 
example generative models estimated maximum likelihood non discriminative criterion collapse important aspects model sacrifice modeling power particularly latent situations 
example due model mismatch pre specified class generative model may flexibility capture information training data 
possible model resources misused encode aspects data irrelevant discrimination task related information 
ultimately results loss valuable modeling power feed subsequent svm layer making late exploit aspects generative model discrimination 
instance maximum likelihood hmm trained speech data may focus modeling power vowels sustained longer consonants preventing meaningful set features discrimination final svm stage 
words iteration generative modeling discriminative learning maximum likelihood estimate adjusted response svm criteria 
simultaneous computation generative model discriminative criterion improve technique chapter maximum entropy discrimination formalism hybrid generative discriminative model desirable qualities far motivated 
proposed med framework principled averaging technique able span large spectrum generative models simultaneously perform estimation discriminative criterion 
method formally encompassed maximum entropy discrimination technique :10.1.1.44.7709
chapter maximum entropy discrimination futile done fewer william ockham possible combine strongly complementary properties discriminative estimation generative modeling 

support vector machines performance gains provide combined elegantly flexible bayesian statistics graphical models 
chapter introduces novel technique called discrimination med provides general formalism methods 
duality maximum entropy theory maximum likelihood known literature :10.1.1.14.5452
connection generative estimation classical maximum entropy exists 
med brings novel discriminative aspect theory forms bridge contemporary discriminative methods 
med involves twist usual maximum entropy paradigm considers distributions model parameters distributions data 
possible approaches combining generative discriminative schools thought exist med formalism distinct advantages :10.1.1.44.7709
instance med naturally spans ends discriminative generative spectrum subsumes support vector machines extends driving principles large majority generative models populate machine learning community 
chapter organized follows 
motivating discriminative maximum entropy framework point view regularization theory 
powerful convexity duality geometric properties elaborated 
explicate solve classification problems context maximum entropy formalism 
support vector machine derived special case 
subsequently extend framework discrimination generative models prove exponential family generative distributions immediately estimable med framework 
generalization guarantees 
med extensions transductive inference feature selection elaborated chapter chapter 
med framework applicable wide range risk ockham truly intended say shall quote motivate sparsity arises constraint discriminative learner maximum entropy discrimination formalism 
chapter 
maximum entropy discrimination bayesian models latent models considered 
mixtures exponential family deserve special attention development context med deferred chapter 
regularization theory support vector machines developing maximum entropy framework regularization theory support vector machine perspective derivation described 
simplicity address binary classification chapter defer extensions chapter 
regularization theory field right formalisms approach possible developments 
contact point machine learning reader regularization theory :10.1.1.48.9258
parametric family decision boundaries theta shall call discriminant functions 
discriminant function specific parameter theta takes input produces scalar output 
sign sigma scalar value indicate class input assigned 
example simple type decision boundary linear classifier 
parameters classifier theta bg concatenation linear parameter vector scalar bias generate linear classifier theta estimate optimal theta set training examples fx xt corresponding binary sigma labels fy find parameter setting theta minimize form classification error 
best possible theta classifier predict labels input examples theta form measure classification error loss functions data point depend parameter theta classification margin 
margin defined theta large positive label agrees scalar valued prediction theta negative disagree 
shall assume loss function 
non increasing convex function margin 
larger margin results smaller loss 
introduce regularization penalty theta models favors certain parameters prior 
optimal parameter setting theta computed minimizing empirical loss regularization penalty min theta theta theta straightforward solution theta achieved recasting constrained optimization min theta fl tg theta fl subject theta gamma fl noted regularization theory limited margin concepts 
general penalty function stabilizer terms may depend regularization criteria wide area possible norms semi norms 
interpretation regularization theory approach solving inverse problems 
spans applications spline fitting pattern recognition employs sophisticated mathematical constructs reproducing kernel hilbert spaces 
chapter 
maximum entropy discrimination introduced margin quantities fl slack variables optimization represent minimum margin theta satisfy 
minimization parameters theta margins fl linear support vector machine seen particular example formulation 
discriminant function linear hyper plane equation 
furthermore regularization penalty theta norm parameters encourage large margin solutions 
slack variables provide svm straightforward way handle non separable classification problems 
primal svm optimization problem min fl tg fl subject gamma fl point focus optimization theta ignore optimization slack variables fl effect restriction resulting classifier support vector machine require linearly separable data 
practice assume slack variables held constant set manually 
unity fl 
restrictive assumption simplify derivations result loss generality 
restriction subsequently permitting consider non separable cases 
solvability point crucial investigate conditions constrained minimization problem equation solvable 
instance cast convex program theta computed uniquely 
convex program typically involves minimization convex cost function convex hull constraints 
mild assumptions solution unique variety strategies converge axis parallel optimization linear quadratic convex programming 
various constrained optimizations scenarios 
depicts convex cost function convex hull constraints arising conjunction multiple linear constraints 
leads valid convex program 
valid convex program non convex constraints non convex cost function convex cost functions convex constraints 
chapter 
maximum entropy discrimination situation promising 
nonlinear constraints combined searchable space forms non convex hull 
prevents guaranteed convergence yields non convex program 
similarly convex program 
culprit non convex cost function theta convex 
solution equation require penalty function theta convex conjunction classification constraints form convex hull 
intersection linear constraints mild conditions form convex hull 
addition evident intersection multiple nonlinear constraints form convex hull 
clear classification constraints regularization framework need linear consistently space linear 
support vector machines kernels inspecting support vector machine immediately see penalty function theta convex linear hyper plane discriminant give rise linear constraints convex hull 
known svm solvable convex program simply quadratic program sequential minimal optimization 
theta nonlinear 
example may wish deal decision boundaries arise generative models 
computed log likelihood ratio generative models xj xj gamma class 
parameter space includes concatenation positive generative model negative scalar bias theta gamma bg 
gives rise nonlinear discriminant functions theta log xj xj gamma unfortunately nonlinear decision boundaries generate search space theta longer convex hull compromising uniqueness solvability problem 
cases nonlinear decision boundaries nonlinear svms handled called kernel trick 
decision boundary nonlinear consider mapping data phi higher dimensional feature space 
theta parameter vector parameterizes higher dimensional hyper plane effectively mimicking nonlinearity original low dimensional space 
furthermore constraints linear search space forms convex hull 
subtlety regularization penalty different feature space original space 
quadratic theta penalty function original space obtain possibly complicated expression feature space 
reasonable case svms vc dimension generalization guarantees hold level feature space 
permits artificially preserve quadratic penalty function feature space map quite complicated original space 
term kernel simply arises optimizing quadratic penalty function feature space requires inner products high dimensional vectors phi implicitly computable kernels explicit mapping phi 
generally may specific regularization penalty mind level original space nonlinearities classifier prevent considering highdimensional mapping trick 
problematic situation case generative models motivates important extension med regularization theory far discussed 
chapter 
maximum entropy discrimination med distribution solutions generalize regularization formulation presenting maximum entropy discrimination framework 
noting necessarily ideal solve single optimal setting parameter theta consider solving full distribution multiple theta values give distribution solutions 
intuition different settings theta generate relatively similar classification performance better estimate distribution theta preserves flexibility single optimal theta 
clearly full distribution theta subsume original formulation choose theta ffi theta theta delta function seen point wise probability mass concentrated theta theta 
type probabilistic solution superset direct optimization theta large theta values yield classifiers close zero theta values yield poor classifiers 
probabilistic generalization facilitate number extensions basic regularization svm approach 
modify regularization approach follows 
distribution theta easily modify regularization approach predicting new label new input sample shown equation 
merely discriminant function optimal parameter setting theta integrate discriminant functions weighted theta sign theta theta theta theta estimate theta 
consider expectation form previous approach cast equation integration 
classification constraints applied expected sense 
inappropriate directly apply theta arbitrary penalty function infinite dimensional probability density functions theta 
considering expectation penalty functions apply canonical penalty function distributions negative entropy 
minimizing negative entropy equivalent maximizing entropy 
maximum entropy theory pioneered jaynes compute distributions moment constraints 
absence information jaynes argues satisfy constraints way committal 
gives rise need maximum entropy distribution close uniform possible 
assume shannon entropy defined theta gamma theta log theta theta 
traditionally maximum entropy community distributions computed subject moment constraints discrimination classification constraints 
discrimination term added specify framework borrows concepts regularization svm theory satisfying discriminative classification constraints margin 
gives novel med formulation finding distribution theta parameters theta min theta gammah theta subject theta theta gamma fl theta negative entropy flexible surrogate penalty function 
generalize cast negative entropy kullback leibler divergence theta target uniform distribution practice possibly parametric restrictions may arise theta prevent generating arbitrary delta functions manner 
point assumed margins fl loss functions held fixed typically set fl 
assumption relaxed subsequently 
chapter 
maximum entropy discrimination gammah theta kl theta kp uniform theta 
kullback leibler divergence defined follows kl theta kq theta theta log theta theta theta prior knowledge desired shape theta may necessarily want favor high entropy uniform solutions 
customize target distribution nonuniform 
replace penalty function kullback leibler divergence prior kl theta kp theta 
gives general minimum relative entropy discrimination mre mred formulation definition find distribution theta parameters theta minimizes kl theta kp theta subject theta fl theta gamma fl theta 
theta prior distribution parameters 
resulting decision rule sign theta theta theta 
traditional continue refer minimum relative entropy approaches maximum entropy 
risk confusion shall adopt convention nomenclature refer definition maximum entropy discrimination 
point evaluate solvability formulation 
depicts problem formulation 
note dealing possibly infinite dimensional space solving parameter vector theta solving theta probability distribution 
axes represent variation coordinates possibly continuous distribution theta theta 
theta penalty function kl divergence convex function theta 
furthermore constraints expectations respect theta means linear theta 
linear constraints guaranteed combine convex hull search space theta regardless nonlinearities discriminant function 
kl med convex program problem formulation 
solution definition valid convex program 
fact solution med classification problem definition directly solvable classical result maximum entropy kl divergence kl pkq written pkq 
chapter 
maximum entropy discrimination theorem solution med problem estimating distribution parameters form cf 
cover thomas theta theta theta gammafl normalization constant partition function defines set non negative lagrange multipliers classification constraint 
set finding unique maximum jointly concave objective function gamma log solution arises dual problem constrained optimization definition primal problem legendre transform 
mild conditions solution exists unique 
occasionally objective function may grow bound prevent existence unique solution situation rare practice 
furthermore typically far easier solve dual problem complexity constraints alleviated 
obvious constraints lagrange multipliers non negativity straightforward realize constraints possibly infinite dimensional distribution theta primal problem 
non negativity lagrange multipliers arises maximum entropy problems inequality constraints primal problem representing classification constraints definition point shall loosen constraint margins fixed allow 
classification scenarios non separable 
med augmented distributions med formulation far assumption margin values fl pre specified held fixed 
discriminant function able perfectly separate training examples pre specified margin value 
may possible practice non separable data sets generate empty convex hull solution space 
need revisit setting margin values loss function 
recall far ignored loss function regularization framework derived med technique held margins fixed 
choice loss function penalties violating margin constraints admits principled solution med framework 
shown earlier case parameters consider distribution margins 
fl med framework 
typically classification vc dimension generalization guarantees encourage large margin solutions performance choose margin distributions favor larger margins 
furthermore varying choice distribution effectively mimic consider various loss functions associated fl 
choosing priors allow non zero probability mass negative margins permit non separable classification ad hoc slack variables svms 
ensure classification constraints give rise empty admissible set 
med formulation give solution joint distribution theta fl 
gives weighted continuum solutions specifying single optimal value regularization approach 
caveat med constraints apply expectations margin values 
satisfying looser problem margin values set transition margin values margin distributions natural previous transition parameter equality constraints primal problem generate lagrange multipliers arbitrary scalars gamma chapter 
maximum entropy discrimination extrema parameter distributions 
multiple margin values training data instance fl aggregate distribution margins typically factorized theta fl theta pi fl 
leads general med formulation definition med solution theta fl parameters theta margin variables fl fl fl minimizes kl theta kp theta kl fl kp fl subject theta fl theta gamma fl 
theta prior distribution parameters fl prior margin variables 
resulting decision rule sign theta theta theta 
solution exists mild assumptions unique 
constraints just expectation constraints parameter distribution expectation margin distribution 
relaxes convex hull constraints need hold specific margin 
constraints need hold distribution margins include negative margins permitting consider non separable classification problems 
furthermore applying med problem longer specify ad hoc regularization penalty theta margin penalty functions fl loss functions specify probability distributions 
distributions convenient specify automatically give rise penalty functions model margins kl divergences 
specifically model distribution give rise divergence term kl theta theta margin distribution give rise divergence term kl fl kp fl correspond regularization penalty loss functions respectively 
terms probability distributions kl divergence trade classification loss regularization common probabilistic scale 
solution non separable med classification problem definition solved follows theorem solution med problem estimating distribution parameters margins augmentations general form cf 
cover thomas theta fl theta fl theta gammafl normalization constant partition function defines set non negative lagrange multipliers classification constraint 
set finding unique maximum jointly concave objective function gamma log details choices priors parameters margins distributions elaborated sections 
possible recast optimization problem maximum entropy formulation generated back regularization form terms loss functions regularization penalties 
med probabilistic formulation intuitive provides flexibility 
instance continue augment solution space distributions entities maintain convex cost function convex constraints 
example include distribution unobserved labels unobserved inputs training set 
introduce continuous discrete variables discriminant function unknown integrate 
distribution theta effectively theta fl principle maintain similar convex program structure dual solution posed portrayed theorem 
types extensions elaborated chapter 
important caveat remains augment chapter 
maximum entropy discrimination distributions maintain balance various priors trying minimize kl divergence 
prior models theta strict may overwhelm prior quantities margins fl vice versa 
minimization kl divergence skewed prior 
information theoretic geometric interpretation interesting geometric interpretation med solution described type information projection 
projection depicted referred relative entropy projection projection :10.1.1.37.8662
multiple linear constraints form convex hull generates admissible set called convex hull referred flat constraint set 
med solution point admissible set closest terms divergence prior distribution theta 
analogy extends cases distributions margins unlabeled exemplars missing values structures probabilistic entities introduced designing discriminant function 
kl med information projection operation 
med probabilistic formalism interesting conceptual connections information theoretic boosting works 
point contact entropy projection boosting adaboost framework developed 
boosting uses distribution weights data point training set forms weak learner 
process iterated updating distribution data weak learner iterations 
hypothesis combined weighted mixture weak learners called final master algorithm 
effectively boosting step estimates new distribution training data minimizes relative entropy prior distribution orthogonal performance vector denoted performance vector cardinality values ranging gamma 
previous weak learner prior probability distribution correctly classifies data point vector training datum index value close 
datum poorly classified gamma corresponding index 
update distribution exponential update rule follows directly classical maximum entropy results exp chapter 
maximum entropy discrimination considering iterative approach individual corrective updates may enforce orthogonality constraints generate full convex hull constrain entropy projection exp gamma ff warmuth argue new distribution provide information current weak hypothesis individual orthogonality constraint 
simultaneously consider orthogonality constraints time new hypothesis provide new information uncorrelated previous hypotheses 
convex hull constraints results exponentiated ff terms equation strongly reminiscent med formulation exponentiated classification constraints lagrange multipliers 
interpret med formulation minimizing divergence prior extracting information possible training data 
information theoretic point contact tishby :10.1.1.122.8863
authors propose minimizing lossy coding input data compact representation maintaining constraint mutual information coding desired output variable 
information theoretic setting gives rise lagrangian optimization gamma fii 
result efficient representation input data extracts information possible terms bits encode relevance output variable 
loose analogy med framework solves solution distribution theta minimally encodes prior distribution theta analogous input vectors respectively classification constraints due training data analogous relevance variables satisfied provide information possible 
important connection lies med kullback early called minimum discrimination information method :10.1.1.14.5452
definition kullback adopts discrimination slightly different discussing 
mainly involves discrimination competing hypotheses information metric hypothesis satisfy additional constraints close prior hypothesis possible 
mechanism proposed kullback similar maximum entropy formalism jaynes proposes describes connections shannon theory communication 
kullback points various important elaborations ultimately minimum discrimination information method finds distribution close possible prior terms kl divergence subject various moment constraints 
information hypothesis involves distributions measurable space opposed distributions parameters med 
furthermore constraints margin classification med give rise discriminative classifier regressor 
med natural continuation kullback approach seen contemporary effort combine current impetus discriminative estimation svm literature corresponding generalization arguments 
computing partition function ultimately implementing med solution theorem hinges ability perform required calculations 
instance need maximize concave objective function obtain chapter 
maximum entropy discrimination optimal setting lagrange multipliers gamma log ideally able evaluate partition function analytically efficiently 
precisely partition function theta fl theta gammafl closed form partition function permits convenient concave objective function optimized standard techniques 
possible choices include convex programming second order methods axis parallel methods 
implementation details novel speed improvements learning lagrange multipliers critical maximization optimizing provided appendix section 
additional partition function comes powerful aspect maximum entropy exponential family distributions inherited med 
property states gradients arbitrary order log partition log respect variable equal expectations arbitrary order corresponding moment constraints respect maximum entropy distribution 
permits easily compute expectations variances theta fl med constraints second derivatives log 
closed form med partition function conveniently obtain expectations follows log theta fl fy theta gamma fl log ar theta fl fy theta gamma fl unfortunately integrals required compute critical log partition function may analytically solvable :10.1.1.14.5452:10.1.1.14.5452
solvable various strategies optimize 
instance axis parallel techniques iteratively converge global maximum 
certain situations may maximized 
quadratic programming 
furthermore online evaluation decision rule training data requires integral followed sign operation may feasible arbitrary choices priors discriminant functions 
usually cumbersome computing partition function obtain optimal lagrange multipliers 
sections shall specify conditions computations remain tractable 
depend specific configuration discriminant function theta choice prior theta fl 
section discuss various choices margin priors bias priors model priors discriminant functions 
margin priors mathematically expand partition function equation noting distribution factorizes follows theta fl theta gammafl chapter 
maximum entropy discrimination theta pi fl theta gammafl theta theta theta theta pi fl gamma fl dfl theta theta pi fl recall optimization function expressed negated logarithm partition function gamma log gamma log theta gamma log fl theta fl fl behave similarly loss functions fl original regularization theory approach negated versions loss functions 
direct way finding penalty terms gammaj fl margin priors fl vice versa 
dual relationship defining objective function penalty terms defining prior distribution parameters prior distribution margins 
instance consider margin prior distribution fl ce gammac gammafl fl integrating get penalty function log fl log fl gamma ce gammac gammafl gamma fl dfl gamma gamma log gamma case penalty incurred margins smaller prior mean fl gamma margins larger quantity penalized associated classification constraint irrelevant corresponding lagrange multiplier possibly vanish 
increasing parameter encourage separable solutions margin distribution peaked setting fl equivalent having fixed margins initial med definition 
choice margin distribution correspond closely slack variables svm formulation choice different loss functions regularization theory approach 
fact parameter play identical role regularization parameter upper bounds lagrange multipliers slack variable svm solution 
shows prior associated potential term negated penalty term 
various classification margin priors penalty terms analytically computable table 
furthermore dotted green line indicates potential function arises margins fixed unity assumes separability 
plots value 
note priors form concave potential functions convex penalty functions desired unique optimum lagrange multiplier space 
noted potential functions force upper bound barrier function allow vary freely long non negative 
may may set upper bound chapter 
maximum entropy discrimination margin prior distributions top associated potential functions bottom 
margin prior fl dual potential term fl fl gammac gammafl fl log gamma fl gammafl log gamma fl gammac gammafl gamma table margin prior distributions associated potential functions 
value priors penalty functions possible particular regression case discussed require quite different margin configurations 
move priors model particular priors bias 
bias priors bias merely subcomponent model due particular interaction discriminant function treated separately 
specifically bias appears additive scalar discriminant 
recall theta seen concatenation parameters consider breakdown theta theta bg 
recall form discriminant functions equation equation theta bias term arises linear models classification models including generative classification multi class classification regression models 
evidently set zero remove effect simply set fixed constant med approach easily permits consider distribution tailor solution specifying prior 
consider possible choices prior possible gaussian prior non informative prior 
chapter 
maximum entropy discrimination gaussian bias priors consider zero mean gaussian prior oe gamma oe prior favors bias values close zero priori assumes balance binary classes decision problem 
prior belief class frequencies slightly skewed may introduce mean prior favors class 
resulting potential term gamma log gamma log gamma oe gamma oe db gamma oe variance standard deviation oe specifies certain classes evenly balanced 
terms potential function constrains quadratic penalty balance lagrange multipliers negative class positive class 
non informative bias priors evidently gaussian prior favor values close zero 
absence knowledge bias reasonable permit scalar value equal preference 
give rise non informative prior 
form prior parameterized gaussian equation variance approaching infinity oe 
stretches gaussian starts behave uniform distribution axis gamma 
resulting potential term naturally lim oe gamma oe maximize potential terms oe grows infinity objective function go negative infinity term parentheses exactly zero 
non informative prior generates extra constraint addition non negativity lagrange multipliers requiring lemma bias prior set non informative infinite covariance gaussian non negative lagrange multipliers med solution satisfy equality constraint point defined priors computational machinery necessary med formulation give rise support vector machines 
chapter 
maximum entropy discrimination support vector machines previously discussed support vector machine cast regularization theory framework solvable convex program due fundamentally linear discriminant function employs theta interpret linear decision boundary considering example log likelihood ratio gaussian distributions class equal covariance matrices 
theta log xj xj gamma shall adopt linear discriminant boundary efficient parameterization choice simple prior exactly synthesize support vector machine 
particular choose gaussian prior weights linear discriminant function med formulation produce support vector machines theorem assuming theta theta fl fl approaches non informative prior fl fl equation lagrange multipliers obtained maximizing subject log gamma gamma objective function strikingly similar svm dual optimization problem 
difference formulation extra potential term log gamma acts barrier function preventing values growing svm lagrange multiplier values clamped greater explicitly extra constraint convex program 
formalisms plays identical role varying degree regularization upper bounding lagrange multipliers 
typically low values increase regularization vary sensitivity solution classification errors robustness outliers permit non separable classification problems 
svm regularization parameter arises ad hoc slack variables permit svm handle non separable data 
potential term log gamma vanishes med gives rise exactly svm separable data 
practice finite med svm solutions identical 
single axis svm optimization greatly simplify support vector machine avoiding non informative prior bias 
assume gaussian prior finite covariance equality constraint omitted 
resulting convex program requires non negativity lagrange multipliers updated objective function log gamma gamma oe gamma chapter 
maximum entropy discrimination possible update single lagrange multiplier time axis parallel manner 
fact update axis analytic med logarithmic barrier function non separable case 
minimal working set case svm updates increase objective done simultaneously lagrange multipliers sequential minimal optimization smo technique proposed platt 
gives med implementation simpler optimization problem lead gains computational efficiency significant change solution produced non informative priors 
kernels med formulation svms readily extends kernel case nonlinearities kernel type immediately folded implicit mapping higher dimensional space 
updated med objective function merely log gamma gamma oe gamma standard inner products input vectors replaced kernel function vectors done svm literature 
med computations remain relatively unchanged linear discriminant case calculations involve inner products input vectors 
generative models point consider generative models med framework 
fundamentally extends regularization svm discriminative frameworks powerful modeling bayesian generative models 
lies strength med technique bridge communities mutually beneficial tools 
consider class problem generative model class xj xj gamma 
generative models directly combined form classifier considering log likelihood ratios follows theta log xj xj gamma aggregate parameter set theta gamma bg includes generative models scalar bias 
merely changing discriminant function med framework estimate generative models guarantee decision boundary give rise optimal classification setting 
naturally discriminant function generally nonlinear give rise non convex hull constraints standard regularization setting 
med framework due probabilistic solution theta discriminant functions behave convex program 
estimating theta med ultimately yield gamma specify generative models data xj xj gamma 
full generative models sampled integrated conditioned direct bayesian framework generative models combine form high performance discriminative classifier plugged theta discriminant function 
depicts estimation maximum chapter 
maximum entropy discrimination discriminative generative models 
show standard maximum likelihood estimation generative models data resulting poor classifier decision boundary generate 
med moves generators slightly combine form accurate classification boundary 
likelihood generative model med moves generators class ellipses decision boundary creates classification separation 
med estimation feasible hinges ability compute function 
show possible obtain partition function analytically generative models xj xj gamma exponential family 
exponential family models argued functions efficiently solved med approach include loglikelihood ratios exponential family distributions 
compute partition function efficiently implement estimation 
give details exponential family form 
known distributions important properties maximum likelihood estimation 
family subsumes wide set distributions members characterized general structure commonly referred natural parameterization xj exp gamma convex distribution normalized space details exponential family interesting properties chapter 
addition exponential family member conjugate prior distribution exp gamma conjugate distribution exponential family convex 
specific combination discriminant function associated prior estimable med framework depends computability partition function objective function optimizing lagrange multipliers associated constraints 
general operations require integrals associated parameter distributions 
particular recall partition function corresponding binary classification case 
consider integral theta theta theta theta theta chapter 
maximum entropy discrimination separate parameters associated class conditional densities bias term gamma expand discriminant function log likelihood ratio obtain theta gamma log xj xj gamma theta factorizes theta gamma substitute exponential family forms class conditional distributions associated conjugate distributions priors 
assume prior defined specifying value suffices show obtain closed form 
derivation gamma identical 
drop symbol clarity 
problem reduced evaluating gamma gammak shown earlier see lemma non informative prior bias term leads constraint 
making assumption get gamma theta gamma theta evaluation results natural property exponential family 
expressions known specific distributions exponential family easily complete evaluation realize objective function holds exponential family distribution log gamma clear compute objective function discriminant function arising exponential family generative models 
fact integration doesn need performed analytic expression objective function terms natural parameterization exponential family distributions 
noted objective function bears strong resemblance evidence term bayesian inference bayesian integration lagrange multipliers act weights bayesian inference 
straightforward point perform required optimization find optimal setting lagrange multipliers maximize concave 
empirical bayes priors point proven feasible estimate generative models exponential family form med assume priors conjugate distribution 
parameters conjugate priors specified quite flexibility designing prior knowledge med formulation 
absence prior knowledge possible recommend default prior conjugate non informative prior empirical bayes prior 
loosely put prior theta specifically gamma posterior distribution parameters data bayesian inference generates 
consider chapter 
maximum entropy discrimination data set fx binary sigma labels fy inputs split positive inputs fx negative inputs fx gamma gamma explicate bayesian inference procedure 
distinguish resulting densities med formulation put symbol bayesian distributions 
bayesian inference class posterior distribution estimated positive input exemplars fx follows jfx fx gj pi similarly negative class generative bayesian posterior model estimated negative input exemplars fx gamma gamma gamma pi gamma gamma gamma gamma bayesian estimate generative model data minimally informative prior sigma 
result distribution generator possible data set 
don want just generator data want discriminator 
med satisfy large margin classification constraints 
simultaneously solution close possible generative model terms kl divergence 
shall bayesian posteriors med priors 
gamma gamma depicts information projection solution med generate bayesian estimate 
effectively try solving distribution parameters close possible bayesian estimate quite similar maximum likelihood estimate case exponential family satisfies classification constraints 
med bayes information projection operation bayesian generative estimate 
motivation absence discriminative information generator possible 
note number advantages type empirical bayes chapter 
maximum entropy discrimination prior include theoretical conceptual practical arguments 
empirical bayes prior precautionary measure take allows flexible med discriminative model generator necessary 
may case discriminator cope missing data noise 
prediction setting input variables missing reconstruct integrate simply med discriminative model surrogate generator distribution 
sparse data situations model may easily satisfy discrimination constraints aspects model remain ambiguous 
cases empirical bayesian prior provides backup generative criterion constrains problem albeit ways helpful task help consistent estimation 
obtain invariance empirical bayes prior get assume fixed prior 
example fixed zero mean gaussian prior produce different med solutions translate training data empirical bayes prior follow translation data bayesian generative model consistently setup relative decision boundary 
furthermore consistency important arguably optimistic situation generative model exactly correct perfectly matches training data 
case bayesian solution optimal med may stray empirical bayes prior obtain infinite training data 
interesting side note standard margin prior distribution equation obtain upper bound lagrange multipliers med solution uses bayesian posterior increases reduce regularization outlier rejection favor perfect classification 
purely practical note empirical bayes prior may provide better numerical stability example 
discriminative med model put little probability mass training data return poor generative configuration perfectly separating data 
undesirable numerically get small values 
xj xj gamma 
prediction new test point may cause numerical accuracy problems far probability mass med discriminative solution 
result loss discriminative power maintain generative aspects model 
full covariance gaussians consider case discriminant function theta corresponds log likelihood ratio gaussians different adjustable covariance matrices 
parameters theta case means covariances 
generative models exponential family previous results hold 
prior choice theta conjugate full covariance gaussian normal wishart 
shall shorthand normal distribution iw shorthand inverse wishart 
choice distributions permits obtain closed form integrals partition function 
shall breakdown parameters generative models bias 
theta gamma 
specifically sigma broken mean covariance components gaussian 
theta sigma gamma sigma gamma gives density means covariances notation closely follows 
prior distribution form jm sigma iw sigma parameters specify prior scalar vector matrix imputed manually 
may get non informative prior 
chapter 
maximum entropy discrimination map values class specific data corresponds posterior distribution parameters data bayesian inference procedure empirical bayes procedure described previous section 
integrating parameters get partition function factorizes fl gamma 
obtain gammad js gamman pi gamma gamma defined intermediate variables scalar vector matrix brevity delta delta gamma scalar weight 
solve gamma proceed exactly manner weights set gammay gamma delta merely step function 
updating done maximizing corresponding negative entropy subject ff log gamma gamma log gamma log gamma potential term corresponds integrating margin margin prior fl gammac ff gammafl fl pick ff ff percentile margins obtained standard map solution 
optimal lagrange multiplier values simple constrained gradient descent procedure 
resulting mre normalized partition function normal wishart distribution generative model final values set maximization sigma iw sigma predicting labels data point final theta involves expectations discriminant function normal wishart 
positive generative class expectation log xj constant gamma gamma gamma gamma expectation negative class similar 
gives predicted label quite simply sign theta theta theta sign theta log xj xj gamma sign gamma log xj gamma gamma log xj gamma delta chapter 
maximum entropy discrimination ml med initialization med intermediate med intermediate med converged classification visualization gaussian discrimination 
computing expectation bias avoided non informative case additive effect merely estimated svm karush kuhn tucker conditions 
obtain discriminative quadratic decision boundaries 
extend linear boundaries explicitly resorting kernels 
course kernels may formalism effectively mapping feature space higher dimensional representation 
linear discrimination covariance estimation framework allows model adaptively modify kernel 
visualization technique set training data 
maximum likelihood technique estimate gaussian discrimination boundary bias estimated separately flexibility achieve perfect classification produces classifier performance equal random guessing 
maximum entropy discrimination technique places gaussians discriminative configuration shown requiring kernels feature space manipulations 
experiments show results minimum relative entropy approach discriminant function theta log ratio gaussians variable covariance matrices standard class classification problems crabs breast cancer wisconsin 
performance compared regular support vector machines maximum likelihood estimation methods 
crabs data set originally provided ripley tested barber williams 
objective classify sex crabs scalar anatomical chapter 
maximum entropy discrimination method training testing errors errors neural network neural network linear discriminant logistic regression mars degree pp ridge functions gaussian process hmc gaussian process map svm linear svm rbf oe svm rd order polynomial maximum likelihood gaussians maxent discrimination gaussians table crabs observations 
training set contains examples sex test set includes examples 
gaussian decision boundaries compared table models 
table shows maximum entropy minimum relative entropy criterion improves gaussian discrimination performance levels similar best alternative models 
bias estimated separately training data maximum likelihood gaussian models maximum entropy discrimination case 
addition show performance support vector machine svm linear radial basis polynomial decision boundaries matlab svm toolbox provided steve gunn 
case linear svm limited flexibility kernels exhibit fitting 
data set tested breast cancer wisconsin data classes malignant benign computed numerical attributes patients tumors training cases test cases 
data wolberg 
compare results produced zhang nearest neighbor algorithm achieve accuracy 
seen table fitting prevents performance kernel svms top performer maximum entropy discriminator accuracy 
method training testing errors errors nearest neighbor svm linear svm rbf oe svm rd order polynomial maximum likelihood gaussians maxent discrimination gaussians table breast cancer classification chapter 
maximum entropy discrimination multinomials popular exponential family model multinomial distribution 
consider case discriminant function theta corresponds log likelihood ratio multinomials theta log xj xj gamma gamma generative models vector consider set counts xj pi ae superscript index dimensionality vector subscript index training set 
scalar term large parentheses multinomial coefficient natural extension binomial coefficient coin tossing die tossing 
scalar term unity vector zero unit entry 
simply scales probability distribution constant factor rewritten follows clarity gamma functions permits consider continuous vectors pi gamma pi gamma generative distribution equation parameterizes multinomial ae vector non negative scalars sum unity ae 
parameters theta case ae positive class negative class bias scalar generative models exponential family previous results hold 
prior choice theta conjugate multinomial dirichlet distribution 
choice distributions permits obtain closed form integrals partition function 
shall breakdown parameters generative models bias 
theta gamma distinguish ae positive class denote parameters negative class ae 
prior dirichlet distribution form gamma ff pi gamma ff pi ae ff gamma typically assume ff pre specified manually empirical bayes procedure satisfy ff 
core computation involves computing component log partition function corresponds model computation bias margins remain previous cases 
need gamma gamma log gamma gamma suffices show compute log chapter 
maximum entropy discrimination gamma ff pi gamma ff pi ae ff gamma log log pi ae dae gamma ff pi gamma ff pi ae ff gamma log pi ae dae theta log gamma ff pi gamma ff pi ae ff gamma pi ae dae theta log gamma ff pi gamma ff pi ae ff gamma dae theta log gamma ff gamma ff tk pi gamma ff gamma ff theta log form objective function maximize obtain setting lagrange multipliers subject constraint gamma log gamma log gamma gamma log fl setting lagrange multipliers permits exactly specify final med solution distribution theta compute predictions classification sign theta theta theta generalization guarantees arguments med terms generalization guarantees 
generative frameworks guarantees asymptotic goodness fit distribution bayesian evidence score bayesian information criterion akaike information criterion seldom guarantees generalization performance models classification regression setting 
furthermore guarantees may distribution dependent inappropriate true generative distribution data source perfectly known 
conversely discriminative approaches specifically target classification regression performance strong generalization arguments move training data testing data 
may distribution independent 
med framework discriminative estimation approach brings classification performance guarantees generative models 
number arguments including sparsity generalization vc dimension generalization pac bayesian generalization 
generalization bounds quite loose small amounts training data better guarantees whatsoever 
furthermore shape generalization bounds demonstrated empirically useful discriminative setting 
large amounts data bounds reasonably tight 
vc dimension due ability med framework subsume svms exactly generating equations separable case benefits generalization guarantees accompany 
chapter 
maximum entropy discrimination course vc dimension vapnik chervonenkis bounds expected risk theta classifier 
assuming loss function theta training exemplars empirical risk readily computed remp theta theta true risk samples outside training set bounded empirical plus term depends size training set vc dimension classifier non negative integer quantity measures capacity classifier independent distribution data :10.1.1.37.8662
bound holds probability gamma ffi theta remp theta log gamma log ffi vc dimension set hyper planes equal 
directly motivate large margin decision boundaries svm 
svm interpreted gap tolerant classifier pure hyper plane 
gap tolerant classifier set parallel hyper planes sphere 
points outside sphere points planes contribute risk points sphere side planes assigned class sigma 
points sphere outside hyper planes vc dimension upper bounded radius resulting sphere margin planes gives upper bound vc dimension feature space max ae oe plausible argument maximizing margin linear classifier 
translate immediately nonlinear classifiers direct kernel mapping back linear hyper planes motivation large margin svms justify large margins med formulation priors put large probability mass larger margins values 
move formal arguments med generalization 
sparsity med solution involves constraint optimization classification constraint training data point classified 
constraint represented lagrange multiplier associated data point 
cases constraints redundant 
apparent classifying data point correctly automatically result correct classification 
constraints involving data points corresponding lagrange multipliers go zero 
svm points close margin small margin values critical role shaping decision boundary generate non zero lagrange multipliers 
support vectors standard svm terminology 
points easily correctly classified large margin zero lagrange multipliers 
med solution depends small subset training data change data points deleted 
gives rise notion sparsity generalization arguments 
argument chapter 
maximum entropy discrimination generalization error denoted ffl expected percentage ratio non zero lagrange multipliers lagrange multipliers 
ffl ffi data points simply count number non zero lagrange multipliers ffi function zero lagrange multipliers value zero unity non vanishing values 
expectation taken arbitrary choices training set means upper bound generalization error approximated cross validation techniques :10.1.1.44.7709
alternatively coarse approximation expectation done simply counting number remaining non zero lagrange multipliers maximizing training set med solution 
pac bayes bounds alternative vc dimension arguments generalization includes pac bounds probably approximately correct valiant 
contributions terms pac bayesian model selection criteria mcallester langford theoretical generalization arguments directly motivate med approach med developed prior generalization results essentially pac bayesian approaches allow combination bayesian integration prior domain knowledge pac generalization guarantees forcing pac framework assume truthfulness prior 
loosely adapt state main results details available original 
effectively generalization guarantees model averaging stochastic model selection criterion favor deterministic 
med model averaging framework distribution models computed 
svm 
new generalization results apply immediately 
med assume prior probability distribution theta possibly uncountable continuous model class 
assume discriminant functions theta bounded real valued hypotheses set training exemplars form sampled distribution compute expected loss fraction misclassifications true distribution recall med correct classification data theta theta theta incorrect classifications form theta theta theta conservative empirical misclassification rate counts number errors generalization guarantees originally averaging binary discriminant functions real ones extended real ones straightforward manner 
may construct med classifier discriminant function 
sigmoidal binary satisfy requirements bounds hold 
alternatively trivial extension find bound considering maximal sphere data implicitly provides limits range discriminant function 
permits scaled version generalization bound 
chapter 
maximum entropy discrimination counting errors positive margin threshold fl theta theta theta fl compute empirical number misclassifications conservative technique margin threshold fl upper bound expected standard misclassification rate 
expected misclassification rate upper bound bound holds probability gamma ffi ed theta theta theta theta theta theta fl fl gamma theta kp theta ln ln ln ffi gamma ideally minimize expected risk classifier data left hand side 
clearly bound motivates forming classifier satisfies empirical classification constraints encapsulated term right hand side minimizing divergence prior distribution second term right hand side 
note increasing margin threshold useful minimizing expected risk 
criteria directly addressed med framework strongly agrees theoretical motivation 
furthermore increasing cardinality training data set bound tighter independently distribution data 
point contact argues optimal posterior types bounds maximum entropy discrimination solution gibbs distribution 
summary extensions maximum entropy discrimination framework regularization theory perspective shown subsumes support vector machines 
solvability med solution demonstrated 
shown med readily generative framework decision boundaries arise exponential family distributions input space 
generalization guarantees provide framework theoretical grounding reinforces flexibility generality 
discuss extensions cascaded med formalism motivate usefulness approach 
chapter extensions maximum entropy discrimination point med formulation bridged generative modeling discriminative performance svms example 
med method elaborated spans wide range machine learning scenarios 
chapter discuss various extensions framework demonstrate flexibility intuitive nature 
exploring extensions introduce possibly intermediate variables discriminant function theta solve augmented distribution theta includes 
resulting partition function typically involves integration analytic number lagrange multipliers optimization complexity remain basically unchanged 
depicts common metaphor augmenting probability variables utilized 
follows principle augment distribution soft margin constraints section 
note caveat add distributions prior careful balance competing goals variances evenly derive meaningful information component aggregate prior model prior margin prior priors introduce shortly 


formulating extensions med 
depicts different scenarios med handle 
extensions multi class classification treated binary classification constraints error correcting codes 
chapter explicate case labels longer chapter 
extensions maximum entropy discrimination discrete continuous regression 
case regression just binary classification svm regression subsumed 
subsequently discuss structure learning opposed parameter estimation particular feature selection applications 
furthermore discuss partially labeled examples transduction classification regression 
lead important generalization requires special treatment extension mixture models mixtures exponential family latent modeling discussed chapter 
oo binary classification multiclass classification regression oo feature selection transduction anomaly detection various extensions binary classification 
med regression med formalism restricted classification 
extension regression function approximation case approach nomenclature :10.1.1.127.1519:10.1.1.127.1519:10.1.1.127.1519
dual sided constraints imposed output interval called ffl tube function described suppose training input examples fx xt corresponding output values continuous scalars fy wish solve distribution parameters discriminative regression function margin variables theorem maximum entropy discrimination regression problem cast follows find theta fl minimizes kl pkp subject constraints theta fl gamma theta fl theta fl fl gamma theta ffl tube svm literature region insensitivity loss function penalizes approximation errors deviate ffl data 
chapter 
extensions maximum entropy discrimination margin prior distributions top associated potential functions bottom 
theta discriminant function prior distribution models margins 
decision rule theta theta theta 
solution theta fl theta fl gammal theta fl gammal theta gammafl objective function gamma log 
typically prior fl differs classification case due additive role output versus multiplicative sided constraints 
fl ae fl ffl ffl gammafl fl ffl oe integrating obtain log fl log ffl fl dfl ffl ffl gammafl fl dfl log ffl gamma ffl gamma log ffl gamma gammaffl gamma ffl gamma log log gamma gamma ffl gamma shows prior associated penalty terms different settings ffl 
varying ffl effectively modifies thickness ffl tube function 
furthermore varies robustness outliers tolerating violations ffl tube 
margin prior tends produce regressor insensitive errors smaller ffl penalizes errors linear loss controls steepness linear loss 
chapter 
extensions maximum entropy discrimination med approximation sinc function noise free case left gaussian noise right 
svm regression assume linear discriminant function linear decision kernel med formulation generates objective function arises svm regression theorem assuming theta theta fl fl approaches non informative prior fl equation lagrange multipliers obtained maximizing subject gamma gamma ffl log gamma log gamma gamma ffl gamma log gamma log gamma gamma ffl gamma gamma gamma gamma seen objective similar svm regression :10.1.1.127.1519:10.1.1.127.1519
additional penalty functions logarithmic terms considered barrier functions optimization maintain constraints 
illustrate regression approximate sinc function popular example svm literature 
sampled points sinc jxj gamma sin jxj interval 
considered noisy version sinc function gaussian additive noise standard deviation added output 
shows resulting function approximation similar svm case 
kernel applied th order polynomial generative model regression case med classification consider med regression scenario regression model linear linear kernel manipulation regression model probability distribution 
regularization epsilon tube properties svm approach readily applied estimation generative models regression kernel implicitly transforms input data modifying dot product data vectors phi phi done explicitly remapping data transformation phi conventional dot product 
permits non linear classification regression basic linear svm machinery 
example th order polynomial expansion replaces vector phi 
chapter 
extensions maximum entropy discrimination setting 
furthermore exponential family mixtures exponential family elaborated subsequent chapter 
modifying discriminant function theta usual linear form 
consider class problem generative model class xj xj gamma 
example gaussian distribution mixture gaussians complex structured model hidden markov model 
form regressor directly combine generative models considering log likelihood ratios discriminant function follows theta log xj xj gamma aggregate parameter set theta gamma bg includes generative models scalar bias 
merely changing discriminant function med framework estimate generative models form regression function 
generators gaussians equal covariance regression function effectively produce linear regression 
discriminant function generally nonlinear give rise non convex hull constraints standard regularization setting 
med framework due probabilistic solution theta discriminant functions behave convex program 
furthermore possible iterative bounds machinery chapter deal latent discriminant regression functions form theta log xj xj gamma feature selection structure learning med framework limited estimating distributions continuous parameters theta 
solve distribution discrete parameters structure learning 
form structure learning feature selection 
feature selection problem cast finding structure graphical model identifying set components input examples relevant classification task 
generally feature selection viewed problem setting discrete structural parameters associated specific classification regression method 
feature selection med framework ignore components input space vectors relevant classification regression task 
naturally provide computational advantages algorithm ignore inputs run time 
feature selection reduce input dimensionality show helps improve generalization accuracy classification regression cf 

omission certain input dimensions permits better generalization leads notion sparsity input dimensionality addition sparsity support vectors lagrange multipliers discussed section 
critical input space high dimensionality irrelevant features data set small 
initially derive feature selection med framework feature weighting permit probabilistic solution 
feature structural parameter probability value 
feature selection process estimates discriminative probability distribution structural parameters estimates discriminative parameter model 
irrelevant features eventually receive extremely low probabilities selected 
feature selection process performed jointly discriminatively model estimation specifically optimize classification regression criterion feature selection usually improve results example svm point start removing features 
chapter 
extensions maximum entropy discrimination feature selection classification med formulation extended feature selection consider augmenting distribution models margins bias distribution feature selection switches 
augmentation paradigm initially discussed section conditions preserve solvability med projection 
consider augmenting linear classifier svm feature selection 
introduce extra parameters linear discriminant function theta familiar correspond linear parameter vector bias parameter usually denoted 
addition scalar parameters introduced binary switches 
structural parameters completely turn feature leave 
solve optimal feature selection brute force method try configurations discrete switch variables 
med formulation consider distribution switches lead tractable computation 
fact switches discrete continuous parameters note violate med formulation 
partition function expectations discriminant functions involve summations integration continuous parameters 
med solution distribution theta includes linear model switches bias theta define prior desired med solution discuss solve optimal projection 
prior reflect regularization linear svm parameters degree feature selection enforce 
words specify coarse terms feature switches set zero remain active 
possible prior solution theta uninformative prior zero mean gaussian prior infinite variance usual white gaussian prior ae gamma ae gammas ae controls prior probability including feature 
prior feature merely bernoulli distribution 
user selects ae setting ae produce original linear classifier problem feature selection 
decreasing ae features removed 
prior distribution parameters med formalism discriminant function readily compute partition function cf 
equation 
solving integrals summations obtain objective function log gamma gamma log gamma ae alternatively finite variance gaussian give quadratic penalty term final objective function gamma oe hard equality constraint 
chapter 
extensions maximum entropy discrimination false positives roc curves splice site problem feature selection ae solid line ae dashed line 
maximize subject 
maximized obtain optimal setting lagrange multipliers 
setting linear classifier merely ae ae gamma ae exp gamma indicates th dimension th training set vector 
bias estimated separately kuhn tucker conditions finite variance bias prior set oe terms large parentheses equation linear coefficients new model denoted experiments test linear feature selection method dna splice site classification problem identify true spurious splice sites dna sequence 
examples fixed length dna sequences length binary encoded bit translation fa gg element vector 
training set consisted examples test set contained examples 
results depicted shows superior classification accuracy feature selection opposed feature selection roughly equivalent svm 
feature selection process drives linear model coefficients zero aggressive pruning manner 
provides better generalization efficiency run time 
picture sparsity resulting model plot cumulative distribution function magnitudes resulting coefficients jc function components linear classification vector 
indicates weights resulting feature selection algorithm small neglected 
derivation feature selection far performed linear models mimic kernel nonlinear classifier mapping feature vectors explicitly higher order representation polynomial expansions 
retain efficiency implicit kernel mappings infinite kernel mappings infeasible ability fine scale feature selection components kernel mapping extinguished 
complexity feature selection algorithm linear number features easily consider small expansions quadratic cubic polynomials explicit mapping 
problem attempted quadratic expansion dimensional feature vectors chapter 
extensions maximum entropy discrimination cumulative distribution functions resulting effective linear coefficients feature selection solid line dashed line 
false positives roc curves corresponding quadratic expansion features feature selection ae solid line ae dashed line 
concatenating outer products original features form resulting dimensional feature space 
shows feature selection helpful compared plain linear classifier improving performance larger expanded feature space 
experiment feature selection classification label protein chains uci repository valid splice sites classes intron exon exon intron ei 
called donor acceptor sites respectively 
chains consist represented binary code 
uncertain base pairs represented mixed version represented 
scalar input dimensions binary output class 
trained exemplars test remaining exemplars testing set 
depicts performance 
training linear classifiers easily separate classes accuracy features causes fitting 
regularization brought varying prune away features ignores outliers 
length protein chain useful determining acceptor donor status ignore dimensions data exemplars 
best performance possible regular svm feature selection ae remains testing vary regularization small amount feature selection quickly improves performance significantly 
experiments indicate level ae gamma ae gamma helps obtain greatest generalization accuracy 
error halved svm count errors error count feature selection 
depicts linear model svm pruned model feature selection technique 
chapter 
extensions maximum entropy discrimination acceptor donor protein classification regularization constant svm varying regularization feature selection levels acceptor donor protein classification 
testing performance unseen protein sequences 
dashed line indicates svm performance solid lines indicate varying performance improvements due feature selection 
optimal feature selection levels problem appear ae gamma ae gamma 
linear parameter magnitude linear parameter magnitude sparsification linear model 
left parameters svm linear model right parameters feature selection technique linear model 
note sparsification parameters set right 
pruning encourages better generalization 
chapter 
extensions maximum entropy discrimination feature selection regression feature selection advantageous regression case map learned inputs scalar outputs 
input features irrelevant especially kernel expansion employ aggressive pruning approach adding switch parameters 
prior ae gamma ae gammas lower values ae encourage sparsification 
prior addition gaussian prior parameters theta quite sparsification properties 
previous derivation feature selection applied regression context 
priors prior margins swapped equation 
shall include estimation bias case gaussian prior oe 
replaces hard constraint soft quadratic penalty making computations simpler 
straightforward algebraic manipulations obtain form objective function gamma gamma ffl gamma oe gamma log gamma log gamma gamma ffl gamma log gamma log gamma gamma ffl gamma gamma log gamma ae gamma objective function optimized concavity unique maximum 
optimization lagrange multipliers controls optimization densities model parameter settings theta switch settings 
joint discriminative optimization feature selection parameter settings 
optimal setting lagrange multipliers resulting med regression function xnew ae gamma ae gamma ae exp gamma gamma xnew bias oe gamma 
experiments evaluate feature selection regression support feature machine popular benchmark dataset boston housing problem uci repository 
total features treated continuously predict scalar output median value owner occupied homes thousands dollars 
evaluate dataset utilized linear regression nd order polynomial regression applying kernel expansion input 
dataset split training samples testing samples 
table indicates feature selection decreasing ae generally improves discriminative power regression 
ffl sensitive linear loss functions typical svm literature shows improvements feature selection 
just sparseness number vectors helps generalization sparseness number features advantageous 
total input features nd order polynomial kernel expansion 
discriminative power pruning beneficial 
trial settings sparsification level prior ae ae ae analyze cumulative density function resulting linear coefficients function chapter 
extensions maximum entropy discrimination linear model estimator ffl sensitive linear loss squares fit med ae med ae med ae med ae table prediction test results boston housing data 
note due data rescaling relative quantities meaningful 
cumulative distribution functions linear regression coefficients various levels sparsification 
dashed line ae dotted line ae solid line ae 
features explicit kernel expansion 
clearly indicates magnitudes coefficients reduced sparsification prior increased 
med regression predict gene expression levels data systematic variation gene expression human cancer cell lines ross log ratios log rat gene expression levels predicted renal cancer cell line measurements gene expression levels different cell lines cancer types 
input data forms dimensional vector output dimensional scalar gene expression level 
training set size limited examples testing examples 
table summarizes results 
ffl med approach 
indicates feature selection particularly helpful sparse training situations 
feature selection generative models mentioned earlier med framework restricted discriminant functions linear non probabilistic 
instance consider feature selection generative model estimator ffl sensitive linear loss squares fit med ae table prediction test results gene expression level data 
chapter 
extensions maximum entropy discrimination classifier 
simple case discriminant formed ratio identity covariance gaussians 
parameters theta means gamma classes respectively discriminant theta log gamma log insert switches turn certain components gaussians giving theta gamma gamma gamma discriminant uses similar priors ones previously introduced feature selection linear classifier 
straightforward integrate sum discrete priors shown equation get analytic concave objective function ae gamma ae gammas ae gamma ae gammar short optimizing feature selection means generative models jointly produce degenerate gaussians smaller dimensionality original feature space 
feature selection process applied density models principle computations may require mean field approximations tractable 
transduction section provide maximum entropy discrimination framework solving missing labels transduction problem 
classification problems labeled data scarce unlabeled data may easily available large quantities 
med framework easily extended utilize unlabeled data forming discriminative classifier integrating unobserved labels 
previous initially med approach transductive classification primarily mean field approximations 
szummer presents alternative transduction approach terms kernel expansions may cast med formalism 
classification setting exact solution resulting med projection intractable just svm transduction 
review mean field approximation case possible local solution 
global information projection solution possible prior unobserved labels described distribution conjugate continuous original distribution models 
provide transduction algorithm computes global large margin solution labeled unlabeled data forcing prior conjugate 
subsequently discuss unlabeled data regression scenario yield tractable global med solution 
transductive classification svm transduction requires search binary labels unlabeled exemplars 
complexity approach grows exponentially 
joachims proposes efficient heuristics approximate process guaranteed converge true svm transduction solution 
svms med approach permits probabilistic treatment search labels somewhat similar spirit relaxation methods 
discrete search problem embedded continuous probabilistic setting 
recall med solves distributions parameters unknown quantities augmenting solution space 
example margins unknown non separable chapter 
extensions maximum entropy discrimination problem introduced solution posteriors theta fl prior 
feature selection structure unknown cascaded final med posterior solution theta fl 
transduction case unlabeled examples unknown 
hypothesize prior posterior distribution 
distribution ideally take form delta functions gamma 
solving distribution say theta fl generalize non separable transductive case theta fl projection larger space 
general solution theta fl theta fl theta gammafl normalization constant partition function set non negative lagrange multipliers classification constraint 
set finding unique maximum objective function gamma log 
distribution required computation partition function remains analytic 
assume prior continuous unlabeled natural choice pointwise delta function ffi ffi gamma integrals intractable 
proceed mean field approximation performed effectively computes integral theta unlabeled locked current estimate computes update theta held fixed 
equivalent assuming distribution theta fl forced factorize theta fl 
details provided produce generalization results unlabeled data useful classification problem 
mean field approximation forces obtain local solution longer unique 
stage iterative algorithm poorly initialized may problem 
guarantee analytic computation non transductive case yield log convex 
consider exponential family distribution 
concave function setting variables 
variables considered data distribution parameters exponential family 
possible find conjugate distribution variables integral analytic 
instance gaussian equivalently quadratic svm conjugate distribution gaussian final concave analytic 
assume data set partitioned sets labeled unlabeled data 
labeled components unlabeled components 
consider vector labels vector lagrange multipliers split follows labels known know labels prior distribution 
prior scaled zero mean spherical gaussian gamma 
interpreted prior individual unlabeled data point pi pi gamma 
consider vectors diagonal matrix form diag diag respectively 
similarly consider data matrix matrix data vectors arranged chapter 
extensions maximum entropy discrimination columns 
divided labeled unlabeled vectors follows simplicity derive transduction assuming linear classifier drop bias term theta 
limit linear decision boundaries generated intersect origin 
restriction circumvented concatenating constant scalar input features formulation show readily admits kernels augmented bias term little extra derivation 
classifier discriminant function theta derive corresponding partition function constant scalar factor fl fl gammafl theta fl fl gamma fl theta fl may consider dealing directly objective function gamma log case decomposition objective function fl solving ultimately obtain standard fl fl log gamma remaining component partition function log fi fi fi gamma fi fi fi gamma gamma gamma provides solution objective function 
analytic concave function single maximum uniquely determined obtain answer theta fl 
theorem assuming discriminant function form theta prior parameters margins theta fl fl gamma fl fl equation med lagrange multipliers obtained maximizing subject log gamma log fi fi fi gamma fi fi fi gamma gamma gamma chapter 
extensions maximum entropy discrimination interesting note case missing data objective function simplifies back regular fully labeled svm case 
objective function maximized techniques 
important various matrix identities kailath matrix partitioning techniques optimization efficient 
optimization gives desired values specify distribution theta fl 
constrained optimization problem solved axis parallel manner similar platt smo algorithm 
fact modify single value time axis parallel type optimization 
joint constraints vector step wise optimization feasible exiting convex hull constraints 
derived efficient update rule chosen lambda guarantee improvement objective function 
update different depending type choose basically labeled unlabeled 
particularly efficient iterate set labeled set unlabeled lagrange multipliers individually 
store running versions large unlabeled data matrix inverse gamma number ways current setting lagrange multipliers compute labels unlabeled data 
way find distribution theta theta current setting integrate obtain classification 
derive computation fl theta fl theta gammafl exp gamma gamma sigma 
obtain classifier merely need mean resulting gaussian distribution 
sigma classifier sign xnew sign gamma xnew delta specifically mean formula simplifications follow gamma gamma xnew xnew xnew definition gamma 
vector effectively defines linear decision boundary 
important choose large unlabeled data influences estimation strongly small values cause vanishing unlabeled lagrange multipliers lock unlabeled label estimates effectively reduce standard svm 
input vectors appear inner products computations formulation readily accommodate kernels 
chapter 
extensions maximum entropy discrimination transductive regression previous assumption gaussian unlabeled data reasonable regression case 
previous section showed unlabeled exemplars binary sigma classification problem dealt integrating gaussian prior 
gaussian continuous distribution match discrete nature classification labels 
regression outputs scalars better suited continuous gaussian prior assumption 
scalar outputs obey gaussian distribution may consider transforming respective cumulative density functions gaussian 
may consider continuous distribution priors 
gaussian advantages conjugate form necessary integrate med linear regression problem results quadratic log partition function non transductive case 
guarantees maintain closed form partition function transductive case 
wish unlabeled data regression scenario advantageous 
basic motivation transductive regression focus model predictions unlabeled data similarly distributed predictions labeled data 
words extrapolate new test regions unlabeled data regression function diverge exhibit unusual behavior 
produce outputs similar generates labeled data 
illustrated example fit noisy sinusoidal data set high order polynomial function 
example note 
standard regression scenario fitting polynomial sin function transduction generates regression labeled data blue dots sharply diverges unlabeled data green circles produces predictions far typical range gamma 
require outputs unlabeled data obey similar distribution probably stay gamma generate similarly distributed output produce better regression fit 
illustrated polynomial fit obey output distribution extrapolate unlabeled data 
important go far regression function follow prior unlabeled data closely compromise labeled data fitting natural regularization properties parameters 
usual med multi variable prior distribution important balance different priors 
development regular non transductive regression setting 
support vector machine typically cast large margin regression problem linear discrimination function epsilon tube insensitivity linear loss 
input data high dimensional vectors xt corresponding scalar labels wish find linear regressor lies ffl output 
regressor discriminant function theta recall objective function regression case theorem 
assume non informative prior zero mean gaussian prior covariance oe obtain slightly modified objective function optimized subject gamma gamma ffl gamma oe gamma log gamma log gamma gamma ffl gamma log gamma log gamma gamma ffl gamma gamma gamma gamma case unlabeled data know particular values introduce prior chapter 
extensions maximum entropy discrimination polynomial regression transductive polynomial regression transductive regression vs labeled regression illustration 
attempting fit sin function high order polynomial 
labeled data shown blue dots unlabeled data put shown green circles axis 
unlabeled data merely fit regression function red line unfortunately diverges sharply away desired function unlabeled data 
polynomial maintain similar distribution outputs roughly unlabeled exemplars produces reasonable regression function :10.1.1.37.8662
integrate obtain partition function 
prior shall unobserved white gaussian prior 
modifies optimization function follows 
observe component depends gamma going back partition function representation component theta exp gammay gamma theta value unknown integrate gaussian distribution prior gaussian prior gives rise computation theta gamma exp gamma exp gammay gamma theta ultimately updated transduction function modified follows unlabeled data exemplars gamma transductive regression case obtain objective function labeled gamma unlabeled gamma gamma ffl gamma oe gamma log gamma log gamma gamma ffl gamma log gamma log gamma gamma ffl gamma gamma gamma gamma uniform prior feasible 
chapter 
extensions maximum entropy discrimination final theta computation straightforward solve optimal setting maximizing 
effectively generates simple linear regression model takes account unlabeled data 
practice values don white gaussian distribution transform white gaussian standard histogram fitting techniques just whitening affine correction solve med regression 
transformation inverted obtain values appropriate original problem 
depicts results ailerons data set camacho addresses control problem flying aircraft 
inputs continuous attributes describe status airplane pitch roll climb rate output control action ailerons 
implicit second order polynomial quadratic kernel regression model 
labeled case trained labeled data points standard svm regression 
med transductive regression case labeled unlabeled examples training 
depicts better regression accuracy transduction techniques appropriate levels regularization non transductive regression remains somewhat fixed despite varying regularization levels 
regularization accuracy rms transductive regression vs labeled regression flight control 
show inverse rms error labeled regression case dashed red line transductive regression case solid blue line varying regularization levels 
appears transduction useful labeled data ambiguous cause large errors extrapolating region sampled new test data unlabeled data 
gaussian prior unobserved variables effectively constrains extrapolation caused fitting preventing unlabeled examples having extreme outputs 
unlabeled examples convex hull labeled ones transductive regression beneficial 
extensions sections motivate extensions cursory level completeness 
thorough derivations results concerning anomaly detection latent anomaly detection tree structure learning invariants theoretical concepts 
med framework just limited learning continuous model parameters 
gaussian means covariances 
learn discrete structures 
instance may consider med chapter 
extensions maximum entropy discrimination learn parameters structure graphical model 
instance theta may partitioned component learns discrete independency structure graphical model component learns continuous parameters probability tables 
med solves continuous distribution discrete continuous model components estimation remain straightforward 
tree structure estimation 
example consider solving tree structures classifier results likelihood ratio tree distributions 
space dimensionality nodes tree connect 
show example nodes connected different tree structures 
configuration left right 
resulting discriminant function form theta log xj xj gamma gamma theta model description composed set sigma continuous parameters tree structure component sigma specifies configuration edges various nodes 
classification constraints involve integration summation discrete structures gamma gamma fl theta gamma fl gamma dfl db similarly computation partition function require integration exponentiated constraints prior gamma gamma fl 
exponential number tree structures connect nodes summing gamma intractable 
due interesting results graph theory matrix tree theorem summing possible tree structures graph done efficiently 
reminiscent section discussed alternative form structure learning 
solved discrete component model feature selection 
similarly sum exponential number feature selection configurations 
problem earlier embedding computation probabilistic med setting solvable efficient way 
details tree structure estimation omitted provided 
mixture models latent variables far seen variations med formalism flexible application different scenarios different models 
key benefit med enjoys ability combine genera chapter 
extensions maximum entropy discrimination tive modeling virtues prior distributions discriminative requirements typically manifested constraints 
far restricted generative models med simple typically uni model distributions 
example shown exponential family classification gaussian multinomial models 
thoroughly harness power generative modeling go simple models consider mixture models latent variable models 
models include sigmoid belief networks latent bayesian networks hidden markov models play critical role applied domains speech recognition vision 
potential clients med formalism benefit strongly discriminative estimation technique opposed traditional maximum likelihood incarnations 
ideally handle latent mixture models med discriminative setting 
type problem decidedly difficult cases seen far 
latent models mixtures give rise logarithms sums negated logarithms sums 
computational difficulties arise various med integrals optimizations intractable 
maximum likelihood bayesian estimation readily addressed jensen inequalities simplify expressions generate iterative variants closed form computations non latent case 
expectation maximization em algorithm tool iteratively solves simpler version intractable latent variable problem jensen lower bounds logarithm sum 
em algorithm designed maximum likelihood estimation insufficient med discrimination involves negated logarithms sums negation flips jensen lower bounds creates undesirable upper bounds 
latent discrimination require novel bounds discriminative variant em algorithm 
chapter propose discriminative counterpart develop novel bounding tools permit discrimination latent models 
treatment chapter focus maximizing conditional likelihood tools developed cml useful latent discrimination med 
chapter latent discrimination cem entities multiplied unnecessarily william ockham discussed frameworks optimizing discriminative power generative models 
include maximum conditional likelihood conditional bayesian inference aforementioned discrimination 
emphasize accuracy task margins background probability 
computational problems quickly arise applied latent models mixture models models hidden variables 
mixture models generative machine learning 
structures mixtures gaussians bayesian networks hidden markov models forth latent generative models just simple exponential family distributions 
latent aspects models prevent mathematically tractable discriminative setting 
statistical model estimation inference require maximization evaluation integration complicated mathematical expressions 
approach simplifying computations find manipulate variational upper lower bounds expressions 
prominent tool computing bounds jensen inequality subsumes informationtheoretic bounds cf 
cover thomas 
maximum likelihood ml estimation incomplete data jensen derive iterative expectation maximization em algorithm 
graphical models intractable inference estimation performed variational bounds 
bayesian integration uses jensen em bounds compute integrals intractable 
discriminative frameworks handle latent models need discriminative version em algorithm bounds uses 
known generative frameworks 
likelihood em algorithm uses jensen inequality lower bound latent log likelihoods 
resulting tractable variational bounds integrated maximized straightforward manner 
generative counterparts discriminative frameworks involve latent log likelihoods negated log likelihoods 
lower jensen inequality risk ockham truly intended say shall quote motivate bounds latent likelihoods treated exactly multiplied exactly cause exponential explosion number terms 
chapter 
latent discrimination cem require type reverse jensen inequality 
derive inequality arbitrary mixtures exponential family 
includes complex mixtures arising hidden markov models 
resulting bounds parameters mixing proportions permit straightforward maximization steps integration 
specifically show mixture model incomplete distribution lower bounded jensen inequality upper bounded reverse jensen inequality single complete family distribution parameters 
discriminative learning reverse bounds tight permit efficient estimation approaches em computationally yields superior classification regression accuracy 
call resulting discriminative em variant conditional expectation maximization cem algorithm 
chapter organized follows 
describe set probabilistic models restrict mixtures exponential family 
argue latent models handled tractably manner exponential families 
typically form intractable expressions multiplied 
formal treatment models exposes generative discriminative criteria intractable 
describe em algorithm way address intractability latent models 
em addressed intractable estimation problem iteratively maximizing lower bound log likelihood jensen inequality 
discriminative criteria require reverse upper bound 
propose bound reverse jensen inequality develop variety cases 
includes mixtures gaussians mixtures multinomials mixing coefficients 
chapter considers sophisticated mixtures structure models hidden markov models aggregated data set bounds 
rigorous derivation proof reverse jensen bound provided chapter justifies latent discrimination cem 
exponential family mixtures restrict treatment latent variables jensen reverse jensen bounds mixtures exponential family family 
practice class densities covers large portion contemporary statistical models 
mixtures family include gaussians mixture models multinomials poisson hidden markov models sigmoidal belief networks discrete bayesian networks :10.1.1.52.696
family closely related generalized linear models form xj theta exp theta gamma theta family shown natural parameterization 
alternative parameterizations exist natural easiest manipulate purposes computing inequality performing discriminative estimation 
theta function cumulant generating function convex theta multi dimensional parameter vector 
typically data vector constrained live gradient space theta theta theta short 
fact duality exists domain function gradient space theta vice versa 
specific property exponential family cumulant generating function just arbitrary convex laplace transform 
directly due normalization property distribution directly generates convexity chapter 
latent discrimination cem theta theta log exp theta dx family special properties conjugates convexity linearity :10.1.1.52.696
furthermore important property class distributions products exponential family distributions remain family 
intrinsic properties exploited derivation reverse jensen bound chapter 
table lists example functions gaussian multinomial distributions 
distribution theta constraints gaussian mean gamma gamma log theta theta gaussian covariance gamma log gamma log theta theta multinomial log gamma gamma log log exp theta exponential gamma log gamma theta theta gamma gamma exp gamma log gamma theta theta poisson log exp theta table sample exponential family distributions 
crucial property maximum likelihood estimation parameters family distribution respect iid data set fully tractable unique straightforward 
log likelihood remains concave parameters family products thereof 
straightforward integrate exponential family distribution conjugate priors obtain fully bayesian estimate parameters 
widely acknowledged family enjoys tractable straightforward estimation properties 
mixtures exponential family graph standard mixture model generalization class exponential family distributions consider mixtures family 
considering simple flat mixture model chapter considers complex mixture cases 
convex combination different family distributions different parameters shown 
mixture done introducing latent variable denoted 
incomplete data representation unobserved multiple family mildly restricting derivations functions satisfying lemma 
standard family distributions spanned 
multinomial shown traditionally cast equation subject constraint ae 
table map exponential family form pi gamma 
standard case choice vector zeros single unit entry expressions simplify 
chapter 
latent discrimination cem models models mixed consider bayes net describe relationship latent variable acts parent emission variable generates distribution xj theta ff exp am xm theta gamma km theta allowing xm vary latent variable regular mixture model typically xm 
mathematical generalization extra flexibility done convenience need consider different types mixtures xm vary xm vector cardinality explicitly computed result function furthermore ff scalars sum unity representing prior model mixture 
assume ff fixed assumption section 
merely think standard mixture model mixture gaussians 
example may consider distribution weight height males females species form distinct clusters 
latent variable binary determines hidden male female variable select different gaussian distributions 
weight height 
type mixture called flat mixture additional relationships clusters hierarchy hidden variables types latent distributions arise frequently machine learning tasks include include mixtures gaussians mixtures experts mixtures multinomials forth 
latent probability distributions need get maximized integrated marginalized conditioned solve various inference prediction parameter estimation tasks 
manipulations quite complex intractable 
mixtures product logarithmic space non exponential family models mixture models latent models treated exactly closed form 
case generative discriminative frameworks 
med just maximum likelihood show exponential family member estimated straightforward manner 
consider mixtures exponential family interpretations arise 
shall illustrating problems different metaphors product space logarithmic space 
consider likelihood independent identically distributed iid data set product space theta pi theta generative model theta data point exponential family aggregate likelihood data set remains computationally tractable 
fact products exponential family exponential family 
words family forms closed set multiplication addition unfortunately 
example obtain family distribution theta pi exp theta gamma theta exp theta gamma tk theta note theta denote aggregate model encompassing individual theta 
chapter 
latent discrimination cem aggregate theta exponential family just easy integrate maximize likelihood single data point theta 
example maximizing likelihood parameter theta simply unique closed form solution theta theta generative model theta exponential family mixture model latent distribution represented sum marginalization possibly continuous set simpler distributions 
unfortunately summations products exponential family distributions exponential family 
example consider case summing exponential family distributions get theta words theta 
case possible configurations simpler distributions expanding get theta pi theta see quite clearly expand product sums effectively summation terms fact exponential number equal point integration calculations solvable bring sum apply simple non latent distributions isolation 
having deal total terms prevents approach tractable toy problems 
exact bayesian integration depicted example cumbersome 
case manipulate log likelihood likelihood maximum likelihood estimation 
logarithm put logarithmic space deceptively appear simplify expressions log theta log pi theta log theta log theta summation tractable number terms terms total 
presence log sum equally intractable prevents direct integration maximization steps 
non exponential family distributions mixtures prevent easy maximization calculations ml maximizing gradients setting zero 
alternatively bayesian inference attempt perform integrals compute example evidence fxg fxg theta theta pi theta theta theta integrating product sums difficult task 
fact intractability resulting products sums equivalent intractability resulting addition chapter 
latent discrimination cem logarithms sums 
highly desirable find surrogate distributions easier manipulate latent distributions log sums 
upper lower bounds log sum give guarantees integrals iteratively maximize objective functions likelihood 
conceptually pull logarithm sum sum logs log sum 
intractability directly avoided introduce bounds upper lower permit tractable multiplication maximization integration 
chapter derive upper lower bounds provide analytic formula obtaining 
basically multi modal complex expressions shown replaced simple exponential family distributions upper lower bound avoid 
focus log sum interpretation sections results directly equivalent product space 
product space convenient integration logarithmic space convenient maximization 
discuss celebrated em algorithm just lower bounds latent log sums alleviate computational intractability 
expectation maximization divide conquer expectation maximization em algorithm finds roots successor old heuristic approaches fitting mixtures models clustering algorithms means 
case mixtures models latent maximum likelihood estimation notions divide conquer motivate em type procedures :10.1.1.114.4996
view maximum likelihood mixture model independent models available collectively describe training data set 
divide data models sort competition model data accounts strongly 
iterating division data individual models breaks difficult fit complex training set points small fitting operations partitions training data 
means models clusters points competing winner take scenario 
em divide conquer operation models greedily take point winner take share soft responsibility assignment data point 
words models describe point better higher responsibility weight 
computationally division permits model estimated data simplifying log sum equation instance 
model estimated weighted configuration data previous responsibility levels 
effectively summation inside logarithm pulled outside avoiding difficulties steps intractable integration bayesian inference equation 
em strategy conquer divide works intuitive mathematical properties maximum log likelihood concavity log function direct applicability jensen inequality forming guaranteed lower bound log likelihood 
fact change objective function conquer divide strategy 
depicts em effectively doing 
step compute lower bound log likelihood jensen inequality step maximize lower bound 
iterating procedures bounding maximizing objective function guaranteed increase monotonically 
unfortunately discriminative criteria em simple conquer divide strategy jensen inequality generate lower bound objective functions 
elaborated sections provide high level discussion 
computationally deferring formal treatment favor intuitive explanation em 
chapter 
latent discrimination cem expectation maximization iterated bound maximization 
differentiates discriminative criteria ml require jensen type lower bounds need corresponding upper bounds 
jensen bounds partially simplify expressions remain 
instance latent distributions need bounded discriminative setting 
metaphorically discriminative learning requires lower bounds cluster positive examples upper bounds repel away negative ones 
conquer divide doesn need discriminative variant conquer divide strategy 
latency conditional discriminative criteria apply em discriminative setting 
combination ml estimates em jensen produced straightforward monotonically convergent estimation procedures mixtures family :10.1.1.52.696
em ml explicitly address task learning system 
non discriminative modeling technique estimating generative model 
consequently em ml suffer assumptions model inaccurate 
ml classifier gamma gamma cml classifier gamma maximum likelihood versus maximum conditional likelihood 
thick gaussians represent thin ones represent 
visualization observe binary classification problem fig 

training data set consists positive class negative class 
sampled identity covariance gaussians class 
gaussian equal probability 
derivations extend multi class classification regression 
chapter 
latent discrimination cem fit data class generative model incorrectly gaussians class equal probability identity covariance 
solutions shown ml cml 
gaussians model depicted iso probability contour 
thick circles represent positive gaussian models thin circles represent negative gaussian models 
see values joint log likelihood conditional log likelihood solution 
note ml larger value cml larger value configurations 
distributions induce classification boundary points positive gaussian model greater negative assigned positive class vice versa 
results decision boundary splits half horizontal line middle positive gaussians overtake top negative ones overtake bottom half 
counting number correct classifications see ml solution performs random chance getting roughly accuracy 
ml model trying cluster data put gaussian models disposal probability mass top data samples belong class 
fact fitting positive data positive model done independently fit negative data negative model 
precisely em iterates objective maximizes likelihood 
objective get generator data classification performance sacrificed 
decision boundary generated model creates horizontal classification strips opposed splitting half 
classify data positive negative positive negative respectively go top bottom gaussians arranged vertical interleaving order 
accuracy fit roughly 
cml attempts form output class label distribution input appropriately suited classification 
cml estimating conditional density propagates classification task estimation criterion 
clear model generator data gaussians don put probability mass samples simply arranged middle provides low value likelihood clearly optimizing cml appropriate finding classifier model 
needed discriminative conditional counterpart em seeks optimize 
describe classification scenario mathematically 
examples training examples corresponding binary labels goal classify data latent variable family model mixture gaussians 
represent latent missing variables 
consider objective functions immediately exhibit aforementioned intractable log sum structures 
generative log likelihood objective function formula log theta discriminative conditional likelihood approach conditional log likelihood log sums negated log sums log jx theta log theta gamma log theta alternatively considered discriminative med approach discrim chapter 
latent discrimination cem function result complications due log sum xj theta log xj theta xj theta gamma log xj theta gamma log xj theta gamma latent log likelihoods discriminant functions recognize presence logarithms sums negated log sums 
cause remain product space interpretation 
em handle log sums jensen inequality manipulate lower bounds log likelihood 
log sum objective function lower bounded lower bounds add form aggregate lower bound 
conditional discriminative criteria observe negated log sums 
negation flip jensen inequality lower bounds 
applying jensen negated components objective functions discriminant functions produce upper bounds 
log sum terms lower bounded negated log sum terms upper bounded 
adding lower bounds upper bounds useless sinc generate desired aggregate lower bound discriminative quantities conditional likelihood med discriminant functions 
show jensen inequality lower bound log sums produce em algorithm 
discrimination derive complementary upper bounds reverse jensen inequality 
reverse bounds structurally similar jensen bounds allowing easy migration ml techniques discriminative settings 
bounds useful mathematical tools non statistical problems 
focus development bounds mixtures exponential family desirable properties permitting establish guaranteed bounds discriminative quantities conditional likelihood maximum entropy discrimination 
bounding mixture models mentioned earlier optimizing likelihood conditional likelihood intractable latent models mixture models equivalently 
em algorithm shown suited maximize likelihood replaces complicated log sum expressions arise simple lower bounds maximized straightforward way 
discrimination conditional likelihood maximization negated log sums lower bounds 
clear need bound latent quantities log sums sides lower upper bounds 
assume generative model mixture exponential family described earlier xjm 
log likelihood optimization data point gives rise log sum term log xjm causes 
propose upper lower bounds log sum appear follows logarithmic space wm log ym jm theta log xj theta gammaw log ym jm theta exp sides allows consider bounds product space exp pi ym jm theta wm xj theta exp pi ym jm theta gammaw similar weaker bound shown gaussian mixture regression 
chapter 
latent discrimination cem inspection clear left hand side right hand side inequalities share similar structure 
homogeneous structure critical lower bounds upper bounds combined way simplify log sums 
furthermore bounds evident longer dealing log sum sum logs pulled log summation 
structure far easier handle computationally 
product space longer dealing sum family distributions products families remain family immediately inherit straightforward computational estimation properties 
specify parameters upper lower bounds wm ym wm ym respectively subscript ranges number latent models 
done shortly give conceptual description parameters roles 
log log log dual sided bounding latent likelihood left hand side inequality follows direct consequence jensen inequality computes guaranteed ym wm 
right hand side direct consequence reverse jensen inequality computes guaranteed ym wm 
bounds basically give weight data points wm translate coordinates xm ym avoid representing sum distributions 
replaced sum exponential family members bounds products exponential family 
said earlier products family remain family intractable latent log likelihood quantities bounded simple family distributions 
bounding illustrated example family gaussian mean 
middle curve concave convex original latent log likelihood upper bound convex consequence negated weight gammaw complete likelihood lower bound concave consequence positive weight complete likelihood 
upper lower bounds contact original log sum quantity point theta referred contact point 
performing iterative maximization em current model estimate get iteratively updated bounding maximization step 
table summarizes meaning parameters 
sections provide equations compute log sum form shown equation current operating contact point theta 
jensen bounds recall definition jensen inequality ef eff concave log summations xj theta involve concave log expectation log sum proba chapter 
latent discrimination cem parameter role jensen wm scalar weight virtual data th model theta ym virtual data vector computed datum xm th model theta additive constant ensuring lower bound equals log sum theta theta reverse jensen wm scalar weight virtual data th model theta ym virtual data vector computed datum xm th model theta additive constant ensuring upper bound equals log sum theta theta table jensen reverse jensen bound parameters 
mixture latent variables 
apply jensen follows log xj theta xj theta xj theta log xj theta xj theta log xj theta traditional denote terms parentheses hm refer responsibilities :10.1.1.114.4996
thought weights model data point shared model generated 
hm xj theta xj theta jensen inequality application recast form shown equation 
manipulation readily shows log sum intractability removed 
recall lower bound wished form log sum log xj theta wm log ym jm theta expand form family notation log ff exp am xm theta gamma km theta wm am ym theta gamma km theta scalar additive constant wm positive scalar weights data ym new virtual data vectors translated original xm 
forms variational lower bound log sum tangential contact theta easier manipulate 
basically log sum sum log exponential family members 
plugging results equation expanded family notation gives parameters lower bound exponential family form log xj theta gamma wm ym theta gamma km theta ym gamma wm hm km theta theta fi fi fi fi theta gamma xm km theta theta fi fi fi fi theta xm wm hm chapter 
latent discrimination cem note positive scalar hm terms responsibilities arise jensen inequality 
quantities relatively straightforward compute 
require local evaluations log sum values current theta compute global lower bound 
bound log sums log likelihood lower bound objective maximize easily 
iterating maximization lower bound computation new theta produces local maximum log likelihood em applying jensen log sums xj theta straightforward 
terms expressions involve negative log sums jensen solving upper bound terms 
want lower upper bounds xj theta need compute reverse jensen bounds 
reverse jensen bounds reversals converses jensen inequality explored mathematics statistics community summarized 
reversals correct form direct discriminative conditional latent learning derived reversal detailed chapter 
strange reverse jensen ef eff possible 
things exploit convexity functions family exploiting concavity log 
reverse bound upper bound log sum form jensen bound sum log exponential family terms 
way upper lower bounds combined homogeneously ml tools quickly adapted new bounds 
derivation reverse jensen inequality long deferred chapter 
simply show calculate inequality required parameters wm ym guarantee global upper bound log sum 
recalling equation note log xj theta gammaw log ym jm theta expanding exponential family form obtain log ff exp am xm theta gamma km theta gammaw ym theta gamma km theta give parameters bound directly refer chapter algebraic derivation 
bound tangential contact theta upper bound log sum log xj theta wm ym theta gamma km theta ym hm wm theta theta fi fi fi fi theta gamma xm theta theta fi fi fi fi theta minw hm theta theta fi fi fi fi theta gamma xm theta theta fi fi fi fi theta theta theta wm hm xm gamma theta theta gamma xm gamma theta justifications convergence em include appeals kullback leibler divergence bregman distances 
concepts newer jensen inequality pre dates sufficient prove em convergence 
find multinomial bounds ff priors theta parameters section 
chapter 
latent discrimination cem introduced function fl simply fl fl log log gamma fl log fl fl gamma log fl fl bound effectively re weights wm translates incomplete data obtain complete data 
wm positive weights pick smallest wm satisfy simple conditions 
set wm larger conditions require smaller wm values mean tighter bound 
condition requires generate valid ym lives gradient space functions typical family constraint local computations log sum values gradients hessians current theta compute global upper bounds 
appendix contains tighter formulation 
fl function upper bound tighter reverse jensen solution wm involves numerical table lookups 
detailed derivation reverse jensen inequality chapter appendix 
lookup tables provides slightly tighter bounds 
furthermore practice compute tighter version wm omitting multiplicative front fl function 
empirically generate reasonable bounds analytic guarantees 
earlier derived simpler looser version bounds wm xm gamma theta theta gamma xm gamma theta visualization plot bounds component unidimensional gaussian mixture model case component binomial unidimensional multinomial mixture model 
jensen type bounds reverse jensen bounds shown various configurations theta jensen bounds usually tighter inevitable due intrinsic shape log sum 
addition viewing visualizations computed higher dimensional bounds sampled extensively empirically verifying reverse jensen bound remained log sum 
mixing proportions bounds recall equation contend mixture family distributions 
ff assumed constants 
variational bound solved theta model parameters allowed change 
may necessary vary ff parameters free optimized 
conceive bounds mixing proportions estimate optimal mixing coefficients 
possible find reverse jensen bound ff theta simultaneously just em jensen handle jointly 
done simply rewriting mixing proportions natural exponential family form seeing concatenated theta parameters 
development mirrors proposed appendix thereof :10.1.1.136.9119
gaussian case bounds tighter letting wm full matrix ignoring contribution chapter 
latent discrimination cem gaussian case multinomial case jensen black reverse jensen white bounds log sum gray 
return mixture model note ff allowed vary theta ff exp am xm theta gamma km theta simplicity define tm exp am xm theta gamma km theta allows simplify expression mixture model theta ff mtm show theta written sum exponential family members parameters ff jensen reverse jensen 
note dealing mixing proportions ff sum unity ff 
furthermore consider change variable dimensional ff space natural compact gamma dimensional space jm ff ff gamma rewrite theta follows theta tm ff tm ff ff ff tm exp log ff ff ff chapter 
latent discrimination cem tm exp log ff ff ff ff tm exp log ff ff gamma ff ff tm exp log ff ff gamma log gamma ff ff gamma tm exp log ff ff gamma log gamma ff ff tm exp log ff ff gamma log gamma ff ff gamma tm exp jm gamma log gamma jm tm exp gamma log gamma jm consider vector reparameterization ff gamma tuple 
rewrite multinomial family member 
recall multinomial functions xm log gamma exp allows rewrite theta gamma tm exp gamma tm exp gamma addition shall introduce virtual data vectors gamma tuples define 
gamma consider virtual data vectors xm zero single dimension furthermore vector xm zeros 
note function typical multinomial model 
rewrite latent likelihood parameterized ff familiar form theta tm exp gamma xm gamma delta tm exp gamma xm xm gamma delta realize rewrite exponential family form bounding techniques applied jensen reverse jensen 
derivations theta case adapted ff effectively multinomial family member 
plug back definition theta exp gamma xm xm gamma delta tm exp gamma xm xm gamma delta exp am xm theta gamma km theta omitting calligraphic font differentiate ones definition tm chapter 
latent discrimination cem note product family terms theta multinomial remains family 
consider agglomerative model theta bar indicates aggregated model contains parameters theta exponential family general form equation 
ff parameters folded exponential family distributions theta 
possible jointly upper bound logarithm theta ff theta reverse jensen approach 
recall typically xm mixture model argued earlier definition mixture exponential families 
encounter situation data xm varying value regular mixture model scenario 
justifies initial precautionary measure indexing data latent variable definition exponential family 
cem algorithm equipped jensen reverse jensen bounds straightforward implement maximum conditional likelihood algorithm discriminative learning approach latent variables 
cem conditional expectation maximization algorithm mirrors em algorithm approach maximizing joint likelihood 
em iterates lower bounding maximizing joint loglikelihood 
cem iterates lower bounding maximizing conditional log likelihood 
due guarantees jensen reverse jensen bounds cem converges monotonically local maximum conditional likelihood 
noted cem convergence slower em reverse jensen bounds looser 
inevitable byproduct shape negated log sum 
maximize conditional likelihood joint likelihood minus marginal likelihood type expression needs maximized parameters theta gamma log theta gamma log theta conditional log likelihood data set simple mixture model 
lower bound joint likelihood term jensen upper bound marginal likelihood term reverse jensen inequality 
gives lower bound conditional loglikelihood step called ce step cem algorithm mi hmi gamma theta mc gamma theta mc delta gamma mci gammaw mci gamma theta mc mci gamma theta mc delta constant terms straightforward maximize right hand side derivatives setting zero step theta mc theta mc ffi mi mci hmi ffi mci mci step unique solution due convexity cumulant generating functions 
fact step corresponds merely maximizing non latent exponential family distribution data weighted hmi mci scalar terms translated mci translated versions original data 
chapter 
latent discrimination cem visualization mentioned earlier cem involves weighted translated virtual data steps 
em involves weighted terms step employs jensen inequality 
reverse jensen inequality applies negated marginal likelihood provides repulsion term called background probability 
cem bounds negated marginal likelihood negated repulsion forces translating data position treating attractive force standard maximum likelihood setting 
xx xx cem data weighting translation 
depicts incomplete data models describe 
figures depicts complete data seen models data weighted class data translated 
similarly depicts model class sees 
interesting analogy translated weighted data depicted 
show mixture model light gaussians assigned class dark gaussian assigned class 
basically cem re weights data model class class data gets translated re weighted 
translation involves adding scaled gradient vector theta mc data 
gaussian case translation effectively moves data point mean model puts side 
repelling negative data incorrect class model translated side 
data models seen 
shows gaussians sees data nearby high weight data far away weight em 
class data points seen translated side model weight 
model effectively repel away 
similarly depicts weighting model different weights correct data different virtual data repulsion class 
depicts lonely model gets data equal weight repulsion term data 
gaussians exponential family model self dual data vectors parameter vectors lie space gradient space theta theta theta 
analogy translating means gaussian 
distributions analogy maintain geometric interpretation loosely higher level gradients cumulant generating function 
chapter 
latent discrimination cem experiments show experiments compare cem em pit conditional likelihood joint likelihood 
experiments straightforward reverse jensen bounds described previous section 
quite possible alternative schemes described annealing see section called data set bound see section helpful obtaining faster convergence quasi global optima remains explored 
cem clustering em clustering conditional log likelihoods log likelihoods classification accuracy cem vs em performance gaussian mixture model 
clustering cem computes em finds shown left 
plots depict conditional likelihood likelihood classification accuracy right 
cem performance shown solid blue line em performance shown dashed red line 
depict toy problem initially posed 
model mixture gaussians class mixing proportions equal gaussians identity covariance 
em cem initialized configuration 
em cem converge monotonically respective objective functions note em quickly converges maximum likelihood solution cem takes iterations converge maximum conditional likelihood solution 
problem maximum likelihood poor criterion resulting classifier em generates classification accuracy random chance 
cem produces classifier accuracy 
compare performance batch gradient ascent 
applied gradient ascent maximum likelihood problem maximum conditional likelihood problem 
bounds 
gradient ascent converge nicely maximum likelihood problem gets stuck local minima applied maximum conditional likelihood problem 
reverse jensen bounds advantages purely local optimization technique gradient ascent 
evaluation cem em standardized uci data set yeast data set 
classes yeast classified continuous features 
inputs chapter 
latent discrimination cem conditional gradient ascent joint gradient ascent conditional log likelihoods log likelihoods classification accuracy gradient ascent conditional joint likelihood 
optimizing gaussian mixture model 
gradient ascent joint likelihood gets stuck conditional 
plots depict conditional likelihood likelihood classification accuracy right 
gradient ascent conditional likelihood shown solid blue line ascent joint likelihoods shown dashed red line 
scaled translated zero mean identity covariance 
assumed poor model equal mixture gaussians class 
gaussian identity covariance restricts power model considerably 
depicts resulting performance em cem 
cem takes long time converge provides better conditional likelihood score better classification accuracy 
training data included exemplars test data included 
table summarizes results suggests cem better suited classification tasks random guessing produce accuracy 
training log likelihood conditional log likelihood accuracy em cem testing log likelihood conditional log likelihood accuracy em cem table cem em performance yeast data set 
deterministic annealing em algorithm known local minima problems avoided uses technique called deterministic annealing effectively replaces current models chapter 
latent discrimination cem conditional log likelihoods log likelihoods classification accuracy cem vs em performance yeast uci dataset 
plots depict conditional likelihood likelihood classification accuracy right 
cem performance shown solid blue line em performance shown dashed red line 
exponential family distribution temperature scaled version follows exp exp temp theta effectively jensen inequality bounding different pdf smoother properties 
similar technique feasible reverse jensen inequality 
gives deterministically annealed type maximum conditional likelihood algorithm annealed cem 
annealing simple operation replacing exponentiation operation exponentiation divide scalar temp temperature value 
follow standard annealing schedule temperature value starts large slowly decremented temperature goes unity 
typically bounds improve convergence obtain quasi global optima 
latent maximum entropy discrimination med framework directly benefit reverse jensen bounds straightforward see permit handle mixture models 
furthermore discrimination better suited classification regression conditional likelihood 
placing mixtures family med discriminant function theta log xj theta xj theta gamma recall constraints satisfied involve expectations discriminant function follows theta fl theta gamma fl expand constraints clear integrals produce intractable due presence summation hidden variable 
chapter 
latent discrimination cem analytic partition function objective optimize 
solution invoke jensen bounds follows loss generality assume gamma swap application jensen reverse jensen bound theta fl log theta gamma log theta gamma gamma fl theta fl log exp pi ym jm theta wm gamma log gamma exp pi ym jm theta gamma gammaw delta gamma fl lower bounded left hand side 
satisfy greater zero constraint lower bound just created jensen reverse jensen automatically satisfy original quantity true expectation constraint 
introduced bounds give stricter constraints med framework avoid 
logarithm operator longer acts summation integrals computed analytically provided theta fl conjugate form discriminant function probability model 
effectively replaced discriminant function theta lower bound upper bound gamma 
propose iterative med algorithm locally optimal unique 
assume start estimated mode model theta fl denote theta ffl step data point discriminant function bounded individually jensen reverse jensen bounds 
done current estimated mode model theta fl theta ffl step solve med optimization bounds obtain current lagrange multipliers 
new lagrange multipliers compute theta fl generate new estimated mode theta steps iterated convergence 
practice variational bound accurate med solution distribution theta peaked 
typically case converge final solution lagrange multipliers objective function settle locally optimal configuration 
latent med technique discussed interesting geometric interpretation 
depicts process interleave jensen reverse jensen bounds iterated med projection calculation 
impossible projection tractably full latent model find closest distribution med prior theta true admissible set large complicated convex hull 
pick arbitrary point theta typically em algorithm maximum likelihood estimate initialize posterior distribution 
find constrained convex hull formed invoking jensen reverse jensen bounds mode current theta theta smaller convex hull admits closed form solution involves family discriminant functions 
find closest point prior direct med projection yields theta 
process iterated small convex hull lies large original hull admissible set slowly updated reach local minimum med projection longer modifies solution theta 
experiments compare latent med standard em framework employed simple mixture gaussians model 
means permitted vary mixing proportions held fixed chapter 
latent discrimination cem iterated latent med projection jensen reverse jensen bounds 
direct projection admissible set give rise intractable med solution due mixture family discriminant function 
stricter convex hull admissible set jensen reverse jensen inequalities give rise simple family discriminant function permit closed form projection 
process iterated converge locally optimal point close prior possible remaining admissible set 
covariances locked identity 
data set pima indians diabetes data set available uci repository 
input forms dimensional feature space output binary class 
training performed input points testing performed remaining points 
table depicts performance em latent med technique reverse jensen bounds cem type iterative loop 
training accuracy testing accuracy em gaussian mixture em gaussian mixture em gaussian mixture med gaussian mixture med gaussian mixture med gaussian mixture table em latent med performance pima indians data set 
table results standard support vector machine note latent med performance achieves comparable polynomial kernel svm equivalently non latent med polynomial kernel 
latent med solution generative model em algorithm mixture gaussians discriminative aspect estimation med provides better classification performance 
svm med experiments regularization constant set 
furthermore svm experiments input space scaled remained unit cube normalization reasons em latent med implementations operated raw original data 
chapter 
latent discrimination cem training accuracy testing accuracy svm st order svm nd order svm rd order svm th order table svm polynomial kernel performance pima indians data set 
simple mixtures clear mixture exponential family distributions specified chapter capture latent model situations typical machine learning 
example difficult represent latencies arise structured graphical model flat mixture 
chapter expands mixture model seen encompass latent bayesian networks structured mixture model situations 
permit consider structures hidden markov models 
jensen reverse jensen inequalities reiterated models shall show efficient algorithms computing bounds parameters polynomial time 
subsequently chapter goes derivation details reverse jensen inequality put forward proof chapter 
show proof case structured mixtures chapter subsumes case flat mixtures chapter 
chapter structured mixture models previous chapter addressed problem discrimination standard flat mixture model 
resulted model avoided utilizing upper lower bounds mapped mixture exponential family model standard exponential family form permitting monotonically convergent maximum conditional likelihood iterative latent med applications 
mixture models described earlier limited span full spectrum latent models latent bayesian networks hidden markov models forth 
additional flexibility required mixture model accommodate called structured models 
structured model depicted differs flat mixture model latent variables simple parent observable variables 
latent variables observables may dependencies markov structure forth 
mathematical point view flat mixture model previously considered parameters independent element summation log sum 
latent model situations particularly dealing structured graphical model elements mixture tied parameter models 
graph structured mixture model hmm chapter motivating complicated class mixtures exponential family goes equation 
done noting hidden markov model described mixture model framework previously encountered subsequently proposing appropriate structured mixture alternatively call mixture mixtures 
jensen reverse jensen inequalities explicated case effectively map latent model back standard exponential family form 
go details computation reverse jensen inequality hidden markov model resolve important issues efficiency dynamic programming efficient algorithms 
show illustrations cem algorithm applied hmm 
results cem hmms elaborated chapter 
discuss case latent bayesian networks general 
interesting chapter 
structured mixture models academic exercise may skipped reader show summation data set log mixture models mapped single structured mixture model 
hidden markov models consider case dealing hidden markov model 
model latent bayesian networks seen mixture exponential families 
important result tree structured dependencies variables exponential family form aggregate exponential family distribution :10.1.1.52.696
extend jensen bound reverse jensen bounds bayesian networks directed acyclic graphs dags general tree structure 
remains crucial map probability distribution hmm latent bayesian network mixture naturally parameterized exponential family distributions fully take advantage property apply reverse jensen bounds just crucial natural parameterizations clearly see bounds flat mixtures previous chapter 
hidden markov model doubly stochastic automaton interesting mixture exponential family distributions important client model reverse jensen bounds proposed discriminative learning 
hmm typically involves sequence output vectors living dimensional space 
state markov model state vectors identify state model 
generally described pdf assume know states hidden js pi js gamma js choices component distributions transition distributions js gamma emission distributions js multinomials gaussians member exponential family 
selecting multinomial model js gamma gives stochastic finite state automaton hmm 
selecting gaussian model js gamma generates linear dynamic system lds kalman filter 
output emission js distribution chosen gaussian expect continuous vector outputs automaton 
output distribution multinomial automaton generates discrete symbols 
way long component distributions exponential family product exponential distributions remains exponential family 
deal hidden markov models observed latent hidden variable marginalization involved 
sum possible settings total configurations 
summation need inefficient computed recursion tree algorithms 
represented follows delta delta delta st likelihood data hidden markov model 
known em jensen generate lower bound probability distribution simple non latent family form 
perform maximum conditional likelihood discrimination need hmm taken generally hidden state machine take noisy measurements evolving markovian dynamics 
linear dynamical system lds treated similar way 
chapter 
structured mixture models upper bound log likelihood 
shall expanding delta delta delta st js pi js gamma js point specific sake clarity define actual distributions hmm 
development doesn cause loss generality 
shall assume dealing hmm gaussian emissions means gaussian estimated covariances locked identity multinomial models describe transition matrix hidden state hidden state kalman filter require gaussian transition probabilities 
state transition probability typically multinomial transition matrix 
state transition expressed standard form natural exponential family form js gamma pi ff exp phi gamma jm psi multinomial exponential family form compute reverse jensen bound 
vector length gamma zero contains th index value current state current state zeros 
jm related ff multinomial model simple transformation 
function function correspond standard ones multinomial table 
specific write transition probabilities follows jm values indexed appropriate gamma state labels respectively js gamma exp phi gamma gamma gamma psi gaussian exponential family form straightforward gives emission probability js exp phi gamma psi exp phi gamma psi simplicity assume prior initial states fixed equal states point write hmm likelihood mixture exponential family distributions follows delta delta delta st exp gamma gamma gamma gamma mixture cast form introduced chapter equation elements mixture exponential family model theta mixture approximately components models gaussian mean parameters multinomial transition parameters jm 
need mixture model form involves summing family distributions models replicated chapter 
structured mixture models data varies 
words consider form indexed data vectors sum xj theta ff mn exp am xmn mn theta gamma km theta equation appear strange mixture model 
latent variables indexes longer simple intuitive statistical meaning latencies equation carried 
mathematical form important generalization flexibility describe probability distribution hmm latent bayesian networks notation equation 
reuse exponential family model theta different components mixture varying data interacts xmn 
xmn various vectors cardinality theta explicitly computed original observations say index variables example may think result arbitrary function operates xmn 
form called mixture mixtures 
clear setting mixture model identical original equation chapter 
go details form derive parameters corresponding jensen reverse jensen inequalities 
subsequently explicitly put hmm form equation show parameters obtained tractably efficiently advantage independency structure hmm directed graphical model 
mixture mixtures motivated double mixture mixture mixtures 
see elaborate mixture bounded form upper lower bounds flat mixture model chapter 
compute analogous parameters bounds derived previously obtain similar terms wm ym jensen bounds wm ym reverse jensen bounds 
actual computation bounds slightly different shown chapter natural generalization previous formulas exactly mixture mixtures setting 
log space consider bounds elaborate log sum log xj theta log ff mn exp am xmn mn theta gamma km theta jensen bounds applying jensen log mixture mixtures log xj theta lower bounds function form chapter different parameter definitions log ff mn exp am xmn mn theta gamma km theta wm ym theta gamma km theta right hand term familiar theta theta function derived em algorithm 
bound static variational lower bound original function tangential contact specified current configuration theta 
theta chapter 
structured mixture models find settings parameters bound wm ym 
scalar wm positive scalar weights data ym virtual data vectors translated mixtures original data points log xj theta gamma wm ym theta gamma km theta ym gamma wm hmn km theta theta fi fi fi fi theta gamma xmn km theta theta fi fi fi fi theta hmn hmn xmn wm hmn convenience uses hmn referred responsibilities 
positive scalars defined follows hmn ff mn exp am xmn mn theta gamma km theta ff mn exp am xmn mn theta gamma km theta obtain parameters ym making sure bound equal original function xj theta theta gradients equal 
formula wm naturally results jensen inequality subsequently 
clear local calculations obtain global lower bound log xj theta 
reverse jensen bounds wish upper bound log sum reverse jensen log ff mn exp am xmn mn theta gamma km theta gammaw am theta gamma km theta case parameters log xj theta wm am ym theta gamma km theta ym wm hmn km theta theta fi fi fi fi theta gamma xmn km theta theta fi fi fi fi theta minw hmn theta theta fi fi fi fi theta gamma xmn theta theta fi fi fi fi theta theta theta wm mn zmn max mn zmn max mn zmn theta gamma xmn gamma theta function fl equation 
bound effectively re weights wm translates incomplete data obtain complete data 
wm positive weights pick smallest wm satisfy simple conditions 
set wm larger conditions require smaller wm values mean tighter bound 
furthermore chapter 
structured mixture models increasing values terms max mn zmn increasing mn zmn yield guaranteed conservative looser bounds 
may necessary terms complicated compute exactly upper bounds may efficient estimate 
condition requires generate valid ym lives gradient space functions typical family constraint local computations log sum values gradients hessians current theta compute global upper bounds 
recognize recipes ym quite similar jensen bound solved different wm values generate dual inequality 
similarity due fact bounds variational tangential contact theta 
values gradients equal original function log xj theta theta natural coupling linear parameters ym clear local calculations obtain global upper bound log xj theta 
fact reverse jensen inequality computational effort tightness hinges ability compute wm summations transformed data 
ym closed form formula scalar parameter trivial usually irrelevant 
simple formula ym require summations data straightforward manipulation definition ym reverse jensen inequality definition ym traditional jensen inequality ym wm wm theta gamma wm wm ym definition quickly isolate requirements positive scalar guarantees virtual data ym remains gradient space cumulant generating function 
example case gaussian mean constraints gradient space theta spans gradients 
case multinomial cumulant generating function theta log exp 
ym vector elements restricted sum elements ym restricted range 
appendix contains tighter formulation 
fl function upper bound tighter reverse jensen solution wm involves numerical table lookups detailed derivation reverse jensen inequality chapter appendix 
furthermore practice omit multiplicative definition wm scales fl function obtain reasonable bounds analytically guarantee 
employ simpler looser version bounds described follows wm max xmn gamma theta theta gamma xmn gamma theta reverse jensen inequality looser bound derived thoroughly chapter 
simpler bound sloppy curvature check 
curvature constraints easy ways construct bounds may conservative generate loose bounds 
best avoid curvature checking bound derivations defer absolutely necessary 
outline heuristics avoiding full computation wm may crucial dealing latent models example structured permit easy computation sum max inner products mn zmn gaussian case bounds tighter letting wm full matrix ignoring contribution chapter 
structured mixture models simplifying heuristics heuristics avoid extra computations involved obtaining wm parameters reverse jensen inequality 
way avoid maximization data may cumbersome hidden markov models max exponential number data configurations 
example simple tighter bound longer globally guaranteed locally guaranteed wm hmn xmn gamma theta theta gamma xmn gamma theta average data needed longer maximization 
implement hmm forward backward type algorithm having solve maximization problem max mn zmn chapter derive max summation hmm case 
additional possible simplification avoid computing wm parameter altogether merely setting wm traditional jensen inequality wm wm assumes upper bound width shape modulo flip translation corresponding lower bound may reasonable assumption 
evidently lazily requires extra computation usual jensen inequality 
iterations acceptable iterative algorithm optimization help accelerate convergence early stages 
wise eventually switch guaranteed bounds involving function avoid divergence 
visualization visualization merely show log sum random scalar values data points single gaussian model single poisson distribution single exponential model 
figures depict jensen lower bound reverse jensen original distribution 
visualization purposes chosen represent bounds product space gaussian logarithmic space 
jensen dashed cyan reverse jensen solid blue bounds original mixture mixtures distribution thick red dots 
gaussian model shown mixture data points product scale opposed logarithmic scale 
chapter 
structured mixture models jensen dashed cyan reverse jensen solid blue bounds original mixture mixtures distribution thick red dots 
poisson model shown mixture data points logarithmic scale 
jensen dashed cyan reverse jensen solid blue bounds original mixture mixtures distribution thick red dots 
exponential distribution shown mixture data points logarithmic scale 
reverse jensen inequality hidden markov models point resume development hidden markov models explicitly cast desired mixture mixtures form equation 
applying jensen inequality hmms straightforward standard em baum welch literature go details reverse jensen inequality case :10.1.1.131.2084:10.1.1.131.2084:10.1.1.114.4996
recall probability density function hmm delta delta delta st exp gamma gamma gamma gamma shall introduce indicator functions clarify notation 
consider case represented delta function unity zero ffi 
allows simplify delta delta delta st exp ffi ffi gamma ffi ffi gamma ffi gamma gamma ffi gamma jm summing exp functions exponential family member 
family member consists inner product aggregate data vector chapter 
structured mixture models aggregate type partition function 
parameters hidden markov model multinomial parameters gaussian emission parameters 
aggregate parameter vector theta seen parameter vectors spliced 
expressed delta delta delta st exp gamma theta gamma theta delta compute reverse jensen bound structure efficient explicitly enumerating terms sum cause 
compute reverse bounds suffices show compute wm parameters 
ym parameters reverse bound result intractable computation wm assume obtaining usual jensen bounds feasible giving wm ym solved wm reverse jensen case obtaining equivalent ym efficient formula ym wm wm theta gamma wm wm ym obtaining corresponding scalar parameter reverse jensen case trivial 
crucial parameters compute wm parameters determine width bounds gaussian multinomial models 
need solve scalars 
distinguish multinomial width parameters gaussian wm index multinomial models wm index gaussian models 
show estimate computationally critical wm gaussian parameters hmm corresponding multinomial parameters hmm 
bounding hmm gaussians consider component theta vector individually 
consider th gaussian emission component dot exponential exp ffi gamma ms vector previous reverse jensen bound definition nomenclature corresponds gamma ffi gamma ffi gaussian case quickly simplifies ffi gamma formula ym straightforward derive simply starting definitions jensen reverse jensen bounds 
chapter 
structured mixture models compute wm multinomial correspond necessary reverse jensen bounds need efficiently evaluate expressions upper bounds ms max ms recognize responsibility terms probability posteriors latent state paths previous model setting theta observations fxg xt written theta 
expand required inner products computations ms ffi gamma theta ffi gamma terms reverse jensen bound need max possible inner products 
need compute maximum path quantity max ms max ffi gamma theta ffi gamma expanding obtain max ms max ffi ffi gamma gamma discrete optimization required probably require integer programming solve exactly probably variant knapsack problem 
find upper bound desired quantity simple quadratic program 
simply introduce scalar variables ffi time steps 
bounded super set binary variables :10.1.1.37.8662
maximizing larger space variables yield true max ms upper bound 
replace optimization max ms max gamma gamma quadratic program variables inequality constraints solved straightforward manner subject box constraints :10.1.1.37.8662
evident maximum usually lies outside convex hull constraints typically cause rail extremes optimization degenerate situations 
get conservative value maximum data magnitude guaranteed upper bound 
chapter 
structured mixture models detail computation component wm bound parameter expected magnitude max ms ffi gamma theta ffi gamma expanding obtain ms ffi ffi gamma gamma theta gamma gamma introduced marginal distribution theta state time state time hmm observations accounted 
marginal distribution state sequence sub string straightforward compute efficiently hmm tree structured graphical model permits efficient computation marginal distributions baum welch forward backward algorithm 
case probability merely theta equal traditional normalized ff fi product familiar forward backward algorithm :10.1.1.131.2084
completeness show compute marginal distribution assume loss generality quantities easy compute forward backward algorithm 
marginal single state ff fi similarly direct formula marginal pair subsequent states rabiner called scaling factor ff js js fi 
bayes rule gives conditionals pairs subsequent states js fxg 
obtain way marginal multiplying pairwise marginal conditional js fxg 
obtain merely sum marginal reasonably efficient process iterated reach require intractable computation 
computation implemented efficiently approximately operations need consider bivariate probability distributions order theta pair time points trellis length factor slower computation forward backward algorithm regular hidden markov models typically tm computation tractable particularly short trellis lengths 
computations outlined readily give components legitimate upper bounds wm bound parameter efficiently doing intractable calculations 
require simple quadratic program results em baum welch computations 
move bounding multinomial parameters 
chapter 
structured mixture models bounding hmm multinomials consider th multinomial component dot inside exponential function follows exp ffi gamma gamma ms vector previous reverse jensen bound nomenclature corresponds ms gamma ffi gamma gamma ffi gamma ffi gamma gamma gamma compute correspond necessary reverse jensen bounds need efficiently evaluate ms ms max ms ms expand inner products follows ms ms ffi gamma ffi gamma gamma gamma gamma data correspond choice multinomial models rewrite follows ffi notational convenience define gamma gamma desired inner product term ms ms ffi gamma ffi gamma ffi ffi ffi gamma ffi gamma ffi ffi obtain desired wm reverse jensen bound parameter multinomials need compute terms inner products 
expected term sum inner chapter 
structured mixture models products weighted probabilities max type term 
max term elucidated max ms ms max ffi gamma ffi gamma ffi ffi integer programming solution may possible maximization solve upper bound produce guaranteed bound reformulating follows 
define scalars follows rq ffi gamma ffi gamma ffi ffi clear scalars positive rq gives constraints 
furthermore constraint variables rq gamma solve upper bound desired quantity simple linear program surrogate variables rq allowing surrogate variables vary continuous parameters constrained range solving flexible maximization original integer programming problem state paths 
optimizing yield conservative upper bound quantity interest produces legitimate bound 
solution solve simple linear form corresponding inequality constraints imposed variables max ms ms max rq turn attention computation expected inner product versus maximum ms ms ffi gamma ffi gamma ffi ffi ffi gamma ffi gamma ffi ffi gamma gamma theta simply marginal distribution state sub string gamma gamma theta readily obtainable tree structure chain structure hmm model intractable computation 
clique size considered variables leads standard em setting appears computing expected data magnitude opposed expected data vectors em requires squared clique sizes 
specifically implementation requires operations need consider variable probability distributions gamma gamma order chapter 
structured mixture models pair time points trellis length slower computation forward backward algorithm regular hidden markov model typically tm computation tractable 
computations give wm wm parameters gaussian multinomial components reverse jensen bound resorting intractable computation 
computations remain efficient appeals linear programming quadratic programming forward backward types dynamic programming 
conditional hidden markov models jensen reverse jensen bounds hidden markov models feasible cem algorithm perform conditional likelihood maximization straightforward way 
section describe scenario necessary optimize conditional likelihood quantity 
situations conditional expression arise due problem formulation result negated log sum type expression 
negated log sums conditional likelihood expressions occur example learning classifier multiple competing hidden markov models learning mixture experts regression function gates hidden markov models 
section focus develop traditionally called input output hidden markov model objective regress certain components time series observable :10.1.1.133.6544
inputs outputs coupled hidden state evolves markov dynamics 
standard hidden markov model portrayed previous sections assume time sequence vectors 
input output hidden markov model split emission vectors components treated input output respectively 
training sequence yt vector pairs time estimate hidden markov model test data reliably predict sequence sequence 
example application learn mapping various passive biological measurements heart rate temperature motion energy complicate output measurement blood glucose easily measurable 
shall assume dealing hmm total underlying states gives rise theta transition matrix 
transition matrix equivalently described multinomial distributions dimensionality equal assume emission model jointly gaussian diagonal covariance matrix 
necessary estimate gaussian covariance terms gaussian mean terms theta state transition matrix 
wish regress natural form conditional distribution yjx natural objective function conditional log likelihood training data log gamma log view term expression log likelihood hidden markov model training data 
second negated term likelihood hidden markov model marginalized applies input data graphical model scenario recall expressions latent quantities involve summation possible paths state space hidden markov model 
words expand chapter 
structured mixture models log gamma log conditional hmm estimation 
effectively maximize joint log likelihood hmm inputs outputs minus marginal log likelihood hmm summed obtain marginal input 
conditional likelihood quantity follows 
log gamma log lower bound left hand term jensen inequality reverse jensen inequality applied right hand term log 
gives lower bound conditional log likelihood maximized closed form 
iterate bounding maximization steps converge local maximum conditional likelihood 
perform regression merely novel data estimate probability distribution hidden states compute estimated output sequence simply alpha beta values forward backward algorithm weight gaussian emission model appropriately time step 
experiments compare estimates conditional likelihood criterion cem usual maximum likelihood em framework standard meteorological time series prediction task 
small toy data set constitutes monthly measurements precipitation temperature water flow san francisco area 
dimensional time series concatenate month represented sinusoidal values range 
input output hmm wish build regress precipitation value remaining inputs 
training performed samples time series testing performed remaining samples 
depicts snapshot training data dimensional time series weather measurements 
trained em cem input output hidden markov model states assuming diagonal covariance gaussian emission models 
gaussians dimensions inputs precipitation level output 
table summarizes resulting log likelihoods rms error em cem algorithms 
particular example cem algorithm slower requiring orders magnitude time attain convergence em algorithm 
due extra computations top usual forward backward algorithm extra looseness reverse jensen bounds require iterations 
resulting testing chapter 
structured mixture models month training data san francisco data set 
dimensional time series depicts monthly levels precipitation water flow temperature month year sinusoid 
performance notable conditional likelihood testing rms error testing shows cem performed favorably compared em prediction 
appropriate quantitative measure performance conditional likelihood testing data best choice task predict precipitation values inputs 
inappropriate penalize reward estimates input components 
joint likelihood test data inappropriate measurement quality estimate 
desired outputs need properly predicted conditional likelihood score rms error depict better regression estimate obtained cem algorithm 
training log likelihood conditional log likelihood em cem testing log likelihood conditional log likelihood rms error em cem table cem em performance hmm precipitation prediction 
shows qualitative performance difference em hidden markov model cem hidden markov model 
note cem version tracks precipitation values testing closely em accurate range variation conservative near constant precipitation estimates em generates 
chapter ambitious application input output hmm regression attempted 
due extremely large size data long trellis length simplifying approximations needed reduce computation time cem algorithm operates roughly time em algorithm 
discriminative hidden markov models section give details implementation med hidden markov models discriminative regression setting 
standard regression conditional regression chapter 
structured mixture models month test predictions san francisco data set 
true precipitation values depicted dashed cyan line filled dots 
em hidden markov model predictions outlined dashed red line connecting points 
cem hidden markov model predictions outlined solid blue line connecting hollow circles 
note cem predictions track true precipitation values closely em predictions span desired range variation align desired output 
previous input output hmm discriminative epsilon tube insensitivity linear loss function form regression function 
implementation classification setting straightforward regression development 
classification applications speech recognition bioinformatics portray discriminative regression hmm novel way incorporate time invariance regression settings 
recall section developed med regression applications discussed generative latent generative models 
consider regression discriminant function theta log sj sj gamma numerator denominator hidden markov models define probability density datum sequence observations single vector 
recall regression constraints arise med framework theta fl gamma theta fl theta fl fl gamma theta expectations discriminant functions intractable employ variational bounds jensen reverse jensen 
constraint need upper bound theta second constraint need lower bound theta 
result creation restrictive constraints generate convex hull contained convex hull original med problem 
set constraints numerator jensen bound denominator 
situation reversed set constraints 
variational bounds med solution distribution theta factorize positive negative hmm parameters emissions transitions prior theta conjugate factorized 
assume invoked inequalities theta operating point constraints theta fl log jm gamma log jm gamma gamma fl chapter 
structured mixture models theta fl fl gamma log jm log jm gamma gamma jensen reverse jensen training sequence data obtain parameters bounds hmm model positive negative 
indexes gaussian emission models multinomial transition matrix models hmm respectively 
compute bounds convert multinomial natural parameterization jensen reverse jensen computations back form section med computations 
gaussian means natural parameterization 
constraints give rise partition function margin exponential prior bias gaussian prior components simple linear med regression fl gamma novel computation involves solving gamma components components explored previous regression problems 
closed form partition function needed estimate optimal setting lagrange multipliers 
models gamma contains gaussian emission parameters multinomial parameters positive numerator hmm 
assume white gaussian priors emission parameters uniform dirichlet priors ff ff notation section multinomials 
suffices show compute partition function component hmms say log jm gamma log jm partition function component broken follows linear theta pi gauss theta pi multi simple log linear component linear gamma gaussian partition function components logarithmic form gamma gamma log usual gaussian family model log gauss log tn gamma tn gamma dn log gauss tn tn tn tn gamma log tn tn tn tn tn tn tn tn tn tn tn tn consider contribution multinomial components 
convert natural parameterization data gamma dimensional vectors tq standard multinomial form dimensional vectors tq done concatenating extra element chapter 
structured mixture models vector sums unity 
integrate obtain partition function reminiscent derivation section superscript index dimensionality tq vectors multi pi gamma ff tq tq tq tq gamma ff tq tq tq tq tq gamma tq pi gamma tq tq gamma tq pi gamma tq computing partition function negative hmm merely jensen reverse jensen parameters permute roles negative logarithm aggregate partition function maximized 
redundancies computations computable efficiently single lagrange multiplier modified time 
permits fast axis parallel implementation 
sufficiently optimized lagrange multipliers current setting compute theta 
obtain theta setting merely find maximum theta 
gaussian models maximum merely mean involves simple update rule negative model parameters updated similarly bounds role swapped tn tn tn tn tn tn th multinomial natural parameters permit simple update rule involving gradients cumulant generating function log exp 
rule merely finds max med solution distribution family form denote vector ones size tq ff tq tq tq tq tq tq max re convert natural representation back usual multinomial parameterization ae section 
iterate updating theta contact point recomputing bounds estimating lagrange multipliers 
continued convergence heuristic stopping criterion 
utilizing med solution theta prediction intractable training performed compute expectations hmm 
run time merely maximum theta theta gives fixed hmm models 
log ratio likelihoods bias scalar parameter compute regression obtain approximate output theta theta theta theta form discriminative regression model inherits dynamic time warping properties hmm optimizing epsilon tube insensitive scalar prediction 
expressions trivially re applied create discriminative hmms classification parameters hmm estimated obtain large margin decision boundary generative models standard maximum likelihood setting 
chapter 
structured mixture models latent bayesian networks shall discuss case bayesian networks general hidden markov models specific example 
hidden markov model chain dependency structure maintains important computational properties useful computing jensen reverse jensen bounds efficiently 
known bayesian networks tree structures take advantage efficient algorithms avoid may possible estimate discriminatively jensen reverse jensen bounds 
recall tree structured dependencies variables exponential family form aggregate exponential family distribution :10.1.1.52.696
latent hidden variables bayes net graph tree structures need summed 
log likelihood latent bayes net represented mixture exponential family distributions 
written mixture mixtures form equation 
subsequently logarithm log likelihood latent bayes net upper lower bounded jensen reverse jensen 
compute bounds merely unfolding latencies mixture mixtures may highly inefficient 
computations wm intractable require enumerating latent configurations bayesian networks 
just demonstrated hidden markov model efficient algorithms avoid exponentially large explicit mixture model need efficient algorithm compute reverse jensen bounds general tree structured bayes nets advantage conditional independency properties graphical model 
naive computation jensen reverse jensen bounds require intractable amounts latent bayesian networks 
possible compute lower bounds efficiently advantage independency structure networks done regular jensen inequality parameters em called junction tree algorithm :10.1.1.114.4996
fact forward backward algorithm modified reverse jensen bound special case general junction tree algorithm 
methods take advantage structure graph efficiently compute expectations 
solving reverse jensen upper bound parameters need efficient procedures take advantage graph structure 
just shown hmm general case latent tree structured bayesian networks may similar efficient reverse jensen algorithms direction definitely merits investigation 
data set bounds far considered bounds lower upper log likelihood single data point single sequence hmm case 
case want bound likelihood data set points 
case reverse jensen inequality applied straightforward way 
main benefit reverse jensen inequality applied form bound applied separately data point may generate efficient appropriately shaped bound 
consider augmentation bounding log sum 
obtain aggregate data set conditional likelihood sum likelihood data point follows log theta gamma log theta conditional log likelihood seen joint log likelihood minus chapter 
structured mixture models marginal log likelihood gamma joint log likelihood log theta negated marginal log likelihood log theta want lower bounds conditional likelihood need lower bound joint log likelihood upper bound marginal log likelihood gets negated 
write marginal log likelihood compactly follows obscures fact latent log theta adding constant term marginal likelihood effect shape bound 
spirit incremental likelihood derivation em bishop 
additive constant vary fixed previous theta parameter values point tangential contact effect shape bounds deltal log theta theta shall reverse em pull summation data logarithm operator 
generate upper bound incremental marginal log likelihood log theta theta log fl theta fi fl theta fi thing note bound fl terms scaled arbitrary amount change shape bound 
need pick fl fi insure guaranteed upper bound 
obvious shortly 
consider right hand side manipulate follows log fl theta fi fl theta fi log fl theta fi theta theta fi theta fi fl theta fi log fl theta fi fl theta fi log fl theta fi fl theta fi theta fi theta fi clear fl positive terms parentheses positive sum unity sum index 
due concavity log function apply chapter 
structured mixture models jensen inequality log fl theta fi fl theta fi fl theta fi fl theta fi log theta fi theta fi log fl theta fi fl theta fi fl theta fi fl theta fi fi log theta theta inspection see equation similar equation left right hand sides flipped 
identical need guarantee term multiplying logarithm right hand side equation unity 
need fl theta fi fl theta fi fi simplified recall fl invariant global scale factor fl fi theta fi pick scale factor arbitrarily chosen numerical precision reasons avoid underflow overflow computationally 
possible choice normalize fl follows fl fi theta fi fi theta fi note fi greater equal unity 
fl theta fi fl theta fi fi clearly terms parentheses multiply fi equal unity conclude fi 
evident guaranteed bound originally proposed equation freedom selecting fi long greater unity 
shall exploit freedom find tightest bound possible 
point bound deltal log fl theta fi fl theta fi deltal log fi theta fi theta fi fi bound holds distribution fi greater unity 
noted fi sum inversely unity fi fl theta fi fl theta fi fi fl theta fi fl theta fi chapter 
structured mixture models write bound follows deltal log fi theta fi theta fi fi deltal log fi theta theta fi expanding include latent values obtain deltal log fi mc theta mc theta fi expression undesirable power operation 
fi applies mixture summation 
consider bounding terms simplify 
invoke jensen power operation pow fi fi convex function fi 
permits upper bound expressions involving power operation generates upper bound deltal manipulating mixture terms taken power applying jensen obtain mc theta mc theta fi mc theta nd theta fi mc theta mc theta fi mc theta nd theta theta theta fi mc theta mc theta fi mc theta nd theta theta theta fi clear left hand side right hand side equal old value theta ensuring generated variational bound tangential contact appropriately 
plug upper bounds individual terms indexed get stricter upper bound deltal deltal log fi mc theta mc theta fi deltal log fi mc theta nd theta theta theta fi holds arbitrary distribution long fi greater inverses sum unity 
specifically assume theta exponential family write compactly follows theta ff mc exp gamma amc theta mc gamma theta mc delta case bound written succinctly deltal log imc fi theta gammafi nd theta ff fi mc exp gamma fi amc fi theta mc gamma fi theta mc delta chapter 
structured mixture models bound variational contact old theta configuration space pdfs 
invoke reverse jensen inequality log sum single time obtain bound data set 
freedom adjusting fi long greater unity inverses sum unity ensure bound tight possible select fi minimize resulting wm values 
applications shown toy illustrations conditional hidden markov models cem 
clearly important applications reach interesting investigate performance real world data set 
chapter describes behavior imitation problem hidden markov model learn agent interacts outside world 
basically stimulus response type model trained real data observing human activity hours 
wish predict agent behavior conditioned outside world measurements train hmm conditional likelihood criterion 
imitation learning application suited cem algorithm show real world results cem em describing implementation details experiment 
chapter chapter derivation reverse jensen inequality far put forth proof 
mathematical reader may find derivation interesting applied reader may wish skip directly chapter see real world application latent discriminative learning structured graphical models 
chapter reverse jensen inequality difficulties strengthen mind labor body 
chapter shall give terse derivation reverse jensen inequality chapter 
derivation generate bounds tight possible involves details 
shown full permit extensions 
review literature contemporary mathematical inequalities community show current converses reversals jensen 
suit requirements discriminative learning functional form bounds easy combine regular jensen em type bounds derive reversal 
background inequalities reversals celebrated jensen inequality formalized century ago roots trace back history 
result heart inequalities statistics mathematics 
example holder inequality easily derived jensen inequality 
jensen simplest form states convex function expectation upper bounded expectation convex function 
excellent review jensen inequality inequalities convex functions various statistical applications text 
furthermore converses reversals jensen inequality provided text 
simple manipulations jensen inequality assuming mixture weighted negative scalars positives ones 
refinements require assumptions properties convex function elements operates 
similarly log sum inequality variants jensen discussed usually just trivial consequences jensen inequality 
reversal jensen important statistics proposed 
instance jensen inequality reversed function applied limited range variation elements applies clamped limited domain 
bound proposed fundamentally curvature prevent tight bounds 
uses find general dual sided bounds csiszar phi divergences chapter 
reverse jensen inequality terms divergences family 
permit map divergences kullback leibler divergence harmonic divergence variational divergence renyi entropy unfortunately bounds literature directly suit purposes 
generate reversals homogeneous regular jensen inequality bounds combined easily em type bounds put requirements elements expectation appropriate 
elements log sum problems exponential family probability densities special properties necessarily ones explored far literature 
start blank slate derive customized reversal 
derivation reverse jensen inequality recalling inequality proof previous chapter log mn ff mn exp am xmn mn theta gamma km theta gammaw am ym theta gamma km theta log xj theta wm am ym theta gamma km theta ym wm hmn km theta theta fi fi fi theta gamma xmn km theta theta fi fi fi theta minw hmn theta theta fi fi fi theta gamma xmn theta theta fi fi fi theta theta theta wm mn zmn maxn mn zmn maxn mn zmn zmn theta gamma xmn gamma theta hmn ff mn exp am xmn mn theta gammak theta mn ff mn exp am xmn mn theta gammak theta fl fl log log gamma fl log fl fl gamma log fl fl derive reverse jensen bound called double mixture case 
derivation subsumes reverse jensen bound single mixture case 
derivation follow somewhat general recipe bounding quantities log sums 
begins making variational bound touch original function tangentially current operating point 
find mapping non linear functions quadratic space computations tractable 
bound simplified noting convexity properties 
subsequently note log partition function gibbs distribution arises log sum exponentiated linear terms needs bounded quadratic 
derivation done dimensional simple case generalized sweeping bound multidimensional case 
subsequently curvature constraints checked bound simpler deal subsume original inequality 
ultimately simple formula bound parameters arises guarantees remains original log sum function 
start log sum needs upper bounded log mn ff mn exp am xmn mn theta gamma km theta gammaw ym theta gamma km theta chapter 
reverse jensen inequality current tangential contact point theta variational bound sides equal log mn ff mn exp am xmn mn theta gamma km theta gammaw ym theta gamma km theta allows isolate log ff mn exp am xmn mn theta gamma km theta wm ym theta gamma km theta contact point theta gradients sides inequality equal allowing isolate ym hmn xmn gamma km theta theta fi fi fi fi theta gammaw ym gamma km theta theta fi fi fi fi theta reinsert definitions ym inequality rearrange terms obtain expression 
note variables remain computed wm scalar values 
wm theta gammak theta gamma theta gamma theta theta log mn ff mn exp am xmn mn theta gammak theta mn ff mn exp am xmn mn theta gammak theta mn hmn theta gamma theta theta gammax mn interesting note terms parentheses multiplying wm scalar values bregman distances bregman divergences 
direct result convexity cumulant generating functions theta exponential family 
case gaussian distributions bregman distances euclidean distance metric origin theta general case exponential family distributions bregman distances kullback leibler divergences distribution theta origin theta 
clear left hand side zero contact point bregman distances positive origin 
encouraging indicates able obtain non vacuous bounds finite wm values due positivity bregman divergences 
point realize difficult try solve wm arbitrary functions 
shall show avoid dealing explicitly functions map cases functions quadratic form 
mapping brevity defining shorthand bregman distances arose earlier fm theta theta gamma theta gamma theta gamma theta theta functions convex minimum zero theta replace functions simplify expressions obtaining wmf theta log exp dmn theta gamma theta zmn gamma theta exp dmn theta zmn gamma theta gamma hmn theta gamma theta zmn chapter 
reverse jensen inequality defined constant scalars dmn constant vectors zmn follows dmn log ff mn am xmn gamma km theta theta xmn zmn xmn gamma theta immediately recognize constants closely related responsibilities follows hmn exp dmn mn exp dmn permits simplification wmf theta log mn hmn exp theta gamma theta zmn gamma theta gamma mn hmn theta gamma theta zmn straightforward redefinitions simplified expression 
hidden possibly complicated non linearities theta space due functions preventing easy solution wm situation alleviated transform space 
define mapping theta space phi space non linear functions simple fm theta gm phi phi gamma theta phi gamma theta mapping convex non negative function convex non negative function possible stretching axes change variable 
geometrically mapping bowl bowl 
maintain minimum theta 
origin consider remain fixed theta stretch domain perform change variables 
convex function theta restricted range consider convex hull restriction domain quadratic obey range 
transformation left hand side equation produce wmg phi log mn hmn exp theta gamma theta zmn gamma theta gamma mn hmn theta gamma theta zmn portrays mapping case 
essentially performing map 
arbitrary strictly convex function turned quadratic 
quadratic arises default theta functions dealing exponential family members log sum gaussians variable means fixed covariance 
transform family member gaussian purposes computing wm parameters reverse jensen inequality 
transform rest expression theta quadratic domain phi point important property associated convexity preserving map 
important condition family shall identify important condition concerning mapping quadratic put forth proof 
show holds wide variety exponential family models example gaussian mean gaussian covariance multinomial gamma poisson exponential distributions 
quadratic unrestricted range superset 
chapter 
reverse jensen inequality general convex bowl theta mapping theta phi quadratic bowl phi convexity preserving map 
convex bowl bregman divergence theta space find displacement map theta phi stretch axes obtain quadratic euclidean distance bowl phi space 
change variables exists permits simpler solution quadratic space 
lemma mild conditions standard exponential family models find change variable mapping variable theta phi equating bregman distance theta arising cumulant generating function theta quadratic function phi minimum theta phi gamma theta phi gamma theta theta gamma theta gamma theta gamma theta theta select arbitrary point theta domain theta 
possible mappings induced mapping theta phi upper bound holds globally phi gamma theta theta gamma theta gamma theta theta gamma theta theta gamma theta gamma theta gamma theta gamma theta gamma theta theta holds choice arbitrary point theta domain theta theta point domain 
furthermore upper bound tangential contact theta theta theta point domain 
property lemma guaranteed hold contact point theta theta sides inequality go zero 
furthermore easy show right hand side eventually diverge negatively move away contact point theta right hand side concave left hand side increasing away contact point 
bound trivial guarantee distant values theta 
exact proof elusive property arise convexity specific attributes 
instance cumulant generating function logarithm laplace transform route may construct argument 
alternatively specific family assumptions steepness may useful 
deriving formal proof condition valid give visual examples holding standard distributions exponential family 
unidimensional case mapping unique modulo mirror flip origin easily compute explicit mapping phi follows phi theta sign theta gamma theta theta gamma theta gamma theta gamma theta theta chapter 
reverse jensen inequality details cumulant generating functions duals distributions refer table previous chapter 
figures show examples bounds implied lemma 
argued convexity preserving map powerful bound associated 
property formula wm drastically simplify 
lemma bound gaussian mean distribution 
random configurations theta theta shown 
solid line lemma upper bound dashed red line 
gaussian mean case trivial consider trivial construct mapping theta phi affine 
gaussians quadratic function regardless dimensionality 
straightforward see various choices theta theta upper bound desired 
lemma bound gaussian covariance distribution 
random configurations theta theta shown 
solid line lemma upper bound dashed red line 
consider varying covariance gaussian quadratic mapping possible 
straightforward see various choices theta theta upper bound desired 
lemma bound multinomial distribution 
random configurations theta theta shown 
solid line lemma upper bound dashed red line 
chapter 
reverse jensen inequality lemma bound gamma distribution 
random configurations theta theta shown 
solid line lemma upper bound dashed red line 
lemma bound poisson distribution 
random configurations theta theta shown 
solid line lemma upper bound dashed red line 
lemma bound exponential distribution 
random configurations theta theta shown 
solid line lemma upper bound dashed red line 
chapter 
reverse jensen inequality applying mapping shall invoke lemma current equation wm progressed wmg phi log mn hmn exp theta gamma theta zmn gamma theta gamma mn hmn theta gamma theta zmn recognize theta bregman distances arising cumulant generating functions 
furthermore zmn represented theta mn gammak theta particular choice theta mn due data vectors xmn living gradient space 
terms exponentiated equation exactly right hand side bound lemma 
replace upper bounds obtain stricter guaranteed constraint wm values wmg phi log mn hmn exp phi gamma theta theta gamma zmn gamma mn hmn theta gamma theta zmn simplicity shall define surrogate whitened data vectors translated scaled versions original data xmn zmn theta gamma zmn stage done away theta terms right hand side wmg phi log mn hmn exp phi gamma theta zmn gamma mn hmn theta gamma theta zmn bound away terms depending directly theta replace phi shall invoke constraint dictated definition bounds 
invoking constraints virtual data recall bounds generate virtual data points ym constrained original xmn data points live gradient space cumulant generating function 
original definition exponential family requirement ym theta theta perfect sense family distributions generating bound log sum mixture valid common constraint data terms gradient space cumulant generating function 
furthermore ym acted function domain function admits vectors gradient space 
insert definition ym derived constraint wm hmn theta theta fi fi fi fi theta gamma xmn theta theta fi fi fi fi theta theta theta clearly need wm satisfied 
furthermore evident setting wm satisfy 
smallest wm satisfies called chapter 
reverse jensen inequality convexity gradient space value wm satisfy constraint 
practice relatively easy compute analytically exponential family distributions formula 
assume find particular gradient space vector generates theta gradient space 
gives equation hmn theta theta fi fi fi fi theta gamma xmn theta theta fi fi fi fi theta theta theta fi fi fi fi theta rewrite equation follows hmn theta gamma theta hmn theta gamma theta gamma theta hmn theta gamma xmn theta gamma theta see scaled negated mixture zmn vectors lies gradient space theta 
shall consider formula wm point equation replace wmg phi log mn hmn exp phi gamma theta zmn gamma mn mn theta gamma theta wmg phi log mn hmn exp phi gamma theta zmn theta theta gamma theta subtract fm theta sides obtaining wmg phi gamma fm theta log mn hmn exp phi gamma theta zmn theta theta gamma theta gamma fm theta terms multiplying right hand side lower bounds lemma 
invoke upper bound get stricter constraint formula wm wmg phi gamma fm theta log mn hmn exp phi gamma theta zmn theta theta gamma phi gamma theta right hand side re replace theta introduced reflect similarity lemma original definition wmg phi gamma fm theta log mn hmn exp phi gamma theta zmn gamma mn mn theta gamma phi gamma theta chapter 
reverse jensen inequality simplified definition zmn equality theta phi wm gamma phi log mn hmn exp phi gamma theta zmn gamma mn mn phi gamma theta point succeeded eliminating complexities arise nonlinear theta representation exponential family quadratic functions 
effectively computations computationally equivalent simple gaussian mean case quadratic cumulant generating function 
recognize logarithm summation exponentiated linear models 
equivalent log partition function gibbs distribution similar logistic function 
map reverse jensen problem finding quadratic upper bounds phi space log partition function gibbs distribution 
provide simple formula wm originally proposed 
bound direct curvature check tight possible 
longer involved derivation follows section section generate tighter bound 
simple loose bound solve wm effectively bounding log partition gibbs distribution quadratic function 
unusual side effect solving reverse jensen bounds 
earlier gaussian processes necessitated quadratic bounds form log partition function 
doctoral dissertation gibbs involved finding bound gaussian processes bound solved approximate provable 
shall show arrive guaranteed bound log partition function ultimately compute wm terms reverse jensen bound 
derivations simple resulting formula guaranteed 
shall employ crude method obtaining bound direct curvature check 
section refine crude manipulation obtain tighter bound 
convenience surrogate variable wm wm gamma currently equation mg phi log mn hmn exp phi gamma theta zmn gamma mn mn phi gamma theta clarity define right hand side function phi phi log mn hmn exp phi gamma theta zmn gamma mn mn phi gamma theta similarly define phi left hand side follows phi mg phi phi gamma theta phi gamma theta evident phi phi zero zero gradients contact point theta theta 
wish upper bound phi phi quadratic tangential chapter 
reverse jensen inequality contact phi phi theta 
requires finding valid scalars satisfy inequality values phi parameter phi phi working phi space simplify things change variable mere translation psi space define psi phi gamma phi words find satisfy psi psi naturally psi psi psi psi log mn hmn exp mn psi gamma mn mn psi noted psi psi zero valued minimum psi 
form tangential contact psi psi necessary variational bound 
note psi vector stacked version individual psi vectors psi psi psi psi psi trivial take second derivatives sides equation get bounds curvature 
effectively provides ordering curvature matrices indicates max mn zmn simple typically loose bound 
specifically obtain psi psi psi psi delta delta delta 
delta delta delta exp psi nm hmn exp mn psi delta delta delta 
delta delta delta exp mn psi mn nm hmn exp mn psi gamma exp psi nm hmn exp mn psi 
exp mn psi mn nm hmn exp mn psi exp psi nm hmn exp mn psi 
exp mn psi mn nm hmn exp mn psi subtractive outer product ignored obtain stricter bound 
furthermore inspection realizing convex combination outer products bounded max clearly see hmn exp mn psi mn nm hmn exp mn psi max mn chapter 
reverse jensen inequality addition trace matrix multiplied identify greater matrix ordering sense trace mn zmn mn simplifies psi psi psi psi delta delta delta 
delta delta delta max delta delta delta 
delta delta delta mn zmn giving settings individual max mn zmn unfortunately bound quite adaptive depend hmn values 
words zmn sample extremely low weight hmn means ignore 
bound depends hmn terms 
tighter bound preferable get tighter bounds done avoid quick curvature test 
bounds simple allow easy implementation 
manipulating psi conservatively obtain intermediate variational upper bounds tighter curvature check 
psi greater upper bound upper bound psi 
pull additive linear term logarithm follows psi log mn hmn exp psi zmn gamma mn hmn psi zmn define stacked version zmn vectors follows zmn zmn zmn vector dimensionality psi vector obtained padding corresponding zmn total gamma zero vectors 
naturally mn psi mn psi allows rewrite psi log mn hmn exp psi zmn gamma mn chapter 
reverse jensen inequality simplify notation define vectors umn zmn gamma mn lets rewrite psi function compactly psi log mn hmn exp gamma psi umn delta compute reverse jensen bound set scalars inequality psi psi holds values psi 
specifically need psi psi log mn hmn exp gamma psi umn delta psi selecting values holds psi vector requires considering psi configurations multidimensional euclidean space 
shall simplify multidimensional problem consider dimensional ray psi space 
bounding dimensional ray consider guaranteeing bound psi certain direction psi 
write psi vector psi fi psi psi function fi psi log mn hmn exp fi psi umn see find quadratic bound uni dimensional function 
words psi fi log mn hmn exp fi psi umn fi find scalar psi satisfies inequality fi 
subscript indicates corresponds certain choice direction psi 
give quadratic bound directional slice 
hope individual uni dimensional bounds aggregated find required psi full multi dimensional problem 
convenience define scalars mn follows vmn psi umn point problem simplified solving dimensional bound value psi psi fi log mn hmn exp mn chapter 
reverse jensen inequality evident definitions mn specifically original definition umn holds mn hmn vmn consequently fi zero left hand side right hand side zero derivatives zero tangential contact 
dimensional log gibbs partition function depicted various random configurations mn hmn lower bounds arise visualization 
types structures need upper bounded quadratic 
dimensional log gibbs partition functions 
solid line gibbs partition function lower bounded dashed red lines 
plotted fi log mn hmn exp mn random selections hmn vmn brevity fi denote right hand side equation 
fi log mn hmn exp mn working directly fi try find intermediate upper bound quantity simpler form fi log fl exp fiu fl exp gamma fl intermediate bound interesting serves fi sign fi longer important 
intermediate scalar parameters fl 
right hand term variational bound tangential contact left hand side fi 
words sides zero zero gradient fi 
find legitimate parameters fl guarantee right hand side term upper bound 
suffices find parameters guarantee curvature right hand term upper bound curvature left hand term th st order terms equal fi 
flat tangential contact fi 
need guarantee inequality logarithm extracted mn hmn exp mn fl exp fiu fl exp gamma fl second derivatives respect fi sides obtain stricter curvature test inequality mn hmn mn exp mn fl exp fiu fl exp chapter 
reverse jensen inequality set parameter follows jv mn satisfying equation satisfy stricter constraints 
cases subsume equation case subtracting positive quantity left hand side trying maintain upper bound 
cases follows ffl case fi positive know exp fiu exp mn 
defined jv mn fi positive 
impose stricter guarantee expand fl exp fiu mn hmn mn exp mn fl exp fiu mn hmn mn exp fiu fl mn hmn mn ffl case fi negative know exp exp mn 
impose stricter guarantee expand fl exp mn hmn mn exp mn fl exp mn hmn mn exp fl mn hmn mn satisfy inequality setting definition parameters fl guarantee intermediate bound valid 
intermediate upper bound log gibbs partition function simplifies possibly large number terms summation mn just parameters 
depicts bound various random cases 
jv mn fl mn mn intermediate bound log gibbs partition functions 
solid line simple intermediate upper bound guaranteed greater gibbs partition function dashed red line 
finding psi satisfies original uni dimensional bound equation merely find psi satisfies stricter intermediate bounds just computed psi fi log fl exp fiu fl exp gamma fl chapter 
reverse jensen inequality invoke simple change variables get rid parameter analysis 
define fiu solve desired psi follows psi max fl fl 
replaced expression right brevity dimensional function fl 
function plotted fl log fl exp 
fl exp gamma 
gamma fl max max fl 
unfortunately maximizing done analytically 
possibility obtain function max 
fl numerically show 
dealing max fl directly may awkward due non analytic nature analytic linear upper bounds 
example various linear upper bounds form afl max fl depicts linear bounds max fl function 
values provide numerically guaranteed upper bounds right hand side listed appendix table 
matlab code derive function relating appendix section 
merely select valid settings obtain upper bound max 
fl 
inserting intermediate bound allows obtain desired psi terms linear bound psi max fl chapter 
reverse jensen inequality linear upper bounds numerical max fl 
show various settings upper bounds afl vary settings 
psi afl psi max mn mn mn hmn mn mn psi mn hmn mn max mn mn quadratic bound log gibbs partition functions 
quadratic bound green line simple intermediate upper bound solid blue line gibbs partition function dashed red line 
expanded parameters fl terms definitions 
depicts desired resulting quadratic bounds formula psi intermediate bounds equation 
optimize possible choices psi value minimized generate tightest bound 
quadratic bound dimensional case 
step fold bound original multidimensional formulation guarantee general full dimensional bounding problem 
scaling multidimensional formulation quadratic bound guaranteed particular slice psi function psi fi psi 
rewrite unidimensional bound terms whitened data vectors zmn furthermore computational efficiency sure computation psi require simple inner products vectors 
recall definition mn psi zmn gamma ij ij ij chapter 
reverse jensen inequality replacing current bound psi get psi mn hmn mn max mn mn psi mn hmn psi zmn gamma ij ij ij max mn psi zmn gamma ij ij ij psi mn hmn psi zmn gamma ij ij psi ij max mn psi zmn gamma psi ij ij ij terms depend scaling shall merely invoke jensen inequality straightforward way quadratic expression 
get intermediate upper bound psi satisfies requirements strictly 
specifically psi zmn gamma psi ij ij ij theta psi zmn gamma psi ij ij ij theta psi zmn theta psi ij ij ij bound current psi expressions obtain stricter condition psi psi mn hmn psi zmn gamma ij ij psi ij max mn psi zmn psi ij ij ij psi mn hmn psi zmn gamma gamma ij ij psi ij max mn psi zmn easily guarantee gamma positive evident allowable values tables see middle term expression negative 
delete obtain stricter bound psi psi mn hmn psi zmn max mn psi zmn recall property definition zmn equation 
dealing normalized unit norm version psi vector psi dot product zmn padded zeroes equivalent dot truncated version zmn truncated version psi psi psi zmn psi zmn psi vector simply vector unit norm corresponding unit norm vector psi entries th index position zeroed 
allows chapter 
reverse jensen inequality rewrite psi mn hmn psi zmn max mn psi zmn inner product written magnitudes vectors times cosine angle psi zmn psi cos 
find stricter upper bound noting psi zmn psi psi mz mn zmn write condition psi guarantee upper bound psi mn hmn psi psi mn zmn max mn psi psi mn zmn definition psi give guaranteed quadratic bound original log partition function arbitrary choice direction psi psi fi log hmn exp fi psi umn fi plug definition psi quadratic expression guarantee holds fi psi mn hmn psi psi mn zmn max mn psi psi mn zmn fi log hmn exp fi psi umn fi point vary chosen values psi direction need fixed guarantee bounds 
words functions direction psi violating bound psi mn hmn psi psi mn zmn psi max mn psi psi mn zmn fi log hmn exp fi psi umn fi return original problem finding quadratic upper bound multidimensional log gibbs partition function psi psi computing valid scalars satisfy psi psi log hmn exp gamma psi umn delta psi written terms fi psi directly plug unidimensional bound derived fi psi fi psi log hmn exp fi psi umn psi fi fi psi fi psi psi mn hmn psi psi mn zmn psi max mn psi psi mn zmn fi psi fi chapter 
reverse jensen inequality divide fi fi case relevant tangential contact fi sides psi psi psi mn hmn psi psi mn zmn psi max mn psi psi mn zmn psi inspection turn aggregate bound individual bounds isolated satisfying bounds individual basis satisfy aggregate bound psi psi psi hmn psi psi mn zmn psi max psi psi mn zmn simplifying dividing psi psi yields psi mn zmn psi max mn zmn free pick arbitrary psi psi pair value 
really index am mn zmn max mn zmn smallest value satisfies tightest bound 
set obtain succinct formula am mn zmn max mn zmn expanding definition wm gamma ultimately obtain wm am mn zmn max mn zmn previous sections definition minw hm theta theta fi fi fi fi theta gamma xm theta theta fi fi fi fi theta theta theta zmn defined follows shown earlier zmn theta gamma xmn gamma theta noted increase wm terms guarantee bound provide stricter bounding 
consequently intermediate terms computation wm difficult compute upper bounded result increase wm guarantee strict global bound 
value depends nature constraints km function 
gaussian case theta arbitrary euclidean vector zero 
cases trivial compute simple algebra 
chapter 
reverse jensen inequality analytically avoiding lookup tables lookup tables mathematically unappealing provide fully analytic treatment am tradeoff 
recall achieved fully analytic bounding process point trying solve psi psi max log fl exp 
fl exp gamma 
gamma fl bounding required maximization function fl log fl exp 
fl exp gamma 
gamma fl deemed difficult exactly solve max fl handled numerically 
find upper bound max fl provide stricter condition psi naturally bound tight resulting numerical linear bounds fully analytic 
define function fl max fl hard compute analytically 
psi fl analytic expression fl difficult note interesting property fl fl fl true limit fl goes zero rule lim log fl exp 
fl exp gamma 
gamma fl fl maximum fl occurs 
fl 
done proving log fl exp 
fl exp gamma 
gamma fl fl proof inequality equivalent proving exp 
exp gamma 
gamma exp fl 
gamma fl expanding exp functions power series comparing terms yields result 
inequality show fl fl fl 
know fl fl fl 
recall fl arose trying find quadratic bound log fl exp 
fl exp gamma 
gamma fl 
psi log fl exp 
fl exp gamma 
gamma fl property noted proved help dr michael ulm fb mathematik universitaet rostock 
chapter 
reverse jensen inequality evidently fl psi fl fl look closely interval fl 
simplicity define fl follows fl log fl exp 
fl exp gamma 
gamma fl replacing rewrite condition follows psi fl split left hand side right hand side components fl fl 
naturally psi fl fl fl guarantee individual inequalities guarantee stricter version inequality equation 
fl fl far solution really developed breakdown 
specify shall able find simple analytic solution fl function fl 
gamma log fl gamma 
gamma log fl gamma gamma log fl gamma gamma 
gamma log fl fl naturally constrained fl fl gamma fl clearly fl fl add form fl breakdown legitimate 
depicts different functions fl fl fl 
relatively easy upper bound fl fl functions individually 
basically fl symmetric function deal side time 
need guarantee quadratic bound greater line slope intercept gamma log fl 
log fl chapter 
reverse jensen inequality fl 
fl 
fl bounding separate components 
straightforward show log fl log fl holds case symmetry 
obtain follows log fl need solve upper bound fl 
symmetry fl quadratic upper bound need consider case 
shall split interval verify bound separate components log fl log fl 

interval log fl 
see fl simplifies follows fl log fl 
fl gamma 
gamma log fl log fl 
find maximizing follows max log fl fl gamma gamma log fl max log fl log fl exp 
fl exp gamma 
gamma fl gamma gamma log fl clear numerator fl gamma gamma log fl monotonically decreasing gradient negative 
denominator monotonically increasing 
maximize fraction want keep small possible set log fl 
gives condition log fl exp gamma log fl fl exp log fl gamma fl log fl gamma log fl log fl log fl gamma fl log fl second interval log fl fl simply zero 
permits fl simplify fl fl 
need upper bound follows fl log fl chapter 
reverse jensen inequality fl log fl log fl exp 
fl exp gamma 
gamma fl log fl concavity logarithm function upper bound log gamma 
invoke upper bound right hand side obtain stricter condition fl exp 
fl exp gamma 
gamma fl gamma log fl fl max log fl exp 
exp gamma 
gamma right hand side function maximized monotonically increasing gradient positive 
maximize set large possible interval considered log fl 
satisfy bounding interval obtain condition fl exp gamma log fl exp log fl gamma log fl fl gamma log fl fl gamma log fl aggregate equation equation obtain bound intervals 
clear equation stricter subsumes equation provably true simply invoking log gamma rule fl gamma log fl gamma fl values fl log fl fl gamma log fl compute psi value sum psi psi log fl fl gamma log fl fl region 
combine value fl follows psi fl fl log fl fl gamma log fl fl reasons evident soon discontinuity fl desirable prevents concavity respect fl variable right hand side 
checking chapter 
reverse jensen inequality curvature log fl fl gamma log fl see functions concave fl 
sudden jump linear function fl prevents concavity 
maintain concavity linear upper bound fl keeps continuity 
replace psi version follows psi fl log fl fl gamma log fl fl gradient value fl fl log fl fl gamma log fl gamma fl gamma fl log fl fi fi fi fi fl log gamma log log log log gamma fact maintain concavity value value slope computed 
upper bound original value fl fl 
merely value result upper bound original linear fl function 
obtain psi fl log log gamma fl log fl fl gamma log fl fl section justify went trouble function fl 
returning original solution psi psi fl effectively derived guaranteed concave upper bound fl 
analytic upper bound fl depicted equation 
fl fl log log gamma fl log fl fl gamma log fl fl final appeal convex duality shall explain gone procedure obtain analytic concave upper bound fl function 
define new upper bound right hand side expression fl fl fl 
depicts functions 
shown fl concave 
fl fl log log gamma fl log fl fl gamma log fl fl chapter 
reverse jensen inequality bounding numerical fl dashed red line analytic fl solid blue line 
recall avoided working fl directly considering upper bounds form afl fl 
ultimately final solution derived bounds form involved varying terms wm am mn zmn max mn zmn idea minimize wm quantities respect possible settings term pairs obtain tight bound possible 
varying tangential contact point fl linear bounds obtain continuum variational bound pairs parameterized variable say 

alternatively may merely parameterize function avoid additional dummy variable continuum pairs 
write succinctly wm min mn zmn max mn zmn manipulating obtain wm max mn zmn min mn zmn max mn zmn note defined pairs upper bound follows afl fl satisfy stricter condition fl fl valid alternative choice pairs afl fl manipulating note convex duality form fl gamma afl max fl fl gamma afl known concave function fl negated convex dual procedure 
formed epi graph linear bounds top chapter 
reverse jensen inequality concave fl function 
property allows standard convex duality result see appendix section clarification fl min afl notation appendix show basic result relating function dual clearly see holds define fl fl gammab 
recall minimization problem obtain wm wm max mn zmn min mn zmn max mn zmn seen function arises similar minimization 
obtain exactly epi graph definition equation simply define term multiplying equation fl fl mn zmn max mn zmn clearly see minimization avoided giving analytic result reverse jensen bound wm max mn zmn mn zmn max mn zmn compute upper bounds need merely solve max expected norms zmn plug ratio function 
concludes derivation 
chapter imitative learning imitation far learn edmund burke discussed generative discriminative learning seen theoretically fused powerful hybrid 
see ideas come applied sense combination discriminative generative tools imitation system 
motivation imitative learning useful tool learning interactive autonomous agents outlined 
chapter outline details implementation 
imitative architecture propose uses perception learn passively human teacher interacts environment 
utilizes resulting models synthesize interactive character responds appropriately perceived external stimulus 
acquire data human teacher generative perception model automatically represents human visual acoustic activity external stimulus 
generative model appropriate provide priori structure complicated perceptual domain able sample model create virtual animations teacher 
generative representation feeds training data discriminative prediction system learns forecast human measurements response stimuli 
discriminative system favored focus resources prediction task 
wish optimize performance ability specifically predict output behavior agent stimulus external world 
sections describing architecture imitative system briefly relate action reaction learning arl platform 
various key issues limitations brought motivate important generative discriminative learning tools 
subsequently data collection method hardware 
generative model described model perceptual domain describes visual data auditory data form coefficient vectors 
discriminative conditional hidden markov model cem learn forecast time series audio visual coefficient vectors 
subsequently apply tools large dataset human interactions generate program learns behavior 
qualitative quantitative evaluation imitation learning 
complete chapter brief summary open questions 
chapter 
imitative learning solid dash mouth openness time time dialog interaction analysis window imitation framework imitation framework describe learns autonomous agent able interact respond appropriately external stimulus world participants 
ability perceive real behavior humans interacting world collect data learn predictive model 
earlier action reaction learning described system learns behavior agents interacting 
seen specific instance imitation having teacher interacting world consider teachers interacting 
arl teacher embodies agent wish learn teacher embodies world state 
review arl framework detail 
action reaction learning involves temporal analysis multi dimensional time series represents interaction agents 
displays stream series 
assume stream generated vision algorithm measures openness mouth 
algorithms run simultaneously different people 
person generates dashed line generates solid line 
imagine individuals engaged conversation 
name solid fellow generating solid line dash lady generating dashed line 
initially interval time axis solid talking dash remains silent 
oscillatory mouth signal low value openness mouth 
solid says pauses 
dash responds discrete oh see 
pauses waits see solid say 
takes initiative continues speak 
solid continues talking non just long 
dash feels need interrupt counter argument simply starts talking 
solid notes taken floor stops hear 
action reaction learning seeks discover coupling past interaction immediate reaction participants 
system learn model behavior dash solid predict imitate idiosyncrasies 
chapter 
imitative learning learn dash reacts current context past seconds activity users akin world state 
process begins sliding window temporal interaction 
window looks small piece interaction immediate reaction users 
window time series forms short term iconic memory interaction highlighted dark rectangular patch 
consequent reaction dash solid highlighted lighter smaller rectangular strip 
strip treated input second strip subsequent behavioral output generate 
predict imitate dash solid system system estimate mouth parameters produce stored 
windows slide training interaction humans pairs generated training data system 
task learning algorithm learn pairs form model relating learning converged generate predicted sequence observes past sequence 
allows compute play actions users dash past interaction participants visible 
learning algorithm discover mouth openness behavioral properties 
example dash usually remains quiet closed mouth solid talking 
solid talked stopped briefly dash respond oscillatory signal 
addition solid talking continuously significant amount time dash interrupt 
simple learning algorithm detect similar data situation predict appropriate response agree system past learning experiences 
note dealing somewhat supervised learning system data split input output system target goal predict process done automatically manual data engineering 
specifies priori constant width sliding window forms width window usually width frame conservatively forecast small step 
system operates unsupervised manner slides windows data stream 
essentially learning uncovers mapping past generate best possible prediction 
interesting feature stressed chapter framework shared action perception space 
synthesizing action similar perceived merely involve copying corresponding parameters 
don need uncover complicated mapping 
aim active mapping problem plagues imitative learning approaches effectively side stepped 
unraveling complex mapping real human activity robot virtual agent imitation necessary 
simple example simple example arl framework action 
demonstrates feasibility approach particular domain permits elucidate weaknesses motivate important extensions 
agent learns imitate teacher head hand gestures 
world state merely person head hand coordinates teacher interacting 
depicts training scenario users interacting performing simple gestures camera tracks head hands coordinates 
tracking done merely chapter 
imitative learning gestural imitation learning framework 
users tracked realtime collect measurements head hand positions 
user sees caricature real time screen 
time series perceptual measurements user shown 
step time represents means covariances gaussians describe head hand positions 
modeling skin colored pixels image spatially mixture gaussians em 
gives coordinates head hands participants time series 
participant parameters mean covariance skin shaped gaussians shown 
time step training data users parameters window past seconds vectorized 
large dimensional space compressed pca dimensions input predict coordinates head hands frame output 
mapping input output performed mixture experts trained maximum conditional likelihood cem minutes interaction 
learned training data model synthesize behavior response new stimuli 
feeding back prediction looping real measurements human interactive agent responds novel gestures real time 
depicts online interaction process training user performing tricky gesture virtual character model response 
simple gestures appropriate context learned data re synthesized stochastic real time manner single user gestures system 
limitations discuss important limitations arl framework lead extensions tools resolve 
lack higher order mapping hard acquire long term behavior 
partly may maintain requisite fixed coordinate system actions perceptions 
pca representation prevents inserting important prior knowledge structure model perception temporal activity restricts arl ability generalize novel behaviors 
example simple nonlinear transformations training data may recognized model 
arl conditional likelihood focus predicting past necessarily discrimination power appropriate 
alternative appropriate conditional criterion predicting teacher actions resynthesized user actions observable 
skin color distribution rgb space modeled em mixture gaussians 
chapter 
imitative learning online interaction learned synthetic agent 
user tracked realtime agent synthesizes reaction activity 
synthesis real time forecast varies slight variations human gestures 
chapter 
imitative learning consistent long term training data previously mentioned arl avoids aim problem direct equivalence action perception 
don layer abstraction recognize gesture remains different perspectives viewing conditions 
take care maintain consistent representations deal changes coordinate system front cameras moved 
action perception layer abstracted away higher order mapping 
means training data short scarce constrained consistent view point 
setup relying having constant bird eye view scene participants 
real organism performs imitative learning luxury fixed view point teacher 
organisms mobile perform long term lifelong learning multi perspective world 
won show solve problem explicitly try alleviate proposing imitative arl learning wearable platform affixed follows human teacher 
extend paradigm collect larger data sets training behavior model 
generalizable behavior representation render learning tractable arl projects window short term memory dimensional approximation pca 
reasonable time series data smooth trajectories 
get slightly smoothed approximation lose detail reconstruction accuracy 
pca may appropriate representation capture prior structure knowledge may perceptual space temporal data 
example constant global translation head hands change meaning gesture 
furthermore speed phase variations gesture cause radical change pca representation gesture may 
unfortunately variations factored pca representation subsequent predictive learning technique difficulty learning invariances generalizing appropriately 
generative models needed capture represent factored way types variations front temporal visual transformations auditory variations 
focusing agent output behavior deficiency arl framework discriminative inappropriate sense predicts participants past 
regression model appropriately avoids wasting resources modeling past predictive value estimating optimize conditional marginal past 
past exactly system interact single human online synthesis scenario 
data direct measurements external world single human triggering system 
sense try form conditional learning problem conditioning variables broken world external world may include agents stimuli 
measurements agent world available training testing online prediction external stimulus available system synthesize agent output measurements 
form conditioning appropriate 
chapter 
imitative learning long term behavior data collection resolve issue maintaining consistent perception teacher behavior propose wearable computer system 
convenient way collect sizeable amount data teacher engages various natural activities preserves regular conditions point view necessary non aim imitative learning framework 
user teacher head mounted microphone camera mounted boom perceives face 
audio video source perception space action space learner 
furthermore camera affixed glasses microphone aimed track context perceived space external world 
wearable collects data small computer channels audio video signals roughly megabytes hour 
channel audio visual imitation learning platform 
instances wearable learning include starner clarkson 
contextual space modeled teacher action perception data collected coupled external context system 
platform provides consistent way collect long term data purposes imitative learning dealing problems mapping action perception 
data processing video captured arrives hz cameras stored images size pixels bit rgb representation 
images illumination normalized histogram fitting operation 
image filtered gaussian rgb mixture model skin color select skin colored pixels images video streams 
focuses modeling resources wearable user face focuses external stimuli head hands people scene occasional random skin colored objects unfortunately 
audio captured arrives khz sampling rate mono microphone bit resolution amplitude 
audio processed computing sound energy thresholding avoid modeling insignificant background noise 
perform fast fourier transform hamming window overlap magnitude values audio phase information discarded 
resulting spectrogram clipped lowest frequencies audio signal highest represented frequency khz 
spectrograms generated approximately hz 
chapter 
imitative learning wearable interaction data 
teacher world video visible spectrograms audio signals 
chapter 
imitative learning portrays frames user walks hallway approaches individual conversation 
frames show user face eye view camera attached glasses 
spectrograms shown interlaced corresponding video images 
higher frequencies left vectors bottom image 
high intensities indicates high audio magnitude spectrograms normalized easy viewing 
task large data set task train imitative agent learning predictive model teacher external stimulus 
data set spans hours hundreds megabytes images spectrograms initially split half form training portion testing portion 
piece training data shown covers approximately seconds worth images teacher external view seconds worth spectrograms microphones 
data split half representation external world audio video representation agent audio video 
world agent 
standard regression formulation need obtain learning training data 
train test audio visual prediction task 
user seen conversing participant external video source 
video signal participant shown frames audio signals spectrograms 
training data portion wish learn model map outside world measurements measurements teacher system imitate 
chapter 
imitative learning generative modeling perception previous chapters argued naive claim raw sensory data captured images spectrograms learned directly seeding learning process structure 
humans learn see learn hear forth machine learning system unwieldy learning task seeded priori structures models algorithms tractable 
priori structure data requirements search space size grow rapidly lead inefficient convergence 
case approaches behavioral model learning edelman proposes darwin series robots darwin iii iv 
direct mapping color responses camera large unstructured neural network connections actuators practical causes convergence problems 
course engineering prior structure equally dangerous delicate balance nature 
engineered structures specify models don agree observed phenomena providing extra knowledge structure hurt 
extra knowledge constraints reduce tractability computability 
algorithm guaranteed find global solution polynomial time local minima poor gradient descent solutions unusual additional constraints capture additional domain knowledge added system 
bayesian networks specification dependencies random variable nodes restricted tree structure propagation algorithms converge additional knowledge may create non trees cyclic links 
propose structured model intuitively plausible signals deal algorithmically feasible 
section describe structured generative model handle various perceptual signals acquired hardware proposed 
model quickly provide useful generalizable representation perceptual data collected imitation learning 
surprisingly generative model consistently applicable image data auditory data dealing 
may naive initially generative model permit mapping high dimensional audio video compact representation greatly facilitate imitative learning 
traditional ways learn summarize complicated multi variate data included example principal components analysis pca factor analysis multidimensional scaling 
data summarization approaches cast generative model framework latent values estimated compactly represent data 
depicts pca generative bayesian network 
pca model theta eigenvectors eigenvalues respectively 
data node split dimension sub nodes xd coefficients thought latent variables 
nodes box drawing replicated times instances data vectors 
eigenvectors effectively forming degenerate gaussian model sampled produce data 
latent variables constrain gaussian single mean value sampled generate spherical gaussian data 
shortcomings linear pca representation encouraged variants bayesian pca independent components analysis auto associator networks nonlinear embedding 
applying techniques images audio time series important piece knowledge overlooked 
aforementioned approaches assume data deal rasterized directly vector form high dimensional chapter 
imitative learning xd 
bayesian network description pca 
model theta includes eigenvectors eigenvalues latent variables coefficients instances data vector box acts replicator nodes instances training data 
depicts data vectors single nodes equivalent merely zooms data vector showing split smaller tuples individual scalars euclidean space 
images audio time series vectors 
single color image single big vector collection small vectors tuples pixels 
tuples vector values specify xy location rgb color 
similarly audio spectrogram vector collection tuples band amplitude frequency 
furthermore univariate time series vector collection tuples coefficient value time value permits generate interesting variant pca datum data set images audio vector collection vectors 
modification model depicted bayesian network 
xn 
mn 
bayesian network representing pca variant collections vectors 
split data vector image audio data time series subcomponent tuples xn agree prior knowledge data structure 
explicit ordering collection pixels tuples longer put fixed vector arrangement 
multinomial variables mn introduced assign unknown discrete label subcomponent associate corresponding location eigenvector subcomponent 
mn compute correspondence data model 
multinomial parameter vector thought vector positive mixture chapter 
imitative learning weights sum unity effectively hard case picks single destination assignment eigenvector component 
stack vectors form large matrix constrained sum unity columns 
matching matrices theta act soft permutation matrices re sorting data element tuples compute eigenspace 
regular pca matrices locked identity 
allow variable doubly stochastic matrices 
regular matching procedures match pair images pair exemplars matching generative model done simultaneously data set 
matrix elements appropriately repeated dimension tuple image tuples elements creating theta matching matrix form matrix size vector image transforming follows mx transform re shuffle tuples forming new data element resulting eigenspace fit accurately reconstruction error 
turns restrict summation columns unity get doubly stochastic matrix forces eigenvectors data tuples distribute soft manner 
equivalent case called way assignment problem 
problem exact polynomial time solution called auction algorithm 
statistical physics approximation called invisible hand algorithm produces rapid result standard em propagation framework problem feasible favored alternative solution doubly stochastic constraints faster convergence performance 
computation effectively iterates pca computations interlaced invisible hand algorithm solutions convergence dozen iterations 
compute soft assignment matrices coefficients eigenspace iteratively time parameters fixed 
solution converge non monotonically plagued local minima 
finding cleaner global model algorithm interesting avenues research 
explicate apply generative model images audio form multivariate time series data 
generative model images collecting small high quality subset images agent external world skin segmented obtain faces 
data form collection tuples gray scale intensity 
effectively representation treats images pixels point clouds collection points considered continuous intensity map 
approximately points sampled image forming point cloud representation 
image sets data agent world learn latent coefficients doubly stochastic matrices eigenspace structured pca model 
depicts model ability reconstruct facial images teacher skin color segmentation dimensional representation compare reconstruction pca directly images 
advantage collection vectors morphing xy just easy varying rgb quantities 
reconstruction error variant orders magnitude better pca level dimensionality reduction 
previous techniques literature utilized optical interesting related side note authors derived statistical physics formulation handling loopy bayesian networks 
chapter 
imitative learning reconstruction facial images pca variant 
flow variations image matching produce better reconstruction pca 
iterations user image reconstruction iterations world image reconstruction agent images world images reconstruction error images self left world right 
eigenvectors 
dashed red line depicts reconstruction error pca solid blue line depicts variant 
show reconstruction accuracy variant compared pca projection eigenvectors 
variant captures image structure permitting pixel permutation 
data set processed agent eigenspace agent image gets coefficients matrix image summarized dimensional coefficient vector results 
similarly world images processed forming dimensional coefficient vectors 
generative model spectrograms similarly apply variant spectrograms vectors 
fact spectrogram structure tuples amplitude frequency 
permuted increase reconstruction accuracy pca shown 
eigenvectors reconstruction accuracy orders magnitude better variant 
note factored magnitude loudness spectrogram representation concatenate coefficient vector 
produces dimensional vector summarizes chapter 
imitative learning dimensional spectrogram data 
iterations user spectrogram reconstruction iterations world spectrogram reconstruction reconstruction error spectrograms self left world right 
eigenvectors 
dashed red line depicts reconstruction error pca solid blue line depicts variant 
spectrogram eigenspaces learned agent audio world audio small subset 
compute coefficient vectors data set agent audio world audio respective eigenspace get dimensional coefficient vector spectrogram time point sequential dataset 
hidden markov models temporal signals representations generate dimensional vector frame agent audio agent video world audio world video 
processing data set obtain multidimensional time series coefficients 
time aligned appropriate interpolation aggregated large dimensional time series 
previous arl formulation pca represent window time series shall describe time series hidden markov model 
pca temporal invariance properties suffers temporal misalignments data 
known hidden markov model suited time series data effectively model time sequential variations 
arises considering hidden state variable permitting summations possible state paths evaluation probability sequence 
depicts graphical model hidden markov model shall simple modifications directly addresses desired task hand 
conditional hidden markov models imitation interested predicting component time series audio video agent generate discriminative prediction task predict cast med framework learning regression function derived hmm likelihood measurements estimate output epsilon sensitive model 
important problems approach 
traditional regression setting output hmm model may exhibit time may perfectly aligned desired output generates appropriate behavior 
epsilon tube type constraint chapter 
imitative learning may inappropriate forces outputs perfectly matched time sample basis 
furthermore performing med regression requires resolution sided constraints output scalars 
training data hundreds thousands samples output dimensions number lagrange multipliers millions making training cumbersome 
employ maximum conditional likelihood surrogate discriminative framework estimate input output hmm :10.1.1.133.6544
discriminative conditional sense utilize cem algorithm estimate hmm parameters elaborated jensen reverse jensen inequality derivations chapter 
simplifying time series learning task subproblems learn separate hidden markov models conditionally 
recall split time series half represents signals correspond world represents signals correspond agent 
split audio video components audio component world signal video component 
agent audio video sequences respectively 
learn hidden markov model predicts jx learn predict agent video jx 
split useful allows hidden states markov model specialize audio video prediction importantly events domain occur different time scales requiring different transition model 
depicts hmm predict agent audio depicts hmm predict agent video 
form called input output hmm structure inputs related outputs hidden state evolves markovian dynamics :10.1.1.133.6544
ya ya ya ya ya xa xa xa xa xa xv xv xv xv xv yv yv yv yv yv xa xa xa xa xa xv xv xv xv xv audio video prediction hmms 
hmm predicting agent audio signal external world audio video stimulus 
hmm predicting agent video input 
train input output hmms cem states assume gaussian emission models diagonal covariance matrices 
need estimate gaussian means covariances state transition matrices models 
hmm dimensional gaussian emission models output dimensional models input 
second hmm dimensional models output dimensional gaussian emission models input 
estimate hmms conditional likelihood focus resources predicting agent behavior input world stimulus 
maximize joint loglikelihood hmm components time series inputs outputs minus marginal likelihood hmm component time series 
process depicted 
permits focus salient stimuli conversations chapter 
imitative learning results significant reaction agent 
episodes data agent expressing interesting responses walking hallway background noise world ignored may contain structure world signal 
standard maximum likelihood scenario external world stimuli get modeled waste resources quite structured 
structures may important useful task related way modeled sake provide information output furthermore conditional likelihood criterion slightly robust model inaccuracies maximum likelihood may sensitive 
behavior agent external world truly follow hmm 
guarantees bayesian maximum likelihood estimation lost 
imposing task agent prediction system conditional maximum likelihood sensitive aspects model data mismatch 
log gamma log conditional hmm estimation 
effectively maximize joint log likelihood hmm inputs outputs minus marginal log likelihood hmm summed obtain marginal input 
optimization requires maximize conditional log likelihood hmm parameters respect time series vectors 
conditional log likelihood simply log gammalog latent distributions due state paths hidden hmm 
write conditional log likelihood log gamma log recognize intractable log sum negated log sum forms discussed previous chapters 
utilize jensen inequality lower bound term reverse jensen inequality lower bound second term 
permits simple step reestimate hmm parameters 
process cem loop hmms iterated convergence conditional likelihood 
experiments hidden markov models trained chapter 
audio prediction hmm initialized iterations em converges cem maximum conditional likelihood 
video prediction hmm optimized cem random initialization 
due large size dataset number samples trellis order hundreds thousands samples compute wm parameters exactly reverse jensen inequality heuristics described chapter optimization efficient 
monotonic convergence properties cem compromised heuristics 
chapter 
imitative learning training testing likelihoods data set compiled basically sample time series multidimensional vectors dimensional precise 
samples training samples testing 
form long continuous sequence train hmms deal trellis sizes samples 
forward backward algorithm counter part reverse jensen inequality need special consideration operate lengthy trellis window 
scaling ff fi probabilities forward backward algorithm appropriately avoid numerical errors :10.1.1.131.2084
furthermore avoid computing reverse jensen bounds exactly computation scales poorly large trellis sizes terms number samples number hidden states 
heuristic wm wm reverse jensen bound width parameter copied width parameter regular jensen bound symmetry type argument 
depicts convergence em approximate cem algorithm hmm 
em converges monotonically maximum likelihood solution inappropriate optimize maximum likelihood setting world state vectors measurable need predicted modeled 
desire conditional mapping conditional criterion suitable 
convergence algorithms required minutes modest pentium iii machine 
iterations joint log likelihood audio predictor iterations conditional log likelihood audio predictor training log likelihoods audio prediction hmm 
show joint loglikelihood iteration show conditional log likelihood iteration 
dashed red line em resulting log likelihoods continuous blue line cem resulting 
note common starting point common em iterations algorithms 
heuristics speed reverse jensen inequality hmm trellis size prevents monotonic convergence cem 
depicts similar behavior learning algorithms video prediction hmm 
em algorithm increases log likelihood compromises conditional log likelihood 
cem counterpart focuses resources opposite direction 
training time algorithms required minutes modest pentium iii machine 
optimization surrogate conditional likelihood ml performed cem em ultimately optimize conditional likelihood test data 
evaluate performance hmms need see estimate unfair evaluate performance test data maximum likelihood 
improve score simply able predict 
predictive hmm needs generate evaluated discriminative final chapter 
imitative learning iterations joint log likelihood video predictor iterations log likelihood conditional log likelihood video predictor training log likelihoods video prediction hmm 
show joint loglikelihood iteration show conditional log likelihood iteration 
dashed red line em resulting log likelihoods continuous blue line cem resulting 
note common random starting point algorithms 
heuristics speed reverse jensen inequality hmm trellis size prevents monotonic convergence cem 
criterion 
traditional classification problems instance classifier accuracy performance metric regression accuracy directly applicable hmm forecasts outputs 
regression accuracy natural acceptable performance metric static function approximation problem 
hmm outputs just inputs sequences 
sequences easily accommodate time warping 
hmm generate time sequences flexible time 
inappropriate consequently merely subtract hmm regressed estimate testing data may perfectly aligned 
example consider case output hmm sinusoid correctly matched shape desired sinusoid testing output data 
small phase shift arise sinusoids misaligned give rise large rms error 
unfair penalize hmm produced desired behavior perfect alignment testing data 
inappropriate static regression evaluation techniques hmm setting 
report conditional likelihood accuracy testing metrics capture invariance predictions impose 
log likelihood em cem joint training joint testing conditional training conditional testing table testing log likelihoods audio prediction hmm 
table depicts testing log likelihoods hmm agent audio prediction task 
show training likelihoods comparison 
em cem perform expected yielding joint likelihood conditional likelihood test scores respectively 
cem performs better conditional likelihood testing matching performance em training fact desired outcome 
note reported quantities logarithmic chapter 
imitative learning scale terms likelihood cem solution times 
log likelihood em cem joint training joint testing conditional training conditional testing table testing log likelihoods video prediction hmm 
situation little unusual video prediction task shown table 
testing log likelihoods hmms training ones matter somewhat similar em cem 
part problem video signals slower time scale audio ones conditional learning may role 
furthermore cem convergence particularly poor due speedup heuristics employing 
cem performs better terms training testing conditional likelihoods quite surprisingly slightly better testing joint likelihood 
results provide quantitative evaluation models algorithms data fit clear hmm prediction systems capable synthesizing imitative behavior qualitatively matches expectations 
section show results predictions animate synthetic character mimics agent teacher comment anecdotally resulting behavior performance 
resynthesis results generative model temporal data hmms generative model perceptual input structured variant pca synthesize agent reactions test data world stimulus measured 
done solving state distributions trellis measurements external stimuli 
words marginalized hidden markov model gaussian emission models component time series 
forward backward algorithm computes distribution hidden states hmm 
hidden states straightforward compute expected value output vectors time point averaging means gaussian emissions weighted corresponding state assignment probability 
method reminiscent hmm regression approaches map input variable output solving hidden states 
include matching visemes phonemes adding stylistic variations dynamics coupling musical instrument expression sound generation synthesizing facial expressions interacting people 
hmms estimated cem parameters optimized specifically type task 
formally type resynthesis regression ff fi ff fi values standard normalized forward backward probabilities vector component mean th gaussian emission model indexes states hmm 
chapter 
imitative learning hidden markov models provide time series predicted vectors audio video coefficients agent 
words time point 
coefficients reconstruct original signals images spectrograms simply multiplying coefficients eigenvectors computed pca variant 
straightforward operations possible synthesize spectrograms point clouds 
unfortunately difficult visualize merely find closest nearest neighbor training data vector render corresponding image agent time point 
resynthesis training data describe predictions hmms produce applied training data 
censor true outputs show input signal external world measurements 
double checking hmm performance training dataset 
hmms trained exemplars process test ability generalize synthesize proper output behavior 
effectively evaluating hmms ability encode training data repeat mimic behavior extrapolation 
show compression argument hmm parameters effectively summarize agent signals compactly original training data representation 
hidden markov models roughly parameters transition matrix theta parameters gaussian mean gaussian diagonal covariance parameters 
total scalar parameters trying summarize dimensional time series approximately samples 
scalars captured scalars 
obtain compression level orders magnitude encode data hmms 
principle rough estimate accuracy number significant digits scalars data hmm accurately specified 
show resynthesis samples spans sample values training data spans time indices testing spans generated individual frames real time hz movies generated hmm predictions reconstructed audio video coefficients movies obviously easier interpret figures unfortunately integrated document 
portray left right order world audio spectrograms agent audio spectrograms cloud points pca representation external world faces representation agent face raw video external world raw video agent 
raw video agent computed point cloud representation pca variant 
coefficient vector representing cloud point find nearest neighbor 
nearest neighbor match images training data set corresponding dimensional vector representations 
efficient nearest neighbor searching implementing kd trees 
lets render image looks point cloud generative model recovering 
crucial find nearest neighbor raw video match cloud points confusing representation causes excessive 
note point clouds translated centered middle image 
spectrograms shown 
time spectrograms increases move top bottom image 
furthermore move image image time increases top bottom 
spectrograms inverted play back sound synthesis problems phase information longer model magnitudes 
heuristics exist reconstructing phase simplified problem random phase values 
causes significant amount noise clicking audio resynthesis audio discernible 
chapter 
imitative learning hmm resynthesis training data 
time increasing top bottom seconds row 
figures arranged left right follows world spectrograms agent spectrograms centered world visual point cloud agent visual point cloud world image data agent image data reconstructed point cloud 
left figures time index time series 
chapter 
imitative learning observing see synthetic agent initiating conversation approaches individual external world 
agent says hi followed human saying hi 
agent says human replies fine 
agent articulations difficult decipher generally sound see hmm interleaved appropriately real human speech external world stream 
agent says sounds hi semantically inappropriate places conversation interleaved appropriately terms timing real human speech audio energy 
furthermore head movement visual cues speech agent synthesized video times agent generating audio 
skip seconds go conversation agent having human transition person time stamps 
data training set important note agent maintains similar interactive behavior changes audio video signals transitions human 
humans female provides similarity spectrogram level 
agent responds real human audio words similar previous hi see forth 
audio energy timing correctly interleaved agent remains quiet actively engaged human conversation external world channel 
important caveat form resynthesis real time causal 
hmm access sequence external world stimuli synthesized agent audio video 
possible anticipate events available online system 
lets flexible predictions hmm synthesize output sequence retrospect obtaining measurements world channel follow output 
example agent approaches individual external world says hi external individual begins speaking immediately new face initially appears noisy background 
form anticipatory synthesized data difficult real time online synthesis setting require external world stimulus trigger system causally 
sense regressing agent activity line hmms unfair consistency achieved synthesized sequences usual causal constraints real time behavior synthesis 
furthermore agent ability synthesize audio semantically meaningfully related external world triggers due linguistic understanding spectrograms 
byproduct hmm fitting fitting training data capturing interaction redundancy repeat generalize novel situations 
fact agent generates nonsensical responses acoustically timed synthesis training data suggests compression achieved hmms discarding information summarizing responses generic responses forming crude notion prototype responses 
resynthesis test data test imitative learning hmms maintained samples fully hidden training algorithms 
samples form continuous sequence minutes interactions just samples training sequence 
depicts resulting synthesized agent just previous format 
agent initially approaches human remains quiet 
range conversing interleaves hi see sure expressions audio human external world channel 
facial motion appears active agent producing audio 
coupling strong training case chapter 
imitative learning hmm resynthesis training data continued 
time increasing top bottom seconds frame 
figures arranged left right follows world spectrograms agent spectrograms centered world visual point cloud agent visual point cloud world image data agent image data reconstructed point cloud 
left figures time index time series 
chapter 
imitative learning sensible responses external world triggers 
meaningful responses training artifacts fitting test shows superficial audio visual interaction occurring primarily driven audio energy external world channel 
insufficient data learn higher order behavioral patterns flat hidden markov model capture meaningful notion hidden state 
access dozen hidden states simple spectrogram visual features 
interesting model recovers timing issues interactions audio integrates smoothly conversation flow forth 
prosodic textural interaction typically difficult design structured synthetic conversational agents speech recognition systems due behavioral opposed syntactical semantic nature 
discussion seen approaches imitative learning 
discussed action reaction learning approach models imitation time series prediction problem learning probabilistic model past 
second proposed alternative imitative approach learning predict agent external world world 
furthermore pca represent audio visual signals proposed structured variant suited handling images spectrograms superior reconstruction accuracy 
pca model temporal data proposed appropriate hidden markov model framework able handle time varying signals handle dynamic time warping 
hmm estimated conditionally cem obtain discriminative predictive distribution produces better forecasting standard em approaches 
model applied large data set human interactions acquired wearable computer synthesize interactions external world stimulus 
system quantitatively performed better em qualitatively generated interesting simple audio visual responses external world channel triggers 
important extension transition real time causal system 
hmms benefited having simultaneous access external world channel sequence start online prediction setting require hmm receive observations causal stream external world channel generating predictions 
compute hmm state trellis available world measurements synthesize agent measurements incremental fashion 
forecasting precise important estimates hmm may reliable estimates bracketed small amount world measurements near 
words synthesis agent small lag frames get better estimates long milliseconds real time responses user 
important caveat acquisition behavior described limited constrained sense 
behavior involve natural language processing visual understanding deep reaction stimulus superficial coupling audio video channels 
clear various synthetic systems utilize speech recognition natural language processing specialized visual interfaces manual animations generate compelling synthetic interactions 
objective recover models data 
learning process uncover slight facial motion responses simple auditory reactions real data 
able synthesize somewhat appropriate times response simple acoustic visual cues external world channel 
hidden markov model manageable level representation recovering chapter 
imitative learning hmm resynthesis testing data 
time increasing top bottom seconds row 
figures arranged left right follows world spectrograms agent spectrograms centered world visual point cloud agent visual point cloud world image data agent image data reconstructed point cloud 
left figures time index time series 
chapter 
imitative learning simple couplings directly data reasonably setting 
furthermore shown conditional estimate performs slightly better regular em formulation objective specifically agent measurements external world triggers 
obtained preliminary albeit simple implementation imitative learning paradigm discussed 
cast framework generative models probabilistic variant pca hmms endowed discriminative estimation conditional likelihood focus task 
paradigm imitative learning flexible implemented large manual user supervision 
simple imitative behavior acquired automatically merely collecting data real human teacher responses external world stimuli attempting mimic agent responses context statistical regression approach 
proposed paradigm amenable sophisticated generative models hierarchical hmms stochastic grammars various specialized speech vision animation algorithms sophisticated results 
various augmentations cascaded imitation learning platform hopefully improve realism synthetic interactions compelling 
chapter thesis motivated situated schools thought generative discriminative learning 
deeply complementary advantages traditional incarnations pose difficult incompatibilities 
provided common mathematical framework unites subsumes strengths 
framework maximum entropy regularization theory jensen reverse jensen inequalities provides principled fusion discriminative generative learning 
span rich flexible space generative models estimate discriminatively maximize performance tasks hand 
probabilistic modeling resources harnessed optimally discriminative criterion avoiding intermediate sub goal estimating generator 
result better performance models 
framework applied real domain imitation learning user audio visual interaction data recorded wearable computer 
imitative learning cast discriminative generative learning task 
data processed re synthesized generative perception behavior learned discriminatively estimated generative models 
result synthetic agent learns stimulus response interactive behavior autonomously unsupervised setting 
agent interactively respond external stimulus mimic teacher behavior response 
provides easy way naive user create synthetic character clone demonstrates simple interactions 
contributions section enumerate detail various contributions thesis 
include conceptual theoretical practical experimental types contributions 
domains relevance contributions include machine learning machine perception behavior modeling statistics mathematical inequalities human computer interaction wearable computing 
ffl motivating situating generative conditional discriminative learning thesis provides motivation referring various works including author depict importance generative learning applied domains 
author demonstrations computer vision human computer interfaces wearable computing emphasize probabilistic approach 
addition various works discriminative learning discussed elucidate deeply complementary advantages schools chapter 
thought generative discriminative 
taxonomy formal treatment various learning approaches described range generative conditional discriminative estimation various levels model integration local prior bayesian averaging 
ffl connecting generative discriminative imitative learning provide arguments expressing imitative learning generative discriminative learning problem 
generative models motivated need actions perceive behavior seed learning priors structures markov assumptions representations 
discriminative learning necessary emphasis prediction accuracy computational arguments saliency focus attention 
practical stand point connection imitation discriminative generative learning feasible avoiding traditional active intra modal mapping problems considering joint action perception space 
maps problem directly time series regression scenario 
ffl med combined generative discriminative framework maximum entropy discrimination med formalism thesis provides rigorous mathematical framework spanning generative discriminative schools thought 
connections regularization theory technique follows natural extension interesting computational advantages 
discriminative support vector machines subsumed range generative exponential family models 
framework combines flexibility bayesian modeling discriminative estimation 
algorithms estimating exponential family support vector machines gaussian models multinomial models portrayed empirical results argue estimation med opposed generative criterion 
addition empirical verification motivate med appeal theoretical generalization guarantees established sparsity vc dimension pac bayes arguments 
ffl extensions med framework demonstrate practical extensibility med framework discussing metaphor augmented distributions permits cascade various estimation problems med framework elegantly 
extend med classification domain regression domain subsume support vector regression method generative model regression 
furthermore discriminative feature selection regression classification settings rendered tractable 
transduction classification regression setting derived closed form 
empirical results justify flexible extensions standard discriminative classification 
various optimization techniques illustrated framework competitive computational stand point 
ffl cem discriminative mixture models rigorous mathematical framework extend discriminative conditional approaches latent domains 
begins recognizing predominance mixtures exponential family domain generative learning 
elucidate arise latent models 
variational bounds motivated efficient principled way resolve 
bound discriminative variant expectationmaximization called conditional expectation maximization proposed provides tractable monotonically convergent algorithm latent discriminative learning 
algorithm reverse jensen inequality shown apply mixtures exponential family 
bounds shown mixtures gaussians multinomials poisson gamma exponential distributions 
monotonic convergence conditional likelihood chapter 
guaranteed shown empirically outperform em techniques 
bounds render latent med computations tractable 
furthermore important manipulations elucidated permit bounds apply data sets mixing proportions variations mixture models 
ffl discriminative graphical models reverse jensen bounds applicable structured graphical models permitting estimated discriminatively 
caveat intractable computation may arise 
demonstrated efficient computation bounds possible structured graphical models deriving reverse jensen inequality hmm 
clearly shows discrimination bound techniques potentially applicable case structured graphical models bayesian networks permit discriminative learning techniques span large portion generative modeling spectrum 
ffl analytic reversal jensen inequality analytic reversal jensen inequality derived globally guaranteed 
reversal goes jensen reversals jensen converses literature specifically target discriminative estimation problems 
reversal spans mixtures exponential family tighter curvature bounds 
statistical mathematical applications possible 
reversal permits dual sided bounds probabilistic quantities single bound standard jensen inequality case 
ffl generative model pca collections tuples proposed generative model seen extension principal components analysis pca collections tuples 
pca applies vectors may appropriate representation topological data requires joint solution correspondence problem subspace estimation problem 
describe generative model introduces correspondence solution latent variables pca derive iterative algorithm estimating relevant parameters 
model readily applicable images types topographic signals provides reconstruction orders magnitude better squared error pca level coding compression 
ffl wearable platform behavior acquisition wearable platform long term behavior acquisition developed 
apparatus capable collecting real time channels video audio capture relevant aspects interaction user outside environment 
platform captures user reaction external world stimulus consistent perceived frame 
provides digitized dataset suited imitation learning agent behavior acquisition 
ffl autonomous imitative learning interactive agents automatic framework proposed learning interactions agent outside world including agents 
approach fully unsupervised results autonomous system able discriminatively predict behavior agent external stimulus 
perceptual signals encoded generative model permits straightforward resynthesis temporal behavior learned conditionally estimated hidden markov model 
simply observing behavior manifested system able learn interactive model manual supervision subsequently synthesize behavior automatically 
chapter 
ffl combining perception learning behavior acquisition demonstrated real time system performs perception learning acquires synthesizes real time behavior 
closes loop generative perception discriminative prediction imitative learning proposed shows common discriminative probabilistic framework extended multiple facets large system 
theoretical tools developed thesis open long term questions immediate directions 
section speculates postulates theoretical aspects challenge stimulate ongoing efforts ideas 
having formed joint generative discriminative framework important explore continuum generative discriminative solutions produce 
mentioned earlier regularization parameter med framework interpolate purely generative empirical bayes model purely discriminative solution 
intuitions garnered appropriate level regularization 
regularization parameters parameters framework epsilon insensitivity regression number latent models principled settings may estimable brute force cross validation 
furthermore intuitions form models amenable benefit discriminative estimation 
immediate problem presence local minima cem med frameworks latent models 
med effectively local minima problem exponential family models promises interesting convergence properties global pseudo global solutions may reach latent situations 
conventional deterministic annealing regularization arguments certainly possible avenues formal approach specifically takes advantage med reverse jensen bounds may prove appropriate 
med cem framework facilitates important extensions demonstrate prove flexibility 
transduction feature selection latent models explored may tip iceberg may open flood gates estimation scenarios 
instance missing corrupted data input space may addressed appropriate prior augmented med projection 
alternatively may consider choosing distributions model priors margin priors bias priors explore effects med solutions 
proposed discriminative generative frameworks permit explore novel probabilistic models necessarily practical purely generative setting 
may include instance un normalizable generative models 
reverse jensen approaches demonstrated efficiently applicable structured graphical models hmm 
clear bounds applied latent bayesian networks efficient implementation derived hmm translated generic setting 
instance general junction tree approach generic recipe computing reverse jensen bounds arbitrary tree structured latent bayes net 
alternatively may consider forms structure independence properties graphical models 
instance latent estimation models independency constraints sufficiency separability constraints causal structures investigated 
various alternative parameterizations bounds structured unstructured models may possible varying derivation reverse jensen inequality leading improved convergence efficient computation 
chapter 
critical effort involves exploring applications reverse jensen inequality 
derivation sub components may impact different situations discriminative learning 
bounds example may useful algorithmically generate optimization routines approximation methods wide range domains 
addition may direct statistical applications 
known jensen heart statistical inequalities build may produce converses inequalities thesis reverse jensen derivation 
similarly general mathematical applications theoretical implications fields explored 
theoretical stand point relation med reverse jensen bounds approaches bayes point machines relevance vector machines boosting exponential update rules generalized additive models elaborated 
deep connections synergistic combinations may feasible 
bridging multiple techniques schools thought predominant theme thesis 
continuing find commonalities learning frameworks paramount avoiding pitfalls field harnessing strengths 
applied discuss addresses applied aspects thesis imitation learning machine perception tools novel applications machine learning algorithms developed 
imitation learning platform described capable far hours data acquisition 
slightly version wearable may possible acquire days worth interaction data 
furthermore adequate hardware platform facilitate data acquisition due discomfort various individuals interacting user somewhat intrusive wearable apparatus 
longer term natural data may lead complex imitation learning higher order behavior couplings modeled redundant patterns isolated 
possibility performing learning online setting permit continuous developmental learning data acquisition process separate model estimation stage behavior synthesis stage 
admittedly implementation imitative learning thesis generic domainindependent 
facial auditory temporal model manual engineering domain specific 
focused effort form parametric model types data front performance may improved 
example may put stronger priors system sophisticated structures effectively seeding learning process knowledge converge realistic autonomous agent 
various visual auditory temporal behavior models need explored including particular hierarchical models potentially tractable discriminative frameworks outlined 
furthermore sophisticated features representations interaction 
include features speech recognition engine contextual ambient audio video powerful facial modeling tracking 
excessive modularization structuring problem may disadvantages abstracts away fixes lower layers processing task hand prevents learning algorithm exploring deeper relationships higher order behavior grounded sensory data 
mimicry social interactive behavior domain interest temporal interaction learning core imitation framework 
fundamentally system discovering discriminative higher order mapping temporal processes 
learning chapter 
invert re synthesize mappings applications ranging control theory music synthesis 
applications considered introduce novel sensors go audio visual platform investigated 
availability practical novel measurement devices accelerometers physiological sensors gps various types instrumentation permit collect vast panorama complex signals 
may behave deterministically predicting require modeling complex higher order behavior 
modeling process cast temporal imitative learning platform discussed may benefit discriminative estimation 
generative discriminative estimation framework med cem powerful applications 
critical area speech recognition domain 
results indicated discriminative hmms outperform methods difficult large corpus recognition tasks switchboard dataset dominated cambridge group discriminative hmm estimation 
unfortunately discriminative variants speech recognition community heuristics approximate bounds local gradient optimization 
furthermore conditional discriminative hmms optimized form large margin decision boundary example 
med cem machinery appears suited tackle problems implemented various tools conditional discriminative hmms successfully framework 
fact list machine learning application domains ranging bioinformatics web page classification simply long enumerate 
fortunately provides endless array challenging problems explore potential clients discriminative generative learning 
chapter appendix appendix provides various standard supplemental derivations implementation details help support main thesis body 
optimization med framework point discuss various implementation details optimization objective function med framework 
important feature concave procedure locally increases monotonically eventually converge global optimum 
consistently give best setting lagrange multipliers dual space optimization 
consistent global convergence guaranteed focus speed convergence discuss multiple algorithms 
natural optimization techniques setting include newton raphson gradient descent line search conjugate gradient descent 
unfortunately take advantage simple important decoupling properties objective function 
limitation portrayed initially presentation simple constrained gradient descent approach 
motivates fast axis parallel approaches benefit decoupling objective function requires local computations 
propose optimized variant axis parallel learns transition subsets variables speed training process 
constrained gradient ascent possible approach maximizing compute gradients respect lagrange multipliers take small direction gamma fi fi fi fi gamma values lagrange multipliers gamma previous ones denotes step size 
form optimization disregard constraints non negative 
problem taken care follows chapter 
appendix surrogate variables squared form vector 
gamma fi fi fi fi gamma constrained gradient ascent optimization med framework 
maintains non negativity constraint vector perform gradient ascent 
problems non informative bias additional constraint 
resolved projecting step unconstrained gradient ascent back plane 
operating space planar constraint behaves quadratic constraint 
projection solvable analytically 
addition speed convergence allow step size vary iteration 
step results increase objective function take step slightly increase doesn result increase take step retry gradient step scaled half 
practice computing gradients updated function slow problems 
compounded fact constantly re projecting constraint surface leads slow convergence 
way vastly improve optimization process consider updating single variable time computing single perturbation 
problems permits decouple computations effectively consider single datum time speeding iteration considerably order equal cardinality data set 
approach elaborated subsections 
axis parallel optimization discussed previous subsection gradient ascent types updates may efficient med framework step requires computations gradients new objective function training data set 
consider updating single lagrange multiplier time computations involve manipulation single data point detail simple sufficient statistics summarize effects rest data 
principle axis parallel optimization similar notion smallest possible working sets platt sequential minimal optimization 
difference working set single variable optimize dimension fixed 
chapter 
appendix axis parallel optimization med framework 
certainly axis parallel optimization advantages disadvantages 
computational efficiency additional advantage med due concavity objective function 
optimizing single variable time guaranteed increase objective iterating axis optimizations eventually converge global optimum 
depicts optimization toy problem 
certain cases update single axis computed analytically 
take example med svm kernel classification gaussian prior covariance oe bias opposed non informative prior 
show resulting objective just equation constraint longer necessary log gamma gamma oe gamma constraints effect objective function lagrange multipliers non negative upper bounded simple analytic update rule exists maximizing lagrange multiplier time 
holding fixed derivatives respect setting zero yield quadratic equation form gammab sigma gamma ac scalars specify quadratic equation oe gamma gamma coe gamma ck gamma oe gamma cy gamma coe solutions quadratic equation clamped evaluated see causes greatest increase objective function 
certain cases difficult obtain analytic update rule single lagrange multiplier 
brent chapter 
appendix method guaranteed search method efficient 
bisection search 
gives maximum objective function single lagrange multiplier numerically computational overhead 
point focus choose axes intelligently axis parallel optimization 
typically axis parallel iterate randomly selecting axis possible choices optimization dimensional eventually objective converges cease optimizing simple heuristic stopping criterion 
optimization typically fast med classification hundreds data points takes just seconds 
discuss efficient strategy random selection bring convergence improvements order magnitude important large data set problems high dimensional feature selection 
learning axis transitions previous approach randomly selecting axis maximizing isolation produce fast learning algorithm med significantly faster smarter routine axis selection 
strategy learn axes critical producing large improvement objective function 
seen order table model puts scalar weight axis measuring expected contribution increase objective function 
sample table distribution update axes crucial increasing frequently irrelevant axes 
natural extension element table consider theta matrix columns corresponds axis optimized rows correspond axis optimize 
row stochastic matrix specifies distribution choice axes iteration candidate attempted 
effectively define markov transition matrix axes 
identifying axes followers current axis sample specifically list get greater expected improvement objective function 
specifically compute improvement objective function brought axis optimization deltaj go old value new axis 
needless say values deltaj non negative axis parallel step guaranteed increase objective 
additional problem deltaj values discounted expect large gains early stages followed exponentially reducing gains near convergence 
model change time varying values deltaj arrive online manner time 
done fitting exponential model values form deltaj ff exp fitting parameterized curve done simple squares criterion online way don need explicitly store values deltaj 
shows fitting procedure values deltaj adjust values deltaj obtain values appropriately discounted deltaj deltaj gamma ff exp seen current true benefit axis choice 
greedy strategy pick axis generated largest deltaj current axis iteration 
form table deltaj expected value axis optimization current axis 
iteration select axis current axis highest value deltaj interleave random axis selections time encourage exploration fill table axis axis discounted objective function increments 
practice need store theta chapter 
appendix iteration approximating decay rate change objective function 
axis axis transition values handful transitions highest discounted deltaj values 
depicts approximately fold increase optimization speed results axis choice strategy med linear regression feature selection problem depicted 
iterations objective function axis parallel med maximization learned axis transition solid line random transition dashed line 
note convex duality briefly review aspects convex duality develop section 
borrowed directly treatment conjugacy duality convex functions pp 
mirror development jaakkola 
standard property closed convex set intersection closed half spaces contain 
concept translated convex sets convex functions 
sets closed proper convex function intersection closed half spaces contain 
hyper planes represented linear functions define closed convex function point wise supremum collection affine functions gamma chapter 
appendix 
gamma simply bringing right hand side set simple inequality equation 
theory conjugacy regarded theory best inequalities type denote set pairs functions inequality valid 
best pairs inequality tightened occur functions mutually conjugate 
words closed proper convex function undergo conjugacy operation produces conjugate dual function symmetric correspondence 
functions obey relation trivial manipulation equation max phi gamma psi max phi gamma psi replace max supremum operations obtain looser inequalities 
equation gives fenchel inequality closed convex function conjugate furthermore easy show dual dual function returns original function conjugacy operation closely related legendre transformation 
numerical procedures reverse jensen inequality reverse jensen bound computable analytically lookup tables numerical procedures 
completeness provide slightly tighter incarnation involves maximizing transcendental function done numerically 
recall possible values specify bounds numerically 
arose parameters linear upper bounds function fl section 
recall needed maximize fl obtain desired function fl max fl 
merely dimensional function allowing solve numerically write look table 
easier handle function considering linear upper bounds parameterized afl fl 
resulting function plotted logarithmic scale better visualization 
furthermore detailed list values provided table matlab code generating numerically 
chapter 
appendix parameters linear upper bounds function 
function relating values linear upper bounds fl function 
matlab code generate lookup table zeros opti zeros opti opti sprintf log exp exp fmin opti log exp exp zeros max table list values linear upper bounds fl function 
furthermore matlab code provided list numerically generate values bibliography amari :10.1.1.37.8662
information geometry em em algorithms neural networks 
neural networks 
arbib rizzolatti 
neural expectations possible evolutionary path manual skills language 
communication cognition 
attias 
variational baysian framework graphical models 
advances neural information processing systems 
azarbayejani pentland 
recursive estimation motion structure focal length 
ieee transactions pattern analysis machine intelligence 
azarbayejani wren pentland 
real time tracking human body 
proceedings image com may 
warmuth 
relative loss bounds line density estimation exponential family distributions 
proc 
th conf 
uncertainty artificial intelligence 
badler phillips webber 
simulating humans computer graphics animation control 
oxford university press 
barber williams 
gaussian processes bayesian classification hybrid monte carlo 
advances neural information processing systems 
barndorff nielsen 
information exponential families statistical theory 
john wiley sons 
barton 
class distributions maximum likelihood estimator unbiased minimum variance sample sizes 
biometrika 
bates loyall reilly 
architecture action emotion social behaviour 
technical report cmu cs school computer science carnegie mellon university 
baum 
inequality associated maximization technique statistical estimation probabilistic functions markov processes 
inequalities 
baum egon 
inequality applications statistical estimation probabilistic functions markov process model ecology 
bull 
amer 

soc 
bell sejnowski 
information maximisation approach blind separation blind deconvolution 
neural computation 
bibliography bell sejnowski 
learning higher order structure natural sound 
network computation neural systems 
bengio frasconi :10.1.1.133.6544
input output hmm sequence processing 
ieee transactions neural networks september 
bertsekas tsitsiklis 
parallel distributed computation numerical methods 
athena scientific 
beymer poggio 
image representation visual learning 
science 
billard hayes 
robot steps robot words 
conference edinburgh 
bishop 
neural networks pattern recognition 
oxford press 
bishop 
bayesian pca 
advances neural information processing systems 
bishop svens 
gtm generative topographic mapping 
neural computation 
blumberg todd maes 
bad dogs ethological lessons learning 
animals animats proceedings fourth international conference simulation adaptive behavior 
borg groenen 
modern multidimensional scaling theory applications 
springer series statistics 
box tiao 
bayesian inference statistical analysis 
john wiley sons 
brand 
structure discovery entropy minimization 
neural information processing systems 
brand 
voice 
technical report mitsubishi electric research labs 
matthew brand aaron hertzmann 
style machines 
kurt editor siggraph computer graphics proceedings pages 
acm press acm siggraph addison wesley longman 
bregler 
learning recognizing human dynamics video sequences 
ieee conf 
computer vision pattern recognition 
bregler omohundro 
nonlinear manifold learning visual speech recognition 
iccv 
brooks breazeal marjanovic scassellati williamson 
cog project building humanoid robot 
lecture notes computer science springer press 
brown 
fundamentals statistical exponential families 
institute mathematical statistics 
brown cocke della pietra della pietra jelinek lafferty mercer 
statistical approach machine translation 
computational linguistics 
buntine :10.1.1.52.696
operations learning graphical models 
jair dec 
bibliography burges 
tutorial support vector machines pattern recognition 
knowledge discovery data mining 
cassell 
embodied conversational agents representation intelligence user interface 
ai magazine press 
charniak 
statistical language learning 
mit press cambridge ma 
clarkson pentland 
unsupervised clustering ambulatory audio video 
icassp 
cooke mataric 
delayed real time imitation complex visual gestures 
proceedings international conference vision recognition action neural models mind machine 
cover thomas 
elements information theory 
john wiley sons 
csiszar 
information geometry alternating minimization procedures 
statistics decisions 
decarlo metaxas 
deformable model shape motion analysis images motion residual error 
international conference computer vision 
della pietra della pietra lafferty 
inducing features random fields 
ieee transactions pattern analysis machine intelligence 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
dietterich bakiri 
solving multiclass learning problems error correcting output codes 
journal artificial intelligence research 
dragomir editor 
journal inequalities pure applied mathematics 
duda hart 
pattern classification scene analysis 
john wiley sons 
dumais 
svms text categorization 
ieee intelligent systems magazine 
edelman williams sporns 
synthetic neural modeling applied real world artifact 
proc 
natl 
acad 
sci volume pages 
edwards lauritzen 
tm algorithm maximising conditional likelihood function 
appear biometrika 
boulic thalmann 
real time interactions virtual agents driven human action identification 
acm conf 
autonomous agents 
pontil papageorgiou poggio 
image representations object detection kernel classifiers 
proceedings asian conference computer vision 
pontil poggio 
regularization networks support vector machines 
advances computational mathematics volume 
bibliography faugeras 
nonlinear method estimating projective geometry views 
sixth international conference computer vision pages january 
fern givan 
online ensemble learning empirical study 
proceedings seventeenth international conference machine learning 
morgan kaufmann 
freeman pasztor 
markov networks super resolution 
proceedings th annual conference information sciences systems 
freund schapire 
experiments new boosting algorithm 
proceedings th international conference machine learning 
freund schapire 
decision theoretic generalization line learning application boosting 
journal computer system sciences 
frey jojic 
estimating mixture models images inferring spatial transformations em algorithm 
proceedings ieee conference computer vision pattern recognition 
funge 
making behave cognitive models computer animation 
phd thesis university toronto graduate department computer science 
johnson hogg 
variable length markov models behaviours 
computer vision image processing 
gershenfeld weigend 
time series prediction forecasting understanding past 
addison wesley santa fe institute studies sciences complexity 
ghahramani beal 
variational inference bayesian mixture factor analysers 
advances neural information processing systems 
ghahramani jordan 
learning incomplete data 
technical report mit massachusetts institute technology 
gibbs 
bayesian gaussian processes regression classification 
phd thesis university cambridge 
girosi jones poggio :10.1.1.48.9258
regularization theory neural networks architectures 
neural computation 
gifford jaakkola young 
graphical models genomic expression data statistically validate models genetic regulatory networks 
pacific symposium biocomputing volume pages 
hastie tibshirani 
generalized additive models monographs statistics applied probability series 
crc press 
heckerman chickering meek kadie 
dependency networks inference collaborative filtering data visualization 
journal machine learning research 
heckerman meek cooper 
bayesian approach causal discovery 
technical report msr tr microsoft research 
bibliography poggio pontil 
face detection gray images 
technical report massachusetts institute technology 
ai memo 
herbrich graepel 
large scale bayes point machines 
advances neural information system processing 
hogg 
synthetic interaction 
www comp leeds ac uk interaction htm 
web page 
hogg johnson morris 
visual models interaction 
spatial temporal modelling applications 
leeds university press 
horn 
image processing speech auditory magnitude spectrograms 
acta 

applied logistic regression 
wiley sons 
woods brass mazziotta rizzolatti 
cortical mechanisms human imitation 
science 
intille 
visual recognition multi agent action 
phd thesis massachusetts institute technology 
blake 
mixed state condensation tracker automatic model switching 
international conference computer vision 
jaakkola 
variational methods inference estimation graphical models 
phd thesis massachusetts institute technology 
jaakkola haussler 
discriminative framework detecting remote protein homologies 
www ai mit edu tommi 
jaakkola haussler 
fisher kernel method detect remote protein homologies 
proceedings ismb 
copyright aaai 
jaakkola haussler :10.1.1.44.7709
exploiting generative models discriminative classifiers 
advances neural information processing systems 
jaakkola jordan 
bayesian parameter estimation variational methods 
statistics computing 
jaakkola meila jebara 
maximum entropy discrimination 
advances neural information processing systems 
jaynes 
information theory statistical mechanics 
phys 
rev 
jebara azarbayejani pentland 
structure motion 
ieee signal processing magazine 
jebara weaver starner pentland 
augmenting experience probabilistic vision wearable computers 
proceedings international symposium wearable computers 
jebara ivanov pentland 
tracking conversational context machine mediation human discourse 
aaai fall symposium socially intelligent agents 
bibliography jebara jaakkola 
feature selection dualities discrimination 
uncertainty artifical intelligence 
jebara pentland 
parametrized structure motion adaptive feedback tracking faces 
ieee conference computer vision pattern recognition 
jebara pentland 
maximum conditional likelihood bound maximization cem algorithm 
advances neural information processing systems 
jebara pentland 
action reaction learning automatic visual analysis synthesis interactive behaviour 
international conference vision systems 
jebara pentland 
reversing jensen inequality 
advances neural information processing systems 
jebara russel pentland 
mixtures eigenfeatures real time structure texture 
proceedings international conference computer vision 
jebara schiele oliver pentland 
dynamic personal enhanced reality system 
technical report massachusetts institute technology vision modeling cambridge ma november 
appears proceedings image understanding workshop 
jebara 
action reaction learning analysis synthesis human behavior 
master thesis mit media laboratory 
vision modeling tr 
jensen 
bayesian networks 
springer 
jensen 
sur les fonctions les entre les 
acta math 
joachims 
transductive inference text classification support vector machines 
proceedings th international conference machine learning 
johnson hogg 
acquisition interaction behaviour models 
ieee conf 
computer vision pattern recognition 
jordan bishop :10.1.1.114.4996
graphical models 
progress 
jordan ghahramani jaakkola saul 
learning graphical models chapter variational methods graphical models 
kluwer academic 
jordan :10.1.1.114.4996
learning graphical models 
kluwer academic publishers 
jordan jacobs :10.1.1.136.9119
hierarchical mixtures experts em algorithm 
neural computation 
kaelbling cassandra 
acting uncertainty discrete bayesian models mobile robot navigation 
proceedings ieee rsj international conference intelligent robots systems 
kaelbling littman 
reinforcement learning survey 
journal artificial intelligence research 
kailath 
linear systems 
prentice hall englewood cliffs nj 
kearns mansour ng 
ron 
experimental theoretical comparison model selection methods 
machine learning 
bibliography kivinen warmuth 
boosting entropy projection 
proceedings th annual conference computational learning theory 
koller sahami 
optimal feature selection 
proceedings th international conference machine learning 
kosowsky yuille 
invisible hand algorithm solving assignment problem statistical physics 
neural networks 
kullback :10.1.1.14.5452
information theory statistics 
dover publications 
kullback leibler 
information sufficiency 
ann 
math 
stat 
lafferty della pietra della pietra 
statistical learning algorithms bregman distances 
proceedings canadian workshop information theory 
langford seeger megiddo 
improved predictive accuracy bound averaging classifiers 
proceedings eighth international conference machine learning 
lauritzen 
em algorithm graphical association models missing data 
comp 
stat 
data anal 
lauritzen 
graphical models 
oxford science publications 
levine tribus editors 
maximum entropy formalism 
mit press 
mackay 
bayesian methods adaptive models 
phd thesis caltech 
magnus 
matrix differential calculus applications statistics econometrics 
john wiley sons 
manning schutze 
foundations statistical natural language processing 
mit press 
mataric 
sensory motor primitives basis imitation linking perception action biology robotics 
nehaniv dautenhahn editors imitation animals artifacts 
mit press 
mataric 
fixation behavior observation imitation human movement 
brain res 
cogn 
brain res 
mcallester 
pac bayesian stochastic model selection 
machine learning journal june 
appear 
mcclelland pentland weng editors 
workshop development learning michigan state university april th 
nsf darpa 
mccullagh nelder 
generalized linear models monographs statistics applied probability vol 
crc press 
meila jaakkola 
tractable bayesian learning tree belief networks 
proceedings sixteenth annual conference uncertainty artificial intelligence 
morgan kaufmann 
meir 
empirical risk minimization versus maximum likelihood estimation case study 
neural computation 
bibliography moore 
imitation facial manual gestures human 
science 
moore 
newborn infants imitate adult facial gestures 
child dev 
moore 
infant understanding people things body imitation folk psychology 
marcel editors body self pages 
mit press cambridge ma 
moore 
explaining facial imitation theoretical model 
early development 
meng rubin 
maximum likelihood estimation ecm algorithm general framework 
biometrika 
minka 
family algorithms approximate inference 
phd thesis massachusetts institute technology 
minka 
inferring gaussian distribution 
www media mit edu papers 
ps gz 
moghaddam jebara pentland 
bayesian face recognition 
pattern recognition 
moghaddam 
yang 
gender classification support vector machines 
proceedings th ieee int ll conf 
face gesture recognition 
morris hogg 
statistical models object interaction 
international journal computer vision 
muller smola ratsch scholkopf kohlmorgen vapnik 
predicting time series support vector machines 
proc 
int 
conf 
artificial neural networks 
johnson hogg learning distribution object trajectories event recognition 
image vision computing 
nakajima pontil poggio 
object recognition detection combination support vector machine rotation invariant phase correlation 
proceedings international conference pattern recognition 
nastar moghaddam pentland 
generalized image matching statistical learning physically deformations 
fourth european conference computer vision eccv 
neal hinton 
new view em algorithm justifies incremental variants 
submitted biometrika 
oliver 
perceptual intelligence statistical modeling human individual interactive behaviors 
phd thesis massachusetts institute technology 
osuna freund girosi 
improved training algorithm support vector machines 
proc 
ieee neural networks signal processing 
russell 
online bagging boosting 
eighth international workshop artificial intelligence statistics 
bibliography papageorgiou poggio 
trainable pedestrian detection 
proceedings international conference image processing 
pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
tong 
convex functions partial orderings statistical applications 
academic press 
pentland liu 
modeling prediction human behavior 
ieee intelligent vehicles 
pfeffer 
sufficiency separability temporal probabilistic models 
uncertainty artificial intelligence 
piaget 
play dreams imitation childhood 
norton new york 
platt 
fast training support vector machines sequential minimal optimization 
burges smola editors advances kernel methods support vector learning 
mit press 
poggio girosi 
sparse function approximation 
neural computation 
rabiner :10.1.1.131.2084
tutorial hidden markov models selected applications speech 
proceedings ieee 
ramachandran 
mirror neurons imitation learning driving force great leap forward human evolution 
essay 
deng 
hmm discriminative training phonetic classification 
icslp 
deng 
speech trajectory discrimination minimum classification error learning 
ieee transactions speech audio processing 
reed marks marks 
neural supervised learning feedforward artificial neural networks 
mit press 
rennie rifkin 
improving multiclass text classification support vector machine 
technical report aim massachusetts institute technology 
ai memo 
ripley 
statistics neural networks chapter flexible non linear approaches classification pages 
springer 
rissanen 
modelling shortest data description 
automatica 
rizzolatti arbib 
language grasp 
trends neurosci 
rizzolatti galles 
premotor cortex recognition motor actions 
cognitive brain research 

convex analysis 
princeton university press 
rubinstein hastie :10.1.1.46.6208
discriminative vs informative learning 
proceedings rd international conference knowledge discovery data mining 
bibliography 
learning playing 
neural computation 
schaal 
imitation learning route humanoid robots 
trends cognitive sciences 
schapire singer 
improved boosting algorithms confidence rated predictions 
proc 
th annual conference computational learning theory 
schiele crowley 
probabilistic object recognition multidimensional receptive field histograms 
international conference pattern recognition 
schiele waibel 
gaze tracking face color 
international workshop automatic face gesture recognition pages 
schoner 
probabilistic characterization synthesis complex driven systems 
phd thesis massachusetts institute technology 
shannon 
mathematical theory communication 
bell system tech 

shwe middleton heckerman henrion horvitz lehmann cooper 
probabilistic diagnosis reformulation internist qmr knowledge base part 
methods information medicine 
sims 
evolving virtual creatures 
proceedings siggraph volume 
slonim tishby :10.1.1.122.8863
agglomerative information bottleneck 
advances neural information processing systems 
smola scholkopf :10.1.1.127.1519:10.1.1.127.1519
tutorial support vector regression 
technical report nc tr neurocolt technical report series 
starner pentland 
visual recognition american sign language hidden markov models 
international workshop automatic face gesture recognition 
starner schiele pentland 
visual contextual awareness wearable computing 
intl 
symp 
wearable computers 
sutton barto 
reinforcement learning 
mit press cambridge ma 
szummer jaakkola 
kernel expansions unlabeled examples 
advances neural information processing systems 
tenenbaum de silva langford 
global geometric framework nonlinear dimensionality reduction 
science 
terzopoulos tu artificial fishes autonomous locomotion perception behavior learning simulated physical world 
artificial life 
thorndike 
animal intelligence 
experimental study associative process animals 
psychological review monograph supplements 
thrun 
probabilistic algorithms robotics 
technical report cmu cs carnegie mellon university april 
thrun pratt 
learning learn 
kluwer academic 
bibliography tipping 
relevance vector machine 
advances neural information processing systems 
tishby bialek pereira 
information bottleneck method extracting relevant information concurrent data 
technical report nec research institute 

deal large amounts serial information retrieval rules suggest hierarchical song memory 
biological cybernetics 
tomasello kruger 
cultural learning 
behav 
brain sci 
tomasello savage rumbaugh kruger 
imitative learning actions objects children chimpanzees chimpanzees 
child dev 
asada hosoda 
state space construction behaviour acquisition multi agent environments vision action 
proceedings international conference computer vision 
ueda nakano 
deterministic annealing variant em algorithm 
advances neural information processing systems 
vapnik 
principles risk minimization learning theory 
advances neural information processing systems 
vapnik 
nature statistical learning theory 
springer verlag 
vapnik 
statistical learning theory 
john wiley sons 
warmuth cesa bianchi krogh 
bounds approximate steepest descent likelihood maximization exponential families 
ieee transaction information theory 

optimal learning neural network 
letters 
weizenbaum 
eliza computer program study natural language communication man machine 
communications association computing machinery 
weng hwang zhang evans 
developmental robots theory method experimental results 
proc 
international symposium humanoid robots 
weston mukherjee chapelle pontil poggio vapnik 
feature selection svms 
advances neural information processing systems 
williams rasmussen 
gaussian processes regression 
advances neural information processing systems 
wilson bobick 
recognition interpretation parametric gesture 
international conference computer vision 
witkin kass 
spacetime constraints 
computer graphics siggraph proceedings volume 
wolberg mangasarian 
method pattern separation medical diagnosis applied breast 
proceedings national academy sciences volume 
bibliography woodland 
large scale discriminative training speech recognition 
proceedings asr paris september 

ramaswamy tamayo mukherjee rifkin angelo reich lander mesirov golub 
molecular classification multiple tumor types 
intelligent systems molecular biology 
bioinformatics discovery note 
yedidia freeman weiss 
generalized belief propagation 
advances neural information processing systems 
yoon burke blumberg schneider 
interactive training synthetic characters 
proceedings aaai 
zhang 
selecting typical instances instance learning 
proceedings ninth international machine learning conference 
