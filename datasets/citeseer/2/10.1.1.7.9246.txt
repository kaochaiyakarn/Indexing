review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
dynamics meaning memory burgess psychology department university california riverside cassandra ucr edu kevin lund psychology department university california riverside kevin ucr edu semantics 
curse man maxwell word stands thing means speaker intends say communicates condition thing listener satisfactorily established skinner pp 
semantic structure natural languages evidently offers mysteries noam chomsky meaning provides fundamental bridge various language cognitive perceptual components language comprehension system 
important attempt model meaning acquired experience specific nature representational form 
chapter attempt deal particularly difficult problem meaning specified 
particular interested meaning represented computational model meaning process representations formed 
review previous models word meaning outside scope chapter see komatsu psychological models deserve mention inspired current computational approaches ways 
collins quillian collins loftus developed hierarchical network model semantic memory 
node link model knowledge represented concepts nodes relations concepts links 
superordinate subordinate relationships hierarchical nature model represented links 
version model spreading activation model collins loftus de emphasized hierarchical nature mental representations favor general notion semantic relatedness 
information retrieval process occurs function spreading activation structured network 
considerable support model spreading activation approach meaning retrieval representation extensively see neely review 
notions semantic connectedness spreading activation perceptual thresholds conceptual retrieval contemporary localist connectionist models burgess lund cottrell 
smith shoben rips feature comparison model hypothesized types semantic features defining features essential meaning concept characteristic features usually true concept 
processing model feature comparison comparison defining features required semantic decision 
processing characteristics spreading activation model feature comparison model better described representational characteristics 
different approach developing semantic system taken osgood colleagues osgood osgood tannenbaum 
ambitious attempt empirically derive set semantic features 
osgood pioneered semantic differential developing set semantic indices words 
procedure person rates word likert scale set bipolar adjective pairs wet dry rough research supported nsf presidential faculty fellow award sbr burgess 
catherine decker art markman sonja anonymous reviewers provided helpful comments want jeff elman providing corpus 
information research computational cognition lab hal demo reprint information hal ucr edu 
correspondence addressed burgess psychology department life science bldg university california riverside ca 
mail cassandra ucr edu 
review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
smooth angular rounded active passive 
example concept eager may get rated high active intermediate wet dry 
meaning word represented semantic profile ratings set adjectives 
aspect meaning represented adjective pair dimension high dimensional semantic space 
distances words space essentially constitutes similarity metric comparisons words sets words 
advantage semantic differential procedure words coordinates semantic dimensions making comparisons straightforward 
drawback procedure requires considerable overhead part human judges 
study reported osgood 
likert scale judgments collected adjective scales words 
human judgments required set semantic features words 
human semantic judgments rips shoben smith people typicality judgments small set words order generate dimensional semantic representation 
meaning models developed judgments word meaning effort extensive small set words 
semantic differential word association norms share problem considerable human overhead acquiring information 
serious problem theoretical level 
berwick argued selecting semantic primitives hazardous game 
different procedures deal issues word meaning acquisition occurs role simple associations learning general knowledge mechanism linking environmental input form mental representation relationship episodic semantic representations creation representations 
representing meaning computational models semantic representations computation models corresponds psychological models just discussed 
section means representing semantic features described encompass computational approaches 
spreading activation model collins loftus feature comparison model smith 
provide inspiration aspects contemporary connectionist models 
feature vectors representing meaning distributed connectionist models hinton shallice mcclelland kawamoto plaut shallice 
models semantic features specifically delineated shape volume 
limitation connectionist models usually just intuitive rationale semantic features 
example mcclelland kawamoto set distributed representations model thematic role assignment sentence processing 
words represented set semantic microfeatures 
nouns instance features human softness gender form 
verbs complex features cause verb causal touch specifies agent instrument touches patient 
model important demonstrated distributed semantic representations account case role assignment handle lexical ambiguity 
similar approaches feature designation frequently connectionist literature basic models word recognition dyer hinton shallice plaut shallice 
empirically derived set semantic features developed de sa seidenberg 
subjects list thought features words 
procedure resulted total responses 
experiments feature representations pattern inter correlations predicted pattern behavioral priming results natural kind artifact categories 
feature lists source word vectors connectionist model word representation 
masson different approach model semantic priming 
vector elements correspond actual aspect meaning simply element word vectors related words elements matched unrelated words 
semantic vectors indicated degree similarity items particular relationship vectors inherently non meaningful 
vector representations commitment particular set features theory meaning vector representations imply certain degree relatedness order model cognitive effects 
approaches binary vectors cases vector elements correspond specific featural aspects word meaning cases simply proportion similar elements dictate general relatedness word meaning 
approaches certain advantages developing review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
models meaning straightforward set complex learning models 
clear features select general model semantic representation small set items concepts nature 
drawback developing set features human feature list norms human responses required word interest 
semantic differential technique experimenter choose semantic dimensions words rated gather large number human judgments 
approaches facilitate development processing models 
gallant attempted extract semantic information directly text large scale corpora 
developed methodology extracts distributed set semantic microfeatures utilizing context word 
drawback approach features core meanings determined human judge 
limitation approaches feature list procedure nature representations foster evolution representational theory 
theoretical computational importance developing principled set meaning features surprising little attempted deriving set 
hyperspace analogue memory hal model discussed rely explicit human judgments determining dimensions represent word deciding word unit acquires representations unsupervised fashion 
model learns representations meaning large corpus text 
concept acquisition process referred global occurrence theory simple associations context aggregated conceptual representations 
memory static collection information dynamic system sensitive context 
dynamic relationship environment representation provides basis system essentially organize recourse internal agent self 
hal model model representation 
chapter hal models development meaning representations implemented process model 
primary goal exceptions statement hal processing model discuss 
implemented hal processing model cerebral asymmetries burgess lund chapter address series critical issues dynamic model memory confront providing representational theory 
argue hal model provides vehicle caused rethink assumptions underlying nature meaning representation 
hal model words slippery customers 
labov developing plausible methodology representing meaning word central serious model memory language comprehension 
large text corpus words initially track lexical occurrence word moving window 
cooccurrences develop dimensional context space see lund burgess full implementational details 
high dimensional context memory space word occurrence model concept acquisition burgess lund 
chad developing lab newest initiatives connectionist model includes hal context vectors meaning component phonology orthography 
table 
sample global occurrence matrix sentence horse raced past barn fell barn horse past raced barn fell horse past raced note values matrix rows represent cooccurrence values words preceded word row label 
columns represent occurrence values words word column label 
cells containing zeroes left empty table 
example uses word occurrence window 
review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
matrix 
refer high dimensional space context space vector element represents symbol usually word input stream text 
symbol part textual context moving window 
constructing memory matrix 
basic methodology simulations reported develop matrix word occurrence values lexical items corpus 
matrix divided occurrence vectors word subjected analysis meaningful content 
analysis occurrence define window size 
smallest useable window width corresponding immediately adjacent words 
spectrum may count words logical division input text occurring equally see landauer dumais 
word window occurrence values inversely proportional number words separating specific pair 
word pair separated word gap instance gain cooccurrence strength pair appearing receive increment 
cognitive plausibility constraint word window decreasing occurrence strength reasonable way mimic span captured working memory gernsbacher 
product procedure matrix number words vocabulary considered 
matrix demonstrate contains significant amounts information simulate variety cognitive phenomena 
sample matrix shown table 
sample matrix models status matrix word moving window just sentence horse raced past barn fell 
example may facilitate understanding process 
consider word barn 
word barn word sentence preceded word twice 
row barn encodes preceding information occurs barn 
occurrence word just prior word barn gets cooccurrence weight intervening items 
occurrence sentence gets occurrence weight intervening words 
adding results value recorded cell 
example uses word moving window important remember actual model uses word window moves word corpus 
characteristics corpus 
corpus serves input hal model approximately words english text gathered usenet 
newsgroups containing english text included 
source number appealing properties 
clear order obtain reliable data large vocabulary large amount text required 
usenet attractive indefinitely supply words text day 
addition usenet diverse 
virtually subject goes allows construction broadly cooccurrence data set 
turns useful attempting apply data various stimulus sets little chance encountering word model vocabulary 
goal hal develop representations conversational text minimally preprocessed dog cat road street dog cat road street 
sample element word vectors words 
vector element continuous value normalized value matrix cell gray scaled represent normalized value black corresponding zero 
gray scaled vectors normalized numeric representations vector elements 
review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
human concept acquisition 
formal business reports specialized dictionaries frequently corpora usenet text resembles everyday speech 
model works noisy conversational input suggests robustly deal problems human language comprehender encounters 
vocabulary 
vocabulary hal model consisted frequently occurring symbols corpus 
half entries standard unix dictionary remaining items included proper names slang words nonword symbols misspellings 
items presumably carry useful information concept acquisition data extraction 
occurrence tabulation produces matrix 
row vector represents degree word vocabulary preceded word corresponding row column represents cooccurrence values words word corresponding column 
full occurrence vector word consists row column word 
experiments groups occurrence vectors 
vectors length viewed coordinates points high dimensional space word occupying point 
representation differences words occurrence vectors measured distance highdimensional points defined vectors distance measured riverside context units see lund burgess 
vector properties 
described element vector represents coordinate highdimensional space word concept distance metric applied vectors presumably corresponds context similarity just item similarity discussed 
vectors viewed graphically seen 
sample words dog cat shown accompanying element vectors elements shown viewing ease 
vector element continuous numeric value frequency normalized value matrix cell 
grey scale represent normalized value black corresponding zero minimal value 
word vectors sparse large proportion word vector elements zero close zero 
word vector seen distributed representation hinton mcclelland rumelhart 
word represented pattern values distributed elements particular vector element participate representation word 
representations gracefully degrade elements removed example small difference performance vector elements elements 
seen words representing similar concepts similar vectors subtle times see 
see lund burgess full description hal methodology 
hal model investigate wide range cognitive phenomena 
goal chapter address series issues central theory memory representation discuss particular cognitive phenomenon detail 
precursor prepared illustrate variety categorization effects hal model investigate 
sections primary literature extensive results referred interim serve conceptual cat dog bread pie milk beer nebraska pennsylvania alberta sorry mad sad oj weather cloud rain snow house building movie book story examine accept consider propose 
dimensional multidimensional scaling solutions common nouns words grammatical categories 
review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
starting point 
results analyses stimuli earlier papers multidimensional scaling algorithm mds projects points high dimensional space lower dimensional space non linear fashion mds attempts preserve distances points possible 
lower dimensional projection allows visualization spatial relationships global occurrence vectors items 
example vector representations carry basic semantic information provides categorization animals foods geographic locations 
category semantics seen 
alcoholic liquids cluster food group young domestic animals cluster separately common labels dog cat 
distances items model variety semantic priming experiments discussed section 
stimuli illustrate particular feature hal meaning vectors model concepts notably problematic representational theory 
concepts weather terms proper names emotional terms segregate meaning spaces 
advantage representing meaning vectors vector element symbol input stream typically word words features words 
translates ability vector representation concepts easily representation basic concepts burgess lund 
important absolutely crucial developing memory model purports general nature 
major aspect categorization hal model address grammatical nature word meaning 
clear categorization nouns prepositions visual inspection mds presentations appear show robust separation various word groups 
important determine categorizations clearly distinguished high dimensional space 
approach analysis variance compares distances intergroup distances 
accomplished calculating combinations item pair distances group comparing combinations item pair distances groups 
mds presentations shown analyses computed differences discussed reliable 
verbs seen 
generalizability hal model capture grammatical meaning traditional semantic characteristics words important feature model burgess burgess lund part motivation refer high dimensional space context space semantic space 
characteristics word meaning model encodes led rethink number assumptions dynamics memory concept acquisition addressed sections 
hal model offers clearly defined way think association learning process relationship basic associations higher order word meaning 
grammatical characteristics encoded word vectors provokes reconsideration syntactic constraints representational modularity 
global occurrence mechanism heart model provides vehicle rethinking meant similarity 
think hal offers general statement similarity models 
result global occurrence mechanism works allowed proposal high dimensional memory models address failure previous computational model deal symbol grounding problem 
role context central issues address 
section comparison hal implementation context model recurrent neural network implementation 
similarity results different implementations strong case strength contextual constraint language input forming conceptual representations 
turn evidence arguments 
rethinking nature associations hal model association semantic categorical knowledge clearly defined 
operational definitions shed light ongoing controversy priming literature meant semantic priming conditions obtained 
critical discussion distinction semantic associative relationships 
experiments word association norms derive stimuli 
word norms confound semantic associative relationships 
cat dog related categorically similar animals associatively tend produce production norms 
typical assumption associative relationships associations caused temporal occurrence language review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
environment 
stimuli constructed semantic categorical associative relationships part manipulated 
illustrate cat dog semantically associatively related 
music art semantically related art show associate music word norms 
conversely bread tends words produced norms word mold 
bread mold similar clearly say relationship bread mold just different items 
story goes mold bread occur 
examples types word pairs seen table 
semantic associative priming 
claim hal encodes experience learns concepts categorically 
associative episodic relationships aggregated conceptual representation 
seen reexamining table 
vector representation barn include row column weighted cooccurrence values words occurred barn moving window 
representation barn stands table episodic 
barn occurred context 
language experienced hal vector representation barn accrues contextual experience result weighted occurrences sum experience resulting generalized representation barn 
important aspect hal attempting model priming 
follows distances hyperspace sensitive generalized categorical relationships 
furthermore associative relationships strong correlation hal distance metric 
tested hypotheses experiments lund burgess different types word relationships illustrated table 
word relationships various combinations semantic associative properties semantic associative combined semantic associative properties 
considerable research shows human subjects sensitive types word relationships lund lund burgess see neely 
replicated finding subjects faster lexical decisions related word trials conditions targets unrelated pairs lund 
second experiment computed context distance related unrelated trials conditions hal 
priming computed experiment distances shorter distances related pairs unrelated pairs representational model 
experiment robust priming semantic semantic plus associative conditions 
distance priming model associated pairs 
result raises intriguing questions representational nature words ongoing controversy priming literature meant semantic priming conditions obtained 
controversy exists part due mixed set results literature investigators obtaining semantic priming association finding semantic priming conditions limit strategic processing 
fischler earliest findings showing strength association correlate priming 
similarly burgess richards pollock semantic priming low proportion related trials naming task 
lupker find priming semantically related word pairs associatively related 
similar set results shelton martin 
single presentation lexical decision task words lexical decisions word 
procedure masks prime target relations subject 
shelton martin find semantic priming conditions 
comparison experiments usually entails comparison methodologies 
experiments obtain table 
example prime target word pairs semantic associated semantic associated relatedness conditions 
semantic associated semantic associated table bed baby ale beer music art mug beer uncle aunt flea ant mold bread ball bat note full set stimuli taken burgess richards pollock 
review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
semantic priming typically avoid lexical decision task part individual presentation procedure shelton martin 
naming task thought sensitive strategic effects may limit sensitivity semantic relations 
clearly experimental procedures task differences play part results 
focusing task differences may divert attention important representational issues just important 
developing representational theory important representational solely procedural issues 
argued experiment sensitivity reflecting semantic priming effect guided strength semantic contextual relationship lund 
set stimuli evaluated detail hal model items shelton martin 
semantic pairs wife peas closely related hal semantic distance metric 
furthermore number semantic associated pairs strongly related categorically road street girl boy see lund 
hal argued condition produce priming simply prime target pairs condition sufficiently similar 
experiments offer compelling evidence increased similarity results priming task constraints usually associated lack semantic priming 
burgess priming word pairs originally 
patients visual neglect result brain damage 
compelling result priming occurred primes impaired visual field 
patients aware prime making difficult argue strategic effect 
result confirmed earlier hypothesis generated hal simulation shelton martin failure find priming due insufficient relatedness semantic condition 
recall individual presentation lexical decision methodology 
replicated methodology set non associatively related word pairs subjects rated similar shelton martin items 
replicated shelton martin items similar items robust semantic priming effect 
appears increased attention representational nature stimuli affords complete understanding semantic constraints methodological issues involved priming 
hal distance metric offers way evaluate stimuli clearly operationalized manner 
item lexicon provides basis stimuli various experiments evaluated directly 
experiments word association norms derive stimuli important realize word norms confound semantic associative relationships 
argue hal offers account initial bottom activation categorical information memory 
provides index information activated automatically 
argued associative semantic information facilitates automatic bottom activation information lupker shelton martin confusion result field having clear operational definition association association participates learning 
hand association operationally defined type word relationships produced person free associates 
unsatisfying definition theoretical level acquisition process nature representation 
confounds types word relationships word association procedure 
word association norms 
intuitive conception word association related degree words tend occur language miller 
spence owens confirmed long held belief empirically 
see relationship word association ranking lexical occurrence held language corpus hal highly associated pairs palermo jenkins norms basis experiment lund 
replicated spence owens effect word association ranking correlated frequency cooccurrence moving window 
correlation strong theirs probably due fact strongest associates cue word 
strongly associated word pairs allowed test question 
extent similarity operationalized hal model related occurrence language highly associated words 
divided strongly associated pairs semantic neighbors associates occurred radius words hyperspace non neighbors pairs review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
words apart 
items strong associates expect word association ranking correlate occurrence frequency hal neighbors non neighbors recall groups words collectively show correlation ranking occurrence 
results striking 
correlation close neighbors correlation 
results suggests popular view association reflected word cooccurrence true items similar place 
word association best represented simple notion temporal contiguity local occurrence 
perspective hal model word meaning best characterized concatenation local occurrences global occurrence range occurrences word history occurrence word vector 
simple occurrence probably better indicator episodic relationship poor indicator categorical semantic knowledge 
way think global occurrence contextual history word 
weighted occurrences summed indices contexts word occurred 
word meaning vectors 
way consider little effect local occurrence information vector similarity remove vector recompute similarity 
consider example cat dog example 
vector cat vector element weighted local occurrence cat preceded dog matrix row weighted cooccurrence cat followed dog matrix column 
word pair remove vector elements correspond local cooccurrences words 
prime target pairs stimuli semantic priming studies described lund originally 
items hal lexicon left related prime target pairs 
procedure resulted sets vectors related pairs original set vector elements set elements corresponding words removed 
vector elements corresponds words removes effect local occurrence 
correlation computed prime target distances sets items 
virtually impact removal vector elements correlation 
may counter intuitive considers removing local occurrence amounts removal word vector elements 
important pattern vector similarity global occurrence particularly rows columns variance largest indicating greater contextual exposure 
rethinking syntactic constraints common word representation carry information semantic grammatical raises questions potential interaction kinds information subsequent comprehension 
burgess lund addressed issue semantic constraint offered simple noun phrase syntactic processing reduced relative sentences 
english language svo bias sentential agent typically subject position 
sentence follows construction simple past tense 
sentence initial words man paid lead parser construct past tense construction 
preposition encountered clear comprehension system sentence structure past participle 
past participle constructions usually difficult understand 
semantics initial noun phrase constrain interpretation initial noun plausible agent verb reading difficulty reduced 

man paid parents 

man paid parents unreasonable 

paid parents unreasonable 
intuitive sense semantic plausibility facilitates interpretation important question psycholinguistics speed take place implication processing architectural modularity 
variety investigators shown semantic plausibility plays immediate role interpretation constructions syntactic reinterpretation necessarily required burgess burgess lund burgess tanenhaus hoffman macdonald macdonald pearlmutter seidenberg tanenhaus carlson trueswell tanenhaus kello trueswell tanenhaus see macdonald review 
investigators type semantic constraint immediately affect review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
sentential interpretation reader initially misinterpret construction strong semantic constraint frazier ferreira clifton rayner carlson frazier 
studies directly compared stimulus sets various investigators 
sets produced results reflected initial semantic effect sets burgess lund burgess taraban mcclelland 
studies strength semantic constraint differed important ways experiments difference predicts reading difficulty eliminated 
burgess lund pursued issue strength semantic constraint syntactic processing evaluating distance highdimensional context space model hal correspond constraint offered sentence initial noun past participle verb vs paid 
context distance noun verb pairs inversely correlated reading ease 
burgess lund context distances stimuli different studies reduced relative sentence constructions simulate results experiments 
study find effect noun context reading time disambiguating region 
studies simulated find context effect suggesting constraining relationship biasing noun verb 
burgess lund results showed hal context distances shorter stimuli studies find context effect study find context effect 
appears hal representations sensitive interaction semantic grammatical information context distance provides measure memory processing accompany sentence comprehension 
hal similarity measure essentially measure contextuality notion expand 
results suggest high dimensional memory model hal encode information relevant just word level 
kinds results certainly general claims modeling syntax high dimensional meaning spaces 
time clear distance metric corresponds constraints different grammatical classes words specific contextual relationships sentences 
furthermore elman shown sentential meaning tracked attractor network dimensional space 
results demonstrated network learn grammatical facts complex sentences relative clauses dependencies 
relationship high dimensional spaces represent correspondence higher level syntactic forms remains exciting controversial domain 
rethinking representational modularity syntactic processor utilize contextual information guide parsing decision controversial issue question presupposes parsing mechanism 
theories parsing driven lexical semantic models word recognition 
notion stage parser syntactic structure built initial recourse available semantics continues dominant theory psycholinguistics clifton ferreira frazier clifton 
models syntactic processing relied increasingly richness lexical semantic system provide various semantic thematic local occurrence information required correctly assign meaning word order burgess lund macdonald tanenhaus carlson 
basic constraint satisfaction models free utilize broad range information acknowledge different sources information vary relative contribution sentence comprehension process 
evidence supports constraint satisfaction approach calls question strict notion modularity processing 
results suggest language processor modular modular performance observed function variety constraints may may available 
parallel issue exists respect modularity representations 
theories language comprehension assume different forms representations syntactic grammatical lexical semantic linguistically distinct regardless position processing modularity burgess burgess burgess lund frazier frazier fodor macdonald tanenhaus carlson 
connectionist word recognition models tended blur distinction consolidating learning different representational sources single layer hidden units elman seidenberg mcclelland 
hal vector acquisition process simply accumulates word representation word surrounding context 
vector element particular word corresponds symbol review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
usually word input stream part contextual history particular word 
word representation corresponds complete contextual learning history function word context frequency cooccurring symbols relative weight moving window 
previous semantic priming word association norms lund grammatical effects burgess lund burgess lund see finch chater suggest hal representations carry broad range information account variety cognitive phenomena 
generality hal representations suggests possible encode types semantic grammatical possibly syntactic information single representation information contextually driven 
increased reliance contextual factors influence syntactic processing need representational theory vital 
propose vector representations acquired hal model provide partial resource 
vector representations product considerable language experience words text simulations reflect words highly diverse set conversational contexts 
model presuppose primitive defining semantic features require experimenter commit particular type set features 
model uses features vector elements words symbols language 
word defined wide range contexts 
rethinking similarity notion word meaning similarity constrained contexts uncontroversial 
possible relationships context word meanings delineated miller charles 
strong contextual hypothesis words semantically similar extent contextual representations similar quite superficially consistent presenting ways miller charles rely heavily commonly held assumption think problematic general model meaning acquisition 
important miller charles similarity closely attached grammatical substitutability 
hal generalizability quite limited acquisition process grammatical substitutability 
context word appears hal word window records weighted occurrences prior word question 
local occurrence abstracted immediately global representation 
result word meaning ultimately little words occur close temporal proximity 
role context transparent hal model 
word meanings arise function contexts words appear 
example cat dog similar occur similar sentential contexts 
similar frequently occur locally 
departure traditional views similarity focuses item similarity 
vector experiment produced important insight simply removing vector elements correspond locally occurring words pair vectors recomputing distance hyperspace 
manipulation virtually difference 
example illustrates lack effect local occurrence relationship road street see 
words synonymous seldomly locally occur 
words occur contexts 
lack effect local occurrence landauer dumais highdimensional memory model appear general feature class model 
result role contextual similarity words may possess elements items similarity due role context 
advantage notion contextual similarity traditional item similarity words related complex thematic ways meaningful distance relations 
example cop arrested traditionally similar items 
contextually similar result distance items reflects relationship agent action aspects lexical entities see burgess lund 
greatly expands potential role similarity memory language models incorporate meaning vectors 
review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
comparison dynamic learning models argued hal dynamic concept acquisition model matrix representing momentary slice time prototypical dynamic learning model probably established connectionist model 
section compare output global occurrence learning algorithm srn simple recurrent network input corpus 
motivation comparison claim models learn context 
hal uses weighted word moving window capture context surrounds word 
example srn comparison elman context target word sentence recurrent layer provides additional set inputs previous word hidden units encoding current word representation learned 
hal srn common words represented distributed fashion high dimensional meaning space 
meaning space hal elements defined input symbols weighted global occurrence procedure 
meaning space elman srn function hidden unit activations 
elman srn trained predict upcoming words corpus 
network trained hidden unit activation values input word word representations 
corpus constructed small grammar sentence frames lexicon words grammar construct set word sentences resulting corpus words 
corpus simply sequence words sentence boundary markers punctuation 
corpus fed neural network consisting input hidden output layers plus fourth context layer echoed hidden layer see 
network trained predict word current word historical information contained context layer 
training hidden layer activation values word taken word representations 
approach replicating global occurrence learning algorithm hal model 
occurrence matrix constructed elman corpus window size word 
context represented elman neural network consisted prior items word vectors extracted occurrence matrix matrix rows representing prior occurrence yielding vectors elements 
vectors normalized constant length order account varying word frequency corpus 
gray scaled representation occurrence matrix lexical items shown 
darker cells represent larger occurrence values rows storing information preceding occurrence columns occurrence 
example matrix shows word glass row preceded words smash break eat column followed animates lion dragon monster presumably agents involved 
casual examination matrix suggests semantic information captured words similar meanings seen similar vectors 
closely examine structure vectors word representation input units word representation output units context layer hidden units 
elman simple recurrent neural network architecture 
glass plate bread sandwich cookie break eat smash think exist sleep see chase smell move cat dog mouse woman boy girl man dragon monster lion rock car book glass plate bread sandwich cookie break eat exist sleep see chase smell move cat dog mouse woman boy girl man dragon monster lion rock car book 
gray scaled representation global occurrence matrix lexical items elman 
review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
constructed hierarchical clustering hal vectors shown alongside clustering obtained elman 
hal performed reasonably separating animate objects subdivisions people dangerous animals safe animals edible objects verbs fragile objects 
categorizations similar elman 
clustering produced hal clean elman srn noted matrix formed conservative word window test sentences words long 
similar results produced approaches generation semantic structure 
apparently dissimilar approaches yield results 
answer techniques capitalize similarity context semantically grammatically similar words order construct representations meanings 
virtually thing approaches common fact context information available 
find basic underlying structure vocabulary argues strongly context valid fundamental carrier information pertaining word meaning 
srn appears little sensitive grammatical nuances 
produces compact representations vectors shorter vocabulary size element hidden unit 
drawback scale real world vocabularies 
tens thousands words tracked just network enormous training difficult time consuming just impossible due sparseness representations learned 
important know global occurrence models yield virtually result srn 
equivalence approaches facilitate understanding general role context develop hybrid models 
symbol grounding problem glenberg raises issues claims serious problems memory models 
symbol grounding problem 
representations memory model extension real world 
lexical items understood respect just lexical items 
grounding representation lexical item physical reality environment cf cummins 
model represents concept vector arbitrary binary features set intuitively reasonable contrived set semantic features clear mapping environment supposes represent 
hal takes different approach problem 
hal vector element coordinate high dimensional space word 
important realize vector element element direct extension learning environment 
word vector element represents weighted frequency value relationship part environment woman man girl boy mouse dog cat monster lion dragon rock car sandwich cookie bread book think sleep exist see smell move chase break smash eat plate glass smell move see think exist sleep break smash chase eat mouse cat monster lion dragon woman girl man boy car book rock sandwich cookie bread plate glass dog verbs nouns animates human animals food opt abs oblig 
hierarchical cluster diagrams elman results hidden unit activation vectors simple recurrent neural network results global occurrence vectors hal model trained elman corpus 
review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
represented element word meaning 
word meaning comprised complete vector 
symbol grounding typically considered problem concepts 
representations memory models grounding environment 
hal different regard 
advantage representational methodology hal representations encoded way concrete words 
language environment incoming symbol stream hal uses input special way 
concepts sense grounded 
second problem faced models develop meaningless internal representations variety input human experience get encoded memory representation inevitably impoverished 
current implementation hal certainly limitation 
learning experience limited corpus text 
raises important currently question 
limitations hal representations due impoverished input higher level symbolic representations required flesh complete memory system argued markman dietrich press 
think hal model sensitive occurrences natural environment human language learner model completely symbol grounded just language stream able capitalize additional information construct meaningful representations 
answer questions premature speculative 
said important issues general model think intriguing speculative arguments high dimensional memory models capture aspects schemata decision making 
higher level cognition previously argued hal word vectors generated global occurrence learning mechanism best regarded encoding information model initial bottom activation meaning memory 
semantic grammatical structure emerge refer global occurrence weighted concatenation thousands simple local cooccurrences associations 
maintained statistical associations produce sophisticated knowledge structures encode richness organisms interaction environment glenberg lakoff 
lakoff argues schemata major organizing feature cognitive system origin primary schemata involve embodiment basic experience 
glenberg takes similar stance concluding complex problem solving scope simple models 
simple association part similarity judgments medin gentner markman maintain higher level structure typically involved making similarity judgments 
easy imagine model hal adequate representing higher level cognition 
markman dietrich press suggest adequacy cognitive model require multiple 
symbolic representations may represent fine grain necessary context sensitivity 
conversely distributed representations limited manage contextual invariance 
section address highdimensional memory models may offer plausible representational account schematic representations forms decision making 
rethinking schemata schema typically considered symbolically structured knowledge representation characterizes general knowledge situation schank abelson 
schemata instantiated distributed representations rumelhart smolensky mcclelland hinton 
rumelhart modeled notion rooms having set microfeatures corresponded aspects various rooms television oven 
microfeatures fill slot schema 
primary difference symbolic account distributed account distributed account schema structured representation distributed schema function connection strengths 
hal notion schema best corresponds context neighborhood 
word hal lexicon isolated high dimensional space 
surrounding word words vary distance 
neighbors words close 
table shows context neighborhoods words beatles frightened prison 
mds solution demonstrate different sets words plausibly categorized 
remains unclear exactly space mds represents 
context neighborhood provides insight nature meaningful information hyperspace 
schema specifically structured context neighborhood 
common components review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
schema context neighbors word provide set constraints retrieval 
context neighborhoods sufficiently salient allow humans generate word neighbors generated word closely related burgess 
neighborhoods provide definition schema sorts definition find dictionary 
criticism spatial models hal words meaning space sense glenberg 
generally true models features provided intuition hand coded derived word norms 
result actual correspondence real input learning environment ultimate representations 
models including hal differ regard see landauer dumais lsa model deerwester dumais furnas landauer harshman elman connectionist approach word meaning 
words symbol grounded respect environment serves input stream language cases 
edelman taken similar approach constructing representations visual environment 
criticism high dimensional space models adequately distinguish words synonyms words antonyms markman dietrich press 
illustrated neighbors bad 
bad closest neighbor 
examples highlight difference item similarity context similarity usually seen adjectives 
bad occur similar contexts bad eye tend close meaning space 
spatial models tend problem 
limitation may problematic suggested markman dietrich 
immediate neighbors contain items related core meaning nice great wonderful better items related bad 
likewise bad neighbors share meaning hard dumb stupid cheap meaning 
despite limitation argue neighborhoods offer sufficient constraint characterize meaning 
problem solving problem solving decision making complex cognitive events representationally view processing 
premature suggest high dimensional memory models purport model range representations provide scaffolding complex problem solving 
high dimensional memory models may useful modeling aspects problem solving hinge similarity 
example tversky kahneman approach decision making uncertain events relies representativeness availability 
hal representativeness captured context similarity 
likewise frequency metric predict availability 
tversky feature contrast model model kinds similarity judgments 
prior tversky similarity relations part considered symmetric 
tversky shown asymmetry rule 
example tversky illustrates north korea judged similar china china north korea 
featural asymmetry acknowledged important component models similarity gentner markman medin goldstone gentner nosofsky models metaphor glucksberg 
metric hal model typically distance metric 
example tiger apart high dimensional space 
useful information know tiger contextually similar tiger bunny eagle 
context distance symmetrical important limitation hal 
noted areas items high dimensional space vary density krumhansl nosofsky 
hal different 
tversky pointed tiger probable response word association task tiger 
tiger units apart context neighborhoods differ items contain density 
hal high dimensional space tiger th neighbor th neighbor tiger asymmetry direction find word norms see table 
nearest neighbors beatles frightened prison 
beatles frightened prison original scared custody band upset silence song shy camp movie court album anxious jail songs worried public review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 

korea china example tversky shows similar asymmetry number intervening neighbors see 
china korea th neighbor hyperspace korea china th neighbor 
density hal model important metric predicting semantic effects 
buchanan burgess lund context density better predictor semantic brain damaged patients context distance word association norm rankings 
characteristics context neighborhoods density neighbor asymmetries important factors modeling representations important problem solving process 
say distance 
ability similarity information sorting important ability 
tversky gati subjects select country set similar comparison target 
frequently choice similarity possible choices 
simulating similarity component country choices implication similarity sorting task important show hal vector representations reflect semantic characteristics geographic locations 
names cities states countries submitted mds procedure see 
reflects vectors categorize locations 
important note english speaking countries separated space asian countries analysis tversky gati sorting experiment requires vector representations countries reflect semantic distinctions countries 
proper name semantics tradition notorious difficult model see burgess press 
simulation tversky gati demonstration attempted show low level contextual information support high level decision making 
tversky gati experiment subjects asked match country similar countries 
countries varied assumption changing third choice country affect countries chosen closest match comparison target 
manipulation third choice country tended cause reversal choices chosen closest match comparison target 
tversky gati assumption subjects finding closest match target country reversal finding pair choices similar assigning remaining item closest match comparison target 
simplistic model disregarding target item theory account choice reversal tversky gati 
evaluate theory context distances computed triplet choices tversky gati table 
note distances computed related target item 
countries smallest context distance considered form match third country considered matched target 
example procedure see 
predicted experiment hal vector representations notable represents successful application model implementation proper name semantics 
tiger china korea 
diagram illustrating asymmetry number context neighbors separating word pairs 
review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
set israel matched england iran tend form grouping 
replaced france set prediction france tend pair england leaving iran best match israel 
fact reversal tversky gati time 
result replicated hal distances 
set iran smallest inter item distance leading england paired israel 
set england france similar hal hyperspace distance brings result humans israel paired iran 
tversky stimuli analysis context distances hal model led expected country picked time similar result human data 
contrast model tversky gati computes similarity items combination common distinctive features 
humans sorting task requires attention directed common features choices result features salient 
different choice option redirects attention common features resulting different pairing 
hal representational model implemented mechanism corresponds attention 
consequently results suggest contextual information available hal vector representations sufficient type decision making 
emphasized claiming hal decision making model 
feel contextual model meaning provide sufficiently rich information concepts information useful higher level decision making 
nebraska illinois kansas england canada japan korea china riverside philadelphia 
dimensional multidimensional scaling solutions countries cities states 
israel england iran israel england france iran set set 
example sets countries tversky gati 
context distances indicated possible pairs choice words 
word pairs shortest distance bold 
review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
hal vector representations 
form function 
frank lloyd wright inner workings models opaque 
shepard characterized criticism connectionist models connectionist system manifests intelligent behavior provides understanding mind workings remain mind 
difficult times understand precise representational nature hidden units psychological reality cleanup nodes 
conversely hal model quite transparent quite simple 
goal hal project little extent possible 
features problematic possibly capture range human abilities center representation non occurring units especially language 
addressing question hal vector representations represent involves number subtle descriptive theoretical issues 
hal vector representations descriptive answer 
meaning vector concatenation local cooccurrences word window 
time words occur episodic trace 
example table occurrence value raced preceded horse 
represents strong episodic relationship window horse raced strong relationship words occurred 
soon raced occurs word cell matrix starts lose episodic nature 
experience accrues vector elements acquire contextual history words correspond 
word experiences words richer context vector 
complete vector row elements column elements approximately variant vector elements provide bulk meaning information 
vector full smaller set features referred global occurrence vector local occurrence simply occurrence item 
hal vector representations theoretical answer 
probably ways coherently consider vector representations 
representations words sense symbolic 
vectors characteristics distributed representations 
consider vector representations simply documentation learning history word contexts worry representation 
possibilities briefly addressed answer question hal vector representations 
vectors symbolic representations 
element vector representation provides coordinate high dimensional space 
vector provides set coordinates constraints converge symbol usually word 
hyperspace retrieve word neighbors usually words 
addition vector element directly corresponds symbol input stream 
course result text input 
evidence suggests process similar global occurrence deal speech segmentation problem phonetic level chater levy 
imagine global occurrence system operates cascaded levels speech segmentation processor meaning processor output 
vectors distributed representations 
meaning word function pattern values different vector elements 
word point hyperspace point convergence thousands flexible constraints 
memory matrix slice time history system encounters language experience 
hal vectors important characteristics distributed representations 
degrade gracefully 
clear extreme considers vector elements suffice purposes 
characteristic distributed representations comprised subconceptual elements 
typical connectionist example dog cat subconceptual features hinton shallice 
presumably features perceptual components concept develop 
hal representations acquired language environment representations take form set concrete objects 
subconceptual features hal just symbols weighted occurrence value symbol 
sense calling word features subconceptual misleading depends nervous system perceptual apparatus parses input 
probably important point development theory review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
meaning notion concepts comprised large set occurrence elements 
cooccurrence values form contextual history word 
dog cat similar hal occur similar contexts small legs pets features constrain appearance particular contexts 
result degree items locally occur little relevance development distributed meaning vector 
recall experiment locally occurring vector elements lesioned 
similarity recomputed effect negligible 
runs counter uses occurrence memory models 
hal distributed vector representations representations contextual meaning lsa landauer dumais 
representation meaning highdimensional space means parallels earlier high dimensional models similarity osgood shepard smith tversky 
important difference hal acquisition model relies context human similarity judgments normative data derivation meaning 
vectors representations learning history 
theory holds core notion temporal contiguity 
hal model temporal contiguity closely related local occurrence 
risk redundant contextual similarity function global occurrence local cooccurrence 
vector element measures words experience context word 
pointed years ago basic principles association best viewed context distributions associations particular events stimuli 
higher order associations global occurrence reveal structure memory language 
simply incorporating order association temporal contiguity memory model invitation underestimate effect association set model 
classical instrumental learning principles home connectionist models certain highdimensional memory models essentially instantiate high order association build semantic grammatical structure 
viewing meaning vectors learning history obviate need representations se 
disadvantage view discomfort generate cognitive scientists 
giving forces theorist realm 
approach focuses relationship learning environment contextual history learner 
failure models influential role years may hinge reliance word association methodologies theoretical limitations 
current high dimensional memory models simulate acquisition process substantial amounts experience order model psychological plausibility range cognitive phenomena 
closer relationship actual learning environment context vector behavior may reduce need reliance host memory metaphors currently employed cognitive science 
view hal representations viewed radical markman dietrich press terribly misguided glenberg 
watkins argued mistake try justify complex models trying model complex phenomena 
regardless global occurrence offers principled approach developing structured representations real environment 
may premature decide ultimate veracity views hal representations 
views relevance highdimensional memory models minimum hope state facilitate discussion nature high dimensional representations 
notion similarity psychological models memory language 
high dimensional memory models hal burgess burgess lund lund burgess lsa foltz landauer dumais similar approaches finch chater schutze conceptual representations product contexts words 
hal model distinguished number simple assumptions concepts acquired 
despite limitations range cognitive phenomena model applied spans basic word recognition meaning retrieval lund semantic dyslexia buchanan grammatical effects burgess lund meaning emotional connotation burgess lund sentence discourse comprehension burgess see foltz landauer dumais 
hal model focused nature representations processing issues 
review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
memory matrix slice time concept acquisition process 
advantage representational issues explored independently processing constraints 
drawback course evaluate interaction 
currently exceptions research hal model 
process acquisition chapter affords look number important issues role associations learning process categorical knowledge formed simpler constructs burgess 
hal representations incorporated mathematical memory processing model asymmetries burgess lund 
furthermore results global cooccurrence mechanism compare favorably neural net implementations earlier 
ability separate computational model representational processing components provide set real valued meaning vectors process provides initiative rethinking host important issues nature similarity representational modularity computational model representations grounded environment 
hal model proposed model initial bottom component meaning activation 
higher level meaning problem solving may scope model previously thought 
despite range problems hal model applied unanswered exciting questions 
important extent global occurrence distributed representations account higher level cognition model expanded encounter plausible environment just language input 
clear hal focus context beneficial continue provide insights contextually dynamic form mental representations role cognitive processing 
medin 

birds feather flock similarity judgments semantically rich stimuli 
journal memory language 
berwick 

learning word meanings examples 
waltz ed semantic structures advances natural language processing pp 

hillsdale nj lawrence erlbaum associates 

cognitive basis linguistic structures 
hayes ed cognition development language pp 
new york john wiley sons 
buchanan burgess lund 

semantic neighborhoods modeling deep dyslexia 
brain cognition 


modelling lexical decision corpus derived semantic representations connectionist network 
unpublished manuscript 
burgess 

simple associations building blocks language modeling meaning memory hal model 
behavior research methods instruments computers 
burgess 
press 
developing semantics proper names 
proceedings cognitive science society pp 
xx xx 
hillsdale lawrence erlbaum associates burgess 
press 
representing proper names objects common semantic space computational model 
brain cognition 
burgess 

computational model syntactic ambiguity lexical process 
proceedings tenth annual cognitive science society meeting pp 

hillsdale nj lawrence erlbaum associates 
burgess lund 

explorations context space words sentences discourse 
discourse processes 
burgess lund 

multiple constraints syntactic ambiguity resolution connectionist account psycholinguistic data 
proceedings cognitive science society pp 

hillsdale lawrence erlbaum associates burgess lund 

modeling cerebral asymmetries semantic memory highdimensional semantic space 

eds right hemisphere language comprehension perspectives cognitive neuroscience 
hillsdale review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
lawrence erlbaum associates burgess lund 

modelling parsing constraints high dimensional context space 
language cognitive processes 
burgess lund 

representing words emotional connotation highdimensional memory space 
proceedings cognitive science society pp 

hillsdale lawrence erlbaum associates burgess lund 
november 
examining issues developmental psycholinguistics high dimensional memory model society meeting philadelphia pa burgess tanenhaus hoffman 

semantic effects syntactic ambiguity resolution 
proceedings cognitive science society pp 

hillsdale lawrence erlbaum associates chater levy 

bootstrapping word boundaries bottom corpus approach speech segmentation 
cognitive psychology 
burgess richards pollock 

semantic associative priming cerebral hemispheres words words don places 
brain language 
chomsky 

aspects theory syntax 
press cambridge 
ma 
clifton ferreira 

ambiguity context 
language cognitive processes 
collins loftus 

theory semantic processing 
psychological review 
collins quillian 

retrieval time semantic memory 
journal verbal learning verbal behavior 
collins quillian 

language user 
tulving donaldson eds organization memory 
new york academic press 
cottrell 

model lexical access ambiguous words 
small cottrell tanenhaus eds lexical ambiguity resolution comprehension human language pp 

los altos ca morgan kaufmann publishers 
cummins 

representations targets attitudes 
cambridge ma mit press 
burgess 
february 
semantic priming effects patients left neglect 
international neuropsychological society tx 
deerwester dumais furnas landauer harshman 

indexing latent semantic analysis 
journal american society information science 


structure associations language thought pp 

baltimore johns hopkins press 
dyer 

distributed symbol formation processing connectionist networks 
journal experimental theoretical artificial intelligence 
edelman 

representation similarity object discrimination 
neural computation 
elman 

finding structure time 
cognitive science 
ferreira clifton 

independence syntactic processing 
journal memory language 
finch chater 

syntactic categories unsupervised learning 
proceedings fourteenth annual meeting cognitive science society 
hillsdale lawrence erlbaum associates fischler 

semantic facilitation association lexical decision task 
memory cognition 
foltz 

latent semantic analysis text research 
behavior research methods instruments computers 
frazier 

comprehending sentences syntactic parsing strategies 
ph thesis university connecticut 
indiana university linguistics club 
frazier clifton 

cambridge ma mit press 
frazier fodor 

sausage machine new stage parsing model 
cognition 
gallant 

practical approach representing context performing word sense disambiguation neural networks 
neural computation 
gentner markman 

effects memory 
psychological science 
gernsbacher 

language comprehension review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
structure building 
hillsdale lawrence erlbaum associates glenberg 

memory 
behavioral brain sciences glucksberg 

understanding metaphorical comparisons similarity 
psychological review hinton mcclelland rumelhart 
distributed representations 
rumelhart mcclelland pdp research group parallel distributed processing explorations microstructure cognition volume foundations pp 

cambridge mit press 
hinton shallice 

attractor network investigations acquired dyslexia 
psychological review 
komatsu 

views conceptual structure 
psychological bulletin 
krumhansl 

concerning applicability geometric models similarity data interrelationship similarity spatial density 
psychological review 
labov 

principles linguistic methodology 
language society 
lakoff 

cognitive semantics 
landauer dumais 
november 
memory model reads encyclopedia passes vocabulary test 
society 
landauer dumais 

solution plato problem latent semantic analysis theory acquisition induction representation knowledge 
psychological bulletin 
lund burgess 

producing high dimensional semantic spaces lexical cooccurrence 
behavior research methods instrumentation computers 
lund burgess 

semantic associative priming high dimensional semantic space 
proceedings cognitive science society pp 

hillsdale lawrence erlbaum associates lund burgess 

semantic associative word relationships high dimensional semantic space 
proceedings cognitive science society pp 

hillsdale lawrence erlbaum associates lupker 

semantic priming association second look 
journal verbal learning verbal behavior 
macdonald 

probabilistic constraints syntactic ambiguity resolution 
language cognitive processes 
macdonald pearlmutter seidenberg 

lexical nature syntactic ambiguity resolution 
psychological review 
markman dietrich 
press 
defense representation 
masson 

distributed memory model semantic priming 
journal experimental psychology learning memory cognition 
maxwell 

singer 
new york ny popular library 
mcclelland kawamoto 

mechanisms sentence processing assigning roles constituents 
rumelhart mcclelland pdp research group parallel distributed processing explorations microstructure cognition volume psychological biological models pp 

cambridge mit press 


automatic semantic similarity priming 
journal experimental psychology learning memory cognition 
de sa seidenberg 

role correlated properties computing lexical concepts 
journal experimental psychology general 
medin goldstone gentner 

respects similarity 
psychological review 
miller 

organization lexical memory word associations sufficient 
waugh eds pathology memory pp 

new york academic press 
miller charles 

contextual correlates semantic similarity 
language cognitive processes 
neely 

semantic priming effects visual word recognition selective review current findings theories 
besner humphreys eds basic processes reading visual word recognition pp 

hillsdale lawrence erlbaum associates nosofsky 

stimulus bias asymmetric similarity classification 
cognitive psychology 
osgood 

ease individual relation polarization attitudes review 
cognitive dynamics conceptual change humans machines 
dietrich markman eds 
culture 
journal social psychology 
osgood 

nature measurement meaning 
psychological bulletin 
osgood 

exploration semantic space personal diary 
journal social issues 
osgood tannenbaum 

measurement meaning 
urbana university illinois press 
palermo jenkins 

word association norms grade school college 
minneapolis mn university minnesota press 


limits occurrence tools theories language research 
discourse processes 
plaut shallice 

connectionist modelling cognitive neuropsychology case study 
hove england lawrence erlbaum associates rayner carlson frazier 

interaction syntax semantics sentence processing eye movements analysis semantically biased sentences 
journal verbal learning verbal behavior 
rips shoben smith 

semantic distance verification semantic relations 
journal verbal learning verbal behavior 
rumelhart smolensky mcclelland hinton 

schemata sequential thought processes pdp models 
rumelhart mcclelland pdp research group parallel distributed processing explorations microstructure cognition volume psychological biological models pp 

cambridge mit press 
schutze 

dimensions meaning 
proceedings supercomputing pp 
new york association computing machinery 

ed 

pathfinder associative networks studies knowledge organizations 
norwood ablex pub 
seidenberg mcclelland 

distributed developmental model word recognition naming 
psychological review 
schank abelson 

scripts plans goals understanding inquiry human knowledge structures 
hillsdale nj lawrence erlbaum associates shelton martin 

semantic automatic semantic priming 
journal experimental psychology learning memory cognition 
shepard 

fully connectionism activated 
sources excitation inhibition 
behavioral brain sciences 
skinner 

verbal behavior 
crofts new york 
smith shoben rips 

structure process semantic memory featural model semantic decisions 
psychological review 
spence owens 

lexical cooccurrence association strength 
journal psycholinguistic research 
tanenhaus carlson 

lexical structure language comprehension 
marslen wilson ed lexical representation process pp 

cambridge ma mit press 
taraban mcclelland 

constituent attachment thematic role expectations 
journal memory language 
trueswell tanenhaus 

semantic influences parsing thematic role information syntactic ambiguity resolution 
journal memory language 
trueswell tanenhaus kello 

verb specific constraints sentence processing separating effects lexical preference garden paths 
journal experimental psychology learning memory cognition 
tversky 

features similarity 
psychological review 
tversky gati 
studies similarity 
rosch lloyd eds cognition categorization pp 

hillsdale lawrence erlbaum associates tversky kahneman 

judgment uncertainty heuristics biases 
science 
watkins 

obfuscation memory 
american psychologist 
