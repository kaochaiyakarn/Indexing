proceedings algorithmic learning theory 
theoretical views boosting applications robert schapire labs gamma research shannon laboratory park avenue room florham park nj usa www research att com schapire 
boosting general method improving accuracy learning algorithm 
focusing primarily adaboost algorithm briefly survey theoretical boosting including analyses adaboost training error generalization error connections boosting game theory methods estimating probabilities boosting extensions adaboost multiclass classification problems 
empirical applications described 
background boosting general method attempts boost accuracy learning algorithm 
kearns valiant pose question weak learning algorithm performs just slightly better random guessing valiant pac model boosted arbitrarily accurate strong learning algorithm 
schapire came provable polynomial time boosting algorithm 
year freund developed efficient boosting algorithm optimal certain sense suffered certain practical drawbacks 
experiments early boosting algorithms carried drucker schapire simard ocr task 
adaboost adaboost algorithm introduced freund schapire solved practical difficulties earlier boosting algorithms focus :10.1.1.32.8918
pseudocode adaboost fig 
slightly generalized form schapire singer :10.1.1.156.2440
algorithm takes input training set ym belongs domain instance space label label set assume gamma discuss extensions multiclass case 
adaboost calls weak base learning algorithm repeatedly series rounds main ideas algorithm maintain distribution set weights training set 
weight distribution training example round denoted 
initially weights set equally round weights incorrectly classified ym gamma initialize train weak learner distribution get weak hypothesis choose ff update exp gammaff normalization factor chosen distribution output final hypothesis sign ff fig 

boosting algorithm adaboost 
examples increased weak learner forced focus hard examples training set 
weak learner job find weak hypothesis appropriate distribution simplest case range binary restricted gamma weak learner job minimize error ffl pr id weak hypothesis received adaboost chooses parameter ff intuitively measures importance assigns deliberately left choice ff unspecified 
binary typically set ff ln gamma ffl ffl choosing ff follows 
distribution updated rule shown 
final hypothesis weighted majority vote weak hypotheses ff weight assigned analyzing training error basic theoretical property adaboost concerns ability reduce training error 
specifically schapire singer generalizing theorem freund schapire show training error final hypothesis bounded follows jfi gj exp gammay ff sign :10.1.1.156.2440:10.1.1.32.8918
inequality follows fact gammay 
equality proved straightforwardly unraveling recursive definition eq 
suggests training error reduced rapidly greedy way choosing ff round minimize exp gammaff case binary hypotheses leads choice ff eq 
gives bound training error ffl gamma ffl gamma fl exp gamma fl ffl gamma fl bound proved freund schapire :10.1.1.32.8918
weak hypothesis slightly better random fl bounded away zero training error drops exponentially fast 
bound combined bounds generalization error prove adaboost boosting algorithm sense efficiently convert weak learning algorithm generate hypothesis weak edge distribution strong learning algorithm generate hypothesis arbitrarily low error rate sufficient data 
eq 
points fact heart adaboost procedure finding linear combination weak hypotheses attempts minimize exp gammay exp gammay ff essentially round adaboost chooses calling weak learner sets ff add term accumulating weighted sum weak hypotheses way sum exponentials maximally reduced 
words adaboost doing kind steepest descent search minimize eq 
search constrained step follow coordinate directions identify coordinates weights assigned weak hypotheses 
schapire singer discuss choice ff case real valued binary :10.1.1.156.2440:10.1.1.156.2440
case interpreted confidence rated prediction sign predicted label magnitude jh gives measure confidence 
generalization error freund schapire showed bound generalization error final hypothesis terms training error size sample vc dimension weak hypothesis space number rounds boosting :10.1.1.32.8918
specifically techniques baum haussler show generalization error high probability pr td pr delta denotes empirical probability training sample 
bound suggests boosting overfit run rounds large 
fact happen 
early experiments authors observed empirically boosting overfit run thousands rounds :10.1.1.49.2457
observed adaboost continue drive generalization error long training error reached zero clearly contradicting spirit bound 
instance left side fig 
shows training test curves running boosting top quinlan decision tree learning algorithm letter dataset 
response empirical findings schapire bartlett gave alternative analysis terms margins training examples 
margin example defined ff jff number gamma positive correctly classifies example 
magnitude margin interpreted measure confidence prediction 
schapire proved larger margins training set translate superior upper bound generalization error 
specifically generalization error pr theta margin high probability 
note bound entirely independent number rounds boosting 
addition schapire proved boosting particularly aggressive reducing margin quantifiable sense concentrates examples smallest margins positive negative 
boosting effect margins seen empirically instance right side fig 
shows cumulative distribution margins training examples letter dataset 
case error cumulative distribution rounds margin fig 

error curves margin distribution graph boosting letter dataset reported schapire 
left training test error curves lower upper curves respectively combined classifier function number rounds boosting 
horizontal lines indicate test error rate base classifier test error final combined classifier 
right cumulative distribution margins training examples iterations indicated short dashed long dashed hidden solid curves respectively 
training error reaches zero boosting continues increase margins training examples effecting corresponding drop test error 
attempts successful insights gleaned theory margins authors 
addition margin theory points strong connection boosting support vector machines vapnik explicitly attempt maximize minimum margin 
connection game theory behavior adaboost understood game theoretic setting explored freund schapire see grove schuurmans breiman 
classical game theory possible put person zero sum game form matrix play game player chooses row player chooses column loss row player payoff column player ij generally sides may play randomly choosing distributions rows columns respectively 
expected loss mq 
boosting viewed repeated play particular game matrix 
assume weak hypotheses binary fh entire weak hypothesis space assume finite 
fixed training set ym game matrix rows columns ij ae 
row player boosting algorithm column player weak learner 
boosting algorithm choice distribution training examples distribution rows weak learner choice weak hypothesis choice column example connection boosting game theory consider von neumann famous minmax theorem states max min mq min max mq matrix applied matrix just defined reinterpreted boosting setting shown meaning distribution examples exists weak hypothesis error gamma fl exists convex combination weak hypotheses margin fl training examples 
adaboost seeks find final hypothesis high margin examples combining weak hypotheses sense minmax theorem tells adaboost potential success weak learner exist combination weak hypotheses 
going adaboost shown special case general algorithm playing repeated games approximately solving matrix games 
shows asymptotically distribution training examples weights weak hypotheses final hypothesis game theoretic approximate minmax maxmin strategies 
estimating probabilities classification generally problem predicting label example intention minimizing probability incorrect prediction 
useful estimate probability particular label 
friedman hastie tibshirani suggested method output adaboost reasonable estimates probabilities :10.1.1.30.3515
specifically suggest logistic function estimating pr gammaf usual weighted average weak hypotheses produced adaboost 
rationale choice close connection log loss negative log likelihood model ln gamma function noted adaboost attempts minimize gammay specifically verified eq 
upper bounded eq 

addition add constant gamma ln eq 
affect minimization verified resulting function eq 
identical taylor expansions zero second order behavior near zero similar 
shown distribution pairs expectations ln gamma yf ji gammayf minimized function ln pr pr gamma reasons minimizing eq 
done adaboost viewed method approximately minimizing negative log likelihood eq 

may expect eq 
give reasonable probability estimate 
friedman hastie tibshirani adaboost logistic regression additive models 
multiclass classification methods extending adaboost multiclass case 
straightforward generalization called adaboost adequate weak learner strong achieve reasonably high accuracy hard distributions created adaboost :10.1.1.32.8918
method fails weak learner achieve accuracy run hard distributions 
case sophisticated methods developed 
generally reducing multiclass problem larger binary problem 
schapire singer algorithm adaboost mh works creating set binary problems example possible label form example correct label labels :10.1.1.156.2440
freund schapire algorithm adaboost special case schapire singer adaboost algorithm creates binary problems example correct label incorrect label form example correct label methods require additional effort design weak learning algorithm :10.1.1.156.2440:10.1.1.32.8918
different technique incorporates dietterich bakiri method error correcting output codes achieves similar provable bounds adaboost mh adaboost weak learner handle simple binary labeled data 
schapire singer give method combining boosting error correcting output codes :10.1.1.156.2440
boosting stumps boosting boosting stumps boosting fig 

comparison versus boosting stumps boosting set benchmark problems reported freund schapire :10.1.1.133.1040
point scatterplot shows test error rate competing algorithms single benchmark 
coordinate point gives test error rate percent benchmark coordinate gives error rate boosting stumps left plot boosting right plot 
error rates averaged multiple runs 
experiments applications practically adaboost advantages 
fast simple easy program 
parameters tune number round 
requires prior knowledge weak learner flexibly combined method finding weak hypotheses 
comes set theoretical guarantees sufficient data weak learner reliably provide moderately accurate weak hypotheses 
shift mind set learning system designer trying design learning algorithm accurate entire space focus finding weak learning algorithms need better random 
hand caveats certainly order 
actual performance boosting particular problem clearly dependent data weak learner 
consistent theory boosting fail perform insufficient data overly complex weak hypotheses weak hypotheses weak 
boosting especially susceptible noise :10.1.1.131.1931
adaboost tested empirically researchers including 
instance freund schapire tested adaboost set uci benchmark datasets weak learning algorithm algorithm finds best decision stump single test decision tree :10.1.1.133.1040
results experiments shown fig 

number classes adaboost sleeping experts rocchio naive bayes prtfidf number classes adaboost sleeping experts rocchio naive bayes prtfidf fig 

comparison error rates adaboost text categorization methods naive bayes probabilistic tf idf rocchio sleeping experts reported schapire singer 
algorithms tested text corpora reuters newswire articles left ap newswire headlines right varying numbers class labels indicated axis 
seen boosting weak decision stumps usually give results boosting generally gives decision tree algorithm significant improvement performance 
set experiments schapire singer boosting text categorization tasks 
weak hypotheses test presence absence word phrase 
results experiments comparing adaboost methods shown fig 

nearly experiments performance measures tested boosting performed significantly better methods tested 
boosting applied text filtering ranking problems classification problems arising natural language processing 
final hypothesis produced adaboost instance decision tree weak learning algorithm extremely complex difficult comprehend 
greater care human understandable final hypothesis obtained boosting 
cohen singer showed design weak learning algorithm combined adaboost results final hypothesis consisting relatively small set rules similar generated systems ripper irep rules 
cohen singer system called slipper fast accurate produces quite compact rule sets 
freund mason showed apply boosting learn generalization decision trees called alternating trees 
algorithm produces single alternating tree ensemble trees obtained running adaboost top decision tree learning algorithm 
hand learning algorithm achieves error rates comparable ensemble trees 
nice property adaboost ability identify outliers examples mislabeled training data inherently ambiguous hard categorize 
adaboost focuses weight hardest examples examples highest weight turn outliers 
fig 

sample examples largest weight ocr task reported freund schapire :10.1.1.133.1040
examples chosen rounds boosting top line rounds middle rounds bottom 
underneath image line form label example labels get highest second highest vote combined hypothesis point run algorithm corresponding normalized scores 
example phenomenon seen fig 
taken ocr experiment conducted freund schapire :10.1.1.133.1040
number outliers large emphasis placed hard examples detrimental performance adaboost 
demonstrated convincingly dietterich :10.1.1.131.1931
friedman suggested variant adaboost called gentle adaboost puts emphasis outliers :10.1.1.30.3515
freund suggested algorithm called brownboost takes radical approach de emphasizes outliers clear hard classify correctly 
algorithm adaptive version freund boost majority algorithm 
schapire drifting games reveal interesting new relationships boosting brownian motion repeated games raising new open problems directions research 

steven abney robert schapire yoram singer 
boosting applied tagging pp attachment 
proceedings joint sigdat conference empirical methods natural language processing large corpora 

peter bartlett 
sample complexity pattern classification neural networks size weights important size network 
ieee transactions information theory march 

eric bauer ron kohavi 
empirical comparison voting classification algorithms bagging boosting variants 
machine learning appear 

eric baum david haussler 
size net gives valid generalization 
neural computation 

bernhard boser isabelle guyon vladimir vapnik 
training algorithm optimal margin classifiers 
proceedings fifth annual acm workshop computational learning theory pages 

leo breiman 
arcing edge 
technical report statistics department university california berkeley 

leo breiman 
prediction games arcing classifiers 
technical report statistics department university california berkeley 

leo breiman 
arcing classifiers 
annals statistics 

william cohen 
fast effective rule induction 
proceedings twelfth international conference machine learning pages 

william cohen yoram singer 
simple fast effective rule learner 
proceedings sixteenth national conference artificial intelligence 

corinna cortes vladimir vapnik 
support vector networks 
machine learning september 

thomas dietterich 
experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning appear 

thomas dietterich bakiri 
solving multiclass learning problems error correcting output codes 
journal artificial intelligence research january 

harris drucker corinna cortes 
boosting decision trees 
advances neural information processing systems pages 

harris drucker robert schapire patrice simard 
boosting performance neural networks 
international journal pattern recognition artificial intelligence 

yoav freund 
boosting weak learning algorithm majority 
information computation 

yoav freund 
adaptive version boost majority algorithm 
proceedings twelfth annual conference computational learning theory 

yoav freund raj iyer robert schapire yoram singer 
efficient boosting algorithm combining preferences 
machine learning proceedings fifteenth international conference 

yoav freund mason 
alternating decision tree learning algorithm 
machine learning proceedings sixteenth international conference 
appear 

yoav freund robert schapire 
experiments new boosting algorithm 
machine learning proceedings thirteenth international conference pages 

yoav freund robert schapire 
game theory line prediction boosting 
proceedings ninth annual conference computational learning theory pages 

yoav freund robert schapire 
decision theoretic generalization online learning application boosting 
journal computer system sciences august 

yoav freund robert schapire 
adaptive game playing multiplicative weights 
games economic behavior appear 

jerome friedman trevor hastie robert tibshirani 
additive logistic regression statistical view boosting 
technical report 

johannes furnkranz gerhard widmer 
incremental reduced error pruning 
machine learning proceedings eleventh international conference pages 

adam grove dale schuurmans 
boosting limit maximizing margin learned ensembles 
proceedings fifteenth national conference artificial intelligence 

satoshi shirai 
decision trees construct practical parser 
machine learning 

jeffrey jackson mark craven 
learning sparse perceptrons 
advances neural information processing systems pages 

michael kearns leslie valiant 
learning boolean formulae finite automata hard factoring 
technical report tr harvard university aiken computation laboratory august 

michael kearns leslie valiant 
cryptographic limitations learning boolean formulae finite automata 
journal association computing machinery january 

richard maclin david opitz 
empirical evaluation bagging boosting 
proceedings fourteenth national conference artificial intelligence pages 

mason peter bartlett jonathan baxter 
direct optimization margins improves generalization combined classifiers 
technical report systems engineering australian national university 

merz murphy 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 

quinlan 
bagging boosting 
proceedings thirteenth national conference artificial intelligence pages 

ross quinlan 
programs machine learning 
morgan kaufmann 

robert schapire 
strength weak learnability 
machine learning 

robert schapire 
output codes boost multiclass learning problems 
machine learning proceedings fourteenth international conference pages 

robert schapire 
drifting games 
proceedings twelfth annual conference computational learning theory 

robert schapire yoav freund peter bartlett wee sun lee 
boosting margin new explanation effectiveness voting methods 
annals statistics october 

robert schapire yoram singer 
improved boosting algorithms confidence rated predictions 
proceedings eleventh annual conference computational learning theory pages 
appear machine learning 

robert schapire yoram singer 
boostexter boosting system text categorization 
machine learning appear 

robert schapire yoram singer amit singhal 
boosting rocchio applied text filtering 
sigir proceedings st annual international conference research development information retrieval 

holger schwenk bengio 
training methods adaptive boosting neural networks 
advances neural information processing systems pages 

valiant 
theory learnable 
communications acm november 

vladimir vapnik 
nature statistical learning theory 
springer 

