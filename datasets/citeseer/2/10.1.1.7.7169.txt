feature selection clustering filter solution dash choi peter scheuermann huan liu dept elect comp eng dept comp sci eng northwestern university arizona state university evanston il tempe az ece northwestern edu asu edu processing applications large number dimensions challenge kdd community 
feature selection effective dimensionality reduction technique essential pre processing method remove noisy features 
literature methods proposed feature selection clustering 
methods wrapper techniques require clustering algorithm evaluate candidate feature subsets 
wrapper approach largely unsuitable real world applications due heavy reliance clustering algorithms require parameters number clusters due lack suitable clustering criteria evaluate clustering different subspaces 
propose filter method independent clustering algorithm 
proposed method observation data clusters different point point distance histogram data clusters 
propose entropy measure low data distinct clusters high 
entropy measure suitable selecting important subset features invariant number dimensions affected quality clustering 
extensive performance evaluation synthetic benchmark real datasets shows effectiveness 
real world applications deal high dimensional data 
feature selection chooses important original features effective dimensionality reduction technique 
important feature learning task defined removal degrades learning accuracy 
removing unimportant features data sizes reduce learning accuracy comprehensibility improve 
learning supervised unsupervised 
supervised learning class label specified instance unsupervised learning uses class label 
literature feature selection classification supervised learning task vast see review 
hand feature selection methods clustering unsupervised task 
arguably reason gap research due fact easier select features classification clustering 
feature selection clustering task selecting important features underlying clusters 
methods proposed wrapper methods feature selection :10.1.1.30.9202
wrapper method feature selection evaluates candidate feature subsets learning algorithm uses selected features efficient learning 
term wrapper extensively feature selection classification 
clustering wrapper method evaluates candidate feature subsets clustering algorithm 
example means clustering algorithm evaluation em expectation maximization evaluation 
wrapper methods classification disadvantages lack robustness different learning algorithms preferable applications accuracy important criterion 
classification quantifiable way evaluating accuracy generally acceptable criterion estimate accuracy clustering see partial list clustering criteria 
matters worse feature selection clustering requires evaluation functions able distinguish clustering different subspaces 
limitations wrapper methods clustering disadvantageous 
propose evaluate filter method feature selection 
filter method definition independent clustering algorithms completely avoids issue lack unanimity choice clustering criterion 
proposed method observation data clusters different pointto point distance histogram data clusters 
ing observation propose entropy measure low data distinct clusters high 
entropy measure suitable selecting important subset features invariant number dimensions affected quality clustering 
experiments real benchmark synthetic datasets show effectiveness proposed method 
properties feature selection assume dataset consists data points instances dimensions features 
shall denote th data point th feature value th point th feature denotes distance points denotes th cluster number clusters 
discuss important properties unsupervised data affect feature selection 
importance features clustering typically gathering information particular application tends gather information possible considering significance feature underlying clusters 
essential data mining pre processing task remove unwanted features performing kdd tasks clustering 
show example synthetic data dimensional spaces respectively 
total points dimensions clusters dimensions cluster having points 
values features follow gaussian distribution clusters values feature define cluster follow uniform distribution 
take features clusters unnecessarily complex see clusters visualize feature say 
features shows formed clusters 
selecting features reduces dimensionality data forming separated clusters 
basically goal select important original features clustering reducing data size computation time time improving knowledge discovery performance comprehensibility 
single dimensional dataset clusters formed single feature takes values separate ranges 
multi dimensional dataset clusters formed combination feature values single features may define clusters 
noted distinct scenarios 
scenario single feature defines clusters independently 
consider feature uniformly distributed takes values separate ranges 
clearly seen important defining clusters 
scenario individual features define clusters correlated features 
consider features uniformly distributed 
note define clusters correlation define distinct clusters 
feature selection algorithm clustering take account scenarios selecting features 
distance histogram show histogram point point distances datasets clusters clusters 
histogram buckets 
distance normalized interval bucket determined multiplying normalized value order select bucket 
counter maintained bucket incrementing time distance belongs particular bucket 
histogram plots plotted point corresponds single bucket 
value plotted point bucket number value frequency counter value bucket important distinction histograms histogram data clusters predictable shape similar bell shape see 
histogram data clusters different distribution see dataset clusters early buckets distances intra cluster ones inter cluster 
typically dataset consists clusters majority intra cluster distances smaller majority inter cluster distances 
observation true wide range data types 
clusters distinct intra cluster inter cluster distances quite distinguishable 
propose method doing clustering distinguish data clusters data clusters 
section introduce method applied section select subset important features 
distance entropy measure efficient calculation stated want develop method distinguish data clusters data clusters 
entropy theory says entropy system measures disorder system 
mathematically gamma log gamma gamma log gamma independent correlated 
effect features clustering data clusters histogram data clusters histogram frequency distances intra cluster distances inter cluster bucket frequency bucket 
distance histograms data clusters probability density point 
second term expression inside summation expression symmetric 
probability point equal uncertain outcome entropy maximum 
happen data points uniformly distributed feature space 
hand data formed clusters uncertainty low entropy 
entropy distinguish data clusters data clusters 
know probability points propose proxy method estimate entropy 
goal method assign low entropy inter cluster distances assign higher entropy noisy distances 
straight forward method obtained substituting probability distance gamma ij log ij gamma ij log gamma ij ij normalized distance range instances normalized range 
entropy minimum maximum distance mean distance 
plot showing entropy distance relationship looks similar dotted curve 
euclidean measure distances manhattan 
extent works distinguishing data clusters data clusters considering stated goal suffers drawbacks 
mean distance meeting point sides left right plot inter cluster distance assigned highest entropy 
entropy increases rapidly small distances assigning different entropy values intra cluster distances 
summary measure fulfill stated goal assigning small entropy intra inter cluster distances 
second drawback easily overcome incorporating coefficient fi equation exponential function place logarithmic function 
regarding drawback set meeting point separate intra cluster inter cluster distances accurately 
considering propose method ij ij exp fi ij exp fi ij exp fi gammad ij exp fi gamma ij ij normalized range 
setting fi shows increasing fi value decrease entropy 
effective fi pos vary fi vary entropy ij distance ij entropy distance 
relationship entropy distance varying fi values itive value 
fi lead second drawback 
large fi fail distinguish intra inter cluster distances 
small fi assign sufficiently small entropy intra intercluster distances 
different fi experimented works set value shown bold curve 
varying effect shifting meeting point sides entropy distance plot see 
value affects total entropy 
histogram plots easy see set properly distinguish data data clusters 
method propose observation easy accurate estimate range intra cluster inter cluster distances 
due fact intra cluster distances typically occupy lowest portion complete range distances 
describe easy robust estimation intra cluster distance range 

starting bin histogram reject bins frequencies low frequency bin higher frequency occurs 

starting bin frequency find bin highest frequency range 
calculate value setting unknowns equation range ij follows set small value set fi set distance ij corresponding bin step step useful dimensionality large 
shown data large points uniformly distributed distances approach maximum distance histogram plot effectively shifted right forcing frequencies data clusters data cluster ij range histogram plot entropy cut entropy distance plot histogram frequency entropy entropy distance plot histogram plot entropy histogram frequency ij 
illustrating effectiveness proposed method lower bins small 
effective estimation reject initial bins frequency small frequency experiments set total frequency 
step tested various results remain insensitive set initial distance range 
reason data clusters intra cluster distances small maximum intra cluster frequency typically occur small distance 
step set maximum entropy 
setting fi discussed earlier section 
explain procedure pictorially 
datasets cluster taken 
entropy distance relationship plot histogram plot data clusters distracted entropy cut line label range explained sub section 
super imposition bucket numbers histogram plot converted range 
step required low dimensional data 
step yields bin corresponding maximum frequency range step calculates 
information total entropy data clusters calculated 
similarly shows estimation data clusters 
information calculated larger 
notable difference figures case data clusters entropy low bins distances having considerably high frequency counts signifying intra intercluster distances data clusters entropy large bins considerably high frequency counts 
proposed method able exploit shape histogram plots assign low entropy data clusters high entropy 
fast method number distance calculations required calculate entropy subset features gamman number points computation required calculate distance pair points taken unit 
obviously quadratic run times impractical large datasets 
faster method calculate entropy described 
basic idea follows 
previous sections major concern assign low entropy intra cluster inter cluster distances 
words approach tries minimize entropy data clusters 
typically large portion distances low entropy total significant contribution entropy 
considering distances having entropy higher threshold entropy thres able find range distance rc higher entropy thres spreads equally sides meeting point entropy distance outside range set low constant value ce equal thres illustrates observation 
range distance rc shown having entropy minimum threshold thres set maximum entropy 
note distances fall range rc approximately entropy calculations avoided 
total entropy little affected distances outside rc replace entropy calculations constant entropy ce value equal thres exploit observation propose algorithm grid blocks data space partitioned equal size grid blocks dividing dimension axis parallel partitions 

data size fit main memory read data main memory separate 
large data residing disk create index data read grid blocks 

pair blocks minimum distance calculated stored memory disk depending size 

point entropy distance calculated block determined 
remaining blocks minimum distance block obtained 
distance falls range rc entropy candidate point points block calculated usual way 
points block entropy pre assigned ce high dimensional data grid blocks sparse empty consider blocks considerable number data points 
way number grid blocks grow exponential manner curse high dimensional partitioning 
uniformly distributed data grid blocks may sparse 
decide go grid block computation total number sparse blocks contain threshold number points full computation done 
large datasets fast method prohibitive sampling 
sampling works entropy measure requires underlying cluster structure retained sample particularly 
feature selection algorithm previous section discussed distinguish data data clusters entropy measure 
section propose feature selection algorithm measure 
feature selection process main steps search generation evaluation subsets features 
entropy measure directly evaluation technique compare subsets features 
possible independent cardinality subsets comparison subspaces define clusters 
irrespective cardinalities subsets low entropy output subset defining formed clusters high entropy output 
regarding important step feature selection search method efficiency measured optimality 
context optimality defined subset entropy minimum 
literature particularly classification search techniques proposed 
see list techniques 
prominent methods exhaustive heuristic random hybrid techniques 
exhaustive methods guarantee optimality impractical due exponential complexity number features 
random methods generate subsets randomly anytime algorithm return best subset point time asymptotically approach optimal subset 
variation pure random methods probabilistic method probability generating subset varies rules 
examples rules genetic algorithm simulated annealing 
commonly heuristic methods feature selection forward backward selection combination 
forward selection method finds best feature features selected features finds best feature 
subset outputs entropy output best subset 
backward selection algorithm opposite forward selection algorithm 
search techniques applied feature selection 
goal propose evaluation method correctly compare data data clusters go details search methods 
experiments forward selection algorithm compare output exhaustive method 
forward selection algorithm loops outer loop iterates total number iris genes singular 
subspace showing clusters features times selects best subset features inner loop selects best feature iteration outer loop 
experimental evaluation evaluation unsupervised learning task clustering arguably difficult supervised task classification simple reason clustering commonly accepted evaluation approach classification accuracy classifier commonly accepted evaluation measure 
feature selection clustering additional disadvantage clusters depend dimensionality selected features feature subset may clusters may incompatible formed different feature subsets 
considering aspects best way evaluating feature selection method clustering check correctness selected features selected features match actual important features 
evaluation criterion evaluate proposed method synthetic datasets know important features 
evaluate benchmark real datasets important features known visualization 
run forward selection method validate result running exhaustive search method check selected optimal subset minimum entropy 
stated results forward selection matches exhaustive method forward selection method outputs optimal subset 
compare wrapper method proposed filter method 
experiments follow guidelines section set parameters fi synthetic datasets synthetic datasets generated simple data generation algorithm run web site re kr choi choi proj gen data html 
clusters defined distributions gaussian uniform 
features define gaussian cluster feature mean standard deviation randomly chosen synthetic iris entropy features entropy features genes singular pts entropy features entropy features 
results range 
similarly uniformly distributed cluster lower higher ranges randomly chosen range 
noisy point features uniformly distributed 
information kept configuration file contains information number clusters number points cluster number noisy points 
example synthetic dataset synthetic features points clusters noise 
features ff define clusters ff noise 
taken adding ff 
shows ff minimum entropy 
re checked exhaustive search entropy minimum ff 
datasets datasets varying number features clusters generated similarly 
experiments conducted features clusters 
half features ff important half ffm noisy features 
example data features ff best subset 
data noisy points 
theta datasets features clusters features clusters features clusters datasets results correct remaining datasets subset ff lowest entropy 
results datasets correct selected features important missed important feature 
benchmark real datasets iris dataset popularly testing clustering classification algorithms taken uci ml repository 
contains classes instances class refers type iris plant 
class linearly separable linearly separable see 
known petal length petal width features important underlying clusters high correlation classes 
performing experiments removed class labels 
plot entropy values subsets features selected 
subset consisting important features ff entropy minimum correctly selected algorithm 
genes dataset taken publicized clustering software cluto available web site www users cs umn edu karypis cluto 
dataset called genes yeast genes data points described profiles features 
run data shows minimum entropy subset ff see 
checked data shown showed clusters dimensional subspace 
exhaustive search returned subset best 
help domain experts process finding significance clusters subspace 
experiment shows proposed method uncover clusters datasets previously unknown clusters hidden subspaces features 
singular points parallel manipulator obtained data experiment conducted robotics laboratory test singularity points parallel manipulator 
parallel manipulator closed loop mechanism moving platform connected base serial kinematics chains legs 
singularity point inside workspace manipulator moves random direction joints locked 
order avoid approach cluster find paths empty space clusters 
data shown example scenario section clusters defined correlation features individual features 
result shows entropy subset consisting features smaller entropy subset see 
comparison wrapper method compared results wrapper method described 
data points clustered partitional clustering algorithm em expectation maximization evaluated invariant criterion trace 
trace gamma chosen criterion invariant non singular linear transformation 
sw gamma gamma scatter matrix gamma gamma cluster scatter matrix th cluster total mean vector data mean vector th cluster gamma matrix transpose column vector gamma trace matrix sum diagonal elements 
larger trace gamma larger normalized distance clusters results better cluster discrimination 
order test wrapper method able select features correctly evaluate subsets exhaustively 
datasets set number clusters correctly run em 
results show example dataset clusters shown maximum trace occurs subset ff trace important subset ff lower ff iris dataset maximum trace occurs subset ff higher actual important subset ff genes dataset maximum trace occurs subset ff visualization shows clusters subspace higher trace important subset ff singular points maximum trace occurs subset ff clusters distinct ff 
sensitivity parameters parameters proposed algorithm affects outcome directly fi 
setting depends finding maximum intracluster frequency turn depends setting higher range intra cluster distances 
experiments set smallest total range distances 
experiments varied results 
note varying number clusters slightly wrapper methods results change drastically 
explain robustness proposed method follows 
subspace contains clusters typically majority intra cluster distances smaller majority inter cluster distances 
setting value robust maximum frequency time 
related earliest methods reducing dimensionality unsupervised learning feature extraction methods principal components analysis karhunen loeve transformation singular value decomposition 
methods reduce number original features create extracted features principal components original ones 
years number methods feature selection clustering proposed wrapper approach 
clustering wrapper method uses clustering algorithm evaluate candidate feature subsets 
wrapper methods categorized select features data global type just fraction data cluster local type 
global type assumes subset features important data local type assumes cluster subset important features 
examples global methods proposed method :10.1.1.30.9202
method described uses means evaluation subsets features 
em expectation maximization trace measure evaluation 
authors propose visual aids user decide optimal number features 
features ranked selected categorical data 
forward backward search techniques generate candidate subsets 
evaluate candidate subset methods measure category utility clusters applying cobweb 
authors proposed objective function choosing feature subset finding optimal number clusters document clustering problem bayesian statistical estimation framework 
examples local wrapper methods 
projected clustering proclus finds subsets features defining important cluster 
proclus finds clusters medoid considering features finds important features cluster manhattan distance 
algorithm called clique divides dimension user divisions 
starts finding dense regions clusters dimensional data works upward find dimensional dense regions candidate generation algorithm apriori 
proposed filter method evaluate feature subsets choose best subset clustering considering effect underlying clusters 
earlier methods proposed clustering wrapper methods require clustering algorithm invariant clustering criterion evaluate feature subsets 
main drawback approach lack unanimous agreement evaluating clusters 
furthermore running clustering algorithm sensitive parameters number clusters equivalent 
real world data information usually hard obtain making unusable cases 
contrast proposed method largely depends parameter range intra cluster distance easier set proposed method quite insensitive 
performed experiments benchmark synthetic real datasets results show proposed method correctly finds important subsets 
comparison wrapper method showed superior evaluation accuracy pro cobweb hierarchical clustering algorithm categorical data 
posed method 
performing clustering proposed method discover clusters subspaces priori information clusters available 
evident experimental study genes dataset 
aggarwal procopiuc wolf yu park 
fast algorithms projected clustering 
proceedings acm sigmod conference management data pages 
agrawal gehrke gunopulos raghavan 
automatic subspace clustering high dimensional data data mining applications 
proceedings acm sigmod conference management data 
agrawal srikant 
fast algorithm mining association rules 
proceedings th international conference large databases vldb santiago chile 
beyer goldstein ramakrishnan shaft 
nearest neighbor meaningful 
proceedings international conference database theory icdt pages 
blake merz 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
cheng fu zhang 
entropy subspace clustering mining numerical data 
proceedings international conference knowledge discovery data mining kdd 
dash liu 
feature selection classification 
international journal intelligent data analysis 
dash liu 
feature selection clustering 
proceedings fourth pacific asia conference knowledge discovery data mining pakdd 
devaney ram 
efficient feature selection conceptual clustering 
proceedings international conference machine learning icml pages 
duda hart 
pattern classification scene analysis chapter unsupervised learning clustering 
john wiley sons 
dy brodley 
visualization interactive feature selection unsupervised data 
proceedings international conference knowledge discovery data mining kdd pages 
fisher 
knowledge acquisition incremental conceptual clustering 
machine learning 
fukunaga 
statistical pattern recognition 
academic press 
jain dubes 
algorithm clustering data chapter clustering methods algorithms 
prentice hall advanced series 
john 
enhancements data mining process 
phd thesis department computer science stanford university march 
kaufman 
finding groups data cluster analysis 
wiley series probability mathematical statistics 
kim street menczer 
feature selection unsupervised learning evolutionary search 
proceedings acm sigkdd international conference knowledge discovery pages 

feature selection preprocessing step hierarchical clustering 
proceedings international conference machine learning icml 

feature selection incremental learning probabilistic concept hierarchies 
proceedings international conference machine learning icml 
vaithyanathan dom 
model selection unsupervised learning applications document clustering 
proceedings international conference machine learning icml pages 
