reliable group rekeying performance analysis yang richard yang steve li brian zhang simon lam department computer sciences university texas austin austin tx xli lam cs utexas edu tr june secure group communications users group share common group key 
key server sends group key authorized new users performs group rekeying group users key changes 
investigate scalability issues reliable group rekeying provide performance analysis group key management system called key trees 
rekeying join leave periodic batch rekeying improve scalability alleviate sync problems rekey messages rekey data messages 
analyses show batch rekeying achieve large performance gains 
investigate reliable multicast rekey messages proactive fec 
observe rekey transport eventual reliability soft real time requirement rekey workload sparseness property group user needs receive small fraction packets carry rekey message sent key server 
investigate tradeoffs server receiver bandwidth requirements versus group rekey interval show determine maximum number group users key server support 
emerging network applications pay view distribution digital media restricted teleconferences pay multi party games research sponsored part nsf 
ani nsa university research program 
mda 
experiments performed equipment nsf 
cda 
secure group communications model 
model protect privacy group communications symmetric group key known group users key server encrypting data traffic group users 
access group key controlled group key management system sends group key authorized new users performs group rekeying group key changes 
specifically group key management system implement types access control backward access control forward access control 
system changes group key new user joins new user able decrypt past group communications called backward access control 
similarly system current user leaves expelled system departed user able access group communications called forward access control 
implementing access control may large performance overheads limit system scalability 
backward access control implemented efficiently new group key distributed encrypting existing group key existing group users 
forward access control harder implement 
send new group key remaining group users user departed approach encrypt new group key remaining user individual key shared user key management system 
straightforward approach scalable requires key management system encrypt send new group key times group size departure 
past years approaches proposed implement scalable forward access control :10.1.1.42.4522
example key tree approach uses hierarchy keys facilitate group rekeying reduces group rekeying complexity log group size 
rekey encoding registration leave join rekey transport individual key rekey message functional components key management service shows functional components architecture group key management system 
registration component authenticates users distributes user individual key 
authenticated users send join leave requests rekey encoding component 
rekey encoding component manages keys system validates requests checking encrypted individual keys generates rekey messages sent rekey transport component delivery 
previous studies focused primarily rekey encoding component particularly processing time required rekey encoding component key server problem reliable transport group rekey messages addressed literature 
group key management system scalable design components needs scalable 
objective study investigate scalability issues components including evaluation batch rekeying algorithms improve scalability large dynamic group characterization rekey transport workload design reliable rekey transport protocol performance analysis system called 
consider registration component 
group key management system deny join leave request identity user sending request needs authenticated 
user needs register system authenticating system receive individual key 
registration authentication protocol large overheads key server bottleneck user registration rate high 
improve scalability registration component key server offload registration workload trusted registrars 
machines running registrars added removed dynamically 
different registrars different authentication protocols authenticate different sets users 
offload registration workload registrars consider workload 
detailed operations register new user please see description keystone system 
second consider rekey encoding component 
show rekeying join leave called individual rekeying key tree approach problems inefficiency sync problems rekey messages rekey data messages see section 
furthermore user join leave rate high delay needed reliably multicast rekey message may large implement individual rekeying 
improve rekey encoding efficiency alleviate sync problems rekeying periodically batch join leave requests 
idea batch rekeying proposed :10.1.1.42.4522
batch rekeying key tree explicit algorithm performance analyzed 
specification batch rekeying algorithm analyze performance evaluate benefits batch rekeying 
evaluation shows batch rekeying reduce number expensive signing operations reduce substantially bandwidth requirements server receivers 
words batch processing improve system scalability highly dynamic group 
third consider rekey transport component 
reliable transport rekey messages received attention previous 
idea fec improve reliability rekey transport discussed community keystone system protocol detail performance analyzed 
common assumption reliable multicast protocols rekey transport prior analyses reliable multicast protocols apply 
observe rekey transport special properties 
observe rekey transport eventual reliability soft real time requirement inter dependencies rekey messages rekey data messages 
second observe rekey transport workload sparseness property key server sends rekey message large block packets receiver needs receive small fraction packets 
rekey transport protocol proactive fec show reliable rekey multicast analyzed converting conventional reliable multicast sparseness property 
approach investigated key server bandwidth overhead number rounds needed transport workload rekey operation determine proactivity factor fec 
fourth consider rekey encoding rekey transport components 
simple membership model show group rekeying interval serves design parameter allows tradeoffs rekeying overheads group access delay degree forward access control vulnerability 
considering system constraints investigate choose appropriate rekey interval determine maximum number users key server support 
improve scalability reliability allow extend centralized key server distributed key servers 
performance analysis shows partitioning users active inactive groups improve system scalability 
particular distributed architectures architecture suitable applications security reliability requirements transferred application data reliable secure software transfer architecture suitable applications security requirement transferred application data secure multimedia applications 
balance organized follows 
section investigate scalability issues rekey encoding component evaluate periodic batch rekeying 
section address issues reliable rekey transport including rekey workload characterization performance analysis rekey transport 
section integrate results section section consider system performance study tradeoffs bandwidth overhead rekey interval 
extensions multiple key servers section 
section 
improving rekey encoding scalability having authenticated registrar user send join request key server 
key server receive leave requests existing users 
rekey encoding component processes requests prepare rekey messages 
discussing issues individual rekeying briefly review key tree idea 
key tree key tree directed tree node represents key 
root key tree group key shared users leaf node user individual key shared user key server 
node represents key call node key tree key node 
key nodes representing individual keys users refer user nodes 
trusted key server manages key tree user key directed path individual key key key tree 
consider group users 
example key tree shown 
group user keys key user individual key key group key auxiliary key shared group key keys change change auxiliary individual keys user nodes leave users example key tree suppose leaves group 
key server need change keys knows change change distribute changed keys remaining users group oriented rekeying strategy key server constructs rekey message traversing key tree bottom 
denotes key encrypted key referred encrypted key encryption 
receiving message user extracts encrypted keys needs 
example needs issues individual rekeying individual rekeying introduces extra delay process user requests issues 
rekey join leave hard control synchronization arise inter dependencies rekey messages rekey data messages 
synchronization achieved sync problems 
consider encryption rekey message 
user receive order decrypt encryption 
may distributed previous rekey message previous rekey message arrived user able recover new key 
consider group key distributed rekey message user 
data messages encrypted group key group key arrived user able decrypt data messages 
result sync problems rekey message delivery delay high join leave requests happen frequently user may need keep old group keys buffer large amount rekey data messages decrypt 
second individual rekeying inefficient 
authentication purpose rekey message needs digitally signed prove originates key server know signing operation large computation bandwidth overheads 
snoeyink suri varghese observed independently derived time different proof know key server request forward access control required lower bound amortized number encrypted keys :10.1.1.16.7732:10.1.1.16.7732
key tree approach achieved complexity lower bound improve performance rekey encoding rekey request 
overcome limit reduce number signing operations need consider batch rekeying 
periodic batch rekeying periodic batch rekeying collects requests rekey interval batch alleviate sync problems improve efficiency 
alleviate sync problems periodic batch rekeying delays usage new group key rekey interval rekey transport guarantee high probability rekey message delivered interval see section 
performance obvious performance gain batch processing join leave requests reduces number signing operations 
number encrypted keys generated batch rekeying sum generated individual rekeying 
consider 
suppose send leave requests 
key server individually need update group key twice time new group key needs encrypted requests batch key server needs update group key 
periodic batch rekeying improves performance expense delayed group access control new user wait longer accepted group departed expelled user stay group longer 
observe group rekeying interval serves design parameter allows tradeoffs rekeying overheads group access delay degree forward access control vulnerability 
accommodate different application requirements tradeoffs performance group access control operate batch modes periodic batch rekeying key server processes join leave requests periodically batch periodic batch leave rekeying key server processes join request immediately reduce delay new user access group communications processes leave requests batch periodic batch join rekeying key server processes leave request immediately reduce exposure users departed processes join requests batch 
investigate tradeoffs section 
batch rekeying algorithms periodic batch rekeying mode key server maintains key tree slightly different key tree described section facilitate key identification strategy proposed 
particular add null nodes represent empty key nodes key tree key server maintain complete balanced key tree 
identify node key tree key server assigns integer ids tree nodes breadth search order id tree root 
rekey interval key server collects join leave requests executes marking algorithm update key tree generate rekey subtree 
objectives marking algorithm reduce number encrypted keys maintain balance updated key tree efficient users identify encrypted keys need 
marking algorithm updates key tree 
key server replaces departed users smallest ids newly joined leaves leaves joins leave joins leave new new strategy example marking algorithm users 
replacing departed users newly joined users algorithm reduces number encrypted keys 
notice departed users replaced 
user nodes key server changes null nodes see left example 
children node null nodes key server changes node null node 
hand key server replaces departed users newly joined users 
key server needs insert remaining new users 
insertion strategies investigated achieve different tradeoffs aforementioned objectives strategy 
strategy add remaining new users key server splits replaced nodes add remaining new users 
splitting newly replaced nodes add remaining new users key server splits leaf nodes left right adds new users see right example 
advantage approach reduces number encrypted keys splits replaced user nodes 
disadvantage user nodes users changed key server need provide new ids individually users addition newly joined users 
notice notification increase key server bandwidth overhead 
strategy 
strategy proposed investigated achieves smaller number encrypted keys strategy 
strategy key server creates tree new users leaf nodes tree departed user node smallest height 
strategy keep key tree balanced strategy 
hand strategy id remaining user modified key server needs provide new ids remaining user addition newly joined users 
strategy 
strategy proposed investigated designed efficient remaining users identify encrypted keys need 
strategy key server replaces null nodes ids newly joined users id node key tree user node null node 
extra joins starting user node id key server splits user node add children moves content user node left child adds new user nodes 
key server repeats process new users added key tree 
disadvantage strategy generates slightly larger number encrypted keys 
advantage strategy key server multicasts id node user node key node rekey message remaining user able independently derive id user node structure key tree modified 
explanation user id changed determines new id please see 
comparing strategies process case evaluation shows difference terms size rekey subtree small 
report analytical results strategy 
updating key tree key server copy key tree marks states key nodes duplicated key tree 
nodes marked states unchanged join leave replace 
mark states user nodes user node marked unchanged changed rules 
user node departed user marked leave node replaced marked replace 
user node marked join replacement null node split previous user node 
mark states key nodes children key node marked leave mark leave remove children 
children marked unchanged mark unchanged remove children 
children marked unchanged join mark join create virtual node contains old key key node replace unchanged children 
node leave replace child mark replace 
call pruned subtree rekey subtree observe edge rekey subtree corresponds encryption parent node encrypted child node 
detail traverse rekey subtree generate rekey message investigated section 
running complexity marking algorithm log 
benchmark shows sun ultra sparc mhz cpu marking algorithm takes ms ms 
hand benchmark running time batch rekeying algorithm boolean function minimization take tens seconds similar group sizes :10.1.1.42.4522
worst scenario analysis analyze worst scenario average scenario performance batch rekeying strategy 
analysis batch rekeying strategy 
metric number encrypted keys 
subsection show consider worst number encrypted keys rekey leave requests assuming joins batch batch rekeying large benefit 
previous discussion know forward access control rekey encoding difficult quantifying benefit batch rekeying scenario instructive 
results worst case performance cases refer interested reader 
average performance section 
consider balanced tree degree height know leaf nodes 
suppose users leave 
observe worst scenario happens departed users evenly distributed tree leaf nodes number overlapped encryptions minimum 
delving detail analysis see appendix assuming derive worst number encrypted keys ld log hand individual rekeying single departed user costs log suppose requests processed individually total ld log encrypted keys 
comparing equation observe difference ld log large benefit batch rekeying substantial 
edges rekey subtree pruned savings larger 
average scenario analysis enc denote average number encrypted keys join leave requests processed user key tree 
simplify analysis assume key tree balanced batch log denote height key tree 
assume departed users uniformly distributed tree leaf nodes 
scenario users different leave probabilities utilized improve performance example huffman type tree minimize number encrypted keys 
exploration analysis scope 
batch rekeying algorithm depends relationship analytical results depend relationship considering number times key node belongs rekey subtree derive analytical expressions average number encrypted keys see appendix enc enc enc enc dn min dn 
plot analytical results 
shows values enc wide range values 
plotted simulation results controlled achieving confidence interval analytical results analytical results match simulations indistinguishable 
observe fixed enc computed simulated enc enc analysis simulation grows linearly 
expected marking algorithm joins replace leaves rekey subtree grows linearly number joins 
fixed enc increases leaves means keys changed decreases keys pruned rekey subtree 
batch rekeying individual rekeying encrypted keys batch vs individual rekeying analytical expressions consider performance gains batch rekeying average number encrypted keys performance metric 
compares batch rekeying individual rekeying wide range values 
observe difference batch individual rekeying large 
batch rekeying generates encrypted keys individual rekeying generates encrypted keys times larger batch generates encrypted keys individual generates encrypted keys batch generates encrypted keys individual generates encrypted keys 
difference larger larger 
providing reliable rekey transport rekey subtree generated rekey encoding component sent rekey transport component delivery 
investigate issues rekey transport 
special characteristics rekey transport workload 

workload provide reliability rekey packets performance 
rekey transport workload encoding algorithm key trees know user needs receive encrypted keys path individual key new group key 
avoid overhead unicasting individually user encrypted keys key server partitions users small number subgroups consider subgroup multicast channel combines encrypted keys subgroup users rekey message may partitioned rekey packets keys fit packet multicasts rekey message users subgroup 
receiving packets rekey message user needs receive packets contain encrypted keys 
result rekey packets user needs depend encrypted keys assigned rekey packets 
investigation rekey transport workload address questions assign encrypted keys rekey subtree subgroup users packets 
assignment algorithm packets user needs receive 
average 
variance 
packets rekey message 
key assignment algorithms improve performance rekey transport protocol desired key assignment algorithm reduces number packets user needs receive 
overhead rekey transport depends users largest numbers packets receive 
desired key assignment algorithm reduces variance 
requirements consider key assignment algorithms breadth assignment bfa depth assignment dfa recursive bfa bfa 
common characteristic key assignment algorithms duplicate encrypted keys encrypted key assigned packet 
proposed investigated different algorithm called user oriented assignment uka 
advantage uka algorithm assigns encrypted keys user packet user needs receive packet receivers 
disadvantage algorithm encrypted keys duplicated packets duplications dominate bandwidth overhead especially mtu small receiver loss rates low 
bfa dfa key server traverses rekey subtree breadth depth order assigns sequentially encrypted keys packets 
horizontally scanning rekey subtree bfa collects keys different users round robin manner 
fairness user reduces variance 
hand bfa spreads keys user multiple packets increases average 
vertically tracing path dfa collects keys user goes user 
expect average smaller dfa 
shared encrypted keys assigned users processed earlier bias causes larger variance 
gain benefits bfa dfa consider bfa 
shows bfa algorithm 
algorithm starts calling bfa root root root node rekey subtree 
algorithm bfa node id node id uniquely identifies node rekey subtree 
pkt global variable denoting rekey packet 
family set containing immediate children 

create local fifo queue 
put node id 
empty pop head element head pkt space contain family node id put children sequentially put encrypted keys family pkt pkt generate new rekey packet call bfa empty pop head call bfa recursive bfa bfa algorithm better understand bfa compare bfa 
space current packet bfa behaves just bfa performance terms variance similar bfa 
current packet full new packet created continuing horizontally scanning global rekey subtree bfa bfa bfa local subtree 
bfa puts related keys reduces average value compared bfa 
illustrates basic idea bfa 
illustration bfa algorithm comparison assignment algorithms verify compared bfa dfa bfa performs terms average variance 
keys packet bfa dfa bfa avg packets average different values consider average value 
plots results bfa dfa bfa rekey subtrees users 
observe wide range values average bfa dfa smaller bfa 
bfa dfa put related keys achieve similar performance 
observe bfa dfa average users need receive packets varied wide range 
clarity shows results 
keys packet bfa dfa bfa average keys packet bfa dfa bfa stdev packets variance different values keys packet bfa dfa bfa variance consider variance 
observe dfa large variance variances bfa bfa smaller 
expected know bfa bfa treat users fairly smaller variances 
clarity shows results 
comparison purpose shows average number packets rekey message wide range values rekey subtree users 
comparing observe user needs receive small fraction packets rekey message 
refer packet bfa packets average rekey message size property user needs receive small fraction packets rekey message sparseness property 
reliable rekey transport protocol rekey workload generated rekey subtree investigate rekey transport protocol 
convention reliable multicast refer group user receiver 
determine rekey transport protocol need consider properties 
reliable multicast protocols proposed analyzed past years consider rekey transport conventional reliable multicast specific properties 
sparseness workload observe rekey transport requirements eventual reliability 
eventual reliability mean receiver able receive encrypted keys 
requirement comes inter dependencies discussed section 
soft real time requirement 
soft real time requirement mean transport rekey message finished high probability start rekey interval 
objective soft real time requirement alleviate sync problems 
address eventual reliability requirement allowing receivers send re synchronization requests recover rekey message time 
provide soft real time rekey transport proactive fec reduce recovery latency 
authors shown round proactive fec approaches reduce delivery latency 
furthermore shown hybrid approach combining fec arq significantly reduce bandwidth requirements large reliable multicast session 
authors compare benefits combining local recovery fec arq hybrid technique conclude multicast scenarios combination offers little improvement fec arq hybrid technique local recovery :10.1.1.35.7747
performance evaluation rekey transport simple round proactive fec protocol 
potential scalability problem reliable multicast protocol feedback implosion problem mechanisms tree feedback aggregation nack avoidance reduce feedback traffic 
furthermore proactive fec reliable multicast protocol rubenstein kurose towsley observed number nack packets reduced increasing proactivity factor 
proposed investigated adaptive proactive fec algorithm evaluations show number nack packets key server controlled adjusting proactivity factor rekey interval 
results assume receiver unicasts feedback packets directly key server 
send original fec packets round collect max largest round generate max fec packets multicast receivers key server protocol consider key server protocol specified 
rekey interval key server runs key assignment algorithm assign keys rekey subtree packets 
convention refer packets rekey message block block size 
generating original packets key server packets individually verifiable flow signing generates proactive fec packets proactivity factor current rekey interval 
generate fec packets key server uses reed solomon codes block size small uses tornado codes large :10.1.1.21.9363:10.1.1.21.9363:10.1.1.35.7747
advantage reed solomon code allows user recover original packets distinct packets base analysis reed solomon code 
tornado codes may require slightly higher number packets recover original packets advantages smaller encoding time may allow user recover packets recovering original packets 
multicasting original proactive fec packets key server waits duration round largest round trip time receivers collects feedbacks receivers 
feedback receiver nack packet containing number packets needs order recover packets 
round key server calculates max largest generates max new fec repair packets multicasts repair packets receivers 
process continues rekey interval 
notice possible key server may receive nack packets receivers rekey interval 
case key server considers nack packets re synchronization requests sends re synchronization packets users reliable unicast 
design objective sure scenario rarely happen discuss scenario 
strategy key server targets multicast rounds rekey interval please see 
round receive specific distinct packets done round set number distinct packets received report key server rounds receive distinct packets recover encrypted keys done round set number distinct packets received report key server receiver protocol sparseness property consider receiver protocol specified 
assume encrypted keys receiver assigned packets 
round receiver checks received packets 
receiver received packets able recover packets fec received distinct packets receiver extracts encrypted keys participate rounds 
receiver need participate rounds receive total distinct pack ets recover original packets including packets 
case receiver sets number distinct packets received reports key server 
comparison purpose shown receiver protocol receiver needs receive packets block workload sparseness property 
distinguish protocols call protocol considers sparseness property protocol second protocol seen conventional reliable multicast protocol protocol ii 
receive distinct packets construct original packets done round set number distinct packets received report key server receiver protocol ii conventional reliable multicast performance reliable rekey transport workload rekey transport rekey transport protocol analyze section performance reliable rekey transport 
determine guidelines system design interested performance metrics 
metric bandwidth overhead define ratio total number packets key server sends block packets including repair packets provide reliability block size 
second metric latency define number rounds deliver rekey message receivers 
analysis assumptions performance analysis considering specific key assignment algorithm consider general key assignment algorithms assume distribution input analysis 
furthermore numerical results section assume number receivers equal receivers value 
balanced key tree degree users know maximum number encrypted keys user needs receive 
value user equal 
result reported numerical results upper bounds results specific key assignment algorithm 
receivers assume high packet loss rate low loss rate simplicity analysis assume receivers independent losses losses different packets independent simulation results correlated losses please see 
numerical results default values 
conversion protocol protocol ii receiver need receive packets rekey message observe previous analyses reliable multicast directly applied 
key observation convert analysis rekey transport workload analysis conventional reliable multicast workload 
particular convert protocol instance receivers run protocol protocol instance receivers run protocol ii 
protocol instance mean session number receivers running protocol 
consider condition round receiver received specific packets total number packets received observe remove receivers satisfy condition round converted analysis instance protocol instance protocol ii reduced number receivers reuse results previous analyses conventional reliable multicast 
specific denote numbers high loss low loss receivers instance protocol high low denote random variables numbers high loss low loss receivers satisfy condition round high low analysis instance receiver protocol high loss receivers low loss receivers converted analysis instance protocol ii high high loss low low loss receivers 
denote high low 
assume losses receivers independent high low high denotes high probability high loss receivers satisfy low denotes low 
remaining issues derive high low high low 
derivation details shown appendix block size rekey message proactivity factor proactivity factor proactivity factor proactivity factor expected number receivers satisfying see benefit sparseness rekey workload shows number receivers satisfying round 
number represents reduction number receivers convert protocol protocol ii reflects savings sparseness rekey workload 
observe message block size large proactivity factor small performance rekey multicast equal performance conventional reliable multicast smaller number receivers 
bandwidth overhead analyze section bandwidth overhead rekey transport 
high loss low loss receivers denote random variable bandwidth overhead receivers run protocol denote mean value random variable 
high loss low loss receivers ii denote random variable bandwidth overhead receivers run protocol ii 
ii denote mean random variable 
conversion protocol protocol ii high low ii derive ii considering protocol ii detailed derivation ii shown appendix shows analytical results rekey transport bandwidth overhead functions block size rekey message proactivity factor 
validate analysis shows simulation results ns simulator 
block size rekey message proactivity factor proactivity factor proactivity factor proactivity factor overhead block size rekey message proactivity factor proactivity factor proactivity factor proactivity factor overhead ns simulation comparing figures observe analytical results match simulation results wide range message block size proactivity factor 
observe sparseness property bandwidth overhead reliable rekey transport high 
rekey message large block size better transport bandwidth efficiency reliably transport rekey message key server needs send large amount repair packets 
smaller rekey message overhead higher 
example rekey message block size key server needs send repair packets 
rekey transport latency measure latency rekey transport number rounds deliver rekey message receivers 
intuitive rekey transport latency depend block size rekey message proactivity factor 
shows simulation results number rounds transport rekey messages different message block size different proactivity factor 
observations 
observe large proactivity factor number rounds transport rekey message large block size smaller smaller rekey message 
counter intuitive expect number rounds transport large rekey message larger smaller rekey message 
explain result notice rekey message large block size proactivity factor large probability receiver receive total packets higher rekey transport latency reduces 
hand proactivity factor small number rounds transport large rekey message larger smaller rekey message 
block size rekey message proactivity factor proactivity factor proactivity factor proactivity factor rekey transport latency ns simulation denote random variable number rounds rekey high loss low loss receivers receivers run protocol ii denote random variable number rounds transport packets high loss low loss receivers receivers run protocol ii 
similar equation high low ii convert analysis protocol protocol ii 
exact calculation number rounds transport rekey message requires complicated calculations involving modeling transition states 
derive upper bound ii 
derivation upper bound shown appendix determine proactivity factor 
previous investigations bandwidth overhead rekey transport latency considered impacts block size rekey message proactivity factor 
rekey subtree key assignment algorithm know block size determined 
proactivity factor protocol parameter rekey transport protocol 
discuss determine 
determine observe key server reduce rekey transport latency number receiver feedbacks increasing proactivity factor 
large key server send proactive repair packets round receivers receive packets round receivers send feedback packets key server key server send repair packets rounds 
example observe rekey message block size key server increases rekey transport latency reduced 
key server reduce rekey transport latency increasing 
notice key server may set large increase bandwidth overhead 
example observe key server sets higher bandwidth overhead dominated proactivity factor increases linearly rekey transport latency stays flat 
number rounds proactivity factor overhead latency overhead latency function observations know key server choose rekey transport latency close bandwidth overhead curve stays flat 
example choice 
real implementation key server know loss properties receivers example independent loss assumption tends overestimate amount redundancy needed losses shared block size rekey message may vary different rounds 
key server dynamically adjust round 
example type strategy key server adjust stochastic aimd additive increase control rekey transport latency close value say rounds 
type strategy proposed investigated key server adjusts way number receivers sending feedbacks close small value say receivers 
performance analysis determine choosing largest proactivity factor gives lowest bandwidth overhead 
tradeoffs bandwidth overhead rekey interval section join leave requests rekey interval group users investigated marking algorithm generate rekey subtree 
rekey subtree section investigated rekey transport evaluated bandwidth overhead 
combining results section section derive bandwidth overhead 
group size user behaviors know function rekey interval rekey interval serves system design parameter group key management system control bandwidth overhead 
furthermore user behaviors system constraints possible group key management find suitable group size scenario group key management system needs partition users groups reduce group size 
remainder section discuss simple membership model 
discuss system performance metrics tradeoffs bandwidth requirements rekey interval discuss system constraints algorithm determine rekey interval number users single key server support 
membership dynamics quantify numbers joins leaves arriving rekey interval need specify arrival rates joins leaves 
arrival rates call membership dynamics depend application user behaviors 
investigation membership dynamics aware almeroth ammar 
showed user join leave behaviors audio multicast session follow exponential distributions 
excess life joins group leaves group rekey interval rekey rekey illustration excess life model number leave requests rekey interval group users specify distribution time user spends group 
denote cumulative distribution function time user stays group 
denote remaining time user stay group group time start time rekey interval 
call excess life user time illustrates concept excess life 
denote mean value time user stays group 
system steady state renewal theory dy probability user leave group rekey interval 
group users expected number leave requests time period 
write indicate number leaves function particular assume amount time receiver spends group exponentially distributed mean value denoting lt 
model number join requests rekey interval assume user arrivals poisson rate membership dynamics modeled system 
evaluation purpose assume group steady state system metrics tradeoffs types entities participate group key management system key server receivers 
accordingly potential bottleneck resources cpu processing demand key server receiver bandwidth requirement key server receiver 
cpu power keeps increasing evaluations show cases cpu demands limiting factors concentrate efforts bandwidth requirements 
determine rekey interval performance metric consider rekey transport latency gives lower bound rekey interval 
formally specify performance metrics key server outgoing bandwidth ks 
ks denote total bytes key server multicasts users order reliably transmit rekey message 
functions rewrite enc enc 
fec denote packet size original rekey packet fec packet re synchronization packet respectively 
denote number encrypted keys packet od denote packet duplication overhead key assignment algorithm number packets generated key assignment algorithm divided number packets generated key assignment algorithm duplicate assignment 
denote mean value multicast bandwidth overhead defined section 
denote mean number re synchronization packets key server needs transmit 
ks ks ks od enc fec receiver incoming bandwidth 
denote total bytes receiver receives multicast 
denote packet loss rate receiver know ks 
assuming probability receiver needs re synchronization small rekey transport delay 
denote number rounds key server transmit rekey message users 
dr denote largest round trip time key server receivers 
dr observation ks increasing functions bandwidth requirements rekey latency increase increase group size 
observe generally high ks close 
key server larger bandwidth receivers plot bandwidth requirement 
plots functions upper assumes receiver stays group minutes lower assumes receiver stays group hour 
observe figures decreasing function observe clearly curves tradeoffs bandwidth requirements access control effectiveness 
determine suitable rekey interval balance performance requirements access control effectiveness achieved 
rekey interval seconds mean time sec packet size enc packet rekey interval seconds mean time sec packet size enc packet bandwidth requirement vs rekey interval system constraints algorithm decide rekey interval maximum number users key server support identify potential system constraints 

ks max ks max ks system specified bandwidth limit key server 
example max ks key server outgoing bandwidth 
constraint specifies lower bound 
max similar max ks max receiver bandwidth limit 
constraint specifies lower bound 
ensure rekey transport finish start rekey interval 
notice constraint number receivers sending re synchronization requests greatly reduced 
constraint specifies third lower bound 
max max constant determined business model application security requirements specifies upper bound example possible specification number departed users group key total users 
membership model means lt set max ln satisfy constraint 
constraints choose max minimize bandwidth requirement lower bounds smaller max possible rekey interval satisfy constraints 
case need determine maximum group size key server support partition users smaller groups 
algorithm determine maximum group size key server support shown 
partition users smaller groups behaviors architectures extend centralized key server distributed key servers kronos please see :10.1.1.106.4394
see section 
max ks max max ks max max max ks max max max min algorithm determine extension distributed architectures previous section shown membership dynamics system constraints possible single key server support number users 
extend scalability single key server provide fault tolerance may necessary multiple key servers group key management system 
performance evaluations multiple key servers show partitioning users active inactive groups improve system scalability 
active inactive subgroup mean users higher lower probability joining leaving 
due page limitation report results separate 
completeness distributed key server architectures 
architectures differ functionalities coordinate multiple key servers 
architecture illustrates architecture kronos 
architecture users partitioned subgroups users form multicast group 
subgroup key server ks registrars reg operates just previous single key server 
key servers synchronized ntp protocol key server designated principle key server pks 
system initialization non pks authenticates pks receives system parameters rekey interval index current rekey interval secret key th rekey interval key server generates new subgroup key encrypting key servers synchronized index key server generate key 
subgroups form larger virtual group common group key 
send data packet group sender simply encrypts packet group key multicasts group 
receiving packet receiver decrypts packet group key 
reg reg ks gk ks gk reg reg pks ks reg reg gk subgroup subgroup group ntp ntp ntp subgroup architecture architecture introduces overhead data delivery subgroups 
overhead synchronizing key servers low 
architecture requires key servers rekey interval 
harder ensure reliable data delivery subgroups 
suitable applications require reliable application data delivery teleconferencing pay view 
architecture illustrates second architecture 
users partitioned subgroups subgroup managed key server 
subgroup operates users share common group key 
ensure communications subgroups key servers organized mesh tcp just rmx :10.1.1.42.4522
key servers share common secret key 
establishment key similar architecture 
send data packet group sender encrypts packet subgroup key multicasts subgroup 
sender reliably unicasts packet local key server 
receiving packet local key server decrypts packet re encrypts forwards packet key servers mesh 
key servers relay packet subgroup 
subgroup subgroup subgroup ks reg reg ks gk reg reg reg reg ks gk tcp tcp tcp gk architecture advantage architecture relaying key servers reliable subgroup provides reliable data delivery delivery process reliable 
result architecture suitable applications require reliable data delivery software distribution 
architecture flexible sense subgroup need coordinate subgroups 
investigated scalability issues reliable group rekeying provided performance analysis 
rekeying join leave periodic batch rekeying improve scalability alleviate sync problems 
analyses show batch rekeying achieve large performance gains 
rekey transport investigations show rekey transport eventual reliability soft real time requirement rekey workload sparseness property 
reliable rekey transport protocol proactive fec 
show reliable rekey transport design analyzed converting conventional reliable multicast 
showed tradeoffs bandwidth requirements rekey interval 
considering system constraints provide guidelines choosing appropriate rekey interval determining maximum number users key server support 
includes investigations dynamic partitioning group users detailed trace experimental evaluations investigations fec encoding schemes better workload sparseness property 
investigation fec encoding schemes sparseness workload especially interesting apply applications stock quote delivery 
acknowledgments ellen zegura constructive comments shepherding final revision 
min kim dong young lee liu wang assistance 
almeroth ammar 
collection modeling join leave behavior multicast group members mbone 
proceedings high performance distributed computing focus workshop hpdc syracuse new york usa august 
link ucsb edu publications html 
sherman 
key management large dynamic groups way function trees amortized initialization internet draft 
byers luby mitzenmacher :10.1.1.21.9363
digital fountain approach reliable distribution bulk data 
proceedings acm sigcomm vancouver sept 
chang engel kandlur saha :10.1.1.42.4522
key management secure internet multicast boolean function minimization techniques 
proceedings ieee infocom volume mar 
chawathe mccanne brewer :10.1.1.42.4522
rmx reliable multicast heterogeneous networks 
proceedings ieee infocom tel aviv israel mar 
harder 
logical key hierarchy protocol mar 

irtf 
reliable multicast research group 
www net rm links html 

irtf 
secure multicast research group 
www com community 
kurose towsley 
comparison server receiver local recovery approaches scalable reliable multicast 
proceedings ieee infocom san francisco ca mar 

scoped hybrid automatic repeat request forward error correction 
proceedings acm sigcomm sept 
levine garcia luna aceves 
comparison known classes reliable multicast protocols 
proceedings ieee icnp columbus oh oct 
li yang gouda lam 
batch rekeying secure group communications 
proceedings tenth international world wide web conference www hong kong china may 
moyer rao rohatgi 
maintaining balanced key trees secure multicast internet draft june 
biersack towsley 
parity loss recovery reliable multicast transmission 
proceedings acm sigcomm sept 
jung biersack carle :10.1.1.35.7747
bad reliable multicast local recovery 
proceedings ieee infocom san francisco ca mar 
rizzo 
effective erasure codes reliable computer communication protocols 
computer communication review apr 
rubenstein kurose towsley 
real time reliable multicast proactive forward error correction 
proceedings nossdav july 
setia jajodia harder 
kronos scalable group re keying approach secure multicast 
proceedings ieee symposium security privacy berkeley ca may 
snoeyink suri varghese 
lower bound multicast key distribution 
proceedings ieee infocom anchorage alaska apr 
towsley kurose pingali 
comparison sender initiated reliable multicast receiver initiated reliable multicast protocols 
ieee journal selected areas communications 
wallner harder 
key management multicast issues architectures internet draft sept 
wong gouda lam 
secure group communications key graphs 
proceedings acm sigcomm sept 
wong lam 
digital signatures flows multicasts 
ieee acm transactions networking aug 
wong lam 
keystone group key management system 
proceedings ict acapulco mexico may 
yang :10.1.1.16.7732:10.1.1.16.7732
secure group key management communication lower bound 
technical report tr university texas austin july revised september 
yang li zhang lam :10.1.1.106.4394
reliable group rekeying performance analysis 
technical report tr university texas austin june 
zhang lam 
lee yang 
protocol design scalable reliable group rekeying 
proceedings spie conference scalability traffic control ip networks denver aug 
number encrypted keys batch processing worst case analysis consider worst case 
consider balanced tree degree height know total leaf nodes 
suppose users leave 
see worst scenario happens receivers left evenly distributed tree leaf nodes amount sharing minimum 
level tree root 
suppose number receivers leave leaving receivers distributed evenly leaf nodes keys level level need changed 
keys need encrypted children 
nodes level path level level keys path need encrypted children 
level total keys need encrypted children 
worst case number keys updated ld log extra left receiver reduce number updated keys 
extra left receivers number reduced key update extra left receiver 
left receivers reduce 
left receivers reduce key updates 
average case analysis show derivation average number encrypted keys batch rekeying 
write mean choose number ways choose element subset element set 
convention intuitively number encrypted keys strongly related size rekey subtree 
example node belongs rekey subtree due leaves encrypted times time children assuming children pruned 
marking algorithm node key pruned users left 
general technique consider probability individual node belongs rekey subtree node expected number children 
average number encryptions sum product amounts non leaf nodes 
illustrates idea 
divide derivation cases node node level level level batch rekeying derivation 
consider case scenario happens system steady state forms basis analysis cases 
pick leave request replace join request 
assume level root key tree level leaves log consider nodes level total ways pick users tree leaves cases tree leaves node leave 
node cases 
know node case encrypted children average number encrypted keys expressed enc replace part leaves joins 
simple efficient method replace leaves left right 
node probability belonging rekey subtree similar previous case node probability pruned leaf nodes node leaves joins replace 
consider th node level probability node pruned leaf nodes subtree leaves leaves left subtree 
number encrypted keys enc enc extra joins replace leaves joins 
efficient method put extra joins put joins replaced leaves put joins left right leaf nodes 
method tries reduce number encryptions tries keep tree balanced 
consider cases node belongs rekey subtree due joins second node belongs rekey subtree due leaves replacing joins 
case new key needs encrypted old counterpart case new key needs encrypted children 
number encrypted keys enc 
alternative strategy alternative strategy add remaining users left right replacing departed users joining users 
strategy consider potential states th node level node update state leave probability state edges coming node rekey subtree 
consider case node update state 
join subtree node edge rekey subtree number edges coming node rekey subtree joined node node min dn number children join plus key old key 
cases average number encrypted keys enc dn min dn 
reliable transport performance analysis define conditions round receiver received specific packets total number packets received receiver received specific packets total number packets received greater equal receiver receive specific packets total number packets received receiver receive specific packets total number packets received greater equal reduction protocol protocol ii notice difference receiver protocol ii conditions receiver successfully receive packets 
consider receiver loss probability number specific packets needs protocol define pc probability receiver satisfy condition round 
pc zr zr zr zr zr zr zr zr assume users require max packets max maximum number packets receiver needs define pc pc max consider high number high loss receivers receiver protocol ii 
high high pc expected value high expressed high high similarly low low pc expected value low expressed low low bandwidth overhead protocol ii consider overhead protocol ii 
assume high loss receivers low loss receivers protocol ii 
assume sender send packets original data packets proactive fec repair packets 
consider receiver loss rate denote number packets needs receive th different packets 
define extend high loss loss receivers 
define max expected value overhead ii max reported results analysis 
notice order reduce calculation overhead possible derive approximation bandwidth overhead 
packets consider probability receiver receive 
denote probability 
extend high loss loss receivers 
define probability receiver receive packets consider confidence probability close 
determine appropriate choose guarantee receivers get packets probability larger max approximation bandwidth overhead ii 
upper bound rounds protocol ii derive upper bound mean number rounds ii 
simulation high loss receivers dominate number rounds 
consider homogeneous high loss rate environment 
denote number rounds transfer packets receivers 
denote 
consider system state number packets receiver needs order total packets 
know define function number receivers greater denotes number receivers missing packet packets system state denote maximum state definitions define terminal states denote set states receivers missing packets number packets denote set states highest number packets missing easy see 
denote expected number rounds go state enter terminal states particular define states 
assume homogeneous environment know matter exactly states start 
defined 
consider state see 
apply step analysis 
assume round state 
sender send repair packets 
denote set potential subsequent states 
know denote transition probability state state define unknown 
derive expressions consider cases 
consider general case sender sender packets 
round sender may send proactively consider second case 
consider case sender sends repair packets state start round consider potential states round 
receivers know exactly received packets sent sender round 
assume receivers received packets remaining receivers received packets 
ai probability receiver loss probability gets packets packets total packets consider case round 
state round 
sending packets sender sends packets 
denote number receivers proactivity factor proactivity factor proactivity factor proactivity factor analytical upper bound rounds number receivers proactivity factor proactivity factor proactivity factor proactivity factor number rounds ns validate accuracy upper bound shows upper bounds derived analysis shows simulation results ns 
comparing figures conclude upper bound sensitively express effect number receivers proactivity factor 
number receivers extremely large proactivity factor small upper bound close actual value 
decide system constraints upper bound derive third constraint 
latency analysis receiver expected number rounds receiver protocol ii consider protocol ii 
consider number rounds ii receiver loss rate get specific max packets block 
consider existence receivers number rounds derived denoted ii ub upper bound 
define receiver states indicates needs packets 
terminal state initial state 
denote expected number rounds needed go state enter terminal state 
convenience denote 
get inductive formulas round second round denotes gets packets packets upper bound ii ub need 
number rounds block size proactivity factor proactivity factor proactivity factor proactivity factor ii ub low loss rate receiver number rounds block size proactivity factor proactivity factor proactivity factor proactivity factor ii ub high loss rate receiver expected number rounds receiver protocol consider protocol consider number rounds denoted receiver loss rate get specific max packets block 
consider existence receivers number rounds derived denoted ub upper bound 
difference protocol ii round 
round protocol condition holds denotes gets packets packets contain specific packets zr zr zr second round induction protocol ii 
number rounds block size proactivity factor proactivity factor proactivity factor proactivity factor ub low loss rate receiver conditional expected latency protocol section derive upper bound receiver condition holds number rounds block size proactivity factor proactivity factor proactivity factor proactivity factor ub high loss rate receiver derive low bound consider case receiver lb zr rr rr upper bound ub ub similarly get ub lb ub value block size value pr low loss rate proactivity factor proactivity factor proactivity factor proactivity factor low loss rate value block size value pr high loss rate proactivity factor proactivity factor proactivity factor proactivity factor high loss rate value block size value pr low loss rate proactivity factor proactivity factor proactivity factor proactivity factor low loss rate value block size value pr high loss rate proactivity factor proactivity factor proactivity factor proactivity factor high loss rate 
