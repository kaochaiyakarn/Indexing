information theoretic determination minimax rates convergence yang andrew barron iowa state university yale university march general results determining minimax bounds statistical risk density estimation certain information theoretic considerations 
bounds depend metric entropy conditions identify minimax rates convergence 
metric entropy structure density class determines minimax rate convergence density estimators 
prove results new direct metric entropy bounds mutual information arises application fano information inequality bounds characterizing optimal rate 
special construction required density class 
study global measures loss squared error squared hellinger distance kullback leibler divergence nonparametric curve estimation problems 
minimax rates convergence determined steps 
lower bound obtained target family densities speci estimator constructed maximum risk constant factor derived lower bound 
global minimax risk considering methods derive minimax lower bounds fano inequality lemma 
methods huber pinsker stone birge nemirovskii devroye 
birge claims fano supported nsf ecs dms 
ams subject classi cations 
primary secondary 
key words phrases 
minimax risk density estimation metric entropy kullback leibler distance 
inequality general replace lemma practical situations 
yu gives lower bound similar terms kullback leibler distance fano inequality 
le cam provides general tools minimax rates theory tests 
upper bounds minimax risk metric entropy conditions birge barron cover van de wong shen birge massart 
focus lower bounds determining minimax rate 
parallel development risk bounds global measure loss results point estimation density functionals density see farrell donoho liu birge massart 
lemma fano inequality previously obtain lower bounds involve restriction local subset function space assumption special properties packing sets subset 
purpose demonstrate situations convergence rate determined global metric function class large subsets 
advantage approach metric entropies available approximation theory function classes see lorentz 
cases necessary uncover additional local packing properties 
proposition representative results obtained 
class functions distance functions size largest packing set functions separated satisfy log kolmogorov entropy sample size 
assume target class rich satisfy lim true 
condition satis ed typical nonparametric classes 
convenience symbols bn means bn bn means bn bn 
proposition cases minimax characterized entropy terms critical separation follows min max ef 
class density functions bounded integrated squared error squared hellinger distance kullback leibler divergence 

convex class densities exists density bounded away zero andd distance 

class functions ffor regression model independent px normal px norm 
proposition minimax risk rate determined metric entropy densities zero 
kullback leibler risk show modifying nonparametric class densities uniformly bounded logarithms allow densities approach zero vanish subsets minimax rate may remain unchanged compared original class 
interesting application bounding risk density estimation support interval unknown boundaries 
outline roughly method lower bounding minimax risk fano inequality 
rst step restrict attention subset parameter space minimax estimation nearly di cult space loss function interest related locally divergence arises fano inequality 
example subset cases set densities bound logarithms 
shall reveal lower bound minimax rate determined metric entropy subset 
proof technique involving fano inequality bounds minimax risk restricting large possible nite set parameter values mg separated amount distance interest 
critical separation largest separation hypothesis mg nearly indistinguishable average tests shall see 
fano inequality reveal indistinguishability terms divergence densities xn xi centroid densities xn xn shannon mutual information xn xn uniform distribution mg 
key question determine separation average divergence small compared distance log correspond maximally distinguishable densities determined 
critical divergence triangle inequality joint densities 
divergence xn centroid shown bounded right order distance xn large diameter set fp mg 
proper convergence rate identi ed provided cardinality subset chosen log bounded suitable constant 
determined solving equal constant metric entropy logarithm largest cardinality ofan packing set 
way metric entropy provides lower bound minimax convergence rate 
previous applications fano inequality estimation diameter set pn see birge similar rough bounds average distance pn pn centroid 
theory obtain suitable bound statistician needs nd possible su ciently large subset mg diameter subset sense order separation closest points subset chosen distance 
apparently bound possible subsets small diameter 
knowledge needed metric entropy special localized subsets 
typical tools involve perturbations densities parametrized vertices hypercube 
interesting involved calculations needed obtain correct order bounds 
su ces know bound metric entropy chosen set 
purpose criticize hypercube type arguments general 
fact success methods mentioned useful applications determining minimax rates estimating functionals densities see bickel ritov birge massart pollard 
density estimation problem consider closely related data compression problem information theory see section 
relationship allows obtain upper lower bounds minimax risk upper bounding minimax redundancy data compression related global metric entropy 
le cam pioneered local entropy conditions convergence rates characterized terms covering packing balls radius balls radius subsequent developments mentioned 
conditions provide optimal convergence rates nite dimensional nite dimensional settings 
section show knowledge global metric entropy provides existence set suitable local entropy properties nite dimensional settings 
cases need explicitly require construct aset 
divided sections 
section main results 
applica tions data compression regression section respectively 
section results connecting linear approximation minimax rates sparse approximation minimax rates respectively 
section illustrate determination minimax rates convergence function classes density estimation regression 
section discuss relationship global entropy local entropy 
proofs lemmas appendix 
main results suppose collection densities fp de ned measurable space respect nite measure parameter space nite dimensional space nonparametric space class densities 
parameter density may place respectively 
xn sample want estimate true density sample 
loss squared hellinger loss integrated squared error losses considered 
determine minimax bounds subclasses fp sg technique appropriate nonparametric classes class densities certain derivative satisfying lipschitz condition 
action space parameter estimates estimator measurable mapping sample space xn collection estimators 
nonparametric density estimation chosen set densities transform densities square root density 
consider general loss functions mappings call loss function distance satis es properties metric 
minimax risk estimating action space de ned rn min max min max understood inf sup respectively minimizer maximizer exist 
rst give de nitions entropies 
de nition nite set said packing set separation wehave logarithm maximum cardinality packing sets called packing entropy kolmogorov capacity ofs distance function denoted md 
de nition set said net exists logarithm minimum cardinality nets called covering entropy denoted vd 
de nitions clear md nonincreasing kolmogorov showed md right continuous metric 
proof works show md right continuous general notion distance 
de nitions slight generalizations metric entropy notions introduced kolmogorov 
accordance common terminology informally call entropies metric entropies distance metric 
choice square root kullback leibler divergence dk log clearly dk asymmetric arguments metric 
presumed shortcoming distance triangle inequality general exist constant dk dk cdk absence inequality advantage distance regards mentioned 
appropriate conditions enabling triangle inequality hold locally 
example demonstrates happen extreme case absence condition 
example consider densities respect lebesgue measure 
largest packing set dk 

log clearly triangle inequality packing number determine minimax rate convergence 
distances consider include hellinger metric dh lq metric dq jp 
assume distance satis es condition 
condition local triangle inequality exist positive constants remarks ad 
condition holds dk class densities bounded logarithms action space including class densities see section 

metric condition satis ed 
condition satis ed packing entropy entropy simple relationship md vd md obtain minimax results general special results choices square root distance hellinger distance lq distance 
assume md 
square root hellinger lq packing entropies denoted mk mh respectively 
subsection give minimax bounds global entropy conditions 
subsections results risk risks 
minimax risk global entropy condition suppose upper bound covering entropy square root distance available 
assume vk 
ideally vk andv order 
similarly md lower bound packing entropy distance ideally order md 
suppose nonincreasing right continuous functions 
avoid trivial case small nite set assume log small 
inff denote call critical covering radius 
continuous radius satis es squared radius covering entropy divided sample size 
trade analogous squared bias variance estimator 
shown upper bound minimax risk 
separation log existence follows right continuity ofm assumption log small roughly separation packing entropy distance divided sample size approximately times square covering radius 
call packing separation commensurate critical covering radius determines lower bound minimax risk 
minimax lower bound theorem suppose condition satis ed distance sample size large minimax risks estimating satis es consequently min max fd dg min max minimum estimators mapping proof packing set maximum cardinality ins distance net dk 
estimator values de arg min minimizer choose takes values nite packing set point inn ifd max condition ad wemust min max fd dg min max fd dg min max min min pw line randomly drawn discrete prior probability restricted denotes bayes average probability respect prior expected value follows min max min pw fano inequality see fano ash page cover thomas pages discrete uniform prior packing set pw xn log log jn shannon mutual information random parameter random sample distributed mutual information equal average respect prior divergence pw xi xn 
upper bounded maximum distance product densities joint density sample space log pw dx log dx max rst inequality follows fact bayes mixture density pw minimizes average divergence choices densities yields larger value amount pw log pw dx 
uniform choose uniform prior pw xn corresponding bayes mixture density distribution respectively 
net dk exists de nition log jg nj vk 
follows choice log nj jg nj nj log jg nj nj log log jn follows 
completes proof theorem 
remarks 
point development standard 
previous fano inequality minimax lower bound takes weak bounds mutual information ni ori max see birge respectively 
exception direct evaluation mutual information gaussian stochastic process models 

improved bound borrowed ideas universal data compression represents bayes average redundancy max px represents upper bound minimax redundancy cn max qx maxw iw maximum priors supported universal data compression interpretations quantities davisson davisson leon garcia see clarke barron yu haussler opper haussler area 
bound px roots barron pp 
general form arbitrary priors px log px density pw 
redundancy bound obtained stage code length log jg nj min log see barron cover section 

inequality minimax risk bounded constant times cn log kn cn maxw iw shannon capacity chan nel fp sg kn kolmogorov capacity ofs 
lower bounds minimax rate provided shannon capacity kol capacity factor 
shannon kolmogorov characterization emphasized 
distance lower bounded suitable distance minimax lower bound risk obtained 
exists constant dk suitably large min max satis es condition natural choice hellinger distance dh 
hellinger distance satisfy triangle inequality densities dh dk locally square root distance behaves hellinger bounded log density ratios 
packing separations commensurate critical covering radius dk dh determined mk log mh log respectively wehave corollary 
corollary exist constants dk satis es condition square hellinger risk min max min max note rst determined packing entropy mk choice mk 
general distance speci cally hellinger distance corollary determined quantities mh assumption relationship distances 
dk locally upper bounded minimax risk bound expressed exclusively terms packing entropy distance yielding corollary 
corollary assume condition satis ed distance exists constant jjp ad wheren determined md chosen md log 
min max 
proof assumption distances dk largest packing set serves covering set dk 
vk md result follows theorem 
applications lower bounds may applied subclass densities fp characterize di culty estimation densities class easy check conditions 
instance densities fp support compact space log square root distance hellinger distance distance equivalent inthe sense upper bounded lower bounded multiples 
upper bound provide upper bound minimax rate convergence construct estimator follows 
technique applies general bound cumulative kullback leibler risk sequences bayes estimates densities 
identify minimax rate uniform prior net 
consider net dk uniform prior corresponding mixture density 
jg nj pi density estimator constructed cesaro average bayes predictive density es pi xi jx evaluated xi equal xi pi jg xj fori 
convexity chain rule barron xi xi jx elog xi xi jx elog xn px inequality derived equation 
combining upper bound lower bound section result 
theorem upper bound covering entropy dk satisfy min max minimization density estimators 
condition satis ed distance mappings contains constructed set allowed estimators min max min max condition contains theorem satis ed fp sg convex 
particular holds action space set densities case metric remaining condition needed second set inequalities satis ed hellinger distance distance respectively 
square root kullback leibler distance condition restricts family consist densities uniformly bounded logarithms remains acceptable action space consist densities see lemma subsection 
obtaining minimax risk bounds quantity plays important role 
derivation upper lower bounds uses uniform upper bounds quantity 
lower bound bound upper bound bounds risk speci estimator 
averages respect uniform priors nets di erent radius choices bounds 
shall see asymptotically radii typically rate 
converge rate minimax rate convergence identi ed theorem 
order su cient conditions hold proof see lemma appendix 
condition metric entropy equivalence exist positive constants small cm condition richness function class lim condition equivalence entropy structure square root distance distance small 
equivalence entropy structure satis ed instance densities target class uniformly bounded away andd taken hellinger distance distance 
condition requires density class large approaches polynomially fast exists constant condition typical nonparametric function classes 
satis ed particular expressed 
situations metric entropies known orders generally tractable check lim md md exact packing entropy function 
condition stated terms presumed known bound md 
conditions satis ed instance 
corollary assume condition satis ed distance satisfying assume fp sg convex 
conditions min max determined equation md inparticular sup logp condition satis ed min max min satis es mh max min max corollary applicable smooth nonparametric classes shall see 
rich classes densities example nite dimensional families analytical den lower bound upper bound derived way converge rate 
instance nite dimensional class mk order log constant order log 
smooth nite dimensional models minimax risk solved traditional statistical methods procedures cramer rao inequality van tree inequality techniques require entropy condition 
local entropy conditions global entropy results obtained suitable parametric nonparametric families densities see section 
minimax rates loss general classes densities assumption upper boundedness square root dis tance distance density class corollary may hold 
theorem applicable resulting minimax lower bounds involve metric entropies dk subsection derive minimax bounds risk appealing covering entropy 
class density functions respect nite measure measurable set 
typically taken compact set need assume dominating measure nite 
normalize probability measure 
packing entropy mq lq metric 
derive minimax upper bounds derive lemma relates risk densities may zero corresponding risk densities bounded away 
addition observed sample xn yn sample generated uniform distribution respect generated independently xn 
zi xi yi probability outcome bernoulli random variables vi generated independently zi density 
clearly new density bounded away family original densities need 
fg new density class 
lemma minimax risks classes relationship min max exn minmax ez minimization left hand side estimators xn minimization right hand side estimators onn independent observations generally min max exn ef min max ez ef proof focus proof assertion case 
proof general lq similar 
change estimation estimation problem show minimax risk original problem upper bounded minimax risk new class 
estimator new class minimax estimator estimator original problem determined risk greater multiple risk new class 
density estimator zi density minimizes functions set fh 
lemma appendix kg general lq norm triangle inequality fh construct density estimator note nonnegative normalized probability density estimator depends xn yn outcomes coin ips vn 
randomized estimator 
squared loss bounded follows avoid randomization may replace expected value yn coin ips vn get ex ex ey ex ney ez ez rst inequality second identity depends max ex ef minimum completes proof lemma 
minimax risk original problem upper bounded minimax risk entropies related 
new packing entropy isf 
upper lower bounds minimax risk 
rst get upper bound 
new class square root distance upper bounded distance 
densities rst inequality familiar relationship distance chi square distance second inequality follows lower bounded 
lete vk denote dk covering entropy ofe vk 
chosen theorem exists density estimator max ef ez nd follows max ef ez nd max ef ez consequently min max exn get estimator terms risk assume supf 
density ine closest hellinger distance 
triangle inequality max ef ez nd max ef ef nd ef nd bounded max ef ez minimax squared risk 
pg lemma upper bound theorem packing entropy density class respect probability measure 
satisfy min max exn addition sup min max ef result upper bounds minimax risk risk supf metric entropy 
relationship lq norms kq supf wehave min max exn get minimax lower bound assumption satis ed classical classes besov lipschitz class monotone densities 
condition exists density minx positive constant fg convex class densities condition satis ed density bounded away zero 
lemma condition subclass packing entropy 
proof packing set corresponds packing set vise versa 
condition densities applying theorem applying theorem conclu sion 
theorem suppose condition satis ed 
thel packing entropy density class respect probability measure satisfy chosen log min max exn class rich distance condition determined 
holds 
sup min max exn min max exn relationship lq distances applying theorem corollary 
corollary suppose rich distance lq distance 
assume condition satis ed 
satisfy mq min max exn packing entropies lq equivalent case familiar nonparametric classes see section examples upper lower bounds con verge rate 
generally uniformly upper bounded density compact set jf gjd know supf kfk 
corresponding lower bound risk may vary depending di erent entropies see birge 
minimax rates loss square root distance condition necessarily satis ed general classes densities 
discuss condition dk results concerning risk 
lemma assume ad metric condition satis ed choice ofs conditions lemma satis ed regression families considered section 
discussed conditions satis ed log densities uniformly bounded lemma shows permitted take action space consist densities theorem applicable square root hellinger distance 
proof lemma assume max dk dk 
max metric dk ad dk dk lemma square root distance dk equivalent conditions su cient satisfaction condition 

fp sg family conditions necessary su cient 
fp sg convex lemma su cient condition satisfaction condition exist constants cd corollary suppose conditions lemma lemma satis ed 
determined mk log 
min max mk lim mk rich distance dk order min max mentioned conditions lemmas satis ed log densities class uniformly bounded 
conditions satis ed instance densities class di erent supports conditions lemma satis ed result provides minimax lower bound involving hellinger metric entropy 
consider estimating density de ned respect measure lemma assume density density dh depending log bounds analogous lemma barron birge massart proposition wong shen theorem 
classes metric entropy structure known hellinger distance hard know distance lemma useful give bound covering entropy distance 
density class mh packing entropy dh 
lemma net dh result net dk log greater log constant depending log wehave vk log log mh vk mh log log 
satisfy mh log log chosen mh log 
theorem result 
theorem density de ned minf maxf fed minf maxf fed due presence log term determination quantities typically order log log respectively nonparametric smooth families chosen mh suspect extra log necessary upper bound regularity condition relating distance hellinger distance 
see barron birge massart related 
results risks densities may section modifying nonparametric class densities uniformly bounded logarithms allow densities approach vanish subsets minimax rates convergence hellinger may remain unchanged compared original class 
result applicable examples 

densities unknown interval support 
ff fa bg dt nonparametric class positive functions uniformly bounded away xed constant force densities uniformly upper bounded 
note densities class uniformly bounded supports supports unknown 

densities polynomial tails unknown boundaries 
ff dt xed positive number class densities allowed approach zero polynomial rates points supports 

densities support unknown set intervals 
ff bi fai ai bi fai ai ak bi ai ai bi kg 
positive integer positive constants constants force densities uniformly upper bounded 
densities continuous densities discontinuous points 
example generalization rst example 
examples type handled theory 
function class 
class nonnegative functions satisfying gd 
nonnegative may equal zero subsets 
take measure probability measure 
consider class densities respect lete fe hd feg gd density classes corresponding respectively 
assumed class smaller class entropy sense 
entropies respectively distance vk vk vk entropies logarithms cardinalities smallest nets ande respectively dk 
new density class wehave result 
theorem suppose vk positive constants lim inf rich dis tance 
contains constant function min max efd min max efd determined min max ef proof theorem min maxf ef satis es vk ne lemma appendix vk ande 
assumption richness see proof lemma 
upper bounds minimax risk rates contains constant function subset note log densities ine uniformly bounded lemma metric entropies ande order 
lower bound rate squared hellinger square distance order corollary 
densities uniformly upper bounded distance densities upper bounded hellinger distance dk 
assumptions theorem metric entropy satis es vk positive constants consequently minimax risk upper bounded order theorem 
completes proof theorem 
apply theorem need bound covering entropy ofe dk 
situations obtain desired inequality vk constants 

suppose sup norm metric entropy ofg satis es constants corresponding densities 

kg kg follows vk su cient condition function gl gl gl lebesgue measure 

suppose hellinger metric entropy ofe satis es mh log constant 
lemma vk mh log constant su ciently small 
vk aa 

suppose small compared sense metric entropy ofg satis es lemma appendix vk positive constant 
lemma appendix wehave constant 
vk rst case allows large puts integrability condition regulate behavior density values approach zero 
second case allows large additional condition 
example rst case fh fg besov class de ned section 
example second case fh fg gd arbitrarily close case includes situation rich nonparametric class parametric class log 
condition case satis ed examples section 
application data compression obtained theorems get bounds minimax redundancy data sion 
xn sample discrete random variable xn qn xn density probability mass function 
redundancy shannon code density qn di erence expected codelength expected codelength shannon code true density xn qn 
formally examine minimax properties game loss qn continuous random variables 
case qn corresponds redundancy limit ne quantization random variable see clarke barron pp 

minimax redundancy lower bounds previously considered rissanen clarke barron rissanen speed yu yu 
results derived smooth parametric families speci smooth nonparametric class 
give general redundancy lower bounds nonparametric classes 
lemma relates game loss qn cumulative risk 
lemma sd qn minf pig max pi minimization left joint densities qn xn minimization right sequences estimators pi samples size xed density 
density qn sample space ofx xn exists estimator qn proof estimator sequence de ne xk jx pk xk fork 
qn xn xk jxk 
qn joint probability density function argument similar proving upper bound part theorem qn log xn qn xn log xk pk 
way density qn rewrite qn xn jx qn qk xjx conditional density joint density qn 
de ning pk qk xjx identity holds 
pk 
density estimator convexity proof upper bound theorem completes proof lemma 
nx pk qn 
collection density functions sample space xn 
lemma fact maximum sum risks greater sum maxima result connecting minimax redundancy minimax risk 
corollary rn nmax sd qn rn min pn pn max se 
rn rn rn rn ri 
smooth nonparametric density classes gives right order minimax redundancy 
parametric classes rich families lower bound may suboptimal 
instance smooth parametric families known see clarke barron minimax redundancy order log number parameters family 
bounded constant 
rn converges zero polynomial rate rn redundancy satis es rn 
particular rn rn ri ni ri minimax determined solving rn holds metric assume fp sg contains probability densities md packing entropy ofs upper bound covering entropy vk ofs dk 
choose choose md log 
theorem assume minimization densities min max qn pn qn choices satisfy requirements hellinger distance distance 
proof theorem lower bound follows theorem lemma equation 
upper bound consider choice jg nj mixture respect uniform prior net qn equation 
proof theorem complete 
interest focused cumulative risk individual risk rn case rn ri direct proof suitable bounds possible fano inequality 
see haussler opper new results direction 
proof idea rissanen see barron 
theorem assume ad chosen large min max qn pn qn mh log 
small 
proof mixture qn jg nj achieves max qn density estimator constructed section accordance 
bn fx ng 
markov inequality bn uniformly density xn ande qx corresponding probability measure 
discretization reduces kullback divergence kullback leibler ke qx bn log bn bn bn log bn qx bn bn log qx bn log log qx bn log packing set dh may consider general distance satisfying condition 
ad forany small large choice set bn contained fx dh 
distinct packing set know bn empty 
bn exists withe bn jn bound maximal total risk follows max pn max log qx bn log log bn log log jn log 
bound holds 
assertion theorem follows 
application nonparametric regression consider regression model yi xi suppose errors normal distribution 
tory variables xi xed density function 
regression function assumed function class case square root distance joint densities family metric 
hd distance respect measure induced letm maximum logarithm cardinality ofany packing set norm 
similarly mq bethe packing entropy ofu lq norm 
assume andm 
choose satisfy log similarly determined equation mq log theorem minimax squared risk regression function estimation lower bounded determined packing entropy follows min max 
generally minimax lq risk min max lq 
proof pu denote joint density regression function pu pv reduces know dx dk equivalent distance 
fu fu 
collection regression estimators maps sample space theorem dk wehave min max equal lq distance theorem yields bound lq risk depends metric entropy lq indicated 
ends proof theorem 
determine upper bounds regression specializing bound theorem 
immediately clear risk estimating loss un jju un bounded risk estimating density pu loss pn estimator pn theorem form un 
suitable bound risk holds regression functions uniformly bounded minimum hellinger distance argument case known 
assume uniformly 
theorem provides density estimator pn ed pu pn follows ed pu pn similar available birge theorem fact density xed pn takes form yjx yjx estimate conditional density ofy happens mixture gaussians posterior uniform prior nets 
xi yi minimizer hellinger distance dh gn jx gn normal density mean variance choices jzj estimator xi yi triangle inequality xi yi follows dh eun dh gn jx gn jx dh gn jx max ed pu max ed pu pn ed pu eun eun concave function chord eun obtain max eun max ed pu theorem result 
theorem assume min max satisfying lim rich min max mq risk min max lq letf bethe packing entropy ofu norm kl log thenf holds constants depending note constructed achieves minimax optimal rate depends density independent variable nding net norm 
condition rich distance show min max fh hk cg max linear approximation minimax rates fundamental sequence linear combinations dense 
ask 
min ai th degree approximation system functions approximation errors bounded ff called full approximation sets 
lorentz gives metric entropy bounds classes treats general banach spaces 
bounds derive metric entropy orders variety function classes see lorentz including sobolev classes 
de ne ni minfk de ned lorentz metric entropy satis es pj ni satis es exists pj ni 
illustrate results 
system consider functions approximated linear system polynomially decreasing approximation error 
upper lower bounds simple calculations seen metric entropy order similarly log log restrictive function classes correspond faster decays approximation error instance corresponding metric entropy order log case satisfy condition 
focus attention approximation error sequences satisfy conditions introduced lorentz 
suppose exist true log lorentz theorem shows rich distance condition satis ed 
secondly lorentz assumed 
chosen equivalently pj ni 
lorentz conditions rate determined kn kn solution setting metric entropy order dimension kn approximation error order density estimation suppose convenience assume functions uniformly bounded sup positive constant lete probability density functions 
large metric entropies ofe order 
fact ed ff gd ef provided 
hard see class ff gd entropy lower bounded isthe entropy 
determined indicated 
assuming lorentz rst condition equation assuming uniform boundedness functions theorem minimax rate density estimation min max ef special case min max ef similarly rate log log requirement functions uniformly bounded satis ed complete orthonormal system approximation error bounds satisfy 
orthonormality yields implies condition implies 
condition equivalent trigonometric system equivalent system dimensional legendre polynomials 
consider estimating regression function 
determined log pj ni 
theorem min max lower bound holds requiring functions uniform bound 
class uniformly bounded requirement satis ed upper bound theorem minimax rate identi ed follows min max get rate optimal convergence rate full approximation setting order mink familiar bias squared plus variance trade mean squared error 
regression example yi xi approximation systems yields natural known estimates achieve rate 
particular chosen gram schmidt process preserving orthonormal respect density 
model uk aj aj estimated yi xi approximation error squared bias jju estimation error variance bounded general basis functions uniformly bounded trade familiar literature see instance cox squares regression estimates barron sheu maximum likelihood log density estimates birge massart projective density estimators contrasts 
best rate occurs kn chosen kn kn course applications know underlying function approximated system impossible know optimal size kn 
suggests need model selection criterion choose suitable size model balance kinds errors automatically data 
results model selection see instance barron birge massart yang barron 
knowledge optimal convergence rates various situations interest permits gauge extent automatic procedure adapts multiple function classes 
sum section function class contained contains pair fundamental sequences sequences yield provides minimax rate conditions discussed minimax optimal estimates available suitable linear estimates 
inter function classes permit linear estimators minimax rate optimal see ne nemirovskii tsybakov donoho johnstone 
lack full approximation set characterization preclude determination metric entropy approximation theoretic means speci cases seen sections 
sparse approximations minimax rates previous section full approximation sets functions de ned linear approx imation respect system get accuracy approximation uses rst basis functions choice works basis functions needed get accuracy 
slowly converging sequences large needed get accuracy phenomenon occurs especially high dimensional function approximation 
instance uses full ap proximation chosen basis dimensional sobolev class partial derivatives behaved approximation error terms converge faster interest examine approximation manageable size subsets terms sparse comparison total needed full approximation 
quite possible subclass functions choice depending sparse subsets produce nearly approximation error full approximation 
function class called sparse terms approximation system give minimax results sparse function classes 
previous section satisfying condition equation 
ik nondecreasing sequence integers sat lim inf ik fi lete lk ik min ai li called th degree sparse approximation system approximation ande 
th term approximate selected ik basis functions 
functions sparse approximation errors bounded fg call sparse approximation set functions xed choice 
note smallest full approximation set contains repeated ik ik times 
containment relationship smaller class corresponds sparse approximation set choice ik 
larger ik provide considerable freedom approximation 
terms metric entropy sparse approximation set larger cor responding full approximation set contains 
shown metric entropy larger logarithmic factor condition ik possibly large 
full approximation approximate sparse approximation set approximating class larger metric entropy ik bigger example ik metric entropy lower bounded order seen metric entropy order order log class examples class functions uniformly bounded cover cardinality order metric entropy log positive constant closure convex hull 
ik ik members cover ik ik order closure convex hull class uniformly sparsely approximated system consisting suitably chosen members class rate sparse terms candidates 
containment result veri ed greedy approximation see jones barron section viii omit 
speci example rg xed sigmoidal function satisfying lipschitz condition sinusoidal function sin 
prove metric entropy bounds 
previous lower bound metric entropy lower bound 
derive upper bound 
minfk 
li ii xed moment 
consider subset gl lk span lk approximation errors bounded basis lk gl lk li coe cients mina am ai li 
previous section know entropy lk upper bounded order pj ni construction hard see net li ii gl lk net 
fewer ik choices basis li entropy ofs upper bounded order log ik log assumption ik 
seen section log wehave order log metric entropy ofs bounded order log factor cover larger class greater freedom approximation 
density estimation suppose functions uniformly bounded 
lete probability density functions 
large metric entropies ofe order 
satisfy log satisfy applying theorem min special case log min max es max es log note upper lower bound rates di er logarithmic factor 
similarly esti mating regression function conditions theorem min max proofs minimax upper bounds estimators constructed bayes averaging net sparse approximation set 
context natural consider estimators subset selection 
upper bound results direction yang barron barron birge massart 
general theory sparse approximation avoid requiring assumption basis functions 
contrast story full approximation sets unchanged gram schmidt process sparse approximation preserved orthogonalization 
consideration functions approxi mated sparse combinations orthonormal basis advantage conditions easily expressed directly terms coe cients 
discuss consequences donoho treatment sparse orthonormal approximation minimax statistical risks 
orthonormal basis sq ij ij 
positive constants may quite small smaller condition ij target class small convergent estimators norm 
roughly speaking sparsity class comes condition ij implies ith largest coe cient satis es selection largest coe cients su cient toachieve small remaining sum squares 
condition ij ensure su ces select largest coe cients rst ik terms su ciently large class sq special case function classes unconditional basis 
uniformly bounded function class orthonormal basis 
basis said unconditional alle je ij ij 
donoho gives results metric entropy classes proves unconditional basis function class gives essentially best sparse representation functions shows simple thresholding estimators nearly optimal 
apply main theorems minimax rates sparse function classes metric entropy results 
sup coe cients ordered decreasing magnitude called sparsity index donoho 
supf optimal exponent metric entropy ofg 
donoho satis es condition ij cl applying theorems regression min max arbitrarily close exponential component minimax risk density estimation ff 
large metric entropy order conditions density estimation min max 
exponential component minimax risk special case sq better entropy bounds available 
triebel shown metric entropy sq upper bounded order order log 
consequence sq uni bounded minimax rate squared risk estimating regression function sq density assuming suitably large density estimation upper bounded log 
examples section demonstrate applications theorems developed previous sections 
seen examples know order metric entropy target class minimax rate determined right away smooth nonparametric classes additional 
results metric entropy function classes see lorentz cited 
sure 
consider function classes examples 
denote lebesgue mea 
ellipsoidal classes 
complete orthonormal system 
increasing sequence constants bk de ne ellipsoidal class fg cg de ne bi tg tdt 
knows covering metric entropy ofe satis es special case bk metric entropy order determined previously obtained kolmogorov trigonometric basis 
increases suitably fast entropy rate derived results lorentz full approximation sets 

classes functions bounded mixed di erences 
de ne function classes having bounded mixed di erences follows 
kd integers rd rv rv rd denote periodic functions nite norm dr jg denote class functions lq dxi tg lq rj maxj rj td mixed lth di erence step tj variable xj tg td xd 
rd mp log functions class uniformly bounded 

besov triebel classes 
kh 
th modulus smoothness lq de ned sup kq 
dt besov norm de ned kb kq see devore lorentz 
triebel classes kr sup dt kq sup norm de ned kf kq de nitions tions besov classes dimensional case see triebel 
include known function spaces spaces sobolev spaces frac tional sobolev spaces bessel potential spaces inhomogeneous hardy spaces 
andf collections functions lq kb kf respectively 
build ing previously obtained sobolev classes birman triebel re nements carl showed mp inclusion relationship besov classes min max constants condition parameters requiring wehave mp 
bounded variation lipschitz classes 
function class bv consists functions satisfying sup jg xi xi supremum taken nite sequence xm 
lip fg kq ch kq cg lipschitz class 
metric entropy satis es mp lip see birman related kolmogorov clements 
lipschitz classes understood special cases sobolev besov classes lipschitz classes play special role determining metric entropy larger classes especially obtaining lower bounds see lorentz chapter 
class functions bv suitable modi cation value assigned discontinuity points devore lorentz chapter lip bv lip lp lip order lp 
classes functions moduli continuity derivatives bounded xed functions 
lipschitz requirements may consider general bounds moduli continuity derivatives 

cr collection functions partial derivatives ck jkj modulus continuity norm rth derivative bounded function 
modulus continuity de nition see devore lorentz 
de ned equation lorentz metric entropy 
order 
concave modulus continuity sup norm metric entropy order identi ed similar classes 

classes functions di erent moduli smoothness respect di erent variables 
kd positive integers ki kd 
collection functions sup jhj ki hg ct ki di erence step variable xi 
stated lorentz metric entropy results full approximation sets polynomial approximation results section metric entropy order terms sup norm metric 

classes 
collection periodic functions xd md am md cos dx bm md sin similar results dx md md md log md 
similarly de ne constraint md md md metric en andg order log log respectively 
note classes dependence en tropy orders input dimension logarithmic factors 

neural network classes 
closure set functions form ci vi bi jc xed sigmoidal function ast ast 
require step function fort fort satis es lipschitz requirement jt jtj 
approximations functions sigmoids achieves error bounded shown barron approximation bound barron gives certain metric entropy bounds 
approximation error uniformly smaller barron theorem 
pp 
improves approximation upper bound constant times uses bounds show metric entropy class step sigmoid lipschitz sigmoid satis es log better lower bound matches upper bound exponent log log equivalent bounded variation class 
density estimation 

assume functions uniformly bounded assume satis es lim inf xed constant 
lete collection probability density functions 
easily seen linear transform gd contained ine section 
suitably large order metric entropies 
theorem determined wehave min max ee specially basis functions satisfy sup functions uniformly bounded 
min max ee fk see pinsker detailed asymptotics trigonometric basis bk see birge similar hypercube con struction trigonometric basis barron birge massart section general ellipsoids 

lete ff uniformly bounded density class 
lemma metric entropy ofe order 
corollary min max eh kp log density class minimax risk distance squared hellinger distance order squared distance log 
lete set probability density functions besov class 
simi de 
functions uniformly bounded 
similarly toe suitably large metric entropy ofe ore order orf 
theorem large min min max eb max eb kp hold fore 
functions orf bounded class order metric entropy subclass orf uniformly bounded 
consequently su ciently large metric entropy ofe ore order orf 
theorem risk ore min max monotonicity property lp norm wehave forp min max kp particular hold 
donoho johnstone kerkyacharian picard obtained suitable minimax bounds case permit smaller rates lipschitz classes previously obtained birge devroye special cases 
log density assumed corollary minimax risk distance order previously shown koo kim 

bv class density functions bv 
suitably large lp metric entropy ofg bv order theorem min max min max kp 

density functions moduli derivative class 
suitably large 

order metric entropies 
chosen min max 

lete densities 
small metric entropy pd ofe order pd min max ev 
lete ande gd densities functions respectively 
uniformly bounded respectively 
small min min max ee max log log note classes dimension appear exponents sample size 
lete densities 
small log min max en better lower bound metric entropy wehave log min max en log log previously modha see yang barron obtain upper rate ignoring logarithmic factor similar class feedforward neural network models minimum description length criterion approximation results barron 
rates slightly better 
upper lower rates agree moderately large rates roughly independent ofd 
regression function estimation 
consider regression problem section 
density explanatory variable assume log nite considered function classes metric entropies norm orders 
theorems 

assume satis es lim constant 
deter mined wehave lower bound min max functions uniformly bounded min max specially min max fk functions fk uniformly bounded min max fk results giving right constant addition rate special ellipsoidal case trigonometric basis see pinsker 

min max log 
orf rates orf andp min max min max donoho johnstone obtained minimax rates 
pointed traditional linear methods achieve minimax optimal rate 
previously results obtained sobolev classes including 
xed design regression estimation positive integer nemirovskii obtained upper lower bounds minimax risks suitable see results gaussian white noise models 
stone gives optimal rates estimating regression function derivatives integer re nement results minimax risk sobolev classes rate right constant see 

bv min max bv result donoho johnstone 

chosen 
min min max max 

forg lower bounds min min max max log gd log respectively classes wehave min min max max 
general log gd log log min max better lower bound metric entropy wehave log min max log log similar upper bound results initially obtained barron neural network models 
data compression 
distance lower bounded half squared distance relationship density estimation data compression know minimax redundancy compressing data string governed density ine ore bounded rate follows consequence theorem min max qn qn eb qn rate obtained yu special lipschitz classes hypercube argu ment information parameter observations 
minimax redundancy rate identi ed theorem min max qn qn log qn similar results stated function classes considered section 
relationship global local metric entropies stated previous fano inequality derive minimax rates convergence involve local metric entropy calculation 
apply technique constructions special local packing sets capturing essential di culty estimating density inthe target class seemingly required usually done hypercube argument 
shown section global metric entropy determines minimax lower rates convergence typical nonparametric density classes 
need put orts search special local packing sets 
distribution early version contained main results section realized connection global metric entropy local metric entropy 
fact global metric entropy ensures existence local packing set property required birge argument 
fact allows bypass special constructions 
show connection global metric entropy local metric entropy comment onthe uses metric entropies 
simplicity consider case metric 
suppose global packing entropy distance 
de nition local metric entropy local entropy logarithm largest packing set local entropy denoted 
local entropy ofs de ned loc lemma global local metric entropies relationship loc proof largest packing set largest packing set respectively partition parts minimum distance rule voronoi partition 
letr arg mine points closest point inn distance di erent points rule ensure re 
note largest packing set exists itfollows roughly speaking ratio numbers points packing sets characterizes average local packing capability 
identity exists jn hand concavity log function log log average local entropies upper bounded di erence global metric entropies 
obvious upper bound local entropies 
ends proof lemma 
sets proof local packing sets property diameter set order smallest distance points 
points apart property assumption locally dk upper bounded enables birge result proposition get lower bound minimax risk 
identify minimax rates convergence assume exist constants ad 
lemma exist subset local packing set log cardinality 
fano inequality diameter bound mutual information birge yields uniformly distributed random variable choosing satisfy max na max na log na proof theorem replaced gets min max similar section cf 
corollary 
di erence bound just obtained compared previous birge lemma avoid requiring explicit construction local packing set 
knowledge entropy ofan ball largest order packing set argument equal ball yields determined provided jjp ad min max holds loc ne chosen packing set 
lower bound optimal rate target class parametric 
instance entropy usual parametric class order log dimension model 
metric entropy di erence orm loc order constant yielding anticipated rate ande rate due birge theorem condition 
condition nonincreasing function 
exists net wehave card bd positive constants exists su ciently small packing set bd satisfying card bd packing set proposition birge suppose condition satis ed 
distance metric bounded multiple hellinger distance satis es 
min max upper bound birge theorem rst part condition 
second part condition wehave am loc suitable relationship dk maximal order local net lower bound follows fano inequality bound discussed accordance birge proposition 
lim condition upper bounds section optimal terms rates dk locally equivalent 
cases di erence considering global local metric entropy 
condition lim characteristic large function classes wehave seen 
case entropy quantities lemma asymptotically equivalent loc contrast lim nite dimensional parametric cases smaller order 
case may determine satisfactory lower bound convergence rate 
results lower bounds know global metric entropy upper bounds lim stronger ho entropy assumption condition local entropy condition gives right order upper bounds global entropy results suboptimal upper bounds logarithmic factor 
inhomogeneous nite dimensional spaces general results available identify minimax rates convergence 
determination minimax rates convergence local entropy conditions provide adaptive estimators di erent kinds approximating models see birge massart barron birge massart yang barron 
appendix proofs lemmas main results proof lemma show condition satis ed conditions 
assume max minimizes densities max rst condition 
consequence condition dk dk second log log log log log log equality special case parallelogram identity see csiszar korner pp 

max rst condition satis ed 
proof lemma proof truncation 
fx gig dh wehave gc forx gc follows gc pp gc gc gd implies rr gd gc gd gd probability density function respect simple calculation gd gd triangle inequality pz pf pg pg gd lemma section completes proof 
log lemma assume conditions satis ed 
satis es chosen log proof lim 
assumption log small log large 
condition 
take large 


similarly 

completes proof 
lemma standard convex analysis 
lemma convex class densities 
xed density suppose exists density 
kf 
proof fix density 
consider expanding squares kf kf hf coe cient hf negative second term right hand side equality dominates sum negative eventually 
tohave hf kf expanding squares simpli cation kf hf arbitrary follows 
lemma probability density functions respect nite measure ifp log log log log 
upper bound distance birge massart lemma 
proof lower bound see yang barron 
lemmas section 
follow notations 
lemma assume vk positive constants 
exist positive constants vk proof rst relate distance dk distances ande withe adding log log eh eh eh log log bound terms sum 
assumptions easily ande follows 

eh log eh eh eh log eh eh eh ke similarly log 
term summation expression log log jje eh ke eh eh rst inequality follows log inequality follows similar inequality peg qq peg peg cd ke 
max ke ife ande nets ine ande dk distance respectively fe provides net dk distance follows vk vk vk uniformly upper bounded lower bounded away distance dk distance equivalent vk constant 
lemma vk constant lemma follows 
lemma nonnegative function class covering entropy ofe fg gd dk upper bounded terms packing entropy ofe distance follows positive constant 
vk proof suppose densities modify density controlled 
fx 
triangle inequality bound follows zz fg bc fg fg bc sup sup modifying net ofe distance described sup net ofe dk 
completes proof 
lemma class nonnegative functions gd 
packing entropies ande distance relationship proof consider 
corresponding densities 
gd wehave subset densities ine separated distance corresponds set separated distance 
follows 
lemma uniformly bounded class measurable functions respect probability measure lete fe corresponding density class 
lq packing entropies ande mq mq respectively 
exist positive constants small mq mq mq log proof 
easily shown kq kq constant depends 
asa consequence mq mq 
ffi mg packing set lq distance ine exists fi kq uniform boundedness assumption log log fi kq depending 
log exists log log fi kq log log fi log kq log fi kq log consequence flog fi jg net lq distance 
relationship covering packing entropies see section mq mq log 
follows 
ash 
information theory wiley interscience new york 
reprinted dover 
barron 
bayes rules consistent information 
open problems communication computation pp 

cover gopinath editors springer verlag 
barron 
neural net approximation proc 
yale workshop adaptive learning syst narendra ed yale university may 
barron 
universal approximation bounds superpositions sigmoidal function ieee trans 
inform 
theory 
barron 
approximation estimation bounds arti cial neural networks machine learning 
barron birge massart 
risk bounds model selection penalization appear probability theory related fields barron cover 
minimum complexity density estimation ieee trans 
inform 
theory 
barron 
information theory ciency preprint 
barron 
sheu 
approximation density functions sequences exponential families ann 
statist 

bickel ritov 
estimating integrated squared density derivatives sharp best order convergence estimates indian statist 
ser 

birge 
approximation dans les theorie de estimation 
verw 
geb 

birge 
estimating density hellinger distance strange facts probability theory related fields 
birge massart 
rates convergence minimum contrast estimators probability theory related fields 
birge massart 
minimum contrast estimators technical report universite paris sud 
birge massart 
estimation integral functionals density ann 
statist 

birge massart 
model selection adaptive estimation appear research papers probability statistics festschrift le cam 
birman piecewise polynomial approximation functions class matem 

birman 
quantitative analysis sobolev embedding theorems application spectral theory th math 
school english translation 
amer 
math 
soc 
transl 


applications piecewise polynomial approximations functions anisotropic classes soviet math 
dokl 
huber 
estimation des minimax 
verw 
geb 
ju 
order growth entropy certain compact classes functions soviet math 
dokl 
carl 
entropy numbers embedding maps besov spaces application eigenvalue problems proc 
royal soc 
edinburgh 

statistical decision rules optimal inference nauka moscow english transl 
amer 
math 
soc 
transl 

clarke barron information theoretic asymptotics bayes methods 
ieee trans 
inform 
theory 
clarke barron 
je rey prior asymptotically favorable entropy risk 
statist 
planning inference 
clements 
entropy sets real valued functions paci math 

cover thomas 
elements information theory wiley newyork 
cox 
approximation squares regression nested subspaces ann 
statist 
csiszar korner 
information theory coding theorems discrete memoryless systems academic press new york 
davisson 
universal noiseless coding ieee trans 
inform 
theory 
davisson leon garcia 
source matching approach nding minimax codes ieee trans 
inform 
theory 
devore lorentz 
constructive approximation springer verlag new york 
devroye 
course density estimation birkhauser boston 
donoho 
unconditional bases optimal bases data compression statistical estimation applied computational harmonic analysis 
donoho 
unconditional bases bit level compression technical report department statistics stanford university 
donoho liu 
rates convergence ii ann 
statist 

donoho johnstone kerkyacharian picard 
density estimation wavelet thresholding preprint 
donoho johnstone 
minimax estimation wavelet shrinkage appear ann 
statistics 
triebel 
entropy numbers approximation numbers function spaces proc 
london math 
soc 
yu 
pinsker 
estimation square integrable probability density random variable english transl 

problems inform 
transmission 
yu 

nonparametric estimation density unknown smoothness theory probab 
appl 

fano 
transmission information statistical theory communication press cambridge mass wiley newyork 
farrell 
best obtainable asymptotic rates convergence estimation density function point ann 
math 
statist 


lower bound risks nonparametric estimates densities uniform metric theory probab 
appl 


density estimation view kolmogorov ideas approximation theory ann 
statist 
haussler 
general minimax result relative entropy preprint 
haussler opper 
general bounds mutual information parameter conditionally independent observations preprint 

estimation nite dimensional parameter gaussian white noise soviet math 
dokl 
capacity smooth signals soviet math 
dokl 

estimation distribution density zap 




bounds risks non parametric regression estimates theory probab 
appl 

jones 
simple lemma greedy approximation hilbert space convergence rates projection pursuit regression neural network ann 
statist 
kolmogorov 
capacity sets function spaces mat 
nauk english transl 

amer 
math 
soc 
transl 

koo kim 
wavelet density estimation approximation statistics probability letters 
kullback leibler 
information su ciency ann 
math 
statist 

le cam 
convergence estimates dimensionality restrictions ann 
statist 
le cam 
asymptotic methods statistical decision theory springer verlag new york 
lorentz 
metric entropy approximation bull 
amer 
math 
soci 

lorentz 
constructive approximation advanced problems springer verlag new york 

approximation dimension bases nuclear spaces mat 
nauk 
modha 
rates convergence density estimation neural networks manuscript 
nemirovskii 
nonparametric estimation smooth regression functions comput 
syst 
sci 

nemirovskii tsybakov 
rates convergence nonparametric estimates maximum likelihood type english transl 

problems inform 
transmission 
plenum publ 

spline smoothing regression models asymptotic ciency ann 
statist 
pinsker 
optimal ltration square integrable signals gaussian noise 
english transl 

problems inform 
transmission 
pollard 
hypercubes minimax rates convergence preprint 
rissanen 
universal coding information prediction estimation ieee trans 
inform 
theory 
rissanen speed yu 
density estimation stochastic complexity ieee trans 
inform 
theory 
shen wong 
convergence rates sieve estimates ann 
statist 

entropy classes andw metric 
akad 
nauk sssr 
english transl 
soviet math 

stone 
optimal global rates convergence nonparametric regression ann 
statist 

approximation functions bounded mixed derivative mat 
inst 
english transl 
proc 
inst 
math issue 

estimation asymptotic characteristics classes functions bounded mixed derivative di erence mat 
inst 
english transl 
proc 
inst 
math issue 

functions real variable macmillan new york 

order growth entropy spaces real continuous functionals de ned connected mat 
nauk 
triebel 
interpolation properties entropy diameters 
geometric characteristics embedding function spaces sobolev besov type mat 
english transl 
math 
ussr sb 
triebel 
theory function spaces ii birkhauser basel boston 
van de 
hellinger consistency certain nonparametric maximum likelihood estimates ann 
statistics 
wong shen 
probability inequalities likelihood ratios convergence rates sieve ann 
statist 
yang 
complexity model selection submitted department statistics yale university 
yang barron 
asymptotic property model selection criteria preprint 

rates convergence minimum distance estimators kolmogorov entropy ann 
statist 

lower bound error nonparametric regression type problems ann 
statist 
yu 
fano le cam appear research papers probability statistics festschrift honor le cam 
yu 
lower bounds expected redundancy non parametric classes ieee trans 
inform 
theory 
yang department statistics iowa state university ames ia andrew barron department statistics yale university box new haven ct 
