incremental support vector machine classification glenn fung mangasarian introduced proximal support vector machine classifier fast simple incremental support vector machine svm classifier proposed capable modifying existing linear classifier retiring old data adding new data 
important feature proposed single pass algorithm allows handle massive datasets huge blocks data say order millions points stored blocks size usually small typically dimensional input space data resides 
demonstrate effectiveness algorithm classify dataset points dimensional input space classes hours mhz pentium ii processor 
keywords incremental classifier massive data classification support vector machines fast simple classification algorithm proposed classifies points assigning closest parallel planes input feature space pushed apart far possible 
formulation interpreted regularized squares considered general context regularized networks leads extremely fast simple algorithm generating linear nonlinear classifier merely requires solution single computer sciences department university wisconsin madison wi 
cs 
wise 
edu 
sciences department university wisconsin madison wi 
cs 
wisc edu corresponding author 
system linear equations 
contrast standard support vector machines svms assign points halfspaces solve quadratic linear program require considerably longer computational time 
advantage structure positive definite matrix constituting system linear equations usually small dimensional input space proximal svm classifier able propose incremental algorithm capabilities old data easily retired new data just easily incorporated classifier 
ii storage capacity required order 
iii computational time order 
iv extremely large datasets order classified incrementally 
data compression large data block size data block size easily carried incremental svm large briefly summarize contents 
section describe proximal linear support vector machine state linear proximal algorithm incremental algorithm 
section introduce linear incremental svm algorithm solving linear proximal svm formulation section incrementally 
section contains numerical test results linear incremental classification algorithm including classification points dimensional input space hours minutes pentium ii mhz machine 
section concludes 
word notation background material 
vectors column vectors transposed row vector prime superscript vector dimensional input space sign function sign defined sign xi sign xi scalar inner product vectors dimensional real space denoted norm denoted 
matrix ai ith row row vector jth column column vector ones arbitrary dimension denoted identity matrix arbitrary dimension denoted linear proximal support vector machine consider problem depicted classifying points ndimensional input space represented matrix membership point ai class specified diagonal matrix plus ones minus ones diagonal 
problem standard support vector machine linear kernel quadratic program parameter min aw 
depicted normal bounding planes bound sets respectively 
constant determines location relative origin 
classes strictly linearly separable error variable case shown plane xw bounds class points plane xw bounds class points follows dii consequently plane xw midway bounding planes separating plane separates completely approximately depicted 
quadratic term twice reciprocal square norm distance bounding planes see maximizes distance called margin 
maximizing margin enhances generalization capability support vector machine 
classes linearly inseparable case shown planes bound classes soft margin bound approximately error determined nonnegative error variable yi dii yi dii 

standard support vector machine classifier space approximately bounding planes equation soft error margin plane equation approximately separating norm error variable minimized parametrically weight resulting approximate separating plane depicted 
plane acts linear classifier follows sign 
ca 
generate proximal svm modify standard svm formulation similar manner optimization problem replaced problem min crn aw note explicit nonnegativity constraint needed component yi negative objective function decreased setting yi satisfying corresponding inequality constraint :10.1.1.17.5760
note norm error vector minimized norm margin bounding planes maximized respect orientation relative location origin 
extensive computational experience indicates formulation just classical formulation added advantages strong convexity objective function 
key idea proximal svm simple funda mental change formulation replace inequality constraint equality follows modification simple changes nature optimization problem significantly 
fact turns write explicit exact solution problem terms problem data show impossible standard svm formulations combinatorial nature 
geometrically formulation depicted interpreted follows 
planes bounding planes anymore thought proximal planes points class clustered pushed far apart possible term objective function reciprocal norm distance squared planes space 

proximal support vector machine classifier space planes xw points sets pushed apart optimization problem 
substituting constraint terms uncon strained optimization problem min aw 
setting gradient respect gives aw 
aw 
noting dd solving il gives mj oej lo oe 
defining solution written expression involves solution system linear equations determined small dimensional matrix 
concreteness explicitly state simple algorithm 
algorithm 
linear proximal svm data points represented matrix diagonal matrix labels denoting class row generate linear classifier follows define vector ones 
compute positive typically chosen means tuning validating set 
ii classify new solution step 
standard svms support vectors consist data points complement data points dropped problem changing separating plane 
standard svm formulation support vectors correspond data points lagrange multipliers nonzero solving data points give answer solving entire dataset 
proximal formulation lagrange multipliers computed explicitly constraint eliminated solving problem shown equation merely multiple error vector consequently components typically nonzero data points usually lie proximal planes concept support vectors modified follows 
define concept support vectors data points ai error vector yi absolute value 
typically pick small data support vectors 
re solving proximal svm problem data points adjusted typically upwards tuning set gives test set correctness essentially identical obtained entire dataset 
turn attention incremental algorithm proximal svm formulation described 
incremental svm classification describe simple procedure applied proximal svm described previous section allow generate new linear classifier retiring old data simultaneously adding new data 
augment input vector define augmented input vector follows coupling definition solution expression linear classifier written explicitly terms input data follows sign ee ede 
key properties classifier size data stored order ee order ede number data points order millions 
ii time solve system linear equations determined positive defi nite ee order 
typically classifier extremely effective massive datasets millions data points 
simple facts describe incremental algorithm capable retiring desired portion data time adding new data generate appropriately altered classifier 
keep notation simple assume current classifier input dataset corresponding diagonal matrix 
suppose old subset data represented submatrix corresponding diagonal submatrix needs retired removed characterization classifier leaving complement determine new classifier new data 
new set data points represented xm new corresponding diagonal matrix needs added characterization new classifier 
classifier updated reflect retirement addition stated incremental algorithm 
algorithm 
linear incremental vm data points represented matrix diagonal matrix labels denoting class row generate incremental linear classifier retiring old data represented submatrix mx corresponding diagonal submatrix old adding new data represented new matrix corresponding diagonal matrix follows ee 
note block data say mx need store relatively small matrix vector 
quantities order respectively allow retire add data classifiers wish 
furthermore incremental process allows handle arbitrarily large datasets successively adding blocks data form demonstrated numerical results section 
note order considered compression larger dataset order mi 
takes operations compute ei operations compute ei di amount computation incremental algorithm grows linearly number data points shown 
numerical testing computations performed university wisconsin data mining institute machine utilizes mhz ii allows maximum gigabytes memory process 
computer runs windows nt server matlab installed 
multiprocessor machine processor experiments matlab single threaded application distribute load processors 
principally interested massive problems focus numerical tests datasets synthetically generated totally stored disk 
sets generated ndc normally distributed clustered dataset generator 
data generated clusters normally distributed points adjustable linear 
dataset consists points dimensional input space 
purpose dataset demonstrate capability incremental svm algorithm obtain linear classifier increments points 
dataset generated stored subsets size points 
testing set size generated read disk compute store memory update memory cl el de cmp ute output ii ll discard 
keep ere 
flow chart linear incremental algorithm 
distribution generate dataset resulting essentially linear separability 
important note block data read disk new block data processed data kept memory matrix size vector size case 
time new block read disk new incoming data processed algorithm matrix vector updated processed data discarded leaving memory ready new incoming data 
process graphically depicted 
proposed incremental algorithm solved point problem hours minutes seconds 
minutes slightly total time spent reading data disk 
training set correctness testing set correctness 
graph exhibiting linear dependence computational time algorithm number data points generate solution 
millions 
computational time linear incremental algorithm function number data points compute solution point dataset 
second dataset points dimensional input space tended simulate months data 
dataset demonstrate algorithm incrementally retire old points add new points successive steps simulate daily retirement oldest data addition newest data 
previous experiment data assumed disk just final separating hyperplane required 
case assume just days data day oldest block data retired points new block data points corresponding new data just collected added 
furthermore assume time update data new separating hyperplane calculated 
data generated way starting final separating hyperplanes differ considerably 
means incoming data constantly changing separating criteria learned changes dynamically respect time 
gradual change resulting hyperplane incrementally retiring old points adding new points observed 
order show gradual change 
normals separating hyperplanes corresponding day intervals 
measure differences planes calculating angle normals 
hyperplane normals compute wi wj cos plot showing angles normals hyperplanes data changes depicted 
proposed fast simple incremental support vector machine classifier proximity class points parallel planes pushed apart appropriately 
principal features proposed algorithm ability retire old data add new data easily effectiveness speed handling massive datasets incrementally ability compress large datasets compression ratio order mn order millions order 
features coupled simplicity hopefully useful classification tool 
research described data mining institute report september supported national science foundation ccr cda air force office scientific research microsoft 
bibliography cherkassky 
learning data concepts theory methods 
john wiley sons new york 
pontil poggio 
regularization networks support vector machines 
advances computational mathematics 
pontil poggio 
regularization networks support vector machines 
smola bartlett sch kopf schuurmans editors advances large margin classifiers pages cambridge ma 
mit press 
fung mangasarian 
proximal support vector machine clas 
provost srikant editors proceedings kdd knowledge discovery data mining august san francisco ca pages new york 
computing machinery 
ftp ftp cs 
wisc cdu pub dmi 
ps 

lcc mangasarian 
reduced support vector machines 
technical report data mining institute computer sciences department university wisconsin madison wisconsin july 
proceedings siam international conference data mining chicago april cd rom proceedings 
fp fp cs wisc cdu pub dmi oo ps 
lcc mangasarian 
ssvm smooth support vector ma chine 
technical report data mining institute computer sciences department university wisconsin madison wisconsin september 
computational optimization applications october appear 
ftp ftp cs 
wisc cdu pub dmi ps 
mangasarian 
generalized support vector machines 
smola bartlett sch kopf schuurmans editors advances large margin classifiers pages cambridge ma 
mit press 
ftp ftp cs wisc cdu math prog ps 
mangasarian 
successive overrelaxation support vector machines 
ieee transactions neural networks 
ftp ftp cs wisc cdu math prog ps 
mangasarian 
active support vector machine clas 
todd leen thomas dietterich volker tresp editors advances neural information processing systems pages 
mit press 
ftp ftp cs wisc cdu pub dmi oo ps 
mangasarian 
lagrangian support vec tor machines 
journal machine learning 
ft ft cs wisc edu pub dmi tech ports oo ps matlab 
user 
mathworks natick ma 
www mathworks com 

ndc normally distributed clustered datasets 
www cs wisc edu data ndc 
vapnik 
nature statistical learning theory 
springer new york 
vapnik 
nature statistical learning theory 
springer new york second edition 

