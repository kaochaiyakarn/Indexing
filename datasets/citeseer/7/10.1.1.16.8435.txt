search engines web dynamics knut fast search transfer asa knut fast rolf fast search transfer asa rolf fast study dimensions web dynamics context large scale internet search engines 
growth update dynamics clearly represent big challenges search engines 
show problems arise components search engine model 
furthermore fast search engine architecture case study showing possible solutions web dynamics search engines 
focus demonstrate solutions practice real systems 
service running live www com major portals worldwide queries day full text documents crawl base documents updated days rate documents second 
discuss evolution web important issues search engines scheduling query execution increasingly heterogeneous architectures handle dynamic web 
search engines grown far popular way navigating web 
evolution search engines started static web relatively simple tools wwww mcb 
altavista launched created bigger focus search engines srr 
marketplace search engines dynamic actors fast www com google inktomi altavista working different technical solutions business models order viable business including paid inclusion paid positioning advertisements oem searching large number analyses structure dynamics web 
drawn web growing high pace dynamics web shifting 
dynamic real time information available web 
dynamics web creates set tough challenges search engines 
section define model internet search engines 
section survey existing studies dynamics web 
focus growth web update dynamics individual documents web 
section provide overview fast crawler describe design meets challenges web growth update dynamics 
continue section similar description indexing search engines 
outline challenges provide benchmarking figures section section respectively 
fast search engine technology case study 
focus web dynamics pose key challenges large scale internet search engines challenges addressed practical working system 
main contribution offer insight large scale commercially operated internet search engine designed implemented 
search engine model practical commercially operated internet search engines centralized architecture relies set key components crawler indexer searcher 
architecture seen systems including www mcb google bp fast search engine 
definition crawler 
crawler module aggregating data world wide web order searchable 
heuristics algorithms exists crawling links 
definition indexer 
module takes collection documents data builds searchable index 
common practices inverted files vector spaces suffix structures hybrids 
definition searcher 
searcher working output files indexer 
searcher accepts user queries runs index returns computed search results issuer 
crawler indexer searcher users local store copy search engine model 
model illustrated 
systems keep local store copy crawled data 
definition local store 
local store copy snapshot web crawling time document 
systems usually run crawler indexer searcher sequentially cycles 
crawler retrieves content indexer generates searchable index searcher provides functionality searching indexed data 
refresh search engine indexing cycle run 
real systems different phases indexing cycle may overlap split different sub phases 
instance unacceptable searcher running crawler indexer 
search systems fundamental steps indexing cycle easily discernible 
literature distinguishes batch incremental crawlers 
different papers different terms definitions 
uses definitions cho garcia molina cgm 
batch crawler starts scratch empty local store indexing cycle fetches document twice cycle 
incremental crawler erases local store 
retrieved sufficient number documents continues re fetching documents updating local store copies 
new indexing cycle starts incremental crawler continues left indexing phase started 
dynamics web section outline nature web dynamics 
define different aspects web dynamics review literature topic 
attempt provide complete review published studies focus number representative significant works 
dimensions web dynamics concept web dynamics dimensions important large scale internet search engines 
web grows size studies agree grows exponential rate 
poses serious challenge scalability search engines aspire cover large part web 
second content existing web documents updated 
search engine coverage important local store copy fresh 
search engine scale large volume documents able refresh documents timely manner 
requirement grow important people start turning search engines breaking news dynamic content 
furthermore link structure web constant flux 
links documents constantly established removed 
dynamics link structure important search engines long link structure important component ranking search results 
link structure ranking creates slow working positive feedback loop entire web popular sites popular expense known new sites 
number studies link structure instance study broder bro 
studies cover dynamics link structure 
consider dynamics link structure 
dimension web dynamics introduced structure 
xml evolving rapidly inter service systems ability search engine get hood presentation engine understand structure semantics data key feature generation engines 
web growth dynamics web grown incredible rate inception years ago 
web started connection data available html page information 
web driving factor establishing new era storage retrieval information seeing 
web took storage application era information era web huge storage retrieval network 
html format description documents 
combination uris html connection file system object uri obvious web servers today commonly applications serve html files directly file system requests 
enormous growth information want publish web created need space advanced publication systems tying business applications web servers 
web platform new information era caused web evolve application transactional space 
trading major application web 
evolutionary trends web erased connection html documents uris actual content 
furthermore limited percentage web indexable search engine 
trading advanced information systems introduce personalized transaction dependent content normally accessible standard web aggregation methods 
studies size web conducted 
conducted early study late ot 
showed unique web servers number documents estimated 
lee giles steve lawrence conducted known tests estimate size web lg explore accessibility indexable web lg 
lg study multivariate analysis search engine coverage overlap estimate size web 
search engines lower bound estimate size web pages 
study search engines result sets overlap measure clearly indexable web account percentage web touched search engine 
looking estimated coverage search engine respect combined coverage hotbot comprehensive time measure approximately coverage 
test repeated extended nature lg 
months passed showed significant increase number indexable documents 
lower bound size estimated documents search engines significant coverage indexable web 
maximum coverage estimated 
ha theory growth dynamics world wide web 
stochastic classes considered growth rates pages site growth new sites 
universal power law predicted distribution number pages site 
brings theories enable determine expected number sites size extensive crawling 
papers discuss growth referred indexable web study performed percentage pages indexable versus non indexable 
study bright planet llc bp introduced concept deep web 
deep web easily identified subset web discussed lg lg non indexable web 
percentage web pages belonging category growing higher rate indexable pages 
natural cause web moving simple document share space information sharing space application sharing space 
key findings study bright planet terabytes information terabytes assumed surface web 
approximately documents 
largest growing category web information 
highly relevant content deep web 
document update dynamics number challenges faced handle document update dynamics largescale internet search engine 
develop model documents updated 
develop crawling strategy maximizes freshness local store document update model 
evaluate performance various update strategies need mechanisms measuring freshness local store 
challenges studied existing literature 
cho garcia molina published studies web documents updated crawling strategies cgm cgm cgm 
models experiments indicate web document updates modeled independent poisson processes 
document di updated poisson process change rate change rates independent 
experiments web indicate average document changed days documents changed days cgm 
cho garcia molina developed statistical estimators poisson parameters various assumptions 
derived estimators uniform random observation web documents known unknown time document update 
new estimators better naive estimator number document updates observed divided observation time cgm 
optimal crawling strategy model crawler local store 
propose framework measuring date local store concepts freshness age 
define freshness document di time probability document date time 
age defined documents date time document update real world 
freshness age interesting measure average freshness age documents time cgm 
term freshness informal manner 
models document change freshness measures optimal scheduling policies incremental crawler 
observations refreshing document uniform update frequencies better document update frequencies proportional estimated document change frequencies scheduling policy optimizing freshness penalizes documents changed 
intuitively documents change soon contribute freshness local store 
scheduling policy optimizing age favors documents changed actual change small 
cybenko bc performed number experiments discover web documents updated 
conclude web documents updated poisson process 
cybenko propose novel measure freshness termed currency 
document di current date time units ago probability 
measure captures aspirations actual achievements regarding freshness 
daily newspaper may day current meaning articles date day ago 
estimate internet search engine containing documents refresh documents day able maintain week currency 
edwards mccurley tomlin emt crawler minimizes number obsolete documents repository making priori assumptions documents updated 
measured document change rates divide crawling resources documents change rate 
solved vast optimization problem find optimal distribution crawling resources 
search engines search technology rich body literature describing practical large scale crawling searching systems particular address issues relating dynamic web 
brin page described early version google system bp 
address issues relating growth web scaling document volume address refreshing local store 
heydon najork describe scalable web crawler hn 
crawler scalable sense required machine resources bounded depend number retrieved documents 
crawler reportedly runs single large machine probably quite easily implemented crawler cluster scale larger document volumes processing load manner similar fast crawler discussed 
heydon najork discuss refreshing crawled documents 
edwards mccurley tomlin emt provide design details crawler 
crawler runs incrementally constantly refreshing documents ensure freshness time described previous section 
provide details scheduling algorithm 
document classified measured update frequency crawling resources divided classes 
formulated solved vast optimization problem computing allocate crawling resources different document update classes 
aggregation dynamic content section fast crawler case study illustrate addressed challenges scaling size web ensuring freshness local store 
overview fast crawler fast crawler consists cluster interconnected machines depicted 
machine cluster assigned partition web space crawling 
crawler machines communicate machines star network 
machines relatively independent exchanging information discovered hyperlinks 
web content retrieval fast crawler deployment overview 
document scheduler distributor crawler machines web document processor local store components data flow single crawler machine 
depicts main components single crawler machine system 
solid arrows indicate flow document content dotted arrows indicate flow document uris possibly associated meta information 
document scheduler responsible figuring document crawl 
conceptually means maintaining huge prioritized queue uris crawled observing informal social code web robots robots exclusion protocol kos various constraints access pattern individual web servers imposed avoid overloading servers crawler requests 
document scheduler sends stream uris document processor 
document processor responsible retrieving documents web servers performing processing 
component consists manager module provides plug interface set processing modules 
manager routes documents pipeline processing modules configuration information document content type architecture provides necessary flexibility quickly support new document processing functionality crawler 
currently processing modules parsing html documents various multimedia formats classification language classification offensive content html parser discovers hyperlinks documents passes back scheduler 
processing crawler stores relevant document content meta information local store 
document content represents bulk data optimized storage system efficiently writing updating individual documents streaming entire document collection indexing step 
need efficient random access reading document content 
hybrid hashing logging storage system 
hirai describe webbase system system similar storage system 
document meta information kept high performance random access database 
distributor exchanges hyperlink information machines crawler cluster 
static mapping relevant hyperlink information crawler machine 
crawler architecture contains important components duplicate detection link ranking respectively 
modules relatively complex outside scope 
fast crawler incremental crawler 
document repository reached target size crawler continues refreshing existing documents detect changes 
fetches new documents replace documents deleted repository crawler discovers deleted web reasons 
crawler stopped 
temporarily suspended part indexing process indexing completed crawler resumes operation left 
scalability large scale crawler scalable respect document storage capacity document retrieval capacity 
architecture crawler machine responsible retrieval processing storage partition document space 
different machines constituting crawler cluster independently exchanging discovered uris 
storage capacity cs processing capacity cp crawler cluster sum capacity individual machine cs cp additional constraint crawler retrieval processing capacity capacity defined total network bandwidth available crawler cluster cn 
total retrieval capacity crawler cluster min usually network bandwidth represents highest cost running large scale crawler sense dimension system note bandwidth communicating internally crawler cluster proportional inbound bandwidth retrieving content web 
internal bandwidth exchanging hyperlink information number hyperlinks proportional number retrieved documents 
easily increase document storage capacity cs adding new machines crawler cluster redefine workload partitioning distributor accordingly 
give extra processing power cp free 
increasing document storage capacity require network bandwidth cn crawler web internally individual crawler nodes comprising cluster 
result crawler scales linearly document storage capacity 
scale linearly document retrieval capacity cr 
retrieving processing storing documents unit time requires linear increase inbound network bandwidth web linear increase network bandwidth machines comprising crawler cluster cn 
scaling document retrieval capacity increasing network capacity assumes machine crawler cluster spare processing capacity handle increased load case additional machines added cluster achieve required processing power 
increase document storage capacity 
total system scales linearly storage retrieval capacity long reasonable balance 
practice usually case want ratio retrieved documents unit time total number documents drop scale system eventually impact freshness 
crawler maintain freshness ratio kept constant scaling constant cr system scales components robust regards failure components probability having working system drop number components 
crawler robust failure machine crawler cluster 
machine works independently scheduling retrieving processing storing documents 
dependency machines exchange information new hyperlinks 
hyperlink information unavailable crawler machine simply queued sending machine designated receiver available 
freshness scheduling fast crawler provides fresh data search engine high document retrieval capacity scheduling algorithm prioritizing retrieval documents updated web 
scheduling algorithm increases constant factor freshness equation previous section 
normal operation crawler refreshing local store approximately constant number documents limited capacity search engine crawler 
state crawler retrieve new documents old documents removed local store crawler attempted refresh document exist web anymore reasons 
document storage capacity increased normally relatively quickly retrieve documents reach steady state 
providing fresh search results users implies short indexing cycles capacity refresh documents local store indexing cycle 
situation scheduling algorithm key element ensuring fresh local store 
maximize freshness spend possible crawler network capacity refreshing documents changed 
fast crawler currently uses relatively simple algorithm adaptively computing estimate refresh frequency document 
basically algorithm decreases refresh interval document changed retrievals increases document changed 
input scheduler prioritizes different documents decides refresh document 
addition scheduler configured global minimum maximum refresh intervals keep refresh rate document sensible bounds allow refreshing documents observed change 
cho garcia molina observe optimal refresh documents updated frequently possible cgm cgm 
intuitively documents obsolete anyway repository indexed 
fact conclude uniform refresh policy documents updated equally superior proportional refresh policy 
practice big problem 
repository size required large scale search engine documents waiting refreshed diminish effect relatively documents updated 
configure scheduler avoid rescheduling document indexing cycle 
freshness heterogeneity optimizing freshness scheduling assume documents freshness requirements 
having fresh copy document repository just valuable having fresh copy document 
situation real world 
consider examples large scale web crawler industry trend offer pay inclusion service content providers 
offer usually comes service level guarantees covering refresh indexing intervals 
trend searching increasingly dynamic content news 
content refreshed indexed value users 
scenarios supported large scale crawler relatively simple scheduling algorithm 
fast crawler solved problem permitting heterogeneity cluster crawler machines 
means different machines configured different storage processing capacity cs cp relatively small number machines dedicated special purposes crawling content paid inclusion programs news sites 
machines configured specially service requirements services mind control load machines sacrificing high capacity efficiency main bulk machines crawler cluster 
keep dedicated machines part cluster just machines machines efficiently share link information computed link ranks cost relatively small small increase complexity distributor 
shows example heterogeneous crawler cluster 
depicts crawler cluster dedicated machines crawling news content addition large scale web crawler 
included machines comprising multimedia crawler illustrate flexibility architecture 
example large scale web crawler typically configured large cs relatively low news crawler configured lower cs maintain high multimedia crawler high cs low just large scale web crawler operating crawler separate part cluster allows control resources different media types 
cooperation providers multimedia crawler news crawler example heterogeneous crawler deployment sub clusters different crawling strategies 
approach experimenting cooperating content providers improve freshness 
different models cooperating content providers section outline different options 
infoseek proposed standard web server meta file named txt 
meta 
content web server crawler meta information pull model 
complement established robots txt de facto standard 
purpose txt file provide information crawlers documents changed mirrors server txt standard able establish crawling indicates today server 
today number content providers proprietary meta files publish information sites updated 

meta 
content web server crawler meta information push model 
pull model permits crawler perform efficient site scheduling 
model crawler periodically fetches meta information web server 
uses information influence scheduler hints new changed documents 
model illustrated 
arrowheads indicate direction requests 
building crawler supports proprietary meta information formats daunting task 
elaborate approach push content meta information directly content web server crawler 
meta 
content proxy 
meta hybrid meta information pull push model 
provider crawler 
model content provider sends notification crawler document added modified deleted site 
simplest form content provider just submits uri document 
method enhanced submitting various meta information entire content documents 
care taken avoid opening crawler search systems spamming abuse service offered partners 
model illustrated dotted arrow indicating optional request 
third approach hybrid 
obstacles having push technology deployed large set content providers 
instance site operators may reluctant increase complexity systems installing additional software components 
hybrid approach involves developing deploying special proxy systems read site meta information files act instructions files push technology update crawler 
systems single interface crawler independent crawler proxies developed deployed independently 
illustrated 
hybrid model quite powerful 
proxy may limit obtaining metainformation server set servers crawled 
instance fetch meta information sources discover hot spots web direct crawlers spots 
establishing communication pipeline providers natural step lies better understanding content provider 
xml significant player describing semantics dynamics content 
searching dynamic content second third components search engine model indexer searcher need handle different dimensions web dynamics 
traditionally search engines batch oriented processes update build indices 
handle growth size web update dynamics traditional designs fall short 
section study aspects solutions indexer searcher handle dynamic web 
scalability size traffic able handle web growth calls architectural solutions 
traditional solutions inverted files equivalent cost building maintaining searching index worse linear 
possible architecture web search engine fast search engine architecture 
handles scalability dimensions size traffic volume 
architecture distributed architecture classes nodes search nodes search node si holds portion index ii 
total index search nodes separate entity holds search kernel searcher searches index ii returns search results 
search nodes interconnection 
dispatch nodes dispatch node hold searchable data 
dispatcher routing distribution collection merging mechanism 
dispatch node receives queries routes set underlying search nodes si sj 
results collected merged sent issuing client 
search node capacities number documents node di query rate traffic capacity ci 
dispatcher capacity dispatching capacity cdi 
cdi depends number search nodes query sent 
components described build simple architecture allow linear scaling data size 
architecture shown 
box si ii search node holding index partition ii 
entire dataset partitioned dispatch linear scaling data size dispatch scaling size capacity search nodes 
dispatch node broadcasting queries search nodes parallel merging results search nodes build final result set 
assuming search node handles query individually search nodes see linear scale increasing size achieved 
time scale query rate replication provide capacity ci 
dispatch node know search nodes column round robin algorithm rotate different columns 
extended architecture illustrated 
sij search node handling partition illustrated dispatcher knows search nodes column load balance achieving linear performance scale 
number search nodes required dataset size derived denotes optimal size search node partition 
number course dependent actual hardware configurations costs 
furthermore number search nodes handle query rate qt derived qt ci 
limitation architecture clearly lies dispatching system 
dispatcher capacity merging results 
optimal merging algorithms order log denotes number sources number entries 
merge performance limited number search nodes receive query parallel 
ensure linear scalability multi level dispatch system configured 
letting dispatcher dispatch part row letting super dispatcher dispatch dispatch nodes tree architecture shown 
number levels built accommodate scale dimensions 
worst case binary tree number nodes linear size data searched 
immediate observation description architecture implicit fault tolerance 
having multiple nodes index partition ii dispatchers fault tolerant detecting timeouts non replies 
ensure faulttolerance level transparency dispatcher searcher utilized redundant dispatcher 
handling update dynamics second dimension web dynamics update dynamics problem facing search engines 
traditionally structures indexing offline building cases highly time consuming process 
ways indexing processes dynamic 
identifying new inverse structures allow online updates 

utilizing heterogeneous architecture allow dynamic changes dataset 
proposed solutions ir community gotten big acceptance industry 
fast search engine uses second approach cope dynamic updates 
looking distributed architecture described indexing easy goal parallelization 
indexing done indexing individually 
separate search nodes handle partition indexing done individually 
multi level dispatch architecture hours solution indexing possible online search node having slight search nodes easily switch nodes line update parts dataset 
system batch oriented time detecting document update pushed live hours 
crawling system allows heterogeneous clusters crawler nodes easy identify sources different update rates 
architecture described building block cluster search clusters cluster different update frequency 
possible cluster solution shown 
example deploy different clusters receiving data feed different crawler cluster 
update frequencies clusters different general update cost time required hardware linear data size 
challenges sn sn days query router sn evolution dynamic web raises significant challenges search engines 
increasing dynamics size intelligent scheduling increasingly important 
able update important parts index timely rate crucial order bring relevant search results users 
intelligent scheduling heterogeneous crawling push technology crucial building aggregation search systems capable scaling web reasonable price 
size web clearly big challenge important question arising need search documents query 
able intelligently classify predict probable subset data set search queries enable build efficient cost effective solutions 
time deep web growing rate higher current indexable web 
unified practical solution aggregate deep web large scale push technology tight integration publication content management systems evolve address challenge 
explosive growth web calls specific search engines 
focused crawling document classification enables crawlers search engines operate efficiently topically limited document space 
scientific search engine com example engine uses focused multi tier cluster 
sn days crawling document classification 
dynamics homogenous character vertical enabling search experience 
discussed dimensions web dynamics 
growth update dynamics clearly represent big challenges search engines 
shown problems arise components search engine model 
fast search engine architecture copes problems key properties 
architecture described quite simple represent novel ideas 
system architecture relatively simple manageable grows 
real life system service level requirements simplicity crucial operating system able develop 
heterogeneous containing intelligence regards scheduling query processing real life example dealing web dynamics issues today 
service running www com major portals worldwide currently handle queries day 
indexing happens days full index size currently full text documents 
documents selected crawled base full text documents 
crawler architecture enables crawl rate documents second 
system inexpensive shelf pcs running freebsd custom search software 
currently approximately pcs production systems 
machines search nodes 
currently machines crawling 
hardware configuration differs depending role machine architecture time acquisition 
machines typically dual pentium machines mbytes memory 
evolution web dynamics raises clear needs intelligent scheduling aggregate web content technology push aggregation 
doing intelligent query analysis processing able sub linear scaling growth web ideas 
possible create multi tier system tier columns rows handles relatively large part popular queries 
tier columns fewer rows handle remaining queries 
ot tim bray 
measuring web proceedings fifth international world wide web conference www 
bc brian george cybenko 
dynamic web 
proceedings ninth international world wide web conference www 

bp sergey brin larry page 
anatomy large scale hypertextual web search engine 
proceedings seventh international world wide web conference www 

bp deep web surfacing hidden value 
white bright planet 
bro andrei broder graph structure web 
proceedings ninth international world wide web conference www 

cgm junghoo cho hector garcia molina 
synchronizing database improve freshness 
proceedings acm international conference management data sigmod 

cgm junghoo cho hector garcia molina 
evolution web implications incremental crawler 
proceedings th international conference large databases 

cgm junghoo cho hector garcia molina 
estimating frequency change 
unpublished 

emt edwards kevin mccurley john tomlin 
adaptive model optimizing performance incremental web crawler 
proceedings tenth international world wide web conference www 

hn allan heydon marc najork 
mercator scalable extensible web crawler 
world wide web 
vol 
december 
jun hirai sriram raghavan hector garcia molina andreas paepcke 
webbase repository web pages 
proceedings ninth international world wide web conference www 

ha bernardo huberman adamic 
evolutionary dynamics world wide web 
technical report 
xerox palo alto research center 
february 
kos martijn koster 
standard robots exclusion 

www org wc robots html 
lg steve lawrence lee giles 
searching world wide web 
science 
april 
lg steve lawrence lee giles 
accessibility distribution information web nature 
july 
mcb oliver mcbryan 
wwww tools taming web 
proceedings international world wide web conference www 

srr richard seltzer eric ray deborah ray 
altavista search revolution 
osborne mcgraw hill 

