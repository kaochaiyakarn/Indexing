parallel mixture svms large scale problems ronan universit de cp succ 
centre ville canada iro umontreal ca bengio idiap cp rue du martigny switzerland bengio idiap ch bengio universit de cp succ 
centre ville canada iro umontreal ca support vector machines svms currently state art models classi cation problems su er complexity training algorithm quadratic respect number examples 
hopeless try solve real life problems having hundreds thousands examples svms 
proposes new mixture svms easily implemented parallel svm trained small subset dataset 
experiments large benchmark dataset forest di cult speech database yielded signi cant time improvement time complexity appears empirically locally grow linearly number examples 
addition surprise signi cant improvement generalization observed forest 
lot done support vector machines mainly due impressive generalization performances classi cation problems compared algorithms arti cial neural networks :10.1.1.9.6021
svms require solve quadratic optimization problem needs resources quadratic number training examples hopeless try solving problems having millions examples classical svms 
order overcome drawback propose mixture svms trained part dataset 
idea svm mixture new previous attempts kwok support vector mixtures train svms part dataset dataset overcome part done ronan idiap cp rue du martigny switzerland 
time complexity problem large datasets 
propose simple method train mixture show practice method faster training svm leads results svm 
conjecture training time complexity proposed approach respect number examples sub quadratic large data sets 
mixture easily parallelized improve signi cantly training time 
organization goes follows section brie introduce svm model classi cation 
section mixture svms followed section comparisons related models 
section show experimental results rst toy dataset large real life datasets 
short follows 
support vector machines support vector machines svms applied classi cation problems generally yielding performance compared algorithms 
decision function form sign dimensional input vector test example class label input vector th training example associated class label number training examples positive de nite kernel function parameters model 
training svm consists nding minimizes objective function subject constraints kernel di erent forms radial basis function rbf exp kx parameter 
train svm need solve quadratic optimization problem number parameters svms large datasets di cult computing training pair require computation solving may take 
note current state art algorithms appear training time complexity scaling closer 
new conditional mixture svms section introduce new type mixture svms 
output mixture input vector computed follows wm number experts mixture output th expert input wm weight th expert module input transfer function example hyperbolic tangent classi cation tasks 
expert svm took neural network experiments 
proposed model trained minimize cost function train model propose simple algorithm 
divide training set random subsets size near 
train expert separately subsets 

keeping experts xed train minimize training set 

reconstruct subsets example sort experts descending order values wm assign example rst expert list examples order ensure balance experts 

termination criterion ful lled number iterations validation error going goto step 
note step algorithm easily implemented parallel expert trained separately di erent computer 
note step approximate minimization usually done training neural networks 
mixtures svms idea mixture models quite old rise popular algorithms known mixture experts cost function similar equation experts trained gradient descent em dataset subsets parameters trained simultaneously 
algorithm quite demanding terms resources dataset large training time scales 
support vector mixture model author shows replace experts typically neural networks svms gives learning algorithm model 
resulting mixture trained jointly dataset solve quadratic barrier dataset large 
divide conquer approach authors propose rst divide training set unsupervised algorithm cluster data typically mixture gaussians train expert svm subset data corresponding cluster nally recombine outputs experts 
algorithm train separately experts small datasets algorithm notion loop reassigning examples experts prediction expert performs example 
experiments suggest element essential success algorithm 
bayesian committee machine technique partition data subsets train svms individual subsets speci combination scheme covariance test data combine predictions 
method scales linearly small positive constant 
experiments 
number training data fact transductive method operate single test example 
previous case algorithm assigns examples randomly experts bayesian framework principle allow nd better assignments 
regarding proposed mixture svms number experts grows number examples number outer loop iterations constant total training time experts scales linearly number examples 
total number examples choose number expert ratio constant number outer loop iterations training time svm examples empirically slightly total training time experts kr kr constants gives total training time 
particular gives 
actual total training time include times training time may potentially grow rapidly 
appear case experiments yielding apparent linear training time 
focus methods reduce training time guarantee linear training time outer loop iteration 
experiments section sets experiments comparing new mixture svms machine learning algorithms 
note svms experiments trained svmtorch 
toy problem rst series experiments rst tested mixture arti cial toy problem generated training examples test examples 
problem non linearly separable classes input dimensions 
show decision surfaces obtained rst linear svm gaussian svm nally proposed mixture svms 
simple linear function linear svms mixture arti cial problem shows clearly algorithm able combine linearly simple models order produce non linear decision surface 
large scale realistic problem forest realistic problem series experiments part uci forest dataset modi ed class classi cation problem binary classi cation problem goal separate class classes 
example described input features normalized dividing maximum training set 
dataset examples allowed prepare series experiments follows kept separate test set examples compare best mixture svms learning algorithms 
validation set examples select best mixture svms varying number experts number hidden units 
trained models di erent training sets examples 
mixtures expert svms gaussian kernel mlp hidden units 
note transfer function tanh 
forest dataset available uci website address ftp ftp ics uci edu pub machine learning databases covtype covtype info 
linear svm gaussian svm mixture linear svms comparison decision surfaces obtained linear svm gaussian svm linear mixture linear svms dimensional classi cation toy problem 
note number examples quite large selected internal training parameters gaussian kernel svms learning rate held portion training set 
compared models single mlp number hidden units selected cross validation units single svm parameter kernel selected cross validation mixture svms replaced constant vector assigning weight value expert 
table gives results rst series experiments xed training set examples 
select variants gated svm mixture considered performance validation set training time 
svms 
selected model experts hidden units 
model hidden units performance test set taken minutes machine minutes machines 
train test time minutes error cpu cpu mlp svm uniform svm mixture gated svm mixture table comparison performance mlp hidden units single svm uniform svm mixture output value expert nally mixture svms proposed 
seen gated svm outperformed models terms training test error 
note training error single svm high hyper parameters selected minimize error validation set values yield lower training error larger test error 
faster machine svm mixture easily parallelized expert trained separately reported time took train machines 
rst attempt understand results say power model lie mlp single mlp pretty bad svms single svm gated mixture divided problem sub problems uniform mixture performed badly 
combination elements 
series experiments order see uence number hidden units number experts mixture 
shows validation error di erent mixtures svms number hidden units varied number experts varied 
clear performance improvement number hidden units increased improvement additional experts exists strong 
note training time increases rapidly number hidden units slightly decreases number experts uses computer expert 
number experts validation error function number hidden units number experts number hidden units comparison validation error di erent mixtures svms various number hidden units experts 
order nd algorithm scaled respect number examples compared mixture experts experts hidden units di erent training set sizes 
table shows validation error mixture svms trained training sets sizes 
range particular dataset mixture svms scales linearly respect number examples quadratically classical svm 
interesting see instance mixture svms able solve problem examples hours computers taken month solve problem single svm 
gure shows evolution training validation errors mixture svms gated mlp hidden units iterations algorithm 
convince loop algorithm essential order obtain performance 
clear empirical convergence outer loop extremely rapid 
veri cation large scale problem order verify results obtained forest replicable large scale problems tested svm mixture speech task 
numbers dataset training time function number train examples number train examples comparison training time mixture svms experts hidden units trained di erent training set sizes 
error function number training iterations number training iterations error train error validation error comparison training validation errors mixture svms function number training iterations 
turned binary classi cation problem task separate silence frames non silence frames 
total number frames frames 
training set contained randomly chosen frames rst frames 
disjoint validation set contained randomly chosen frames rst frames 
test set contained randomly chosen frames frames 
note validation set select number experts mixture number hidden units 
frame parameterized standard methods speech recognition rasta coe cients rst second temporal derivatives described coe cients fact input window frames yielding input features examples 
table shows comparison single svm mixture svms dataset 
number experts mixture set number hidden units set svms set 
seen mixture svms times faster single svm cpu yielded similar generalization performance 
train test time minutes error cpu cpu svm gated svm mixture table comparison performance single svm mixture svms speech dataset 
new algorithm train mixture svms gave results compared classical svms terms training time generalization performance large scale di cult databases 
algorithm appears scale linearly number examples examples 
results extremely encouraging suggest proposed method allow training svm models large multi data sets reasonable time 
training neural network stochastic gradient takes time grows quadratically conjecture case large data sets reach solution method clearly sub quadratic training time respect number training examples 
address questions guarantee linear training time experts 
better results obtained tuning hyper parameters expert separately 
approach types experts 
acknowledgments rc swiss nsf nancial support project fn 
yb nserc funding agency network support 
cole noel lander durham 
new telephone speech corpora 
proceedings european conference speech communication technology eurospeech 
bengio 
svmtorch support vector machines large scale regression problems 
journal machine learning research 
cortes vapnik 
support vector networks 
machine learning 
robert jacobs michael jordan steven nowlan geo rey hinton 
adaptive mixtures local experts 
neural computation 
kwok 
support vector mixture classi cation regression problems 
proceedings international conference pattern recognition icpr pages brisbane queensland australia 
osuna freund girosi 
training support vector machines application face detection 
ieee conference computer vision pattern recognition pages san juan puerto rico 
pellegrini 
local experts combination trough density decomposition 
international workshop ai statistics uncertainty 
morgan kaufmann 
tresp 
bayesian committee machine 
neural computation 
vapnik 
nature statistical learning theory 
springer second edition 
