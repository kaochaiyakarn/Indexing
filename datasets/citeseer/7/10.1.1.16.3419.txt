learning nonlinear dynamical systems em algorithm zoubin ghahramani sam roweis gatsby computational neuroscience unit university college london london wc ar www gatsby ucl ac uk expectation maximization em algorithm maximum likelihood parameter estimation data sets missing hidden variables 
applied system identi cation linear stochastic state space models state variables hidden observer state parameters model estimated simultaneously 
generalization em algorithm parameter estimation nonlinear dynamical systems 
expectation step extended kalman smoothing estimate state maximization step re estimates parameters uncertain state estimates 
general nonlinear maximization step di cult requires integrating uncertainty states 
gaussian radial basis function rbf approximators model nonlinearities integrals tractable maximization step solved systems linear equations 
stochastic nonlinear dynamical systems examine inference learning discrete time dynamical systems hidden state xt inputs ut outputs yt 
state evolves stationary nonlinear dynamics driven inputs additive noise xt xt ut lowercase characters indices denote vectors 
matrices represented uppercase characters 
zero mean gaussian noise covariance outputs nonlinearly related states inputs yt xt ut zero mean gaussian noise covariance vector valued nonlinearities assumed di erentiable arbitrary 
models kind examined decades various communities 
notably nonlinear state space models form cornerstones modern systems control engineering 
examine models framework probabilistic graphical models derive learning algorithm em 
exception best knowledge rst addressing learning stochastic nonlinear dynamical systems kind described framework em algorithm 
classical approach system identi cation treats parameters hidden variables applies extended kalman filtering algorithm described section nonlinear system state vector augmented parameters 
approach inherently line may important certain applications 
furthermore provides estimate covariance parameters time step 
contrast em algorithm algorithm attempt estimate covariance parameters 
important advantages em algorithm classical approach 
em algorithm provides straightforward principled method handing missing inputs outputs 
second em generalizes readily complex models combinations discrete real valued hidden variables 
example formulate em mixture nonlinear dynamical systems 
third di cult prove analyze stability classical line approach em algorithm attempting maximize likelihood acts lyapunov function stable learning 
sections describe basic components learning algorithm 
expectation step algorithm infer conditional distribution hidden states extended kalman smoothing section 
maximization step general case section describe particular case nonlinearities represented gaussian radial basis function rbf networks section 
extended kalman smoothing system described equations need infer hidden states history observed inputs outputs 
quantity heart inference problem conditional density ut yt captures fact system stochastic inferences uncertain 
gaussian noise assumption restrictive nonlinear systems linear systems nonlinearity generate non gaussian state noise 
authors just aware tresp volume applied em essentially model 
tresp method uses multilayer perceptrons mlp approximate nonlinearities requires sampling hidden states mlp 
gaussian radial basis functions rbfs model nonlinearities analytically sampling see section 
important confuse extended kalman algorithm simultaneously estimate parameters hidden states eks estimate just hidden state part step em 
linear dynamical systems gaussian state evolution observation noises conditional density gaussian recursive algorithm computing mean covariance known kalman smoothing 
kalman smoothing directly analogous forward backward algorithm computing conditional hidden state distribution hidden markov model special case belief propagation algorithm 
nonlinear systems conditional density general non gaussian fact quite complex 
multiple approaches exist inferring hidden state distribution nonlinear systems including sampling methods approximations 
focus classic approach engineering extended kalman smoothing eks 
extended kalman smoothing simply applies kalman smoothing local tion nonlinear system 
point space derivatives vector valued functions de ne matrices respectively 
dynamics linearized xt mean kalman lter state estimate time xt xt ut xt xt xt output equation similarly linearized 
prior distribution hidden state gaussian linearized system conditional distribution hidden state history inputs outputs gaussian 
kalman smoothing linearized system infer conditional distribution see gure left panel 
learning step em algorithm re estimates parameters observed inputs outputs conditional distributions hidden states 
model described parameters de ne nonlinearities noise covariances complications arise step 
may computationally feasible fully re estimate example represented neural network regressors single full step lengthy training procedure backpropagation conjugate gradients optimization method 
alternatively partial steps example consisting gradient steps 
second complication trained uncertain state estimates output eks algorithm 
consider tting inputs xt ut outputs xt 
conditional density estimated eks full covariance gaussian xt xt space 
points mixture full covariance gaussians input output space gaussian clouds data 
integrating type noise non trivial form simple ine cient approach problem draw large sample gaussian clouds uncertain data samples usual way 
similar situation occurs section show choosing gaussian radial basis functions model complications vanish 
forward part kalman smoother kalman lter 
fitting radial basis functions gaussian clouds general formulation rbf network clear special forms consider nonlinear mapping input vectors output vector ix hi ax bu zero mean gaussian noise variable covariance example form represented substitutions xt ut xt xt ut xt 
parameters coe cients rbfs hi matrices multiplying inputs respectively output bias vector rbf assumed gaussian space center ci width matrix si sij exp ci ci goal model data 
complication data set comes form mixture gaussian distributions 
analytically integrate mixture distribution rbf model 
assume data set nj uj observe samples variables paired gaussian cloud data nj 
mean covariance matrix cj 
pi hi ax bu set parameters fh hi bg 
log likelihood single data point model const maximum likelihood rbf mixture gaussian data obtained minimizing integrated quadratic form min xz nj uj uj dx dz rewrite slightly di erent notation angled denote expectation de ning objective written min xu derivatives respect premultiplying setting zero gives linear equations ij whichwe solve hz ij ij ij ij words expectations angled brackets optimal parameters solved set linear equations 
appendix expectations computed analytically 
derivation somewhat laborious intuition simple gaussian rbfs multiply gaussian form new unnormalized gaussians space 
expectations new gaussians easy compute 
tting algorithm illustrated right panel gure 
xt gaussian gaussian evidence evidence linearize models gaussian evidence ut inputs outputs time output dimension input dimension illustrations steps algorithm 
left panel shows information extended kalman smoothing eks infers hidden state distribution step 
right panel illustrates regression technique employed step 
mixture gaussian densities required gaussian rbf networks solved analytically 
dashed line shows regular rbf centres gaussian densities solid line shows analytic rbf covariance information 
dotted lines show support rbf kernels 
results tested algorithm learn dynamics nonlinear system observing inputs outputs 
system consisted single input state output variable time relation state time step tanh nonlinearity 
sample outputs system response white noise shown gure left panel 
initialized nonlinear model linear dynamical model trained em turn initialized variant factor analysis 
model rbfs xt space uniformly spaced range automatically determined density points xt space 
initialization algorithm discovered sigmoid nonlinearity dynamics iterations em gure middle right panels 
experiments need done determine practical method real domains 
inputs outputs time log likelihood lds iterations em left data set training rst half testing rest consists time series inputs ut outputs yt 
middle representative plots log likelihood vs iterations em linear dynamical systems dashed line nonlinear dynamical systems trained described solid line 
note actual likelihood nonlinear dynamical systems generally computed analytically shown approximate likelihood computed eks 
solid curve comes initialization linear dynamics ends nonlinearity starts learned 
right means xt xt gaussian posteriors computed eks dots sigmoid nonlinearity dashed line rbf nonlinearity learned algorithm 
point algorithm observe xt xt pairs inferred inputs outputs current model parameters 
discussion brings classic algorithms statistics systems engineering address learning stochastic nonlinear dynamical systems 
shown pairing extended kalman smoothing algorithm state estimation step radial basis function learning model permits analytic solution step em algorithm capable learning nonlinear dynamical model data 
side ect derived algorithm training radial basis function mixture gaussians 
initial approach potential limitations 
step modify centres widths rbf kernels 
possible compute expectations required change centres widths requires resorting partial step 
low dimensional state spaces lling space pre xed kernels feasible strategy needs exponentially rbfs high dimensions 
second em training slow especially initialized poorly 
understanding di erent hidden variable models related help devise sensible initialization heuristics 
example model nested initialization rst learned simple linear dynamical system initialized variant factor analysis 
third method learns batches data assumes stationary dynamics 
extended handle online learning nonstationary dynamics 
belief network literature dominated methods approximate inference markov chain monte carlo variational approximations 
knowledge rst instance extended kalman smoothing perform approximate inference step em 
eks theoretical guarantees variational methods simplicity gained wide acceptance estimation control literatures method doing inference nonlinear dynamical systems 
currently exploring generalizations method learning nonlinear multilayer belief net works 
zg support ontario gatsby charitable fund 
str supported part nsf center systems engineering nserc canada award 
expectations required fit rbfs expectations need compute equation ij ij ij ij hx ij hz ij ij 
starting easier ones depend rbf kernel ij ij ij observe gaussian rbf kernel equation nj get gaussian density mean covariance ij cij ci extra constant normalization cij ij dx expf ij ij ci expectations ij ij 
ij ij ij ij hx ij ij ij hz ij ij ij ij dx js jci jj expf ci ci ci jc ci tresp 
fisher scoring mixture modes approach approximate inference learning nonlinear state space models 
volume 
mit press 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal statistical society series 
jordan ghahramani jaakkola saul 
variational methods graphical models 
machine learning 
kalman bucy 
new results linear ltering prediction 
journal basic engineering asme 
ljung 
theory practice recursive identi cation 
mit press cambridge ma 
moody darken 
locally tuned processing units 
neural computation 
neal 
probabilistic inference markov chain monte carlo methods 
technical report crg tr 
rauch 
solutions linear smoothing problem 
ieee transactions automatic control 
shumway sto er 
approach time series smoothing forecasting em algorithm 
time series analysis 
