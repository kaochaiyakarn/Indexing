eric mohr yale university mohr cs yale edu lazy task creation increasing granularity parallel programs david kranz kranz lcs mit edu parallel algorithms naturally expressed ne level granularity ner mimd parallel system exploit ciently 
builders parallel systems looked programmer parallelizing compiler increase granularity algorithms 
explore third approach granularity problem analyzing strategies combining parallel tasks dynamically run time 
reject simpler load inlining method tasks combined dynamic load level favor safer robust lazy task creation method tasks created retroactively processing resources available 
strategies grew mul cient parallel implementation scheme languages 
describe mul implementations lazy task creation contrasting machines performance statistics show method ectiveness 
lazy task creation allows cient execution naturally expressed algorithms substantially ner grain possible previous parallel lisp systems 
key words phrases parallel programming languages load balancing program partitioning process migration parallel lisp task management 
numerous proposals implementations applicative languages parallel computers 
robert halstead jr dec cambridge research lab halstead crl dec com way come granularity problem parallel algorithm written naturally resulting program produces tasks ner grain implementation exploit 
researchers look hardware specially designed handle ne grained tasks looked ways increase task grouping number potentially parallel operations single sequential thread 
orts classi ed degree programmer involvement required specify parallelism parallelizing compilers spectrum language constructs giving programmer ne degree control 
attractive world programmer leaves job identifying parallel tasks parallelizing compiler 
achieve performance compiler create tasks su cient size estimating cost various pieces code 
execution paths highly data dependent example recursive symbolic programs cost piece code unknown compile time 
known costs tasks produced may ne grained 
languages allow mutation shared variables quite complex determine parallel execution safe opportunities parallelism may missed 
spectrum language leave granularity decisions programmer providing tools building tasks acceptable granularity propositional parameters 
ne control necessary cases maximize performance costs programmer ort program clarity 
parameters appearing program require experimentation calibrate mayhave repeated di erent target machine data set 
code run parallel code multi user machine parameterization may ine ective amount resources available code unpredictable 
similar problems arise parallelizing compiler parameterized details certain machine 
taken intermediate position research mul parallel version scheme construct multilisp 
programmer takes burden identifying computed safely parallel leaving decision exactly division take place run time system 
mul means annotating programs identify parallelism worrying granularity programmer task expose parallelism system task limit parallelism 
experience functional style common scheme programs program parallelism expressed quite easily adding small number forms may yield large number concurrent tasks run time 
ort involved little required systems parallelizing compilers programmer sure code away parallelism available 
note dynamics parallel programming shared functional languages philosophy goals para functional approach similar 
order support programming style deal questions ciency 
encore multimax implementation mul system orbit compiler proof underlying parallel lisp system cient gure toachieve task granularity 
look dynamic mechanisms run time system advantage avoiding parameterization problems mentioned earlier 
key dynamic strategies controlling granularity fact construct correct operational interpretations 
canonical expression declares child computation may proceed parallel parent continuation straightforward interpretation child task created compute parent task computes reversing task roles possible parent task compute child task computes importantly ne grained programs usually correct parent task compute rst ignoring 
inlining parent task eliminates overhead creating scheduling separate task creating placeholder hold value 
multimax trademark encore computer 
returns object called placeholder eventual value placeholder said unresolved value available 
task attempting value unresolved suspended value available 
touch value cause task suspended unresolved 
inlining correct lead deadlock described section 
inlining mean program run time granularity size tasks executed run time signi cantly greater source granularity size code constructs source program 
program execute ciently average run time granularity large compared overhead task creation providing course parallelism preserved achieve load balancing 
rst dynamic strategy consider load inlining 
strategy means system loaded separate task evaluate inline evaluating current task 
load threshold indicates tasks queued system considered loaded 
call encountered simple check task queue length determines separate task created 
simple load inlining strategy works programs drawbacks see section led consider strategy inline task provisionally save information tasks selectively un inlined processing resources available 
words create tasks lazily 
lazy task creation strategy means start evaluating current task save information continuation moved separate task processor idle 
say idle processors steal tasks busy processors task stealing primary means spreading system 
execution tree ne grained program overabundance potential fork points 
goal lazy task creation convert small subset actual forks maximizing run time task granularity preserving parallelism achieving load balancing 
subsequent discussion contrasted eager task creation fork points result separate task 
example help ideas concrete 
example simple example spectrum possible solutions granularity problem consider algorithm written scheme program sum leaves binary tree define sum tree tree leaf 
tree leaf value tree sum tree left tree sum tree right tree direct execution psum tree 
leaf leaf value left right de ne tree datatype 
natural way express parallelism algorithm indicate recursive calls sum tree proceed parallel 
mul indicate adding define psum tree tree leaf 
tree leaf value tree psum tree left tree psum tree right tree natural expression parallelism algorithm ne grained 
eager task creation program create tasks sum tree depth average number tree nodes handled task 
shows execution pictorially circled subset tree nodes handled single task 
task creation cheap task breakdown lead poor performance 
ideal task breakdown maximizes run time task granularity maintaining balanced load 
divide conquer program means expanding tree breadth rst spawning tasks processors busy expanding tree depth rst task processor 
refer ideal task breakdown breadth rst saturation 
shows execution pictorially system processors 
achieve ideal task breakdown 
parallelizing compiler able increase granularity unrolling recursion eliminating futures example want ne grained tasks spread quickly possible breadth rst 
compiler possibly produce code supplied information strategy adding relies evaluating operands left right argument evaluation went right left psum tree right tree evaluate completion psum tree left tree began parallelism realized 
execution psum tree processors 
define psum tree tree cutoff depth leaf 
tree leaf value tree cutoff depth psum tree left tree cutoff depth psum tree right tree cutoff depth code psum tree available processing resources making transformation general di cult task parameterization drawbacks noted earlier 
control task creation explicitly 
parallel constructs programmer may supply predicate evaluated run time determine separate task created predicate tests length queue achieving ect load inlining 
style mechanism create hypothetical construct write psum tree similar example 
example cutoff depth speci es depth tasks created 
predicate cutoff depth tells inline recursive call 
cutoff depth value achieve execution shown level futures inlined 
solution problems 
code complex addition cutoff depth longer completely straightforward tell program doing 
second program parameterized cutoff depth argument associated calibration issues noted previously 
load inlining lazy task creation attempts approximate perfor mance psum tree sacri cing clarity psum tree 
ideal run psum tree processor system load inlining rst occurrences nodes nd processors free separate tasks created breadth rst 
depending value threshold parameter tasks may created backlog high cause inlining 
large surplus tasks able cost creation inlining substantial subtree depth rst 
ideal run psum tree lazy task creation representing subtree rooted provisionally inlined continuation representing subtree rooted immediately stolen idle processor 
likewise futures inlined continuations stolen remaining idle processors 
processors busy subsequent futures provisionally inlined stealing takes place processor winds executing circled subtrees 
execution pattern depends oldest rst stealing policy idle processor steals task oldest available fork point 
example oldest fork point represents largest available subtree task maximal run time granularity 
consider idealized execution patterns match real life execution patterns methods 
comparison dynamic methods load inlining appealing simplicity fact produce results programs noted factors decrease ectiveness 
major factor inlining decisions irrevocable decision inline task way revoke decision time clear time doing bene cial 
list summarizes drawbacks inlining sections discuss turn basis comparing dynamic strategies 

programmer decide apply inlining load threshold 
inlined tasks accessible processors starve inlined tasks pending 

deadlock result inlining types programs 

implementation task queue processor load inlining creates tasks created optimal division 

load inlining ine ective programs ne grained parallelism expressed iteration 
programmer involvement load inlining automatic mechanism requires programmer input 
programs run signi cantly faster eager task creation load inlining programmer identify load inlining applied 
example load balancing crucial coarse grained program creating relatively tasks inlining large tasks hurt load balancing lengthening tail period processors nishing tasks 
lazy task creation load balancing su er inlining decisions 
worst lazily inlined tasks continuations stolen 
cost stealing task comparable creating eager task performance signi cantly worse eager task creation 
lazy task creation safely programs danger degrading performance 
load inlining programmer get involved supplying value load threshold experience shown choosing right value crucial performance di cult experimentation 
lazy task creation requires parameterization programmer freed burden 
load inlining mean processors idle continuations inlined tasks begun execute 
problems caused bursty task creation parent child welding 
bursty task creation refers fact opportunities create tasks may distributed unevenly program 
moment task inlined may appear plenty tasks available execute time tasks nish executing may opportunities create tasks 
consequently processors may go idle continuations inlined tasks available execution 
problem arises lazy task creation continuations available stealing 
parent child welding refers fact inlining effectively parent child task 
inlined child blocked waiting resolve event parent blocked available execution 
lazy task creation information kept inlined child allows child decoupled blocked allowing parent 
deadlock serious problem load inlining programs irrevocable inlining correct optimization 
see inlining lead deadlock consider program nding primes 
uses standard prime nding algorithm checking odd integer primality looking divisors primes far futures introduce parallelism getting di culty accessing ends single list adding primes reading primes front divisor testing 
initially primes bound lazily generated list odd primes 
function find primes generates tail primes rst making nd odd primes checking primality primes primes generated 
shows di erent possible scenarios execution find primes 
shows eager task creation separate created test value primality 
futures scheduled order testing large value block walking list known primes futures testing small values unresolved 
shows possible scenario eager execution futures executed futures unresolved 
blocking shown example able run completion rst elements list primes needed determine prime 
shows possible execution snapshot load inlining 
inlining find primes causes parent task test additional value successive futures inlined task test values illustrates important manifestation load inlining find primes inlined task containing recursive call delay creates object spawn task avoid race condition letrec binding 
eager task creation created value eager task creation execution order arbitrary subject data dependencies 
load inlining single may handle values deadlock occur load inlining 
possible execution scenarios find primes 
find primes executed continuation task testing values tests largest value rst 
interaction cause deadlock happen scenario 
inlining futures created task test order 
expect ultimate value list containing elements representing rest list 
representing value 
call prime 
attempt access second element primes block second element inside unresolved second element won available gets testing deadlock occurred 
type deadlock possible lazy task creation decoupling blocked tasks mentioned 
inlined task separated parent programs deadlock free eager task creation deadlock free lazy task creation 
tasks behavior load inlining programs psum tree analyzed 
assumes processor maintains local task queue inlining decisions local queue length 
shows ways need maintain task local queue leads non execution 
lone processor executing subtree height creates tasks just second removing task define find primes limit letrec primes cons delay find primes find primes lambda limit rest find primes prime 
primes cons rest rest cons primes define prime 
primes prime car primes cond prime prime zero 
mod prime prime 
cdr primes program find primes deadlock load inlining 
queue moment transfer lead creation tasks 
derives upper bound tasks processors points bound guarantees asymptotically minimal task creation overhead problem size grows exponentially doesn tell story asymptotically acceptable overhead may achieved problem size grows running time measured days 
experience overhead task creation load inlining signi cant problems substantial size 
bottom line load inlining distributed task queues unable achieve oldest rst scheduling tasks created represent small subtrees 
example consider happens transfer removes task queue processor time encounters call nd queue empty create new task evaluate call 
position program call tree really matter chance determined timing transfer operation 
majority potential fork points lie leaves tree represent small subtree 
possible central queue distributed queues decrease number tasks contention introduced alternative probably unacceptable certainly scalable 
better alternative oldest rst scheduling policy lazy task creation seen task counts section lazy task creation results fewer tasks load inlining 
tasks created oldest rst scheduling able inline larger subtrees giving better approximation execution 
fine grained iteration parallel programs bushy call trees example programs contain data level parallelism expressed iteration linear data structure 
load inlining ective increasing run time granularity programs 
see consider versions parallel map shown 
versions apply function element list methods parallelizing iterative loop 
load inlining lazy task creation ciency increased ne grained programs task able inline tasks increasing average run time task granularity reducing overhead due task creation 
versions parallel map tasks unable inline subtasks leading high task creation overhead ne grained 
parmap cars parent task loops list calling application list element 
program inlining futures increase granularity parent task tasks created ne grained inlining opportunities 
best task large granularity small granularity leading poor performance 
parmap appears call map rest list parent task applies current list element 
conceivable case inlining create tasks large granularity parent inline futures making real inline example involves recursion lists similar dynamics appear iteration arrays 
define parmap cars null 
elt car rest parmap cars cdr cons elt rest define parmap null 
rest parmap cdr elt car cons elt rest futures making real practice system load low initially small tasks created 
numerous processors tasks executed faster created backlog necessary load inlining builds 
unfortunately lazy task creation su ers problems load inlining type program 
eager stealing policy necessary timely scheduling tasks leads case small tasks poor performance 
approaches possible considered complex dynamic methods making compiler granularity information cases 
may harsh fault dynamic task combination strategies failing improve execution programs ne grained data level parallelism expressed iteration 
sequentiality iteration inherently limits parallelism task creation overhead nonexistent parallelism loop parmap cars exceed tf tl cost computing vs cost loop iteration 
ne grained loop ratio represents real limitation number processors kept busy 
case expressing algorithm convenient way inherently limits parallel performance 
mentioned view programmer identify computed parallel worrying division take place run time 
iteration says lot increase performance programmer doesn provide algorithm inherent parallelism 
line reasoning lists bad data structure program data level parallelism structure requires sequential iterative style access 
knew advance optimal number list elements handled single task elements need code versions parallel map traversed create tasks 
random access nature arrays better choice division nature 
free programmer specifying explicitly array elements chunked tasks 
way solving apply problem philosophical framework conquer division array index set parmap interval shown 
array visible function index argument array calculation 
method exposes abundant parallelism requiring programmer specify exact partition array elements tasks 
lazy task creation interacts program bushy call tree approximating partition runtime 
perfect solution parmap interval somewhat complex corresponding iterative loop 
increase complexity small program free parameterization 
discuss ideas improving solution 
implementation seen lazy task creation strong advantages load inlining 
explore implementation issues determine overhead lazy task creation acceptably minimized 
dynamic methods increase ciency ignoring selected instances 
lazy task creation requires maintaining information provisionally inlined allow processor steal continuation cleanly 
cost maintaining information critical factor determining nest source granularity handled ciently 
cost incurred new define parmap interval lo hi lo hi lo mid lo quotient lo hi mid hi mid lo lo half parmap interval lo mid lo parmap interval mid hi hi touch lo half task created large overhead overwhelm ne grained program 
comparison cost stealing task somewhat critical inlining occurs cost stealing task small compared total task ultimately performs 
cost stealing continuation kept cost creating eager 
stealing continuation requires splitting existing stack conventional stack implementation requires copying frames stack 
alternatively linked frame implementation splitting stack requires pointer manipulations 
care taken implementation ensure normal operations pushing popping stack frame comparable cost conventional stack operations 
pursued avenues implementation conventional stack implementation encore multimax version mul implementation alewife multiprocessor 
basic data structures operations lazy task creation common implementations discussed 
lazy task queue task maintains queue continuations called lazy task queue shown abstractly 
making lazy call corresponding instance source code task rst pushes pointer continuation lazy task queue 
return continuation stolen processor dequeues 
refer producer lazy tasks processor stealing called consumer 
consumers remove frames head lazy task queue producer pushes pops frames tail 
tells lazy task creation story producer task shows stack growing upward frames 
frames continuations code parmap interval lazy calls pointers frames placed lazy task queue 
note oldest continuation head bottom queue newest continuation tail top queue 
point lazy call occurs corresponding code shown new continuation pushed stack pointer pushed lazy task queue 
inlined completes execution stealing occurs simply returns rst popping lazy task queue removing pointer tail queue shown 
idle consumer decides steal continuation lazy task queue 
correctly change stack look eager created compute creating placeholder modifying stack value returned call resolve supply value placeholder passed directly continuation consumer calls passing unresolved placeholder argument 
shows completed steal operation looks eager created processor producer evaluating child consumer evaluating parent note important feature stealing operation consumer interrupts producer 
implementations take care guard kinds race conditions ensure correctness stealing operation 
consumers may race steal continuation second producer trying return continuation may race consumer trying steal 
encore implementation implemented lazy task creation version mul running encore multimax bus shared memory multiprocessor 
multimax stack lazy task queue tail head data structures lazy task creation 
stack lazy task queue tail head returning lazy call causes continuation dequeued 
stack lazy task queue tail head lazy call causes continuation queued 
producer stack lazy task queue placeholder continuation stolen 
tail head lazy task queue data structures operations 
consumer stack base ltq tail ltq head lazy cont lazy cont lazy cont frame frame frame lazy task queue implemented conjunction conventional stack 
processors national semiconductor processors relatively general purpose registers fairly powerful memory addressing modes 
synchronization processors possible test set instruction acquires exclusive access bus 
implementation stacks represented conventionally contiguous sections heap 
seen lazy task queue kept contiguous memory top part stack 
producer pushes lazy continuations queue grows downward stack frames grow upward 
stealing continuations ectively shrinks stack removing information ends head lazy task queue bottom frames stack 
stack ows gap stack frames lazy task queue gets small may reclaim space created steal operations contents may copied new stack twice original size 
steal stack pictured consumer rst locates oldest continuation ltq head pointer lazy cont pointer frame 
consumer replaces frame stack continuation directing producer resolve placeholder 
consumer copies frames frame bottom live area stack indicated base new stack updating base ltq head appropriately 
guard race conditions mentioned earlier lock entire stack plus lock continuation lazy task queue 
producer modi es ltq tail consumers modify ltq head base 
lazy call return wenow lazy task queue operations somewhat detail 
gives assembler pseudocode showing expression compiled encore mul lazy task creation 
lazy call return example show crucial lazy task queue operations enqueuing dequeuing lazy continuation 
rst block entry call shows compiled code lazy call continuation containing standard call stack pointer current stack lazy task queue pointers tail referenced set pointer 
code shows bytes allocated lazy task queue area stack lazy continuation continuation lock 
storing continuation pointer call initializing lock increment ltq tail pointer lazy continuation available stealing 
need test explicitly ow lazy task queue stack ow check simply tests size empty region actual stack growing upwards lazy task queue growing downwards 
calling push return lf call stack return address 
shared outof line routine serves continuation lazy calls 
shown second block code 
see synchronization guard interference consumer trying steal lazy continuation producer trying return 
returning producer rst acquires lazy task queue item lock encore interlocked test set instruction busy waiting lock currently held slight simpli cation actuality current stored block data kept locally processor ltq tail referenced double indirection capability ns 
lambda entry standard stack ow test instructions 
push addr call push return address current continuation stack move ltq tail stack get pointer tail lazy task queue move sp store pointer stack continuation lazy task queue move initialize lazy task queue entry lock add ltq tail stack lazy continuation cially enqueued push addr return lf call call return return lf call standard call unknown procedure instructions 
call standard continuation code including call unknown procedure instructions 
return lf call move ltq tail stack get pointer lazy task queue tail test set try lock tail item lazy task queue br clr pop ltq successful go pop busy wait loop lock tail item lazy task queue 
pop ltq sub ltq tail stack lazy continuation cially dequeued adjust sp remove return lf call address stack standard return instructions 
assembler pseudo code showing lazy call return encore implementation 
consumer 
lock acquired return address top stack guaranteed valid case original value call resolve placeholder continuation stolen 
dequeuing tail entry lazy task queue return normally 
usually case continuation lazy call known appears tail call position code shown streamlined generating return lf call code line 
optimization saves instructions increases code size slightly implemented current system 
steal operation gives algorithm stealing lazy continuation processor lazy task queue 
task stolen chosen round robin search processors lazy task queues 
locks acquired continuation stolen producer stack avoid races consumers continuation locked avoid race producer trying return 
continuation chosen necessary locks obtained replace producer stack continuation resolve newly created placeholder update producer base ltq head pointers 
point producer stack consistent state unlock head item lazy task queue 
bottom producer stack copied consumer stack care old continuation newly swapped 
consumer executing stolen continuation passing placeholder argument 
producer processor stealing occurs 
eventually return swapped continuation providing value placeholder 
blocking remaining loose discussion happens lazy task queue task blocks touching unresolved 
lazy task queue part state queued lazy tasks inaccessible 
potential deadlock problem arises load inlining 
simple solution adopted bite tail 
stack split lazy con producer stack unlocked point possibility stack ow operation discussed earlier con ict copying operation 
allocate initialize data structures placeholder new task object new stack 
look continuation steal 
poll processors nd current stack non empty lazy task queue ltq tail ltq head 
try lock stack locked skip processor 
try lock head item lazy task queue locked skip processor 
steal continuation 
head item locked pointer cp stack 
cp points stack frame representing continuation 
bottom stack portion cp base pointer copied new stack 
replace continuation resolve placeholder 
update base ltq head pointers 
consistent state unlock head item copy bottom portion 
unlock stack 
return top continuation new stack passing placeholder argument 
algorithm steal operation encore implementation 
tail lazy task queue top piece blocked steal operation placeholder created communicate value pieces split stack 
executing processor continue piece stack contains continuations lazy task queue 
lazy tasks inaccessible 
dequeues tail lazy continuation returns passing placeholder argument 
essence stolen task tail lazy task queue 
problem solution goes preference oldest rst scheduling ectively created task newest potential fork point 
performance su er task small granularity 
blocking may result possibly leading entire lazy task queue 
improved solution avoids problems implemented alewife discussed section 
alewife implementation encore implementation lazy task creation just described performs reasonably lowering overhead construct sources overhead 
strict operations doing 
checks operands 

checking stack ow 

global resource bus locking operations 
alewife machine cache coherent machine developed mit distributed globally shared memory designed address problems 
processing elements modi ed sparc chips 
modi cations interest fast traps strict operations futures support full empty bits memory word 
strict arithmetic operation memory operates trap occurs explicit checks needed 
full empty bits allow ne grained locking alewife includes memory referencing instructions trap full empty state referenced location expected 
alewife implementation lazy task creation stack represented doubly linked list stack frames order minimize copying stealing operation 
scheme frame link previously allocated frame link frame allocated 
push frame pop frame operations simply load instructions 
important feature scheme stack frames deallocated popped 
subsequent push re frame meaning average case cost stack operations associated procedure call return close cost operations conventional stacks 
frame link set empty frame allocated 
strategy avoids need check explicitly stack ow doing push frame operation frame available push frame operation trap trap handler allocate new frame 
earlier version described initial alewife implementation 
version stealing lazy task involved copying topmost stack frame 
version described avoids copying xes subtle bug original version 
frame divided separate data structures referred stack frame frame stub 
stack frames form doubly linked list described sparc trademark sun microsystems section 
stack frame contains local temporary variables ordinary stack frame 
addition contains pointer associated frame stub 
frame stub pointer back associated stack frame 
reason separating structures clear 
implementation lazy task queue threaded frame stubs 
figures show lazy call stealing operations graphically 
gures register names fp frame pointer register 
points current stack frame frame stub 
lazy task queue tail register 
modi ed producer 
points current frame stub 
head lazy task queue 
memory consumers processors steal frames head queue 
full empty bit serves lock limiting access potential consumer time 
stack frame slots slot points frame current stack frame push operation performed 
push frame operation performed simply loading fp fp 
ifthe frame allocated fp marked empty 
cont slot points continuation frame current stack frame pop operation performed 
pop frame operation load cont fp fp 
data number slots local variable bindings temporary results 
lf frame slot points associated frame stub 
frame stub slots ltq slot points frame stub lazy task queue tail queue 
location full empty bit lock arbitrating consumer stealing continuation producer trying invoke continuation 
ltq prev slot points previous frame stub lazy task queue head queue 
ltq link lazy call code stores slot return address consumer steals frame continuation 
continuation stolen consumer reads return address replaces placeholder object creates 
ltq ltq prev ltq link ltq frame lf frame data cont fp just lazy call 
frame slot points associated stack frame 
implementation call lazy call ordinary procedure call preceded push frame operation followed pop frame operation 
contrasts common approach pushing frame procedure entry deallocating procedure exit comments subject appear section 
shows stack frames relevant registers look just lazy call call push frame operation occurred 
note stack frame pointer points frame top stack cont pointer points stack frame bottom stack 
memory location contents left blank gure contents unimportant indeterminate example slot leftmost frame empty point currently unused frame 
left hand part frame slot see example ltq slots indicates full empty bit corresponding memory word set empty 
lazy task queue frames 
consumer discover seeing ltq slot frame stub pointed empty task frames slot point rst frame 
shows situation just lazy call 
frame stub associated current stack frame pointed fp joined lazy task queue 
accordingly changed point frame stub ltq ltq prev links updated needed maintain doubly linked lazy task queue 
note rightmost frame stub logically part lazy task queue serving convenient header object doubly ltq ltq prev ltq link ltq frame lf frame data cont ltq ltq prev ltq link ltq frame lf frame data cont fp just lazy call 
fpc fp placeholder steal new structures shaded 
linked queue 
middle frame part lazy task queue simply part stack 
current frame stub ltq link eld contains address lazy call continuation required 
consumer steals continuation lazy call eventually return 
code return restore state airs depicted pop frame operation associated lazy call performed 
shows state producer consumer tasks consumer steals continuation task shown 
consumer task state variables shown appended 
shaded areas shaded arrows show structures created consumer 
ltq ltq prev ltq link ltq frame lf frame data cont fpc fp placeholder steal frames belonging producer shown black 
alternate view situation shown 
note consumer stack part looks just producer stack just original lazy call just producer case normal return lazy call 
ectively consumer taken continuation created placeholder stand value called computation performed producer forced early return lazy call supplying placeholder call returned value 
arrow consumer data structures placeholder value returned consumer registers 
consumer producer ltq link eld point newly created placeholder 
producer completes computation nds continuation stolen looks nd placeholder resolve computation value 
synchronization unusual ltq link marked empty contains useful data 
technique handles close races returning producer stealing consumer 
inspecting ltq ltq prev pointers returning producer discover continuation stolen consumer stored placeholder ltq link eld 
correct operation ensured having consumer set ltq link eld empty ag placeholder installed having producer wait empty ag attempting read placeholder 
producer returning lazy call distinguishes situations shown figures locating frame stub pointed ltq prev eld frame stub pointed looking ltq eld continuation stolen eld empty continuation stolen 
algorithm lazy calls spelled detail pseudo code shown 
line code lazy call starts label lf call code stolen line code shared lazy calls 
algorithm consumer nd steal continuation 
producer explicitly noti ed steal operation performed stack resources producer may continue continuation stolen may consumer 
complexity algorithm stealing results fact 
particular consumer copy rightmost frame stub ltq slot frame stub performs lazy calls 
frame stub shared producer calls consumer confuse producer 
approach drawback push frame operation occurs procedure call lazy entry point procedure mitigating factors 
push frame pop frame operations inexpensive instruction 

compiler optimizations eliminate pop push sequences cancel detected eliminated 

push frame operations procedure entry turn unnecessary due conditional branches approach delays sure necessary 
know net ect approach believe di erence signi cant 
return issue lazy task queue task blocks unresolved 
preserve oldest rst scheduling laziness task creation lazy task queue accessible normal stealing consumers 
accomplished placing entire blocked task lazy task queue task queue appropriate processor 
consumers may steal task running queued blocked task processor may steal lazy task queue course task marked blocked processor attempt run 

select processor inspection load pointer register leaving empty 
empty move processor 
enforces mutual exclusion consumers 

load ltq register leaving ltq empty 
empty processor lazy task queue empty write back move processor 

store 
step commits steal operation ends exclusion consumers 
consumers steal continuations processor consumer continues steal operation 

load ltq link register 
consumer return address lazy call 

create placeholder object save address register retval 

store retval ltq link leaving ltq link empty 
producer trying return continuation stolen step frees producer proceed deposit result placeholder 

create stack frame frame stubs shown shaded link shown shaded arrows 
data structures accessed consumer producer consumers need synchronize operations 

set fpc pointers properly execute return address giving retval returned value 
algorithm steal operation alewife implementation 
blocked tasks runs useful 
solution addresses problems raised section 
discussion advantages disadvantages implementations 
main disadvantage conventional stack implementation copying performs 
appear amount copying required stealing operation potentially unlimited cost stealing lazy task unlimited 
lf call load fp fp push stack frame 
load lf frame fp temp address new frame stub 
store continue ltq link temp pc consumer return 
store ltq prev temp lazy task queue backward store temp ltq forward links 
move temp advance lazy task queue tail pointer 
call procedure 
load ltq prev temp dequeue lazy task queue tail empty ltq temp trap stolen continuation stolen 
move temp reset lazy task queue tail pointer 
continue load cont fp fp pop stack frame 
stolen wait ltq link empty 
load ltq link temp get placeholder resolve 
resolve placeholder temp value returned procedure 
terminate current task nd new 
assembler pseudo code showing lazy call return alewife implementation 
technically true somewhat misleading overhead copying stealing continuation viewed cost creating continuation rst place 
program ne source granularity little lazy calls able push items stack require signi cant copying 
program creates large continuations requiring lots copying fair amount ofwork push information stack cost copying signi cant comparison 
exception argument program builds lot stack enters loop generates futures define example build stack call loop define loop 
loop stealing rst lazy task continuation requires copying built stack 
argued cost signi cant compared cost building stack rst place 
example stolen continuation immediately creates lazy task steal copy information 
fact spreading processors example lazy tasks requires built stack information copied times 
easy solutions problem 
loop rewritten resemble parmap parmap cars see section resulting program built stack copied 
inserted call loop resulting program built stack copied 
appears ects copying conventional stack implementation minimized 
attractive eliminate copying altogether linked frame implementation described alewife 
implementation certainly cient lazy task operations 
somewhat di cult gauge exactly overhead introduced sequential sections code 
rami cation re stack frames frames xed size choosing correct frame size involves trade small frame size chosen frames needing space need create ow vector increasing costs accessing frame elements memory allocation 
large frame size chosen frames contain lot unused slots 
lead frequent garbage collection valuable space cache virtual memory factors minimal today memory rich systems 
current alewife implementation uses frame size slots 
accumulate experience promising implementation technique making nal evaluation 
performance section performance gures mul implementations 
experiments conventional stack version yale encore multimax ns processors megabytes memory 
figures linked frame version obtained detailed simulator alewife machine 
mul run time system code benchmarks compiled sparc instructions interpreted simulator 
overheads due creation blocking scheduling accurately re ected statistics 
memory referencing delays simulated experiments 
comparison sequential assessing performance multiprocessor system important comparisons best sequential implementation 
assessment done steps 
parallel version running processor compare sequential version 

performance improve processors added 
rst step compare mul best possible sequential implementation close purposes 
second step considered section 
encore implementation sequential comparison shows overhead due compiler inserted 
checks strict operations 
encore implementation engineered minimize overhead cost signi cant programs 
table compares running times sequential programs program run mul processor 
mul programs run times long counterparts 
alewife hardware traps eliminate overhead sequential parallel times processor identical 
speedup gures measure close get ideal linear speedup compared sequential version 
minimizing memory referencing delays crucial performance distributed memory machine 
alewife distributed caching scheme reduces need remote preliminary results current research mit new scheduler alewife mul show performance simulation network delays 
programs described section 
interesting note presence hardware tag checking may signi cant machines supporting parallel lisp machines supporting sequential lisp 
time seconds program mul ratio fib queens mergesort speech compiler permute table comparison running times encore mul 
cost lazy task queue operations mentioned earlier crucial minimize overhead lazy calls 
statistics implementations additional cost lazy call conventional call pushing continuation lazy queue popping encore instructions sec alewife instructions encore instructions eliminated compiler optimization mentioned section saving roughly sec 
alewife sequence probably cheaper risc instructions sparc simpler ns instructions 
important factor encore time synchronization done expensive mechanism test set instruction acquires exclusive access bus 
alewife full empty bits provide synchronization 
cost stealing continuation processor task queue critical steals relatively rare 
seen stealing task encore implementation comparable cost creating eager 
stealing task alewife implementation noticeably cheaper linked frame stack implementation allows cleaner steal 
machine operation number instructions encore eager encore steal copied alewife steal instruction counts include aspects creating executing task allocating initializing placeholder task stack objects queueing dequeuing task resolving placeholder 
benchmarks discussion actual mul programs synthetic benchmark grain designed measure ectiveness various task creation strategies range task granularities 
grain adds perfect binary tree parallel divide conquer structure similar psum tree returning leaf executes delay loop speci ed length allowing granularity control 
timing trials range granularities get ciency pro le task creation strategy 
ciency trial calculated formula ts ntp case sequential time ts mul program futures parallel time tp measured processors 
ciency means perfect speedup 
tree depth trials ensures processor idle time start tail minimal close perfect speedup achievable 
granularity gures top table tell ns instructions leaves execute delay loop return include instructions implement basic divide conquer loop 
average source granularity half gure internal nodes tree delay loop executed account half futures program 
instruction counts di erent alewife due risc instruction set source code ciency gures roughly comparable 
expected high cost eager task creation leads poor ciency ne granularities 
inlining performs somewhat better expect 
main problem processors far tasks created roughly possible total 
see impact mechanism discussed section 
lazy task creation calls converted actual tasks resulting better performance 
overhead lazy calls signi cant hurting ciency nest granularities 
lower overhead lazy calls alewife leads somewhat greater ciency 
table shows performance statistics mul programs allowing comparison speedup task creation strategy 
column labelled seq time gives elapsed time running benchmark mul processor futures removed 
times seconds encore simulated sparc cycles alewife 
remaining columns show relative speedup parallel benchmark run processors 
load inlining lbi load threshold chosen case give fastest time processors 
table shows number tasks created strategy 
header benchmark shows maximum number tasks possible benchmark equals number produced eager task creation 
fib standard brute force doubly recursive program computing nth fibonacci number case 
program ne grained extremely little computation performed calls 
eager task creation overhead creating futures completely overwhelms calculation processors sequential time improved 
load inlining eliminates overhead creates tasks ideal execution create 
lazy task creation produces better approximation shown smaller number tasks 
ne granularity overhead lazy calls signi cant seen processor speedup column 
queens nds possible solutions known queens problem 
queen placed row board time time queen legally placed appears recursive call nd possible solutions stemming current con guration 
source granularity particularly ne lazy task creation noticeable improvement 
processor speedup column table gives indication overhead task creation strategy 
eager number shows cost creating eager futures signi cant program 
number load inlining better overhead load inlining small task created 
lazy task creation higher cost provisionally inlining task leads slightly smaller number 
lazy task creation substantially reduces number tasks created allowing performance increase processors 
speech real program part multi stage speech understanding system developed mit 
stage essentially graph matching problem nding closest dictionary entry spoken utterance 
uses method resembling parmap interval divide conquer division dictionary leaf compares utterance dictionary entry 
granularity comparison coarse eager task creation doesn perform badly improvement lazy task creation modest 
grain ciency leaf task granularity number ns instructions machine strategy encore eager encore lbi encore lazy alewife lazy table ciency implementations synthetic grain benchmark 
fib seq relative speedup machine strategy time seq encore eager encore lbi encore lazy alewife lazy queens seq relative speedup machine strategy time seq encore eager encore lbi encore lazy alewife lazy speech seq relative speedup machine strategy time seq encore eager encore lbi encore lazy alewife lazy table performance mul benchmarks absolute times seconds encore sparc cycles alewife 
fib queens speech eager eager eager encore alewife encore alewife encore alewife lbi ltc ltc lbi ltc ltc lbi ltc ltc table number tasks created mul benchmarks benchmarks table shows ects mechanism discussed section lazy task creation gives consistently better approximation execution load inlining 
task counts lazy task creation variable run run due variable timing steal operations believe di erences encore alewife task counts re ect signi cant di erences implementations 
related load inlining studied previously mul parallel lisp system available deque size sense current load 
analytical model load inlining programs psum tree developed 
analytical results generally agree empirical observations load inlining mul prior mul prior explored alternative lazy task creation 
strategy reduces task creation overhead queued task executed processor created 
strategy takes advantage phenomenon lazy task creation leverages parallelism abundant tasks executed locally 
executing tasks lazy task creation appears cheaper scheme furthermore scheme works programs fork join style parallelism 
lazy task creation restriction interacting unlimited lifetime futures mul 
potential deadlock load inlining described example section plausible scenario painted 
interesting note selective load inlining possible sophisticated programmer ensure inlining performed cause deadlock 
solution requires programmer accurately recognize situations potential deadlock exists er advantages lazy task creation 
perform lazy task creation intended fork join cobegin style programming 
implemented top modula extension modula 
task created lazily program calls proc data proceeds 
free processor looks unanswered help requests steals applies proc data 
requester nishes calls see task stolen 
proceeds looks 
performance evaluated parallel quicksort programs program searches occurrences string group les 
principal di erence style lazy task creation mul lazy futures invoking lazy task creation requires signi cantly larger amount source code written performed proc broken separate procedure argument block passed data explicitly allocated lled nally procedures called 
synchronization value retrieval lazily created task explicit responsibilities programmer 
contrast mul necessary insert keyword enjoying bene ts lazy task creation 
stylistic di erences lead implementation di erences lazy implementations directly manipulate implementation objects stack frames built implementation case 
think ciency improvements result approach systems di erent hard conclusive comparison 
case mechanics systems di erent close relationship underlying philosophies 
philosophy encouraging programmers expose parallelism relying implementation curb excess parallelism resembles data ow researchers concerned throttling 
main purpose throttling reduce memory requirements parallel computations increase granularity generally xed ne level data ow architectures 
throttling serves purpose preference depth rst scheduling directly related lazy task creation 
encouraged performance statistics support theoretical bene ts lazy task creation 
programs bushy call trees programmer identify parallelism ectively ignoring granularity considerations 
group programs deserve study ne grained parallelism expressed iteratively see section 
suggested solution problem rewriting programs bushy call trees higher level solution involving programmer preferable 
envision developing data abstraction aggregates set high level operations cient underlying parallel implementations ideas connection machine lisp model 
high level language manipulating aggregates need exhibit properties powerful express common patterns parallel computation aggregates 
translatable cient parallel code 
really parts ciency question di erent sources overhead worry 
certainly minimize parallel processing overheads due task creation idle processors communication weighing con icting needs partitioning load balancing locality 
lazy task creation addresses issues de nitely needed 
worrying overheads rst deal considerable overhead introduced high level constructs functional expressions data aggregates rst place 
abstraction lose ciency won able regain cost parallelism 
second ciency issue waters relevant 
observes programming high level functional expressions data aggregates powerful approaches typically cient writing code hand 
solution essentially provides high level language manipulating aggregates powerful express commonly looping idioms 
translatable cient sequential code loops 
de nitely commonality problem addressed approach 
provided solid framework captures common patterns sequential computation aggregates hides low level details 
attempting capture common patterns parallel computation aggregates hide low level details 
noting promising similarity ideas extending waters approach handle parallelism preliminary stages 
important issue scalability 
encore machine alewife simulation described assume memory equal cost unreasonable assumption large scale multiprocessor 
investigating lazy task creation strategy augmented take advantage locality shared memory systems physical memory distributed 
extra record keeping burden lazy calls cheap cheapest implementation normal calls incremental cost lazy call strongly uenced hardware architecture 
example linked frame implementation shown section bene ts greatly alewife architecture support full empty bits memory accessed ciently side ect load store instruction 
linked frame implementation requires memory operations call memory operations lazy call 
architectures processors register windows contemplated approach potential eliminating memory operations register window associated bit processor register indicating logically part lazy queue register window unloaded due window ow trap frame linked memory data structure representing queue 
reduce cost lazy calls expect large fraction lazy calls return associated register window having unloaded 
mechanism provided querying processor see contains continuations event memory interrupting processor request unload continuations needed processors 
costs bene ts idea currently known 
larger quest wehave engaged provide expressive power elegance lowest possible cost 
complete success endeavor unnecessary programmers shun favor lower level cient constructs 
success encourage programmers express parallelism programs levels granularity forcing granularity source code level best performance 
lazy task creation moves closer ideal producing acceptable performance greatly reducing number tasks created benchmark programs section 
ideal may achieved completely step direction making cheaper increases number situations cost bar 
acknowledgments special dan nal alewife version 
randy osborne richard kelsey paul hudak helpful comments drafts kirk johnson speech application sloan foundation ibm department energy fg er darpa support 
agarwal lim kranz kubiatowicz april processor architecture multiprocessing th annual int 
symp 
computer architecture seattle may 
arvind culler data ow architectures annual reviews computer science annual reviews palo alto ca pp 

kubiatowicz agarwal limitless directories scalable cache coherence scheme mit vlsi memo 
submitted publication 
culler managing parallelism resources scienti data ow programs ph thesis dept electrical engineering computer science cambridge mass june 
gabriel mccarthy queue multi processing lisp acm symp 
lisp functional programming austin tex aug pp 

goldberg multiprocessor execution functional programs int 
parallel programming oct pp 

goldman gabriel preliminary results initial implementation acm symp 
lisp functional programming snowbird utah july pp 

goldman gabriel sexton parallel processing lisp springer verlag lncs proceedings japan workshop parallel lisp june university sendai japan 
watson manchester prototype data ow computer comm 
acm january pp 

halstead multilisp language concurrent symbolic computation acm trans 
prog 
languages systems october pp 

halstead assessment multilisp lessons experience int 
parallel programming dec pp 

hudak para functional programming computer august 
hudak goldberg serial combinators optimal grains parallelism functional programming languages computer architecture springer verlag lncs september pp 

hudak smith para functional programming paradigm programming multiprocessor systems th acm symposium principles programming languages january pp 

kranz halstead mohr mul high performance parallel lisp acm sig plan conference language design implementation portland june pp 

kranz kelsey rees hudak philbin adams orbit optimizing compiler scheme proc 
sigplan symp 
compiler construction june pp 

kranz orbit optimizing compiler scheme yale university technical report yaleu dcs rr february 
mohr kranz halstead lazy task creation technique increasing granularity programs proceedings symposium lisp functional programming june 
moss managing stack frames smalltalk proc 
sigplan symp 
interpreters interpretive techniques june pp 

control parallelism manchester data ow machine springer verlag lncs functional programming languages computer architecture portland oregon september pp 
model press 
steele jr hillis connection machine lisp fine grained parallel symbolic processing acm symp 
lisp functional programming cambridge ma august pp 

vandevoorde roberts abstraction controlling parallelism int 
parallel programming august pp 

waters series common lisp language second edition steele jr digital press maynard ma 
waters optimization series expressions part user manual series macro package massachusetts institute technology technical report aim january 
analysis dynamic partitioning springer verlag lncs proceedings japan workshop parallel lisp june university sendai japan 
parallel lisp programs stanford computer science report stan cs june 

