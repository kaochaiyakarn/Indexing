face recognition long term observations gregory shakhnarovich john fisher trevor darrell artificial intelligence laboratory massachusetts institute technology gregory fisher trevor ai mit edu 
address problem face recognition large set images obtained time task arising surveillance authentication applications 
set sequence images provides information variability appearance face robust recognition 
discuss di erent approaches information show cast statistical hypothesis testing problem classification task leads naturally information theoretic algorithm classifies sets images relative entropy kullback leibler divergence estimated density input set stored collections images class 
demonstrate performance proposed algorithm medium sized data sets approximately frontal face images describe application method part view independent recognition system 
recognition context visual surveillance applications topic growing interest computer vision 
face recognition generally posed problem recognizing individual single mug shot successful systems developed bochum usc 
separately systems tracking people unconstrained environments increasingly robust able track individuals minutes hours 
systems typically provide images users low medium resolution possibly multiple viewpoints long periods time 
optimal recognition performance information images user included recognition process 
long term recognition problems sets observations compared list known models 
typically models obtained sets images process thought set matching problem 
possible schemes integrating information multiple observations early late integration approaches 
early integration schemes consist selecting best observation set quality metric simply averaging observations prior classification 
case late integration common statistical approach take product likelihoods observation 
approach simple justified certain conditions fails account presence outliers observation data fact data expected observed certain variability repeated observations face individual yield typical set faces individual presume human faces occur natural variability 
propose approach face recognition sets images directly compare models probability distributions observed model faces 
variability observed data considered comparing distribution models outliers natural variation handled properly 
approaches distribution modeling matching possible general framework 
develop method gaussian model appearance distribution matching criterion divergence measure 
method observations models compared ciently closed form expression 
addition potentially accurate approach cient approaches require comparing observation model example image model distributions compactly parameterized 
evaluated approach di erent data sets frontal face images 
collection people observed distance frontal view indoor ce environment 
second computed proposed scheme integration multiple viewpoints images cameras arbitrary views user face combined generate virtual frontal view 
data sets distribution matching technique ered equal improved recognition performance compared traditional approaches showed robustness respect choice model parameters 
assume independence samples set images ignoring example dynamics facial expression 
disregarding potentially important cue remove assumption consecutive frames making easier recognize person sparse observations available instance surveillance systems subject face camera time 
furthermore training set may derived sparse unordered observations sequence 
remainder organized follows 
start discussion previous problem known methods appropriate classifying sets images 
sections statistical analysis leading distribution matching algorithm described detail section 
section contains report empirical comparative study discussed methods followed discussion section 
previous area recognition single face image established 
local feature global template methods extensively explored 
state art today defined family recognition schemes related eigendecomposition data directly linear discriminate analysis local feature analysis images achieve high performance 
face recognition image sequence generally set images subject relatively published studies 
common idea published recognition performance improved modeling variability observed data 
proposed detection tracking algorithms dynamics consecutive images face integrate recognition tracking framework 
variation individual faces modeled framework active appearance model 
temporal signature face defined feed forward neural network order classify sequence 
people recognized sequence rotating head images trajectories low dimensional eigenspace 
assume image associated known pose situation uncommon practice 
papers temporal constraints sequence images consecutive time crucial 
interested recognition looser assumptions images necessarily finely sampled ordered time 
late integration strategies number methods applicable situation multiple observations person face accumulated time 
instance general problem fusion evidence multiple measurements 
kittler statistical interpretation number common methods cross modal fusion product maximum majority rules appropriate late integration set observations single modality 
example shown assumption samples set distributed product rule maximum likelihood classifier set argmax argmax th sample set observations max rule chooses identity highest estimated likelihood single face image mean rule prefers identity gives highest mean likelihood input images 
majority rule instance voting strategy observes classification decisions input images separately picks label assigned largest number images 
lacking clear statistical interpretation combination rules applied scores likelihood values product mean distances feature space 
early integration msm algorithm combination rules generally late integration strategies combine likelihoods observations 
alternative approach consists combining input images mapping sequence feature space classification performed 
simplest example technique classification mean image sophisticated approaches capture higher order moments distribution 
particular mutual subspace method msm yamaguchi noteworthy 
msm test set images represented linear subspace spanned principal components data 
subspace compared subspaces constructed training data dissimilarity measured minimal principal angle subspaces approach works coarsely sampled sequences general unordered sets observations 
consider entire probabilistic model face variation eigenvalues corresponding principal components means samples disregarded comparison 
schematic examples illustrate shortcomings ignoring eigenvalues means 
ellipses correspond principal components data sets points obtained gaussian distributions solid line dotted dashed mean di erent covariance matrices 
fact noise slightly rotates ellipse plane 
terms distribution divergence closer mutual subspace approach fails recognize principal subspaces identical 
shortcomings subspace angle matching clearer 
principal components lie subspace principal subspace slightly rotated 
addition case important information similarity contained positions means disregarded msm 
msm approach desirable feature builds compact model distribution observations 
ignores important statistical characteristics data show section decisions may statistically sub optimal 
develop alternative approach takes account means covariances data grounded statistical approach classification 
statistical recognition framework assume images kth person face distributed underlying probability rule input face images distributed minimal angle vectors subspaces mean di erent means fig 

illustration di erence mutual subspace method distribution divergence method 
ellipses embedded correspond principal components estimated distributions 
minimal principal angle measure solid lines closer dotted terms distribution similarity kl divergence norm dashed chosen 
task recognition system find class label satisfying argmax pr subject pr confidence threshold conclude lack class reject input 
note score producing classifiers choose identity maximizes score function probability ectively assume posterior class probability identities monotonic score function 
setting minimal threshold score su cient decision equivalent setting 
deal rejection mechanisms assume fact equal database subjects 
set images distributed solving amounts optimally choosing hypotheses form statistics referred sample hypothesis samples case training set kth subject test set sets come distribution 
single sample point available known optimal hypothesis test performed choosing maximizing posterior probability sample 
larger sample set available optimal test comparison posteriors di erent models set 
classes equal prior probabilities comparison likelihoods test set di erent class models 
section discuss ways performing test relationship kullback leibler divergence distributions 
reality distributions unknown need estimated data follow estimate densities space frontal face images multivariate gaussian 
subject density estimated training samples subject face 
algorithm described require uses temporal adjacency images input sequence 
density matching hypothesis testing show relationship classical hypothesis testing density matching divergence 
analysis supported subsequent experiments recognition done comparing sets observations direct estimation kl divergence densities inferred training data model densities densities inferred samples test principled alternative standard approaches 
follows analysis ary hypothesis test stated follows 
hk pk goal determining hypothesis best explains data notation implies kth sample set drawn jth density 
discussion follows 
indicates sample set test density drawn 
indicates kth sample set model density inferred sample set 
commonly ary hypothesis test 
hk mild conditions equivalent 
quantify compare ability inferred model densities explain samples test equation infer density test samples quantify compare ability explain model samples 
assuming 
independent distributed classes equal prior probability known neyman pearson lemma see optimal statistic choosing hypothesis log likelihood function argmax log hypothesis test argmax log hypothesis test 
note implication samples determination best hypothesis 
assumption shown log likelihood samples drawn model distribution expected respect asymptotic behavior log lim log known asymmetric kullback leibler divergence defined log dx 
shown equality 
divergence quantifies ability density explain samples formalized information theoretic notion relative entropy 
context ary hypothesis testing see asymptotically gather samples expectation finite sample set choose model density closest sample density divergence sense 
light recast ary hypothesis test trying estimate divergence distribution underlying training data distribution underlying sample data test 
di erence hypothesis tests direction computes divergence 
hypothesis selection rules argmax argmax respectively 
consequently investigate methods approximate ary hypothesis test direct computation estimation kl divergence 
depending assumptions unavoidably introduce errors distribution estimates turn introduce errors estimate divergence 
commonly method assume parametric density form gaussian parameters estimated training data 
log likelihood samples test computed equation plugging estimated gaussian densities 
equivalent finding gaussian densities closest true model densities sense second gaussian distributions finding closest sense distribution sample test 
secondly estimate parameters distribution test sample subsequently evaluate log likelihood training data equation 
equivalent finding distribution closest true test distribution training distributions selecting closest estimated testing density sense 
result estimate gaussian distribution parameters training test data compute divergences analytically 
classification kullback leibler divergence section define proposed classification scheme estimated model input probability densities computation details scheme 
phase algorithm consists modeling data distribution single multivariate gaussian constructed factors density principal subspace isotropic noise component orthogonal subspace 
fairly standard procedure 
second novel phase closed form evaluation kl divergence models estimated input set ones estimated subject training data 
resulting values treated score classification rejection decisions 
computation dkl pk general case multivariate distributions evaluating dkl di cult computationally expensive task especially high dimensions typically performed means numeric integration computationally expensive especially high dimensions 
result case normal distributions provides closed form expression dkl log tr dimensionality data number pixels images means training set kth subject input set respectively covariance matrices estimated gaussians kth subject input set respectively 
estimating finding eigendecomposition operation compute determinant linear time product eigenvalues 
calculation matrix products require additional workload 
subjects database need kd operations 
compute dkl exchange indices 
kl divergence asymmetric measure expect results di erent directions 
normal approximation face appearance densities sake completeness describe calculation straightforward application pca done 
orthonormal matrix eigenvectors columns diagonal matrix non decreasing eigenvalues 
kd auto correlation matrix kth set images 
denote subscript components decomposition training set images kth subject subscript components decomposition test image set 
maximum likelihood estimate multi dimensional gaussian data 
assumption true dimensionality data noise excluded lower choose subset eigenvectors corresponding desired retained energy mk mk chosen eigenvectors define principal linear subspace model explain variance complementary orthogonal subspace replace remaining diagonal elements mean solution estimates distribution isotropic gaussian noise variance shown minimize kl divergence true distribution gaussianity assumption correct 
estimated density faces th class product gaussian densities subspaces written full dimensional space mean images th class 
kmk 

mk ki similarly estimated density observed face sequence 
important point need computed subject time subject entered database 
experiments order evaluate relative performance proposed algorithm compared integration algorithms mentioned 
comparisons data sets described containing frontal alternatively set mk directly techniques heuristic robust respect discovering true dimensionality face images 
data set obtained single camera face detector collect images proposed virtual view rendering technique generate frontal pose images set arbitrary views 
experiments compared performance algorithms distribution comparison algorithm kl divergence msm comparison mean log likelihood set comparison log likelihood mean face max min mean combination rules likelihoods individual images max min mean combination rules classifier scores individual images distances principal subspace 
majority vote likelihoods scores individual images due space constraints keep graphs scaled conveniently report results methods achieved reasonably low classification error energy level 
methods reported performance significantly inferior reported ones 
conversation video visual hull generated face views fig 

representative examples data experiments 
note pose lighting variation synthetic rendering artifacts conversation face sequences evaluate discussed methods conventional data obtained single monocular camera experimented data set containing video clips subjects filmed conversation refer data set conversation data 
person represented clip captured ccd camera fps 
faces detected fast face detection algorithm resized pixel gray level images 
false positives manually removed 
images pose frontal degrees smaller range tilt yaw 
frames person training set rest testing 
estimate behavior courtesy mitsubishi electric research lab discussed algorithms di erent input sizes partitioned test images sets frames seconds sets respectively 
examples shown representative amount variation data 
decided perform recognition task low resolution images reasons 
interest compare performance virtually rendered images described section small necessity 
second reduction computation storage space 
shows results frames 
cases dkl achieved performance statistically indistinguishable algorithms 
sets images error rate sets frames performance algorithms perfect 
optimal energy cut point algorithms algorithms including show robustness point staying low error wider range absence clear principled way choosing dimensionality principal subspace encouraging quality 
experiments sets frames di erence algorithms vanishes achieve zero error pca dimension 
observe robust performance respect pca dimensionality kl divergence mean face likelihood max rule likelihoods 
energy principal subspace recognition error kl msm mean log likelihood likelihood mean max single frames energy principal subspace recognition error kl msm mean log likelihood likelihood mean max single frames fig 

experiments conversation data sequences frames 
note frames algorithm achieve zero error rate 
view independent recognition rendered images previously developed methodology view independent multi modal recognition multiple synchronized views 
framework views subject rendered desired viewpoints fast implementation image visual hulls vh 
viewpoints set subject pose estimated trajectory mode operation relevant classifiers 
identity derived combining face recognizer operating nearly frontal views face gait recognizer extracts geometric features form sequence profile silhouettes 
energy principal subspace recognition error kl msm mean log likelihood likelihood mean max single majority likelihood performance set recognition algorithms di erent pca dimensions recognition accuracy rank kl msm log likelihood set log likelihood mean performance function fig 

experiments visual hull generated data sequences frames input images face classifier rendered virtual camera placed obtain frontal view user 
face image assumed roughly canonical scale orientation extent solving pose variation problem 
problem changing expression hair style remains 
addition images low resolution implementation due typical distance meters subjects physical cameras 
examples images training input face recognizer vh system 
general recognition performance data set lower typical data 
error rate reported face recognition larger data set included short input sequences 
worth mentioning frontal face view sought small spatial angle initial estimate face orientation face detected number virtual views nearly frontal 
number available virtual face images time frames typically larger order magnitude 
results shown computed leave cross validation 
sets synthetic faces subjects input rest data training 
classification algorithms evaluated di erent energy thresholds varying typically single principal component chosen typically components 
experiments conversation data address issue choosing right seen plots data sets producing best results range 
behavior algorithms including notably robust respect choice dataset dkl achieves best performance misclassification rate compared log likelihood input set computed 
values error rate associated dkl lower methods 
quantify robustness method shows classification performance function rank threshold 
vertical axis corresponds percentage correct identities ranked di erent measures 
robust performance algorithm solid line similar mean face likelihood discussion algorithm recognition set observations classifying model built set classifying individual observations combining results 
approach motivated casting classification sets statistical hypothesis testing problem uncommon statistical community best knowledge vision 
experimental results accordance theoretical analysis support statistically motivated density matching classification 
additional potential benefit algorithm fully realized current implementation reducing computational costs recognition sequential pca sophisticated matrix manipulation algorithms 
intend continue experiments extending range data larger number classes greater variability 
data assumption underlying distributions gaussian allows simple computation may true 
currently investigating alternative statistical models 
current integration strategy ignores dynamics sequence 
fact classifies sets sequences images assume meaningful temporal constraints images 
expect including dynamics face appearance available algorithm improve classification 
interesting direction extend distribution matching approach features principal components images 
instance may estimate distributions salient features eye mouth detectors 
alternatively active appearance model may fit image training input sets distributions computed feature values may compared 
general approach introduced applied recognition faces objects variation appearance images 

peter belhumeur joao hespanha david kriegman 
eigenfaces vs fisherfaces recognition class specific linear projection 
ieee transactions pattern analysis machine intelligence july 

zoran sven 
face recognition multi pose image sequence 
proceedings nd int symposium image signal processing analysis pages croatia 

brunelli poggio 
face recognition features vs templates 
ieee transactions pattern analysis machine intelligence 

thomas cover joy thomas 
elements information theory 
john wiley sons new york 

trevor darrell david neal pedro 
plan view trajectory estimation dense stereo background models 
proceedings international conference computer vision vancouver bc july 

edwards taylor cootes 
improving identification performance integrating evidence sequences 
proceedings ieee conf 
computer vision pattern recognition volume pages 

gong 
tracking recognition face sequences 
european workshop combined real synthetic image processing broadcast video production pages hamburg germany 

ismail haritaoglu david harwood larry davis 
real time surveillance people activities 
ieee transactions pattern analysis machine intelligence august 

josef kittler robert duin jiri matas 
combining classifiers 
ieee transactions pattern analysis machine intelligence march 

solomon kullback 
information theory statistics 
john wiley sons new york 

moghaddam tony jebara alex pentland 
bayesian face recognition 
pattern recognition 

moghaddam alex pentland 
probabilistic visual learning object representation 
ieee transactions pattern analysis machine intelligence 

atick 
local feature analysis general statistical theory object representation 
network computation neural systems 

louis 
statistical signal process detection estimation time series analysis 
addison wesley publishing new york 

gregory shakhnarovich lily lee trevor darrell 
integrated face gait recognition multiple views 
proceedings ieee conf 
computer vision pattern recognition hi december 

paul viola michael jones 
robust real time object detection 
technical report compaq cambridge research laboratory cambridge ma february 

wiskott kruger von der malsburg 
face recognition elastic bunch graph matching 
proceedings international conference image processing pages heidelberg 
springer verlag 

yamaguchi ken ichi maeda 
face recognition temporal image sequence 
ieee international conference automatic face gesture recognition pages nara japan 


dual di erential geometry associated kullback leibler information gaussian distributions parameter deformations 
sut journal mathematics 
