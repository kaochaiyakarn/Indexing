token passing simple conceptual model connected speech recognition systems young russell thornton cambridge university engineering department july describes simple powerful model connected word recognition viewed process passing tokens transition network 
advantages unifying view 
various apparently different connected word algorithms represented conceptual framework simply changing network topology application grammatical constraints straightforward importantly entire structure independent actual underlying pattern matching technology 
illustrate power conceptual model concludes describing done uk alvey sponsored project token passing paradigm enabled pass algorithm straightforwardly extended include generation multiple alternatives context free syntactic constraints 
godin lockwood investigated pass op bridle level building lb myers rabiner connected word rithms depth eventually showed syntax constraints applied algorithms essentially identical 
godin lockwood conventional formulation template matching viewed finding minimum cost path matrix local distances template matching viewed extending path template 
problem viewpoint specific dtw technology op lb algorithms fundamentally similar actual code implement different 
example level building dtw connected speech general entirely straightforward convert pass hidden markov model hmm 
crucially template template matching treated differently extending higher level processing include say complex grammatical constraints trivial 
describes simple conceptual model connected ward recognition term word denotes basic recognition unit may equally diphone phoneme transition network structure 
model equally applicable dtw hmm recognition shown op lb algorithms just simple topological variants 
furthermore model simple easy extend include generation alternative solutions complex high level control mechanisms 
organised follows 
general token passing model relationship various existing algorithms described section 
section extensions model include context free grammar constraints generation alternatives described 
section application model working laboratory system experimental results multiple alternatives various modes grammatical constraint 
token passing model isolated word recognition consider isolated word recognition 
token passing model vocabulary word represented word model finite state network form shown 
associated pair connected states word model cost pj associated state local cost dj 
unknown speech signal assumed usual form sequence vectors xl xt occupation state time implies cost matching associated speech vector xt dj 
possible sequence states model io il initial state represents possible alignment model speech signal 
total cost alignment speech recognition similarity model unknown speech assumed inversely proportional minimum cost alignment 
sj minimum cost alignment segment xl xt unknown speech model starting state state principle optimality cost computed recursion sj rain pj dj giving efficient way computing required minimum value rain allowable final state word model 
entirely straightforward known 
similarity model hidden markov model particularly self evident 
distinction maintained purposefully emphasise token passing restricted hmm 
discussed section jt 

xt structure word model equation normally evaluated representing matrix shown calculating matrix elements column column 
style evaluation basis godin lockwood 
central point alternative style evaluation token passing leads simpler powerful generalisation 
state word model capable holding moveable token moment holds single alignment cost time token state holds value 
terms tokens algorithm implied equation reformulated somewhat informally follows initialisation model initial state holds token value states hold token value oc algorithm state pass copy token state connecting states incrementing value pij dj discard original tokens state find token state smallest value discard rest termination examine final states token smallest value gives required minimum matching score 
te choose max 
time compute column time column time matrix partial alignment costs token passing algorithm illustrated 
corresponds time synchronous search strategy alignments represented tokens advanced frame input cycle main loop 
notice possibility having multiple initial final states allowed necessary isolated word recognition needed 
relation dtw hmm pattern matching noted correspondence token passing model hidden markov models obvious direct 
hmm transition probabilities aij output probabilities bj transition cost pij set equal log aij local cost dj set equal log bj equation max log log sj denotes maximum log probability model state generating sequence xl xz 
equation standard viterbi decoder equation hmm levinson 
dtw state word model associated frame template topology similar shown assumed 
case transition costs pij set equal fixed penalties case case 
shown denotes additive horizontal penalty denotes additive vertical penalty designed penalise paths distance propagate update tokens time frame pk pjk keep best token arriving state basic token passing algorithm matrix stray far away diagonal 
local cost dj set equal local distance frame template frame unknown 
equation sj denotes partial minimum accumulated distance frames unknown frames 
equation standard asymmetric dynamic programming decision rule form connected word algorithms 
fact dtw hmm recognition share similar formulae noted bridle juang 
dtw effectively special case hmm recognition 
connected word recognition token passing framework outlined section extension connected word recognition trivial 
individual word models simply connected looped composite model shown 
token passing algorithm applies effectively implementing pass op algorithm 
transition costs associated external arcs looped model reflect relative frequencies word occurring known equal 
ref frame local distance optimal path 
asymmetric decision rule test frame dtw recognition path search problem tokens connected word recognition token passing practice course connected word recognition wish know identity best matching word sequence just alignment cost 
cater basic token passing algorithm extended follows 
tokens assumed hold path identifier partial alignment cost path identifier simply pointer record word boundary information called word link record 
time steps taken addition listed basic algorithm token propagated external arc time create new containing change path identifier propagating token point new record extension algorithm illustrated 
seen effect token propagation potential word boundaries recorded linked list structure completion time path identifier held token minimum alignment cost trace back linked list find best matching word sequence corresponding word boundary locations 
tokens point new token propagated 
tim recording word boundary information relation existing connected word algorithms noted basic looped composite model shown reproduced corresponds directly pass op algorithm 
level building lb algorithm implemented connecting models left right sequence shown 
comparing equivalent op algorithm clear op algorithm efficient time space complexity proportional number word model instances 
interesting note lb algorithm originally defined myers calculates length sequence matches length sequence matches token passing implementation shown calculates levels parallel 
implies nested loops myers algorithm reordered godin lockwood show possible 
pass level building pass level building topologies figures illustrate application finite state regular grammar syntactic constraints token passing scheme achieved trivially connecting word models way required word sequences allowed 
syntax constraints applied distinction op lb algorithms disappears choice topology depends requirements syntax preferences algorithm 
syntax said op said lb real distinction precise constraint offered worth extra computation memory implied fold increase word model instances 
op lb algorithms viewed different special cases token passing algorithm finite state syntax constraints 
london ster ster manchester ster manchester styles syntax control word model abstraction preceding sections shown token passing approach allows op lb connected word algorithms implemented single algorithm simply choosing appropriate network topology 
furthermore individual word models apply equally dynamic time warp hidden markov model technologies 
crucial aspect token passing view way individual word models contribute recognition process determined entirely flow tokens book keeping necessary record word boundary positions word identities depends flow tokens 
clear internal processing carried word model separated model processing 
possible view word models units take tokens input produce tokens output 
section abstraction simplify description complex connected word algorithms actual operation word models represented simply procedure stcp means cycle main loop algorithm section executed 
token passing scheme token effectively represents minimum cost partial alignment sequence word models part unknown speech currently seen 
propagation token word model corresponds adding model sequence viewed clear controlling flow tokens word models corresponds implementation grammatical constraints 
shown treating word models pattern matching units enables standard interface defined high level processing components high level control tokens pattern standard interface separation high level control low level pattern matching syntax constraints parsing dialogue control system low level pattern matching 
existence standard interface allows speech recognition architecture built essentially independent underlying pattern matching technology matter step word models procedure implemented 
externally required time word model consumes token input representing best match sequence models segment input speech outputs token representing best match sequence models model segment input speech example approach taken alvey project young young 
initial architecture built word models dtw matching implemented hardware follow concentrated hmm word models minimal change system 
extensions model continuous speech recognition system proper syntactic constraints vital achieving acceptable recognition performance 
constraints usually applied pr ior limiting connections word models described 
net result applying syntactic knowledge way system computes best matching sequence word models conform regular grammar implied finite state network word model connections 
certain types application input language small users trained adhere basic scheme works 
ambitious grammars needed users trained speak prescribed syntactic forms database inquiry system general public problems 
regular grammar covering large input language may fail provide sufficiently tight constraints 
particularly true input language specified terms context free rules intuitive application designer compiled automatically approximately equivalent finite state network 
finite state regular grammar language includes strings desired context free language general include strings context free language compilation inevitably imply weakening constraints 
may advantage able apply context free constraints directly connected word algorithms 
systems rely strong priori syntactic constraints robust speakers stray outside prescribed grammar 
alternative approach extend basic connected word algorithm generate lattice word alternatives single best matching sequence 
syntactic knowledge applied posteriori parsing phase priori constraints giving flexibility design error recovery strategies 
section implementation direct context free constraints generation alternatives described 
mechanisms appear mutually exclusive fact case 
generating multiple alternatives combination strong priori syntactic constraints simply means alternatives grammatically correct 
subsequent semantic pragmatic processing may able benefit availability alternatives 
furthermore multiple alternatives important able focus generation process parts utterance contain important information 
done easily conjunction direct context free constraint mechanism non terminal grammar dominates definite substring terminals 
specification number alternatives allow associated non terminal 
corresponding entity finite state grammar making difficult control generation alternatives 
fact optimal combination mechanisms weakened set context free rules priori constraints generate multiple alternatives full set rules posteriori parsing 
discussed section 
direct context free grammar constraints direct context free grammar constraint mechanism described assumed allowed input language defined set extended bnf production rules wirth 
order apply context free grammar constraints token passing scheme grammar rules compiled set linked syntax networks form illustrated 
nodes syntax network types links terminals nonterminals 
link nodes store tokens points recognition decisions recorded 
terminal nodes correspond word models non terminal nodes refer separate sub networks representing rhs corresponding grammar rule 
word models networks shared unique instance word model sub syntax network terminal fragment context free rules journey nb place place rib london leeds york 
compile london word model link node collect propagate exit entry tokens tokens word link records implementing direct context free constraints non terminal rules 
notice implies recursive rules expansion require infinitely instances allowed 
extended bnf formalism grammar rules allows loops branches included handled constraint scheme difficulty little real loss expressive power 
types node combined way arc connects terminal non terminal link node vice versa 
syntax network exactly entry exit zero internal link nodes 
terminal non terminal node exactly arc leading link node may number 
link nodes viewed filters remove best lowest cost tokens passing 
representation context free grammar constraints implementation connected word recognition token passing framework relatively straightforward 
basic idea tokens propagate networks just finite state case token enters non terminal node transferred entry node corresponding sub syntax token enters terminal node transferred entry node corresponding word model 
similarly tokens reach exit word models sub syntaxes transferred back node owns 
flow tokens illustrated 
dotted line corresponds standard interface shown earlier 
flow tokens interface sequence word models visited token conform context free grammar rules 
detail algorithm connected word recognition context free grammar rule constraints follows initialisation store token cost entry node top level syntax store token cost cx link nodes algorithm propagate entry tokens top level copy tokens terminal nodes entry nodes corresponding word models step word models copy tokens exit nodes word models back corresponding terminal nodes propagate exit tokens top level termination token stored exit node top level syntax gives required best matching word model sequence 
procedures handle token propagation propagate entry tokens propagate exit tokens recursively defined follows propagate entry tokens syntax link node syntax node copy token non terminal copy token entry node corresponding sub syntax propagate entry tokens store token cost oo propagate exit tokens syntax link node syntax node preceding non terminal propagate exit tokens copy token exit node sub syntax back corresponding non terminal node filter tokens record decisions context free grammar stochastic arc syntax networks carry transition cost exactly word models 
token propagation procedures modified add costs tokens propagated 
procedure filter tokens called node preceding link 
simply examines token cost token exceeds token token replaced token procedure record decisions called link token filtering completed 
simply creates token see section changes path identifier token point new 
completion algorithm exit node top level syntax hold token path identifier points corresponding word recognised sequence 
tracing back preceding path identifiers yields actual recognised word sequence 
generating multiple alternatives obtaining multiple alternative word matches basic token passing scheme achieved recording best lowest cost tokens emitted syntactically distinct word boundary just best 
note best token propagated 
total cost potential word boundary recorded word link records cost individual word easily determined subtracting cost word boundary cost word boundary completion recognition processing converted lattice alternative word matches processed chart parser winograd 
process illustrated 
record best tokens emitted word boundary 
convert chart time recording multiple word matches generate multiple alternatives system direct context free constraints described necessary modify filter tokens record decisions procedures 
filter tokens procedure maintains list tokens link node 
terminal non terminal node preceding token added list lower cost tokens stored 
processing filter tokens link node completed record decisions procedure creates token stored linking additional alt field 
discards tokens best changes path identifier best token point corresponding 
number tokens saved link node set individually corresponding rule enabling generation alternatives focussed semantically rich regions input 
example parameter nb attached non terminals place indicate best alternatives journey best alternatives place saved 
word lattices generated procedures necessarily include best matching sequences token passing system common conventional pass algorithm word match available possible point possible start point 
intrinsic feature algorithm essential efficiency 
tokens enter node compete best propagated 
actual best matching sequences required word matches computed possible start point 
best sequences dynamic programming young 
procedure computationally expensive 
sub optimal procedure described cheap compute practice appear generate useful alternatives 
example application preceding sections described token passing approach design connected word recognition systems shown extended generate multiple alternatives 
section application token passing design voice operated database inquiry system briefly described experimental results indicate relative advantages various possible combinations priori posteriori syntactic processing 
project uk alvey sponsored project year collaborative venture british telecom logica cambridge university engineering department 
project concerned recognition algorithm development se concerned aspects voice system design arose intended telephone conversational question answer system general public 
example application area tackled train timetable inquiries 
shows block diagram final architecture 
central control system resides frame dialogue controller dc young proctor 
dc operates cycles asking user questions processing replies query fully understood 
system knowledge syntax semantics form context free grammar rules stored rule database 
start recognition cycle dc activates distinct contextually relevant rule subsets rule database 
subsets apply priori syntactic constraints subsequent pattern matching posteriori parsing 
actual speech recognition takes place levels word level phrase level 
word level input speech matched frame synchronously word models dtw variant basic token passing scheme described section 
handle interword pauses word model augmented include optional preceding looped silence state 
word model fixed cost frame wildcard connected parallel place upper limit cost match 
phrase level flow tokens word models controlled transition networks built automatically grammar multiple alternatives recorded described section 
input speech consumed recorded word boundary information lists converted chart word alternatives processed chart parser 
chart parser modified form standard bottom algorithm dialogue controller input rules frame speech output subsystem context free grammar rule database chart parser activate parser rules word phrase level activated matching recognition constraint rules path speech scores input word level matching ii system architecture winograd concept active edge agenda dispensed 
inactive edges built vertices processed fixed right left order 
complete parse required syntactically valid interpretations wanted modified algorithm functionally identical standard algorithm executes order magnitude faster uses memory 
full chart built consist large number arcs edges chart parsing terminology spanning segments input 
parser initially assigns cost terminal edge equal matching cost computed corresponding word model 
higher level spanning edge assigned score equal sum edges subsumes 
edge labelled non terminal subsumes sequence edges 
corresponding syntax rule 
see cost assigned edge practice situation shown usual case matches subset constituents 
cases constant cost input frame added frames spanning arcs edges yj yk cost wildcard threshold denotes length edge labelled value chosen greater average cost input frame correct word match incorrect 
method dealing unexplained segments input investigated simplest effective 
costs assigned edges chart lowest cost phrase structures built parsing process extracted 
doing parser insist edge spans entire input 
example possible syntactic interpretations input 
cost computed equation sx sf cost computed equation set equal total number frames input sx sg procedure ensures syntactically valid segments input match part fully spanning syntactic interpretation ignored 
semantic interpretation performed stripping final parse tree semantically irrelevant branches matching slots flame dialogue controller 
effects performance syntactic constraints separation rules system allowed effects recognition performance balance priori processing investigated 
version full scale user trials 

yn 
yl 
yn example edge configurations chart employed conventional finite state syntax constraints 
system worked users knew preprogrammed syntax rules performance dropped rapidly naive untrained users system forced adapt rapidly isolated word style interaction 
version described allows rules priori constraints weakened whilst retaining full set grammar rules parsing semantic interpretation 
investigate effects syntactic constraints performance system measured different constraint mechanisms finite state priori constraints derived fi om context free task grammar original system 
cf context free priori constraints derived directly task grammar 
context free constraints derived set weak rules consistent task grammar enforce function word content word ordering higher level structure 
wildcard placed phrasal rules account unknown words 
null priori constraints 
cases parsing semantic interpretation identical 
case alternatives generated fully spanning syntactic interpretations input allowed 
cases alternatives recorded semantically important nodes alternatives recorded 
test material consisted syntactic sentences set conformed task grammar non syntactic sentences ns set conform semantically similar 
ns set extracted transcripts real british rail enquiries grammatically unusual realistic 
note ungrammatical vocabulary unknown system 
results tables speakers ic st read sentences realistically possible 
dtw recogniser trained individually speaker 
performance terms slot recognition rate word recognition rate meaningful quantity measured type system 
full definition slot lies scope intrinsically recursive closely tied method dialogue control implementation 
slot corresponds roughly single item information utterance way example sentence want leave travel york slots 
leave 

time digit 

york opposed arrive opposed time departure place destination place test cf null ic st ic ns st ns table percentage slot recognition rate best alternative test cf null ic st ic ns st ns table percentage slot recognition rate best alternatives cases recognition rate calculated nc nt iv nc number slots correctly recognised nt total number slots number insertion errors 
table seen syntactic set sentences little difference conventional finite state estate constraints direct context free cf constraints 
non syntactic ns set dramatic drop performance case worst 
case better performance cf case mainly due sophisticated parsing possible availability alternatives 
priori constraints weakened case little change performance set considerable improvement ns case 
constraints null case performance sensitive ungrammaticality falls case 
strategy weak priori syntactic constraints conjunction full parsing lattice alternative word matches appears offer robust strategy type conversational speech system 
table shows effects including second best global interpretation input slot matching algorithm 
case pattern results emerges small significant improvement case 
depending design dialogue controller generation semantic alternatives may useful 
described simple conceptual model connected speech recognition token passing 
advantages approach firstly allows low level pattern matching abstracted way clean interface word level phrase level processing 
great practical benefit enabling higher levels speech recognition system independent actual pattern matching technology 
second main advantage token passing view straightforward 
view characteristics connected word algorithms discussed godin lockwood largely self evident token passing viewpoint 
furthermore reasoning complex extensions considerably simplified examples phrasal level incorporation direct context free grammar constraints semantically focussed multiple alternative generation described 
performance issues regard syntactic constraints 
system model relative effects tight weak priori syntax constraints investigated 
users input forms syntax lies outside preprogrammed task grammar performance drops dramatically conventional tight priori constraints 
combination weak priori syntactic constraints full posteriori parsing lattice alternative word matches appears robust 
availability semantic alternatives gives smaller performance improvement may worthwhile dialogue controller designed exploit 
project funded uk alvey directorate collaborative venture british telecom research laboratories logica uk cambridge university engineering department 
assistance support cooperating partners gratefully acknowledged 
bridle bridle js 
stochastic models template matching important rela tionships apparently dif rent techniques asr 
proc ioa autumn conf vol pt 
bridle bridle js brown md chamberlain rm 
algorithm connected word recognition 
proc icassp pp paris france 
final evaluation 
proc th fase symposium speech pp 
godin lockwood godin lockwood dtw schemes continuous speech recognition unified view 
computer speech language vol pp 
juang juang 
hidden markov model dynamic time speech recognition unified view 
technical journal vol pp 
levinson levinson se rabiner lr sondhi mm 
appli cation theory probabilistic functions markov process automatic speech recognition 
vol pp 
myers rabiner myers cs rabiner lr 
level building time war ing algorithm connected word recognition 
ieee trans assp vol pp 
tk 
element wise recognition continuous speech composed words fi om specified 
vol pp 
winograd winograd language cognitive process volume 
addison wesley pp 
wirth wirth algorithms data structures programs 
prentice hall series automatic computation englewood cliffs new jersey 
young young sj 
generating multiple solutions fi om connected word dp recog nition algorithms 
proc ioa autumn conf vol pt pp 
young young sj 
designing conversational speech 
proc iee vol pt 
young proctor young sj proctor ce 
design implementation di control voice operated database systems 
computer speech language press 
young young sj russell nh thornton 
speech recognition ii 
proc icassp vol pp new york 

