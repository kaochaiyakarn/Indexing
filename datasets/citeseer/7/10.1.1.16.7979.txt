introduce simple sequential write benchmark improve linux nfs client write performance 
reduce latency write system call improve smp write performance reduce kernel cpu processing sequential writes 
cached write throughput nfs files improves factor 

network attached storage nas easy way manage large amounts data 
applications access data stored nas various standard internet protocols nfs 
storage compete locally attached storage requires nas provide equivalent performance 
nas server performance understood client performance long terra 
linux servers proliferate enterprise information infrastructures performance linux nfs client emerges factor critical success complex applications database mail services network attached storage 
efficient access shared data laboratories extensive linux workstations depends superior nfs client performance 
interested equally important goals 
goal narrow improve performance linux nfs client 
pursue broader goal identifying factors influence nfs client performance 
document written part linux scalability project citi 
described supported network appliance incorporated 
linux nfs client write performance chuck lever network appliance incorporated cel com peter honeyman citi university michigan honey citi umich edu understand nfs client performance issues developed simple file system benchmark measures write latency throughput 
describe rationalize benchmark identify means improve application write performance files accessed linux nfs client 
suggest ways apply benchmark comparative techniques client performance general 
remainder organized follows 
section detail development benchmark identify issues distinguish client server performance benchmarking 
section benchmark expose correct latencies linux write system call 
section outline areas exploration conclude 

measuring nfs client performance section develop rationale simple sequential write benchmark bonnie 
benchmark developed specialized hardware described section includes smp linux nfs clients connected prototype network appliance filer gigabit ethernet 

client performance issues nfs client right design client responsible ordering bytes managing network server congestion handling complex issues implementing distributed file system 
leaves server simple scalable 
fact nfs servers maintain little state 
satyanarayanan justify architecture pointing typical client server distributed systems workstations cycles burn consequently nfs client tends complex interfere efficiency correct behavior 
measuring nfs server performance understood 
computer science literature contains examples benchmarks meant quantify nfs system performance server performance 
spec sfs typical nfs server benchmark 
remove client behavioral performance variations benchmark results spec sfs uses user space nfs client access nfs servers test 
client performance measurement differs server performance measurement 
generic file system benchmarks biased exercising weaknesses disk storage terribly useful nature file system implementation uses network device back 
example typical file system benchmark tests random sequential read write requests 
client sends random write requests server fast sends sequential ones 
performance differences exist random sequential nfs accesses measuring server disk performance client behavior 
nfs client performance depends performance networks servers 
problematic operate nfs client server difficult isolate performance problems specific client 
slow server network cause application performance problems relatively easy identify fix demonstrate faster server performance degrade client performance 
client wire write request behavior affects server performance scalability 
clients modulate application unfortunate random access pattern help servers scale better 
relationship client server carefully considered dissecting client performance issues 
focus client ability get requests server 
may approach issues server scalability arise client misbehavior 
way measure client performance eliminate performance bottlenecks downstream components fast networking technologies non volatile ram server push client hard possible see breaks 
just sfs uses client test different servers simple memory server developed compare clients fairly 
approach compares performance behavior single client typical workloads variety networking conditions server types 
approaches necessary wary bias traditional file system benchmarking measuring disk behavior factors important client performance 
borrow approaches study 
hardware test bed consists high performance smp linux client hardware connected highperformance gigabit ethernet switch prototype network appliance filer 
included test bed cpu linux server single cpu solaris nfs clients report 
comparing behavior performance clients servers exposes performance issues escape attention 

related little related focuses specifically nfs client performance 
improving nfs performance amounts helping server disks efficiently improving client caching strategies dahlin adds write clustering clients help server scalability adding new features protocol quite nfs 
martin culler investigate nfs behavior high performance networks address implementation specific issues existing clients 
closest previous describes performance improvement reduced cpu loading elimination data copies bsd reno nfs implementation 

inter run variance linux experience performance measurement linux taught expect large variations results individual benchmark runs version software hardware configurations 
benchmarks performed authors past revealed variations performance parts linux kernel including virtual memory subsystem scheduler parts system correctness depend global kernel lock 
outlying data points skew average results masking relevant behavior 
variations common commercial operating systems solaris 
best results linux excellent hampered outliers leaving moderate performance average 
measurements reported illustrate phenomenon 
forward progress ignore variations 
time experience resolves issues wish untuned system behavior consistent 
address generally report single run results 
shape results typically consistent run run including highly variable outlying results 
interested trends precise measurements noting anomalies 

introducing simple write benchmark started measuring linux nfs client bonnie understand aspects linux client performance combination simple typical load 
refined benchmark include small part suite tests performed bonnie 
section discuss left 
simple microbenchmark complex application simulation provides immediate feedback additional effects application processing improving repeatability results 
offers workload drives specific components client surgical precision 
microbenchmark offer clear assessment real world application performance impact 
benchmark program block sequential write portion bonnie file system benchmark 
test measures quickly application write kb chunks fresh file 
writing fresh file narrows focus write code pathways client read preexisting file data server complete write requests 
write throughput depends behavior kernel vm networking rpc layers offers generic picture file system performance 
addition raw write performance important typical real world workloads 
read write operations network intensive data transmitted requests 
client caching performance application read requests client writes reflect network efficiencies latencies directly 
sequential writes minimize disk latency seek time typical disk servers 
pointed section gain little new information client comparing random sequential results 
considered testing memory server chose start benchmark require atypical server modifications 
simple typical application run client exercises critical paths client server 
bonnie includes final close call elapsed time throughput calculations capture occurs write 
local file systems dirty data remains system data cache final close operation 
fair comparisons nfs flushes completely close due close open semantics local file systems may delay flushing improve perceived performance benchmark reports throughput results writes subsequent flush operation final close operation 
result throughput measurement reported megabytes second mbps calculated dividing total number bytes written amount time benchmark just respective operation writes flush close 
benchmark reports system call latency 
calculate throughput dividing average system call latency average byte size request 
reducing system call latency immediate positive effects throughput 
get heart system call misbehavior necessary record actual average latency 
demonstrate jitter variation latency call drastically degrades data throughput test easily revealed examining actual results computed averages 

write latencies linux nfs client report results benchmark run smp linux client files linux nfs server network appliance filer 
goal identify correct write performance problems 
section describes software hardware configuration sections report measurements describe fixes 
finish description improvements linux nfs client resulting 

systems test section describe systems tests 
client system client software runs dual processor pentium iii system iii le chipset 
processors mhz fc pga packages kb level cache 
front side bus sdram speed mhz 
mb pc registered sdram system 
client gb ibm eide drive 
limitations south bridge ide controller runs multiword dma mode 
chipset supports bit mhz pci slots ga gigabit ethernet nic supports base copper 
card uses alteon ii chipset 
system runs linux kernel red hat distribution 
filer network appliance filer prototype eighteen gb seagate lc scsi drives 
single mhz fc pga pentium iii kb level cache mb ram mb nvram pci card 
system supports bit mhz pci slots contain logic isp scsi controller fiber optic gigabit ethernet card intel chipset 
data stored system contained raid volumes 
system runs pre release network appliance data operating system special options enabled test volume include atime update option eliminates inode write activity workloads consist read requests 
option probably effect write intensive workloads 
test volume contains disks single raid group 
snapshots enabled tests 
linux server linux nfs server way intel system nx 
mhz pentium iii cpus kb level cache 
front side bus sdram speeds mhz 
system contains mb sdram seagate scsi drives varying model controlled scsi controller 
system network connected ga base ethernet nic installed bit mhz pci slot 
system runs linux kernel red hat distribution 
nfs files stored system reside single physical disk raid 
maximize server write performance async export option throughput results reported linux server comparable production server 
benchmark results produced prototype hardware software necessarily reflect performance released product 
systems connected single extreme networks summit ethernet switch 
copper connections cat fiber connection filer standard multi mode 
packets enabled switch systems test benchmarks 
mentioned network links gbps full duplex 
network appliance filer linux nfs server mounted typical mount options nfs version udp 
kernel linux kernel nfs server support sizes larger 
smaller causes linux client synchronous network writes resulting significant drop write throughput 
addition sizes match block size simple write benchmark 
network lock manager disabled testing reduce protocol overhead 
test overhead caused lock manager interaction quantify baseline overhead data transfer 
ethernet frames easy optimization improve data throughput 
frames networks allow large frame sizes unsuitable situations 
realistic local widearea networks smaller frames useful study cost packet fragmentation reassembly 
chose leave frames disabled tests 

local versus network write performance compare performance sequential writes local file system ext fs client performance sequential writes networked file system nfs served filer linux nfs server 
compares cache performance ext cache performance nfs regard back performance 
ext cached write performance target nfs client cached write performance 
test calculates write throughput dividing total number bytes written elapsed time required write system calls complete 
shows throughput results include write calls including final flush close calls included 
allow better comparison results included ext fs usually flush close 
write throughput kb sec file size mb linux nfs async filer local ext 
local nfs cached write performance 
write throughput measured test files sizes mb mb 
note large peak memory write performance local files appear nfs files 
nfs memory write throughput remains constrained network server throughput 
writes local files fast memory available cache dirty data 
test file size approaches size client memory performance drops raw disk speeds 
contrast nfs client constrains write throughput ample memory available cache writes 
test application generate data fast nfs server take matter small file little write buffering appears occur client 
subsection explore limitation 

periodic latency spikes early testing discovered write system call latency varies wildly periodically 
explore write system call latency execute benchmark single mb file residing network appliance filer report latency write system calls test 
typical result shown 
writes complete microseconds periodic jump latency approximately system calls 
latency slow system calls milliseconds 
relatively slow calls calls run inflate mean latency run microseconds call excluding calls exceeding millisecond microseconds call factor 
actual write system call latency linux client count write system calls 
write system call latency 
shows write system calls mb benchmark run 
periodically write system calls take milliseconds increasing mean latency decreasing throughput 
observe similar results network appliance filer linux nfs server 
latency spikes appear write requests wire 
eliminating spiky latency behavior lower average write latency improve write throughput 
instrumented linux nfs client write code path record time required step write system call 
linux kernel gettimeofday kernel function capture wall clock time side target section code record timings kernel log 
discovered places linux nfs client delays writer threads keep memory usage check 
delays writers number pending write requests inode mounted file system exceeds fixed limits 
inode request count grows larger max request soft value kernel nfs client forces writer thread schedule pending writes inode wait completion resuming current request 
mount request count grows larger max request hard value kernel nfs client suspends thread writing file system thread signals fewer max request hard requests 
internal write request larger page 
implementation employ hysteresis smooth request load 
actual write system call latency linux client count write system calls 
write system call latency periodic flushes 
show entire benchmark run mb file 
latency axis 
periodic spikes write system call latency gone average latency grows worse time 
limits prevent large backlog write requests 
classic time space tradeoff 
limiting amount space available buffer nfs writes operating system avoids expense reclaiming pages network server latency speeds 
avoids memory starvation write requests pending server unavailable 
system call test generates write requests bytes pages requests 
test application write calls internal requests queued test file inode 
server lagging may requests writes older past system calls 
system calls client flushes inode write request queue 
produces spiky latency seen 
linux nfs client separate daemon called nfs flushes cached write requests writing application 
minimize cost writes client cache requests available memory 
solaris nfs client example flush write requests application requests fsync close client allocate memory new requests case vfs layer blocks writer 
see periodic latency spikes gone 
mean latency improve entire run writes case average latency microseconds 
furthermore latency increases time 
investigate behavior section 

list scans sequential write performance scalability problems result lengthy data structure traversals 
establish data structure traversal limits throughput kernel profiling tool provides sample driven histogram kernel execution pinpoint areas heavy cpu usage 
profiler exposed functions nfs client consume significant cpu resources benchmark run nfs find request nfs update request inline function nfs find request 
helper function scans sorted list inode write requests find request matches application current write request 
list maintained order increasing page offset file 
eliminating periodic write request flushing inode list longer 
sequential benchmark causes client traverse list completely write system call find matching request client adds new request list 
improve scalability implemented hash table similar hash tables linux kernel manage client outstanding write requests 
hash table supplements inode write request list 
finding pending write request faster memory cost bytes request bytes inode plus size hash table 
linux vfs layer passes write requests larger page file systems time 
nfs client builds rpc request maintains page write requests inode list ordered page offset 
modification inserts requests hash table requesting inode page offset request 
requests page inode kept hash bucket overlapping requests detected searching requests single bucket 
client usually caches single write request page maintain write ordering normally issue 
write requests coalesced chunks just client generates write rpcs 
write latency milliseconds call linux client count write system calls 
write system call latency scalable data structures 
write latency remains low number outstanding requests increases entirety benchmark run mb file 
comparison latency axis figures 
see improvement hash table track write requests 
write system call latency run averages microseconds call mean original client latency spikes excluded see 
sustained memory throughput sequential write benchmark mbps compared mbps mb file 
notice gap greatly reduced jitter calls middle 
gap appears runs filer 
investigate section 

global kernel lock smp hardware having eliminated extra flush write path implemented scalable hash table track write requests compare write throughput performance client network appliance filer way linux nfs server 
typical run write benchmark mb file filer sustains mbps network throughput 
benchmark reports generate mbps writes 
hand linux server sustain mbps network throughput filer network throughput benchmark writes rate greater mbps faster filer run 
explore unexpected behavior examine write latency 
shows histogram write system call latencies 
calls take microseconds take longer 
distribution shows slow calls file resides faster servers 
surprisingly client requires overhead buffer writes sending data slow server 
verified result server mbps ethernet 
benchmark writes memory faster server sustains mbps second network throughput 
kernel execution profiling shows benchmark runs global kernel lock taken nfs commit write contention smp hardware 
lock section fourth largest cpu consumer kernel exercised twice fifth largest consumer 
profile analysis section shows lock taken nfs commit write contributor cpu time sampled lock section 
smp hardware single writer thread uses cpu data flushed write system call flushed nfs client write daemon nfs 
kernel lock contention results single writer thread flush daemon generate network write requests 
nfs holds global kernel lock awake flushing requests 
suspected flush daemon causing contention removing global kernel lock daemon little improvement 
instrumented write path find time spent kernel spends microseconds write request network layer sock sendmsg called rpc layer rpc request 
accounts time request spent waiting nfs client write path acquire kernel lock 
development linux kernel global kernel lock removed linux network implementation 
longer necessary hold kernel lock calling network layer release lock reacquire sock sendmsg returns 
permits writing processes progress network layer sends current request 
number calls write system call latency milliseconds linux nfs server async 
write system call latency different servers 
shows latency write calls benchmark run mb file 
runs minimum latency filer run large number lengthy calls 
average latency client memory writes increases file stored faster server 
shows improvement write system call latency occurs removing kernel lock sock sendmsg 
run calculated results improve mean write system call latency drops benchmark runs new client versus microseconds filer versus microseconds linux filer maximum latency drops microseconds microseconds 
calculating averages excluded data point runs 
latency write system call millisecond runs 
note minimum latency hardly changes 
agrees idea latency variation code path issue results writer waiting acquire resource lock 
ran mb benchmark lock modification 
filer benchmark runs mb second better increase earlier mb second 
benchmark runs mb second linux server improvement 
lock contention measured profiler number calls write system call latency milliseconds linux nfs server async 
write system call latency lock contention 
shows maximum latency latency variation jitter clearly reduced 
average filer writes take longer writes linux nfs server difference smaller 
minimum latency remains roughly suggesting latency variation case result lock contention 
entirely gone 
results summarized table 
unexpected inversion performance gone client runs faster server slower 
discuss section 

cost responding server replies network appliance filer provides better network throughput linux nfs server applications writing filer run slower 
despite fact client processing required filer writes don require additional commit rpc client throughput fast server hampered lock contention cost handling server replies higher rate 
faster server forces client amount time 
explains unexpected inversion client performance 
network lock lock mbps mbps mbps linux async mbps mbps mbps table 
application write throughput lock modification 
network write throughput compared application write throughput writing mb file 
removing global kernel lock rpc layer improves cached write throughput files residing network appliance filer linux nfs server 
section explains applications write faster slower server 
tests single application writer thread contending single flusher thread show ideal scaling 
client single cpu expect find flusher thread cpu time away userlevel writer thread increasing server throughput increases 
client cpu writer thread flusher thread interfere 
suspect faster servers exacerbate smp linux clients issue addressed 
recall short period write system call latency lower average 
explained reduced smp lock contention interrupt load filer briefly stops responding network write requests file system checkpoint 
effect filer behaves infinitely slow server period momentarily eliminating smp lock contention client 
flusher thread blocked application writer thread active 
threads compete writer allowing client return control quickly application 
words difference slowest fastest writes due client cost responding server replies 
fast networking introduces significantly increased interrupt loads 
new network device driver api linux may help especially single processor systems improving system behavior intense interrupt loads result client communicating highperformance server low latency network 
system recognizes device producing interrupts high rate masks device interrupt polls 
workload decreases interrupt re enabled keep latency reasonable 
tech nique expanded mogul ramakrishnan 
hope explain network layer takes microseconds rpc request mhz processor 
suspect ip fragmentation major expense 
frames feature gigabit ethernet may help reducing need fragmenting reassembling large rpc requests ip layer extend wans general 
removing global kernel lock write path yields considerable improvements throughput application concurrency 
happens rpc layer acquires global kernel lock ensure integrity internal data structures 
removing global kernel lock rpc layer allow smp system multiple network interfaces process rpc request time allowing concurrent writes separate files separate servers separate client cpus 

final measurement illustrates modifications improved client write performance 
modifications nfs write performance memory available buffer write requests drops server throughput rate client exhausts memory 
left side shows memory write performance nfs files considerably improved 
write performance longer limited network server speeds 
client scalability defects continue cause memory writes files network appliance filer mbps slower files linux nfs server 
right side shows client memory exhausted filer sustains greater network write throughput linux nfs server 
nfs write performance writes local files 
believe due costs client responding server replies 
costs include interrupt handling network processing clearly greater simply managing disk write throughput kb sec file size mb linux nfs async filer local ext 
local nfs cached write performance revisited 
shows write throughput test files sizes mb mb 
nfs write throughput considerably improved compared 
application write throughput longer tracks network write throughput small nfs files 
maximum cached write throughput nearly servers 
throughput local test test linux nfs server immediately trail file sizes exceed physical memory size client benchmark able sustain high data throughput longer test file resides network appliance filer 
conjecture filer nvram acts extension client page cache allowing writes server proceed near local memory speed server nvram full 
fact filer able process requests faster client buffers available little 
workloads hold file open long time write asynchronously requirement data permanent write system call complete slower linux nfs server slight advantage 
applications write immediately flush close applications require data permanence write system call returns databases network appliance filer greater network disk throughput performs better 
cached writes slightly slower client applications regain control sooner flush close file writing faster server 
client scalability improves applications take advantage improved memory write throughput better network throughput 

releases linux nfs client writing initial drafts shared maintainer linux nfs client 
built ideas creating safe version patch remove global kernel lock rpc layer nfs client write path 
patch available web site experimental patches section 
simple change write request queuing logic reverse order list results hash table experiment allow sequential writes insert new requests request list constant time walking entire list 
network appliance replaced flushing logic described section entirely new system 
appears linux kernel releases 
changes occurred kernel direct comparison meaningful 
hope analyze improvements 

discussion describe simple sequential write benchmark measure file system write latency throughput 
show benchmark reveals performance scalability problems linux nfs client describe modifications linux nfs client improve application write latency throughput 

observations client benchmarking nfs server job store data metadata organized way move data network cards disks efficiently possible 
measuring behaviors understood 
hand client role translate adapt nfs protocol local environment efficiently 
subtle task 
client complex completely dependent performance servers disks microbenchmark large suite tests focus analysis small parts client behavior 
result studies identified areas client implementation directly affects application throughput 
areas documented previous 
networking efficiency packet fragmentation reassembly handling packet loss eliminating data copies handling heavy interrupt loads optimizing number network requests contribute cost handling server replies 
caching efficiency effective caching nfs clients perform local file systems 
means making best available memory properly implementing cache coherency 
scheduling unfortunate write scheduling application performance server scalability 
lock contention number cpus avoiding lock contention critical 
direct bearing client performance scales adding cpus network interfaces 
data structure efficiency power clients amount cached data grows vital manage efficiently 
current efforts focus developing suite microbenchmarks aspects style mcvoy lmbench 

identified specific issues linux client deserve investigation 
continues hope evolve benchmarks measure areas 
want assess impact global kernel lock scalability linux nfs client 
want continue investigating slower servers allow faster memory write throughput linux nfs clients general continues variance benchmark runs linux 
especially want prove comparative methodology real application domains 
keep study point focused mainly microbenchmark determine real world impact changes 
techniques valuable surveying client implementations 
hope explore improvements linux nfs client affect behavior corner cases face advanced deployments outside research lab file locking specialized caching behavior performance databases massively parallel applications combined storage 
authors gratefully acknowledge assistance colleagues andy adamson smith james newsome steve molloy university michigan brian pawlowski network appliance spencer sun microsystems especially helpfulness thorough linux nfs client 
special freenix reviewers shepherd jim 
intel citi hardware study 
source simple write benchmark patch linux kernel includes modifications discussed available citi web site www citi umich edu projects nfs perf patches 
bray bonnie source code 
netnews posting 

card ts tweedie design implementation second extended filesystem proceedings dutch international symposium linux december 

dahlin want anderson patterson quantitative analysis cache policies scalable network file systems acm sigmetrics conference measurement modeling computer systems may 

michael 
personal communication september 

hitz lau malcolm file system design nfs file server appliance usenix technical conference proceedings january 

protocol networked information 
www org protocols html 
improving write performance nfs server usenix technical conference proceedings january 

lessons learned tuning bsd reno implementation nfs protocol usenix technical conference proceedings january 

quite nfs soft cache consistency nfs usenix technical conference proceedings january 

martin culler nfs sensitivity high performance networks sigmetrics performance joint international conference measurement modeling computer systems may 

mcvoy staelin lmbench portable tools performance analysis usenix technical conference proceedings june 

mogul ramakrishnan eliminating receive livelock interrupt driven kernel usenix technical conference proceedings january 

benchmark 
see www org 
ousterhout douglis beating bottleneck case log structured file systems proceedings acm symposium operating system principles january 

pawlowski smith hitz nfs version design implementation usenix technical conference proceedings june 

satyanarayanan howard nichols sidebotham spector west itc distributed file system principles design proceedings th acm symposium operating system principles december 

callahan network file server performance benchmark usenix technical conference proceedings june 

spencer rfc nfs version protocol specification ietf draft standard december 

standard performance evaluation 
spec sfs 
www spec org osg sfs 
sun microsystems rfc nfs network file system protocol specification ietf network working group 
march 

sun microsystems rfc nfs network file system version protocol specification ietf network working group 
june 


linux nfs client pages 
see www uio src 
bruce laddis generation nfs file server benchmarking usenix technical conference proceedings june 
