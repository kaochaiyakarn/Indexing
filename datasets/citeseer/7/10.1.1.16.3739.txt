comparative study linear feature transformation techniques automatic speech recognition thomas reinhold philips gmbh wei 
aachen germany email pfa research philips com widely open questions concerning properties linear discriminant analysis lda account success speech recognition systems 
order gain insight nature transformation compare lda mel cepstral feature vectors respect criteria decorrelation ordering property invariance linear transforms automatic learning dynamical features data dependence transformation 

structure typical continuous speech recognizer consists front feature analysis block followed statistical pattern classifier 
interface feature vector ideally contain information speech signal relevant subsequent classification insensitive irrelevant variations due changes acoustic environment time low dimensionality order minimize computational demands classifier 
types feature vectors proposed 
study representations derived fft analysis speech signal subsequent perceptually motivated bandpass filtering power density spectrum log spectrum coefficients mel frequency cepstral coefficients mfcc features obtained linear discriminant analysis lda applied log spectral cepstral feature vector 
note mfcc lda features result linear transform log spectrum feature vector different properties 
lda proven improve discrimination speech feature space 
led improvements recognition performance small large vocabulary speech recognition 
widely open questions properties lda account success speech recognition systems 
order gain insight nature transformation compare lda mfcc respect criteria decorrelation ordering resulting vector components invariance linear transforms automatic learning dynamical features data dependence robustness 
properties cepstral transform linear discriminant analysis lda section various properties transformation techniques investigated 
experimental evidence section 

decorrelation initial log spectral feature vector computed smoothing spectrum overlapping triangular kernels 
leads relatively high correlation feature vector components observed described sietill database left image 
note order concentrate characteristics different feature sets show static features 
log spectral feature vector input transformations 
assuming correlation matrix input data exhibits toeplitz structure neglecting boundary effects shown cosine transform leads decorrelated features 
measured correlation matrix cepstral feature vector diagonal original vector cf 
left middle 
definition lda transformation delivers uncorrelated features 
complete decorrelation accomplished lda observed right 
importance decorrelated feature sets speech recognizer studied section 
feature log spectrum feature feature cepstrum feature feature lda feature correlation coefficient correlation matrices log spectral cepstral lda feature set measured sietill database 

ordering compactness feature set order come feature set small possible feature transformations able concentrate relevant information classification features 
addition order transformed features class separability resulting easy reduction feature set simply leaving ones list 
measurement class separability measure originally introduced criterion computation lda trace criterion tr tr sw denotes class sb class scatter matrix 
matrix product sw sb stands ratio variance variance inside classes 
higher ratio better class separation 
convert matrix product scalar necessary defined optimization problem trace equals sum eigenvalues sw sb 
elements main diagonal measure contribution single feature class separation 
illustrates measure different feature sets described sietill database 
separability cepstrum lda feature cumulative separability cepstrum lda feature class separation individual features log spectral cepstral lda feature sets measured sietill database 
left picture demonstrates ordering feature set achieved lda lesser extent cepstrum transformation 
separability measures cumulated right picture illustrating fact lda able concentrate separability features cepstrum worse log spectrum 
conclude lda able allow recognition fewer features cepstrum 
order features separability making feature reduction easy 

invariance linear transforms lda transformation matrix invariant linear transforms 
features result matter input cepstral vector obtained linear transformation log spectral vector original log spectral vector 
verified property holds approximately presence feature vector augmented time differences resulting word error rate similar log spectral cepstral input vector lda vectors augmented time differences respective domains applying lda transform 

automatic learning dynamical features time derivatives generally included feature set order measure spectrum variations directly 
lda able incorporate frames transform optimally feature vector argued lda able compute derivatives implicitly invent better measures variations spectrum 
compared vector static features explicit inclusion time derivatives 
turned perform slightly better 

data dependence robustness channel mismatch contrast cepstrum lda transformation matrix 
reported experiments sensitivity lda snr mismatches training test 
carried cross test experiments transformation matrix determined acoustic environments different testing data 
performance lda transformed features dropped cepstrum features multiple input frames 

experimental results 
structure recognizer tests described filtered sampled speech signal khz 
subsequent point fft computed ms hamming windowed ms portion speech signal 
power spectral density coefficients determined convolution power density spectrum triangular kernels nonlinear frequency spacing 
logarithm computed resulting coefficients basis different feature vectors log spectral average coefficients subtracted channel included additional component 
features order time derivatives computed subtracting features frames past appended feature vector resulting features 
cepstrum component log spectral feature vector transformed discrete cosine transform augmented time derivatives described 
cepstral coefficient contains average log spectral channels come features 
lda component log spectral vector transformed lda matrix computed existing segmentation log spectral training hmm states class definition 
connected word recognition algorithm hidden markov models emission probabilities modeled continuous laplacian densities approx 
state single standard deviation vector pooled states models 
transition probabilities state state trained fixed priori values non zero loop skip forward transitions 
employ viterbi approximation training recognition probability word replaced probability state sequence 
details acoustic modeling 

speech corpus conducted experiments sietill digits corpus 
corpus contains german digit strings recorded telephone lines 
male female speakers uttered digit digit variable length strings 
male female speakers arbitrarily chosen training corpus rest recognition corpus 
sietill digits characterized large variety line speaker characteristics 
word vocabulary models separate model word gender single background noise model 

decorrelation acoustic modeling approximation assuming decorrelated feature vector components 
suppose feature set exhibiting low correlation lead better performance 
order verify conjecture tested feature vector types described full length 
input information discarded reducing number features differences error rate solely stem different amount decorrelation features table 
corpus log spectrum cepstrum lda sietill table word error rates full length vectors 
results show decorrelation achieved cepstral transformation sufficient exhibit significant decrease error rate lda leads improvement rel 
disappointing performance cepstrum may due inclusion time derivatives cepstrum transform lda cepstrum transform able remove correlations features derivatives 

ordering compactness feature set order test ordering property compactness different feature sets ran recognizer different numbers features 
selected features chosen separability measure described including time derivatives 
cepstrum lda simply remove features log spectral features required choices order time derivatives channels features 
results summarized table features log spectral cepstrum lda table word error rates different numbers features 
cepstral features perform better log spectral features number features 
lda reduces number 
addition look performance features cepstrum rel 
better log spectrum features lda rel 
better cepstrum 

invariance linear transforms chapter 
stated features produced lda nearly independent feature space input lda time derivatives included 
order verify different input vectors lda component log spectral vector component cepstral vector described 
output consisted best lda features 
results table 
corpus log spectral input cepstral input sietill table word error rates different inputs lda small increase rel 
cepstral input 
may due different number input features lda 

automatic learning dynamical features adjoined frames time derivatives single vector input lda transform feature vector length 
comparison adjoined frames time derivatives vector input lda 
reduction comes fact derivatives computed subtracting frame actual incorporate larger time period 
meaningful comparison keep number input frames lda seeing cases 
results shown table 
frames derivatives frames derivatives rel 
change table word error rates multiple frames input lda 
input frames lda able compute time derivatives implicitly better normally done explicit method 
input frames inclusion time derivatives input performs better 

data dependence robustness channel mismatch order test data dependency lda computed lda transformation matrix different corpus 
vocabulary sietill recorded car hand set mounted microphone 
different speakers uttered approx 
sentences containing digit strings 
effect computed lda acoustic modeling data recorded completely different channel different snr 
computed lda frame frames derivatives car database 
resulting lda transformation matrix carried training recognition runs sietill data best features 
results including cepstrum lda trained environment comparison table 
results confirm hypothesis lda transformation heavily depends data computing transformation matrix 
include frames derivatives input performs worse simple cepstral feature vector 
cepstrum lda environment frame frames lda cross environment frame frames table word error rates different lda transformation matrix 

properties widely linear transforms log spectral feature vectors cepstrum linear discriminant analysis lda studied view approximations due implementation real life speech recognizer 
shown lda delivers compact informative features yielding better recognition accuracy shorter feature vector 
big drawback lda data dependence sensitive changes acoustic environment 


davis comparison parametric representations monosyllabic word recognition continuously spoken sentences ieee trans 
assp 

hunt lef comparison acoustic representations speech recognition degraded speech proc 
icassp glasgow uk may pp 

ney linear discriminant analysis improved large vocabulary continuous speech recognition proc 
icassp san francisco ca march pp 

fukunaga statistical pattern recognition academic press boston 

welling ney experiments linear feature extraction speech recognition proc 
eurospeech madrid spain sept pp 

robustness linear discriminant analysis preprocessing step noisy speech recognition proc 
icassp detroit may pp 

aubert ney continuous mixture densities linear discriminant analysis improved context dependent acoustic models proc 
icassp minneapolis mn vol 
ii pp 
