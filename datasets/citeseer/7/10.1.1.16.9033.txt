high performance systems build collections digital library donna cornell digital library research group upson hall ithaca ny cs cornell edu distributed web content spread thousands servers 
high performance hardware software essential ective download analysis organization content 
describe experience highly parallel web crawling system mercator construct automatically collections scientific resources national science digital library 

distributed messy digital library world web 
wide variety online resources represented url 
better descriptions web library lempel moran www rapidly expanding hyperlinked collection unstructured information :10.1.1.167.4587
ability extract informational nuggets web focused collections highly desirable 
knowledge systems extract personalized content web hand involved national ort create largest line library science mathematics engineering technology 
collections url exist subject areas hand crafted 
approach scale 
order control costs allow longterm success growth digital libraries rely increasingly automated procedures including ways find build collections 
bulk collection methods provide route provide collections vast number topic areas 
www com web nsf bulk collection takes roles 
preservation goal bulk collection copy internet archive 
collection building tries look promising keep selected subset 
draft submission april reports automatic collection generation massively parallel web crawling 
prime example application distributed technologies high performance computing digital library arena 
believe unique search engines process query time exploit parallelism process number topic areas simultaneously ciently building collections collections 
process su cient results approach appear feasible reportable 
section describes web crawler research 
section describes approach collection building 
sections explain parallel tradeo 
concludes sections 
description refer 
description reviews projects building topic specific collections see 

mercator automatically build collections topic related web documents searching clustering start web crawler 
crawlers tools web research find crawler essential mining web :10.1.1.13.4762
starting august access mercator fast web crawler located palo alto ca 
developed systems research center digital developed compaq 
written java mercator extensible perfect vehicle research bulk collection building 
adapted shows conceptually mercator works 
mercator special personality 
simply fast scalable distributed 
job download pages run analyzers downloaded page optionally enqueue child links downloading 
loop shown executed continuously typically hundreds threads belong single instantiation java crawl object henceforth called crawl 
primary data object crawl url frontier shared threads 
researcher opportunities add flavor crawl 
opportunity analysis stage url frontier get ip address get robots txt urls urls url filters dequeue enqueue fetch allowed 
page fingerprint 
page seen 
analyze page optional final analyzer link extractor download page system block diagram mercator web crawler 
main components url frontier dns resolver downloader document link extractor 
sequence analyzers 
applied downloaded page 
extension mercator base analyzer class consists constructor called crawl process method called downloaded page 
process method gets content page information page url 
analyzers apply order specified configuration file 
final analyzer link extractor supplied mercator extracts links downloaded page sends url filtering stage 
point mentioned extracted child links encapsulated object 
page processing user specified information added 
url filters 
filters simply encoded configuration file discard urls particular domains 
remaining urls urls urls passed object chance add parent information desired pick download priority 
goes frontier object adds proper queue 
user override frontier naming custom classes configuration file 
machine runs mercator accessed version 
done remotely cornell 
noted analyzer link extractor mercator download pages second running threads 
noted parallel crawlers days pages second benchmark distributed crawler built brooklyn polytechnic described claims achieve level performance :10.1.1.13.4762
general programming additions extending basic mercator objects altering mercator code 
testimony clean design construction java implementation mercator 
engine search query crawl web topic centroid virtual collection topic new collection virtual collection collections synthesize hierarchical topic list 

putting online collections topic specific educational material usually done hand 
scale rate web growing exponentially 
question explored group suppose powerful crawler running fast computer high bandwidth connection internet backbone far get building automated digital library 
define collection set html pdf postscript pages topic 
collection built crawling long desirable find items collection topic area 
explains di erent approach building collections 
start collections list topics build collections 
centroid constructed virtual collection centroid construction explained 
crawl synthesize collections downloaded documents placing document centroid closest 
items collection represented things url merit indicating degree item belongs collection 
quality collection depends initial input topic hierarchy centroids metric assign downloaded documents collections 
ciency collection building depends able focused crawl 
search engines useful finding page related topic authoritative page hub useful building collections crawl entire web subjects cover relatively little specific area science mathematics :10.1.1.120.3875
decided new technology called focused crawling find sets urls documents related specific subjects 
implement focused crawl content analysis link analysis 
html pages downloaded words extracted build weighted term vector matched cosine correlation term vectors centroids representing topic areas 
document tentatively classed nearest subject vector correlation degree document considered collection 
correlation su ciently high links rank relevance vs rank original google comparison google focused crawl topic query 
html page followed 
link pdf ps file added collection parental correlation values 
similar focused crawling projects typically start core collections hand selected documents centroid built 
start topic hierarchy subject index leverage google return documents subject construct centroid details section 
logic easily coded mercator extending base classes adjusting parameters run configuration file 
get idea cosine correlation predicting document relevance topic selected collections closer inspection 
document collection visually checked determine page belonged 
relevance judgements hand possible measure precision collection dividing number relevant documents size collection 
helpful plot number relevant documents versus document rank rank decreasing correlation order 
collections built top ranking documents 
shows precision collection documents plane geometry collections resulted short focused crawl 
precision levels 
making collection smaller keeping highest ranking documents raises precision slightly 
comparison google search engine achieved search results plane geometry subject descriptor search query falls results continues decline 
means large crawl minutes correlation level corr performance term vs term centroids documents downloaded term centroids term centroids precision results random collection term centroid vs term centroid 
topic plane geometry 
topics mathematics 
increasing duration crawl tends increase precision levels 
general success implementation focused crawling depends heavily initial subject descriptors 
index term list worked developing math subject descriptors curriculum outline st nd grade science failed contained broad general terms 
experiment building collections topics foreign law similarly disappointing 
appears automatic approach best suited technical fields fairly precise topic taxonomies 

performance tuning mercator performance web crawler finely tuned 
long analysis time bottleneck crawl web maximum speed 
factor long takes analyze downloaded document length number centroids document compared 
fixed number centroids shorter centroids processed faster longer ones 
precision su er fewer terms centroid 
concerns experiment see 
fact really longer centroids tended bring irrelevant topics 
top highly weighted terms kept making centroid doubling length means important terms included 
improve short query running short crawl updating idf values dictionary actual word frequencies observed large number documents 
example www org 
idf inverse document frequency measures importance term distinguishing document 
common terms low idf 
quantum information cryptography communication computation term doc term doc quantum quantum information computation cryptography photons physics states computation information research bob new cryptography state photon group entangled photons classical classical physics oxford set theory experimental phys alice states course teleportation phys processing entanglement computing communication rev experimental bit centroid collection nanotechnology weights determined information original centroid input reweighted recomputed idf values 
shows topic vector collection quantum information initially assigned weights shows topic vector weights recomputed include revised idf 
documents inspected recompute idf values dictionary turn recompute weights centroids 
particular centroid recomputation weights selecting top terms subject 
importantly general words information new downgraded 
way minimize computation crawl minimize size dictionary word lookup faster 
construct dictionary union centroids quite short words 
furthermore shorter centroids smaller dictionary tends 
approach minimizing size dictionary unusual information retrieval applications large dictionaries get maximum coverage 
building collections limited size recall issue 
need focus documents terms common centroids 
recomputing idf values dictionary increases precision crawl 
results recomputed weights 

building collections parallel inherently parallel crawler mercator number opportunities exploit parallelism building collections 
course crawl done parallel aspects collection building parallelizable 
seeding collections seeding collections done parallel takes constant time 
query constructed topic concatenating hierarchical topic terms 
example numerical analysis computational linear algebra 
parallel queries rank effect recomputing idf values original recomputed precision results idf weights dictionary updated 
search engines retrieving top hits topic 
quite small range 
query words pages search engine combined centroid ik done parallel specifically parallel extract terms list ik collection weight construct term vector top terms 
weights term frequency hits specific query central bottleneck computing term weights centroid 
crawling collections seeded crawl proceed parallel distributed shared memory 
advantage shared memory shared disk able shared frontier shared urls seen shared documents seen 
access data allowed equally processes running parallel access synchronized 
critical single web server handled single crawler process avoid multiple simultaneous hits server 
process independently update part frontier 
crawling parallel building collections big ciency win 
bulk elapsed time spent waiting document download 
downloads done parallel process get blocked 
collection building crawl continues parallel days weeks collections grow 
idea pick collection size accumulate best items collection 
time allowing collections overlap keep data item indicates closest collections item belong 
writing crawl data disk separate process leisure construct collections keeping highest scoring items 
process called collection 
gives intuitive feeling rate collections assembled 
main reason continuing crawl days increase minimum correlation 
average cluster value highest score grows quickly documents levels 
collection description collections described composing updating html page collection way parallel process running asynchronously collection collector collection data url url urls parents 
ps generate html page shown 
heading constructed collection topic 
series items separated horizontal lines item consists heading taken element parent context list contains url text surrounding url parent url labeled anchor text parent 
items collection parent grouped parent heading 
collection pages go digital library 
continuous parallel crawls plus post processing pages posted daily basis 
crawl proceeds occurrence new highly scoring documents occur low probability users alerted 
generate dublin core metadata item collection basically page scraping operation remains done 

plans continuing collection synthesis include making runs determine relative impact starting crawl various locations web generating dublin core metadata describe collections 
major goal sure high correlation means item highly useful addition collection 
way relation correlation metrics actual relevance improved penalize documents appear unsuitable digital library collections resumes calls papers 
accomplished machine learning recognize certain page types course descriptions lower correlation score 
optimal combinations threshold cuto increase ciency crawl 
question far look give crawl direction tunnelling currently researched described forthcoming 
crawl data consists information document correlation centroids url correlation nearest centroid id centroid 
number documents downloaded min avg max correlations math collections average maximum average average average minimum collection grows new documents downloaded 
axis minimum correlation averaged collections assembled 
short crawl necessary build item collections 
collections reach item limit pages downloaded 
top line graph maximum correlation collection item correlation centroid averaged collections 
maximum downloaded documents 
extending crawl useful raising minimum average correlations 
example description page generated collection items automatically 

research proceed di erent directions point done shows existing distributed computing technologies digital library arena 
described application distributed parallel crawlers building digital library content automatically 
level quality hand created collections aggregations built massive scale 
tunable approach collection synthesis extensible massively parallel crawler disposal quality collections improve particularly areas math science 

funded part nsf project prism iis 
go bill arms suggesting particular research focus automated collection building carl lagoze providing funds support project 
acknowledge considerable technical help received systems research center compaq especially raymie stata marc najork richard 

arms 
automated digital libraries ectively computers skill tasks professional librarianship 
lib magazine magazine digital library research july 

collection synthesis 
proceedings second acm ieee cs joint conference digital libraries portland 
burner 
crawling eternity 

chakrabarti van den berg dom 
focused crawling new approach topic specific web resource discovery 
proceedings eighth international world wide web conference pages toronto canada may 
heydon najork 
mercator scalable extensible web crawler 
world wide web dec 
kleinberg :10.1.1.120.3875
authoritative sources hyperlinked environment 
journal acm 
kleinberg kumar raghavan tomkins 
web graph measurements models methods 
asano imai lee nakano editors computing combinatorics th annual international conference cocoon tokyo japan july 
springer verlag lncs pp 


compiling document collections internet 
sigir forum fall 
available www acm org sigir forum pdf 
lagoze ed arms gan ingram kra 
core services architecture national digital library science education 
proceedings second acm ieee cs joint conference digital libraries portland 
lempel moran 
salsa stochastic approach link structure analysis 
acm transactions information systems apr 
najork heydon 
high performance web crawling 
technical report research report compaq src sept 
salton 
automatic information organization retrieval 
mcgraw hill new york 
shivakumar garcia molina 
finding near replicas documents web 
webdb international workshop world wide web databases valencia spain mar 
lncs 
shkapenyuk suel :10.1.1.13.4762
design implementation high performance distributed web crawler 
proceedings th international conference data engineering icde san jose ca feb march pages 
