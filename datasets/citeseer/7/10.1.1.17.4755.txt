additive models boosting inference generalized divergences john lafferty school computer science carnegie mellon university pittsburgh pa lafferty cs cmu edu framework designing incremental learning algorithms derived generalized entropy functionals 
approach bregman divergences associated class additive models constructed legendre transform 
particular parameter family bregman divergences shown yield family loss functions includes log likelihood criterion logistic regression special case closely approximates exponential loss criterion adaboost algorithms schapire natural parameter family varies 
show quadratic approximation gain bregman divergence results weighted squares criterion 
leads family incremental learning algorithms builds extends interpretation boosting terms additive models proposed friedman hastie tibshirani 
logistic regression widely statistical methodology classification problems maximum likelihood 
boosting popular technique combining simple weak learners accurate classifier voting scheme learners votes chosen minimize exponential loss criterion 
friedman hastie tibshirani presents statistical interpretation adaboost algorithms freund schapire schapire singer :10.1.1.30.3515
particular friedman show boosting algorithms result building additive models newton updates exponential loss function making comparison boosting stepwise logistic regression methods formalized logitboost algorithm 
perspective main summarized graphically :10.1.1.30.3515
predicting random variable gamma class case adaboost logitboost algorithms fit model form gammaf 
adaboost algorithms exponential loss function theta gammayf logistic regression uses log likelihood criterion theta log gamma gamma yf delta delta denotes expectation respect empirical sample 
plot loss functions function yf shown error mean squared error 
appear proceedings twelfth conference computational learning theory research supported part nsf ccr nsf iis ibm university partnership award 
note plot corresponding shown log gamma gammayf delta log likelihood log gamma gamma yf delta log likelihood loss resulting loglikelihood curve falling exponential criterion error region gamma yf lies exponential criterion region plot 
additive models differ constant multiplicative factor plot compares loss functions additive model specified log gamma curves shown region gamma yf appropriate domain confidence rated predictions probabilities 
course exponential curve lies log likelihood sufficiently large negative values yf 
yf exponential log likelihood squared error misclassification 
loss functions additive models exponential criterion log likelihood squared error 
exponential log likelihood loss functions similar properties significant gap region gamma yf lead qualitatively quantitatively different behavior real data 
experiments carried demonstrate logitboost adaboost algorithms yield significantly different models 
main result current show gap bridged unified framework statistical inference 
particular show family incremental learning algorithms derived bregman divergences constructed include stepwise logistic regression logitboost algorithm special case closely approximate adaboost algorithm natural parameter family varies 
appropriately chosen divergences family expect resulting learning algorithms adaboost indistinguishable practice 
show learning algorithms interpretation terms weighted squares regression sheds additional light adaboost differs standard likelihood approaches 
informally bregman divergence oe convex function oe measures convexity oe points relative linear approximation bregman divergences include kullback leibler divergence special case rich theory associated convex duality 
particular fundamental results information geometry maximum entropy method generalize bregman divergences 
machine learning literature warmuth colleagues means obtaining loss bounds broad class line learning algorithms 
indicate bounds techniques obtain maximum likelihood estimation kullback leibler divergence exponential families analogues general bregman divergences 
statistical inference techniques bregman divergences attractive reasons 
framework strong axiomatic justification csisz ar 
applies inference probability distributions inference unnormalized measures useful applications image processing 
enables principled treatment missing hidden data 
mathematical framework bregman divergences draws geometry convex duality familiar theory exponential families method alternating projections exploited useful establishing convergence properties 
historical note stepwise learning algorithms maximum entropy framework extension bregman divergence minimization began early ibm watson research center 
published research unpublished briefly described ibm invention disclosures carried knowledge boosting style classification procedures machine learning literature :10.1.1.43.7345
resulting feature selection algorithms exponential models developed bear strong resemblance adaboost logitboost important differences 
relevant issues mentioned briefly discussion concludes extended 
section show gain log likelihood logistic regression approximated quadratic order summarizing results friedman 
section introduces notion bregman divergence section shows legendre transform bregman divergence defines family additive models 
section discusses special case parameter family divergences shown csisz ar axiomatic justification loss functions family closely approximate exponential criterion 
sections show estimate quadratic order gain due adding single feature leading family boosting style algorithms helps bridge gap logistic regression adaboost algorithms 
section concludes brief discussion directions 

approximate gains section briefly discuss relevant results introducing notation perspective 
random variable denote class label predicted feature vector represented random variable consider exponential models form deltaf deltaf delta 
refer functions features understanding may compound features built elementary features decision trees instance 
denote empirical distribution determined collection training examples ffi ffi 
jj denote average kullback leibler divergence conditional distributions respect jj log notation denote expectation respect particular denotes expectation respect denotes conditional expectation expectation respect ep 
ep denote log likelihood ep log 
model general need additive model feature 
define gain feature respect largest improvement likelihood result adding def max ep gamma ep max ep jj gamma ep jj 
normalizing term 
result shows change log likelihood ep gamma ep simple interpretation approximated quadratic order derive analogous result bregman divergences section 
proposition 
quadratic order gain approximated def gamma ep gamma proof 
log gamma log log ep gamma ep ep gamma log 
calculating taylor expansion log see log gamma gamma delta result ep gamma ep ep gamma gamma theta gamma ep gamma gamma theta gamma maximized ep gamma gamma choice ep gamma ep ep gamma gamma yielding statement 
approximation reformulated large deviations result estimating change likelihood due adding feature chi square interpretation gain 
facts known statistics literature learned stephen vincent della pietra 
chi square interpretation gives useful significance test feature selection 
simple proofs statements appendix 
corollary 
parameter family exponential models non conjugate prior distribution parameter gaussian mean zero variance oe 
log probability events log log log gamma ng log corollary 
fixing consider empirical distribution sample size drawn 
quadratic approximation distributed ep fl fl theta gamma gamma chi squared distribution degree freedom 
important special case class problem gamma 
consider restriction binary features gammayf gamma 
situation analysis particularly simple intuitive 
notation set gamma gamma representation indicator variable 
express quadratic approximation gain terms variance gamma ep ep gamma ep gamma relaxing restriction binary features minimizing pointwise yields gamma gamma equivalent weighted squares criterion 
analysis leads directly logitboost algorithm 
similar analysis leads discrete adaboost algorithm freund schapire 
case consider unnormalized models form measure 
objective function judge additive model exponential criterion ep assume features binary form gammayf gamma 
ep gamma ep gamma gammayf yf gamma maximizing quadratic approximation function find yf gain exponential criterion estimated ep max ep gamma ep yf choosing pointwise maximize approximation leads choice arg max yf arg min gamma facts 
boosting style algorithms selected regression technique decision trees weight chosen maximize full gain ep gamma ep quadratic approximation leading log ffi yf gamma ffi yf specifies discrete adaboost algorithm 
sections show analysis extended broad class loss functions allowing reconcile adaboost logitboost algorithms 

generalized divergences oe gamma 
strictly convex function bregman divergence oe discrete measures defined oe oe gamma oe gamma oe gamma oe obtain mean squared error gamma oe log obtain divergence extended kullback leibler divergence log log gamma generally phi gamma 
strictly convex bregman divergence phi defined phi phi gamma phi gamma phi delta gamma 
kullback leibler divergence models exponential family interpreted bregman divergence defined parameter vector 
exponential model form gamma deltaf divergence jj equal bregman divergence oe gamma log jj gamma delta gamma log gamma log tutorial generalized entropy measures refer 
elementary proof basic duality theorem pythagorean property bregman divergences proof case divergence :10.1.1.43.7345
practical algorithm minimizing bregman divergences subject linear constraints solving generalized maximum entropy problems 
framework incremental stepwise learning divergences 

legendre transforms additive models basic tool analysis legendre transform 
transform defines class additive models bregman divergence 
provides link primal dual problems central convex optimization divergences 
transform defined respect set ae collection probability distributions events denoted delta set positive measures denoted proceeding need precise term additive model 
definition 
ae set measures 
additive model defined action fl theta gamma 
satisfying homomorphism property fl fl fl 
lemma 
convex function phi gamma 
phi bregman divergence defined measures ae define legendre transform ffi phi ffi phi arg max delta gamma phi map 
ffi phi defines additive model delta help clarify notation point probability distribution dot product delta vector expectation viewing random variable proof lemma 
calculus variations argument characterize ffi phi suppose collection measures 
parameter family measures ffi phi dp dt fi fi fi ffip 
definition legendre transform delta ffip gamma phi ffi phi delta ffip phi delta ffip derivative ffip arbitrary see distribution ffi phi uniquely determined condition phi ffi phi phi follows ffi phi ffi phi phi ffi phi phi phi ffi phi showing ffi phi ffi phi ffi phi 
delta collection probability distributions need carry constrained maximization similar calculation apply introducing lagrange multiplier constraint delta denotes vector see equation satisfied maximum delta ffip gamma delta ffip gamma phi ffi phi delta ffip phi delta ffip showing ffi phi uniquely determined condition phi ffi phi phi gamma 
prove homomorphism property suffices show ffi phi 
purpose distinguish constrained unconstrained problems temporarily denoting unconstrained action fi phi constrained action determines probability distribution ffi phi simplify notation denoted unique constant vector gamma fi phi probability distribution 
homomorphism property unconstrained problem proved ffi phi gamma fi phi gamma gamma gamma fi phi gamma gamma fi phi gamma fi phi gamma gamma fi phi ffi phi result ffi phi gamma proving homomorphism property constrained problem 
normalizing term referred cumulant generating function consistent standard terminology exponential families 
focus particular collection bregman divergences 
collection seen easy computationally leading learning algorithms practical logistic regression boosting 
yf exponential log likelihood squared error misclassification bregman 
loss functions log likelihood exponential criterion bregman divergences ff values ff interval chosen increments deltaff 
curves generated numerically estimating cumulant generating functions newton method 
oe ff family convex functions defined ff oe ff gamma log gamma ff log gamma ff ff gammaff gammax ff ff gamma ff ff 
associated family bregman divergences ff discrete distributions ff ff gammaff ff gamma ff ff ff gamma gamma ff specializes itakura saito distortion log gamma ff yields extended kullback divergence log gamma fact lim fi fi gamma fi log simple verify constitutes continuous family bregman divergences itakura saito distortion divergence 
family strong axiomatic justification csisz ar scope current review axiomatic formulation 
lagrange multipliers simple calculation determine legendre transform ffi ff associated bregman divergence ff acting set probability distributions delta determines family additive models 
proposition 
legendre transform ffi ff ffi ff gamma ff gamma ff gamma gamma delta ff gamma ff 
ff ffi gamma gamma ffi gamma consider conditional models class case gamma gammayf boosting logistic regression models discussed earlier 
interested behavior gain ff gamma ff ffi ff ff varies 
fixing want consider loss varies function 
ff know scaled curve log gamma gamma yf delta plotted 
ff compute normalization explicitly easily computed numerically newton method 
resulting family loss functions plotted values ff 
clear plot bregman divergences closely approximate exponential criterion near decision boundary ff 
duality information geometry elementary proof fundamental duality maximum likelihood minimum divergence exponential models :10.1.1.43.7345
simple proof extends bregman divergences quite easily 
fundamental fact allows view learning problem different ways 
initial measure 
define fp delta fq delta ffi phi rg denote closure euclidean topology 
theorem 
suppose phi 
exits unique delta satisfying phi phi phi arg min phi arg min phi 
properties determines uniquely 
proof duality relies computational lemma bregman divergences 
lemma derive quadratic approximation gain 
lemma 
fix consider parameter family models tf ffi ff cumulant generating functions 
dt fi fi gamma phi delta gamma phi delta gamma phi dt fi fi phi delta gamma dt fi fi fi phi gamma gamma phi gamma phi hessian phi 
proof 
lemma phi dt fi fi fi fi tf ffi ff dt fi fi fi fi phi tf ffi ff dt fi fi fi fi phi tf gamma gamma equality follows dt delta gamma dt delta delta 
derive second equality note dt phi dt phi gamma phi gamma phi delta gamma gamma dt phi delta gamma gamma delta gamma delta gamma equality consequence delta delta 
calculation dt fi fi fi fi phi dt fi fi fi fi gamma delta gamma gamma delta dt fi fi fi fi gamma gamma phi gamma second equality follows property 
remainder proof theorem follows steps proof kullback leibler divergence :10.1.1.43.7345
special case duality reinterpret explanation boosting adjusting weight current weak learner uncorrelated labels 
th round adaboost adds feature weight ff additive model probability assigned th sample ff weight ff chosen minimize exponential loss criterion ff 
notation yf distribution satisfying assigns nonzero probability training samples 
duality min jj min ff jj min log ff relationship developed 

weighted regression lemma previous section allows approximate loss bregman divergence due adding feature additive model quadratic approximation extends analysis section 
assume distributions normalized similar results apply positive measures necessarily probability distributions 
proposition 
bregman divergence phi quadratic approximation gain max phi gamma phi ffi phi def gamma gamma gamma phi gamma proof 
lemma second order phi gamma phi ffi phi gamma gamma gamma phi gamma maximizing respect yields statement proposition 
quadratic approximation gives direct correspondence results note practical alternative feature selection compute full gain generalized iterative scaling algorithm 
computationally demanding gives accurate assessment value feature 
duality theorem interpretation terms constrained optimization 
bregman csisz ar divergences ff oe ff gamma gamma oe ff diagonal matrix diagonal entries gammaff applying proposition class case simple calculation shows newton updates select features result weighted squares criterion arg min ff gamma ff gamma weights ff gammaff gamma gammaff gammaff gamma gammaff ff results weights gamma logitboost algorithm 
ff decreases weights place relatively emphasis events current model certain 
subset family weights ff shown 
discussion statistical framework building incremental stepwise classification algorithms generalized divergences 
approach bregman divergences similarity measure probability distributions uses legendre transform define class additive models 
bregman csisz ar divergences ff shown yield parameter family loss functions includes log likelihood loss logistic regression special case closely approximates exponential loss criterion adaboost algorithms 
considered gain due adding single feature additive model showed gain approximated second order 
leads class weighted stepwise regression procedures includes logitboost algorithm friedman hastie tibshirani special case 
class bregman divergences enjoys number useful qualities addition machine learning literature may applications 
exploited warmuth della pietra similarity measures convexity properties allow bounds auxiliary functions easily derived 
interpretation terms generalized maximum entropy principle projection operators defined bregman divergences useful proving convergence various learning algorithms constrained optimization procedures 

weights weighted squares regression derived ar divergences values ff 
practical empirical standpoint believe ways methods design effective practical feature selection algorithms 
worked extensively closely related set techniques incrementally adding features maximum entropy model applied text segmentation problem 
evaluated real adaboost algorithm level decision trees weak learners logistic regression maximum entropy methods result superior performance 
particular difference stepwise techniques logitboost adaboost algorithms new feature included additive model model feature weights generalized iterative scaling algorithm 
better accounts correlations features expect result models prone overfitting corresponding algorithms freeze feature weight included model 
practice observed approach extremely robust overfitting 
hope gain better understanding behavior research experimentation 
main result current show bregman divergences generalize complement standard statistical methods stepwise logistic regression 
observations build correspondence boosting logistic regression established experience incremental maximum entropy minimum divergence methods range practical problems 
empirical success boosting algorithms calls better understanding properties believe statistical perspective complements boosting roots error bounds pac model learning offers advantages 
classical techniques logistic regression fully suffice explain boosting argued bregman divergences help close gap 
statistical information theoretic perspective may allow development new variations voting style learning algorithms may effective practice 
appendix statistics gain appendix give proofs corollaries proposition describe statistics quadratic approximation gain due adding single feature small weight additive model 
similar results derived general bregman divergences 
proposition 
quadratic order gain approximated def gamma gamma corollary 
parameter family exponential models non conjugate prior distribution parameter gaussian mean zero variance oe 
log probability events log log log gamma ng log proof 
definitions nl approximation proposition approximately gammaa gamma gamma nq gamma 
carrying integration find oe oe gammaa gamma oe oe gamma gamma exp gamma oe gamma logarithms log log gamma ng oe gamma log showing leading orders oe 
corollary 
fixing consider empirical distribution sample size drawn distribution 
quadratic approximation distributed fl fl theta gamma gamma chi squared distribution degree freedom 
proof 
value th sample point gamma gamma fl independent identically distributed mean zero variance large author manfred warmuth interesting discussions bregman divergences fernando pereira inquiring years ago relationship multiplicative update algorithms incremental maximum entropy method 
stephen vincent della pietra essential contributions 
barndorff nielsen information exponential families statistical theory wiley new york 
beeferman berger lafferty statistical models text segmentation machine learning special issue natural language learning cardie mooney eds 
berger della pietra della pietra maximum entropy approach natural language processing computational linguistics pp 

brown fundamentals statistical exponential families institute mathematical statistics lecture notes monograph series volume hayward california 
bregman relaxation method find common point convex sets applications solution problems convex programming ussr computational mathematics mathematical physics pp 

breiman friedman olshen stone classification regression trees wadsworth belmont ma 
carbonell yang lafferty brown pierce liu cmu report tdt segmentation detection tracking proceedings darpa broadcast news conference appear 
censor lent iterative row action method interval convex programming optim 
theory appl 
pp 

csisz ar squares maximum entropy 
axiomatic approach inference linear inverse problems ann 
statist pp 

csisz ar generalized projections non negative functions acta math 
pp 

csisz ar maxent mathematics information theory maximum entropy bayesian methods hanson silver eds kluwer academic publishers 
della pietra della pietra lafferty inducing features random fields ieee trans :10.1.1.43.7345
pattern analysis machine intell april pp 

della pietra della pietra lafferty bregman distances iterative scaling auxiliary functions unpublished manuscript 
della pietra della pietra lafferty statistical learning algorithms bregman distances proceedings canadian workshop information theory toronto 
freund schapire experiments new boosting algorithm machine learning proceedings thirteenth international conference pp 

friedman hastie tibshirani additive logistic regression statistical view boosting technical report department statistics stanford university august 
herbster warmuth tracking best regressor proceedings eleventh annual conference computational learning theory madison wi 
jaynes papers probability statistics statistical physics rosenkrantz ed reidel publishing dordrecht holland 
kivinen warmuth additive versus exponentiated gradient updates linear prediction information computation pp 

kivinen warmuth boosting entropy projection proceedings twelfth annual conference computational learning theory santa cruz ca 
rockafellar convex analysis princeton university press princeton nj 
schapire strength weak learnability machine learning pp 

schapire singer improved boosting algorithms confidence rated predictions proceedings eleventh annual conference computational learning theory madison wi 

