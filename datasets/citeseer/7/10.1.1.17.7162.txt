relieving hot spots world wide web rina panigrahy tech computer science iit submitted department electrical engineering computer science partial ful llment requirements degree master science massachusetts institute technology june massachusetts institute technology 
rights reserved 
author department electrical engineering computer science may certi ed david karger professor laboratory computer science thesis supervisor accepted arthur smith chairman department committee graduate students relieving hot spots world wide web rina panigrahy submitted department electrical engineering computer science may partial ful llment requirements degree master science describe family caching protocols distributed networks decrease eliminate occurrence hot spots network 
hot spots web sites swamped large number requests pages 
protocols particularly designed large networks internet delays caused hot spots severe feasible server complete information current state entire network 
protocols easy implement existing network protocols tcp ip require little overhead 
protocols local control cient existing resources scale gracefully network grows 
caching protocols special kind hashing call consistent hashing 
roughly speaking consistent hash function changes minimally range function changes 
development consistent hash functions able develop caching protocols require users current consistent view network 
believe consistent hash functions may eventually prove useful applications distributed name servers quorum systems 
thesis supervisor david karger title professor laboratory computer science tom leighton david karger eric lehman matthew levine daniel lewin help cooperation 
contents problem related practical world wide web proxies operative caching harvest push caching prefetching cache consistency related theoretical randomization hashing finding minimum time broadcast trees prefetching algorithms contribution presentation model random trees protocol analysis latency swamping improvement protocol better bounds storage consistent hashing de nitions construction implementation analysis basic solution inconsistent world swamping storage ultrametric distances protocol analysis swamping storage fault tolerance chapter problem thesis study problem hot spots world wide web 
hot spots occur time large number clients wish simultaneously access data single server 
site provisioned deal clients simultaneously service may degraded lost 
experienced hot spot phenomenon context web 
web site suddenly extremely popular receive far requests relatively short time originally con gured handle 
fact site may receive requests swamped typically renders unusable 
hard predict sudden changes popularity 
making site inaccessible heavy tra destined location network near interfering tra nearby sites 
web increased occurrence impact hot spots 
famous examples hot spots web include jpl site levy comet struck jupiter ibm site deep blue kasparov chess tournament political sites night election 
cases users denied access site hours days 
examples include sites identi ed web site day sites release new versions popular software 
describe family distributed caching protocol decrease eliminate occurence hot spots web 
protocols randomization ensure load serving requests balanced caches 
number cached copies page adapts dynamically changing popularity 
protocols local control cient existing resources scale gracefully network grows 
caching protocols special kind hashing call consistent hashing 
roughly speaking consistent hash function changes minimally range function changes 
development consistent hash functions able develop caching protocols require users current consistent view network 
allows new caches added network having know latest set caches 
giving tools discuss past practical theoretical 
section gives detailed discussion practical 
thesis ignore issue updates pages protocols developed static web pages 
dynamic pages give rise issues maintaining cache consistency totally ignored 
believe previous done cache consistency directly protocols 
section discuss existing methods maintaining cache consistency 
section discusses theoretical 
related practical approaches overcoming problem hot spots proposed 
kind replication strategy store copies hot pages internet spreads serving hot page servers 
approach wide clients share proxy cache 
proxy caches long reduce web tra network 
tries satisfy requests cached copy failing forwards request home server 
dilemma scheme bene users share cache cache liable get swamped 
problem making group caches function 
page request served locally cached caches 
disadvantage approach leads lot communication caches making unsuitable large networks 
chankhunthod developed harvest cache scalable approach tree caches :10.1.1.21.1584
advantage cache tree cache receives page requests children siblings ensuring requests arrive simultaneously 
independent approach called prefetching useful reducing web latency 
idea send client requested page set pages request 
world wide web proxies proxy special server caching medium sized network 

going directly home server page clients network rst query proxy 
proxy page cached gives clients gets copy page home server caches cache forwards clients 
receiving individual requests client popular server receives request proxy reducing network tra rst request popular document clients able obtain document faster requests served locally proxy 
setting proxy server easy popular web client programs proxy support built 
size network containing proxy large proxy get swamped requests 
limits bene popular server derive proxy caches 
operative caching tried group caches function 
user request page directed arbitrary cache 
page stored returned user 
cache forwards request caches special protocol called ip multicast multicast protocol transmission packets subset hosts network 
page cached request forwarded home site page 
disadvantage technique number participating caches grows multicast number messages caches unmanageable 
tool develop consistent hashing gives way implement distributed cache requiring caches communicate time 
discuss chapter 
harvest harvest system rst implement hierarchical caching large scale :10.1.1.21.1584
consists number caches arranged hierarchy spread internet 
harvest cache con gured arbitrary number parents siblings 
harvest cache needs page queries siblings parents check page 
page immediately fetched source responds earliest positive reply 
ensures page obtained source low latency 
theoretical standpoint harvest approach lacks scalability uses hierarchy caches pages 
request new page pass top level hierarchy large load caches near top hierarchy distinct pages requested simultaneously 
developers harvest cache claim pose problem practice 
argue root level caches located internet back bone capable handling maximum request rate allowed bandwidth back bone 
push caching push caching introduced gwertzman seltzer harvard 
essential idea replicate popular document number replicas proportional popularity document 
envision network infrastructure thousands push cache servers les may 
central registry service tracks available push cache servers helping servers decide replicate objects providing list available push cache servers demand 
network topology taken account deciding push page 
network topology hard obtain gwertzman seltzer geographical distances approximation network latencies machines 
azer bestavros uses approach push caching addition uses second technique call speculative service 
idea server responds client request sending addition document requested number documents speculates requested client near 
similar technique prefetching discuss 
prefetching prefetching tries locality requests client pages web site 
users usually browse web hyperlinks web page 
hyperlinks page refer pages stored server 
typically pause page loaded user reads displayed material 
time client prefetch les accessed soon pointed current page avoiding retrieval latency les requested 
retrieval latency reduced just overlapped time user spends reading decreasing access time 
padmanabhan mogul propose predictive prefetching scheme world wide web servers tell clients les requested user clients decide prefetch les local contents local cache 
server maintains dependency graph depicts pattern accesses di erent les stored server 
graph node le weight associated arc nodes 
weight arc indicates requested accessed 
dependency graph updated dynamically server receives new requests 
currently requested weight arc higher prefetch threshold le considered candidate prefetching 
cache consistency caching strategy caches caching page needs worry pages pages modi ed frequently 
dynamic pages lead inconsistencies cached copies 
study issue cache consistency 
existing methods deal issue apply protocol 
value caching greatly reduced cached copies updated original data change 
cache consistency mechanisms ensure cached copies data eventually updated re ect changes original data 
gwertzman seltzer studied di erent techniques maintaining cache consistency context world wide web 
cache consistency mechanisms currently internet time live elds client polling invalidation protocols 
time live eld essentially estimate time object expires 
implemented expires header eld protocol 
requested document ttl elapsed considered expired fresh copy document obtained original server 
client polling technique periodically check back server determine cached objects valid 
basic assumption young les modi ed frequently old les older le modi ed 
example page day old fetched probably modi ed daily basis nt kept hours 
page month old fetched probably modi ed frequently probably kept days stale 
update threshold expressed percentage object age 
object invalidated time validation exceeds update threshold times object age 
invalidation protocols required weak consistency su cient distributed le systems rely invalidation protocols ensure cached copies stale 
invalidation protocols depend server keeping track cached data time item changes server noti es caches copies longer 
problem invalidation protocols expensive 
servers keep track objects currently cached 
machine data cached noti ed server continue trying reach cache know invalidate object noti ed server 
gwertzman seltzer trace driven simulations compare di erent mechanisms cache consistency web 
conclude weak cache consistency protocol reduces network bandwidth consumption server load ttl invalidation protocol tuned return stale data time 
related theoretical theoretical side plaxton rajaraman introduced idea randomization hashing balance load network 
done context synchronous network nodes parallel machine 
assume existence special priority messages reach node may swamped low priority messages clearly case web 
ravi studies problem nding optimal broadcast trees graph useful building multicast trees 
theoretical study prefetching done 
randomization hashing plaxton rajaraman rst conduct theoretical study problem providing fast concurrent access shared objects synchronous network distributed computation 
basic idea construct object random tree caches computed global hash function 
random trees ensures load balancing caches global hash function easy compute structure tree having store arrangement caches trees 
node needs object sends request random node level tree 
caching node senses large number request cached object pushes object children 
assumed node swamped large number requests able push object children 
ensures popularity object exceeds number cached copies number cached copies increases geometrically matches demand object 
plaxton rajaraman theoretical analysis protocol model containing nodes node process log requests time step prove requests satis ed high probability ino log time steps 
ideas apply problem relieving hot spots web 
assume messages machines prioritized node may messages low priority reached high priority message 
assumptions true case internet 
method take consideration varying distances di erent machines web 
assume uniform distance metric nodes parallel machine 
finding minimum time broadcast trees model web weighted graph nodes representing machines weights denoting latencies machine ask minimum time required broadcast popular document residing node nodes graph 
optimal broadcast correspond sending page spanning tree 
tree suitable multicast protocol 
ravi studied special case problem synchronous framework time step processor received document allowed communicate neighbors network 
special case problem known np complete regular planar graphs 
ravi gives log log log approximation algorithm broadcast time problem node graph 
prefetching algorithms prefetching studied learning problem involves predicting page accesses user 
vitter krishnan give rst provable theoretical bounds prefetching performance 
approach optimal data compression methods optimal prefetching 
model request trace mth order markov source states correspond previous requests evaluate prefetching algorithm relative best online algorithm complete knowledge structure transition probabilities markov source 
contribution describe tools data replication give caching algorithm comes drawbacks preceding approaches additional desirable properties 
rst tool random cache trees combines aspects structures chankhunthod plaxton rajaraman 
chankhunthod tree caches coalesce requests 
plaxton rajaraman balance load di erent tree page assigning tree nodes caches random hash function 
combining best features chankhunthod plaxton rajaraman methods prevent server swamped high probability property possessed chankhunthod plaxton rajaraman 
addition protocol shows reduce memory requirements signi cantly increasing cache rates caching pages requested su cient number times 
second tool new hashing scheme call consistent hashing 
hashing scheme di ers substantially plaxton rajaraman practical systems 
typical hashing schemes job spreading load known xed collection servers 
internet xed collection machines 
machines come go brought network crash 
worse information machines functional propagates slowly network clients may incompatible views machines available replicate data 
standard hashing useless relies clients agreeing caches responsible serving particular page 
consistent hashing may help solve problems 
hashing schemes consistent hashing assigns set items buckets bin receives roughly number items 
standard hashing schemes small change bucket set induce total remapping items buckets 
addition hashing items slightly di erent sets buckets gives slightly di erent assignments items buckets 
apply consistent hashing tree caches scheme show scheme client aware constant fraction caching machines 
addition believe consistent hashing useful applications multiple machines di erent views network agree common storage location object communication 
presentation chapter describe model web hot spot problem 
model necessarily simplistic rich develop analyze protocols believe may useful practice 
chapter describe random tree method caching protocol ectively eliminates hot spots simpli ed model 
simpli ed model assumes latencies pairs machines browser knows caches system 
independent chapter chapter consistent hashing method solve hot spots model involving inconsistent views 
chapter combine techniques show protocol works browsers inconsistent views set caches 
chapter propose simple delay model captures varying distances machines internet 
show protocol easily extended realistic delay model 
chapters consider fault tolerance protocol 
chapter discuss extensions open problems 
chapter model chapter presents model web hotspot problem 
classify computers web categories 
requests web pages initiated browsers 
permanent homes web pages servers 
caches extra machines protect servers browser requests 
set caches number caches note categories may disjoint 
machine simultaneously cache server browser 
server home xed set pages 
caches able store number pages set may change time dictated caching protocol 
assume content page unchanging 
believe past done handling dynamic pages caching protocols discussed section directly applied protocols 
set pages denoted machine may send request machine aware 
typical types messages requests pages pages 
machine receives messages quickly ceases function properly said swamped 
adversary decides pages requested browsers 
adversary see random values generated protocol adapt requests observed delays obtaining pages 
consider models 
consider static model single batch requests processed require number page requests number caches 
latency measures time message machine arrive machine denote quantity 
practice course delays internet simply characterized 
value regarded best guess optimize lack better information correctness protocol depend values measure throughput price connection congestion exactly accurate 
note latency function message size issue discussed section 
objective hot spot problem satisfy browser page requests ensuring high proba bility cache server swamped 
basic requirement prevent swamping additional objectives 
rst minimize cache memory requirements 
protocol requiring cache store large number pages 
second objective naturally minimize delay browser experiences obtaining page 
chapter random trees chapter introduce rst tool random trees 
standard solution handle large number requests page server set proxy caches protect server obtain page browser requests page proxy caches turn request server page 
problem scheme choosing size set caches assigned protect server 
set small caches overwhelmed page requests 
set large server swamped page requests caches assigned protect 
natural extension introduce layers proxy caches cache layer requests caches layer :10.1.1.21.1584
ensure ratio number caches layer number layer large 
suggests arrange caches balanced tree bounded degree server root tree 
browser needs page request cache random leaf node serves request page cache delegates request parent 
focus attention su ciently brief interval time ignore eviction pages cache sends request page parent internal node gets requests page caches tree leaves maximum demand page tree satisfy swamping times demand single machine satisfy 
popularity page increases copies page get cached tree 
fact page tree threshold level number caches roughly equal number requests page 
caches threshold level see requests page tend cached copies threshold level see requests tend copies page 
popularity page increases threshold level moves increasing number copies page 
way number cached copies automatically adapts changes popularity page 
far talked handling large number requests page 
need extend protocol handle potential hotspots web 
try tree cache pages 
observe root machine receive rst request page 
distinct pages requested caches close root receive requests load balanced evenly caches 
solution create distinct tree caches page 
fact way achieve load balancing page tree caches arranged random order technique plaxton rajaraman 
ensures machine near root pages providing load balancing 
describe protocol 
simplify presentation start simple caching protocol simpler world 
particular simpli cations model 
machines know caches 

distances machines uniform mi mj restrictions show protocol behavior 
high proba bility machine swamped 
request browser may need go log proxy caches prove necessary prevent swamping 
total cache space small fraction number requests evenly divided caches 
chapter analyze protocol assumption machines full knowledge caches 
chapter extend protocol scenario latencies pairs machines uniform 
section de ne protocol precisely 
section analyze protocol bounding load cache storage cache uses delay browser experiences getting page 
protocol just harvest hierarchy caches :10.1.1.21.1584
hierarchy pages di erent page 
hierarchy caches arranged random order similar approach taken plaxton rajaraman 
associate page rooted ary tree called tree represent tree caches page 
term nodes nodes trees 
number nodes tree equal number caches tree balanced possible levels bottom full 
number nodes tree rank breadth rst search order 
protocol described running trees support requests pages take form tuple consisting identity requester name desired page sequence nodes request directed sequence caches act nodes 
determine sequence cache node nodes mapped machines 
root tree mapped server page 
nodes mapped caches hash function distributed browsers caches 
order create copies pages requests parameter requests cache see acting particular node store copy page 
hash function parameters protocol follows browser browser wants page picks random leaf root path tree maps nodes machines asks leaf node page 
request includes name browser name page path result mapping 
cache cache receives request rst checks see caching copy page process getting cache 
returns page requester gets copy necessary 
increments counter page node acting asks machine path page 
counter reaches caches copy page 
case cache passes page requester obtained 
server server receives request sends requester copy page 
analysis section analyze performance protocol 
simplify analysis assume certain number requests arrive simultaneously 
restricted model static sense notion requests arriving time 
number static requests average number requests cache 
static analysis simply look paths taken requests respective trees 
node receives requests children cache page 
node sends requests parent 
analysis broken parts 
showing latency processing request small assumption machine swamped 
show machine swamped 
conclude showing cache need store pages protocol properly 
latency protocol delay browser experiences obtaining page determined height tree 
request forwarded leaf root latency twice length path log request satis ed cached copy latency 
request stops cache waiting cache copy latency request started tree 
note probably large practice latency quite small 
protocol increases delay getting pages existence tree schemes harvest cache suggests acceptable practice 
note practice time required obtain large page multiplied number steps path travels 
reason page transmitted path pipelined fashion 
cache middle path start sending data cache soon receives need wait receive page 
means protocol increase delay getting small pages overhead large pages negligible 
bound optimal constant factors protocol forbids swamping shown lemma 
lemma protocol handle requests page simultaneously machine serving requests average latency log hops request 
proof consider making requests single page 
look directed graph nodes corresponding machines directed edges corresponding links page sent 
graph degree node 
number nodes reachable home server page steps constant fraction nodes log steps away home server 
means average distance home server requesting machines log 
swamping analyze number requests machine gets protocol simpli ed model 
note server receive requests page 
assume server handle requests page 
remains analysis number requests received caches 
intuition analysis 
analyze number requests directed tree nodes various pages 
give weights tree nodes 
analyze outcome tree nodes mapped hash function actual caching machines machine gets requests total weight nodes mapped 
bound mapped weight rst give bound case node assigned random machine 
weighted version familiar balls bins type analysis 
analysis gives bound exponential tail 
lemma develop tools ensure bounds hold balls assigned bins log way independently 
achieved universal hash function map tree nodes machines 
analysis random theorem chosen uniformly random space functions 
probability parameter number requests cache gets log log log log dq log log dq log log requests note log average number requests cache browser request give rise log requests trees 
tree page cache occur log log log log log log term arises leaf nodes times balls bins adversary chooses devote requests page leaf expected receive requests 
corollary chosen uniformly random space functions 
probability parameter cache gets requests log log nc log log nc log dq log nc dq log nc log nc proof bound theorem holds cache probability caches probability bound holds caches simply parameter replace nc get corollary 
prove theorem rest section 
split analysis parts 
analyze requests cache due presence leaf nodes trees analyze requests due presence internal nodes add 
requests leaf nodes request page goes random leaf node page tree 
denotes number leaf nodes tree 
associate weight rp leaf node tree number requests expected receive 
map weighted leaf nodes pages set caches bound total weight assigned cache 
argue total number requests received cache high probability close total weight assigned 
note simply say requests mapped randomly set caches di erent mapping requests nodes rst mapping nodes caches 
leaf node page tree associate weight wp rp machine chance arbitrary leaf node page 
denote event jth leaf node tree assigned probability 
try bound total weight assigned cherno bounds weighted sum poisson variables weights possibly greater 
note weight wp rp 
apply cherno bounds weighted sum poisson variables weights 
gives bound log log log holds probability argue high probability number leaf node requests machine gets close random variable assignment tree nodes machines denote set leaf nodes get assigned observe random variable function denote function 
random variable hl denote total number requests received machine due presence leaf nodes 
need provide high probability bound hl denote high probability bound just proved 
pr hl pr pr hl jw pr pr hl jw pr pr hl jf know rst probability sum choose appropriately second part sum set leaf nodes claim total number requests written sum independent poisson random variables 
poisson variable request page set gets request 
denote expected value sum cherno bounds know probability sum lnn set lnn second probability claim probability random variable hl requests internal nodes lnn bound number requests gets due presence internal nodes 
think protocol rst running trees 
node associate weight equal number requests receives 
weighted nodes balls randomly assigned set caches bins 
standard balls bins type analysis bound total weight falling bin 
internal node gets dq requests child node gives requests page 
consider arbitrary arrangement paths requests respective trees 
requests bound number nodes get dq requests 
fact bound number nodes trees receive requests qj log dq 
nj denote number nodes receive requests 
rp number requests page rp requests gives rise logd requests trees total number requests log log dq lemma gives bound nj nj log lemma total number internal nodes receive qx requests proof look rp requests page paths produced requests tree 
consider tree internal nodes induced paths 
node get requests child node gets qx requests downward degree 
look nodes downward degree 
parent child respectively 
replace downward degree nodes single edge connecting eliminate nodes downward degree equal preserve degrees nodes 
left tree node downward degree 
tree number leaves half total number nodes 
sum downward degrees equal total number edges number vertices minus 
number leaves tree number requests nodes downward degree xy rp rp total number nodes trees receive qx requests rp clearly log requests 
preceding lemma tells nj number nodes receive requests 
nj logd probability machine assumes nj nodes assignments nodes machines independent probability machine receives nodes nj cz order right hand side small set nj log log 
note log nj term nj nj log 
log log probability log nj probability log dq total number requests received due internal nodes order gets log dq nj log dq nj log dq log dq logd log log log nj log log log log nj log log log log log log dq logd log log log log log dq log log dq log log combining high probability bounds internal leaf nodes say machine log requests probability simplifying get theorem 
log log log dq log log dq log log log dq 
replacing log dq expression tightness high probability bound section show high probability bound proven number requests received machine tight 
lemma exists distribution requests pages machine gets log log log log dq log log dq log requests probability proof show bounds tight constant factors need show distributions give rise terms 
requests di erent page gives rise log requests respective trees 
total number requests generated log expected number requests received log justi es presence log term bound 
dq log justify log dq log term adversary divide requests equally pages page gets requests 
cherno bounds probability second level nodes particular pages trees receive dq requests 
probability machine second level node particular tree total number second level nodes trees dq probability atx dq second level nodes dq dq log reduce log dq 
second level node pages log expected receive dq requests 
probability machine receives requests 
log log log dq log log dq log term adversary devote requests hot page 
leaf positions leaf node gets requests expectation 
probability machine occupy log requests expectation 
log log analysis way independent log log log leaf positions receive far assumed hash function perfectly random 
practice hash families universal small extend high probability analysis functions chosen random universal hash family 
rst prove general lemma allows high probability results sum fully independent random variables extended case random variables way independent 
pn lemma hi hi random 
assume high probability bound holds fully independent probability parameter 
increasing function satis es property positive integer sum random variables hi way indepen dent 
log high probability holds proof probability sum way independent random variables apply markov inequality th power sum 
pr jk high probability bound wish prove 
choose probability exceeding need bound 
expanded sum products product term consist hi hi way independent jk 
bound fact exceeds probability tells lies probability recall log log log log sum terms log decreases factor 
sum starts behaving geometric sum 
largest term sum sum 

substituting equation get shown probability lemma extend high probability bound proved theorem case log way independent hash functions 
corollary high probability bound proved theorem number requests cache gets holds constant factors selected log universal hash family 
proof number requests received machine expressed weighted sum bernoulli random variables 
associate bernoulli random variable ui th node tree page mapped attach weight wi event ui number requests received node 
fully independent ui fully independent 
way independent ui 
apply lemma 
denote bound proved theorem 
grows logarithmically satis es condition 
deduce bound holds constant factors chosen log universal hash family 
idea bounds prove forth 
improvement protocol better bounds observe log log log term bound proved theorem occurs due requests arising leaf nodes 
leaves machine occurs log times log log leaf nodes tree probability modify protocol slightly having leaf nodes log mc leaf nodes parameter set 
turns having nodes tree log mc nodes get rid log log log term bound choosing de nitely impact latency getting page impact signi cant latency log log nc iso log ifn theorem tree log mc nodes chosen uni random space function 
probability parameter number requests cache gets log dq log log dq log log proof proof similar theorem 
divide analysis parts 
analyse number requests cache gets due presence leaf nodes 
requests leaf nodes page tree leaf nodes 
machine chance occurring particular leaf node occur log mc leaf nodes expectation 
cherno bounds occur log mc log leaf positions high probability requests occur log mc log leaf nodes requested pages trees probability assignment machines leaf nodes occurs log mc log times tree expected number requests gets log mc log iso log log mc 
assignment machine leaf nodes xed number requests gets sum independent bernoulli variables 
cherno bounds gets probability conclude gets log log requests log mc log log requests log mc probability replacing get bound log nr log mc log holds probability number gets due presence internal nodes doing exactly dq log analysis proof theorem get bound logd log dq log log holds probability combining bounds requests due leaf internal nodes get bound log dq log log dq log nr log log 
setting get desired log log mc result 
storage section discuss amount storage cache order protocol 
amount storage required cache simply number pages receives requests acting particular node 
needs keep counter page requested node gets mapped total number counters maintained caches number requests space required maintain counter negligible comparison space required typical page expect dominate storage requirements cache 
lemma bounds total number cached pages system cache 
lemma total number cached pages machines log probability cache log cached pages probability number cached copies proportional number requests constant proportionality small choosing large 
average number requests cache 
term expression number cached pages sense 
rest section devoted proving lemma 
perform analysis assuming tree nodes randomly mapped caches deduce lemma bound holds mapping done wise independent hash function log show total number cached pages nodes ln high probability follows standard balls bins argument probability number cached pages machine qc log log 
studying distribution weights storage counts nodes particular tree 
consider certain tree tp page suppose rp requests page node level expected number requests node receives rp quantity drops level 
certain level expectation ed 
call threshold level page 
bound number cache copies parts number threshold level number 
threshold level pessimistic assumption node receives requests caches page 
number nodes level decreasing total number nodes threshold level times number nodes level threshold 
de nition number nodes threshold level page number nodes threshold level qd total levels 
total number copies pages cached thresholds re remaining task bound number cache copies threshold level 
generating function argument level separately 
threshold level 
note throwing rp balls requests bins nodes 
bin counts causes cache copy receives requests 
probability bins receives balls probability set bins receives total qj balls qj qj 



er qb qj qj qj consider generating function jxj probability exactly bins get balls 
upper bounded deduce generating function upper bounded term term generating function sequence exp bx 
fact threshold level deduce function upper bounded term term exp re 
consider probability generating function number threshold nodes pages receive requests 
simply product pages upper bounded product generating functions exp re exp re exp re consider level threshold 
times nodes expected number requests machine divides factor impact analysis 
expectation place replaced level threshold replace ed 
continue analysis get bound exp re qd probability generating function provided 
similarly levels threshold probability generating function bounded exp re qd 
probability generating function total number nodes pages threshold levels receive requests bounded exp re exp re red exp follows probability cache copies threshold levels quantity red 
ln shown probability number cached pages threshold levels iso ln 
chapter consistent hashing chapter de ne new hashing technique called consistent hashing 
motivate technique simple scheme data replication internet 
consider single server large number objects clients want access 
natural introduce layer caches clients server order reduce load server 
scheme objects distributed caches responsible roughly equal share 
addition clients need know cache query speci object 
obvious approach hashing 
server hash function evenly distributes objects caches 
clients hash function discover cache stores object 
consider happens set active caching machines changes client aware di erent set caches 
situations plausible internet 
distribution done classical hash function example linear congruential function ax mod inconsistencies catastrophic 
range hash function example changed item hashed new location 
suddenly previously cached data useless clients looking di erent location 
consistent hashing solves problem di erent views 
de ne view set caches particular client 
assume views inconsistent substantial machine aware constant fraction currently operating caches 
client uses consistent hash function map object caches view 
analyze construct hash functions consistency properties 
smoothness property 
machine added removed set caches expected fraction objects moved new cache minimum needed maintain balanced load caches 
second client views total number di erent caches object assigned call spread small 
similarly client views number distinct objects assigned particular cache call load small 
consistent hashing solves problems discussed 
spread property implies presence inconsistent views world object directed small number caching machines 
distributing object small set caches insure access clients lot storage 
load property implies cache assigned unreasonable number objects 
smoothness property implies smooth changes set caching machines matched smooth evolution location cached objects 
section de ne ranged hash function precisely de ne quantities capture di erent aspects consistency 
section construct practical hash functions exhibit extent 
de nitions section formalize relate notions consistency 
domain items set buckets 
view subset buckets ranged hash function function form function speci es assignment items buckets possible view 
bucket item assigned view 
notation fv place 
items assigned usable buckets require fv view ranged hash family family ranged hash functions 
random ranged hash function function drawn random particular implicitly speci ed ranged hash family 
remainder section state relate reasonable notions consistency regarding ranged hash families 
notational conventions ranged hash family ranged hash function view item bucket 
discussion focus particular subset items jij number items subset 
chapter assume denotes total number buckets chapter set caches set buckets 
load de ne set views 
ranged hash function bucket load quantity number distinct items get mapped di erent views note set items assigned bucket view load hash function maximum load bucket 
load hash family random variable load randomly chosen hash function family 
property says distinct items person thinks belongs bucket mapping items buckets uniform consistent hash function low load 
spread vv set views altogether containing distinct buckets individually containing buckets 
ranged hash function particular item spread quantity number distinct buckets gets mapped di erent views spread ranged hash function maximum spread item 
spread hash family random variable spread randomly chosen hash function family 
idea spread people di erent views set buckets 
person tries assign item bucket consistent hash function 
property says entire group di erent opinions bucket contain item 
clearly consistent hash function low spread items 
balance ranged hash family balanced view containing buckets item randomly chosen function selected hash family probability mapped bucket view 
monotonicity ranged hash function monotone views fv implies fv fv 
ranged hash family monotone ranged hash function property says items initially assigned set buckets new buckets added form item may move old bucket new bucket old bucket 
main result consistent hashing theorem shows existence ciently computable balanced monotonic ranged hash family logarithmic spread load 
construction give construction ranged hash family properties 
random functions rb ri map items buckets real numbers range respectively 
hard implement completely random functions practice demand functions rb ri map points log ncv way independently high probability parameter 
function rb maps buckets randomly unit interval ri items 
fv de ned bucket minimizes ri words mapped bucket closest create interval bucket responsible items falling interval 
items buckets mapped randomly items tend uniformly distributed buckets 
consider happens new bucket added 
items new buckets earlier assigned buckets move new bucket 
reasons apparent need point unit interval associated bucket 
assuming number buckets range need log points bucket constant easiest way view bucket replicated log times rb maps replicated bucket randomly 
denote described hash family clearly lower bound size view spread load large making view sizes equal 
assume view consists certain fraction buckets 
implementation section show hash family just described implemented ciently 
specif ically expected running time single hash computation 
expectation choice hash function 
expected running time adding deleting bucket log total number buckets views 
simple implementation uses balanced binary search tree store points unit interval corresponding buckets 
nd bucket item mapped simply needs search ri search tree 
give desired bucket closest real interval 
buckets log intervals search tree depth log 
single hash computation takes log time 
time addition removal bucket log insert delete log points bucket 
trick reduces expected running time hash computation 
idea divide interval roughly log equal length segments keep separate search tree segment 
time compute hash function time determine interval ri plus time lookup bucket corresponding search tree neighbors 
rst time 
expected number points segment second time expectation 
practical limitation hash functions hash real numbers assumed ri rb think random real numbers sequence random bits pick random bits distinguish point points 
turns high probability su ces compute rst log bits real number distinguish 
analysis theorem proves hash family described desired properties consistent hash family 
theorem functions rb ri log nv way independent view contains fraction buckets random function chosen ranged hash family described properties 
monotone 

spread item log nv probability greater 
load bucket log nv probability greater 
balance probability xed view containing buckets pr fv prove theorem rest section 
note monotonicity immediate 
new bucket added items move closest bucket associated points 
items move old buckets proof theorem requires technical lemma gives upper bounds sum bernoulli variables variables way independent 
lemma sum wise independent binary random variables pr bk ii pr jx proceed prove claims theorem series lemmas 
rst show spread load properties 
looking large interval unit real interval high probability view bucket point interval 
lemma speci es length interval 
lemma consider interval length log nv log unit real interval probability view bucket point interval 
proof xi random variable denoting number points buckets view vi interval length log bucket points associated view 
assume exactly points view worst case xi ml 
choose value probability nv points log nv way independent ml log nv exists conditions lemma apply 
choose pr xi pr jxi mlj ml nv log nv log union bound get pr xi vx pr xi nv shown view bucket falls interval length lemma spread item log nv probability proof fix 
know lemma consider interval length log nv log centered itm probability view bucket point interval 
view gets mapped bucket interval 
get bound bounding number bucket points fall interval random variable denoting number points interval length ri coming union views 
log points total cl log nv 
mapping log nv way independent nd lemma applies 
follows 
pr log nv nv nv proved iso log nv probability nv bound holds replace gives desired result 
lemma load bucket log nv probability proof bucket log points unit interval 
item assigned closest points bucket points 
log points associated consider interval containing point associated bucket side 
gives collection log intervals item maps fall collection intervals 
rst bound total length collection intervals bound number items fall 
get bound load fix log points associated call point know lemma look interval length log nv left probability log view bucket interval 
similarly look interval length right probability view bucket interval 
probability bucket distance right ofy left assuming event occurs responsible segment length interval 
items mapped interval length centered closest point placed bucket closer endpoints unit interval responsible length interval 
follows union bound views log points associated responsible interval length log log nv probability log call event assuming event occurred load bounded number items mapped set measure log nv interval 
proof spread de ne random variable equal number items interval 
items nv apply lemma 
consider applying part lemma part ii case 
case follows log nv log nv probability nv 
probability log nv load log nv 
bound holds replace log giving desired result 

proof lemma implies useful rest corollary probability mapping buckets pr fv bin view nv 
proof saw proof lemma total length intervals bucket responsible log nv probability log item mapped random point unit interval log nv chance assigned bucket replacing log proves 
remains show balance property hash family 
note probability item getting assigned particular bucket exactly total length parts unit interval bucket responsible 
lemma bounds section unit interval bucket assigned responsibility 
lemma assume log points mapped unit interval log nv way indepen dently buckets 
bucket denote length measure set points closer points bucket point 
probability length log nc log 
proof bucket point unit interval 
lies right point associated closer point point bucket say right responsible main result probability single bucket right responsible log nc fraction unit interval nc 
union bound implies log buckets right responsible log nc log fraction unit interval high probability symmetry bucket sided responsible twice right responsible log nc log 
show main result xing bucket portion unit interval right responsible consist non overlapping intervals bounded left points 
suppose shrink intervals moving right endpoints leftward length interval multiple log 
log intervals shrinks decrease total length intervals total length intervals shrinking log total length shrinking log set implies bucket right responsible fraction unit interval right responsible point collection non overlapping intervals bounded left points length total length collection intervals total length bound probability intervals get point associated bucket 
expected number log points falling collection intervals log rb log way independent denotes number points falling collection intervals lemma pr pr jx log cj log log number collections log intervals total length lengths multiples exactly number ways partition log log integral parts log log log 
log log log log union bound probability collections intervals contains point associated buckets log log log choose nc 
gives log nc log assumed log wish log nc 
gives log nc log constant set condition satis ed 
simply set log nc 
proves probability nc total length assigned log bucket log log nc log nc way independent 
log buckets bound holds buckets probability lemma xed set buckets probability pr fv log nc log conditioned choice assignments items buckets log nc way independent 
proof fix 
lemma says probability log points associated responsible log nc log interval 
probability mapped equal interval length 
rst statement lemma follows 
function ri assumed map items points log nc way independently chosen rb items assigned log way independently intervals buckets 
setting previous lemma gives desired balance property 
corollary xed set buckets probability pr fv conditioned choice assignments items buckets log nc way independent 
chapter basic solution inconsistent world chapter apply techniques developed section simple hot spot protocol developed chapter 
relax assumption clients know caches 
assume machine knows fraction caches chosen adversary 
di erence protocol mapping consistent hash function 
change ect latency 
analyze ects swamping storage 
basic properties consistent hashing crucial showing protocol works 
particular blowup number requests storage proportional spread load hash function 
swamping theorem implemented log nrc way independent consistent hash function theorem view consists caches probability cache gets log log nr log nr log requests 
proof look di erent trees caches di erent views page number di erent views total number requests denote number caches tree 
view nodes tree get mapped caches di erently 
overlay di erent trees obtained mapping nodes caches view get new tree node set caches 
due spread property consistent hash function log nr caches appear node combined tree high probability 
fact requests true nodes trees requested pages 
ep denotes event appears jth node combined tree page know corollary probability event load iso log nr high probability 
condition event log nr happens high probability 
cache node sends requests node combined tree sends requests 
adapt proof theorem case 
theorem machine aware caches node assigned machine probability wenow assign node machine probability 
scenario caches node sends requests parent occurs node independently probability 
rest proof similar theorem 
analyze number requests gets new scenario 
split analysis parts 
analyze hits cache due presence leaf nodes trees analyze hits due presence internal nodes add 
hits leaf nodes tree leaves 
assigned leaf probability expected occur log leaf nodes tree 
fact cherno bounds occurs log log leaf nodes trees probability assignment leaf nodes get log log requests total requests expectation fact high probability conclude gets log requests probability hits internal nodes proof theorem think protocol rst running trees 
internal node gets dq requests child node gives requests page 
consider arbitrary arrangement paths requests respective trees 
node receive dq requests 
nj denote number nodes receive requests log dq 
denote ratio number browsers number caches view 
requests gives rise log requests trees total number requests log lemma prove nj nj log dq nj logd nj logd nodes assigned probability 
show probability assigned nj nodes 
log nj log log nj probability log dq total number requests received due internal nodes order log dq nj log log log nj equation bound nj expression simpli es log dq log 
proved probability log dq number requests gets acting internal nodes log dq log requests 
replacing log dq get bound holds probability combining high probability bounds internal leaf nodes tells probability gets log dq log requests 
far assumed ep fully independent log nr way independent 
take account lemma tells bound holds ep log nr way independent 
log nr probability theorem follows 
storage techniques similar proof theorem get lemma 
lemma total number cached pages machines log logd probability cache log cached copies high probability proof proof theorem overlay trees caches di erent views page due spread property consistent hash function log nr caches appear node combined tree high probability 
ep denotes event appears th node combined tree page know corollary probability event iso load 
scenario caches occurs node independently probability 
rest proof similar theorem 
de ne threshold level page level combined tree node expected receive ed 
bound number cached copies pages threshold level generating function argument bound number copies threshold level 
exactly analysis get total number nodes pages receiving requests log log probability combined tree node contains machines total number cached copies pages get multiplied factor giving bound log log 
bound number cached copies machine log log nodes assigned probability 
cherno bounds high probability gets log nodes 
lemma get bound log way independent 
chapter ultrametric distances assumption pair machines communicate equal ease obviously unreal istic adapt protocol realistic model section 
recall request machine page machine stages asks page obtains page returns page latency page request de ned duration stages 
duration rst third stages function ease communication modeling ease communication machines internet tricky 
internet communications protocol tcp ip gives formal guarantee time pass message machines 
empirically time vary considerably due network congestion changes routing hardware 
compiling statistics past communications may obtain reasonably accurate typical time pass packet machines 
assume typical communication times available 
particular machine requests page machine duration rst third stages page request 
furthermore assume machine knows machine may know distance machines say storage required information linear number machines 
latency page request expressed terms example browser requests page cache cache forwards request server latency page request 
extend protocol restricted class functions particular assume ultrametric 
formally ultrametric metric obeys strict form triangle inequality max 
ultrametric natural model internet distances essentially captures hierar nature internet topology example machines university equidistant farther away university farther continent 
logical point point connectivity established atop physical network generally case latency sites determined highest level physical communication link traversed path 
de nition ultrametric hierarchical clustering points 
distance points depends smallest cluster containing 
example dis tance machines university distance machines di erent universities country 
addition modeling communication latency models put machines 
large pages maximizing throughput important mini latency 
throughput typically determined maximum congestion physical commu nication link path implementing virtual point point connection machines ultrametric 
protocol modi cation protocol browser needs page uses caches away server page 
size tree equal number caches distance server 
doing insure path server contain caches unnecessarily far away metric 
mapping done consistent hash function vital element solution 
clearly requiring browsers nearby caches cause swamping cache server near browsers 
order avoid cases degenerate browsers close cache clusters ultrametric caches restrict set may protocol 
restriction cluster ratio number caches number browsers may fall recall sake analysis restriction equivalent imagining requests originate caches cache allowed requests 
restriction sense real world caches evenly spread internet 
necessary clear large number browsers clustered cache forced swamp cache modi ed protocol 
analysis clear protocol de nition ultrametric latency depth tree log times latency browser server 
need look swamping storage 
intuition inside cluster bounds proved unit distance model apply 
monotone property consistent hashing allow restrict analysis log clusters 
summing clusters log blowup bound 
swamping theorem ultrametric 
suppose browser request 
log protocol arbitrary cache gets log logd log log dq log log dq requests probability 
log proof arbitrary cache prove theorem consider clustering machines distance denote resulting clusters cs consider request machine receives 
request path smallest cluster ci containing browser request server requested page 
request receives associated cluster caching protocol run 
ri denote number requests receives associated ci nd maximum number browsers contribute ri browser outside ci possibly contribute cluster bigger ci de nition ri request browser contribute ri try bound ri ci denote number caches cluster ci lower bounded density caches ci note ri simply number requests receives due ci browsers playing caching protocol ci caches 
theorem deduce ri log log log log dq log log dq log requests probability conclude probability receives requests number clusters pr receives requests sx ci pr receives requests ci unfortunately number clusters large number caches bound 
lemma improve bound showing consecutive clusters grow fast counted single cluster 
lemma set consecutive clusters ci ci ci ci ri probability proof rj ci ci ci requests received part cluster cj protocol cj nodes 
observe page di erence structure smallest tree built caches ci largest tree built caches ci adding leaf positions tree overlap 
basic idea proof modify protocol clusters obtain protocol worse real protocol apply theorem modi ed protocol 
speci cally show smallest set caches ci page requests clusters ci ci caching protocol get requests real protocol 
observe due monotone property consistent hashing scheme described earlier decrease number caches lling tree positions previously lled deleted caches change lled remaining caches 
removed set caches number times appears internal node tree go 
original places appeared remain unchanged 
reducing number caches receive requests 
look number requests cache gets due presence internal nodes due presence leaf nodes 
due presence internal nodes cache get number requests got trees size ci theorem 
due presence leaf nodes cache get number requests got trees size ci 
total number requests cache gets previous lemma implies summing clusters compute bound number requests need sum clusters twice size previous 
sum log terms bound achieve log times previous bound 
storage techniques similar proof theorem get lemma 
lemma total number cached pages machines logd cache log log log cached pages probability proof easy prove bound log total number cached pages machines 
request gives rise log tree total number requests received caches log nodes caches page receives requests 
clearly log cached copies caches 
bound number cached pages single machine bound number requests proved theorem 
bound stated theorem log log dq log 
receive requests page cached number cached pages number requests divided gives desired result 
chapter fault tolerance basically plaxton rajaraman fact protocol uses random short paths server fault tolerant 
consider model adversary designates caching machines may ignore attempts communication 
remember adversary get see random bits simply designate machines top tree 
restriction speci ed fraction machines view 
protocol preemptive caching pages done 
server goes pages distributed inaccessible algorithm 
problem eliminated standard techniques rabin information dispersal algorithm 
ignore server faults 
chapter analyze minor modi cation protocol show ensures page request satis ed high probability 
observe analysis request satis ed quite simple look path machines request travels check 
request gets 
say path tree clean encounter dead caches 
lemma ensures random request path chance clean 
lemma log log probability log fraction paths tree clean 
proof start server count fraction paths remain clean way leaves 
denote fraction dead machines 
say node clean path node root clean 
level just server contains log caches 
probability arbitrary node dead log expected number dead caches second level df nodes depth clean 
cherno bounds probability caches dead inductively argue probability fraction nodes depth clean 
proved 
depth number clean paths depth extend give di paths 
depth tree fraction nodes depth clean paths depth fraction expected dead caches depth cherno bounds probability fraction encounter dead caches depth completes inductive step 
setting log get probability log fraction paths clean 
modi cation protocol quite simple 
choose parameter send requests page 
logarithmic number requests su cient give high probability requests goes 
clearly increase total load network log factor 
practice sending requests simultaneously browser wait certain time interval sending request 
note communication thing internet failure get quick response machine particularly indication 
focused tolerance faults detection 
way decide machine consistent hash functions trivial reassign machines 
decide machine remove view 
chapter focused particular caching problem handling read requests web 
believe ideas broader applicability 
particular consistent hashing may useful tool network di erent users di erent views network need agree location resource having communicate 
remains open deal time modeling internet communication protocols guarantees regarding time delivery 
packet level guarantees regarding eventual delivery 
suggests modeling internet kind distributed system 
clearly model guarantees regarding delivery times best hope prove classical liveness safety properties underlying distributed algorithms 
clear prove caching swamping model 
think signi cant research done proper way model aspect internet 
believe interesting open questions remain regarding method consistent hash ing 

universal consistent hash function evaluated ciently 
tradeo achieved spread load 
kind perfect consistent hash functions constructed de spread load bounds give 
theoretical problems consistent hashing give handle 
bibliography chankhunthod peter danzig chuck neerdaels michael schwartz kurt worrell :10.1.1.21.1584
hier internet object cache 
usenix proceedings 
azer bestavros 
speculative data dissemination service 
robert devine 
design implementation ddh distributed dynamic hashing algorithm 
proceedings th international conference data organizations algorithms 
feeley morgan karlin levy thekkath 
implementing global memory management cluster 
proceedings th acm symposium operating systems principles 
sally floyd van jacobson steen mccanne ching gung liu lixia zhang 
reliable multicast framework light weight sessions application level framing sigcomm witold litwin marie anne neimat donovan schneider 
lh scalable distributed data structure 
acm transactions database systems dec jacob lorch david berger 
making world wide web caching servers cooperate 
proceedings world wide web conference 
naor wool 
load capacity availability quorum systems 
proceedings th ieee symposium foundations computer science pages november 
peleg wool 
availability quorum systems 
information computation 
greg plaxton rajaraman 
fast fault tolerant concurrent access shared objects 
proceedings th ieee symposium foundations computer science 
rabin 
cient dispersal information security load balancing fault tolerance 
journal acm 
schmidt alan siegel aravind srinivasan 
cherno hoe ding bounds applications limited independence 
proc 
th acs siam symposium discrete algorithms 
venkata padmanabhan je rey mogul 
predictive prefetching improve world wide web latency 
sigcomm 
ari kevin 
world wide web proxies 
computer networks isdn systems 
international conference world wide web elsevier science bv 
available www cern ch luotonen ps ravi 
approximating minimum broadcast time 
focs 
grigni peleg 
tight bounds broadcast networks 
siam journal discrete math may pp 

james margo seltzer 
case geographical push caching 
personal commu nication 
palmer zdonik fido cache learns fetch 
inproceedings international conference onvery large databases september 
salem 
adaptive prefetching disk bu ers 
goddard space flight center tr january 
je rey scott vitter krishnan 
optimal prefetching data compression 
focs 
stephen deering david cheriton 
multicast routing datagram internetworks ex tended lans 
acm transactions computer systems may 
james margo seltzer 
world wide web cache consistency 
personal communication 

