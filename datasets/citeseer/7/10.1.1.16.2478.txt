predicting discrete sequences fractal representations past peter ti georg dor ner austrian research institute arti cial intelligence vienna austria phone email georg ai univie ac propose novel approach building nite memory predictive models similar spirit variable memory length markov models vlmms 
models constructed rst transforming block structure training sequence geometric structure points unit hypercube longer common sux shared blocks closer lie point representations 
transformation embodies markov assumption blocks long common suxes produce similar continuations 
prediction contexts detecting clusters geometric block representation training sequence vector quantization 
compare model classical xed order variable memory length markov models data sets di erent memory stochastic components 
fixed order markov models mms fail large data sets advantage allowing variable memory length exploited 
department computer science engineering university technology bratislava slovakia department medical cybernetics arti cial intelligence university vienna vienna austria data sets predictive models superior comparable performance vlmms construction fully automatic shown problematic case vlmms 
data set vlmms outperformed classical mms 
set models perform signi cantly better mms 
remaining data set classical mms outperform variable context length strategies 
keywords variable memory length markov models iterative function systems fractal geometry chaotic sequences dna sequences volatility prediction running head predicting discrete sequences fractal representations past statistical modeling complex sequences fundamental goal machine learning due wide variety applications ron singer tishby genetics speech recognition nadas nance uhlmann brillinger 
models sequences generated stationary sources assuming particular underlying mechanistic system markov models mms nite order uhlmann wyner 
implicit assumption nite memory process 
statistical models de ne rich families sequence distributions give ecient procedures generating sequences computing probabilities 
mms hard estimate due familiar explosive increase number free parameters yielding highly variable estimates increasing model order 
consequently low order mms considered practical applications 
approaches proposed literature ron singer tishby laird saul nadas rissanen weinberger rissanen feder willems shtarkov uhlmann wyner overcome curse dimensionality mms share basic idea xed order mms consider variable memory length markov models vlmms deep memory just really needed ron singer tishby 
prediction contexts variable length vlmms represented prediction sux trees psts rissanen 
relevant prediction context de ned deepest node pst reached root reading input stream reversed order 
prediction sux trees constructed top ron singer tishby ron singer tishby weinberger rissanen feder bottom guyon pereira uhlmann wyner fashion 
schemes strongly depend construction parameters regulating candidate context selection growing pruning decisions uhlmann wyner guyon pereira 
appropriate values parameters derived asymptotic considerations 
practical applications parameters set modeler see quite inconvenient problematic see uhlmann 
uhlmann wyner suggest optimize construction parameters values minimization model complexity measured example akaike information criterion akaike 
study vlmm model selection uhlmann uhlmann proposes resampling strategy estimate asymptotic behavior di erent risk functions 
practical applications strategy may applicable tting individual vlmms highly time consuming 
introduce nite context predictive models similar spirit vlmms 
key idea approach geometric representation candidate prediction contexts contexts long common suxes contexts produce similar continuations mapped close contexts di erent suxes potentially di erent continuations correspond points lying far 
selection appropriate prediction contexts left vector quantizer 
dense areas spatial representation potential prediction contexts correspond contexts long common suxes attention vector quantizer 
organization sections framework nite memory sources introduce predictive models classical variable memory length markov models 
section contains detailed comparison studied model classes data sets di erent origin representing wide range grammatical statistical structure 
discussion summarizes empirical results outlines directions current research 
statistical modeling complex sequences consider sequences nite alphabet ag symbol generated stationary information sources 
sets sequences nite number symbols exactly symbols denoted respectively 
denote string empirical probability nding block denoted 
string said allowed block sequence 
set allowed blocks denoted information source weinberger rissanen feder alphabet ag de ned family probability measures blocks 
consistent measures satisfy condition ws applications useful consider probability functions consistent easy handle 
achieved example assuming nite source memory length formulating conditional measures pl ws pl called context function blocks presumably small nite set prediction contexts task learner rst nd appropriate context function estimate probability distribution data 
hand nite memory model prediction 
hand sequence generator initiating rst block letting produce continuation symbol distribution 
speci examples nite memory learners introduce novel approach constructing nite memory sources geometric representations training sequences 
denotes empty string 
fixed order markov models classical markov models mms xed order blocks relevant prediction context chosen priori length sux uv words making prediction symbol symbols relevant 
formally context function markov models mms order interpreted natural homomorphism corresponding equivalence relation blocks blocks share sux length factor set set equivalence classes blocks equivalence consists blocks mentioned large sux lengths estimation prediction probabilities infeasible 
increasing model order number probability distributions estimated rises leaving learner problem cope strong curse dimensionality 
variable length markov models curse dimensionality classical markov models lead authors develop called variable memory length markov models vlmms 
task vlmm estimation appropriate context function giving rise potentially smaller number contexts considered 
achieved permitting suxes blocks di erent lengths depending particular block brie review strategies selecting representing prediction contexts 
suppose long training sequence potential prediction context length predict symbol empirical estimates ws symbol aw prediction probability symbol aws aw respect extended context aw di ers signi cantly adding symbol past helps symbol predictions 
decision criteria suggested literature 
example extend prediction context symbol kullback leibler divergence symbol distributions candidate prediction contexts aw weighted prior distribution extended context aw exceeds threshold ron singer tishby guyon pereira aw log kl exists symbol ron singer tishby small positive construction parameters kl supplied modeler 
variants decision criteria see weinberger rissanen feder uhlmann wyner 
natural representation set prediction contexts associated symbol probabilities form prediction sux tree pst ron singer tishby rissanen 
edges pst labeled symbols internal node outgoing edge labeled symbol 
nodes pst labeled pairs string associated walk starting node root tree 
block corresponding prediction context deepest node pst reached walk labeled reversed string starting root 
algorithm building psts form ron singer tishby ron singer tishby guyon pereira initial pst single root node initial set candidate contexts fs aj grow 
pick aw remove 
add context pst growing necessary nodes provided condition holds 
provided jvj sv grow add sv depth resulting pst tree grown root leaves 
string meet criterion de nitely ruled descendants added step 
idea keep provision descendents meet selection criterion 
general values grow kl decrease size constructed pst increases 
prediction sux trees usually constructed parameter scheme introduced ron singer tishby 
scheme varies parameter kl grow case happen small values low probability subsequences included potential contexts step pst construction 
resulting psts speci greatly training sequence 
improve xing growth parameter grow small positive value varying acceptance threshold parameter kl usually removes tting ect larger psts 
smaller psts corresponding larger values kl perform poorly small xed value grow results considering unnecessarily speci contexts 
empirically procedure ratio related parameters grow kl give best results 
grow small positive construction parameter sj empty string 
variable memory length markov models vlmms usually compactly described stochastic machines sms 
brie sms nite state machines state transitions take place probabilities prescribed distribution generating process started initial state time step machine state time step moves state outputting symbol transition probability set prediction contexts encoded pst state set corresponding sm contains leaves pst plus contexts added symbol driven state transition probabilities properly de ned see ron singer tishby ron singer tishby guyon pereira 
sms representing vlmms state sets known probabilistic sux automata psa ron singer tishby weinberger rissanen feder 
vlmms emulated corresponding psts psa representations vlmms give higher processing speed 
psa longest suces precomputed states psts longest suces dynamically determined guyon pereira 
fractal prediction machines propose novel approach learning statistical structure symbolic sequences call fractal prediction machines fpms 
fpms similar spirit vlmms derive context function ecient way 
main idea fpm rst transform blocks appearing training sequence points dimensional vector metric space sux structure blocks coded cluster structure 
equivalence relation de ning context function constructed vector quantizing geometric representations allowed blocks 
way direct control number predictive contexts time avoid auxiliary construction parameters employed pst construction see section 
chaos game representations basis transformation symbolic strings points called chaos game representation cgr originally introduced je rey study dna sequences see oliver garcia oliver li 
symbolic sequences formally studied ti revealing desired properties purposes 
basis chaos game representation sequences alphabet ag iterative function system ifs barnsley consisting ane contractive maps acting dimensional unit hypercube dlog ae kx contraction coecient maps 
chaos game representation cgr sequence obtained follows ti 
start center hypercube 
plot point provided th symbol example consider sequence symbol alphabet 
ane maps unit square corresponding symbols de ned symbols associated unit square corners respectively 
map rst keep notation simple slightly abuse mathematical notation depending context regard symbols integers referring maps dxe smallest integer contract contract contract symbol symbol symbol illustration iterative function system chaos sequence representations symbolic streams 
symbol associated unique corner black unit square top left 
seeing symbol unit square contracted shifted corner associated symbol 
process iteratively repeated new symbols arrive 
increasingly longer sequences coded shrinking copies original black unit square 
construction step resulting unit square labeled sux coded black 
contracts unit square shifts corresponding corner unit square 
illustrated gure 
map black unit square top left contracted shifted lls position associated symbol 
shift vectors schematically shown corresponding symbols appearing corners unit square 
process iteratively repeated 
assume symbol 
unit square contracted time contracted shifted upper right corner unit square 
seeing symbol say result previous step contracted shifted lower right corner unit square 
note iteratively making contractions shifts ectively code history seen symbols black inside unit squares gure correspond seen strings schematically written top squares 
example black square top left gure codes state total ignorance string seen 
black inside unit square labeled corresponds strings symbol 
black unit square labeled lies corresponding strings shaded area codes strings 
likewise black region unit square labeled corresponds strings 
properties chaos game representation cgr symbolic sequences importance 
histories symbols sequences sequences share common sux points representations cgr cgr lie close 
second longer common sux shared smaller region containing points cgr cgr 
deriving appropriate context function slightly modify concept chaos game representations compute chaos block representation cbr sequence constructed plotting points chaos game representations cgr allowed blocks representation single block resulting single point de ned map blocks unit hypercube center hypercube 
maps corresponding symbols appearing blocks de ned 
obtain multi set points cbr containing geometric representations allowed blocks set cbr codes sux structure allowed blocks sense ti sux length jvj string rv dimensional hypercube side length jvj longer common sux shared blocks closer blocks mapped chaos block representation cbr hand euclidean distance points representing blocks pre length di er symbol property nding appropriate context function easily done performing vector quantization vq chaos block representation cbr training sequence vq metric space metric positions codebook vectors cvs cv representing subset points cbr closer metric cv error substituting cvs points represent minimal 
words cvs tend represent points cbr lying close metric 
distance function consider distance jx euclidean distance xd compared metric metric sensitive smaller distances emphasizing larger ones 
vector close geometric representations completely di erent blocks may lie close 
happens example blocks alphabet geometrically represented iterative function system acting 
remedy may lower contraction ratio issue optimal contraction ratio respect training sequence vector quantizer currently investigated 
quantization metrics positions cvs median mean respectively set points represent 
classical markov models de ne prediction context function equivalence blocks time equivalence reads blocks class images map eq 
represented codebook vector 
case set prediction contexts identi ed set codebook vectors fb refer predictive models context function fractal prediction machines fpms prediction probabilities determined number blocks ua training sequence point eq 
allocated codebook vector fpm construction summarize described fractal prediction machines constructed follows 
calculate chaos block representation cbr training sequence containing point representations eq 
allowed blocks 
partition hypercube regions vm running vector quantizer set cbr 
regions metric space note fpms depend cluster density geometric block representations controlled contraction parameter see eq 

smaller yield dense clusters 
furthermore quantization geometric representations controlled magni cation factor ritter schulten bauer der herrmann vector quantization scheme 
magni cation factor relates asymptotic considerations frequency codebook vectors quantized region frequency block representations region 
nd formal relationship contraction factor magni cation factor vector quantizer dynamics fpm context transitions 
related issues currently investigation 
voronoi compartments aurenhammer codebook vectors fx min points allocated codebook vector 
set counters zero 
code block point increment counter 
prediction context codebook vector associate symbol probabilities experiments compared fractal prediction machines fpms classical variable memory length markov models referred mm vlmm pst prediction sux tree respectively 
experiments performed data sets various origin di erent levels subsequence distribution structure 
data sets comprise classical symbolic sequences studied previously dna sequences text sequences bible sequences obtained quantizing chaotic time series known deep complex structure quantized laser data feigenbaum sequence ties events measure zero points land border compartments broken index order sequence derived quantizing time series real world stochastic process historical dow jones industrial average 
choosing data sets aim demonstrate fpms outperform classical xed order exible variable order markov models 
time demonstrate feasibility transforming continuous time series symbolic streams subsequently mms vlmms fpms learn structure 
quantizing real valued time series symbolic streams understood useful information reduction technique symbolic dynamics 
certain conditions stochastic symbolic models quantized chaotic time series represent natural compact way basic topological metric memory structure underlying real valued trajectories see eld young 
analogous ideas context stochastic real valued time series put forward uhlmann 
introduces new class hybrid real valued symbolic models called quantized variable length markov chains describes class real valued stochastic processes 
roughly vlmms constructed quantized sequences step distribution de ned mixture local say gaussian densities corresponding individual partition elements symbols 
mixture weights correspond symbol probabilities symbolic model vlmm 
uhlmann proves key results 
class constitutes representational basis stationary real valued processes 
particular class weakly dense set stationary appropriate partition function symbols nding optimal maximum likelihood setting achieved exclusively nding optimal underlying vlmm symbolic level 
modeling quantized time series great importance 
quantization approach ective study nancial time series modeling ti 
see uhlmann giles lawrence tsoi papageorgiou 
experimental setup experiments constructed fpms contraction coecient see eq 
means clustering macqueen buhmann norms vector quantization tool 
psts representing vlmms constructed kullback leibler criterion eq 

dna coding vs non coding regions data methods dna alphabet consists symbols purposes correspond symbols respectively 
rst experiment classi ed dna sequences coding non coding classes 
contrast non coding sequences coding dna strands contain protein coding genes 
locating coding genes necessary step dna analysis 
model class classi cation module consists models coding expert built coding sequences non coding expert built non coding ones 
presentation unseen dna sequence classi cation module decision probabilities assigned sequence experts 
dna sequences short subsequences allowed uniform subsequence distribution 
models studied xed order markov models perform experiment 
collected large data set vertebrate dna sequences test gene structure prediction programs 
data set extracted portion coding sequences coding training set di erent portion coding sequences coding test set 
applies non coding sequences 
training test sets consisted coding non coding sequences 
length sequences ranged 
maximal memory depth set 
account triplet structure www es evaluation index html coding genes 
model class model size built di erent models coding regions constructed coding training set intergenic regions constructed non coding training set 
tested model performance calculating normalized negative log likelihood nnl models test sequences 
model pair classi es test sequence coding nnl achieved coding expert lower non coding expert 
sequence classi ed non coding 
likelihood calculated follows 
denote empirical block frequency counts nite memory source built probability model initiated rst block assigns continuation pm js jc likelihood sequence respect model determined pm pl pm js normalized negative log likelihood calculated log pm normalized negative log likelihood measures amount statistical surprise induced model ron singer tishby 
results classi cation results summarized contingency table containing items true positives tp number coding sequences correctly classi ed coding true negatives tn number non coding sequences correctly classi ed non coding false positives fp number non coding sequences incorrectly classi ed coding false negatives fn number coding sequences incorrectly classi ed noncoding 
base logarithm number symbols alphabet contingency table performance measures calculated hit rate hr proportion correctly classi ed sequences hr tp tn tp tn fp fn sensitivity sen proportion coding sequences correctly classi ed coding sen tp tp fn speci city sp proportion non coding sequences correctly classi ed non coding sp tn tn fp correlation coecient cc pearson product moment correlation coecient particular case binary variables cc tp 
tn fn 
fp tp fn 
tn fp 
tp fp 
tn fn cc alternative measure prediction accuracy cc corresponds perfect prediction cc expected random prediction 
classi cation results summarized tables 
experiment fpms perform worse vlmms vlmms achieve performance classical mms 
mcnemar test everitt level test signi cance model performance di erences 
psts built xed growth strategy grow perform signi cantly better fpms comparable size 
size psts controlled indirectly construction parameters pst experts coding non coding pairs approximately size 
mms signi cantly outperform norm norm fpms psts built ratio grow kl schemes 
classical mms dicult beat experiment sux structure dna strands uniform 
gure show geometric representations blocks coding non coding training sequences 
compared geometric representations blocks laser feigenbaum sequences gures table classi cation results fpms dna experiment 
models classify unseen strings dna coding positive class non coding negative class sequences 
hit rate sensitivity speci city percentages 
column signif collects signi cance results mcnemar test level applied pairs classi ers comparable number free parameters mean classi er signi cantly worse corresponding markov model xed growth pst classi er respectively marks signi cance dots appear model pair corresponding size exist 
model contexts hit rate sensitivity speci city corr 
coef 
signif fpm 





fpm 




table classi cation results mms psts dna experiment 
psts constructed parameter xed growth parameter grow ratio grow kl schemes identi ed pst pst fg pst respectively 
sizes pst classi ers shown sizes coding non coding pst experts respectively 
details see previous table 
model contexts hit rate sensitivity speci city corr 
coef 
signif pst pst fg 
pst mm 


cbr coding dna cbr noncoding dna geometric chaos block representations cbr blocks dna coding left non coding right training sequences 
structure dna blocks norm vector quantizers place codebook approximately uniform grid similar formed mms 
fpms constructed perfectly uniform square grid mimic corresponding mm 
poorer performance fpms caused deviations codebooks regular grids 
distribution allowed blocks dna sequences chaotic laser sequence section subtle special self similar feigenbaum subsequence metric structure section 
small construction parameter values parameter ratio pst construction schemes prone tting best pst results achieved xed growth parameter grow construction 
bible data methods second experiment tested model experiments ron singer tishby language data bible ron singer tishby 
alphabet english letters blank character symbols 
trained classical mms vlmm books bible book genesis 
models evaluated basis normalized negative log likelihood eq 
unseen portion characters book genesis 
constructing pst ron singer tishby set maximal memory depth 
built pst nodes 
compared likelihood results model obtained ron singer tishby mms vlmms 
training test sets ron singer tishby 
vlmm set maximal memory length 
fpms constructed vector quantizing norms dimensional geometric representation blocks appearing training set 
results nnl results test set shown gure 
pst fpms clearly outperform mms 
fpms appear perform slightly better pst 
unfortunately able expand experiment giving results various pst sizes construction schemes 
training sequence contains approximately symbols alphabet characters 
ultrasparc workstation fpm experiments nished days 
reproduce pst reported ron singer tishby 
pst construction procedures worked extremely slow recall maximal memory depth set alphabet symbols resulted small psts 
months computation able nd suitable parameters alphabet symbols contexts nnl mms fpms pst test text genesis mm fpm fpm pst normalized negative log likelihoods nnl achieved nite context sources unseen text book genesis 
test sequence ron singer tishby 
mm pst results reproduced ron singer tishby 
yield series psts size 
respect speed self organizing character fpm construction proved great advantage 
laser chaotic regime data methods third experiment trained models sequence quantized activity changes laser chaotic regime 
deterministic chaotic dynamical systems usually organize behavior chaotic attractors containing regions di erent levels instability sensitivity small perturbations initial conditions measured local lyapunov exponents 
periods relatively predictable behavior followed periods unpredictable development due nite precision measuring devices computing machines 
quantizing chaotic trajectory symbolic stream symbol corresponds region state space system evolves technique known symbolic dynamics obtain rough picture basic topological metric memory structure trajectories see 
relatively predictable subsequences having various levels memory structure followed highly unpredictable events usually requiring deep memory 
example experiment chaotic laser produces periods oscillations increasing amplitude followed sudden dicult predict activity collapses see gure 
model sequences simple class stochastic models studied nite context sources need vary memory depth respect context 
exactly thing variable memory length models 
data set long sequence fd di erences successive activations real laser chaotic regime 
sequence fd quantized symbolic stream fs symbols corresponding low high positive negative laser activity change taken www cs colorado edu andreas time series santafe html normal extreme normal extreme parameters correspond percent percent sample quantiles respectively 
number positive di erences approximately negative di erences 
upper lower laser activation increases decreases sample considered extremal lower upper laser activation increases decreases viewed normal 
quantile set 
shows portion rst laser activations histogram di erences successive activations 
dotted vertical lines show cut values corresponding quantiles respectively 
rst symbols remaining symbols laser symbolic sequence formed training test sequences respectively 
constructing nite context sources mms vlmms fpms training sequence maximal memory depth set evaluated normalized negative log likelihood nnl see eq test sequence respect tted models 
results results shown gure 
classical mms order outperformed fpms comparable number contexts 
di erence performances fpms constructed norm norm procedures experiment tried vector quantization techniques classical kohonen selforganizing feature maps sofm kohonen sofm star topology neuron eld ti dynamic cell structures sommer deterministic annealing hierarchical clustering rose gurewitz fox 
got model performances comparable models obtained means clustering 
clustering deterministic annealing took enormous time apparent improvement resulting predictive models 
laser activity histogram activity diff activity diff left rst activations laser chaotic regime 
right histogram di erences successive activations 
dotted vertical lines show cut values corresponding quantiles respectively 
symbols corresponding quantization regions appear top gure 
nnl fpms mms laser data mm fpm fpm contexts nnl psts laser data pst pst fg pst pst pst normalized negative log likelihoods nnl laser test sequence respect nite context sources built laser training sequence 
markov models indicated mm 
fpms corresponding norm norm constructions indicated fpm fpm respectively 
psts constructed parameter scheme psts build xed growth parameter grow psts constructed ratio related growth threshold parameters grow kl indicated pst pst fg pst respectively 
discussed section small values kl grow parameter pst construction scheme lead including low probability subsequences potential prediction pst contexts 
results psts greatly tting training sequence line indicated pst gure 
line indicated pst fg traces achieved xed growth parameter grow pst construction scheme 
acceptance threshold parameter kl varied 
tting ect larger psts disappeared smaller psts corresponding larger values kl perform poorly xed small value grow resulted considering unnecessarily speci contexts 
show results procedure constructing psts ratio related parameters grow kl lines indicated pst 
small kl ratio value low prevent tting ect 
psts constructed ratios achieve performances comparable fpms 
experiment demonstrates vlmm construction highly dependent construction parameters parameter scheme ron singer tishby may result speci models strongly tting training sequence 
fpms hand constructed simply enlarging codebook vector quantization phase show deterioration performance increasing number prediction contexts illustrate di erence xed order variable context length markov models plot gure geometric representations blocks appearing training sequence see eq 
geometric representations prediction contexts mm pst fpms comparable size approximately contexts 
geometric representations training sequence blocks shown dots upper left part gure 
geometric representations prediction contexts th order mm shown circles lower right blindly cover unit square regardless actual block distribution training sequence 
prediction contexts vlmm pst constructed ratio related parameters contexts cbr laser data fpm contexts pst contexts mm contexts chaos block representations cbr blocks laser training sequence upper left prediction contexts fpms upper right pst lower left mm lower right 
chaos block representations prediction contexts shown circles contexts norm constructed fpm shown crosses 
grow kl suxes allowed blocks geometric representations prediction contexts concentrate areas inhabited representations allowed blocks see section 
context selection criteria favor prediction contexts probability exceeds acceptance threshold grow symbol probabilities signi cantly di er extended contexts 
result lower left gure sort conditional vector quantization geometric representations training sequence blocks aim cover set accepted allowed blocks set prediction contexts account associated symbol probabilities 
fpm contexts shown upper right gure correspond codebooks constructed vector quantization circles crosses norms 
feigenbaum sequence data methods fourth experiment applied models feigenbaum binary sequence strict topological metric organization allowed subsequences see 

sequence obtained quantizing time series resulting known logistic equation chaotic regime respect sign negative non negative 
highly specialized deep prediction contexts needed model sequence 
classical markov models succeed full power admitting limited number variable length contexts exploited 
sequence studied symbolic dynamics number interesting properties 
topological structure sequence structure allowed blocks regarding probabilities described context sensitive tool restricted indexed context free grammar eld young 
second block length distribution blocks uniform just probability levels 
third block distributions organized self similar fashion freund ebeling 
transition ranked distributions block lengths 

achieved rescaling horizontal rank relative frequency plots self similar rank ordered block distributions feigenbaum sequence di erent block lengths indicated numbers plots 
self similarity relates block distributions block lengths 

connected arrows 
vertical axis factor respectively 
plots feigenbaum sequence block distributions seen gure 
numbers plots indicate corresponding block lengths 
arrows connect distributions scaling self similarity relationship 
sequence speci ed composition rule chose feigenbaum sequence increasingly accurate modeling sequence nite memory models requires selective mechanism deep prediction contexts 
created large portion feigenbaum sequence trained series classical mms variable memory length mms vlmms fractal prediction machines fpms rst symbols 
symbols formed test set 
maximum memory length vlmms fpms set 
results due special metric structure feigenbaum sequence block length block distribution uniform just probability levels issues concerning growth parameter grow pst construction prominent previous experiment relevant 
report just vlmm results corresponding parameter pst construction scheme 
constructing series increasingly complex vlmms varying construction parameter appeared troublesome task 
previous experiment pst construction procedure smoothly varying construction parameter 
experienced highly non regular behavior intervals parameter values yielding unchanged psts tiny regions parameter space corresponding large spectrum pst sizes 
impossible simply iteratively change parameters small amount save resulting psts done previous experiment spent fair amount time nd critical parameter values 
contrast fully automatic construction fpms involved sliding window length training set window position mapping block appearing window point eq 
vector quantizing norms resulting set points codebook vectors 
quantization step computed predictive probabilities eq 

analogous gure previous experiment 
dimensional geometric representations training sequence blocks form dense separated clusters 
case vector quantization norms gives identical codebooks norm fpm constructions yielded results 
variable context length models quickly grasp structure allowed blocks 
rigid xed order mms specializing deeper contexts spare resources cover missing subsequences 
normalized negative log likelihoods nnl eq test set computed tted models exhibited step increasing tendency shown table 
investigated ability models reproduce block distribution training test sets 
done letting models generate sequences length alphabet symbols train seq 
fpm fpm pst mm cbr feigenbaum sequence prediction contexts dimensional chaos block representations blocks binary feigenbaum training sequence bottom 
shown geometric representations prediction contexts fpms pst mm approximately prediction contexts 
representations prediction contexts shown circles contexts norm constructed fpm shown crosses 
table normalized negative log likelihoods nnl feigenbaum test set 
model contexts nnl captured block distribution fpm pst mm equal length training sequence block length computing distance block distribution training sequences 
block distributions test training sets virtually 
table show block lengths distance exceed small threshold 
set 
experiment distance exceeded large amount 
classical mm totally fails experiment context length far small enable mm mimic complicated subsequence structure feigenbaum sequence 
fpms vlmms quickly learn explore limited number deep prediction contexts perform comparatively 
explanation step behavior log likelihood block modeling behavior vlmms fpms scope 
detailed analysis see ti dor ner 
brie mention combining knowledge topological metric structures feigenbaum sequence 
freund ebeling careful analysis models show inclusion prediction context leads abrupt improve ment modeling performance 
fact show vlmms fpms constitute increasingly better approximations nite self similar feigenbaum machine known symbolic dynamics eld young 
financial data data methods nal data set consisted quantized daily volatility changes dow jones industrial average djia feb april transformed time series returns log log predictive models predict direction volatility move day 
ti show quantization symbol approach volatility prediction outperform traditional econometric models arch garch families bollerslev 
financial time series known highly stochastic relatively shallow memory structure 
addition account stationarity nancial time series daily values usually kept short 
case dicult beat low order classical mms 
perform better mms developing deeper specialized contexts hand easily lead tting 
considered squared return volatility estimate day volatility change forecasts volatility going increase decrease historical returns interpreted buying selling signal option market straddle see 
noh engle kane 
volatility decreases go short straddle sold increases take long position straddle bought 
respect quality volatility model measured percentage correctly predicted directions daily volatility di erences 
series returns fr transformed series fd di erences successive squared returns partitioned series fd daily volatility moves non overlapping intervals containing values spanning approximately years 
interval partitioned training set returns djia series returns percent djia february till april 
solid vertical lines indicate division intervals dotted vertical line interval indicates split training validation sets 
rst values training set interval forms test set previous interval 
rst values validation set remaining values 
series returns djia seen gure 
solid vertical lines indicate division intervals dotted vertical line interval indicates split training validation sets 
interval predictive models trained training set candidate models selected validation set selected candidate models tested test set formed rst values training set interval 
way got partially overlapping epochs series fd containing values spanning approximately years 
training sets epochs overlap 
applies test validation sets 
epoch transformed training series fd daily volatility di erences sequence symbols quantile technique laser data experiment see eq 

quantile validation test sets quantized cut values determined training set 
maximum memory length vlmms fpms set weeks 
trained classical mms psts fpms various numbers prediction contexts extremal event quantiles 
model class model size quantile test set selected validation set performance 
performance models quanti ed percentage correct guesses volatility change direction day 
symbol sum conditional symbol probabilities model greater model guess considered correct 
results epochs test set performances models selected validation sets shown gure 
subjected di erences model performances epochs parametric non parametric wilcoxon paired signi cance tests 
results signi cance tests summarized table 
tests reveal fpms signi cantly outperform vlmms 
norm fpms perform signi cantly better mms 
tests suggest mms signi cantly outperform psts constructed parameter scheme 
restricting test mms appear signi cantly better pst scheme 
experiment illustrates practical problems tting vlmms 
training sequences experiment relatively short symbols approximately years 
considering stationarity issues hardly substantially larger 
addition nancial time series known highly stochastic relatively shallow memory structure 
pst construction schemes develop specialized prediction contexts small psts 
case validation set strategy completely prevent psts overestimating memory structure data 
epoch correct daily djia volatility diff fpm fpm mm pst prediction performance hit rates mms psts fpms epochs quantized daily volatility moves djia 
fpms constructed norm procedures indicated fpm fpm respectively 
performances psts constructed parameter solid line xed growth parameter grow dashed line ratio grow kl dotted line schemes identical 
table tests signi cance model performances epochs djia experiment 
item reports results test model corresponding row signi cantly outperforms model associated column signi cance suggested parametric non parametric wilcoxon paired tests marked respectively 
double star plus means signi cance level single star plus corresponds signi cance level means signi cance 
psts constructed parameter xed growth parameter grow ratio grow kl schemes denoted pst pst fg pst respectively 
model fpm fpm pst pst fg pst mm fpm fpm pst pst fg pst mm discussion experiments fractal prediction machines fpms performed variable memory length markov models vlmms 
exception dna sequence due uniform distribution contexts classical markov models mms favored 
case fpms performed worse vlmms 
remaining cases fpms outperformed classical mms showed decisive advantage vlmms respect model performance ease construction case bible text computational demands vlmm revealed 
contrast fpms eciently estimated data set variety numbers contexts case quantized laser data experiments pointed severe estimation algorithm vlmms fpms proved robust ective case feigenbaum sequence fpms achieved level performance measured negative log likelihood vlmm construction fpms troublesome 
case quantized nancial data fpms signi cantly outperformed vlmms mainly due availability short training sequences rendering estimation vlmm dicult 
summary experiments demonstrate fractal prediction machines ecient viable candidate learning statistical structure symbolic sequences classical markov models appropriate due existence deep structure involving relevant contexts 
main advantages approach self organizing character constructing series fractal predictive models fractal prediction machines fpms increasing size 
vector quantization covers geometric block representations training sequence increasingly large codebooks natural self organized manner 
predictive models constructed codebooks compared model selection criterion validation set performance 
constructing series increasingly large models enter model selection phase important issue attained little attention vlmm literature 
practical applications larger alphabets long sequences constructing set potential candidate vlmms take prohibitively long time 
results vlmm literature usually tted models stressing memory requirement advantage vlmms classical mms 
little said particular model selected set potential candidates dicult arrive solution see example ron singer tishby ron singer tishby 
guyon pereira study ways constructing increasingly complex vlmms increasing source memory construction parameters kept xed xing long source memory gradually changing single parameter keeping construction parameters xed 
scheme experimentally shown yield superior performance guyon pereira 
noted guyon pereira construct series increasingly complex vlmms large set ap news corpus containing characters setting maximal memory depth 
shallow memory construction enabled authors construct series vlmms realistic time 
larger memory lengths lead exponential increase pst construction time 
addition mentioned section construction parameters selection non intuitive task may require lot interactive steps 
respect fpm construction intuitive number codebook vectors directly corresponds number predictive contexts easier automate growth predictive models directed codebook growth self organizing quantization algorithms faster 
compare memory depth ron singer tishby bible experiment illustrated laser data experiment considering di erent vlmm parameter selection strategies lead completely di erent learning scenarios 
contrast simple fpm construction free defects 
interestingly gives similar results norm fpm algorithms variable memory length strategies better classical xed order markov models signi cant sux structure long allowed blocks training sequence explainable considering pre de ned relatively small sux length 
natural language demonstrated bible experiment example situation 
example provided feigenbaum sequence 
dna sequences stand opposite uniform sux structure 
case dicult outperform classical mms 
better performance achieved specialized models incorporating priori knowledge gene structure expressed hidden markov model topology krogh similarity searches respect known amino acid sequences 
allowing variable memory length double edged sword 
especially shorter sequences relative alphabet size variable memory length model construction specializes overly deep prediction contexts small model sizes 
shown dow jones industrial average experiment case model selection strategies eliminate overlearning ects 
fair note fpms emerge experiments potentially interesting favorable alternatives vlmms far lack sound theoretical background comparable supporting vlmms ron singer tishby weinberger rissanen feder uhlmann wyner 
proceeding direction theoretically analyzed multifractal properties basis predictive models construction geometric block representation ti relationship chaos block representation contraction factor magni cation factor vector quantizer dynamics fpm context anonymous reviewers suggesting norm fpm construction scheme transitions 
wish owen kelly helpful comments variable length markov models anonymous reviewers helpful suggestions 
supported austrian science fund fwf research project adaptive information systems modeling economics management science sfb 
austrian research institute arti cial intelligence supported austrian federal ministry science transport 
akaike 

new look statistical model identi cation 
ieee transactions automatic control 
aurenhammer 

voronoi diagrams survey fundamental geometric data structure 
acm computing surveys 
barnsley 

fractals 
new york academic press 
bauer der herrmann 

controlling magni cation factor self organizing feature maps 
neural computation 
bollerslev 

generalized autoregressive conditional heteroscedasticity 
journal econometrics 
brillinger 

examples scienti problems data analysis neurophysiology 
comp 
graph 
statistics 
sommer 

dynamic cell structure learns perfectly topology preserving map 
neural computation 

evaluation gene structure prediction programs 
genomics 
uhlmann 

extreme events return volume process discretization approach complexity reduction 
applied financial economics 
uhlmann 

dynamic adaptive partitioning nonlinear time series 
biometrika 
uhlmann wyner 

variable length markov chains 
annals statistics 
uhlmann 

model selection variable length markov chains tuning context algorithm 
annals institute statistical mathematics press 
buhmann 

learning data clustering 
arbib 
eds handbook brain theory neural networks 
bradford books mit press 
eld young 

computation onset chaos 
zurek 
eds complexity entropy physics information sfi studies sciences complexity pp 

reading ma addison wesley 
everitt 

analysis contingency tables 
london chapman hall 
freund ebeling 

self similar sequences universal scaling dynamical entropies 
physical review 
giles lawrence tsoi 

rule inference financial prediction recurrent neural networks 
proceedings conference computational intelligence financial engineering pp 

new york city ny 
guyon pereira 

design linguistic postprocessor variable memory length markov models 
proceedings international conference document analysis recognition pp 

montreal canada ieee computer society press 


chaos generic economic data 
int 
journal bifurcation chaos 
je rey 

chaos game representation gene structure 
nucleic acids research 


modern theory dynamical systems 
cambridge uk cambridge university press 


mathematical foundations information theory 
new york dover publications 
kohonen 

self organizing map 
proceedings ieee 
krogh 

methods improving performance hmm application gene nding 
proceedings th international conference intelligent systems molecular biology pp 

menlo park ca aaai press 
laird saul 

discrete sequence prediction applications 
machine learning 
macqueen 

methods classi cation analysis multivariate observations 
proceedings th berkeley symposium mathematical statistics probability pp 
berkeley ca university california press 
nadas 

estimation probabilities language model ibm speech recognition system 
ieee trans 
assp 
noh engle kane 

forecasting volatility option prices index 
journal derivatives 
papageorgiou 

mixed memory markov models time series analysis 
proceedings conference computational intelligence financial engineering pp 

new york city ny 


finding words unexpected frequencies acid sequences 
journal royal statistical society 
rissanen 

universal data compression system 
ieee trans 
inform 
theory 
ritter schulten 

stationary state kohonen self organizing sensory mapping 
biol 
cybern 
ron singer tishby 

power amnesia 
advances neural information processing systems pp 

morgan kaufmann 
ron singer tishby 

power amnesia 
machine learning 
rose gurewitz fox 

statistical mechanics phase transitions clustering 
physical review letters 
ti 

learning extracting initial mealy machines modular neural network model 
neural computation 
ti dor ner 

recurrent neural networks iterated function systems dynamics 
international icsc ifac symposium neural computation pp 

ti 

extracting nite state representations recurrent neural networks trained chaotic symbolic sequences 
ieee transactions neural networks 
ti 

spatial representation symbolic sequences iterative function systems 
ieee transactions systems man cybernetics part systems humans 
ti dor ner ch 

understanding state space organization recurrent neural networks iterative function systems dynamics 
wermter sun eds hybrid neural symbolic integration pp 

heidelberg springer verlag 
ti ch dor ner 

symbolic dynamics approach volatility prediction 
appear abu mostafa lebaron lo weigend eds computational finance 
cambridge ma mit press 
weinberger rissanen feder 

universal nite memory source 
ieee transactions information theory 
willems shtarkov 

context tree weighting method basic properties 
ieee trans info theory 

