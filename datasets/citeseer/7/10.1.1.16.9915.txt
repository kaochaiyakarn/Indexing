neural networks april invited article bayesian approach neural networks review case studies lampinen aki vehtari laboratory computational engineering helsinki university technology box fin hut espoo finland mail 
lampinen hut fi aki 
vehtari hut fi give short review bayesian approach neural network learning demonstrate advantages approach real applications 
discuss bayesian approach emphasis role prior knowledge bayesian models classical error minimization approaches 
generalization capability statistical model classical bayesian ultimately prior assumptions 
bayesian approach permits propagation uncertainty quantities unknown assumptions model may generally valid easier guess problem 
case problems studied include regression classification inverse problem 
thoroughly analyzed regression problem best models restrictive priors 
emphasizes major advantage bayesian approach forced guess attributes unknown number degrees freedom model non linearity model respect input variable exact form distribution model residuals 
keywords bayesian data analysis hierarchical models neural networks comparison models lampinen vehtari bayesian approach neural networks review case studies bayesian data analysis uncertain quantities modeled probability distributions inference performed constructing posterior conditional probabilities unobserved variables interest observed data sample prior assumptions 
bayesian data analysis berger bernardo smith gelman 
neural networks bayesian approach pioneered buntine weigend mackay neal reviewed mackay neat bishop 
neural networks main difficulty model building controlling complexity model 
known optimal number degrees freedom model depends number training samples amount noise samples complexity underlying function estimated 
standard neural networks techniques means determining correct model complexity setting network desired complexity crude computationally expensive 
bayesian approach issues handled natural consistent way 
unknown degree complexity handled defining vague non informative priors hyperparameters determine model complexity resulting model averaged model complexities weighted posterior probability data sample 
model allowed different complexity different parts model grouping parameters exchangeable identical role model common hyperparameter 
addition assumed complexities probably similar hierarchical hyperprior defined variance hyperparameters groups 
problem standard neural network methods lack tools analyzing results confidence intervals results 
bayesian analysis yields posterior predictive distributions variables interest making computation confidence intervals possible 
contribution discuss bayesian approach statistical modeling section emphasis role prior knowledge modeling process 
section give short review bayesian mlp models mcmc techniques marginalization 
real world modeling problems assess performance bayesian mlp models compare performance standard neural networks methods statistical methods 
application problems regression problem predicting quality concrete concrete manufacturing process section approximating inverse mapping tomographic image reconstruction problem section iii classification problem recognizing tree trunks forest scenes section 
discuss experiments relation related studies bayesian neural networks 
bayesian approach key principle bayesian approach construct posterior probability distributions unknown entities model data sample 
model marginal distributions constructed entities interested variables study 
parameters parametric models predictions non parametric regression classification tasks 
posterior probabilities requires explicit definition prior probabilities parameters 
posterior probability parameters model gf data bayes rule io ih oid gf likelihood parameters prior probability normalizing constant called evidence model gr 
term gf denotes hypotheses assumptions defining model choice mlp network specific noise model results conditioned assumptions clear prefer term gf explicitly equations 
notation normalization term directly understandable marginal probability data lampinen vehtari bayesian approach neural networks review case studies conditional gf integrated chosen assumptions prior comprise dlo 
having models likelihood model comparing probabilities models term evidence model 
widely bayesian model choice method models bayes factors see kass raftery 
common notation bayes formula gf dropped easily causes denominator kind probability obtaining data studied problem prior probability data modeling 
role prior statistical models describing prior information explicitly distinguishes bayesian approach maximum likelihood ml methods 
important notice role prior knowledge equally important approach including maximum likelihood 
basically generalization prior knowledge discussed training samples provide information points prior knowledge provides necessary link training samples measured samples 
important free lunch nfl theorems proven help understand issue 
wolpert wolpert shows class approximating functions limited learning algorithm procedure choosing approximating function readily perform worse better randomly measured training set ots error averaged loss functions 
theorem implies possible find learning algorithm universally better random 
words assume priori learning algorithm learn training data generalize training set samples 
wolpert macready wolpert cross validation cv method model selection analyzed depth shown nfl theorem applies cv 
basic result papers priors functions choosing function cv performs average random algorithm anti cv function largest cv error chosen 
practice means cv choose large infinite set models guarantee generalization 
easy understand intuitively situation chosen algorithm happens minimize error training set set algorithms large high chance fitting overfitted solution exists set 
noted due computational limitations cv practice choose models typically thousands choice models imposes strict prior functions 
nfl theorems invalidate cv practical model selection 
implications principal emphasizing priori selection plausible solutions necessary cross validation model selection respect crossvalidation provide alternative require prior knowledge modeling 
practice statistical models parametric models neural networks probably contain strict priors little prior knowledge 
example discrete choice model gaussian noise model represents infinite amount prior information 
finite amount information correspond probability gaussian noise model probability zero alternatives 
functional form model may predetermined polynomial fitting number degrees freedom may fixed neural networks trained error minimization 
large amount prior information maximum likelihood models model parameters determined solely data maximize likelihood diw minimize negative log likelihood error function 
goodness prior knowledge separates bad ml models 
bayesian approach certain part prior knowledge specified explicitly form prior distributions model parameters parameters prior distributions 
complex models neural networks relation actual domain knowledge experts priors model parameters simple may practice difficult incorporate sophisticated background information models priors parameters 
lampinen vehtari bayesian approach neural networks review case studies considerable advantage bayesian approach gives principled way inference prior knowledge lacking vague forced guess values attributes unknown 
done marginalization integrating posterior distribution unknown variables explained detail section 
lot done find non informative priors specify complete lack knowledge parameter value 
approaches uniform priors jeffreys prior jeffreys priors berger bernardo 
see kass wasserman review yang berger large catalog different non informative priors various statistical models 
bayesians non informative priors referred objective bayesian approach contrast informative subjective priors reflect subjective opinions model builder 
light nfl theorems requires hypothesis space constrained contains sufficient amount prior information needed able learn generalizing model 
non informative priors fixed guessed choices moved higher levels hierarchical models 
goel degroot shown hierarchical models training data contains information hyperparameters higher hierarchy prior posterior hyperparameters equal 
models sensitive choices higher levels implying higher level priors general informative subjective 
way hierarchical prior structure specify partial lack knowledge controllable way 
example difficult choose gaussian longer tailed noise model include prediction model non informative uniform prior noise models posterior probabilities noise models determined objectively match noise distribution realized model residuals 
section example student distribution unknown number degrees freedom noise model comprising near gaussian longer tailed distributions integrating posterior distribution predictions 
advice design hierarchical prior structures robust noise models gelman 
typical attribute difficult guess advance complex statistical models correct number degrees freedom depends number training samples distribution noise samples complexity underlying phenomenon modeled 
general complexity model defined number total number degrees freedom models multiple dimension complexity 
bayesian approach vague prior total complexity called effective number parameters hierarchical prior structure allow different complexity different parts model 
example parameters may assigned different groups group parameters assumed hyperparameter different groups different hyperparameters 
hyperprior defined explain distribution hyperparameters 
section discuss detail example type called automatic relevance determination prior 
approximations marginalization principle marginalization principle leads complex integrals solved closed form multitude approaches differ integrals approximated 
closest ml approach maximum map approach posterior distribution parameters considered parameters sought maximize posterior probability wid oc diw minimize negative log posterior cost function log diw log 
weight decay regularization example technique gaussian prior weights negative log prior 
main drawback approach gives tools setting hyperparameters due lack marginalization nuisance parameters 
weight decay example variance term guessed set external procedure cross validation 
degree bayesian principle utilized empirical bayesian approach specific values estimated hyperparameters 
mlp networks approach introduced mackay evidence framework mackay called type maximum likelihood approach berger lampinen vehtari bayesian approach neural networks review case studies practical bayesian method neural networks 
evidence framework hyperparameters set values maximize evidence model io marginal probability data hyperparameters integrated parameters lo diw ic dw 
gaussian approximation posterior parameters facilitate closed form integration resulting posterior specified mean gaussian approximation network posterior mean weights 
full bayesian approach fixed values estimated parameters hyperparameters 
approximations needed integrations hyperparameters obtain posterior parameters parameters obtain predictions model shown eq 

correctness inference depends accuracy integration method depends problem approximation method appropriate 
methods approximating integrations neural network models include markov chain monte carlo techniques numerical integration discussed detail section ensemble learning barber bishop aims approximate posterior distribution minimizing kullback leibler divergence true posterior parametric approximating distribution approximations jordan approximating integration tractable problem mean field approach winther problem simplified neglecting certain dependencies random variables 
worth noticing full hierarchical bayesian models large amounts fixed prior knowledge selection parametric form distributions priors noise models uncertain assumptions 
models guesses exact values parameters smoothness coefficients hyperparameters guesses exact forms distributions 
goodness model depends guesses practical applications necessary carefully validate models bayesian posterior analysis gelman cross validation gelfand vehtari lampinen 
implies practice bayesian approach sensitive prior assumptions classical methods 
discussed detail section 
bayesian learning mlp networks give short overview bayesian approach neural networks 
concentrate mlp networks markov chain monte carlo methods computing integrations approach introduced neal 
detailed treatment neal describes flexible bayesian modeling fbm software package main tool case problems reviewed 
result bayesian modeling conditional probability distribution unobserved variables interest observed data 
bayesian mlp natural variables predictions model new inputs posterior distribution network weights rarely interest 
posterior predictive distribution output new input new training data xo yo 
obtained integrating predictions model respect distribution model xnew xnew denotes model parameters hyperparameters prior structures 
probability model measurements contains chosen approximation functions noise models 
defines likelihood part posterior probability term oc io 
probability model regression problem additive error www cs toronto edu radford fbm software html lampinen vehtari bayesian approach neural networks review case studies mlp function tanh 
denotes parameters hidden layer weights biases output layer weights biases respectively 
random variable model residual 
multivariate problems outputs handled changing output eq 
vector having common residual model outputs completely separate models hierarchical model common parts common hidden layer separate output weights common hierarchical noise model 
class classification problems probability binary valued target value computed logistic transformation exp class classification problems probability class target value computed softmax transformation cross entropy exp ix exp fk residual models notation shorthand rla denotes parameters distribution random variable argument shown explicitly 
commonly gaussian noise model rr tz cr denotes normal distribution mean tz variance cr 
choosing hyperprior cr may knowledge somewhat informative prior 
example minimum reasonable value noise variance estimated measurement accuracy repeated experiments 
hyperprior informative non informative convenient choose form distribution accordance method sample posterior distribution 
note results general sensitive choices hyperprior level discussed section confirmed studies see rasmussen :10.1.1.17.729
checked serious analysis especially form prior needs compromised reasons computational convenience 
framework study see section hyperparameters sampled gibbs sampling 
convenient priors conjugate distributions produce full conditional posteriors form 
variance gaussian conjugate distribution inverse gamma producing prior cr inv gamma parametrization inv gamma cr cr cr exp vcr cr equal scaled inverse chi square distribution gelman appendix 
parameter number degrees freedom scale parameter 
parametrization prior equivalent having prior measurements averaged squared deviation cr 
fixed values cr chosen produce vague prior cr reasonably flat range parameter values plausibly arise 
cr similar neal 
lampinen analyzed multivariate regression problem residuals outputs may correlated 
multivariate normal residual model full matrix conjugate hyperprior inverse wishart distribution allowing gibbs sampling matrix 
noise model eq 
noise variance cr assumed sample 
regression vehtari bayesian approach neural networks review case studies problems sample different noise variance cr variances governed common prior corresponding noise model yn xn cr inv gamma cr inv gamma fixed hyperparameters vo cr vo aw 
prior spread variances cr average variance determined vo fixed 
parametrization residual model equal student distribution fixed degrees freedom 
allow higher probability models similar noise variances hyperparameter vo hyperprior models similar variances large vo corresponding tight prior spread variances cr giving high probability realized variance 
approximately distribution noise model unknown degrees freedom 
similar treatment results assume normal residuals different variances common longer tailed distribution residual model geweke 
preferable leads simpler noise models discussed detail 
problems noise variance functionally dependent explanatory variables typically subset model inputs model noise variance xn noise inv gamma cr vo fixed cr vo 
see bishop example input dependent noise model separate mlp model estimate dependence noise variance inputs 
practical problems gaussian residual model applicable 
may error sources non gaussian density target function may contain peaks training data sufficient estimate data different noise variances sample 
gaussian residual model samples exceptionally large residuals handled outliers pre manual manipulation data 
option longer tailed residual model allows small portion samples large errors 
model laplace double exponential distribution 
appropriate form residual distribution known advance correct bayesian treatment integrate priori plausible forms 
study student distribution tails controlled choosing number degrees freedom distribution 
number difficult guess advance set hierarchical prior prediction integrate posterior distribution data 
tails determined fit model data 
integration degrees freedom done gibbs sampling see section discretized values residual model tv ud inv gamma cr denotes set values step ud uniform distribution integer values discretization chosen equal prior value results roughly truncated exponential prior geweke spiegelhalter 
simple way sample discretization metropolis hastings algorithm hastings gave experiments equal results slightly slower convergence 
lampinen vehtari bayesian approach neural networks review case studies priors model parameters typical prior assumptions theory related smoothness approximation 
tikhonov bishop widely method inverse problems functions large derivatives chosen order penalized 
mlp model minimizing curvature second derivative bishop training derivatives target values lampinen leads complex treatise partial derivatives non linear models depend inputs weights 
convenient commonly prior distribution gaussian linear models directly related model derivatives complex interpretation non linear mlp case discussed section 
gaussian priors weights ab aw ab variance hyperparameters 
conjugate inverse gamma hyperprior aj inv gamma va similarly gaussian noise model 
fixed values highest level hyperparameters case studies similar neal 
appropriate depend somewhat network topology 
discussed neal average weights assumed smaller feeding units hyperprior scaled number inputs typical values wi wl 
automatic relevance determination prior importance inputs section discuss simple hierarchical prior mlp weights called automatic relevance determination ard mackay neal 
ard group weights connected input 
common variance hyperparameters weight groups different hyperparameters 
example ard prior study wkj ak inv gamma inv gamma average scale aa determined level hyperparameters similar fashion noise model example 
ard evidence framework aa estimated maximum likelihood priors penalty large variability 
fixed values case studies aw corresponding vague aa determined data 
hyperparameter hyperprior similar fashion noise model example 
ard prior proposed automatic method determining relevance inputs mackay neal irrelevant inputs smaller weights connections hidden units important weights 
separate hyperparameters weights irrelevant inputs tighter priors reduces weights effectively zero having common larger variance input weights 
lampinen vehtari bayesian approach neural networks review case studies determining relevance inputs great importance practical modeling problems choosing inputs models analyzing final model 
see general discussion ways assess importance inputs non linear models 
common notions importance predictive importance increase generalization error variable omitted model causal importance change model outputs caused change input variable 
note causal importance directly measurable inputs uncorrelated inputs manipulated independently related causality relations actual system modeled 
ard relevance measure input related size weights connected input 
linear models weights define partial derivatives output respect inputs equal predictive importance input case non correlated inputs causal importance 
non linear neural networks situation complex small weights layer compensated large weights layers nonlinearity hidden units changes effect input way depends inputs 
illustrate effect ard prior consider mlp linear output layer 
th order partial derivative mapping xk wkj th derivative constraining layer weights largest effect higher order derivatives th order polynomial term wkj may partly explain success weight decay reg type prior effective smoothing prior 
hand produce linear mapping small high order derivatives layer weights need small sigmoids operate linear part second layer weights correspondingly larger 
layer weights measure derivative linear relation matter important network may contain direct output weights account linear relation neal ard coefficients weights comparable ard coefficients hidden layer weights 
note adding input output weights model identifiable may slow convergence mcmc considerably neal 
simple example demonstrate non linearity input largest effect relevance score ard predictive causal importance 
target function additive function inputs see fig 
equal predictive importance input 
network weights evidence approximation mackay shown fig 
easy see weights connected inputs linear transformation smallest 
fig 
shows predictive importance mean absolute values second order derivatives output respect input relevance estimates ard posterior standard deviation gaussian prior distributions weight group 
example illustrates inputs large linear effect low relevance measures ard 
reason cautious ard choose remove inputs models rank variables importance analysis model 
note ard favorable prior demonstrated case studies contribution strict assumption input weight groups variance non linearity 
variance assumed ard informative probably correct prior 
markov chain monte carlo method neal introduced mcmc implementation bayesian learning mlps neal 
basic mcmc methods applications statistical data analysis gilks theoretical treatment robert casella 
mcmc complex integrals marginalization approximated drawing samples joint lampinen vehtari bayesian approach neural networks review case studies 

latent var 
latent var 
latent var 
latent var 
latent var 
latent var 
example ard importance inputs 
target function additive function inputs 
plots show univariate transformations inputs 
predictive importance input equal rmse terms latent functions scaled equal variance uniform input distribution hidden output layer weights input hidden layer weights hidden units network weights test function fig 
estimated evidence framework mackay 
true leave input error mlp ard mean abs derivative nd mlp ard mean abs derivative mlp ar weight std input variable different measures importance inputs test function fig 

note ard coefficients closer second derivatives derivatives local causal importance error due leaving input predictive importance 
lampinen vehtari bayesian approach neural networks review case studies probability distribution model parameters hyperparameters 
example squared error loss best guess model prediction additive zero mean noise model corresponds expectation posterior predictive distribution eq new xnew new 
approximated sample values drawn posterior distribution parameters new xnew 
note samples posterior distribution drawn learning phase may computationally expensive predictions new data calculated quickly stored samples eq 

mcmc samples generated markov chain desired posterior distribution stationary distribution 
framework introduced neal hybrid monte carlo hmc algorithm duane sampling parameters gibbs sampling geman geman hyperparameters 
possible sampling schemes see rios de freitas 
hmc elaborate monte carlo method efficient gradient information reduce random walk behavior 
gradient indicates direction go find states high probability 
detailed description algorithm repeated see neal list web page fbm software 
gibbs sampling parameter turn sampled full conditional distribution parameter parameters data 
example hyperparameter eq 
sampled id rr 
iw done efficiently prior chosen conjugate distribution 
gibbs sampling main sampling method bugs system spiegelhalter bayesian modeling tool convenient experimenting hierarchical bayesian models 
amount data increases evidence data causes probability mass concentrate smaller area need samples posterior distribution 
fewer samples needed evaluate mean predictive distribution tail quantiles quantiles 
depending problem hundreds samples may practical purposes 
note due autocorrelations markov chain getting near independent samples converged chain may require tens thousands samples chain may require hours cpu time standard workstation 
convergence diagnostics visual inspection trends potential scale reduction method gelman 
alternative convergence diagnostics reviewed brooks roberts robert casella 
see discussion choice starting values number chains 
choosing initial values early stopping reduce burn time chain reached equilibrium distribution 
general author experience suggests convergence mcmc methods mlp slower usually assumed published studies mcmc chains may burn stage producing sort early stopping effect selection model complexity 
sensitivity bayesian approach prior distributions explained bayesian approach averaging probable models probability computed chosen distributions noise models parameters approach may sensitive www mrc cam ac uk bugs lampinen vehtari bayesian approach neural networks review case studies bayesian mlp target data prediction error bars mlp esc target data prediction error bars test function demonstrating sensitivity bayesian mlp early stopped committee mlp esc wrong noise model 
shows sample noise realization resulting predictions bayesian mlp left mlp esc right 
see text explanation error bars 
bad guesses distributions classical methods model selection carried external procedure cross validation fewer assumptions mainly assumption training validation sets correlated 
respect bayesian models overfitted terms classical model fitting produce complex models small posterior estimates noise variance 
check assumptions bayesian models carry modeling simple classical methods linear models early stopped committees mlps 
bayesian model gives inferior results measured test set cross validated assumptions questionable 
computer simulation illustrates sensitivity bayesian approach correctness noise model compared early stopped committee esc robust method case studies 
basic early stopping statistically inefficient sensitive initial conditions weights part available data train model 
limitations easily alleviated committee early stopping mlps different partitioning data training stopping sets mlp krogh vedelsby 
caution early stopping committee baseline method mlps 
target function data shown fig 

modeling test repeated times different realizations gaussian laplacian double exponential noise 
model mlp gaussian noise model 
shows sample noise resulting predictions 
error bars confidence intervals predicted conditional mean output input measurement noise included limits 
esc intervals simply computed separately value networks 
computing confidence limits early stopped committees straightforward simple ad hoc method gives similar results bayesian mlp treatment 
summary experiment shown table 
paired test esc significantly better bayesian model noise model wrong 
simple problem methods equal correct noise model 
correct bayesian approach integrating noise models explained section shown practice case problem section course trouble example 
implication issue practical applications bayesian approach usually requires expert standard approach devise reasonable assumptions distributions include different options models integrate done results experience consistently better approaches 
lampinen vehtari bayesian approach neural networks review case studies table demonstration sensitivity bayesian mlp mlp esc wrong noise model 
models noise model gaussian actual noise gaussian laplacian double exponential 
statistical significance difference tested pairwise test shown value probability observing equal larger error means methods equal 
errors rms errors prediction true target function 
bayesian mlp mlp esc significance noise rmse rmse difference tailed gaussian laplacian case regression task concrete quality estimation section report results bayesian mlps regression concrete quality estimation problem 
goal project develop model predicting quality properties concrete part large quality control program industrial partner project 
quality variables included compression strengths densities days casting bleeding water extraction spread air measure properties fresh concrete 
quality measurements depend properties stone material natural crushed size shape distributions grains composition additives amount cement water 
study explanatory variables 
collecting samples statistical modeling expensive application sample requires preparation sand mixture casting test pieces waiting days final tests 
study samples designed cover practical range variables collected concrete manufacturing 
small sample problems selection correct model complexity important needs done finer resolution problems large amounts data 
hierarchical bayesian models tempting alternative 
study mlp networks containing hidden units chosen coarse experiments indicated size network contained sufficient surplus degrees freedom compared required number 
method early stopping committee mlp networks different division data training stopping sets member 
networks initialized near zero weights guarantee mapping smooth 
report results variable air measures volume percentage air concrete 
air positive skewed distribution mean median logarithmic transformation variable 
ensures allows simpler additive noise models case nearly exponentially distributed variable 
performance models estimated fold cross validation 
compare methods paired test cv 
method exhibits somewhat elevated probability type error suggest difference difference exists low type error difference exists analyzed dietterich 
methods early stopped committee mlp esc gaussian process model neal non parametric regression method priors imposed directly correlation function resulting approximation 
gp approach viable alternative mlp models problems training sample size large 
estimated prediction errors table 
column noise model letter indicates normal noise model tv student distribution unknown degrees freedom respectively 
mcmc sampling basic models done fbm software sampling model tv noise distribution done matlab code derived netlab toolbox 
posterior values table correspond long tailed distribution model residuals 
ard tails longer reduction weights near zero corresponds simpler models larger residuals samples 
www ncrg aston ac uk netlab lampinen vehtari bayesian approach neural networks review case studies table performance comparison various mlp models gaussian process gp model predicting air variable concrete manufacturing 
rmse values show standardized model residuals relative standard deviation data 
see table pairwise comparison models 
method noise model ard rmse std 
mlp esc 
bayesian mlp 
bayesian mlp tv 
bayesian mlp 
bayesian mlp tv 
gaussian process table posterior analysis number degrees freedom distribution residual model 
table shows posterior mean quantiles method noise model ard bayesian mlp tv bayesian mlp table shows values pairwise comparisons methods obtained paired tests 
note test quantity comparing predictive performance models tables rms error long tailed residual models allow large errors cost rms error function 
serves posterior model checking rmse relevant error measure application want sure long tails model residuals led 
general making residual model flexible shifts posterior mass simpler priori probable models high likelihood obtained matching residual model realized residuals 
results listed 
best models gaussian process gp model bayesian mlp ard student tv distribution unknown degrees freedom noise model equal performance 
best models flexible informative priors 
mlp models tv noise model outperformed gaussian noise models confidence level mlp tv ard confidence level early stopped committee mlp esc basic bayesian mlp gaussian noise model differ significantly 
just adding bayesian treatment basic model help application possibility informative hierarchical priors utilized 
adding ard bayesian model significantly better just longer tail noise model tv ard bayesian model better esc mlp confidence level fig 
shows distribution ard coefficients model 
variable names disclosed request industrial partner project 
variable indices printed bold face correspond variables chosen final model 
selection done manually aided backward elimination technique 
computational burden bayesian approach case study heavy 
example sampling best model ard distributed noise model unknown degrees freedom took hours cpu time mhz compaq alpha workstation 
matlab implementation roughly lampinen vehtari bayesian approach neural networks review case studies table pairwise comparisons various mlp models predicting air variable 
values matrix values obtained paired tests 
values rounded nearest number percent value indicates value 
values reported column winning method 
looking row wise see methods performed method row 
comparison method noise model ard 




bayesian mlp 
tv 
bayesian mlp 
bayesian mlp tv 
gaussian process ard coefficients log median posterior distribution ard coefficients standard deviation gaussian prior input weights model 
variable indices printed bold face chosen final model 
vehtari bayesian approach neural networks review case studies times slower corresponding native implementation fbm software run simpler models main hmc parameters length individual chains step size neal heuristic step size adjustment persistence parameter window length windowing 
burn stage contained chains actual sampling chains samples stored posterior analysis comparison number iterations corresponds error models iterations training 
find values hyperparameters cross validation discretization possible values hyperparameter require amount cpu time note fold cv error require iterations just ard hyperparameters input weights practically impossible search cv 
simple forward selection inputs model fixed required cv error evaluations requiring order magnitude cpu time bayesian model bayesian approach expensive computational load complex real world problems cheap alternatives review results variable study similar results obtained variables showing worked bayesian model realistic noise models priors consistently best model measured cross validation bayesian mlp gaussian process model similar performance target variables choice application matter convenience hierarchical linear models tested problem preliminary results clearly inferior basic mlp models mlp esc bayesian mlp normal noise model models analyzed reported 
preliminary tests performed evidence framework re estimating algorithm hyperparameters mackay converge realistic network sizes expected known gaussian approximation posterior hold number samples small 
case ii inverse problem electrical impedance tomography section report results bayesian mlps solving ill posed inverse problem electrical impedance tomography eit 
full report proposed approach lampinen 
aim eit recover internal structure object surface measurements 
number electrodes attached surface object current patterns injected electrodes resulting potentials measured 
inverse problem eit estimating conductivity distribution surface potentials known severely ill posed methods obtain feasible results 
fig 
shows simulated example eit problem 
volume bounded circles image represent gas bubble floating liquid 
conductance gas lower liquid producing curves shown fig 
shows resulting potential signals image recovered 
lampinen proposed novel feedforward solution reconstruction problem 
approach computing principal component decomposition potential signals eigenimages bubble distribution autocorrelation model bubbles 
input mlp projection potential signals principal components mlp gives coefficients reconstructing image weighted sum eigenimages 
projection potentials images eigenspace reduces correlations input output data network actual inverse problem representation potential signals image data 
reconstruction principal components dimensional potential signal eigenimages resolution pixels 
training data consisted simulated bubble formations overlapping circular bubbles image 
compute reconstructions mlps containing hidden units 
models tested mlp esc bayesian mlp see section 
input projection ard prior difference results verified preliminary tests model ard prior full tests 
lampinen vehtari bayesian approach neural networks review case studies example eit measurement 
simulated bubble formation bounded circles 
current injected electrode lightest color opposite electrode grounded 
gray level contour curves show resulting poten tial field 
injection electrode relative changes potentials compared homogeneous background 
curves correspond injections different electrodes 
table errors reconstructing bubble shape estimating void fraction reconstructed images 
see text explanation models 
method classification error relative error vf relative error direct vf tv inverse mlp esc bayesian mlp method study iterative inversion eit forward model total variation see information 
approach conductivity distribution sought minimize cost function defined squared difference measured potentials potentials computed conductivity distribution forward model 
minimization carried method requiring iteration steps 
known bubbles background constant total variation 
penalty function total sum absolute differences adjacent area elements forcing solution smoother penalizing abrupt changes total change monotonic curve equal independent steepness contrast say squared differences pull solution low gradient solutions 
fig 
shows examples image reconstruction results 
table shows quality image reconstructions measured error void fraction percentage erroneous pixels segmentation test set 
important goal process tomography application estimate void fraction proportion gas liquid image 
proposed approach goal variables estimated directly explicit reconstruction image 
column table shows relative absolute error estimating void fraction directly projections potential signals 
solving real problems non linear learning models ability assess confidence output necessary 
fig 
shows scatter plot void fraction versus estimate bayesian mlp confidence intervals 
quantiles computed directly posterior predictive distribution model output 
void fraction large forward model non linear current lampinen vehtari bayesian approach neural networks review case studies example image reconstructions mlp esc upper row bayesian mlp middle row tv inverse lower row 
mlp plots actual bubble shown gray blob contour detected bubble black line 
tv inverse estimated bubble shown gray blob actual bubbles upper images 
non conducting disturbances inverse problem ill posed especially disturbances far electrodes 
ambiguity clearly visible confidence intervals ci wide model may large errors 
model simulated data promising results preliminary experi ments real data 
case iii classification task forest scene analysis section report results bayesian mlp classification forest scenes 
objective project assess accuracy estimating volumes growing trees digital images 
locate tree trunks initialize fitting trunk contour model classification image pixels tree non tree classes necessary 
main problem task large variance classes 
appearance tree trunks varies color texture due varying lighting conditions gray black white birch species dependent variations pine bark color ranging dark brown orange 
non tree class diversity larger containing terrain tree branches sky 
diversity difficult choose optimal features classification 
reviewed large number potentially useful features 
total features gabor filters orientations frequencies generic features related shape texture common statistical features texture filters color histogram features 
due large number features classifier methods suffer curse dimensionality 
results case demonstrate bayesian mlp competitive high dimensional problem 
total images collected ordinary digital camera varying weather conditions 
labeling image data done hand identifying types tree background image blocks different textures lighting conditions 
study pines considered 
estimate classification errors different methods fold cross validation error estimate pictures training left error evaluation scheme repeated times 
mlp models contained hidden units logistic output layer 
tested models lampinen vehtari bayesian approach neural networks review case studies void fraction target scatterplot void fraction estimate 
table cv error estimates forest scene classification 
see text explanation different models 
classification std comparison method error 


cart 
knn loocv 

bayesian mlp 
bayesian mlp ard knn loocv nearest neighbor classification chosen leave cross validation cart classification regression tree breiman 
cv error estimates collected table 
differences significant partly due having fold cross validation different images different error rates 
causes extra variance classification results cv reduces significance differences variance comes variations task variations methods 
mlp models clearly win methods best method bayesian mlp ard just slightly better mlp models fig 
shows example image classified different methods 
visually bayesian mlp ard gives spurious detections methods 
ard reduces effect features weakly correlating classification larger windows robust features dominate 
hand causes classifier thin trunks parts trunks clearly visible 
www cs utoronto ca delve methods knn class home html lampinen vehtari bayesian approach neural networks review case studies forest scene knn cart mlp esc bayes mlp bayes mlp ard examples classified forest scene 
see text explanation different models 
discussion reviewed bayesian approach neural networks concentrating mlp model mcmc approximation computing marginal distribution variables study joint posterior unknown variables data 
real applications assessed performance bayesian approach compared methods 
important advantage bayesian approach case studies possibility handle situation prior knowledge lacking vague forced guess values attributes unknown 
best bayesian models restrictive hierarchical priors bayesian approach inherently subjective selection prior probabilities final bayesian models subjective corresponding classical error minimization methods 
example subjectively guess advance number degrees freedom models distribution model residuals degree complexity non linearity model respect input variable pre fixed maximum likelihood models 
ways handle selection total model complexity degree classical approach early stopped committee mlp models hierarchical models handle difficult issues characteristic bayesian approach 
important issue bayesian modeling sensitivity results prior assumptions 
goodness model measured probability model data prior assumptions incorrect assumptions yield wrong models respect reality having large probabilities 
bayesian approach sensitive bad guesses classical approach choice best method partly different assumptions choice model parameters cross validation models 
practical applications bayesian approach usually requires expert standard approach devise reasonable assumptions distributions include different options models integrate correct bayesian approach 
earlier published studies comparable include mackay neal rasmussen williams neal penny roberts :10.1.1.17.729
cited studies include regression classification cases 
classification likelihood model usually contains hyperparameters regression problems noise model crucial part solution 
cited studies noise model normal distribution fixed shape 
authors lampinen vehtari bayesian approach neural networks review case studies knowledge comparable studies regarding noise models bayesian mlp published earlier 
bayesian models issues discussed gelman spiegelhalter 
mackay mlp ard normal noise model evidence framework real regression problem 
longer tailed noise model possible outliers omitted hand 
evaluation evidence problematic results improved validation set choose best models committee 
ard errors significantly increased 
model won ashrae prediction competition significant margin 
slightly modified ard ranking ard hyperparameters manually predetermined performed evidence framework real case problem 
normal noise model 
penny roberts evidence framework assessed artificial real problems 
performance comparable best alternative methods accordance results experiments empirical bayesian approach somewhat inferior full bayesian approach studies basic model gaussian noise similar bit better mlp esc 
main ard rarely useful case irrelevant inputs contradiction results 
quite possible hierarchical ard prior performs considerably better full bayesian approach evidence approach 
rest cited studies similar mcmc method full bayesian approach 
neal mlp model ard distribution noise model performed better alternative methods real regression problem 
performance ard normal noise model compared 
real classification problem significant benefit ard 
convergence problems suspected 
artificial problems irrelevant inputs ard improved results 
rasmussen mlp gp models ard normal noise model performed better models linear model knn mars mlp esc real artifical regression problems :10.1.1.17.729
williams evidence framework compared mcmc method real classification problem 
evidence framework performed smaller training sets full data set performance difference significant 
number data points gets smaller number weights mlp gaussian approximation evidence framework 
case problem ard prior change classification results significantly 
benchmark problems studied neal 
problems regression tasks normal noise model 
main proper comparison alternative method mlp esc apparent lack convergence runs mcmc serious flaw noise model benchmark tasks 
cases mcmc probably converged noise model correct bayesian approach performed better mlp esc model ard prior generally performed bit better corresponding model ard 
classification tasks studied 
main results equal better alternative evidence non bayesian methods results sensitive exact values highest level hyperprior parameters performance ard controversial ard improved results cases considerably deteriorated results cases 
possible reason authors note convergence mcmc chains ard slower results may non equilibrium states 
accordance results 
mcmc diagnostics ensure convergence ard gave worse results increased burn time 
summarize bayesian approach mlp networks mcmc approximation marginalization gave better results alternative non bayesian methods case problems 
general best models informative priors underlining importance explicitly specifying lack knowledge guessing values attributes unknown 
emphasized results data analysis depend assumptions approximations bayesian approach automatically give better results classical approach 
bayesian models need validation data set model complexity validation final model essential modeling approach 
vehtari bayesian approach neural networks review case studies acknowledgments study partly funded project promise applications probabilistic modeling search graduate school electronics telecommunications automation 
authors providing expertise case study aiding problem setup providing tv inverse method case study 
bibliography barber bishop 

learning bayesian neural networks 
bishop editor neural networks machine learning volume nato series computer systems sciences pages 
springer verlag 
berger 

statistical decision theory bayesian analysis 
springer series statistics 
springer nd edition 
berger bernardo 

development priors 
bernardo berger dawid smith editors bayesian statistics pages 
oxford university press 
bernardo smith 

bayesian theory 
john wiley sons 
bishop 

curvature driven smoothing learning algorithm feed forward networks 
eee transactions neural networks 
bishop 

neural networks pattern recognition 
oxford university press 
bishop 

regression input dependent noise bayesian treatment 
mozer jordan petsche editors advances neural information processing systems pages 
mit press 
breiman friedman olshen stone 

classification regression trees 
chapman hall 
brooks roberts 

assessing convergence markov chain monte carlo algorithms 
statistics computing 
buntine weigend 

bayesian back propagation 
complex systems 
de freitas niranjan gee doucet 

sequential monte carlo methods train neural network models 
neural computation 
dietterich 

approximate statistical tests comparing supervised classification learning algorithms 
neural computation 
duane kennedy 

hybrid monte carlo 
physics letters 


predictive sample reuse method applications 
journal american statistical association 
gelfand 

model determination sampling methods 
gilks richardson spiegelhalter editors markov chain monte carlo practice pages 
chapman hall 
gelman 

inference monitoring convergence 
gilks richardson spiegelhalter editors markov chain monte carlo practice pages 
chapman hall 
gelman carlin stern rubin 

bayesian data analysis 
texts statistical science 
chapman hall 
vehtari bayesian approach neural networks review case studies geman geman 

stochastic relaxation gibbs distributions bayesian restoration images 
eee transactions pattern analysis machine intelligence 
geweke 

bayesian treatment independent student linear model 
journal applied econometrics supplement 
gilks richardson spiegelhalter editors 
markov chain monte carlo practice 
chapman hall 
goel degroot 

information hyperparameters hierarchical models 
journal american statistical association 
hastings 

monte carlo sampling methods markov chains applications 
biometrika 
penny roberts 

empirical evaluation bayesian sampling neural classifiers 
niklasson boden ziemke editors proceedings th international conference artificial neural networks pages 
springer 
jeffreys 

theory probability 
oxford university press rd edition 
jordan ghahramani jaakkola saul 

methods graphical models 
jordan editor learning graphical models volume nato science series behavioural social sciences 
kluwer academic publishers 
kass raftery 

bayes factors 
journal american statistical association 
kass wasserman 

selection prior distributions formal rules 
journal american statistical association 
krogh vedelsby 

neural network ensembles cross validation active learning 
tesauro touretzky leen editors advances neural information processing systems pages 
mit press 
lampinen 

background knowledge multilayer perceptron learning 
visa editors sc proceedings loth scandinavian conference image analysis volume pages 
pattern recognition society finland 
lampinen vehtari 

bayesian neural network solve inverse problem electrical impedance tomography 
johansen editors sc proceedings th scandinavian conference image analysis volume pages 
pattern recognition society denmark 


prior information generalized questions 
technical report aim massachusetts institute technology artificial intelligence laboratory center biological computational learning department brain cognitive sciences 


bayesian field theory 
technical report ms tp institut theoretische physik 
mackay 

practical bayesian framework backpropagation networks 
neural computation 
mackay 

bayesian non linear modelling prediction competition 
ashrae transactions pt pages 
ashrae 
mackay 

probable networks plausible predictions review practical bayesian methods supervised neural networks 
network computation neural systems 
vehtari bayesian approach neural networks review case studies rios 

issues bayesian analysis neural network models 
neural computation 
neat 

bayesian training backpropagation networks hybrid monte carlo method 
technical report crg tr dept computer science university toronto 
neal 

bayesian learning neural networks volume lecture notes statistics 
springerverlag 
neat 

assessing relevance determination methods delve 
bishop editor neural networks machine learning volume nato series computer systems sciences pages 
springer verlag 
neat 

regression classification gaussian process priors discussion 
bernardo berger dawid smith editors bayesian statistics pages 
oxford university press 
penny roberts 

bayesian neural networks classification useful evidence framework 
neural networks 
rasmussen 

evaluation gaussian processes methods non linear regression 
phd thesis department computer science university toronto 
robert casella 

monte carlo statistical methods 
springer text statistics 
springerverlag 


measure importance inputs 
online 
technical report sas institute cary nc usa 
revised june 
available ftp ftp sas com pub neural importance 
spiegelhalter thomas best gilks 

bugs examples volume version 
mrc biostatistics unit institute public health 


review bayesian neural networks application near infrared spectroscopy 
eee transactions neural networks 
karjalainen 

electrical impedance tomography basis constraints 
inverse problems 
vehtari lampinen 

bayesian neural networks classify forest scenes 
editor intelligent robots computer vision algorithms techniques active vision volume proceedings sp pages 
sp vehtari lampinen 

bayesian neural networks correlating residuals 
proceedings international joint conference neural networks cd rom number 
eee 
vehtari lampinen 

bayesian model choice cross validation predictive densities 
technical report laboratory computational engineering helsinki university technology 
vehtari lampinen 

mcmc sampling bayesian mlp neural networks 
amari giles gori editors proceedings international joint conference neural networks volume pages 
eee 
williams 

bayesian neural networks classify segmented images 
proceedings lee fifth international conference artificial neural networks number conference publications pages 
institution electrical engineers 
winther 

bayesian mean neural networks gaussian processes 
phd thesis university copenhagen 
vehtari bayesian approach neural networks review case studies wolpert 

existence priori distinctions learning algorithms 
neural computation 
wolpert 

lack priori distinctions learning algorithms 
neural computation 
wolpert macready 

free lunch theorems search 
technical report tr santa fe institute 
yang berger 

catalog noninformative priors 
isds discussion institute statistics decision sciences duke university 
