parallel evaluation multi join queries wilschut jan peter apers university twente box ae enschede netherlands phone apers cs utwente nl number execution strategies parallel evaluation multi join queries proposed literature performance evaluated simulation 
give comparative performance evaluation execution strategies implementing parallel database system prisma db 
experiments done processors 
basic strategy determine execution schedule minimum total cost parallelize schedule execution strategies 
strategies coming literature named sequential parallel synchronous execution segmented right deep full parallel 
experiments clear guidelines strategy 
years research done design implementation performance parallel dbmss 
teradata bubba bac hc brg gamma dgs xprs examples systems implemented papers written performance 
performance evaluation systems mainly limited simple queries involve join operations 
developments direction support nonstandard applications complex data models availability high level interfaces tend generate complex queries may contain larger numbers joins relations 
consequently development execution strategies parallel evaluation multi join queries drawn attention scientific community 
number strategies proposed hos scd performance evaluated simulation 
comparative experimental performance evaluation available 
describes proposed strategies common framework 
strategies implemented prisma db comparative performance evaluation done 
results yield clear guidelines choice strategy 
published proceedings acm sigmod san jose california usa implementation platform experiments 
prisma db full fledged parallel relational dbms abf 
fully functional prototype running node multiprocessor machine 
research various directions gre wi wfa 
potential system parallelism large number processors possibility implement wide variety parallel execution strategies study parallelization multi join queries 
prisma db mainmemory dbms experiments described refer main memory context 
concluding section discusses applicability results disk systems 
optimization multi join queries system sac pioneer area optimization multi join queries centralized environment 
system join trees restricted linear trees available access structures inner join operand optimally exploited 
system chooses cheapest sense minimal total costs linear tree contain cartesian products 
subsequently remarked kbz restriction linear trees may choice parallel systems 
space possible join trees large restriction linear trees dropped 
lst swg partially heuristic algorithms proposed aim limiting time spent searching space possible query trees cheapest 
proposes parallelize search 
papers cost formula evaluates total costs query tree influence parallelism account 
obviously optimizing response time complex query sufficient optimize minimal total costs 
exploitation parallelism taken account 
search space results possible trees possible trees taken account gigantic 
overcome problems hos proposes phase optimization strategy multi join queries 
phase chooses tree lowest total execution costs second phase finds suitable parallelization tree 
researchers agree assumption adopt reasons 
reasonable assume parallelism large extent compensate increased total amount 
second schedule minimal total costs small intermediate results transmission costs parallel execution schedule low 
third phase optimization reasonable way cut optimization time 
lastly missing best execution plan big problem long assure come bad kbz 
phase phase optimization easily handled standard query optimization 
second phase finding suitable parallelization join tree subject 
organization organized follows 
section shortly introduces prisma db shows different strategies execution implemented prisma db discusses results earlier research context prisma db explain results 
section describes execution strategies multi join queries tradeoffs detail 
section describes comparative performance evaluation section summarizes discusses results 
prisma db prisma db extensively research area parallel query processing wi 
previous research followed lines 
system experiment large scale intra operation parallelism single operation queries wfa 
second theoretical study behavior pure inter operation parallelism multi join queries done 
combines lines research study inter parallelism execution multi join queries experimentation 
section describes system hardware prisma db query execution engine results previous research explain results 
system prisma db full fledged parallel main memory relational dbms designed implemented netherlands 
goal prisma project provide flexibility architecture query execution strategy enable experiments functionality performance system 
flexibility implement various strategies parallel evaluation multi join queries evaluate performance 
prisma db currently run node shared multi processor 
node consists processor mbytes memory disk communication processor 
full description design architecture implementation prisma db ame abf 
flexible query execution prisma db query execution engine prisma db consists query single scheduler multiple operation processes processor 
operation processes access data fragments stored main memory processor directly execute relational operations 
results stored local memory split sent processors processing 
pool operation processes kept alive system scheduler claim free operation processes execution query 
scheduler initializes participating operation processes relational operation executed 
coordination operation processes done operation processes 
way coordination parallelized 
extended relational algebra xra internal representation queries 
language consists normal relational operations extended primitives grouping recursive query processing 
language allows expression wide range parallel execution plans query 
relational operation executed arbitrary number processors result operation distributed efficiently arbitrary number destinations 
operations allocated explicitly processors 
way intra operator parallelism arbitrary degree achieved 
allocating independent join operations disjoint sets processors results inter operator parallelism 
inter operator pipelining implemented allocation pipelined joins disjoint sets processors 
flexibility yields possibilities implement wide variety strategies multi join queries 
results previous research parallel execution single operator queries wfa studies intra operator parallelism main memory database systems 
study concluded observed linear speedup small numbers processors extrapolated larger numbers processors 
caused fact overhead starting operations processors overhead increases increasing degree parallelism dominates actual processing time decreases increasing degree parallelism large degree parallelism 
optimal number processors appears proportional square root size operands 
consequence larger problems allow larger degree parallelism 
concluded optimal number processors parallel execution operation smaller way join tree explain parallel multi join strategies 
parallel execution strategy multi join query uses parallel hash join algorithm constituent binary joins 
apart intra operator parallelism inter operator pipelining parallelism may may 
strategies regarded differ way inter operator parallelism intra operator parallelism 
note concentrate adding inter operator parallelism 
means available processors may distributed operations join tree 
allow single processor concurrently different join operations 
strategies described detail 
way join tree example 
constituent joins tree labeled number indicates relative amounts join operations 
second join operation top needs times computation time top join operation 
note processor utilization diagrams subsections come just sake simplicity numbers identification join operations 
sequential parallel execution sp sequential parallel execution strategy simplest way evaluate multi join parallel 
strategy inter operator parallelism 
constituent joins executed sequentially parallel available processors join operation 
strategy require pipelining join operations simple hash join algorithm 
shows processor allocation indicated join sign bushy tree right deep segments 
segmented right deep execution lid contrary se segmented right deep execution uses pipelining addition intra operator parallelism 
strategy proposed inspired scd 
schneider sch scd describes differences possible parallelism left deep right deep linear join trees simple hash join individual join operations 
right deep tree join operations executed parallel probe phases executed extensive pipelining 
left deep trees hand allow parallel execution probe phase join operation build phase 
concluded study due possibilities extensive exploitation pipelining right deep trees perform better left deep trees 
results schneider extended bushy trees 
proposes see bushy tree segmented right deep tree bushy tree consists right deep segments see 
right deep segments evaluated inter operation parallelism lln terminology inner join operand build hash table called left operand outer join operand probe hash table called right operand 
full parallel evaluation example join tree 
executed parallel 
available processors distributed join operations proportionally amount operation 
operation starts working soon input available 
shows possible processor allocation example query idealized processor utilization chosen allocation 
bottom join operations start immediately processors allocated operands available base relations 
join operation labeled wait time operands start producing output see section 
top join operation may start immediately hashing left operand 
wait right operand available processor fully utilized join operation 
strategy depends cost function estimate amount join operation 
clear strategy offer perfect load balancing 
tradeoffs number barriers prevent performance gain parallelism 
general discussion issue deg 
barriers affect execution strategies introduced different way resulting number tradeoffs 
startup startup time time needed start join operations processors 
operations started startup time may dominate actual computation time see section sp strategy uses operation processes number operation processes equal product number operations join tree number processors 
fp strategy uses operation process processor 
startup overhead large sp small fp se rd middle 
coordination parallel execution multi join queries needs redistribution operands subsequent 
coordination overhead due synchronization tuple transport 
contrast wfa prisma db keeps operation processes running minimize startup overhead prisma db tuple stream sender receiver shake hands tuple transport start 
number tuple streams increases dramatically degree parallelism sender receiver sender consists operation processes receiver consists operation processes tuple streams see section 
overhead starts count large numbers processors 
sp uses processors operation sp suffers coordination overhead 
fp suffers se rd middle 
discretization error utilization diagrams clear sp achieves perfect load balancing ideal case 
caused fact processors get exactly amount assuming non skewed data partitioning 
strategies achieve perfect load balancing due discretization errors distribution operations processors 
simple example point clear 
pieces candy distribute kids get pieces get assuming chop candy chunks 
uneven distribution candy kids caused fact discrete numbers kids 
strategies papers distribute processors operations discrete entities general distribution fair due discretization errors distribution 
leads load imbalance skew 
obviously error decreases increasing ratio number processors number operations 
number processors distribute large number operations small discretization error small 
follows sp suffer discretization error sp distribute processors different operations rd se suffer moderately error subset operations share system time fp suffers available processors distributed operations 
delay pipelines rd fp exploit pipelined parallelism 
form parallelism incurs delay pipeline operation process top pipeline wait tuples arrive 
size delay depends shape query tree number join pipeline size join operands discussed section 
obviously factors affects execution strategies studied different way 
expected extent strategy affected factors depends shape query tree parallelized 
example rd expected fine right oriented trees left linear tree 
similarly se expected better bushy trees trees linear 
sp hand expected sensitive shape query tree 
experiments find tradeoffs reality 
performance evaluation stated study second phase phase optimization parallelization strategy 
phase finds join tree minimal total costs multi join second phase generates parallel execution strategy plan 
keep problem manageable decided study multi join query 
join query vary parallelization strategy number processors shape query tree size problem 
test data query join query studied performance evaluation consists relations contain equal numbers wisconsin tuples bdt 
tuples consist unique integer attributes number attributes total size bytes tuple 
relations joined integer attributes join projected second integer attributes remaining attributes operands result operation wisconsin relation equal size operands 
test problem similar problem sch 
possible join trees query total execution costs 
individual join operations equal costs sizes operands 
differences response time caused differences shape tree parallelization 
regular tree suitable study effectiveness various parallelization strategies 
test relations generated prisma data care taken correlation exists second attribute relations unique integer attributes different relations 
avoid favoring strategy decided join query start ideal data fragmentation 
means join query base relations fragmented join attribute join processors join 
join operations base relation operands need redistribution operands prior join operation 
reasonable alternative starting full fragmentation relations fragmented participating processors 
place sp special position strategy start ideal data fragmentation 
experimental setup said experiments vary parallelization strategy number processors shape query tree size problem 
parameters varied experiments 
parameter values chosen 
query shapes experiments parallelization strategies sp se rd fp 
strategies described 
problem sizes small experiment uses relations consisting tuples total tuples involved query 
large experiment uses relations consisting tuples amounting total tuples query 
sizes referred experiments 
experiment number processors varied experiment processors 
total size query large run fewer processors 
explained section expect strategies perform differently different query shapes 
especially interested difference linear bushy trees difference left right oriented trees 
query shapes query right linear right oriented long bushy wide bushy left oriented long bushy left linear tree 
generation parallel execution plans generator execution plans strategies specific join tree 
generator takes join tree cardinalities operand relations parallelization strategy number processors input yields execution plan xra output 
generator uses cost function calculate execution costs joins join tree 
decided simple cost function relative costs individual join operations join tree 
nl cardinalities join operands cardinality result cost main memory join estimated anl bn cr 
formula set operand base relation operand intermediate result 
set 
rational formula follows 
operand tuples hashed operand intermediate result retrieved network 
result tuples created send network 
formula assumes time spent single action tuple hashing retrieving network sending network order magnitude taken unity 
result tuples created sent network amounts units tuple 
operand tuples hashed retrieved network intermediate result 
amounts units operand tuple 
cost function may overly simple sense try estimate costs precisely 
example indicated parallelization query tree influences total costs operations query tree 
strategy allocates operations partially node transmission costs lower estimated 
parallelization may influence need redistribute operands joins 
principle impossible real accurate estimate costs individual join operations join tree 
experiments show cost estimate generates execution plans parallel behavior 
figures show results experiments query shapes 
figures left diagram contains results experiments right diagram contains results experiments 
figures corresponds query shape indicated 
diagrams response times seconds axis 
response times measured elapsed time moment scheduler starts scheduling query operation process finishes 
axis shows number processors 
diagram shows results strategies studied 
left linear join tree shows results left linear query trees 
linear tree independent subtrees se allocates available processors sequentially join 
way se degenerates sp linear trees 
left linear tree show right deep segments rd allocates available processors sequentially left linear query tree processors join operation 
rd degenerates sp 
diagrams show coinciding performance sp se rd experiment 
clear sp case se rd works reasonable small numbers processors performance degenerates larger numbers processors 
experiment shows effect stronger experiment 
performance degradation explained startup costs coordination overhead 
sp needs start operation process join processor 
processor case operation processes need initialized 
coordination overhead redistribution operands may large 
intermediate results 
fragments fragments generates tuple streams 
processor case operand generates tuples streams coordinated 
experiment shows extensive performance degradation experiment 
result corresponds performance results single operation queries see section 
fp execution query tree show performance gain parallelism 
experiment performance low degree parallelism sp 
caused fact fp suffers constant delay long linear pipeline 
larger number processors fp suffers delay pipeline negative effect startup coordination overhead sp stronger 
small number processors fp suffers load imbalance due discretization errors distribution processors operations 
fp performs better large number processors 
left oriented bushy join tree shows results left oriented bushy query tree 
results sp similar results left linear tree 
fits expectation sp sensitive shape query tree 
figures show similar behavior sp query shapes 
results show se rd better left linear case fp higher numbers processors 
shape query suitable rd se 
rd profits independent right deep segments short tree 
se profits independent subtrees small 
result room inter join parallelism rd se 
explains performance rd se tree sp fp 
behavior fp similar behavior linear tree close inspection data shows performance small numbers processors slightly worse linear tree 
may surprising pipeline tree shorter linear case 
result explained earlier research see section 
step bushy pipeline pipeline tree causes delay proportional size operands 
low degree parallelism operands fragment join relatively large delay step pipeline large 
higher degree parallelism operands fragment join smaller delay step pipeline smaller 
explains relatively bad behavior small numbers processors better behavior larger degree parallelism 
wide bushy join tree shows experimental results wide bushy query tree 
query tree suitable se tree wide resulting nice independent subtrees 
results show performance se 
large experiment se wins small experiment se fp 
fp performs small experiment 
caused fact operands small fp suffer sp left oriented bushy query tree sp wide bushy query tree delay pipeline 
large number operands se uses operation processes fp startup coordination overhead dominates 
previous case fp suffers pipeline delay small number processors 
results bad performance small number processors large operands explained previous case 
speedup characteristics outperform strategies performance large number processors 
rd performs better previous case tree right oriented 
sp performs similar query shapes 
right oriented bushy join tree shows results right oriented bushy tree 
behavior sp similar 
se sensitive orientation tree behavior resembles behavior 
reason fp behaves similar left oriented bushy tree 
tree suitable rd orientation right tree fairly long probe pipeline formed 
left operands pipeline processed independently parallel disjoint sets processors 
consequence rd performs best tree 
noted fp performs rd higher degrees parallelism 
right linear join tree shows results right linear tree 
results closely resemble th results rd strategy coincides expected fp 
strategies form linear pipeline join operations process parallel 
se sensitive orientation tree se coincides sp similar left linear tree 
sp right oriented bushy query tree sp right linear query tree left linear fp fp left bushy fp fp wide bushy fp se right bushy rd rd right linear fp rd best response times seconds query trees 
strategy number nodes experiment gave results parentheses 
comparison performance various query shapes shows minimal response times query shape problem sizes 
table shows cases bushy tree gives best minimal response times 
difference larger small experiment 
apparently delay pipeline linear trees gets prohibitive 
constant delay linear pipeline important small problems larger problems 
strategies suffer pipeline delay sp se processors linear trees give performance 
fp gives best performance left oriented trees 
fp best bushy experiment se experiment 
seen fp gets close se experiment 
right oriented trees 
rd works best fp comes close see figures 
disk systems experiments main memory dbms prisma db 
feel results applicable disk systems 
prisma db assumption entire database fit memory need able host data related query 
disk system small main memory small host single join operation entirety pay inter join parallelism join need share available memory resulting increased disk traffic sch 
systems sp evaluate multi join queries 
case main memory system host part join tree results evaluate fitting sub tree parallel 
case operands sub tree retrieved disk prior joining main memory techniques evaluate tree 
summary discussion reports experiences implementation strategies implementation multi join queries prisma db 
shown prisma db provides flexibility implement wide variety parallel execution strategies complex queries 
experiments processors done 
get performance improvement processors 
summarizing may drawn experiments sp works fine small number processors larger number processors startup coordination overhead get prohibitive 
number processors overhead starts dominate higher larger amount 
queries overhead large sp easiest strategy need cost function estimate costs individual join operations 
se works wide bushy trees performance longer trees 
linear trees se degenerates sp 
se works wide bushy trees performance degenerates linear trees 
rd works right oriented trees 
right linear rd degenerates fp 
left linear rd degenerates sp 
rd sensitive orientation tree se width tree 
rd works wide range query trees 
fp gives best performance entire range query shapes large numbers processors 
performance experiment small numbers processors caused discretization error delay bushy pipeline large fragment join operands 
fp mainly prohibited pipeline delay 
bushy trees overhead decreases increasing number processors 
sp lesser extent rd se prohibited startup coordination overhead increases increasing number processors 
fp expected eventually yield best performance bushy trees processors added 
bushy trees give better performance results linear trees 
results clear guidelines parallel implementation multi join queries formulated 
small number processors sequential parallel execution sp easiest best way evaluate multi join query parallel 
larger numbers processors full parallel execution fp performs quite 
se rd perform differently sized problems suitable query shapes 
fp se rd need cost function estimate costs constituent binary joins tree 
possible choose linear bushy tree equal processing costs bushy chosen bushy trees allow effective parallelization 
rd trees contain left deep segments 
possible cost penalty mirror parts query right oriented practice rd expected quite 
noted rd uses memory fp hash table needs built 
fp gives best performance results small queries 
means allows large degree parallelism relatively small queries 
caused fact overhead strategy relatively small main overhead decreases increasing number processors 
observation expect fp best job scaling larger numbers processors 
experiments reported done regular query synthetic database 
quite interesting strategies real life applications 
ame america ed proc prisma parallel database systems springer verlag new yor heidelberg berlin 
abf apers van den berg grefen kersten wilschut prisma db parallel main memory relational dbms ieee transactions knowledge data engineering december 
apers wilschut understanding large scale parallelism data management keynote te rd pdis conf austin texas usa september 
bdt bitton dewitt benchmarking database systems systematic approach proc vldb conf florence italy october november 
bac boral alexander clay copeland franklin hart smith valduriez prototyping bubba highly parallel database system ieee transactions knowledge data engineering 
brg development cross hc database computers proc th workshop france june 
dbc industrial supercomputer database machines parle par france june 
chen lo yu young segmented right deep trees execution pipelined 
proc th vldb conf vancouver canada august 
chen yu wu scheduling processor allocation parallel execution multi join queries proc th data engineering conf tempe arizona usa february 
dgs dewitt schneider hsiao rasmussen gamma database machine project ieee transactions knowledge data engineering march 
deg dewitt gray parallel database systems high performance database systems communications acm june 
gre grefen integrity control parallel database systems phd thesis university twente 
grefen wilschut prisma db user manual memorandum inf universiteit twente enschede netherlands 
hos hong stonebraker optimization parallel query execution plans xprs proc conf miami beach florida usa december 
wilschut implementation performance evaluation parallel transitive closure algorithm prisma db proc th vldb conf dublin ireland august 
chen yu parallel execution multiple pipelined hash joins proc acm sigmod minneapolis mn may 
kbz boral zaniolo optimization nonrecursive queries proc th vldb conf kyoto japan august 
valduriez zait effectiveness optimization search strategies parallel execution strategies proc th vldb conf dublin ireland august 
lst lu shan tan optimization multiway join queries parallel execution proc th vldb conf barcelona spain september 
sch schneider complex query processing multiprocessor database machines phd thesis computer sciences technical report universiteit wisconsin madison usa september 
scd schneider dewitt performance evaluation parallel join algorithms shared multiprocessor environment proc acm sigmod portland may june 
scd schneider dewitt tradeoffs processing complex join queries hashing multiprocessor database machines proc th vldb conf brisbane australia august 
sac selinger astrahan lorie price access path selection relational database management system proc acm sigmod boston usa 
spiliopoulou parallelism pipeline optimization join queries parle paris france june 
srivastava optimizing multi join queries parallel relational databases proc rid pdis conf san diego california usa january 
stonebraker katz patterson ousterhout design xprs proc th vldb conf los angeles ca august september 
swg swami gupta optimization large join queries proc acm sigmod chicago il june 
wi wilschut parallel query execution main memory database system phd thesis university twente 
wilschut apers dataflow query execution parallel main memory environment proc st pdis conf miami beach florida usa december 
wilschut apers dataflow query execution parallel main memory environment journal distributed parallel databases 

wilschut apers pipelining query execution proc international conference databases parallel architectures applications miami usa march 
wilschut apers parallel query execution prisma db proc prisma workshop parallel database systems netherlands september america ed springer verlag new york heidelberg berlin 
wfa wilschut apers parallelism main memory system performance prisma db proc th vldb conf vancouver canada august 
wig wilschut van model pipelined query execution proc mascots symposium january san diego california 
zait parallel query processing dbs proc rid pdis conf san diego california usa january 
