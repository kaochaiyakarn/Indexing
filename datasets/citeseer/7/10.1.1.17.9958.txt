design linguistic postprocessor variable memory length markov models guyon pereira bell laboratories room holmdel nj usa phone 
fax bell laboratories murray hill nj usa isabelle pereira research att com design linguistic postprocessor character recognizers 
central module system trainable variable memory length markov model vlmm predicts character variable length window past characters 
system composed finite state automata including main vlmm proper noun vlmm 
best model reported literature brown achieves bits character brown corpus 
corpus model trained times data reaches bits character times smaller parameters 
model designed handwriting recognition applications ocr problems speech recognition 
keywords linguistics finite state automata probabilities statistics statistical languages statistical grammars grammar inference regular languages handwriting recognition speech recognition markov models hidden markov models grams 
communication process receiver powerful model information source sender needs transmit information predicted model residuals 
sonhe people exploit naturally result information theory communicate cryptic handwriting 
rely fact exchanging information people sanhe language share sanhe background knowledge model information source 
bell laboratories actively investigating handwriting means human machine communication 
handwriting recognition contextual information similar trying decode encrypted message proper decoder 
machines language model approximated humans odds successful decoding better 
initial results statistical model english trained large text cor pus 
model extension variable memory length markov models vlmms ron singer tishby 
basic components probabilistic finite state au equivalent markov models variable order 
automata built set variable length character strings appearing frequently training corpus having predictive power character 
avoid overfitting problem capacity model controlled method structural risk minimization srm 
take advantage modular decomposition model cascaded probabilistic automata 
train main module factor capitalization number strings symbols proper nouns 
allows standard english corpora train main module keeping flexibility modify models capitalization numbers symbols proper nouns models needs particular applications 
modules defined composed special case approach ct 
allows graph search techniques including beam search having build explicitly composed automaton 
classical measure performance language models cross entropy test set estimates character average length shortest encoding test set encoder decoder pair access model probability estimates 
cross entropy performance measure allows comparisons code lengths obtained various coding schemes 
instance iso code requires bits character 
brown corpus classical coding schemes huffman coding lempel ziv coding bits character 
shannon human subjects estimated entropy english bits character 
trained characters ap news model tested characters brown corpus reaches bits character state machine approximately parameters 
performance achieved best model reported literature brown bits character brown corpus 
intrinsic ambiguity handwriting symbols may different meanings symbol components may grouped different ways 
language constraints needed disambiguate handwriting 
model uses word trigrams parameters estimated training set words approximately characters 
model times smaller trained times data appears competitive performance 
statement problem handwriting recognizer produces handwritten observation mis candidate interpretations iz clock clock absence language knowledge interpretations equally 
querying language model linguistic probability string pick interpretation consistent language clock 
observation probability interpretation proportional product observation likelihood linguistic probability estimate provided recognizer estimate provided language model 
literature offers choice statistical language models various levels complexity 
probabilistic finite state automata pfsa simplest statistical language models widely successful ones 
include lexicon trees tries grams markov models 
easily combined recognizer including neural networks hidden markov models 
design addresses problem getting high prediction accuracy flexibility computational resources 
modular approach provides flexibility tailor system specific applications allows weigh tradeoffs beam search techniques 
variable memory length markov models allows trade modeling accuracy memory varying single complexity parameter 
goal approach eventually outperform best reported language models keeping model practical size 
reason report results respect widely available evaluation resource brown corpus 
example deterministic graph cyclic graph tri gram model predicting character previous ones 
evolve random initial state machine produces ah ha ha ah aha hah ha ah ah hah hah acyclic graph lexicon tree containing words ah aa hha ha hah 
overview design probabilistic finite state automata system combination probabilistic finite state automata pfsa 
pfsa conveniently represented graph 
finite number states represented nodes graph 
transitions states arcs graph associated emission acceptance character finite alphabet certain probability states cr character emitted accepted 
probabilities outgoing arcs node sum 
simplest case outgoing arcs sanhe node emit accept sanhe character 
graph property called deterministic pa cr transition node uniquely determined character emitted accepted cr starting state sequence characters generated machine possible trace ambiguity full sequence states generated 
markov models grams lexicon trees examples deterministic automata see 
linguistic models deterministic graphs 
probability string characters simply product transition probabilities corresponding path convention hi ol 
example substitution model alphabet characters model substitute character produced pfsa pfsa 
composed graph 
general case outgoing arcs sanhe node emit accept sanhe character 
discrete hidden markov models hmm ones speech handwriting recognizers correspond second kind pfsa 
pfsa language models combine particularly easily recognizers finite state automata including recognizers chains characters ers unsegmented data dynamic tinhe warping hidden markov models see instance examples 
probability best interpretation recognizer scores language model equation probability best path product graph corresponding automata 
best path product graph searched dynamically building product graph 
beam search process line characters recognized see examples 
step small number candidate successor states kept corre sponding probable interpretation 
tradeoff quality approximation speed calculation memory allocation monitored number candidate paths kept step 
modular architecture split language model independent modules 
modules new technique variable memory length markov models vlmm allows training fairly compact probabilistic finite state machine code modal code length cross entropy ls ziv brown el humans table comparison code length cross entropies brown corpus 
entropies calculated brown corpus comes shannon experiments human subjects book malone 
high predictive power 
main module vlmm 
alphabet lowercase characters plus encompass broad categories strings proper nouns numbers symbols sub models including proper noun model vlmm models numbers symbols model substituted corresponding sub model 
example model substitution shown 
modularity desirable reasons sub models modified depending application having retrain main model training faster smaller machines need trained overfitting avoided fewer weights need estimated memory required fewer states weights need stored model necessary build graph composed automaton beam search performed querying sub models associated needed 
error measures entropy consider language source produces character strings probability distribution pz model underlying probability distribution pm 
predictive power model measured cross entropy respect actual distribution pz hf pz 
log lower bound uh 
upper bounds intrinsic entropy english ranged bits character obtained shannon human subjects guesses letter prefix literary text cover king gambling estimate bits character 
table compare results average code lengths character known compression schemes cross entropy best published model measured finite state machine finite state machine markov model order trigram model 
examples language source 
test set brown corpus 
values points fox performance language model 
stationary ergodic markov process states emits characters expression character entropy rate valid pr logp measure cross entropy finite corpus characters empirical estimate character corpus discussion validity estimate see 
train language models follow paradigm ri minimization srm cross validation 
available corpora divided disjoint sets training set validation set te set 
model class organized nested subsets models 
example model class markov models structure obtained organizing models nested subsets increasing markov order 
guarantees vary capacity monotonically having calculate subset model minimum cross entropy training set selected 
models retained model smallest cross entropy validation set chosen 
quality chosen model evaluated test set cross entropy 
ha ha ah ha ha ha ha ah hah aha ha ha ah hah ha ah ah ah ha ah ah ha ha ha ah ha ha aha hah ha 
aha ha ah ha ah hah ha ah ha ha ha aha ah hah hah aha ah ha ah aha aha aha ha ha ah ah ah ha aha ha aha ha ah ah ha ha ah ha ah hah hah ha ha hah ha ha ah ha ah ha aha aha ha aha ha 
ha ha aha aha ha ah hah ah ha ha ah ha ha aha ha 
extract character long training sequence generated automaton 
variable memory length markov models principle method optimizing length prefixes capacity control markov models determine probability character window past characters prefix 
standard markov models order rs gram models fixed size prefix rs characters variable memory length markov models vlmm variable length prefixes 
training markov model order rs simply amounts computing frequencies strings length rs 
framework structural risk minimization varying rs capacity control mechanism see section 
section explain principle method train vlmms capacity control mechanism 
assume prefix length rs predict character ac cording estimate iw iw 
differs significantly adding character cr past helps predicting better 
possible decision criterion kullback leibler divergence character distributions different prefixes weighted prior distribution xh lw og lw ah exceeds threshold longer prefix accepted 
wise prefix retained 
easily shown expression ah training increase ah incurred going model prefix simpler model perform structural risk minimization induce structure class vlmms states hm randomly generated text hha 

aa ha aha aha ha ha ha ha ha aha ha hah ha aha ha ha ha ah ha ha ha aha ha aha 
table entropies models 
estimate intrinsic entropy model generated sequence characters model 
reaches hc limit large training sets 
training set size test set size generated independently model 
best model obtained 
show significant difference ha rr suggesting training set size large avoid overlearning 
cs zc cs log log logz 
varying capacity control mechanism superior varying order regular markov models see section 
number parameters model better performance achieved performance fewer parameters needed 
example illustrate principle method figures 
show automaton language generate training test data 
state depicted square box initial state pfsa 
dashed line arcs correspond initial transient context available solid line transitions correspond stationary part pfsa 
show sequence characters generated training 
show automata obtained various values 
table show corresponding entropies small sequence character produced 
automaton achieves best performance predict new sequences produced largest smallest largest capacity 
demonstrates training sequence long avoid overlearning problem 
short training sequences smaller models better possible obtain accurate probability estimates prefixes 
vlmms learned data machines obtained training data various values 

example prefix tree corresponding prediction suffix tree pst prefix tree depth dp built fi om training data 
process training strings length entered tree root leaves 
new branches grown needed 
word ha highlighted 
pst depth ds built prefix tree 
prefix ha highlighted 
entered backwards pst 
probabilities shown node pst correspond prefix tree probabilities ll words training section briefly outline training algorithm 
details 
transition probabilities vlmm decision criterion ah derived estimates cr cr cr calculated various values simplest estimates ch number times string appears training data total length training sequence 
count strings length dp grow called prefix tree depth dp similar lexicon tree trio 
example tree shown 
build tree window fixed length dp slid text training corpus 
strings appearing window added tree root leaves 
time branch attained entering string counter 
associated branch incremented 
memory limitations computer eventually reached training large corpora natural language 
imposes tree maximum number example vlmm pfsa pfsa obtained pst 
identical original finite state machine transient states 
states useful stationarity assumed 
nodes 
number reached branches visited need pruned 
branch pruned counter branch root counter threshold value 
prefix tree contains information necessary build vlmm 
vlmm training facilitated building intermediate structure called su free pst 
pst strings entered reverse order 
emphasize pst prefix tree trained text read backwards 
weights associated branches tree 
node reached accepting string associated prefix tree probability 
string length possible walk backwards sux tree get longest sux sigma provides best available probability estimates character example assume want predict probabilities character follows string 
highlighted path sux ha tree 
corresponds highlighted path provides probabilities character 
algorithm train pst initialize pst candidate strings pst single root node 
pick remove 
su add pst growing necessary nodes 

sp add corpus number characters ap news training set brown test set ap news train subset training alice valid valid unix man valid unix man tex files table text corpora training testing 
associated press ap news corpus training brown corpus testing 
subset ap corpus train estimate performance training set 
sets valid valid validation sets 
string chef cl suffix cl length ds pst depth dp prefix tree depth zs pst pruning threshold zp prefix tree pruning threshold alphabet 
notice tree grown root leaves 
string meet criterion ah suffix cs definitively ruled descendents added step 
precaution keep provision descendents meet selection criterion 
possible emulate vlmm pst important practice build corresponding pfsa gain processing speed 
pfsa longest matching suffixes precomputed states pst longest suffixes dynamically determined 
show vlmm built pst 
node vlmm corresponds node pst 
states pfsa labeled string prefix read backwards corresponding pst node root 
example shaded state corresponds highlighted path 
outgoing arc state emits character probability js cr lw ends state labeled suffix wcr pst suffix hah ah 
vlmm state emits character probability zero build connection state state labeled suffix wcr pst 
matching input text character machine state return initial state loose past context predict prior probability js cr 
better backoff techniques may worth exploring situation 
experimental results section detailed study training main vlmm module system 
number characters number nodes table prefix trees trained ap news corpus various sizes training set number characters dp 
corpora training testing listed table 
chosen illustrate wide range difficulties 
data preprocessed substitute strings strings contiguous characters containing number strings contiguous characters containing symbols sets numbers proper nouns 
substitution remaining uppercase characters turned lowercase 
resulting alphabet characters including space vw pace 
leaning curves prefix tree depth trained entire training set pruning threshold results summarized table 
cs varied size training set obtain learning curves shown 
model performance cross entropy bits character 
intrinsic entropy automaton approximated cross entropy text characters generated random automaton evolve dynamics 
validation refers average cross entropy validation sets see table grey shading indicates standard deviation 
estimate intrinsic entropy starts zero increases rapidly reach maximum characters decreases 
factors competing dominant effect part curve increase entropy due increase number states machine second part curve entropy decreases better estimates pfsa weights obtained 
validation set begins infinity decreases rapidly 
starts leveling characters suggesting training data improve model substantially prefix tree depth 
show corresponding evolution size pfsa 
stops validation set entropy number connections standard dev 
intrinsic entropy log num training char variations entropy function number training examples vlmms trained ap news data alphabet char 
training set parameters dp validation sets table 
size machine shown scale different units 
increasing characters maximum number nodes prefix tree reached pruning algorithm starts eliminating nodes 
affect immediately test cross entropy continues decreasing steadily characters 
confident prefix tree pruning necessary memory limitations dominant capacity control mechanism 
model selection varying show results obtained varying cs 
depth suffix tree set maximum ds dp 
train refers fixed size subset training set see table 
previously validation refers average validation sets see table grey shading indicates standard deviation 
validation curve give clear indication overlearning 
starts leveling value 
cs cross entropy validation set zi 
number states pfsa number connections 
optimum cross entropy validation set cross entropy test set brown corpus bits character 
valid tion set entropy train set entropy sic entropy 
number states pst depth log standard dev 
comparison capacity control methods prefix tree trained ap news data alphabet characters parameters dv 

allowed vary 
entropy 
machine entropy machine sizes sizes member flight report including months dispute 
leaders lot ground pairs due planner lux said defender spokesman standards arms responded victory side honored arrest ethnic procedure text generated vlmm trained ap news data parameters jr js andes 
show text generated random machine just described 
text generated obviously grammatical english words actual english words 
sonhe invented words quite amusing mental st ing somebody reads text aloud quite paying attention really sounds radio news 
model selection varying ds performed control experiment fixing zs allowing ds vary 
varying ds classical means regularization models 
results varying ds cs shown 
validation curve starts leveling depth ds 
point validation performance approximately 
number cells associated pfsa number connections 
sanhe value obtained ds cs 
corresponding number states number connections re spectively 
sanhe performance machine obtained cs capacity control criterion times states times connections machine obtained ds capacity control criterion 
difference ds cs large expected 
glance models difficult large values number distinct strings grows exponentially number distinct strings corpus fi exceeds sp pruning threshold prefix tree manageable limiting principal capacity control criterion sp 
corresponding posterior test set test set cross entropy characters proba prior proba bits char 
coma os os quote table parameters performance punctuation sub models 
test set test set cross entropy prior proba bits char 
table parameters performance numbers symbols proper noun sub models 
training sub models main model trained sub model 
punctuation sub models zero order markov models uni gram models 
parameters performances reported table 
number symbol models order markov models figures 
performances reported table 
contribution entropy sub model corresponding cross entropy calculated test data restricted alphabet sub model test set 
char pfsa model numbers symbols 
number model character represents numbers represents states associated letter represents letter represents states associated 
symbol model letter represents numbers letters symbols represents states associated 
contributions modules summarized table 
capitalization model implemented take cross entropy capitalization model brown 
estimate entropy models hm bi 
total parameters weights connections states 
performed control experiment training vlmm raw text 
cross entropy model obtained bits character 
modular ap proach provides increased flexibility sacrificing performance 
word trigram models years leading method designing efficient language models 
demonstrate vlmms built character level viable alternative 
allow building finite state automata modest sizes compared word trigram models competitive performance terms entropy corpus brown corpus 
structural risk minimization allows control tradeoffs memory speed accuracy varying single continuous parameter 
finite state automata composition allows reconfigure language model needs different applications having model contribution number number test set cross entropy states connections bits char 
main model punctuation capitalization total table summary contributions test cross entropy modules 
retrain core language model 
design targeted line handwriting recognition 
combining model neural network handwriting recognizers way 
possible modeling techniques described may relevant speech recognition phonetic units characters 
acknowledgments gratefully acknowledge discussions colleagues bengio le cun solla pednault freud jurafsky icsi singer ron hebrew university 
ron singer tishby 
power amnesia 
cowan editor advances neural information processing systems volume 
morgan kauffmann 
vapnik 
estimation dependences empirical data 
springer verlag new york 
guyon vapnik boser bottou solla 
structural risk tion character recognition 
moody editor advances neural information processing systems volume pages 
morgan kaufmann 
pereira riley sproat 
weighted rational transductions application human language processing 
arpa natural language processing workshop 
kucera francis 
computational analysis day american english 
brown university press providence ri 
ney 
stochastic grammars pattern recognition 
laplace de mori editors speech recognition understanding advances volume nato asi series berlin heidelberg 
springer verlag 
furui sondhi editors 
speech sigal 
marcel dekker new york ny usa 
jelinek 
trigrams 
eurospeech 
guyon wang editors 
systems neural network volume 
world scientific singapore august appeared book chapter series machine perception artificial intelligence vol 
world scientific singapore 
shannon 
prediction entropy printed english 
bell system january 
cover king 
convergent gambling estimate entropy english 
ieee tras 
theory july 
cover thomas 
theory 
wiley series telecom 
wiley new york 
katz 
estimation probabilities sparse data language model component speech recognizer 
ieee acoustics ad sigal 
jelinek 
self organized language modeling speech recognition 
alex kai fu lee editors speech chapter pages 
morgan kaufmann san mateo california 
brown della pietra mercer della pietra lai 
estimate upper bound entropy english 

guyon henderson 
line script recognition neural networks hidden mar kov models 
icassp adelaide australia 
bengio le cun henderson 
globally trained handwritten word recog nition spatial representation space displacement neural networks hidden markov models 
neural systems 
morgan kauffmann 

