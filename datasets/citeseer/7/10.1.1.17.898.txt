distributed value functions jeff schneider carnegie mellon university pittsburgh pa jeff schneider cs cmu edu weng keen wong carnegie mellon university pittsburgh pa cs cmu edu andrew moore carnegie mellon university pittsburgh pa cs cmu edu martin riedmiller university karlsruhe karlsruhe frg ira uka de interesting problems power grids network switches traffic flow candidates solving reinforcement learning rl properties distributed solutions desirable 
propose algorithm distributed reinforcement learning distributing representation value function nodes 
node system ability sense state locally choose actions locally receive reward locally goal system maximize sum rewards nodes time 
node allowed give neighbors current estimate value function states passes 
value function learning rule information allows node learn value function estimate weighted sum rewards nodes network 
representation node choose actions improve performance system 
demonstrate algorithm distributed control simulated power grid 
compare methods including global reward signal nodes act locally communication nodes share rewards value function information 
results show distributed value function algorithm outperforms conclude analysis problems best suited distributed value functions new research directions opened 
interesting problems candidates solving reinforcement learning rl properties distributed solutions desirable 
state action space large distributed approach performing computation desirable computational speedups coarse grain parallelism possible 
systems access sensors actuators inherently distributed making distributed solution method attractive alternative implementing global high bandwidth communication network 
potential applications include control power grids distribution resource water gas automobile traffic control electronic network routing control robot teams 
fig 
provides intuitive sense kinds applications envision algorithms 
related reinforcement learning dynamic methods optimal control fairly understood 
contrast distributed reinforcement learning mature concept harder formulate analyze theoretically 
approach assume restricted interaction nodes 
example transfer function dynamics local state depend local state local control value function trivially split components depending local states controls solution optimally controlling entire system just composite solution created solving individual parts 
course making assumption eliminates interesting problems 
completely segregated approach taken additional allowance choice controls node may restrict space controls available 
resulting algorithm requires global information provide provably optimal solution 
distributed elevator control addressed reinforcement learning shared global state cost information agents act independently 
provides similar framework context competing cooperating agents 
homogeneous agents seek learn similar value functions improve learning speed performance exchanging learned policies sensing weiss proposes bucket brigade scheme credit assignment cooperating reinforcement learning agents 
agent acting locally independently communicate globally auction arbitrate subset agents allowed take global controls state 
similarly approaches different behaviors learned system combined idea choosing actions accomplish goals 
similarly economic models form basis credit assignment methods 
packet routing domain completely distributed approaches taken 
problem node decision neighboring node rout packet 
global state space list packets system current locations destinations 
node information packets queue 
global cost function average length time takes send packets network node stores computes length time take send packets 
part routing algorithm nodes periodically ask neighbors long predict take deliver packet routed 
empirical studies promising shown problems ringing instability adapting changing load conditions 
distributed value functions description method refer sample problem diagrammed fig 

job distribution network minimize cost function providing resources customers 
shows nodes chain assumptions connectivity graph 
problem easily written global reinforcement learning problem states represent flow rates resource levels actions represent changes flow rates rewards penalties represent satisfaction customer requests costs incurred doing 
distributed formulation problem changes ffl state space 
node observes flows resource levels presumably sense locally represented may case state variables observable nodes 
ffl action space 
node ability choose flow control actions called assume action variable global formulation chosen exactly node distributed formulation 
ffl rewards 
node receives reward presumably satisfaction customers called assume sum reward functions distributed case equal global reward function global case goal distributed formulation optimize sum time 
global rl formulation usual approach solving resource distribution problem globally available states actions rewards leads bellman equation max fl ja value function fl temporal discount factor 
equation may solved methods including reinforcement learning variants 
result optimal value function represents expected discounted sum rewards optimal policy fl obtained value function may generate optimal control decisions finding action gives maximum eq 

distributed rl formulation main purpose introduce evaluate algorithms distributed rl 
section describe 
common literature various forms new 
start assuming action variable chosen particular node reward function broken sum term received node 
simplicity continue assume state globally observable 
question local state customers customers global reward global action provider customers global state distributor nodes sample resource distribution network 
shaded circles dual meaning 
represent physical distribution centers network resource providers customers logical nodes distributed rl formulation problem 
points state sensed actions taken rewards received 
observability addressed 
approach define value function node computed choose node actions 
global reward step distributed rl node sense act learn locally base learning control decisions global reward signal 
results bellman equation form max fl ja represents action variables available node choose 
unknown action choices nodes affect probabilities transfer function 
refer combined global state action spaces single global reward distributed nodes 
unfortunately probability right hand side eq 
isn defined node depends policy neighboring nodes hard estimate 
alternative learning formulation described 
addressing issue simultaneous solution set equations observe value function retains meaning expected discounted sum rewards resulting policy fl node attempting learn value function hindered lack knowledge actions nodes take having local state available addressed 
method fully distributed depends global broadcast reward signal 
local naive fully distributed formulation node acts optimize rewards communicate neighbors 
results bellman equation form max fl ja assuming global reward function computed probably information transmitted node sensors central location broadcast nodes local signal available node 
assumed reward function computed directly node local sensors actions 
addressing issue simultaneous solution set equations observe value function retains meaning expected discounted sum rewards resulting policy fl catch composite policy longer guaranteed optimal system 
easy see example terms example fig 

suppose provider offer satisfy customers penalty node customers greater node customers 
resource flow distribution nodes node act satisfy customers greater expense starving 
note result bad penalties equal quadratic 
distributed reward way nodes communicate act help neighbors exchange information immediate rewards receive 
considers weighted average local rewards neighbors get equation node value function max fl ja weighting function determines strongly node weight immediate rewards node average 
assumed zero pairs nodes neighbors non zero 
equation contains variable action chosen node assume node get see action choice node report resulting reward hope result greedy behavior cares neighbors propagate network 
consider system discrete states value function represented table see solution takes form fl node uses value function policy decisions act improve long term rewards neighbors tradeoffs weights 
unfortunately help non neighbors see eq 
node component rewards immediate neighbors 
reusing previous example node act starve node customers 
distributed value function nodes act assist non neighbors value function equation updated nodes exchange information immediate rewards value functions max fl ja point changed bellman equation point say properties solution 
deterministic system example write solution form thing note weights average eq 
longer equal weights eq 
previously eqs 

pairs nodes zero weight equation may receive non zero weights 
case nodes act help non neighbors 
order see weighting non neighbors occurs consider system organized node chain node uses simple average neighbors 
remove max eq 
assuming nodes implement fixed optimal policy 
simplify assuming system state making stateless ll fix 
value function equations sums eq 
expanded fl fl fl state transition probabilities expected value right hand side eq 
reduced single term 
solution equations fl equations match eq 
different coefficients mentioned 
importantly see value function node includes positive weight rewards received node nodes communicate directly original intent algorithm 
real case numerous states eq 
matrix equations node state pair weighted sum states returns equation 
solution set equations just matter linear algebra resulting weights rewards depend transition probabilities states 
formulation effectively created distributed representation value functions nodes 
node trying learn weighted sum expected rewards nodes network 
chosen weighting function sum value functions nodes result expected weighted sum rewards nodes just global rl approach 
accomplished access local rewards communication value function information immediate neighbors 
local state learning previous section discussions global state information available nodes 
order fully distributed method necessary local state 
intuitively easiest think terms function approximation 
case traditional global reinforcement learning function approximators represent value function common 
main purpose handle state spaces large stored tabular form speed learning generalization large state spaces 
cases convergence proofs reinforcement learning extended function approximation 
case local state distributed rl think exactly value function approximation node chosen differing sets features locally observable state function approximators 
intuition speculate convergence results may extend algorithms answering question 
additionally question chosen function approximator particular problem remains important 
practical problem introduced local state variables 
equations derivation written strictly terms value function 
glance lead kind value iteration coupled learning system models solve problem example 
done easily local state means neighbors state common language communicate 
isn possible node ask am state choose action state put action choose representation states actions 
despite problem reward value functions universal language 
constructing learning algorithm solve problem requires nodes communicate terms 
learning rule node implement learn value function eq 
gamma ff ff fl max node performs updates specified equations online 
learning done online done line trace system passes various states need neighbors specify state action taken 
necessary transmit current estimated value state land iteration 
experimental results tested algorithm simulated power grid problem 
formulation simplifies simulation keeps number state variables smaller 
simulation real power grid requires complex real variables 
simulation uses variable resistors means controlling power flow 
historically hasn possible real power grids new devices developed allow 
system components refer fig 
providers 
power generation facilities treated fixed voltage sources 
customers 
represent cities desired voltage desired power requirement fixed resistance load 
distributors 
distributors represent physical entities nodes distributed rl formulation 
intelligent control decisions 
variables control problem follows local actions 
node choice power line connected 
request resistance line doubled halved left 
lines go distributors providers distributors cities controlled distributor 
line goes distributor nodes get requests resistance change 
arbitration scheme shown determines actual change resistance 
resistors possible values attempts increase maximum value decrease minimum value result staying 
node node request request halve double halve halve halve halve double double double double local state 
distributor receives state information line depending connected distributor distributor neighboring voltage higher states neighboring voltage increased lowered remained iteration states resistor minimum maximum states 
possible states total 
distributor city distributors item city needs voltage 
possible states total 
distributor provider items 
possible states total 
complete local state space node cross product states line attached 
example node connects city provider node sense possible states 
local reward 
local reward received node solely satisfaction local customers city directly attached 
voltage available desired level node receives grid grid grid grid dr dr dr dr global reward local distributed reward dr distributed value function graph results log scale showing cumulative cost iterations policy evaluation mode 
penalty equal difference reward signal zero 
note problem distributors connected distributors customers 
local reward base decisions learning 
see effects experiments 
ran distributed rl algorithm power grids shown fig 

note practical run traditional global rl method smallest grid hundreds millions states 
run learning steps followed evaluation phase learn followed policy time chose randomly 
fl 
ff initially decayed 
exploration rate started stays steps decays 
distributed reward distributed value function cases chosen compute simple average node neighbors 
trials initial random setting resistors chosen 
system operates steps resistors changed new random settings 
loop continues learning evaluation phases 
fig 
table summarize experimental results 
show average cost negative reward step evaluation phase confidence intervals 
averages trials took minutes mhz pentium ii 
results show distributed value function performing best case grid grid global reward best 
speculate cities sources re grid grid grid grid customers customers customers customers customers customers customers provider customers customers customers customers provider provider provider provider customers customers customers customers customers customers provider customers customers power grids experiments 
algorithm grid grid rank avg cost rank avg cost global reward sigma sigma local sigma sigma dist reward sigma sigma dist value fn sigma sigma grid grid global reward sigma sigma local sigma sigma dist reward sigma sigma dist value fn sigma sigma table average reward step policy evaluation mode algorithms different power grids 
ward 
lumping global reward signal doesn hide information cases 
local performs poorly shows locally greedy approaches aren sufficient solve problem 
fact distributed reward performs reasonably distributed value function indicates necessary consider rewards immediate neighbors 
local distributed reward suffer nodes cities attached case neighboring distributors cities attached source local reward 
summary general algorithm distributed reinforcement learning allows nodes benefit nodes system requiring communication immediate neighbors 
results show algorithm performing power grid control application 
remaining question choose order get best result 
derivation observed different algorithms result similar looking solutions eqs 

difference effective weighting function 
local global reward offer specific function distributed reward distributed value function offer class functions depending chosen 
distributed value function imitate global reward fully connected graph uniform weights local zero weights neighbors imitate distributed reward 
find may cases distributed reward performs best 
question conditions solution eq 
yields optimal nearoptimal policy entire system 
obvious choices 
graph nodes fully connected nodes weight equally global reward case uninteresting practically requires full communication nodes 
possible put bounds suboptimality specific weightings 
interesting extension method case neighbor relation weighting function change dynamically 
simple case structure grid may change learning process due external causes 
sophisticated version nodes may actions allow change neighbors 
appropriate nodes mobile agents traveling occasionally encountering choosing teams 
asada noda hosoda 
coordination multiple behaviors acquired vision reinforcement learning 
proceedings international conference intelligent robots systems iros pages 
baird 
residual algorithms reinforcement learning function approximation 
machine learning proceedings twelfth international conference 
morgan kaufman 
baum 
manifesto evolutionary economics intelligence pages 
springer verlag 
bertsekas 
dynamic programming optimal control 
athena scientific 
boyan littman 
packet routing dynamically changing networks reinforcement learning approach 
neural information processing systems volume 
choi yeung 
predictive routing memory reinforcement learning approach adaptive traffic control 
neural information processing systems 
crites barto 
improving elevator performance reinforcement learning 
neural information processing systems 
gordon 
stable function approximation dynamic programming 
th international conference machine learning 
littman 
markov games framework multiagent reinforcement learning 
proceedings eleventh international conference machine learning pages 
schneider 
exploiting model uncertainty estimates safe dynamic control learning 
neural information processing systems 
schneider boyan moore 
value function production scheduling 
international conference machine learning 
singh cohn 
dynamically merge markov decision processes 
neural information processing systems volume 
subramanian druschel chen 
ants reinforcement learning case study routing dynamic networks 
proceedings ijcai 
sutton barto 
reinforcement learning 
mit press 
tan 
multi agent reinforcement learning independent vs cooperative agents 
proceedings tenth international workshop machine learning pages 
watkins 
learning delayed rewards 
phd thesis cambridge university 
weiss 
distributed reinforcement learning 
robotics autonomous systems 
