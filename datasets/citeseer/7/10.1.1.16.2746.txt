position statistical inference techniques integrate neural network bayesian network models william hsu department computer science beckman institute university illinois urbana champaign cs uiuc edu ai uiuc edu statistical methods shown direct neural network analogs surveyed discuss sampling optimization representation methods feasible applied conjunction place neural networks 
foremost gibbs sampler successful role convergence heuristic derived statistical physics probabilistic learning interpretation 
review various manifestations gibbs sampling bayesian learning relation traditional simulated annealing specializations instances em application model construction technique bayesian network formalism 
examine ramifications advances markov chain monte carlo methods learning backpropagation 
consider bayesian network formalism informs causal reasoning interpretation neural networks prescribes optimizations efficient random sampling bayesian learning applications 

background 
overview statistical inference methods connectionist learning analogs correspondences understood support pure probabilistic interpretations 
architectures learning rules key properties convergence ann methods shown admit emulation random sampling algorithms 
better understanding probabilistic theory provides optimizations lead faster empirical convergence significant advance applications improved semantic organization 
instance knowing encode symbolic information ann model extract trained model benefit having intensional interpretation pearl terms connectionist system 
equally important generation usage causal model domain theory capability account intermediate state system yielding immediate benefit incremental learning 
advantages accrue integration symbolic methods principled analysis ann methods random sampling 
discuss primary issues concerning integration probabilistic methods learning connectionist complements hybrid systems equivalents alternative implementations 
typified algorithms inferring hidden causation especially constructing bayesian belief networks semantically better understood 
issue develop formal prescriptive framework cf 
valiant quantitative pac analysis bias inductive learning 
position defend milieu described random sampling problem determinant computational learning complexity concept hypothesis languages pac framework 
second issue problem uncertain reasoning statistical learning unknown data 
identify specific aspect unknown data survey shortcomings current approaches especially anns 
consider capability tolerate unknowns better understanding random sampling analogues ann methods 
foremost markov chain monte carlo methods stochastic simulation algorithms generate markov chains known stationary distributions 
gibbs sampler turn best known markov chain monte carlo techniques performs bayesian network learning corresponds precisely auxiliary simulated annealing boltzmann machine architecture 
research gibbs sampling maximum entropy relatives em led development generalized incremental variants classical algorithms hidden markov model hmm learning 
random sampling analysis learning error backpropagation feedforward networks critical permits development hybrid systems comparable improved convergence properties 
chief purpose studying random sampling algorithms emulate neural network learning develop efficient probabilistic methods retain data parallelism anns achieving higher level semantic clarity 
bayesian belief network formalism fits description advanced research annealing learning algorithms bayesian networks 
examine primarily ann architectures admit dualities bayesian networks 
take full advantage ability representations apply network compilers map architecture 
master control mechanism needed optimize opportunistic change representation accuracy semantic clarity quality explanations causal reasoning traces data parallelism 

gibbs sampling hopfield networks feedforward network learning gibbs sampler stochastic simulation heuristic achieves maximum likelihood approximation irreducible markov chain model equivalence class reachability states communicate 
property target model necessary condition convergence algorithm uses gibbs sampling 
purpose acquire parameters train approximating model observable function 
learning problem common pattern recognition applications speech handwriting 
gibbs sampling originates statistical physics alternatively referred method 
gibbs sampling applies directly bayesian networks full data parallelism cf 
pearl token passing model generalized hopfield networks simulated annealing asymmetric boltzmann machines 
annealing function enhanced monte carlo sampler meets gibbs criterion typically conjunction metropolis loop canonical sampling method optimization annealing 
note properties discrete hopfield networks symmetric bidirectional weighting links retained generalization 
neal shown learning error backpropagation feedforward networks augmented adaptive random sampling approach called hybrid monte carlo algorithm 
results faster convergence traditional annealing retains benefits annealing naive random sampling 
hybrid monte carlo algorithm fully compatible gibbs sampling assumed sample gibbs distribution default case 

information theoretic foundations em expectation maximization known estimate maximize phases forward backward relaxation algorithm generates maximum incomplete data joint priors 
em widely recognized connectionist literature highly flexible learning method 
general relative gibbs sampling applied fields pure probabilistic learning pattern recognition hidden markov models hmms 
em shown manifestation maximum entropy principle 
related incarnations em appear probability theory literature title baum welch algorithm specifically targeted hmm learning 
set hidden states observable alphabet emitted probabilistic function stochastic transitions states 
parameter acquisition algorithms discover source modeling sequence symbols transition probabilities ij output emission probabilities ij tuples oe 
typical application model learned em discussed 

applications 
supervised learning case study em pattern recognition 
shown inception designed precisely applications unknown data 
difficult aspects uncertain reasoning intelligent systems perspective noisy pattern recognition extremely common 
manifests faults input acquisition unreliable sensors subjective data limitations statistical computation component window learning schemes described 
viterbi algorithm dynamic programming method computes maximum stochastic models learned em variants especially hmms transition probabilities acquired algorithm 
suited linear pattern recognition problems signal mapped supervised learning applied 
examples include prediction protein secondary fold difficult problem attack extensive domain knowledge pure random sampling specific subproblems 
research protein folding compares pure probabilistic methods simple hmm learning em followed viterbi matching traditional feedforward backprop knowledge kbann ann methods em shown outperform extant connectionist systems 
notable considering pure statistical approach fixed width windows 
best predictor time hybrid system combining pure statistical memory connectionist learning backprop connectionist front data fusion feedforward net trained backprop 
compared system em achieved comparable cross validated prediction accuracy 

unsupervised hmm learning supervised learning parameters hmms popular method training pattern recognition system 
spatiotemporal sequence modeling signal wish predict sensor output high uncertainty domain biomedical monitoring general applicability subsymbolic systems including anns spatiotemporal sequence learning witnessed ability generate maximum likelihood estimates incomplete sample data 
current research examines ability lack thereof simple recurrent networks acquire temporal regularities observable markov processes duration runs periodicities research delay ann emulation viterbi em algorithms demonstrated viterbi algorithm adequately modeled viterbi network time delay neural net direct correspondence output units hmm states 
addition em implementations specifically baum welch algorithm emulated alpha networks named iterative relaxation parameter forward pass 
integration effort pure probabilistic connectionist interpretations acted catalyst expose information theoretic underpinnings kullback leibler divergence cross entropy mutual information measure 

gibbs sampling brief survey gibbs sampling general heuristic derived statistical mechanics encompasses broad family markov chain monte carlo algorithms common simulation constraint 
applies problems input consists joint priors data vector multi dimensional parameter elements random variables 
random variables correspond elements discrete hopfield bayesian network bayesian networks shall consider distinction 
desired output sequence denoting markov chain network states conditional probabilities activations weights 
wish simulation markov chain random perturbations amenable annealing methods faster convergence 
gibbs sampling selects follows jq ji jg ji jg observable distribution jth random variable time increment sampled joint prior probabilities stationary markov chain state transition model 
gibbs sampling feasible bayesian optimization neural networks conditional probabilities sampled arbitrary groups parameters 
referred study intelligent systems specifically bayesian networks locality 
anns locality typically respected statistical character conditionals highly complex multimodal 
gibbs sampling desirable property fully compatible simulated annealing 
synergy extensible parallel distributed relaxation 
classes annealing algorithms supervised learning stochastic backpropagation achieve convergence pattern recognition phase associative memory systems especially hopfield networks 
efficacy gibbs sampling depends application learning versus recall distributed nature inferential problem modularity bayesian networks literature 
stochastic modeling requirements simply existence stationary distribution sampled process positive recurrence 
anns process describes conditional probabilities network component states weights feedforward network learning neurons boltzmann machines possibly incomplete observed data 
sufficient condition may relaxed necessary condition irreducibility 
review typical learning problems served gibbs sampling york gives proof sufficiency irreducibility leading result markov process induced gibbs sampling yields maximum condition 
survey discusses interesting application sampler construction knowledgebased system relating learning characteristic gibbs sampler fusion propagation pearl framework fusion propagation structuring 

theoretical foundations 
gibbs sampling annealing methods gibbs sampling viewed variation traditional stochastic search algorithm metropolis applied simulated annealing 
best known application gibbs sampling best known annealing ann methods boltzmann machines hinton boltzmann machines discrete hopfield networks simulated annealing gibbs sampling ordering mechanism simulation 
boltzmann machine general hopfield networks arbitrary monte carlo methods 
important note gibbs sampling applied part hybrid monte carlo algorithm feedforward network training serves learning function 
contrast training machine learning sense occurred stage gibbs sampling applied boltzmann machines 
simulated annealing avoid local minima convergence phase pattern network stochastically updated stable attractor reached 
constraint satisfaction networks associative memories method generalizable bayesian neural networks 

constraint satisfaction bayesian networks discuss biologically plausible specialization parallel boltzmann machines generalization asymmetry 
stipulation turns extremely useful study probabilistic semantics neural networks constraint satisfaction networks normalizations weight probabilities dualize causal bayesian networks 
simple bayesian network shown sparse bipartite boltzmann machine dual 
additional caveat additional semantic issue bayesian networks true causal reasoning evidential reasoning belief revision systems depends degree match random variables defined propositions system modeled events predictions 
merely observing subset boltzmann machines interpreted bayesian networks automatically respect property probabilistic model 

hybrid stochastic methods feedforward networks hybrid monte carlo algorithm investigated neal method overcoming shortcomings traditional error backpropagation 
new method stochastic training augments backpropagation similar traditional metropolis loop simulated annealing 
common idea perturb weights weight decay order lower susceptibility local minima cost slower convergence 
hybrid monte carlo algorithm differs metropolis algorithm full hamiltonian energy measure kinetic potential energy 
purpose augmentation account effect gradient total energy 
new algorithm amenable weight decay framework resulting system better empirical performance pure backpropagation gaussian approximation learning backpropagation 
principle annealing hybrid monte carlo simulation facilitates arbitrarily close approximation opposed requiring approximator components drawn family gaussian conditional distributions cf 
mackay 
main benefit general bayesian framework avoidance overfitting regularized weight decay 
improvement gibbs sampling demonstrates benefits normalized information theoretic measure free energy versus traditional entropy measure absolute relative supports development needed higher order metric comparing feedforward architectures different learning rules heuristics 
neal results strongly underscore point markov chain monte carlo approaches applicable pure feedforward architectures just small subset anns constraint satisfaction hopfield networks 
augmentation backpropagation learning classical stochastic methods metropolis algorithm shown oversimplified approach leaving including theoretical developments done area 

ramifications 
gibbs sampling bayesian learning surveyed gibbs sampler shown optimizing improvements applied annealing applications learning situations bayesian networks 
discussed orthogonal improvement adding momentum term annealing random sampling justified statistical thermodynamics interpretation 
improvements generate hybrid monte carlo family algorithms 
seen hybrid monte carlo enhance performance backprop learning 
gibbs sampler metropolis algorithm augmented kinetic energy applied annealing problems including fusion propagation bayesian networks subset stochastic convergence constraint networks 

causal reasoning constraint satisfaction crucial research objective arises interpretation asymmetric parallel boltzmann machines bayesian networks causal reasoning interpretation class anns 
theory lends higher level proximity symbolic intelligent systems probabilistic basis inductive learning 
improved understanding leads turn advances application bayesian learning methods traditionally symbolic endeavors case problem solving analogical reasoning prototypes 
expect bayesian network interpretation yield insights incremental reinforcement learning knowledge systems uncertain domains problems anns applied force rarely remarkable success 

network efficiently representable functions quantitative characterization envisioned sought extensive success network efficiently representable functions 
term employed russell norvig refers class functions manifold correspond dual manifold neural networks meet certain unspecified complexity restrictions 
current research spatiotemporal pattern possible metrics characterizing respect temporal regularity especially duration periodicity potentially rich field study theory anns hinging development improved probabilistic information theoretic semantics certain ann models quantitative theory 

current current research investigates ramifications bayesian network asymmetric boltzmann machine duality terms practical application gibbs sampling supervised learning extension known duality temporal persistent augmentations bayesian networks compared recurrent networks extension full hybrid monte carlo approach including model recurrent networks probabilistically sound framework 
focusing capability acquire spatiotemporal regularities bayesian simple recurrent networks adaptivity ranging simple persistence complex dynamical systems modeling 
believe characterization capabilities motivation gauge progress developing theory methods integrate bayesian neural networks 
ackley hinton sejnowski 
learning algorithm boltzmann machines 
cognitive science 

amari 
information geometry em em algorithms neural networks 
neural networks 
de 
learning asymmetric parallel boltzmann machines 
neural comp 
bourlard morgan 
connectionist 
kluwer boston ma 
delcher kasif goldberg hsu 
probabilistic prediction protein secondary structure causal networks 
proceedings th national conference ai aaai volume pages 
elman 
finding structure time 
cognitive science 
haussler 
quantifying inductive bias ai learning algorithms valiant learning framework 
intelligence 
heckerman 
tutorial learning bayesian networks 
technical report microsoft 
kirkpatrick gelatt vecchi 
optimization simulated annealing 
science 
mackay 
practical bayesian framework backpropagation networks 
neural comp 
myllymaki 
mapping bayesian networks boltzmann machines 
proceedings applied decision technologies pages 
neal 
asymmetric parallel boltzmann machines belief networks 
neural comp 
neal 
bayesian training backpropagation networks hybrid monte carlo method 
technical report university toronto 
neal 
connectionist learning bayesian networks 
intelligence 
neal 
probabilistic inference monte carlo methods 
technical report crg tr university toronto 
neal 
bayesian learning neural networks 
springer verlag new york ny 
pearl 
probabilistic reasoning intelligent systems 
morgan kaufmann san mateo ca 
russell norvig 
artificial intelligence modern approach chapter 
prentice hall englewood cliffs nj 
thrun 
explanation neural network learning 
kluwer academic publishers norwell ma 
york 
gibbs sampler expert systems 
intelligence 
zhang mesirov waltz 
hybrid system protein prediction 
journal molecular biology 
