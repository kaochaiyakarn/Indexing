representation evolution neural networks martin department computer science vi university dortmund germany mail informatik uni dortmund de evolutionary approach developing improved neural network architectures 
shown possible genetic algorithms construction backpropagation networks real world tasks 
network representation developed certain properties 
results various application 
performance neural networks highly depends architecture networks parameters 
determine architecture net size structure connectivity greatly affects performance criteria learning speed accuracy learning noise resistance generalization ability 
mathematical methods architecture parameters networks 
design decisions normally achieved rules thumb common approach designing neural networks build network test desired function change structure parameters repeat evaluation criteria fulfilled 
computational developers viewpoint procedure expensive gives reliable insight architecture performance relation 
judd lin vitter shows learning general choosing optimal network topology np complete problems 
shown placing constraints topology help learning tractable 
motivates usage evolutionary approach heuristic finding optimal architectures network representation places useful constraints architecture 
approaches evolutionary design neural networks 
straight forward method determine architec ture network miller network structure mapped binary con matrix cell matrix deter mines connection units ex 
set matrices created randomly corresponding networks trained 
networks performed best task allowed produce offsprings generation 
new offsprings created parent networks combining manipulating connection matrices 
fixed number cycles best network chosen 
main problem approach matrix representation network 
incorrect network structure feedback connections long codes larger networks result 
sophisticated approach harp 
blueprints networks network described parameters number layers layer size connections layers 
representation possible place constraints networks architecture reduce number incorrect networks 
incorrect networks trained evaluated number evolution cycles needed get networks reduced 
approach avoids representation problem described schiffman produce offsprings best networks selected con added removed networks worst networks replaced offsprings best 
section introduces genetic algorithms method search better networks 
section focuses network representation fulfills useful properties operators create new net works 
page genetic neural networks basis evolution set individuals networks population 
better individual adapts environment greater chance survive produce offsprings 
individual seen state search space case space possible feedforward networks 
individual assigned fitness value reflects ability adapt environment 
genetic algorithms iterative procedure iteration called gen eration 
iteration natural inspired principles selection reproduction applied population 
selection mechanism determines individuals allowed produce offsprings generation 
probability individual allowed reproduce number offsprings fitness 
supervised learning networks directly provides measure fitness 
restrict evolution backpropagation networks 
evolution cycle characterized follows 
genetic algorithm typ 
backpropagation networks interpreted network simulator 
simulator networks trained training data environment performance test data evaluated 
simulator delivers quality criteria mean square error training time size network device fitness evaluation scal ing 
device calculates actual fitness value network performance relatively fitness networks 
prohibit networks high fitness values dominate population generations compensate low fitness diversity search sigma scaling method rescale fitness networks 
fitness calculated weighted factors fitness wl error connections units epochs fitness network expected number offsprings assigned 
roulette wheel selection method chooses networks proba bility relative fitness population size reached 
chosen networks put mating pool allowed reproduce 
reproduction achieved genetic operators mutation crossover ensure offsprings new related parent networks 
prevent best network manipulated operators automatically put new generation 
evolution cycle iterated fixed number times 
evolution actual population best networks statistical measures evaluate process collected saved 
evolution networks ga implies genotypic representation individuals 
allows genetic operators modify knowledge individuals structure 
starting randomly generated initial population genotypic networks network generator builds networks genetic operators weir distinguished low high level rep 
low level representation specifies exactly network parameter connection unit 
causes large search space ga number iterations increases dramatically 
contrast high level representation description networks connection patterns parameter determine number distribution units characterize networks struc ture 
search space smaller networks restriction architectures excluded 
page works harp miller representations types able sure correctness coded networks 
rep resentation backpropagation networks allows correct networks prevents incorrect networks participating evolution cycle saves useless training 
population size number iterations reduced num ber problem tractable 
excluding incorrect networks gives evidence generation networks solve problem genetic search focus finding improved networks 
additionally training time networks significantly reduced layers sparsely connected fully 
representation allows correct networks genetic operators produce incorrect networks 
network specified describing main components 
network parameter area specifies learning rate momentum term connections 
layers layer area specifying number unit layer connections layers 
insure input output unit correct networks distinguish projective receptive connections 
learning rate momentum net 
parameter 
layer ih ibi receptive parameter connections connection network representation receptive connections connect actual layer preceding layers 
units receives connections unit precedent layer 
projective connections insure unit output connected layer 
structure network evolve freely variable number con areas 
connection pattern layers motivated neurophysiological receptive fields determined parameters 
radius parameter specifies connection radius unit layer 
size radius percent relative size destination layer 
determine receptive field offset relative position actual unit added 
second parameter determines density connections receptive field percentage units connected actual unit 
density layer projective lb receptive size connections il connection bi radius radius density density layer representation addressing destination layers difficult variable number layers connection areas 
solution relative addresses 
connections related layer receptive connections related preceding layers 
destination parameter gives percentage layers skipped connections preceding layer established 
type connections illustrates principles projective receptive connections 
network representation requires modification classical genetic operators 
starting idea mutation introduce maintain genetic diversity crossover inherit features page operators developed high level representation obey underlying ideas 
operators relative point crossover 
mutation operator applied networks mating pool chosen probability mutation rate selected parameters representation increased decreased learning rate momentum size randomly selected layer radius density randomly selected connection area 
parameters upper bound lower bound maximum change rate prevent changes radical 
learning rate momentum term allowed change total range vary total range 
point crossover operator exchanges layers networks 
networks chosen mating pool probability crossover rate viewed linear ordering layers randomly chosen number specifies crossing point layers exchanged 
point crossover operator exchanges layers randomly chosen crossing points 
networks chosen mating pool probability crossover rate illustrates crossover 
pl lo poi point crossover due relative addressing destination layers usage parameters fixed values layers exchanged networks loosing correctness 
shown network representation closed operators correct networks participate evolution 
results evaluate approach selected applications backpropagation networks 
task discriminate edges corners 
training data consisted patterns pattern binary matrix displays edge corner 
difficult task nate handwritten digits different writers xll matrices showing possible digits 
third application consists vectors real valued components encode control knowledge improve proofs prolog 
proven feed forward networks capable arbitrary exact function approximation 
despite theoretical result fox showed difficult great number architectures approximate mexican hat function 
vectors coor building dimensional mexican hat function 
developed evaluation criteria online performance best network generations evolution process function approx 
networks evolution process 
func page tionality approach evaluated capability evolve networks solve task reliability reproduce results high probability 
second criteria measures quality evolved networks evaluating performance task comparison standard ar tasks 
tasks ga discovered architectures capable solving task better architectures 
gives example evolutions progress 
population networks evolves generations network trained epochs 
online performance averaged fitness generation performance averages best fitness values generations 
evolution process stands network performance better comparable standard networks 
standard networks mean fully connected input hidden output architecture 
applications population size networks generations typical yields similar results 
show reliability approach repeated experiment times gained statistical certainty experiments result best network range average best network 
successful evolution process compared best evolved network task standard network number units stan dard network number connections far known results publications 
charts results applications 
edge detection 
results edge detection edge detection task genetic network units connections performed better standard network con network connections 
classification capability networks 
network digit dis digit discrimination results fi om digit discrimination connections performed standard network number connections slightly worse network twice connections 
classification capability networks 
results learn control knowledge loo lo results proof optimization ing control knowledge comparable digit task 
network connections performed better standard network number connections slightly worse network twice connections 
compared ultsch control knowledge learned networks achieved 
astonishing result gained function approximation 
var ied number hidden units standard net page function approx results function approximation application works get epochs 
genetic network con performed times better standard network connections 
results give strong evidence underlying structure essential performance network 
backpropagation networks efficiently developed gas perform better standard networks 
experiments run sparc station couple hours days 
successful application gas determine architecture neural networks shows auto mated design networks possible 
resulting networks proved better standard architectures 
best evolutionary networks application solved tasks correct 
nearly genetic network showed superiority networks comparable number connection units learning epochs 
applied function approximation task genetic network needed times learning time showed better approximation quality standard networks 
results give strong evidence underlying structure essential performance network 
emerging question context structure influences learning process gain knowledge structure performance relation superior genetic neural networks 
fox er thrun 
learning error driven decomposition 
kohonen simula kangas editors artificial neural networks volume pages amsterdam june 
north holland 
goldberg 
genetic algorithms search optimization machine learning 
addisonwesley 
harp samad guha 
designing application specific neural networks genetic algorithm 
touretzky editor proceedings ieee conference neural processing systems volume pages san mateo 
denver morgan kaufmann 
judd 
neural network design learning 
mit press bradford book cambridge ma 
lin vitter 
complexity issues learning neural networks 
technical report tr cs department computer science brown university providence ri 

algorithmen zur op 
master thesis department computer science university dortmund dortmund po box february 
miller todd hegde 
designing neural networks genetic algorithms 
schaffer editor proceedings third international conference genetic algorithms pages san mateo 
arlington morgan kaufmann 

zur 
master thesis university dortmund department computer science dortmund frg 
belew 
dynamic parameter encoding genetic algorithms 
technical report cse university california san diego ca 

performance evaluation evolutionary created neural network topologies 
international page workshop parallel problem solving nature pages iii dortmund frg 
university dortmund department computer science ultsch hartmann weber 
optimizing symbolic proofs connectionist models 
kohonen simula kangas editors artificial neural networks volume pages amsterdam june 
north holland 

combining neural evolution ary learning aspects approaches 
technical report technische 
white hornik 
universal approx imation feedforward networks hidden layer activation functions 
proceedings international joint confer ence neural networks 
san diego ca ieee 
page 
