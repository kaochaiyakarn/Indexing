comparison discriminative training criteria optimization methods speech recognition ralf schluter wolfgang boris mu ller hermann ney lehrstuhl fur informatik vi rwth aachen university technology aachen germany received october received revised form march accepted april aim build common framework class discriminative training criteria optimization methods continuous speech recognition 
uni ed discriminative criterion likelihood ratios correct competing models optional smoothing 
uni ed criterion leads particular criteria choice competing word sequences choice smoothing 
analytic experimental comparisons maximum mutual information mmi minimum classi cation error mce criterion optimization methods gradient descent gd extended baum eb algorithm 
tree search restricted recognition method word graphs reduce computational complexity large vocabulary discriminative training 
mce training method word graphs cient calculation discriminative statistics introduced 
experiments performed continuous speech recognition arpa wall street journal wsj corpus vocabulary words recognition continuously spoken digit strings ti digit string corpus american english digits sietill corpus telephone line recorded german digits 
mmi criterion analytical experimental results indicate signi cant di erences eb gd optimization 
acoustic models low complexity mce training gave signi cantly better results mmi training 
recognition results large vocabulary mmi training wsj corpus show signi cant dependence context length language model training 
best results obtained unigram language model mmi training 
signi cant correlation observed language models chosen training recognition 
elsevier science rights reserved 
speech communication www elsevier nl locate dieser arbeit ist die scha ung eines fur eine klasse von und fur die 
wird ein de das auf von und modellen 
sich durch die wahl der sowie der 
fur die maximum mutual information mmi und minimum classi cation error mce sowie optimierung mittels gd und baum eb werden und 
die des bei gro em wurde durch eine methode zur der 
fur mce training wird eine methode zur berechnung corresponding author 
tel fax 
mail addresses informatik rwth aachen de ter ney informatik rwth aachen de ney 
see front matter elsevier science rights reserved 
pii schluter speech communication auf 
es wurden fur unter verwendung des arpa wall street journal wsj sowie fur die zi ti digit string sietill deutsch 
und ergebnisse keine auf signi zwischen eb und gd optimierung des mmi 
mce training ergebnisse als mmi training fur 
mmi training bei gro em wsj eine signi von der des 
ergebnisse wurden mit einem im mmi training 
es keine signi zwischen der wahl der fur training und werden 
elsevier science rights reserved 
resume le de ce travail est de de nir un cadre commun un ensemble de apprentissage discriminant de optimisation pour la reconnaissance de la parole continue 
nous un discriminant sur le rapport entre la des corrects 
ce general conduit ade nir des speci ques par le des sequences de mots en concurrence par de la methode de 
des sont menees pour les information mmi de classi cation minimum mce ainsi que pour leur optimisation par la de gradient gd algorithme baum eb 
une methode de reconnaissance restrictive sur une recherche est pour la de apprentissage discriminant pour les 
de plus une methode ete dans apprentissage mce des graphes de mots pour le calcul des 
des experiences de reconnaissance de parole continue ont ete menees sur le corpus arpa wall street journal wsj de mots ainsi que pour la reconnaissance de chi res sur les corpus ti digit string sie till par telephone 
les ont pas mis en evidence des di erences signi entre les optimisation eb gd pour le mmi 
pour des de apprentissage mce des signi que apprentissage mmi 
les de reconnaissance pour apprentissage mmi avec un grand sur le corpus wsj une forte la taille du pour le de langage utilise pendant apprentissage 
les ont ete pour un de langage avec apprentissage mmi 
correlation signi ete entre le du de langage pour apprentissage pour la reconnaissance 
elsevier science rights reserved 
keywords discriminative training maximum mutual information minimum classi cation error corrective training speech recognition 
shown discriminative training methods able produce consistent case small parameter sets large improvements performance comparison conventional maximum likelihood ml training criterion 
applications discriminative training methods speech recognition maximum mutual information mmi bahl brown chow kapadia normandin normandin normandin valtchev minimum classi cation error mce chou paliwal criterion 
mce training approximation error rate training data optimized mmi training optimizes posteriori probability training utterances class separability 
exist discriminative training method guaranteed converge practical conditions ort develop parameter optimization techniques fast reliable convergence 
commonly parameter optimization techniques discriminative training extended baum eb algorithm gradient descent gd method 
eb extension standard baum welch algorithm designed optimization mmi criterion 
eb rst developed discriminative training discrete probabilities gopalakrishnan normandin normandin extended continuous densities normandin 
optimization mce criterion usually performed combination gd 
schluter special choice step sizes gd optimization single gaussian density parameters showing eb algorithm gd fact similar give similar recognition results case mmi criterion 
discriminative training speech recognition important point choice competing word sequences accumulation statistics discriminative model 
number experiments performed best recognized mmi best incorrectly recognized mce word sequence discrimination 
mmi criterion known corrective training ct normandin mce call approximation falsifying training ft optimizes spoken word sequence expense best competing word sequence 
reduce complexity brown assumed training data segmented spoken word sequence unigram language model mmi training 
competing word sequences reduced independent lists competing words word position 
approximations best lists competing word hypotheses experiments reported mmi chow mce training chou 
especially large vocabulary applications cient way collecting statistics competing word sequences word graphs 
rst mmi criterion normandin valtchev 
especially large vocabulary applications determination set competing word sequences recognition training data takes computational load needed schluter speech communication discriminative training 
valtchev recognition done word graphs initially obtained training data acoustic rescoring discriminative training iteration step 
aspect discriminative training large vocabulary speech recognizers introduces language models training views 
firstly language model initial recognition competing word sequences training chosen 
secondly choice language models discriminative training impact resulting acoustic models 
question arises extent recognition results particular language model depend language models chosen training 
mce training approach vocabulary words language model training reported give better results word pair grammar cases word pair grammar evaluation chou 
valtchev bigram language model mmi training speech recognizer vocabulary 
clearly improvements comparison baseline ml results diminished increasing context length language model recognition 
goal uni ed approach cient discriminative training small large vocabulary continuous speech recognizers class discriminative training criteria schluter optimization methods schluter 
approach formulation mmi training normandin includes mmi mce criterion corresponding corrective ct falsifying training ft approximations respectively 
experiments comparing criteria varying degrees model complexity extending comparison 
parameter optimization technique chosen extension eb 
approach formally independent particular criterion question dependence particular criteria solely contained accumulators discriminative statistics call discriminative averages 
schluter speech communication comparison eb gd schluter completed include mixture densities showing strong similarities eb gd optimization 
shown eb algorithm interpreted means nding optimal step sizes gd optimization discriminative criteria 
original method word graphs accumulation discriminative statistics far applies mmi training 
computational ciency word discriminative training methods discussed highly depends word graphs extend method apply criteria mce 
mce training spoken word sequence needs excluded set competing word sequences 
general spoken word sequence removed word graph word sequences removed time 
propose algorithm word graphs correctly cient mce training 
reduction computational requirements large vocabulary discriminative training approach constrained recognition word graphs determination competing word sequences 
constrained recognition approach preserves recognition accuracy signi cantly reduces training times 
addition shown discriminative training constrained recognition performs better word graph rescoring xed word boundary times 
investigations discriminative training completed systematic investigations interdependence language model choice large vocabulary mmi training recognition 
shown recognition performance mmi trained models signi cantly depend choice language model context length training 
results indicate considerable correlation choice language models training recognition 
remaining part organized follows 
section unifying approach discriminative training including comparison eb gd optimization 
section focuses cient ways accumulation discriminative statistics 
section set comparative experiments small vocabulary speech recognition section experiments constrained recognition mmi training comparative experiments language models varying context length mmi training large vocabulary speech recognition systems discussed 
section 
discriminative criteria section unifying approach class discriminative training criteria including mce mmi related criteria special cases 
furthermore close relation parameter optimization methods gd eb shown analytically 
addition interdependence discriminative training language models training discussed see table 

unifying view discriminative training training data shall training utterances consisting sequence xr acoustic observation vectors xr xrt corresponding sequence wr wr wrt nr spoken words 
emission probability acoustic observation sequence xr word sequence wr shall denoted ph 
parameter represents set parameters acoustic model 
language model probability word sequence wr de ned wr 
language model probabilities assumed 
de ne uni ed discriminative training criterion xr xr log log wr pa wr table list symbols uni ed discriminative criterion set parameters acoustic model index speech utterance number time frames utterance tr xr sequence acoustic observation vectors xr utterance nr number spoken words utterance wr sequence spoken words wr utterance set alternative word sequences utterance ph acoustic emission probability density utterance spoken word sequence wr ph posteriori probability spoken word sequence utterance wr language model probability spoken word sequence utterance weighting exponent fr smoothing function corresponding derivative value derivative utterance respectively time frame index state hidden markov model hmm density index mixture density component csl mixture weight density state lsl mean vector parameter single gaussian density state rsl covariance matrix single gaussian density state sl variance vector single gaussian density state diagonal covariance parameters lsl single gaussian probability density rsl single gaussian emission probability density ph mixture gaussian emission probability density conditioned state kronecker delta equals st xr state optimal viterbi alignment path time utterance word sequence index mixture density component maximizes emission probability state acoustic observation crt forward backward fb probability observe state time utterance word sequence crt generalized fb probability observe state time utterance alternative word sequences usually polynomial function acoustic observations csl discriminative averages function acoustic observations respect density state spk sl averages function acoustic observations respect density state spoken word sequences gen sl averages function acoustic observations respect density state alternative word sequences auxiliary function extended baum eb algorithm step size gradient descent gd optimization mean parameter lsl dr step size gradient descent gd optimization variance parameter sl denotes set discriminated competing word sequences sum denominator evaluated 
choice set competing word sequences optional smoothing function optional weighting exponent determine choice particular criterion 
table examples schluter speech communication step size gradient descent gd optimization mixture weight parameter csl ds iteration constants eb algorithm te xr posterior probability observe word word boundaries tb te utterance acoustic observations xr slope sigmoidal smoothing function choice listed mmi mce criterion ct ft criterion 
ml criterion contained uni ed approach listed 
ideally mce ft criterion represent sentence error rate training data 
especially ft argument schluter speech communication table choice set competing words discrimination smoothing function weighting exponent criteria included uni ed criterion criterion smoothing function word sequences included exponent ml identity mmi identity recognized including spoken ct identity best recognized mce recognized excluding spoken ft best recognized excluding spoken smoothing function score di erence spoken word sequence best recognized word sequence di erent spoken word sequence 
step function ft criterion represent sentence error rate training data score di erence lower zero correctly recognized utterances greater zero 
order obtain criterion di erentiable respect acoustic parameters smoothed version step function chosen usually sigmoid function 
mce criterion best recognized recognized word sequences excluding spoken word sequence chosen training order obtain smoothing ect competing word sequences scores near best recognized word sequence 
criterion included uni ed approach mmi criterion sum posteriori probabilities spoken word sequences wr training data corresponding acoustic observations xr 
smoothing function needed mmi criterion smoothing function identity function 
mmi mce training sets competing word sequences usually approximated word sequences determined recognition pass training data indicated table 
cases ct ft criterion determination set competing word sequences de nition weighting exponent redundant choice ct ft mutually dependent 
choice weighting exponent brackets cases 
discriminative training criteria included uni ed approach represent sums logarithmic optionally smoothed likelihood ratios 
words objective discriminative training methods discussed optimize likelihood spoken word sequence expense competing model de ned sums competing word sequences 
sums competing word sequences extended include sums models restricted represent word sequences methods frame discriminative training bahl woodland represented uni ed approach discussed 
supposition smoothing function increasing uni ed discriminative criterion maximized acoustic parameters optimization uni ed criterion tries simultaneously maximize emission probabilities spoken word sequence minimize weighted sum emission probabilities competing word sequence acoustic observation sequence training utterance 
weights sum competing word sequences language model probabilities relative spoken word sequence 
uni ed discriminative criterion optimizes class separability words consideration language model 

optimization discriminative criteria experiments apply continuous mixture density hidden markov models hmm acoustic modelling 
probability density state de ned ph csl sl rsl state indices identi ed corresponding mixtures indices 
index represents gaussian mixture ty density rsl parameters lsl mixture weights csl mean vectors lsl pooled state density speci variances rsl 
addition de ne forward backward fb probability crt mixture time word sequence acoustic observation sequence xr training utterance viterbi approximation ney fb probability equals states best alignment path st xr zero crt ph st ph st viterbi st xr ph kronecker delta function similarly de ne fb probability rt mixture time acoustic observation sequence xr training utterance accumulated set competing word sequences rt pa pa pa pa crt pa pa pa pa st xr viterbi formal di erentiation uni ed discriminative criterion respect parameters acoustic emission probabilities leads expression xr af log pa wr pa pa ctr wr ctr log rsl sl rsl sk expressions occur frequently de ne discriminative averages schluter speech communication functions individual acoustic observations separated contributions spoken spk competing word sequences gen csl spk xr spk sl gen sl fr xr log sl sl fr tr crt wr rsl fr tr crt rsl xrt xrt pa wr pa pa clearly smoothing function leads utterance wise weighting derivatives fr 
de nitions formal di erentiation uni ed discriminative criterion reduced csl log rsl similar expression holds case state speci parameters hs 
order simplify expressions de ne state speci discriminative averages cs csl viterbi approximation case state time alignment apply maximum approximation calculation mixture densities 
best density index observation state shall denoted 
viterbi approximation maximum approximation mixture level discriminative averages simpli ed schluter speech communication csl xr fr tr st xr wr crt xrt xrt 
comparison di erent criteria noted characteristics different training criteria discussed completely basis discriminative averages cf 
eq 
parameter optimization depends discriminative averages 
basic features distinguish di erent criteria discriminative averages derivative smoothing function including parameters set word sequences discrimination weighting exponent discuss mmi criterion ct criterion approximation mmi mce criterion ft approximation mce 
criteria applied reported frequently literature 
table summarizes characteristics criteria 
acoustic observation gives signi cant emission probability state best alignment path spoken word sequence generalized fb probability state near equal 
discriminative averages weighted di erence fb probabilities contribution corresponding observation discriminative averages cancel small 
process cancelling criteria remembered case mce training spoken word sequence removed set mce training frame wise cancelling occur state spoken word sequence contributions spoken word sequences excluded generalized fb probabilities 
case mce training additional method weighting applied utterance level derivatives smoothing function give high weights spoken word sequence near decision boundary 
contribution se recognized utterances cancels mce contrast mmi contribution cancels simultaneously corresponding acoustic observations utterance 
way badly recognized utterances contribute reestimation mce 

parameter optimization case gaussian mixture emission densities criterion show eb gd optimization nearly equal special choice step sizes gd 
explicit reestimation formulae derived case state speci diagonal covariances 
similar formulae apply case density speci pooled diagonal full covariances 

extended baum eb algorithm discriminative training mmi criterion usually applies extended version baum welch training eb algorithm gopalakrishnan normandin normandin 
case continuous emission probabilities uni ed criterion maximized auxiliary function derived normandin tr xr wr pa xr crt wr crt log hs ds log xj hs optimized iteratively 
noted auxiliary function originally derived mmi criterion convergence proved case discrete probability models gopalakrishnan 
approach generalized cover objective functions necessarily rational respect probability models uni ed criterion 
discuss discriminative training respect continuous probability models 
mmi criterion shown extend eb algorithm continuous case normandin equally done generalization 
corresponding iteration constants ds needed guarantee convergence nite continuous case normandin means convergence guaranteed realistic conditions 
performed extension order transfer method choosing step sizes eb algorithm gradient descent 
motivation fact eb reported perform better gd kapadia 
motivation provide common optimization framework equally applied criteria included uni ed approach 
noted step sizes gd optimization comparison eb algorithm agreement results independent theoretical considerations gd step sizes mce training chou 
applying maximum approximation mixture density calculation di erentiation auxiliary function respect new iterated parameters leads expression reestimation formulae derived setting corresponding derivatives equal zero os csl log log xj analogous case discrete probabilities normandin integral term enables convergence smoothing discriminative averages corresponding parameters previous iteration 
constants ds control convergence rate 
reestimation gaussian mixture densities state speci diagonal variance parameter represents initial parameter set density consisting mixture weight csl gaussian mean vectors lsl variance eb algorithm schluter speech communication obtain reestimation equations mean vectors sl eb mixture density sl eb csl csl variance vector eb eb cs ds csl sl cs ds csl cs ds sl corresponding mixture weight csl eb csl eb ds ds csl exists intuitive explanation eb reestimation formula 
reestimation equations equally obtained setting derivatives uni ed criterion respect parameters zero assuming discriminative averages csl occurring resulting equations independent new parameters assuming discriminative average smoothed ds multiplied previous parameters smoothing csl csl csl lsl sl noted reestimation eq 
mixture weights exact derivatives criterion smoothed versions proposed normandin spk sl spk gen sl gen leads smoothed reestimation equation csl csl spk sl spk spk sl spk gen sl gen cs gen sl gen cs csl requires new iteration constants cs magnitude smoothed terms di ers corresponding terms means variances 
schluter speech communication 
gradient descent performing gradient descent parameter optimization iterative reestimation equation applied parameters gradient descent obtain reestimation equations sl gd sl dl sl gd dr dr csl gd csl case eb algorithm derivatives reestimation mixture weights replaced smoothed versions normandin 

comparison gd eb schluter derived step sizes case gradient descent leading reestimation formulae parameters single densities resemble eb algorithm 
comparison extended mixture density modelling 
special step sizes obtained gd dl sl sl csl ds csl dr cs ds ds csl step sizes gradient descent obtain relations reestimated parameters gd eb provided initial parameters equal sl gd sl eb csl gd eb cs ds lsl sl eb csl gd csl eb clearly means mixture weights equally reestimated gd eb variances di er sum weighted squared step sizes corresponding means 
dependence particular criterion applied contained discriminative averages resemblance gd eb holds criteria contained uni ed approach discussed 
looking reestimation formula mean vectors nd step sizes gd proportional corresponding variance 
result inherited eb algorithm comparison transferred gd additional assumptions 
variance factor step sizes gd reestimation gaussian mean vectors cf 
eq 
introduced independently chou theoretical arguments 

iteration control proofs convergence exist gd chou case discrete probabilities eb baum gopalakrishnan 
case eb reestimation continuous emission probabilities convergence proven nitesimal step sizes normandin 
practice reasonable fast convergence achieved eb case iteration constants ds chosen way variances remain positive normandin 
addition ensure denominators reestimation equations remain nonsingular 
density speci variances condition positive variances leads inequalities quadratic iteration constants solved explicitly give lowest iteration constant ensuring positive variance valtchev 
hand possible nd explicit formula lowest iteration constant ensuring condition positive variances case pooled state speci variances 
due second term eq 
prevents explicit solution sl ds occurs denominator summation densities 
order nd smallest iteration constants ensuring positive variances case state speci pooled variances require eb rmin csl bs positive constants rmin 
value rmin provides lower limit variances depends magnitude acoustic features 
value appropriate approximately times lower usual magnitude variances observed experiments 
value lower limit denominators deter mined magnitude counts spk sl sl corresponding di erence csl spk sl sl cf 
eq 

preliminary experiments developed heuristic formula calculate bs obtain optimal training convergence max sgs gs max sg max spk sg gen sg idea formula choose bs magnitude csg far ratio max sg low 
ratio low contributions spk sg gen sg nearly cancel requires approaches xed limit 
iteration constants large leads low convergence rates follows eq 

reestimation eq 
variances eb case calculated estimation minimal iteration constant ds min schluter speech communication ful lling constraint positive variances acoustic feature component ds min csl csl lsl rmin cs csl csl lsl lsl reestimation choose ds max ds min max cs terms maximization sure constraint denominators variances ful lled cf 
eqs 
respectively 
global factor controls convergence iteration process high values leading low step sizes 
substituting choice iteration constant ds eq 
realized constraint denominators eb case implies upper bound bsr resulting step size gd 
upper bound step sizes reached step size estimated constraint positive variance high 
iteration constants mixture weights cs similar expression applied sure denominators reestimation equations mixture weights positive non singular case magnitude near di erences relative counts corresponding reestimation equations cs max max spk sl spk gen sl gen iff max spk sl spk csl gen sl gen schluter speech communication experiments factors ti digit string corpus sie till wsj corpus optimal terms convergence rate recognition results training corpus 
general convergence guaranteed step sizes su ciently small 
addition estimation step sizes training procedure method 
iteration step checked word error rate training data doubled 
occured corresponding iteration step removed fall back previous reestimation statistics temporarily saved reestimation repeated iteration constant 
large vocabulary applications special step size control active error rate training data showed strictly monotonic behaviour iterations 

choice language models discriminative training large vocabulary speech recognition systems language models introduced new aspect compared small vocabulary applications 
de nition discriminative criteria discussed clear best choice language models training 
firstly levels choice language models important 
recognition competing word sequences 
discriminative criterion 
correlation training recognition 
rst aspect considerable ect 
worst case non matching language model recognition competing word sequences lead missing word sequences word graphs cause problem word graph densities high 
second point signi cant acoustic parameters obtained mmi training directly depend language model 
clear ect di erent language models discriminative training correlations language models training recognition unseen test data 
mmi training easily shown contributions parts training utterances decrease increasing score difference corresponding competing parts 
applies sentences words single hmm states 
hypotheses conceivable 
correlation hypothesis 
respect recognition situation expect acoustic models need optimization su ciently discriminate correct incorrect word sequences 
argument holds strong correlation language models chosen training evaluation concluded 
covering hypothesis 
respect quality acoustic model language model usually largely improves recognition accuracy cover lead away de ciencies acoustic models 
ect call suboptimal language models training 
choice language models training considerably correlate chosen evaluation 

estimation discriminative statistics section algorithm word graphs mce training cient constrained recognition approach word graphs determination competing word sequences 

discriminative training word graphs possibility include best recognized word sequence set competing word sequences application best lists chow chou 
best lists time alignment reestimation done word sequence contained best list 
di erent word sequences nbest list usually di er words states calculations done best lists redundant 
redundancy prevented word graphs discriminative training introduced valtchev 
word graph shall set word hypotheses boundary times tb te word respectively de nes set predecessor successor word hypotheses 
viterbi approximation account boundary times word word graph known viterbi alignment word sequence eq 
divided viterbi alignments individual word sequence st xr st xr tb te word part word sequence spans time tb te 
viterbi alignment done word word graph independently separates sum competing word sequences 
de ne reweighted posterior probability te xr hypothesizing word word boundary times tb te complete set acoustic observations xr utterance te xr fw wjt te xr pa xr sum numerator runs word sequences covered word graph contain word boundary times tb te 
method calculate word posterior probabilities valtchev similar calculation forward backward probabilities hmm states graph state transitions replaced word transitions word graph 
detailed description algorithm including gram language models wessel 
depending pruning characteristics chosen producing word graphs spoken word sequence included word graph 
term reweighted refers exponent te xr represents true posterior probability signi cant word sequences included set schluter speech communication discriminative training viterbi alignment spoken word sequence forced word graph constrained recognition 
spoken word sequence pruned time alignment included fb calculated 
small large vocabulary applications word graphs produced word pair approximation schwartz austin 
word probabilities viterbi alignment generalized fb probability simpli es expression viterbi crt fw te xr st xr tb te sum runs words contained set competing word sequences represented word graph pass time word probabilities word graphs complexity calculation generalized fb probabilities linear number words processed word graph covering possible word sequence resulting word graph 
noted discriminative training type word posterior probabilities applied successfully improve con dence measures large vocabulary speech recognition tasks corresponding languages wessel 

mce training word graphs performing mce training spoken word sequence wr utterance excluded calculation reweighted word probabilities eq 

order able perform calculation ciently word graphs spoken word sequence excluded word graph 
general possible exclude spoken word sequence word graph excluding word sequences time particular word hypotheses spoken word sequence schluter speech communication part sequences 
sum word sequences word graph represented including spoken word sequence performed rst subtracted probability spoken word sequences necessary te xr fw jw wr te xr fv pa xr fw te xr fw jw wr te xr pa xr fv pa xr te xr fw jw wr te tr xr xr fv pa xr te xr cf 
eq 
calculated word graphs discussed section 
note word graph contain multiple copies spoken word sequence having different word boundary times 
re ected sums subtracted numerator denominator eq 


constrained recognition word graphs small vocabulary applications discriminative training performed unconstrained recognition iteration step 
large vocabulary applications unconstrained recognition training corpora iteration discriminative training clearly unrealistic terms computation time 
valtchev discriminative training wsj si training corpus reported unconstrained recognition performed order produce initial word lattice constrained recognition iteration step discriminative training 
preliminary experiments discriminative training applying acoustic language model rescoring word graphs xed boundary times showed little ect degradations performance 
consequence developed method constrained recognition boundary times relaxed intervals boundary times word graph 
time frame new word hypotheses started word hypotheses starting exactly time frame word graph allowed approach words starting time frames vicinity time frame de ned interval ds ds shown section word graph fig 

successor word candidates obtained word graph reduce possible search space constraining lexical tree illustrated fig 

method extended constrained recognition enables recognize new word sequences originally represented word graph produced simple acoustic language model rescoring word graph boundary times subsequent word hypotheses match 
addition approach advantage tree lexicon 
experiments time interval frames ds 
order reduce computation time viterbi state alignment paths constrained recognition saved disk need estimated word wise accumulation statistics 
bit fig 

section word graph showing word hypotheses having times time interval ds ds 
approach constrained recognition word hypotheses serve successor word candidates word hypotheses time frame 
ih dh uh ih oo aa ih 
experiments small vocabulary ooh ooh experiments comparison discriminative training methods optimization criteria carried ti digit string leonard corpus american english digit strings sietill corpus telephone line recorded german digit strings 
table information corpus statistics summarized 
recognition systems digit string corpora word hmms continuous emission distributions 
characterized follows sietill recognition system german digits including gender dependent word silence hmms states plus state silence gender bit bid book fig 

schematic view lexical tree constraining recognition successor word candidates determined word graph shown fig 

words allowed hypothesized represented closed squares deactivated arcs lexical tree indicated dithered lines 
schluter speech communication mixture gaussian densities global pooled state speci diagonal covariances cepstral features rst derivatives second derivative energy ms frame shift 
ti digit string recognition system english digits including oh gender dependent word silence hmms states plus state silence gender single gaussian densities state speci diagonal covariances cepstral features rst second derivatives ms frame shift 
baseline recognizers digit string recognition apply ml training viterbi approximation ney results serve starting points additional discriminative training 
detailed description baseline system small vocabulary speech recognition welling 

training procedure complexity discriminative small vocabulary initialized standard ml training 
standard ml training consists number expectation maximization em iterations viterbi approximation followed mixture density splitting step 
procedure give optimal results ml training 
highest number densities mixture iteration viterbi training took gender resulting real time factor rtf alpha pc 
training procedures ml training follows 
order speed training table corpus statistics sietill ti digit string corpus corpus recording language test train female male total sent 
digits sent 
digits sent 
digits sietill telephone german test train ti microphone english test train schluter speech communication times experiments performed additive fashion ct followed mmi mce ct indicated detail 
single densities discriminative training initialized iterations ct served common starting point mmi mce training ct iterations 
ft training single density models iterations performed ml training 
mixture gaussian densities densities state discriminative training initialized iterations ct served common starting point mmi mce training respectively ct iterations 
ft training models densities mixture iterations performed ml training 
mmi mce training models densities mixture iterations performed initialization ml training 
acoustic models discriminative training exactly ml training number densities mixture number trained parameters training method considered 
terms computational complexity discriminative training methods discussed dominated recognition training data 
training times mmi ct mce ft show minor variations 
iteration discriminative training took slightly resulting rtf pha pc times time needed ml iteration 

convergence proof convergence exists eb training parameters continuous density hmms non nitesimal step sizes rst investigated convergence behaviour discriminative criteria applied 
rst experiments applied ct eb gd optimization methods 
iteration factors pooled variances sie till state speci variances ti digit string cf 
figs 
relatively steady convergence gd eb 
similar results observed word error rates test training data shown fig 
male portion ti digit string corpus 
clearly convergence test training data comparable holds female portion ti digit string corpus 
convergence error rate training data criterion iteration 
convergence observed ct criterion fig 
word error rates fig 
sietill training corpus show jumps course fig 

ct criterion function iteration index single gaussian densities ti digit string training corpus 
fig 

word error rate function iteration index ct single gaussian densities male portion ti digit string corpus 
training iterations single mixture gaussian densities 
preliminary experiments varying iteration factors showed despite jumps choice optimal convergence rate 
observed mmi mce ft criterion 
shown figs 
discriminative training shows non monotonic behaviour criteria error rates training data due fact convergence fig 

ct criterion function iteration index single mixture gaussian acoustic models sietill training corpus 
fig 

word error rates training corpus function iteration index corrective training ct single mixture gaussian acoustic models sietill training corpus 
schluter speech communication guaranteed 
parameter sets discriminative training digit string recognition chosen best recognition results training corpus 

recognition results especially single densities sietill corpus relative improvements word error rate compared ml training obtained 
noted baseline system ml training needed times parameters order equal results discriminative training single densities 
experiments sietill corpus mixture densities ml training needed twice number parameters equal corresponding discriminative results 
results obtained sietill corpus mixture densities best known authors 
case ti digit string corpus interesting fact reduction errors training data table fig 

hand shows strong homogeneity ti digit string corpus single densities ability model corpus completely signi cant numbers errors 
hand clearly brings limitation corrective training having errors training data prevents progress iteration process 
case sietill corpus word error rate nearly zero errors discriminative training occurs high number densities state cf 
table indicating contrast ti digit string corpus detailed acoustic modelling needed describe sietill corpus properly 

comparison parameter optimization tables results discriminative training comparing eb gd optimization 
expected analytically consistent di erences results gd eb reestimation observed ti digit string sietill corpus employing kinds acoustic modelling 
note comparative results sietill corpus schluter speech communication table recognition results ti digit string corpus 
word wer sentence error rates ser maximum likelihood ml corrective training ct extended baum eb gradient descent gd optimization 
single gaussian densities state speci diagonal covariance corpus criterion method del ins wer ser train ml ct eb gd test ml ct eb gd table recognition results sietill corpus 
word error rates wer maximum likelihood ml corrective training ct extended baum eb gradient descent gd optimization 
gaussian mixture densities densities state pooled diagonal covariance lda corpus criterion method del ins wer train ml ct eb gd test ml ct eb gd best reported produced optimizing baseline recognition system 
kapadia reported eb optimization performed better variants gd optimization 
results show compared eb performance gd matter appropriate step sizes cf 
eqs 

consequence comparison eb gd optimization arbitrarily chose gd algorithm experiments 
mce training applied gd parameter optimization formalism nding optimal step sizes obtained comparison gd eb case ct mmi criteria 

comparison discriminative criteria table shows recognition results sietill corpus obtained ml ct mmi ft mce training 
single densities best result word error rate obtained mce training mmi training ct ft gave word error rates 
reason performance mce training low model complexity outliers ignored chances modelled coarse models 
contrast mmi ct try correct outliers 
furthermore single competing word sequence mce contrast ft introduces smoothing facilitates process nding optimizing parts coarse model possible lead improvements 
mixture densities mce mmi ft criteria give consistently better results ct criterion 
corrective training mixture densities error rate training corpus rapidly reduces number word errors training corpus nearly zero 
corrective training misrecognized word sequences contribute reestimation improvement obtained 
observed ti digit string corpus single densities recognition errors occurred training data 
densities mixture signi cant di erence mce mmi detected 
densities mce improves mmi 
considering best results table recognition results sietill corpus 
word error rates wer minimum classi cation error mce maximum mutual information mmi corrective training ct gd optimization maximum likelihood ml training 
gaussian mixture densities pooled diagonal covariance lda densities mixture training criterion error rates training test del ins wer del ins wer ml ct mmi ft mce ml ct mmi ft mce ml mmi mce independent number densities mixture error rate discriminative training methods mce ft give better results mmi ct marginally better mce ft produces best word error rate sietill corpus means relative improvement nearly comparison best ml result 
higher numbers densities mixture methods deteriorated sietill data 
mce criterion comparative experiments suggest mce criterion best choice training models arbitrary complexity 

experiments large vocabulary experiments large vocabulary continuous speech recognition performed order evaluate constrained recognition approach investigate interdependence mmi training choice language models training 
experiments carried arpa wall street journal wsj corpus 
table gives information corpus statistics 
recognition system wsj corpus characterized follows recognition lexicon containing words plus variants schluter speech communication decision tree triphone states plus state silence gender independent gaussian densities global pooled diagonal covariance cepstral features rst derivatives second derivative energy ms frame shift bigram trigram language model 
small vocabulary applications baseline recognizer applies ml training viterbi approximation ney results serve starting point additional discriminative training 
description rwth large vocabulary continuous speech recognition system ney 
number di erent words observed training corpus twice number words contained recognition lexicon 
words added recognition lexicon discriminative training table corpus statistics arpa wsj nov development evaluation test training set corpus wsj speakers sent 
words nov eval nov dev train schluter speech communication contains words plus variants 
additional problem half words training recognition lexicon unknown language models recognition 
preliminary tests special language models discriminative training produce improvements original language models test corpora 
words unknown language model recognition mapped unknown word class renormalized number words included 
consequence language model perplexities training corpus signi cantly higher test corpora 
perplexities language models corresponding corpora summarized table 

training procedure constrained recognition complexity discriminative training large vocabulary initialized standard ml training consists number em iterations viterbi approximation followed mixture density splitting step 
procedure give optimal results ml training 
optimal number mixture densities pre table language model perplexities arpa wsj training testing corpora sented iteration viterbi training took wsj training corpus resulting rtf alpha pc 
discriminative training exactly acoustic models ml training number mixture densities resulting number free parameters 
large vocabulary tasks discriminative training methods computationally extensive 
training time needed determination calculation discriminative part criterion discriminative averages 
performing unconstrained recognition obtained word graphs approximately wsj training data word graph density 
word graphs took mb disk space compression 
completion unconstrained recognition pass training data took bit week alpha pc resulting rtf 
recognition time reduced rtf extended constrained recognition resulting word graph described section 
table shows recognition results mmi training rescoring constrained recognition comparison initial ml results 
clearly determination competing word corpus perplexity zero uni bi bi phr tri tri phr training nov dev 
nov eval 
notations bi phr tri phr refer language models containing phrases 
table comparison rescoring constrained recognition word graphs determination competing word sequences discriminative training 
results arpa wsj nov corpus training recognition bigram language model training criterion determination alternative word error rates word sequences dev eval dev eval ml mmi rescoring constrained recogn 
table comparison full unrestricted recognition constrained recognition word graphs ds 
recognition bigram language model recognition method search space number wer rtf states arcs trees words full constrained search space indicated numbers state arc tree word hypotheses 
real time factors rtf correspond alpha pc 
results arpa wsj nov corpus 
sequences constrained recognition performs better word graph rescoring word boundaries initial word graphs left unchanged rescoring 
constrained recognition chosen subsequent experiments mmi training 
shown table deterioration recognition performance constrained recognition algorithm reduced corresponding recognition time factor resulting rtf alpha pc 
note experiments performed unseen data 
corresponding di er training 
including calculation word probabilities reestimation process single iteration step mmi training arpa wsj training corpus took days resulting rtf alpha pc 

convergence changing discriminative training large vocabulary tasks optimization methods developed small vocabulary word recognizers transferred 
especially method obtain estimations iteration constants pooled variance checked large vocabulary 
similar small vocabulary applications mmi training wsj training corpus convergence observed mmi criterion fig 
smoothly word error rate training data fig 

note contrast small vocabulary application fig 
convergence word error rates training corpus schluter speech communication development test set similar fig 

consequently choice evaluation best recognition results development test set 

interdependence language models mmi training order check hypotheses interdependence language models discriminative training stated section experiments language models varying context length training recognition performed wsj corpus shown table 
initial recognition constrained recognition trigram training performed trigram language model constrained recognitions unigram bigram fig 

mmi criterion function iteration index wsj training corpus 
schluter speech communication fig 

word error rates function iteration index mmi training wsj training corpus wsj nov development test set 
training performed bigram 
order distinguish recognition mmi training test set recognition referred test 
testing bigram trigram language models clearly best results obtained unigram language model mmi training resulting relative improvements word error rate 
testing bigram results training trigram language model worse training 
testing trigram results training trigram language model slightly better training 
best results obtained unigram language model mmi training resulted word error rate trigram language model testing 
experiment correlation language models chosen training testing examined 
shown table comparison ml training improvements obtained mmi training bigram language model training remained approximately testing bigram trigram phrase bigram phrase trigram language model 
cases relative improvements word error rate comparison ml training ranged 
noted sets experiments clearly support covering hypothesis stated section 
suggests language models accurate fact able cover de ciencies acoustic models weighting contributions mmi training 
experiments indicate improvements obtained discriminative training particular language model fairly independent choice language model evaluation 
table comparison language models mmi training recognition 
results arpa wsj nov corpus language models criterion word error rates test training dev eval dev eval bi ml zero mmi uni bi tri tri ml zero mmi uni bi tri bi phrase ml bi mmi tri phrase ml bi mmi 
unifying approach discriminative training small large vocabulary speech recognition 
approach comparison frequently applied minimum classi cation error mce maximum mutual information mmi criteria performed 
acoustic models low complexity mce criterion give better performance mmi criterion su cient model complexity signi cant di erences observed 
parameter optimization gradient descent gd extended baum eb algorithm investigated 
case mmi criterion special step sizes gd optimization showing strong similarities eb gd optimization 
consequently experiments show signi cant di erences gd eb 
unifying criterion similarity gd eb nd optimal step sizes gd optimization eb algorithm 
case mce criterion approach lead convergence case mmi training eb optimization 
large vocabulary applications discriminative training extended constrained recognition method word graphs developed 
approach give better performance acoustic language model rescoring 
combination word graph methods accumulation discriminative statistics presents improved method cient realization discriminative training large vocabulary speech recognition 
experiments performed recognition continuous digit strings large vocabulary speech recognition 
digit string recognition ti digit string corpus american english digits sietill corpus telephone line recorded german digits 
tasks relative improvements word error rate observed comparison ml training 
largest improvements obtained low complexity acoustic models ml trained acoustic models needed times parameters outperform schluter speech communication tively trained models 
results obtained sietill corpus best known authors 
mmi training large vocabulary speech recognition investigated special interdependence choice language models training recognition 
experiments performed arpa wsj corpus 
best results obtained unigram language model mmi training 
trigram language model recognition relative improvement obtained comparison ml training leading word error rate test data 
signi cant correlation choice language models training recognition observed 
partly supported siemens ag munich 
bahl brown de souza mercer 
maximum mutual information estimation hidden markov model parameters speech recognition 
proc 
internat 
conf 
acoustics speech signal processing tokyo may vol 
pp 

bahl padmanabhan nahamoo gopalakrishnan 
discriminative training gaussian mixture models large vocabulary speech recognition systems 
proc 
internat 
conf 
acoustics speech signal processing atlanta ga may vol 
pp 

baum 
inequality applications statistical estimation probabilistic functions markov processes model ecology 
bull 
am 
math 
soc 

brown 
acoustic modeling problem automatic speech recognition 
ph thesis department computer science carnegie mellon university pittsburgh 
normandin 
inter word coarticulation modeling mmie training improved connected digit recognition 
proc 
internat 
conf 
acoustics speech signal processing april minneapolis mn vol 
pp 

chou juang lee 
segmental gpd training hmm speech recognizer 
proc 
internat 
conf 
acoustics speech signal processing march san francisco ca vol 
pp 

chou lee juang 
minimum error rate training best string models 
proc 
internat 
schluter speech communication conf 
acoustics speech signal processing april minneapolis mn vol 
pp 

chou lee juang 
minimum error rate training inter word context dependent acoustic model units speech recognition 
proc 
internat 
conf 
speech language processing september yokohama japan vol 
pp 

chow 
maximum mutual information estimation hmm parameters continuous speech recognition best algorithm 
proc 
internat 
conf 
acoustics speech signal processing april albuquerque nm pp 


comparative study linear feature transformation techniques automatic speech recognition 
proc 
internat 
conf 
spoken language processing october philadelphia pa vol 
pp 

gopalakrishnan nadas nahamoo 
inequality rational functions applications statistical estimation problems 
ieee trans 
inform 
theory 

generalization baum algorithm functions nonlinear manifolds 
proc 
internat 
conf 
acoustics speech signal processing may detroit mi vol 
pp 

kapadia valtchev young 
mmi training continuous phoneme recognition timit database 
proc 
internat 
conf 
acoustics speech signal processing april minneapolis mn vol 
pp 

leonard 
database speaker independent digit recognition 
proc 
internat 
conf 
acoustics speech signal processing march san diego ca pp 

ney 
acoustic modeling phoneme units continuous speech recognition 
proc 
fifth european signal processing conf september barcelona pp 

ney welling wessel 
rwth large vocabulary continuous speech recognition system 
proc 
internat 
conf 
acoustics speech signal processing april seattle wa vol 
pp 

normandin 
hidden markov models maximum mutual information estimation speech recognition problem 
ph thesis department electrical engineering mcgill university montreal 
normandin 
maximum mutual information estimation hidden markov models 
lee paliwal eds automatic speech speaker recognition 
kluwer academic publishers norwell ma pp 

normandin 
improved mmie training algorithm speaker independent small vocabulary continuous speech recognition 
proc 
internat 
conf 
acoustics speech signal processing may toronto canada vol 
pp 

normandin de mori 
highperformance connected digit recognition maximum mutual information estimation 
ieee trans 
speech audio process 

normandin 
mmie training large vocabulary continuous speech recognition 
proc 
internat 
conf 
spoken language processing september yokohama vol 
pp 

ney aubert 
word graph algorithm large vocabulary continuous speech recognition 
comput 
speech lang 

paliwal sagisaka 
minimum classi cation error training algorithm feature extractor pattern classi er speech recognition 
proc 
european conf 
speech communication technology september madrid vol 
pp 

woodland 
frame discrimination training hmms large vocabulary speech recognition 
proc 
internat 
conf 
acoustics speech signal processing may phoenix az vol 
pp 


discriminative training continuous speech recognition 
proc 
european conf 
speech communication technology september madrid vol 
pp 

schluter 
comparison discriminative training criteria 
proc 
internat 
conf 
acoustics speech signal processing may seattle wa vol 
pp 

schluter ney welling 
comparison optimization methods discriminative training criteria 
proc 
european conf 
speech communication technology september rhodes greece vol 
pp 

schwartz austin 
comparison approximate algorithms nding multiple best sentence hypotheses 
proc 
internat 
conf 
acoustics speech signal processing may toronto pp 

valtchev odell woodland young 
lattice discriminative training large vocabulary speech recognition 
proc 
internat 
conf 
acoustics speech signal process may atlanta ga vol 
pp 

valtchev odell woodland young 
mmie training large vocabulary recognition systems 
speech communication 
welling ney 
connected digit recognition statistical template matching 
proc 
european conf 
speech communication technology september madrid vol 
pp 

wessel schluter 
word probabilities con dence measures 
proc 
internat 
conf 
acoustics speech signal processing may seattle wa vol 
pp 

