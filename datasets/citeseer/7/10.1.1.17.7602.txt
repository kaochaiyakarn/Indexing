stacking bagged models kai ming ting ian witten department computer science university waikato hamilton new zealand cs waikato ac nz investigate method stacked generalization combining models derived different subsets training dataset single learning algorithm different algorithms 
simplest way combine predictions competing models majority vote effect sampling regime generate training subsets studied context bootstrap samples method called bagging disjoint samples call 
extends studies stacked generalization learning algorithm employed combine models 
yields new methods dubbed bag stacking dag stacking 
demonstrate bag stacking effective classification tasks training samples cover just small fraction full dataset 
contrast earlier bagging results show bagging bag stacking stable unstable learning algorithms dag stacking 
find bag stacking dag stacking higher predictive accuracy bagging show bag stacking models derived different algorithms effective bagging 
wolpert proposed stacked generalization general method high level model combine lower level models achieve greater predictive accuracy 
met success regression tasks breiman application classification tasks limited 
successfully applied stacked generalization classification tasks ting witten domain significant amount research model combination 
previous research largely restricted simple methods majority vote weight averaging breiman hansen salamon perrone cooper oliver hand 
term bagging refers majority vote combine multiple models derived single learning algorithm bootstrap samples breiman 
call similar uses disjoint samples bootstrapping 
investigates learned model majority vote combine individual models adopting framework stacked generalization 
call resulting methods bag stacking bootstrap samples training data individual models dag stacking uses disjoint samples 
breiman concluded bagging works unstable learning algorithms decision tree learners 
contrast results show bagging bag stacking dag stacking stable learning algorithms individual models derived just small fraction full dataset 
implication results models derived quickly training data methods bagging bag stacking dag stacking apply learning algorithms previously thought 
concentrates comparing predictive accuracy bagging bag stacking dag stacking combine models derived single learning algorithm different learning algorithms 
section formally introduces notions bagging bag stacking dag stacking 
section reports results obtained stacking models derived single learning algorithm section examines stacking models derived different learning algorithms 
section discusses issues followed related 
bagging bag stacking dag stacking training dataset xn yn class value nth instance xn vector representing attribute values consider subsets samples produced sampling regimes bootstrap samples randomly sample replacement subsets size disjoint samples randomly sample replacement disjoint subsets size kn learning algorithm derive models subsets 
learning algorithm called level generalizer resulting models level models 
bagging method breiman uses majority vote combine classification outputs models derived bootstrap samples 
method call uses majority vote combine outputs models derived disjoint samples 
majority vote consider higher level learning algorithm combine level models spirit stacked generalization wolpert ting witten 
previous implementations stacked generalization employ cross validation generate higher level data 
test set models despite fact subsets train models 
suppose output classes ki denote probability kth model assigns ith class test instance vector kn ki ki gives kth model class probabilities nth instance testing process data assembled output models kn called level data 
learning algorithm call level generalizer derive model predicts class level data 
called level model 
classify new instance level models produce vector ki pk pki input level model output final classification result instance 
estimate method predictive accuracy completely separate test dataset depending sampling strategy produce data level models derived call implementation stacked generalization dag stacking bootstrap disjoint samples aggregation stacking 
subsections describe pertinent details level level generalizers 
level generalizers learning algorithms level known decision tree learner quinlan nb re implementation naive bayesian classifier cestnik 
unpruned trees derived breiman discovered aggregation bagged models eliminate overfitting 
necessary implementation level generalizers produce output class probabilities instance cases exhibit formulas estimate 
consider leaf decision tree instance falls 
number training instances class leaf suppose majority class leaf gamma gamma theta nb ijx posterior probability class instance ijx ijx experiments confirm 
cases class level model predicts instance breiman claims bagging improve predictive accuracy learning algorithms unstable unstable learning algorithm small perturbations training set produce large changes derived model 
nb unstable stable respectively enables investigate bag stacking dag stacking conditions 
section learning algorithm nb derive level models 
section 
level generalizer previously discovered stacked generalization works multi response linear regression algorithm mlr level generalizer ting witten 
consequently algorithm bag stacking dag stacking 
mlr adaptation squares linear regression algorithm breiman regression settings 
classification problem real valued attributes transformed multi response regression problem 
original classification problem classes converted separate regression problems problem class instances responses equal class zero 
input mlr level data 
linear regression class simply lr ff ki ki choose coefficients fff ki minimize yn xn yn gamma ff ki ki coefficients fff ki constrained nonnegative 
accomplished constrained squares algorithm described lawson hanson derive non negative regression coefficients class 
position describe working mlr 
classify new instance compute lr classes assign instance table datasets experiments dataset training classes attr 
test dna satellite letters shuttle class greatest value lr lr stacking models derived algorithm describe experiments investigate dag stacking models derived single learning algorithm 
datasets moderate large ones statlog project michie breiman summarized table 
includes separate training test sets 
small datasets investigate models derived fraction original dataset 
subsections compare predictive error rate bagging bag stacking dag stacking 
vary data size level models derived number level models combined 
bag stacking dataset parts training data derive models entirely separate test set assess error rate 
es error rate single model derived eb ebs error rates bagging bag stacking respectively 
tables show figures level generalizers nb respectively 
table uses es testing error rate pruned tree noted earlier unpruned trees 
columns give eb ebs level models derived just instances training set small values size samples indicated column 
results available letters dataset take long compute see section 
table error rates bagging bag stacking models dataset es eb ebs eb ebs eb ebs dna satellite letters na na shuttle dataset bold face indicate error rates lower value es addition underlining compare results value eb obtained largest values level models letters dataset trained size full dataset 
lowest error rates eb ebs dataset marked 
figures bold show remarkable result bagging bag stacking achieve better predictive accuracy single model derived entire dataset level models derived small fraction dataset 
apparent dna satellite datasets combining models datasets combining nb models 
cases bag stacking yields superior performance bagging cases combining models dna dataset satellite dataset 
table error rates bagging bag stacking nb models dataset es eb ebs eb ebs eb ebs dna satellite letters na na shuttle turn attention underlined figures 
apparent bagging bag stacking models error rate tends decrease training size increases expect 
evidence bagging bag stacking nb models mixed 
expected trend followed dna dataset satellite dataset error rate increase letters dataset error rate bag stacking demonstrates shape trend bagging follows normal trend increases 
cases predictive error rates bagging bag stacking decrease increases 
bagging bag stacking models lowest error rate achieved bag stacking models derived small fraction full dna dataset 
datasets lowest error rate achieved bag stacking models derived size full dataset 
bagging bag stacking nb models lowest error rate achieved bag stacking models derived small fraction full dataset table bagging bag stacking models dataset es eb ebs ed eds dna data satellite data letters data shuttle data datasets 
dag stacking section compares methods combining models 
investigations error rates calculated test set entirely separate training data ed eds error rates dag stacking respectively 
figures es eb ebs previous subsection included ease comparison 
results tabulated table models table nb models 
column indicates values subsets disjoint models necessary kn cases 
order reduce number tests done es eb ebs figures previous re choosing subsets instances possible 
dna dataset training instances split equal subsets 
satellite dataset table bagging bag stacking nb models dataset es eb ebs ed eds dna data satellite data letters data shuttle data training instances split subsets split subsets 
letters half training data split subsets 
shuttle data training instances split subsets split subsets 
examination values ed eds final column tables reveals dag stacking bag stacking yields lower predictive error rate 
exception combining models satellite dataset 
comparing bag stacking dag stacking ebs vs eds gives clear indication method superior 
true comparing bagging eb vs ed summary summarizing experiments find ffl stacking mlr yields lower predictive error rate majority vote combining bagged models ffl bag stacking dag stacking comparable predictive accuracy bagging ffl bagging bag stacking better small fraction entire dataset derive models ffl bagging bag stacking dag stacking unstable stable nb learning algorithms 
bag stacking models derived different algorithms section investigate bag stacking models derived nb compare predictive error rate bagging 
table shows result bagging bag stacking number models generated nb sample size 
final columns give corresponding figures learning algorithm 
results reveals important feature bagging predictive error rate base models fairly close bagging improve accuracy 
apparent datasets 
difference error rate eb homogeneous models large case performance bagging models derived heterogeneous level generalizers falls far short better homogeneous cases 
hand dna dataset heterogeneous model outperforms better homogeneous models difference error rate smaller 
accords results ting witten combining different types learning algorithms majority vote 
bag stacking suffers problem far smaller extent datasets show 
investigate phenomenon tried combining unequal numbers models derived nb different values long homogeneous cases yield comparable performance 
datasets combining heterogeneous models performs worse combining homogeneous models investigation satellite letters 
shuttle dataset difference homogeneous models great comparable performance obtained see tables 
table bagging bag stacking level models different algorithms nb nb dataset eb ebs eb ebs eb ebs dna theta theta theta satellite theta theta theta letters theta theta shuttle theta theta table bagging bag stacking different numbers level models learning algorithms nb nb dataset eb ebs eb ebs eb ebs satellite letters refer tables choose values homogeneous cases comparable performance 
datasets choose near best performing bagging bag stacking nb models satellite dataset letters dataset 
choose comparable performing derived models satellite dataset letters dataset 
re state results settings second third columns table 
bag stacking tolerant differences level performance expect bag stacking heterogeneous models yield better predictive accuracy cases bagging experiment 
results heterogeneous model shown column table 
bagging eb heterogeneous model lower error rate homogeneous models just case letters dataset difference error rates small 
bag stacking ebs heterogeneous model yields lower predictive error rates homogeneous ones cases 
results confirm expectation 
discussion full dataset generate level model results bagging agreement breiman bagging increases predictive accuracy unstable learning algorithms stable ones 
just small proportion data generate level models find bagging improve predictive accuracy stable learning algorithms 
accordance results studies base line behavior different kind ting low ting witten uses different model combination methods majority vote contradicts conventional wisdom data better 
certainly significant implications learning time level model uses training data obtained faster 
bagging bag stacking ideally suited parallel processing level model constructed independently 
uses mlr method level learning benefit parallelism 
class problem regression class carried independently cpus 
execution time mlr depends number classes involved training set size 
models combined class problem number attributes level data ki regression algorithm executed times 
shows execution time mlr datasets sun machine 
letters classes execution time increases dramatically number models 
increase little linear datasets maximum number classes 
note training data size level learning need learning algorithm learning curve levels tail curve training data size increases large databases mlr requires part full dataset yield model performs 
number models mlr execution time dna minutes satellite minutes letters hours shuttle hours computation time mlr thought obvious bag stacking outperform bagging additional level learning inevitably provides information mere majority vote 
learning algorithm suitable level generalizer 
ting witten show learning algorithms tested mlr performs satisfactorily nb ib 
note cases size full dataset required train level models improvement bag stacking bagging usually small 
reason level data produced majority data train level models 
probabilities kn employed level data estimated 
way rectify problem bag estimation breiman get better estimate probabilities 
related research reported inspired breiman ting witten 
breiman introduces idea bagging breiman shows combining models majority vote derived small fraction entire dataset gives accuracy better single model derived entire dataset 
contribution show stacking procedures generally works better combining majority vote 
wolpert introduced stacked generalization long ago ting witten shows convincingly classification tasks 
key output class probabilities level models level data mlr level generalizer 
incorporates framework cross validation details differences implementations summarized appendix 
stacked generalization classification tasks limited focus evaluates results just datasets leblanc tibshirani chan stolfo kim bartlett merz fan 
researchers investigated various methods combining models produced single learning algorithm entire dataset 
different models generated varying learning parameters hansen salamon perrone cooper kwok carter oliver hand kononenko kovacic different sampling methods freund schapire ali pazzani 
techniques combine individual models include weighted majority vote weighted averaging bayesian likelihood combination distribution summation 
uses learning algorithm perform level learning 
ting low study base line behavior empirically 
theoretical includes kearns seung meir 
shows stacked generalization successfully applied combine bagged models derived single multiple learning algorithms 
stacking mlr yields lower predictive error rate majority vote combining bagged models 
dag stacking stable unstable learning algorithms subsets cover small fraction full dataset derive level models 
combining models derived different learning algorithms necessary models perform comparably order guarantee increased predictive accuracy bagging bag stacking 
bag stacking tolerant bagging differences level performance 
dag stacking shown comparable bagging bag stacking 
finding opens application dag stacking line data constantly arrives batches 
apply stacked generalization framework arcing breiman yielding stacking models 
plan investigate method near 
acknowledgments authors new zealand marsden fund financial support research 
appendix implementations stacked generalization appendix key differences stacked generalization described ting witten 
denote sg ease 
differences training process mainly affect computational requirements 
suppose level models 
ffl sg relies cross validation obtain level data 
suppose fold cross validation employed 
learning time level model testing time instance computational time required preparation level data jc size give dataset ffl bag stacking dag stacking need just round learning level model 
computational time required prepare level data conclude sg needs factor computation time bag stacking dag stacking learning algorithm nb 
bag stacking dag stacking sg training data necessary 
sg requires final training uses entire dataset level model complete training process 
re training required bag stacking dag stacking 
reader referred ting witten differences initial proposal stacked generalization wolpert successful implementation classification tasks 
ali pazzani error reduction learning multiple descriptions machine learning vol 
pp 

breiman 
stacked regressions machine learning vol 
pp 

breiman 
bagging predictors machine learning vol 
pp 
breiman 
bias variance arcing classifiers technical report department statistics university california berkeley ca 
breiman 
pasting bites prediction large data sets line ftp stat berkeley edu users pub breiman pasting ps 
breiman 
bag estimation ftp stat berkeley edu users pub breiman 
ps 
cestnik 
estimating probabilities crucial task machine learning proceedings european conference artificial intelligence pp 

chan stolfo comparative evaluation voting meta learning partitioned data proceedings twelfth international conference machine learning pp 
morgan kaufmann 
fan chan stolfo comparative evaluation combiner stacked generalization proceedings aaai workshop integrating multiple learned models pp 

freund schapire experiments new boosting algorithm proceedings thirteenth international conference machine learning pp 
morgan kaufmann 
hansen salamon neural network ensembles ieee transactions pattern analysis machine intelligence pp 

kearns seung learning population hypotheses machine learning pp 
kluwer academic publishers 
kim bartlett error estimation series association neural network systems neural computation pp 
mit press 
kononenko kovacic learning optimization stochastic generation multiple knowledge proceedings ninth international conference machine learning pp 
morgan kaufmann 
kwok carter multiple decision trees uncertainty artificial intelligence shachter eds pp 
north holland 
lawson hanson solving squares problems siam publications 
leblanc tibshirani combining estimates regression classification tr department statistics university toronto 
meir 
bias variance combination estimators case linear squares tr dept electrical engineering technion haifa israel 
merz 
dynamic learning bias selection proceedings fifth international workshop artificial intelligence statistics ft lauderdale fl unpublished pp 

michie spiegelhalter taylor machine learning neural statistical classification ellis horwood limited 
oliver hand pruning averaging decision trees proceedings twelfth international conference machine learning pp 
morgan kaufmann 
perrone cooper networks disagree ensemble methods hybrid neural networks artificial neural networks speech vision editor chapman hall 
quinlan 
program machine learning morgan kaufmann 
ting low model combination multiple data batches scenario appear proceedings ninth european conference machine learning 
ting witten stacked generalization appear proceedings fifteenth international joint conference artificial intelligence 
long version available working department computer science university waikato 
www cs waikato ac nz cs staff html 
wolpert 
stacked generalization neural networks pp 
pergamon press 
