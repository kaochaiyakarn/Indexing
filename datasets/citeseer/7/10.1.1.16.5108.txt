radial basis function networks nonparametric classification function learning adam department computer science concordia university montreal pq canada email cs concordia ca heinrich niemann institute pattern recognition university erlangen erlangen germany email niemann informatik uni erlangen de apply normalized radial basis function networks function learning nonparametric classification 
simple parameter learning technique proposed convergence rates convergence empirically trained networks studied theoretically computer experiments 

artificial neural networks turned useful learning nonlinear mappings random observations designing nonlinear discriminant functions 
nonlinear function learning classification closely related 
problems try recover unknown nonlinear mapping estimate unknown bayes decision set independent observations sampled unknown distribution function 
multilayer perceptron mlp radial basis function networks rbf widely network architectures signal processing pattern recognition 
hidden layer mlp network output produced linear combinations outputs hidden layer nodes neuron maps weighted average inputs nonlinear threshold sigmoid 
hidden layer rbf network hidden nodes map distances input vectors center vectors outputs nonlinear kernel radial function 
sigmoidal mlp networks shown universal approximators able approximate arbitrary nonlinear mapping ir ir 
bounds approximation errors iterative approximation schemes rates convergence terms hidden layer size research supported nserc alexander von humboldt foundation 
studied 
approximation estimation error results integrated barron enabling show rates convergence function learning problem 
rbf nets received considerable attention 
girosi showed rbf nets solutions regularization problem empirical risk combined additive term penalizing departure smoothness :10.1.1.48.9258
universal approximation ability rbf nets proven park sandberg xu 
studied universal consistency rbf nets function estimation 
approximation error convergence rates girosi tradeoff approximation estimation error function estimation studied niyogi girosi linder consider normalized rbf called nets obtained dividing radial function rbf net sum radial functions 
way outputs hidden layer size normalized add 
normalized rbf nets introduced moody darken universal consistency rates convergence studied xu 
convergence analysis rbf network basic approaches applied 
approach learning theory tools vapnik chervonenkis dimension random covering numbers complexity regularization obtain universal convergence rates rbf nets 
approach similarity nets kernel regression estimation pointed tools nonparametric regression estimation applied studying convergence rates nets 
bound empirical risk net expressed terms kernel regression estimate performance training sequence established 
authors studied generalization ability subclass nets diagonal covariance matrix identical diagonal elements yielding radially symmetric radial basis functions 
consider larger class networks studied networks diagonal covariance matrices positive arbitrary elements 
propose simple computationally efficient algorithm learning parameters networks 
apply networks estimate arbitrary nonlinear mappings observations design asymptotically bayes optimal classifiers nonparametric classification problem 
divided sections 
section describe networks parameter learning problem 
section net classifier introduced 
convergence rates nets function learning classification discussed section 
section report simulation results performance empirical classifiers 

rbf nets definitions parameter learning pair random vectors ir theta ir jx xg corresponding regression function 
nonlinear prediction accuracy predictor ir ir measured expected risk gamma ey 
known optimal predictor minimizing expected risk coincides regression function inf gamma gamma distribution known principle determined 
learning predictor determined training sequence independent identically distributed copies 
assume independent chosen class functions increasing complexity 
concerned empirical predictor classes radial basis networks hidden nodes 
standard rbf nets gamma gamma gamma output layer scalar weights ir dimensional center vectors hidden layer consisting nodes theta positive definite covariance matrix weights determining receptive field basis function radial basis function ir ir usually unimodal gaussian type function 
normalized rbf nets obtained normalization network sum radial functions gamma gamma gamma gamma gamma gamma net may rewritten nonlinear probability weights satisfying 
covariance matrix written dr rotation matrix determining orientation diagonal matrix determines size receptive field consider rbf nets diagonal covariance matrix diag rotationally symmetric gamma gamma gamma general solutions equation gamma gamma gamma arbitrary constant ir main axes oriented coordinate axes 
vector parameters 
determining parameters training sequence called parameter learning 
possible parameter learning strategies common 
minimize empirical risk gamma respect class nets defined 

cluster denote cluster centers determine remaining parameters minimizing empirical risk 

determine centers output weights directly assignment assuming size learning sequence larger number nodes hidden layer sample replacement subset fx assign determined rules specified section 
strategies described strategy 
general demanding computationally difficult evaluate theoretically 
approach 
faced difficult nonlinear optimization problem minimizing respect choose strategy 
simplest effective computationally demanding 
assuming parameter learning approach 
yields network gamma gamma gamma gamma gamma gamma network requires parameter learning output weights center vectors determined data chosen independently data 
performance studied section 
networks diagonal covariance matrix equal elements trained strategy 
related parzen density estimate nh gamma ir ir normalized kernel scalar bandwidth 
similar network called probabilistic neural network proposed specht 
networks constrains related kernel regression estimate gammax gammax weighted average nonlinear weights gammax hn gammax hn adjusted sample size 
average estimate conditional mean output input equivalently regression function jx 
apparent connection networks kernel regression estimation exploited section study asymptotic performance 
nonparametric classification classification pattern recognition problem observation random vector ir determine value corresponding label binary random variable values gamma 
decision function ir gamma goodness measured error probability pfg yg 
known decision function minimizes error probability ae gamma jx called bayes decision error probability pfg yg bayes risk 
nonparametric approach joint distribution unknown decision learned training sequence consists independent copies ir theta gamma pair 
formally decision rule function ir theta ir theta gamma gamma error probability pfg jd note random variable depends random training sequence notational simplicity write 
sequence classifiers fg called universally consistent pfg jd gamma probability distribution 
intuitively clear pattern recognition closely related regression function estimation 
seen observing function defining optimal decision just regression function jx 
estimate regression function expected yield performance decision rule classifier ae gamma 
known inequality pfg jd gamma pfg yg gamma gamma gamma fi fi delta delta provides link performance nonlinear mapping predictor empirical classifier 
consistency rates convergence section convergence rate convergence results normalized rbf nets function learning classification 
theorem convergence ey suppose max id 
ball centered finite radius gamma pfg jd gamma probability distribution 
condition defines class bounded radial functions compact support 
class may enlarged include functions infinite support replaced regularity condition ci sup dx expense having assume bounded condition obviously fulfilled classification 
condition basically satisfied positive riemann integrable bounded away zero origin 
holds gaussian functions radial functions tails decreasing polynomially order 
conditions mean receptive field parameters reduced size learning sequence rate 
theorem rate convergence assumptions theorem hold addition assume compact support satisfies lipschitz condition order gamma gamma yjj gamma gamma pfg jd gamma gammaa probability rate convergence depends smoothness expressed best rate gamma function learning gamma classification obtained 
better rates ones stipulated theorem obtained smoother imposing additional constraints theorem remain true full matrix 
consequence fact covering numbers sets gamma gamma gamma scaled sets shape invariant orientation depend point performance net training sequence deduced performance upper bound introduced lemma 
specifically gamma gamma inf class nets full matrix net trained method set independent ej gamma ej gamma straightforward calculations ej gamma gamma theorem right side converges rate gamma 
observe imply convergence gamma inequality valid training sequence 

simulation results tested normalized rbf net classification algorithm dimensional data sets 
data points drawn independently uniform distribution square gamma theta gamma classified classes decision rule obtain 
experiments decision rules ae sin px gamma gamma 
set experiments learning strategy 
set parameters net 
set centers weights neurons respectively 
gammax kernel function 
generated data different size set diagonal elements inverted covariance matrix gamma table 
covariance matrix neurons 
data size gamma gamma table 
data sizes diagonal elements covariance matrix learning strategy 
shows examples resulting networks considered data sizes 
generated samples data size trained network obtain network output functions 
regression error assumed regression function identical decision function numerically computed regression error gamma dx 
averaged samples estimate expected regression error probability misclassification numerically computed dx averaged samples estimate expected classification error rate 
sample computed empirical regression classification errors gamma compare empirical errors estimated expected errors data size calculated averages samples 
shows results 
consider non overlapping classes bayes error zero expected empirical estimated error terms converge zero 
note empirical regression error smaller expected regression error stated second set experiments learning strategy 
train parameters net 
sample points class boundary decision boundary sample points class boundary decision boundary sample points class boundary decision boundary sample points class boundary decision boundary sample points class boundary decision boundary sample points class boundary decision boundary 
resulting networks learning strategy data points 
indicates data points boundary classes sin px decision boundary generated net 
estimated expected regression error average empirical regression error estimated probability misclassification average empirical classification error 
expected empirical error rates terms data size learning strategy 
drawn sample described 
chosen points obtain random sample created neuron initial center initial weight 
initial values diagonal elements inverted covariance matrix set values set experiments see table 
starting initial values iteratively computed parameters standard steepest descent method convergence 
iteration partial derivatives computed line search performed direction negative gradient find minimum 
shows examples discriminant boundaries generated resulting networks different parameter settings 
class points class points class boundary decision boundary neurons class points class points class boundary decision boundary neurons class points class points class boundary decision boundary neurons class points class points class boundary decision boundary neurons class points class points class boundary decision boundary neurons class points class points class boundary decision boundary neurons 
resulting networks learning strategy 

decision function 
indicates data points classes boundary classes decision boundary generated net 
centers large crosses coincide neuron centers lengths horizontal vertical cross arms proportional gamma gamma respectively 

studied new class radial basis network function learning algorithms classifiers derived normalized rbf networks trained simple assignment algorithm 
obtained convergence rates convergence terms number hidden units 
provided suggestions optimal size hidden layer learning lipschitz regression functions classification lipschitz posteriori probabilities 
computer experiments compared training strategies normalized rbf nets 
simulation results confirm theoretical predictions algorithms performance 
barron universal approximation bounds superpositions sigmoidal function ieee trans 
information theory vol 
pp 

barron approximation estimation bounds artificial neural networks machine learning vol 
pp 

devroye lugosi probabilistic theory pattern recognition 
springer new york 
girosi rates convergence radial basis functions neural networks artificial neural networks speech vision ed chapman hall london 
girosi jones poggio regularization theory neural networks architectures neural computation vol :10.1.1.48.9258
pp 

applied nonparametric regression 
cambridge university press cambridge 
hornik stinchcombe white multilayer feedforward networks universal approximators neural networks vol 

linder lugosi nonparametric estimation classification radial basis function nets empirical risk minimization ieee trans 
neural networks vol 
pp 

linder radial basis function networks complexity regularization function learning ieee trans 
neural networks vol 
pp 

moody darken fast learning networks locally tuned processing units neural computation vol 
pp 

niyogi girosi relationship generalization error hypothesis complexity sample complexity radial basis functions neural computation vol 
pp 

park sandberg universal approximation radial basis function networks neural computation vol 
pp 

scott multivariate density estimation theory practice visualization 
wiley new york 
smoothing methods statistics 
springer new york 
specht probabilistic neural networks neural networks vol 
pp 

xu yuille radial basis function nets kernel regression approximation ability convergence rate receptive field size neural networks vol 
pp 

