sequential algorithm training text classifiers david lewis lewis research att com william gale gale research att com bell laboratories murray hill nj usa bruce croft van rijsbergen eds sigir proceedings seventeenth annual international acm sigir conference research development information retrieval springer verlag london pp 

ability cheaply train text classifiers critical information retrieval content analysis natural language processing tasks involving data partly fully textual 
algorithm sequential sampling machine learning statistical classifiers developed tested newswire text categorization task 
method call uncertainty sampling reduced fold amount training data manually classified achieve level effectiveness 
text classification automated grouping textual partially textual entities 
document retrieval categorization routing filtering clustering natural language processing tasks tagging word sense disambiguation aspects understanding formulated text classification 
amount online text increases demand text classification aid analysis management text increasing 
advantage formulating text processing tasks classification methods statistics machine learning form text classifiers automatically 
machine learning require manually annotating training data class labels annotation takes skill expense instance building classification rules hand 
text available economically labeled subset sample data chosen label 
random sampling usually effective 
texts class members atypical texts labeled random sample usually contain negative examples positive ones 
support training classifier distinguish positive negative examples 
relevance feedback kind nonrandom sampling 
effect users asked label texts current classifier considers class members 
approach call relevance sampling reasonable strategy text retrieval context user interested seeing relevant texts effectiveness final classifier produced 
relevance feedback proposed finding examples unusual word senses 
relevance feedback problems approach sampling 
works poorly classifier improves susceptible selecting redundant examples 
relevance sampling sequential approach sampling labeling earlier examples influences selection ones 
describes alternative sequential approach uncertainty sampling motivated results computational learning theory 
uncertainty sampling iterative process manual labeling examples classifier fitting examples classifier select new examples class membership unclear 
show probabilistic classifier trained uncertainty sampling 
test method text categorization task showed reductions fold number examples labeled produce classifier effectiveness 
learning queries classifier learned fewer examples learning algorithm allowed create artificial examples membership queries ask teacher label 
learning tasks creation artificial examples problem 
artificial text created learning algorithm legitimate natural language expression probably uninterpretable human teacher 
exception large quantities previously labeled text available automated text categorization deployed replace aid existing staff manual indexers 
queries refers membership queries text retrieval queries 

create initial classifier 
teacher willing label examples apply current classifier unlabeled example find examples classifier certain class membership teacher label subsample examples train new classifier labeled examples 
algorithm uncertainty sampling single classifier 
algorithms learning queries proposed filter existing examples creating artificial ones 
algorithms ask teacher label examples class membership sufficiently uncertain 
definitions uncertainty estimating classifier trained previously labeled data produce correct class label unlabeled example 
looking sampling method querying call approach uncertainty sampling 
seung opper sompolinsky theoretical analysis query committee qbc algorithm unlabeled example draws classifiers randomly version space set classifiers consistent labeled training data 
infinite stream unlabeled data assumed qbc asks teacher class labels examples chosen classifiers disagree 
freund seung shamir tishby extend qbc result wide range classifier forms 
prove certain assumptions number queries examining random examples logarithmic generalization error decrease quickly queries examples 
precisely generalization error decreases 
terms number queries generalization error decreases exponentially fast 
provocative result implies effect training labeled data gotten cost obtaining unlabeled data labeling logarithmic fraction 
qbc assumptions include data noise free perfect deterministic classifier exists possible draw classifiers randomly version space problematic real world tasks 
effectiveness qbc related methods real world tasks remains determined 
heuristic alternative qbc random drawing classifiers version space learning algorithm pick single classifier version space 
classifier classification decisions estimate certainty certainty estimate select examples 
single classifier approach uncertainty sampling theoretical failings including underestimation true uncertainty biases caused classifiers 
hand experiments single classifier arbitrary queries select subsets labeled data shown substantial speedups learning 
relevance sampling proven quite effective text retrieval uses single classifier 
uncertainty sampling algorithm presents algorithm uncertainty sampling finite set examples single classifier 
ideally number examples selected iteration larger values may appropriate scoring selecting examples expensive 
algorithm type classifier predicts class provides measurement certain prediction probabilistic fuzzy nearest neighbor neural classifiers satisfy criterion easily modified 
difficult requirement measurements relative certainty produced classifier formed training examples 
uncertainty sampling similar strategy training misclassified instances 
difference data labeled classifier guess examples misclassified 
note initial classifier plays important role may long period random sampling examples low frequency class 
probabilistic text classifier section describe classifier form produces estimates jw posterior probability example pattern belongs class estimates probability decide example assigned class estimate classification correct 
describe classifier trained uncertainty sampling classification 
probabilistic classifier classifiers estimate posterior probability bayes rule jw wjc theta wjc theta applied variety text classification tasks including text retrieval text categorization word sense identification 
disjoint exhaustive set classes example may belong observed pattern 
wjc conditional probability example pattern belongs class prior probability example belongs class treat case classes gamma 
case useful express relative posterior probabilities odds ratio cjw cjw theta wjc wj huge number possible estimation wjc wj direct observation training set futile 
making certain independence assumptions decomposition cjw cjw theta jc fact cjw gamma cjw plus arithmetic manipulations get expression cjw cjw exp log gammap log jc exp log gammap log jc equation rarely directly text classification probably estimates cjw systematically inaccurate 
reason inaccuracy independence assumptions producing equation incorrect words features defined natural language 
problem typically small hard estimate problem compounded training set random sample 
logistic regression provides partial solution problems 
general technique combining multiple predictor values estimate posterior probability 
form estimate cjx exp mxm exp mxm number approaches logistic regression text classification proposed 
similarity equation equation prompted try particularly simple approach log likelihood ratio bayesian independence formulation single predictor variable cjw exp log jc exp log jc intuitively hope logistic parameter substitute hard estimate prior log odds equation serve dampen extreme log likelihood ratios resulting independence violations 
fact simple formulation text categorization compared complex formulations suggested authors 
note approach probably appropriate documents widely varying lengths 
careful distinguish example corresponding pattern different examples may feature values training classifier step equation estimating values jc 
estimator jc pi np np nn np np np nn cni nn np nn nn nn np nn nn numbers tokens positive negative training sets respectively pi ni correspondingly number instances positive negative training sets number features 
ad hoc estimator loosely justified analogy expected likelihood estimator 
estimator attempts avoid extreme estimates log likelihood ratio nn different sizes instance sampling procedure starts finding positive examples 
text classification typically huge set potential instance types distinct words collection documents 
feature selection reduce set equivalently lock values improve effectiveness 
feature quality measure pi ni log jc selected features order value specified fraction experiments reported total score training examples reached 
done separately features positive negative log likelihood ratios 
feature selection performed log likelihood values compute log jc training example 
logistic regression find values give best fit value probability class membership 
uncertainty sampling probabilistic classifier uncertainty sampling simple classifier estimates cjw 
iteration current version classifier applied example examples estimated cjw values closest selected corresponds classifier uncertain class label 
adopted slightly complex method scoring examples choosing examples closest examples closest 
method guarantees half examples selected iteration exact duplicates examples score score 
addition evidence training pairs examples opposite sides decision boundary useful 
classification probabilistic classifier advantage classifier provides accurate estimates cjw certain assumptions decision theory gives optimal rule deciding example assigned class 
ij penalty loss incurred deciding class true class 

assign example class exactly cjw gamma cjw cjw gamma cjw instance desire minimum error rate types incorrect decisions equally bad appropriate losses 
experiment conducted experiment see uncertainty sampling reduce amount labeled data necessary train classifier comparison random sampling relevance sampling 
training method probabilistic classifier section 
classifiers trained perform text categorization task news story titles 
training test category number freq 
number freq 
bonds ireland budget hostages table 
categories experiments number occurrences frequency occurrence training test sets 
data set titles items appeared ap newswire early divided randomly training set titles test set titles 
titles processed lower casing text removing punctuation 
word boundaries defined whitespace 
titles full text items minimize computation 
categories assigned keyword keyword line ap item 
keyword string characters indicating content item 
keywords required identical updated items news story practice considerable reuse keywords parts keywords story story year year aspects controlled vocabulary 
defined categories ap titles particular substrings appeared keyword field 
instance stories assigned bond category keyword shown bold savings bond sale plunge rate cut treasury announces percent reduction savings rate flood flood victims permitted cash savings bonds early mesa bond exchange offer wednesday report wall street bond firms ban political contributions bond james bond gave name fictional agent people bond julian bond rights movement needs individuals leaders clinton president elect plays touch football bank failures bank failures year low taxes taxes savings bonds treasury shifts borrowing away long term bonds tougher political contribution rules bonds proposed categories defined fashion somewhat messy semantically 
julian bond james bond included savings bonds lose items financial bonds keyword truncated misspelled emphasized aspect story 
perfect categorization category definitions possible 
believe serious problem interested relative absolute effectiveness categorization methods 
categories provide useful test robustness methods errors training data 
categories defined shown table frequencies training test sets 
categories chosen relatively low frequencies providing reasonable number positive examples training test sets 
training initial classifier required uncertainty sampling algorithm produced set words suggested teacher just classifiers constructed texts user requests text retrieval systems 
avoid experimenter bias starting subsample positive examples category randomly selected training set 
feature selection words examples addition words chosen described section 
run starting examples train initial classifier iterations uncertainty sampling subsample size carried described section 
subsample selected category labels looked examples added set labeled examples training classifier 
classifier produced iteration example selection iteration retained evaluation 
order study impact starting subsample quality final classifier repeated process times category time different starting subsample positive examples 
compared uncertainty sampling relevance sampling random sampling 
relevance sampling carried identically uncertainty sampling examples highest values cjw chosen values close 
random samples combined starting subsamples positive examples truly random samples various sizes 
fashion training sets sizes produced 

larger sets included smaller ones 
classifier formed sets training methods uncertainty relevance sampling classifier evaluation guide sampling 
runs done starting subsamples category giving total runs category 
evaluation treated categories binary classification task evaluated classifiers category separately 
classifiers evaluated applying minimum error rate loss parameters test items comparing classifier decisions actual category labels 
classifiers trained random samples evaluated 
classifiers formed iterations uncertainty relevance sampling plus th iteration evaluated 
text categorization effectiveness measures recall precision defined follows recall number test set category members assigned category number category members test set precision number test set category members assigned category total number test set members assigned category comparing classifiers desirable single measure effectiveness 
van rijsbergen defined measure combination recall precision satisfying certain measurement theoretic properties pp 
gamma fi pr fi parameter fi ranges infinity controls relative weight recall precision 
fi corresponds equal weighting recall precision 
get single measure effectiveness higher values correspond better effectiveness recall precision equal importance define fi gamma fi results table shows category mean fi classifiers formed uncertainty sampling formed relevance sampling full training set 
show effectiveness just starting examples plus randomly selected examples 
gives sense quality initial classifier 
categories uncertainty sample texts resulted classifier substantially effective initial classifier formed relevance sample texts 
classifier usually similar better effectiveness trained texts 
classifiers trained random sample texts cases low effectiveness 
plots effectiveness sample size uncertainty sampling relevance sampling random sampling 
results categories omitting strategy worked 
uncer 
rand 
rel 
full category mean sd mean sd mean sd mean sd bonds ireland budget hostages table 
mean standard deviation fi training initial examples combined uncertainty selected examples random examples relevance selected examples remaining examples 
means runs uncertainty relevance sampling runs random full sampling 
discussion shows classifier effectiveness generally increases sample size sampling methods faster sequential methods 
sequential methods uncertainty sampling substantially outperforms relevance sampling 
results hold categories widely varying absolute levels effectiveness 
superiority uncertainty sampling relevance sampling particularly notable low frequency categories limited danger relevance sampling positive examples 
difference uncertainty sampling relevance sampling lower frequent categories 
uncertainty sampling better higher mean lower standard deviation 
cases effectiveness levels reached random sampling training examples reached uncertainty sampling examples training randomly selected examples gives greatly inferior results 
cases reaching level effectiveness requires times randomly selected examples examples selected uncertainty sampling 
caution required comparing results small uncertainty samples large random samples 
categories mean fi classifier trained uncertainty sample examples exceeds training full training set examples 
means aspect classifier training making effective large training sets 
feature selection villain 
method produced features applied full training set previous suggests 
graphs average effectiveness hide variation run run 
standard deviations shown table amount mean effectiveness meaning quality final classifiers somewhat unpredictable 
source variation initial subsamples positive examples 
subsample badly unrepresentative category initial classifier ineffective considerable delay uncertainty sampling started find additional positive examples 
initial classifiers raw effectiveness lead different parts space examples searched 
influencing early classifier formation starting subsamples impact effectiveness making words required features 
seen standard deviations full column table classifiers trained set examples difference runs category required features provided initial examples 
second source variation fluctuations quality successive classifiers 
uncertainty sampling process inherently exploratory deficiencies classifier produced iteration leading selection compensating examples iterations 
results demonstrate effective text classifiers created obtaining large amounts unlabeled data labeling small fraction 
tested uncertainty sampling text categorization task equally applied classification task 
variations absolute levels effectiveness expected categories simply harder category definitions particularly noisy 
text retrieval obvious application tradeoff retrieving texts useful user vs texts system learn considered 
tradeoff issue filtering routing information dissemination applications cost judging nonrelevant examples amortized longer period operation 
uncertainty sampling benefit classification approaches natural language processing tasks 
number projects annotated annotating large corpora support training statistical methods tasks 
results suggest gathering huge unlabeled corpora uncertainty sampling annotate small subset task may cheaper equally effective 
domains text processing large data sets available benefit 
questions remain answered uncertainty sampling 
important practical ones teacher knows form final classifier 
estimates classifier effectiveness enable teacher track progress new methods needed produce estimate sample random 
effectiveness estimates aid selecting classifier classifiers formed iterations 
alternately methods stabilizing fluctuations iteration iteration pursued 
variety extensions improvements uncertainty sampling explored 
need determine relationship subsample size effectiveness larger subsamples require computation 
subsample size increased redundancy subsamples decreased 
efficiency improvements include accurate efficiently trained classifier sampling picking examples satisfying threshold uncertainty uncertain examples 
simultaneous training classifiers multiple classes interest 
currently formulated uncertainty sampling requires underlying training algorithm produce reasonable classifiers small training sets 
meant tinker feature selection parameter estimation avoid producing pathological behavior small samples 
currently exploring variations uncertainty sampling robust respect problems classifier training 
summary text cheap information form knowing classes text belongs expensive 
automatic classification text provide information low cost classifiers built expensive human effort trained texts manually classified 
demonstrated uncertainty sampling sharply reduce amount text manually labeled create effective classifier 
uncertainty sampling potential applications variety text processing tasks domains large amounts unclassified data available 
acknowledgments jason catlett fitzpatrick yoav freund trevor hastie robert schapire sebastian seung advice useful comments ken church making available text processing tools help 

hayes 
intelligent high volume text processing shallow domain specific techniques 
paul 
jacobs editor text intelligent systems current research text analysis information extraction retrieval pages 
lawrence erlbaum hillsdale nj 

fuhr lustig 
automatic indexing system air phys research application 
proc 
sigir pages 

cochran 
sampling techniques 
john wiley sons new york rd edition 

salton buckley 
improving retrieval performance relevance feedback 
journal american society information science 

gale church yarowsky 
method disambiguating word senses large corpus 
computers humanities 

ghosh 
brief history sequential analysis 
ghosh sen editors handbook sequential analysis chapter pages 
marcel dekker new york 

angluin 
queries concept learning 
machine learning 

white 
selecting concise training sets clean data 
ieee transactions neural networks march 

cohn atlas ladner 
improving generalization self directed learning 
appear machine learning 

mackay 
evidence framework applied classification networks 
neural computation 

seung opper sompolinsky 
query committee 
proceedings fifth annual acm workshop computational learning theory pages 

mitchell 
generalization search 
artificial intelligence 

freund seung shamir tishby 
information prediction query committee 
advances neural informations processing systems san mateo ca 
morgan kaufmann 

hwang choi oh marks ii 
query learning applied partially trained multilayer perceptrons 
ieee transactions neural networks january 

davis hwang 
attentional focus training boundary region data selection 
international joint conference neural networks pages baltimore md june 

hart 
condensed nearest neighbor rule 
ieee transactions information theory may 

utgoff 
improved training incremental learning 
sixth international workshop machine learning pages 

fuhr 
models retrieval probabilistic indexing 
information processing management 

lewis 
evaluation phrasal clustered representations text categorization task 
proc 
sigir pages 

maron 
automatic indexing experimental inquiry 
journal association computing machinery 

cooper 
inconsistencies probabilistic information retrieval 
proc 
sigir pages 

mccullagh nelder 
generalized linear models 
chapman hall london nd edition 

cooper 
probabilistic retrieval staged logistic regression 
proc 
sigir pages 

fuhr pfeifer 
combining model oriented description oriented approaches probabilistic indexing 
proc 
sigir pages 

robertson 
statistical problems application probabilistic models information retrieval 
report british library london 

gale church 
poor estimates context worse 
speech natural language workshop pages san mateo ca june 
darpa morgan kaufmann 

duda hart 
pattern classification scene analysis 
wiley interscience new york 

goldstein editor 
associated press manual 
addison wesley reading ma 

croft harper 
probabilistic models document retrieval relevance feedback 
journal documentation 

van rijsbergen 
information retrieval 
butterworths london second edition 

bookstein 
information retrieval sequential learning process 
journal american society information science september 

david lewis jason catlett 
heterogeneous uncertainty sampling supervised learning 
proceedings eleventh international conference machine learning 
appear 
bonds ireland budget hostages 
mean fi values text classifiers trained uncertainty solid line relevance dotted line random dashed line samples titles ap corpus 
means runs uncertainty relevance sampling runs random sampling 
results shown categories 
frequency category training set shown parentheses 
