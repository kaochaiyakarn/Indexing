scalable application layer multicast banerjee bobby bhattacharjee christopher describe new scalable application layer multicast protocol specifically designed low bandwidth data streaming applications large receiver sets 
scheme hierarchical clustering application layer multicast peers support number different data delivery trees specific desirable properties 
show group members maintain state constant number control overhead constant 
extensive simulations protocol narada application layer multicast protocol internet topologies 
results show groups size protocol lower link stress improved similar latencies similar failure recovery properties 
importantly able achieve results orders magnitude lower control traffic 
results wide area testbed experimented member groups distributed different sites 
experiments average group members established maintained paths incurred maximum packet loss rate members randomly joined left multicast group 
average control overhead experiments kbps groups size 
multicasting useful primitive scaling multi party applications potentially decouple size receiver set amount state kept single node network including data source 
deployment network layer multicast widely adopted commercial isps large parts internet incapable native multicast decade protocols developed 
application layer multicast protocols change network infrastructure implement multicast forwarding functionality exclusively hosts :10.1.1.22.6538:10.1.1.12.7544
application layer multicast protocols increasingly implement efficient commercial content distribution networks 
new application layer multicast protocol called smop scalable multicast overlay protocol specifically designed support applications large receiver sets 
applications include news sports ticker services see www com see www com real time stock quotes updates yahoo 
market tracker popular internet radio sites 
applications characterized large potentially tens thousands receiver sets relatively low bandwidth soft real time data streams withstand loss 
refer class large receiver set low bandwidth real time data applications data stream applications 
data stream applications unique challenge application layer multicast protocols large receiver sets usually increase control overhead relatively data amortizing control overhead difficult 
smop implement large data stream applications provably small constant control overhead produces low latency distribution trees 
possible im authors dept computer science university maryland college park md usa high smop concentrate exclusively low bandwidth data streams large receiver sets 
application layer multicast protocol evaluation basic idea application layer multicast shown 
native multicast data packets replicated routers inside network application layer multicast data packets replicated hosts 
logically hosts form overlay network goal application layer multicast construct maintain efficient overlay data transmission 
application layer multicast protocols send identical packets link efficient native multicast 
intuitive measures goodness stress stretch defined 
stress metric defined link counts number identical packets sent protocol underlying link network 
stretch metric defined member ratio pathlength source member versus length unicast shortest path 
consider application layer multicast protocol data source unicasts data receiver 
clearly multi unicast protocol minimizes stretch cost stress links near source number group members require control overhead single point 
protocol robust sense number group member failures affect members group 
application layer multicast protocols discus including smop purely oblivious underlying network topology 
general application layer multicast protocols evaluated dimensions ffl quality data delivery path 
quality tree measured topological metrics stress stretch node degrees 
ffl robustness overlay 
hosts potentially stable routers important application layer multicast protocols mitigate effect receiver failures 
robustness application layer multicast protocols measured extent data delivery different members fail time takes overlay restore delivery members 
comparison aspect application layer multicast protocols 
ffl control overhead 
efficient network resources control overhead members low 
important cost metric study scalability scheme large member groups 
existing approaches number different application layer multicast schemes proposed literature 
classified broad categories tree approaches mesh approaches 
tree approach members directly construct overlay tree topology data delivery additional network layer multicast application layer multicast fig 

network layer application layer multicast 
square nodes routers circular nodes hosts 
dotted lines represent peers overlay 
control links monitored maintained allow quick recovery member failures 
yoid almi examples tree approach :10.1.1.12.7544
name suggests mesh approach members mesh overlay topology multiple paths exist pairs members 
member participates mesh topology generates source specific tree members 
narada gossamer examples mesh approach :10.1.1.22.6538
approach produce overlay constant stretch stress bounds simultaneously arbitrary distribution members network 
possible better underlying topology known 
centralized topology aware tree building algorithm described stretch pair points bounded constant factor 
tree degree member unbounded 
slight modification algorithm simultaneously guarantee constant degree bound members log bound stretch 
smop trees goals smop develop efficient scalable distributed tree building protocol require underlying topology information 
specifically smop protocol reduces worst case state control overhead member log maintain constant degree bound group members approach log stretch bound possible topology aware centralized algorithm 
additionally show average member maintains state constant number members similarly incurs constant control overhead topology creation maintenance 
previous approaches smop exclusively tree mesh approach 
create control topology higher connectivity tree 
control topology equivalent mesh data delivery path implicitly defined way mesh structured additional route computations required 
smop hybrid tree mesh approaches 
centralized topology aware algorithm described possible bound pair wise toend stretch distributed algorithm smop 
simulations wide area experiments pairwise paths fact bounded log stretch bound 
analysis various bounds simulation performance evaluation smop 
simulations compare smop narada application layer multicast protocol 
narada proposed efficient application layer multicast protocol small group sizes 
extensions subsequently proposed tailor applicability high bandwidth media streaming applications groups studied simulations implementation 
lastly results wide area implementation quantify smop run time overheads convergence properties various group sizes 
roadmap rest structured follows section ii describe general approach explain different delivery trees built smop theoretical bounds smop protocol 
section iii operational details protocol 
performance evaluation methodology section iv detailed analysis smop simulations section widearea implementation section vi 
elaborate related section vii section viii 
ii 
solution overview smop protocol arranges set hosts hierarchy basic operation protocol create maintain hierarchy 
hierarchy implicitly defines multicast overlay data paths described section 
member hierarchy crucial scalability members bottom hierarchy maintain state constant number members 
members top hierarchy maintain soft state logn members 
logically member keeps detailed state members near hierarchy limited knowledge members group 
hierarchical structure important localizing effect member failures 
latency distance metric hosts 
constructing smop hierarchy members close respect distance metric mapped part hierarchy allows produce trees low stretch 
smop hierarchy described similar member hierarchy scalable multicast group re keying 
hierarchy layered multicast capable network constructed network multicast services scoped expanding ring searches 
builds necessary hierarchy unicast infrastructure provide multicast capable network 
fact follows hierarchy construction maintenance multicast services created application layer multicast protocol efficiently leveraged scheme scalably implement multicast group key distribution mechanisms 
rest section describe smop hierarchy defined invariants maintain describe establish scalable control data paths 
hierarchical arrangement group members smop hierarchy created assigning members different levels layers illustrated 
layers numbered sequentially lowest layer hierarchy layer zero denoted 
hosts layer parti fig 

control data delivery paths layer hierarchy 
hosts members clusters 
hosts members layers host leader cluster comprising hosts 
cluster leaders layer layer cluster leaders layer form layer layer form layer topological clusters layer joined layer hosts fig 

hierarchical hosts smop layers logical entities overlaid underlying physical network 
tioned set clusters 
cluster size gamma constant consists set hosts close 
cluster cluster leader 
protocol chooses graph theoretic center cluster leader set hosts cluster cluster leader minimum maximum distance hosts cluster 
choice cluster leader important guaranteeing new joining member quickly able find appropriate position hierarchy small number queries members 
hosts mapped layers scheme hosts part lowest layer clustering protocol partitions hosts set clusters 
cluster leaders clusters layer join layer shown example 
layer clusters abcd example assume centers respective clusters clusters chosen leaders 
form layer clustered create single cluster cfm layer center cluster leader 
belongs layer 
smop clusters layers created distributed section 
hold distribution hosts different layers ffl host belongs single cluster layer 
ffl host cluster layer occur cluster layers gamma fact cluster leader lower layers 
ffl host layer layer ffl cluster size bounded gamma 
leader graph theoretic center cluster 
ffl log layers highest layer single member 
denote cluster comprising hosts xy 
define term super cluster host assume host belongs layers gamma layer xyz 
cluster belongs highest layer layer gamma leader cluster 
super cluster defined cluster higher layer leader belongs 
follows super cluster defined host host belongs top layer super cluster layer immediately highest layer belongs 
example cluster cfm layer super cluster hosts smop host maintains state clusters belongs layer belongs super cluster 
control data paths host hierarchy define different overlay structures control messages data delivery paths 
neighbors control topology exchange periodic soft state refreshes generate high volumes traffic 
clearly useful structure higher connectivity control messages cause protocol converge quicker 
illustrate choices control data paths clusters size 
edges indicate peerings group members overlay topology 
set hosts arranged clique panel clusters layer hosts cluster leaders clusters 
form single cluster layer host leader cluster layer rest cl denote cluster layer member belongs 
defined belongs layer control topology smop protocol illustrated panel 
formally control topology peers member belongs layers exactly members corresponding clusters belongs layers cl cl 
example panel member belongs layer control path peers members cluster contrast member belongs layers control path peers members cluster cluster 
control topology member cluster exchanges soft state refreshes remaining members cluster 
procedure layers clusters cl cl cl cl gamma fh pg fig 

data forwarding operation host received data host lows cluster members quickly identify changes cluster membership turn enables faster restoration set desirable invariants described section ii violated changes 
delivery path multicast data distribution needs loop free duplicate packet detection suppression mechanisms need implemented 
choose tree data delivery path smop protocol 
specifically data source data delivery path source specific tree implicitly defined control topology 
member executes instance procedure decide set members needs forward data 
panels illustrate consequent source specific trees sources members respectively 
call basic data path 
summarize cluster layer control topology clique data topology star 
possible choose structures cluster ring control path balanced binary tree data path 
analysis cluster hierarchy gamma members 
control topology host belongs layer peers hosts exchange control messages 
general host belongs layer higher layer peers hosts layers control overhead member 
cluster leader highest layer cluster host peers total log neighbors 
worst case control overhead member 
follows amortized cost analysis control overhead average member constant 
number members occur layer higher layer bounded 
amortized control overhead average member log log asymptotically increasing control overhead average member log worst case 
holds analogously basic data path stress members 
log upper bound acceptable control topology high data rates hosts may unable unwilling forward data log hosts 
define enhanced data path local transformations basic data path highbandwidth streams video stress member data path reduced constant cost additional constant control overhead members 
outline enhancement mechanism appendix 
invariants properties described analysis hold long hierarchy maintained 
objective smop protocol scalably maintain host hierarchy new members join existing members depart 
specifically section maintains set invariants ffl layer hosts partitioned clusters size gamma 
ffl hosts belong cluster host belongs single cluster layer ffl cluster leaders centers respective clusters form immediate higher layer 
iii 
protocol description section describe smop protocol 
due space constraints packet formats concentrate high level protocol details 
assume existence special host members know priori 
nomenclature developed call host rendezvous point rp 
host intends join application layer multicast group contacts rp initiate join process 
ease exposition assume rp leader single cluster highest layer hierarchy 
interacts cluster members layer control path bypassed data path 
clearly possible rp part hierarchy leader highest layer cluster maintain connection rp complexity 
real application streaming media transfer rp distinguished host domain data source 
smop protocol main components initial cluster assignment new host joins periodic cluster maintenance refinement recovery leader failures 
discuss turn 
new host joins new host joins multicast group mapped cluster layer illustrate join procedure 
assume host wants join multicast group 
contacts rp join query panel 
rp responds hosts highest layer hierarchy 
joining host contacts members highest layer panel identify member closest 
example highest layer just member default closest member layer members 
host informs members cluster 
contacts members join query identify closest member panel iteratively uses procedure find cluster 
rp rp rp join join attach fig 

host joins multicast group 
important note host belongs layer center gamma cluster recursively approximation center members clusters part layered hierarchy 
querying layer succession top hierarchy layer results progressive refinement joining host find appropriate layer cluster join close joining member 
outline operation pseudocode procedure 
procedure 
distance joining host host highest layer possible host join cluster approximately away nearest cluster leader 
built refinement mechanisms described section 
join latency involves message overhead log query response pairs 
join latency depends delays incurred exchanges typically log round trip times 
protocol aggressively locate possible peers joining member overhead locating appropriate attachments joining member relatively large reduce delay member joining multicast group receipt data packet overlay allow joining members temporarily peer data path leader cluster current layer querying 
example querying hosts closest point attachment temporarily peers leader layer cluster data path 
allows joining host start receiving multicast data group single round trip latency join 
joining higher layers important invariant hierarchical arrangement hosts leader cluster center cluster 
members join leave clusters cluster leader may occasionally change 
leadership cluster layer changes existing leader removes layers higher attached 
new leader tries join current super cluster definition cluster layer existing leader joined new leader replaces existing leader cluster layer super cluster information stale currently invalid new leader invokes ba procedure cl query rp gamma find dist dist cl cl gamma query gamma decrement cl cl gamma endwhile join cluster cl fig 

basic join operation member join layer new member 
part layer gamma query gamma seeks membership information cl gamma member query rp gamma seeks membership information topmost layer hierarchy rp procedure terminate layer cluster maintenance refinement member cluster sends heartbeat message seconds cluster peers control topology 
message contains distance estimate member possible inaccurate estimate distance members immediately joins cluster 
cluster leader includes complete updated cluster membership heartbeat messages members 
allows existing members set appropriate peer relationships new cluster members control path 
cluster level cluster leader periodically sends immediate higher layer cluster membership super cluster members cluster cluster 
cluster member state sent unreliable messages kept cluster member soft state refreshed periodic heartbeat messages 
member declared longer part cluster independently members cluster receive message configurable number heartbeat message intervals 
cluster split merge cluster leader periodically checks size cluster appropriately splits merges cluster detects size bound violation 
cluster just exceeds cluster size upper bound gamma split creates clusters size 
single departure clusters subsequently require cluster merge operation meet size lower bound 
reason relax size upper bound leave lower bound unchanged 
new upper bound cluster split equal parts parts guaranteed avoiding immediate subsequent merge 
size cluster exceeds leader initiates cluster split operation 
set hosts pairwise distances cluster split operation partitions subsets meet size bounds maximum radius graph theoretic sense new set clusters minimized 
similar center problem known nphard additional size constraint approximation strategy leader splits current cluster equal sized clusters maximum radii clusters minimized 
chooses centers partitions leaders new clusters transfers leadership new leaders messages 
new clusters violate size upper bound split new leaders identical operations 
size cluster cl layer leader falls leader initiates cluster merge operation 
note belongs layer cluster cl 
chooses closest cluster peer cl leader layer cluster cl 
initiates merge operation cl sending message updates members cl merge information 
similarly updates members cl 
merge removes layer cluster cl 
refining cluster attachments phase period rapid membership changes group joining member may able locate closest cluster attach cluster 
may true cases higher layer members 
member layer say periodically probes members super cluster leaders layer clusters identify closer cluster layer closer cluster probing member leaves current cluster joins closer cluster 
host departure leader selection host leaves multicast group sends remove message clusters joined 

fails able send message cluster peers detects departure non receipt periodic heartbeat message leader cluster triggers new leader selection cluster 
remaining member cluster independently select new leader cluster depending estimates center members 
multiple leaders re single leader cluster exchange regular heartbeat messages appropriate flags center problem typically wait till timeout split cluster splitting exactly crosses efficiency reasons 
challenge choose ldr update transfer leader lost member independently chooses new leader competing leaders reconcile single leader selected fig 

restructuring cluster leader departs 
time candidate leaders detect multiplicity 
shown 
possible members inconsistent view cluster membership transient cycles develop data path 
cycles eliminated protocol reconciles cluster view members restores hierarchy invariants 
iv 
experimental methodology analyzed performance smop detailed simulations internet wide implementation 
simulation environment compare performance smop schemes multi unicast native ip multicast core tree protocol narada application layer multicast protocol 
internet experiments benchmark performance metrics direct unicast paths member hosts 
clearly native ip multicast trees unit stress link forwards single copy data packet 
unicast paths lowest latency consider unit stretch 
thy provide compare application layer multicast protocols 
data model experiments model scenario data stream source multicasting group 
chose single uniformly random data source generating constant bit rate data 
packet data sequence effectively samples data path overlay topology time instant entire data packet sequence captures evolution data path time 
performance metrics compare performance different schemes dimensions ffl quality data path measured different metrics tree degree distribution stress links routers stretch data paths group members 
particular apart averages metrics report variances distributions 
ffl recovery host failure hosts join leave multicast group underlying data delivery path adapts accordingly reflect changes 
transience particularly studies show may case native unicast latency compare performance schemes :10.1.1.38.1850
host failures path hosts may unavailable 
possible multiple paths exist single host cycles develop temporarily 
observe effects measured fraction hosts correctly receive data packets sent source 
recorded number duplicates host 
simulations multicast protocols number duplicates insignificant zero cases 
ffl control traffic overhead report mean variance distribution control bandwidth overheads routers hosts 
simulation experiments implemented packet level simulator different protocols 
network topologies generated transit stub graph model gt itm topology generator 
topologies simulations routers average node degree 
hosts attached set route chosen uniformly random stub domain nodes 
number hosts multicast group varied different experiments 
simulations modeled loss links data loss due congestion notion background traffic jitter 
data lost application layer multicast protocol fails provide path source receiver duplicates received path 
simulations study dynamics multicast protocol effects data distribution implementation performance affected factors additional link latencies due congestion drops due cross traffic congestion 
mentioned section narada protocol involves aggregate control overhead 
simulation setup unable simulate narada groups size larger completion time simulations order day single run experiment mhz pentium iii machine gb ram 
implementation narada implemented entire narada protocol description 
implement narada high bandwidth extensions described described narada mesh application layer multicast approach designed primarily small multicast groups 
narada initial set peer assignments create overlay topology done randomly 
initial data delivery path may poor quality time narada adds links discards bad links overlay 
narada aggregate control overhead mesh nature requires host periodically exchange updates refreshes hosts 
protocol defined number user defined parameters needed set 
include link add drop thresholds link add drop probe frequency periodic refresh rates mesh degree experimented wide range values parameters understand behavior narada observed interesting trade offs choosing parameters 
specifically ffl mesh degree bound hosts strictly enforced ensure connectivity 
additional mechanisms limit degree data path mesh 
ffl clear tradeoff choosing high versus low frequency periodic probes add drop links mesh 
high frequency allows members aggressively add drop bad overlay links respectively 
leads frequent changes data paths mesh lead temporary loss data path members 
effect different route changes state old route temporarily maintained mitigate effect route change 
observed effect experiments high periodic probe frequency especially parameter set higher route packet exchange frequency 
contrast low probe frequency leads stable paths implies mesh topology takes long time stabilize 
simulation results experiments simulated wide range topologies group sizes member join leave patterns protocol parameters 
smop set cluster size parameter experiments 
broadly findings summarized follows ffl smop trees data paths stretch comparable narada ffl stress links routers lower smop especially multicast group size increases ffl failure recovery schemes comparable 
ffl smop protocol demonstrates possible provide performance orders magnitude lower control overhead groups size 
results representative experiment captures different aspects comparing various protocols 
simulation representative scenario experiment different phases join phase leave phase 
join phase set members join multicast group uniformly random simulated time seconds 
hosts allowed stabilize appropriate overlay topology till simulation time seconds 
leave phase starts time seconds hosts leave multicast group short duration seconds 
repeated times second intervals 
remaining members continue part multicast group till simulation 
member departures modeled host failures damaging effect data paths 
experimented different numbers member departures single member members leaving second window 
sixteen departures group size short time window drastic scenario helps illustrate failure recovery modes different protocols better 
member departures smaller sizes cause correspondingly lower disruption data paths 
show results group size experiments reported smop performs increasingly better larger group sizes 
time secs hosts join join smop narada fig 

average link stress simulation average receiver path length hops time secs hosts join join smop narada ip multicast unicast fig 

average path length simulation experimented different periodic refresh rates narada 
higher refresh rate recovery host failures quicker cost higher control traffic overhead 
narada different values route update frequencies periods probing mesh members add drop links overlay 
results report results route update frequencies seconds labeled narada seconds labeled narada 
second update period corresponds ran second update period heartbeat period smop set seconds 
note run smaller heartbeat period smop significantly increasing control overhead control messages limited clusters traverse entire group 
varied mesh probe period narada observed data path instability effect discussed 
results set narada mesh probe period seconds 
data path quality figures show average link stress average path lengths different protocols data tree evolves member join phase 
note shows actual path lengths hosts stretch ratio average path length members protocol average path length members multi unicast protocol 
explained earlier join procedure smop aggressively finds points attachment members overlay topology smop tree converges quicker stable value seconds simulated time 
contrast narada protocols gradually improve mesh quality consequently data path longer duration 
average data path length converges stable value hops seconds simulated time 
corresponding stretch 
narada path lengths improve time due addition links mesh 
time stress tree gradually increases narada decides add drop overlay links purely stretch metric 
cluster data dissemination smop reduces average link stress general large groups smop converges trees lower average stress 
experiment smop tree lower stretch narada tree experiments narada tree slightly lower stretch value 
general comparing results multiple experiments different group sizes see section concluded data path lengths receivers similar protocols 
figures plot cumulative distribution stress path length metrics entire member set members time data paths converged stable operating point 
stress links multi unicast scheme significantly large tail links close source stress 
contrasted better stress distribution smop narada 
narada uses fewer number links topology smop comparably aggressive adding overlay links shorter lengths mesh topology 
due emphasis shorter path lengths stress distribution links smop 
links stress higher narada compared smop 
distribution path lengths protocols comparable 
multi unicast scheme comparison shows shortest path length distribution stress links ignored 
failure recovery control overheads investigate effect host failures results second part scenario starting simulated time seconds set members leave group second period 
repeat procedure times members leave simulated time seconds group reduced members 
members leave protocols heal data distribution tree continue send data partially connected topology 
show fraction members correctly receive data packets duration 
narada smop similar performance average protocols restore data path remaining receivers seconds correctly serve members 
ran experiment second refresh period narada 
lower link stress cumulative distribution link stress overlay stabilizes unicast truncated extends stress smop narada unicast fig 

stress distribution simulation number hosts overlay path length hops cumulative distribution data path lengths overlay stabilizes smop narada ip multicast unicast fig 

path length distribution simulation refresh period caused significant disruptions tree periods seconds tree receive data 
lastly note data distribution tree smop connected topology possible expect failure recovery results better structures alternate paths built atop smop 
show byte overheads control traffic access links hosts 
dot plot represents sum control traffic kbps sent received member group averaged second intervals 
second time slot dots plot remaining host multicast group corresponding control overheads narada smop 
curves plot average control overhead protocol 
expected groups size smop order magnitude lower average overhead simulation time seconds average control overhead smop kbps versus kbps narada 
time instant narada shown average control overhead kbps 
note smop control traffic includes protocol messages including messages cluster formation cluster splits merges layer promotions leader elections 
aggregate results set aggregate results group size varied 
purpose experiment understand scalability different application layer multicast protocols 
entire set members join seconds run simulation seconds allow topologies stabilize 
table compare stress network routers links overlay path lengths group members average control traffic overheads network routers 
metric mean standard deviation 
showed experiment narada smop tend converge trees similar path lengths 
stress metric network links routers consistently lower smop group size large greater 
interesting observe standard deviation stress changes increasing group size protocols 
standard deviation stress increased narada increas source fig 

internet experiment sites direct unicast latencies ing group sizes 
contrast standard deviation stress smop remains relatively constant clustering smop distributes data path evenly different links underlying links regardless group size 
control overhead numbers table different ones column table average control traffic network router opposed control traffic host 
control traffic gets aggregated inside network overhead routers significantly higher overhead host 
router overheads report values narada version route update frequency set seconds 
recall protocol narada performs relatively poorly members leave efficient specifically times overhead groups size narada version 
refresh messages smop sent second intervals 
vi 
internet wide implementation implemented complete smop protocol experimented implementation month period time secs hosts join followed periodic leaves sets leave smop narada fig 

fraction members received data packets duration member failures 
simulation control traffic bandwidth kbps time secs control traffic bandwidth access links join leave smop avg narada avg fig 

control bandwidth required host access links simulation group router stress link stress path length bandwidth overheads kbps size narada smop narada smop narada smop narada smop table data path quality control overheads varying multicast group sizes simulation member groups distributed different sites 
experimental topology shown labeled 
unfortunately experiments larger groups feasible testbed 
implementation results protocol overheads closely match simulation experiments believe simulations provide reasonable indication smop implementation behave larger group sizes 
implementation specifics conducted experiments data sources different sites 
representative set experiments data stream source located site 
indicate typical direct unicast latency milliseconds site sites 
estimated way latencies obtained sequence application layer udp probes 
data streams sent source host site hosts smop overlay topology 
implementation experimented different heartbeat rates results section set heartbeat message period seconds 
implementation estimate latency hosts various protocol operations including member joins leadership changes estimated latency hosts low overhead estimator sent sequence application layer udp probes 
controlled number probes adaptively observed variance latency estimates 
raw latency estimates distance metric simple binning scheme map raw latencies small set equivalence classes 
specifically latency estimates considered equivalent mapped equivalence class resulted faster convergence overlay topology 
specific latency ranges class ms ms ms ms ms ms ms greater ms compute stretch hosts internet experiments ratio latency source host overlay direct unicast latency host 
specifically experiments host receives data packet forwarded member overlay tree immediately sends back overlay hop acknowledgment back logs round initial transmission data packet receipt entire experiment done sum overlay round trip latencies data packet referring back logs host 
estimate way overlay latency half round trip latency 
obtain unicast latencies immediately experiment terminates 
clearly ideal long running experiments unfortunately concurrent computation unicast latencies perturbed experimental data resort computing unicast latencies experiment completed 
stress cumulative distribution stress members members members fig 

stress distribution testbed implementation scenarios internet experiment scenarios phases join phase rapid membership change phase 
join phase set member hosts randomly join group different sites 
hosts allowed stabilize appropriate overlay delivery tree 
period rapid membership change phase starts host members randomly join leave group 
average member lifetime group phase set seconds 
simulation studies member departures allow study worst case protocol behavior 
remaining set members organize stable data delivery tree 
results different groups size members 
data path quality show cumulative distribution stress metric group members overlay stabilizes join phase 
group sizes typical members unit stress members experiments 
stress remaining members vary 
members precisely cluster leaders different layers recall cluster size lower upper bounds experiments respectively 
stress members reduced high bandwidth data path enhancements described appendix 
larger groups number members higher stress experiments number clusters number cluster leaders 
expected increase logarithmic group size 
plot cumulative distribution stretch metric 
plotting stretch value single host group sites located member hosts site plot mean confidence intervals 
apart sites sites near unit stretch 
note source data streams experiments located site hosts sites low latency paths source host 
actual latencies overlay paths stretch sites distribution stretch members fig 

stretch distribution testbed overlay latency ms sites distribution latency members fig 

latency distribution testbed sites shown 
sites latencies ms ms ms respectively 
primary contribution latencies packet processing overlay forwarding hosts 
table ii mean maximum stretch different members direct unicast latency ms source sites different sizes 
mean stretch sites low cases see relatively large worst case stretches stretch worst case member group size 
failure recovery internet experiments observe effects group membership changes data delivery tree 
record successful overlay delivering data changes overlay topology 
rapid membership change phase experiment begins initial member set stabilized appropriate overlay topology 
phase average lifetime member group seconds minute period 
plot time fraction members successfully received different data packets 
total group membership changes happened duration 
plot cumulative distribution losses seen time secs distribution losses packets random membership change phase members average member lifetime secs fig 

fraction members received data packets group membership continuously changed testbed packets lost cumulative distribution losses members random membership change phase members average member lifetime secs fig 

cumulative distribution total number lost packets different members entire sequence packets rapid membership change phase testbed different members entire minute duration 
maximum number losses seen member members encounter losses 
rapid changes group membership largest continuous duration packet losses single host seconds typical members experienced maximum continuous data loss seconds true members 
failure recovery statistics data stream applications deployed internet 
note experiment suffered heavy losses data packets times received members respectively 
control overheads control traffic overheads kbps table ii different group sizes 
overheads include control packets sent received 
show average maximum control overhead member 
observed control traffic members lies kbps kbps different group sizes 
fact members require kbps control traffic topology management 
interestingly average control overheads distributions change significantly group stress stretch control overheads kbps size mean max 
mean max 
mean max 
table ii average maximum values different metrics different group sizes testbed group size varied 
worst case control overhead fairly low kbps 
vii 
related closely related projects explore implementing multicast application layer 
classified broad categories mesh narada gossamer tree protocols yoid almi :10.1.1.22.6538:10.1.1.12.7544
yoid defines distributed tree building protocol hosts almi uses centralized algorithm create minimum spanning tree rooted designated single source multicast data distribution 
project similar effort create application layer multicast overlay tree approach 
bayeux architecture application layer multicast hosts organized hierarchy defined tapestry overlay location routing system :10.1.1.140.3129:10.1.1.11.1973
level hierarchy defined set hosts share common suffix host ids 
technique proposed plaxton locating routing named objects network :10.1.1.19.7440
overcast protocol organizes set similar proxies called overcast nodes distribution tree rooted central source single source multicast 
distributed tree building protocol create source specific tree manner similar yoid 
rmx provides support reliable multicast data delivery hosts set proxies called reliable multicast proxies :10.1.1.20.299
application hosts configured affiliate nearest rmx 
architecture assumes existence overlay construction protocol proxies organize appropriate data delivery path 
tcp provide reliable communication pair peer proxies overlay 
viii 
new protocol multicast 
main contribution extremely low overhead hierarchical control structure different data distribution paths built 
results show possible build maintain application layer multicast trees little overhead 
focus low bandwidth data stream applications scheme generalizable different applications appropriately choosing data paths metrics construct overlays 
believe results significant step www net fig 

data path enhancements delegation 
constructing large wide area applications application layer multicast 
andersen balakrishnan frans kaashoek morris 
resilient overlay networks 
proceedings th acm symposium operating systems principles october 
ballardie francis crowcroft 
core trees cbt architecture scalable multicast routing 
proceedings acm sigcomm 
banerjee bhattacharjee 
scalable secure group communication ip 
proceedings internation conference network protocols november 
calvert zegura bhattacharjee 
model internetwork 
proceedings ieee infocom 
chawathe :10.1.1.22.6538
scattercast architecture internet broadcast distribution infrastructure service 
ph thesis university california berkeley december 
chawathe mccanne brewer :10.1.1.20.299
rmx reliable multicast heterogeneous networks 
proceedings infocom 

chu rao seshan zhang 
enabling conferencing applications internet overlay multicast architecture 
proceedings acm sigcomm august 

chu rao zhang 
case system multicast 
proceedings acm sigmetrics june 
deering cheriton 
multicast routing datagram internetworks extended lans 
acm transactions computer systems may 
francis 
yoid extending multicast internet architecture 
white www aciri org yoid 
gupta 
steiner points tree metrics don really help 
symposium discrete algorithms january 
jannotti gifford johnson kaashoek toole 
overcast reliable multicasting overlay network 
proceedings th symposium operating systems design implementation 
shi verma waldvogel :10.1.1.12.7544
almi application level multicast infrastructure 
proceedings rd usenix symposium internet technologies systems march 
plaxton rajaraman richa :10.1.1.19.7440
accessing nearby copies replicated objects distributed environment 
acm symposium parallel algorithms architectures 
savage anderson aggarwal becker cardwell collin hoffman snell vahdat voelker zahorjan :10.1.1.38.1850
detour case informed internet routing transport 
ieee micro 
zhao kubiatowicz joseph :10.1.1.140.3129
tapestry infrastructure fault tolerant wide area location routing 
tech 
report ucb csd university california berkeley april 
zhuang zhao joseph katz kubiatowicz :10.1.1.11.1973
bayeux architecture scalable fault tolerant wide area dat dissemination 
eleventh international workshop network operating systems support digital audio video nossdav 
appendix data path enhancements high bandwidth applications basic data path smop imposes data forwarding responsibility cluster leaders 
consequence members joined higher layers cluster leaders lower layers joined 
required forward higher volume data members joined lower layers 
data forwarding path suited high bandwidth applications video distribution 
define enhancement basic data path allowing cluster leaders delegate data forwarding responsibility cluster members deterministic manner 
explain data path delegation assuming data originating leader highest cluster topology 
delegation mechanism equally applicable data originating member minor modifications 
consider host belongs layers higher layer 
corresponding clusters layers cl cl 
basic data path described section ii receives data leader cluster cl topmost layer 
responsible forwarding data members clusters cl cl gamma clusters remaining layers 
enhanced data path forwards data members cl cluster lowest layer 
additionally delegates responsibility forwarding data members cl members cl gamma gamma 
cluster sizes bounded gamma member cl gamma gets delegated forwarding responsibility members cl 
cluster leader delegate forwarding responsibility member cluster 
member belongs multiple layers belongs single cluster layer leader respective clusters layer 
leader topmost cluster 
member delegated forwarding responsibilities new peers 
member receives data member leader topmost cluster cl delegated forwarding responsibility 
data path transformation illustrated example 
consider basic data path host source panel 
host leader clusters 
basic data path required forward data members clusters layer layer 
enhanced data path panel delegates members cluster forward data members cluster 
particular sets new data path peers ha ha ha members leaders clusters receive data cluster leader receive data members delegated described 
member enhanced data path forwards data members clusters cluster additionally may delegated forward data members 
total number data path peers member enhanced data path bounded constant depends cluster size 
member reduce data path peers organizing intra cluster data path structures star ring path cluster implies member data path peers 
course cost increased endto latencies 
