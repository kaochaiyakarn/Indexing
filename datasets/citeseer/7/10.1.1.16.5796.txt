automatic storage management parallel programs vincent lefebvre paul feautrier laboratoire prism universit de versailles st avenue des versailles france 
mail lefebvre paul prism fr article deals automatic parallelization static control programs 
parallelization process removal memory related dependences usually realized translating original program single assignment form 
total data expansion high memory cost 
technique partial data expansion leaves untouched performances parallelization process help algebra techniques polytope model 
keywords automatic parallelization memory management array dataflow analysis scheduling 
article deals automatic parallelization technique polytope model 
method applied provided source programs static control programs limited loops assignment array affine subscripts 
step extraction exact dependences array data flow analysis 
memory related dependences due reuse data deleted total data expansion 
transformed program single assignment property residual dependences constitute data flow 
program parallelized scheduling method automatically satisfies sequential constraints inherent data flow 
single assignment form translation high memory cost memory size order iteration space example matrix multiplication take memory 
clearly unacceptable ameliorated producing multiple assignment parallel code 
aim new technique partial data expansion 
show schedule associate parallel program minimal memory expansion 
section describe polytope method total data expansion 
section technique partial data expansion 
show technique replace total expansion parallelization process polytope method 
polytope method techniques algorithmes described section directly taken compiler developped university versailles feautrier team 
framework zz polyhedra polyhedron set vectors satisfies set linear inequalities 
bounded polyhedron called polytope 
zz module set integral points generated integer combination basis vectors 
zz polyhedron intersection zz module polyhedron 
basic problem zz polyhedra question emptiness 
linear integer programming question solved cut method integrated parametric integer programming pip tool 
straightforward application pip computes lexicographic maximum zz polyhedron 
static control programs static control programs built assignment statements loops 
data structures arrays arbitrary dimensions 
loop bounds array subscripts affine functions loop counters integral structure parameters 
operation may named hr xi statement iteration vector components values surrounding loop counters 
component counter loop iteration vector constrained loop bounds 
iteration domain statement set instances described conjunction inequalities surrounding loops 
loop counters integers iterations domains set integer vectors inside polytopes 
take running example program matrix vector program matrix vector real integer 
program statements 
operation hs execution statement counters loops surrounding 
symbolic constant structure parameter 
iteration domain fi ng 
sequential execution order lexicographic order noted expression indicates statement statement program text 
nrs number loops surrounding xj gamma fact operation hr xi executed operation hs yi written hr xi oe hs yi 
shown hr xi oe hs yi rs rs 
nrs hr xi oe hs yi hr xi oe hs yi ae nrs nrs rs rs running example hs oe hs hs oe hs semantic analysis dependences ensure parallel program determinate gives results sequential 
take account dependences exist operations source program 
operation associate sets set memory cells read set memory cells modified conditions distinguish kinds dependences oe flow dependence written ffi anti dependence written ffi output dependence written ffi ffi may precise associate dependence depth instance operations flow dependence depth written ffi means oe 
array data flow analysis static control programs known output dependences anti dependences artificial due reuse memory 
kinds dependences called false dependences 
real dependences define inherent semantic program subset flow dependences direct flow dependences 
direct flow dependence data flow definition operation operation memory cell provided write executions means value read produced remaining flow dependences artificial dependences called spurious flow dependences 
removal artificial dependences data restructuring called data expansion 
technique detailed section 
direct flow dependences computed data flow analysis 
determine memory cell read operation operation oe gives value execution operation called source read source max oe fv ffi wg computation source done pip parametric integer programming algorithm cf details 
result analysis quasi affine tree level predicates tests affine forms loop counters structure parameters 
leaves operation names 
symbol indicates array cell study modified 
sources gathered data flow graph dfg 
fig 
gives dfg matrix vector program 
memory cell referenced operation source operation hs ji gamma hs gamma hs ii hs ji hs ji hs ii hs ni dfg matrix vector program program transformations step delete false dependences spurious flow dependences total data expansion 
realized translating source program single assignment form 
residual dependences program direct flow dependences dfg 
second step parallelize single assignment form program scheduling 
aim change operation execution order program set operations data flow left untouched 
total data expansion total data expansion gives program single assignment property memory cell allocated data receive value produced operation execution program 
way associates memory cell operation 
find algorithm translating static control program single assignment form 
step complete renaming statement associates specific data structure store values produced operations instances totally expands data structures indexed iteration vector data flow replacing rhs corresponding source 
translates program matrix vector single assignment form obtains code program matrix vector real inss inss inss inss 
inss inss inss inss inss notice total data expansion created array inss elements array inss elements replace scalar 
parallelization scheduling computes time function gives partial execution order parallel program account sequential constraints data flow 
operation execution time source defines set linear constraints 
complexity reasons finding exact solution practicable 
limits oneself affine multi schedules 
case running example schedule function gamma gamma operation front gathers operations execution time 
operations front executed parallel 
note execution time fact logical time imagine corresponds execution parallel program unbounded number processors execute operation unit time 
set possible execution times enumerated lexicographic order 
parallel program enumerate possible date build parallel code apply affine transformations iteration space program 
done operations original program executed lexicographical order transformed iteration space 
final lexicographical order schedule function 
translates matrix vector program scheduled fortran gets code program matrix vector real inss inss inss eq 
inss 
eq 
inss inss ge 

le 
inss inss eq 
inss inss notice total data expansion induced split different statements parallel code 
reduced data expansion parallelized programs translating source program single assignment form high memory cost 
clear case running example scalar array gets arrays data size 
aim define method partial data expansion reduces memory expansion induced parallelization replaces single assignment form translation parallelization process 
constraint schedule deduced dfg remain valid presence output anti dependences 
intuitive presentation method 
optimized storage management parallelized programs intuitive approach precise conventions notations 
value produced operation memory cell stored 
set gathers operations direct data flow set operations executed read fu source vg utilization set 
execution time read parallel program 
subset operations execute read max consider memory cell execution parallel program single assignment form 
distinguish periods see fig 

period memory cell stays empty execution associated 
running example inss inss stays empty execution hs ji gamma 
period ii execution stores 
operations read 
time useful 
fhs ig 
read hs 
time read 
period iii memory cell read anymore execution parallel program 
useless 
useless stays inss program ii iii execution read execution order parallel program direct data flow read value produced time single assignment parallel program clear periods iii store values 
stores values output dependences appear parallel program 
problem define automatic method partial data expansion ensures parallel program obtained valid 
related papers devoted problem eliminating false dependences 
try eliminate dependences reduced memory cost 
find techniques come automatic parallelization community systolic community 
interesting notice techniques close methods 
papers automatic parallelization community deal array privatization 
privatization technique allows thread allocate variable private storage 
loop transformed parallel loop privatization replaces original array access local array 
prove privatization similar scalar array expansion 
privatization may require space expansion creates copy processor number processors cooperating execution parallel loop number iterations 
lam padua tu propose optimize array privatization help dfg 
adapts method partial expansion consists maintaining output dependences duplicate flow dependences 
solution proposed systolic community 
programs taken account single assignment form 
try create output dependences don invalidate data flow estimating lifetime variable 
darte build results padua introduced graph transformations eliminate false dependences 
give unified framework transformation prove problem determining minimal renaming np complete 
utility span value method partial data expansion notion utility span value 
clear utility span corresponds period ii see fig 
reside memory 
utility span value subsegment latency execution time front executed parallel program 
definition utility span time span production read parallel program 
estimate utility span running example 
gamma reside 
utility span store values changing data flow operations reintroduce output dependences operations 
atomic entity study memory cell previous methods value 
main advantage notion variable lifetime applied programs necessarily single assignment form 
subsection show conditions output dependence verify harmless parallel program 
output dependences called neutral dependences 
neutral dependences consider operations rule imposes 

case program single assignment form systematically output dependence 
optimizing storage means introduces memory reuse parallel program want operations 
clear possible iff basic rule verified spite output dependence 
output dependence valid parallel program subsegments spans separate 
output dependence called neutral output dependence 
definition output dependence neutral schedule function iff doesn change data flow parallel program built help 
precisely gives characteristics neutral output dependence ffi ffi parallel program see fig 
ffl executed 
ffl access conflict ffl utility spans separate read read direct data flow neutral output dependence utility span utility span time neutral output dependence ffi ffi parallel program extension output dependence considered neutral single operation constitutes 
utility spans separate 
operations share memory cell read computing 
means write occurs read ffl output dependence hs ji hs neutral hs executed hs ji utility span 
parallel program 
ffl hs executed utility span hs ji output dependence hs ji hs neutral parallel program 

ffl output dependence hs ji hs neutral hs 
utility spans separate operations stored memory cell sure hs read writing 

notice operations belong operations front output dependence ffi ffi non neutral parallel program 
data expansion ensure stored different memory cells 
fact memory requirement parallel program strongly linked parallelism degree size operations fronts schedule function 
seen running example utility span output dependence hs ji hs ji neutral operations belong front decide output dependence neutral parallel program precise estimation utility span value 
estimation help reconstruct data space program adjusting data size utility spans 
final purpose build program direct flow dependences neutral output dependences 
approach consisted maintain neutral output dependences original program parallel version 
method directly dependent original data space reduce data size programs single assignment form 
decided improve technique independent original data new method article output dependences existing program partial expansion necessarily original version 
utility span consider operation hr xi 
wants determine subsegment corresponds utility span operation 
lower bound subsegment directly 
problem compute upper bound 
recall execution time parallel program operation utilization set 
determining time uses techniques data flow analysis 
main difference lexicographic maximum computation sequential execution order oe execution order schedule function 
consider statements operation read parallel program operations instances belong 
set candidates hs brs built scanning dfg 
consider gives operations sources read instances takes account leaf hr concerning instance conjunction predicates lead leaf 
candidate hs brs leaf source read characteristics ffl corresponds existing instance ffl hs yi source hs yi hr verified 
hr rs candidate hr rs terms linear equalities inequalities hr rs zz polyhedra 
hs brs union candidates built source instances set hs brs disjunction zz polyhedra 
clear operation reads instances executed hs max brs statements may read data taken account 
real read maximum max ls source function 
determine just applies function leaf leaves symbol left untouched 
different utility spans gathered utility span graph gives operations utility span operation executing read 
symbol indicates useless output value 
fig 
gives matrix vector program 
operation utility span hs ii hs hs ji gamma hs hs ii gamma gamma hs ii 
matrix vector program partial data expansion step partial array scalar expansion process decides shape index function statement left hand side 
second step consists partial renaming process decides statements share data structure left hand side 
partial array expansion aims partial array expansion statement ffl want build structure specifically associated statement give shape number dimensions size dimension index function data left hand side restructured program 
ffl specifications build provides memory reuse output dependences operations instances output dependences neutral parallel program 
instance running example index function lhss may lhss lhss 
output dependence hs ji hs neutral 
lhss lhss 
output dependence operations neutral parallel program 
ffl elaboration independent original data structure lhs problem build 
recalls neutral output dependence kill value utility span 
respect rule instance take account maximum duration utility span parallel program 
operation hr xi duration obtained subtracting lower bound utility span upper bound 
writes parameter gamma considers gamma 
leaf multi linear expression term loop counters structure parameters 
maximum duration utility span instances maximum value iteration domain linear expression term structure parameters symbol 
notice considers 
maximum utility span durations matrix vector program fig 

implies statement utility span duration instance maximum utility span duration gamma maximum duration utility spans matrix vector program 
wants protect instance utility span build way verified greatest utility span instance 
chosen impose value killed algorithm builds data structure summarized ffl starts scalar 
ffl elaboration iterative number iterations equal nrr number loops surrounding 
iteration called partial expansion depth depth loop considered nrr gamma 
ffl partial expansion depth consists 
computing expansion degree depth gives number elements new dimension adds 

indexing new dimension mod index function built previous iterations counter loop outer surrounding mod modulo operator expansion degree computed previous step 
ffl process provides neutral output dependences nrr problem compute partial expansion depth avoids non neutral output dependences operations hr xi hr operation hr xi build set candidates gathering operations hr share memory cell hr xi ffl operations exist ffl sequential execution order hr xi oe hr ffl utility spans separate rr set candidates decomposed unions zz polyhedra 
lexicographic maximum max oe rr output dependences operations hr xi hr hr xi oe hr hr follows inequalities iteration vectors expands depth hr xi gamma sure non neutral output dependence depth appear hr xi 
verified instance degree maximum value hr xi max hr xi fig 
indicates expansion degree structures lhs set matrix vector program 
output dependences depth statements expansion degrees final data structure final lhs lhss lhss 
lhss lhss 
lhss lhss 
final results partial array expansion matrix vector program lhss fully expanded lhss array elements 
output dependences depth neutral parallel program expansion depth 
notice statement leaves untouched shape array lhs values read 
due fact stores final results program 
fact shape original data structure lhs elaboration starts specific case original datum 
partially expanded way instances belong front stored different memory cells partial expansion totally dependent parallelism degree 
partial renaming partial renaming process decide different statements share data structure 
consider statements partial expansion builds structures different shapes 
renaming process authorized share array rectangular hull 
clear statements share data iff sharing generate non neutral dependence left hand side statements 
fr gammat index function 
verify operation hr xi ht zi output dependence fr gammat fr gammat 
killed ht zi utility span 
killed hr xi utility span case partial expansion decompose candidates sets disjunctions zz polyhedra 
zz polyhedra empty transformation legal 
integral solutions share data structure 
finding minimal number renaming np complete problem see 
method consists building graph similar interference graph code process classical compiler optimize registers allocation 
graph vertex represents statement program 
edge vertices iff shown share data structure left hand side non neutral output dependence ffi ffi applies graph greedy coloring algorithm 
clear vertices colour data structure lhs 
matrix vector program edge interference graph 
means array lhs 
rectangular hull lhss lhss lhss array elements 
transformed program try reuse original variables soon possible especially variables store output values array maintained exactly corresponds storage requirement parallel program 
just data flow 
replaces rhs corresponding source ffl replace leaf form hr fr data structure lhs built partial array expansion partial renaming ffl replace void leaf original source matrix vector program gets program matrix vector real integer 
clear statement useless fusion removed parallel program 
builds parallel program schedule function statement find fortran program matrix vector real integer eq 

ge 

le 
aim reached method effectively reduce memory cost data expansion process static control programs see fig 

see example data size parallel program original data size 
simplification memory access cases reduce complexity parallel code 
primitive latency reduced removal 
partial data expansion may avoid split statements case fortran code single assignment form 
statements original data total expansion partial expansion inss inss inss data structures generated total partial data expansion second example cholesky factorization subsection results obtain cholesky factorization algorithm 
sequential version start program integer real real sqrt original data storage scalar array array respectively elements 
fig gives dfg program 
translates program single assignment form total data expansion memory cell referenced operation source operation hs ii hs ki gamma hs gamma hs ii hs ki hs ii hs ii gamma hs gamma hs ii hs ji hs ki gamma hs gamma hs ji hs ki hs ji hs ki hs ii hs ji gamma hs gamma hs ji hs ji hs ii dfg program gets program real real inss real inss real inss real inss real inss real inss integer inss inss inss inss inss inss sqrt inss inss inss inss inss inss inss inss inss inss inss inss see total data expansion created arrays data size original version 
schedule function computed compiler 
gamma gamma help dfg schedule function applied technique partial data expansion 
step estimation utility span value construction see fig 
operation utility span hs ii hs hs ki gamma hs hs ii gamma gamma hs ii hs ji gamma gamma hs ji hs hs ki gamma hs hs ji gamma gamma hs ji program computes maximum duration utility span value see fig 

process data restructuring partial data expansion gives structures lhs fig 
interference graph fig 

partial renaming finds hand hand share data structure lhs 
gets code program integer real real real real var statement utility span duration instance maximum utility span duration gamma gamma maximum duration utility span program statements expansion degrees final data structure final lhs lhss lhss 
lhss lhss 
lhss lhss 
gamma lhss lhss 
gamma lhss lhss 
gamma lhss lhss 
results partial array expansion program sqrt var var var var see expansion limited scalar gets dimension elements create array var gamma elements 
total expansion data space partial expansion 
partial data expansion generated expression 
fig 
gives comparaison results obtained applying different existing methods reduced memory share data share data interference graph program expansion 
column statements list 
columns give shape different data structures finds lhs statements source program single assignment form program version restructured method applied single assignment form version cf technique 
statements inss inss inss inss inss inss var inss inss inss inss inss inss comparaison methods compiler method replace translation single assignment form 
parallelization process applied order 
array data flow analysis 
scheduling real flow dependences 

partial memory expansion 
construction parallel program aim reached method effectively reduce memory cost data expansion process static control programs 
obtained important results 
performances strongly linked parallelism degree schedule function 
better parallelism higher memory cost conversely 
explained simply way 
mean degree parallelism simply mean size fronts 
output dependence front operations write different location 
may control memory expansion improve results adjusting schedule architecture 
recall considered parallel program executed target architecture unbounded number processors 
program general executed directly size fronts fact limited real parallelism provided target architecture 
suppose target architecture pipeline cray processor 
case operations front instances statement 
size front limited size vector registers instance 
adjust schedule way ffl parallel loop loop nest 
ffl front operations variant strip mining technique case matrix vector program imposes schedule function xi pi xi pi xi pi schedule find method partial expansion expansion degrees depth 
array lhs dimension elements case fortran code generated program matrix vector real integer eq 
min 
ge 

le 
min min min eq 
min min notice statement removed array elements needed lhs 
approach array privatization technique takes account fact real parallelism provided target architecture number iterations 

method reduce data space parallel programs directly provided single assignment form 
consider instance original version matrix vector program single assignment form 
schedule function reduces original data array elements fig 

means method reduce original data size program memory requirement necessary schedule function original data size 
property due fact data storing output values partial data expansion independent program 
maydan amarasinghe lam 
array data flow analysis array privatization 
principles programming languages 
darte robert vivien 
removal anti output dependences 
technical report rr laboratoire lip ecole normale sup erieure de lyon france feb 

environnement de programmation un acc el erateur de calcul parall ele 
th ese de universit de rennes france iv num ordre 
jalby 
strategy array management local memory 
proc 
th languages compilers parallel computing aug 
feautrier 
parametric integer programing 
rairo recherche op sept 
feautrier 
dataflow analysis array scalar 
int 
parallel programming february 
feautrier 
efficient solutions affine scheduling problem part ii multidimensional time 
int parallel programming december 
lefebvre feautrier 
storage management parallel programs 
ieee editor fith euromicro workshop parallel distributed processing pdp pages 
london 
united kingdom jan 
ieee computer society press 
li lee 
symbolic array dataflow analysis array privatization program parallelization 
supercomputing 

data compiling system uniform recurrence equations 
technical report university louis strasbourg france 

padua wolfe 
compiler optimizations supercomputers 
communications acm december 
tu padua 
array privatization shared distributed memory machines 
proc 
third workshop languages compilers distributed memory machines boulder colorado 
wilde 
memory reuse analysis polyhedral model 
robert editors euro par parallel processing vol pages 
springer verlag lncs august 
wolf lam 
data locality optimizing algorithm 
proc 
acm sigplan conf 
programming language design implementation june 

