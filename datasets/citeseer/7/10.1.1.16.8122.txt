alternate objective function markovian fields gatsby ucl ac uk yee teh cs toronto edu sam roweis roweis cs toronto edu gatsby computational neuroscience unit ucl queen square wc ar london uk department computer science university toronto king college rd toronto canada labelling prediction tasks trained model test performance quality single time marginal distributions labels joint distribution label sequences 
propose new cost function discriminative learning accurately re ects test time conditions 
ecient method compute gradient cost maximum entropy markov models conditional random fields extension models involving hidden states 
experimental results show new cost give signi cant improvements provides novel ective way dealing label bias problem 

input output markovian models input output modelling sequential data fundamental problem arising machine learning applications including tracking objects video streams labelling tagging sections documents identifying motifs functional regions amino acid nucleotide chains 
problem cast estimating state label sequence fs observations fo labels states may missing test time training testing time 
practice observations noncausal feature vectors designed algorithm implementer summarize complex underlying raw data stream indicating presence certain elements current time past 
goal model certain aspects joint distribution states labels 
focus supervised learning setting labels partially observed training time task test time predict labels sequence input observations 
consider online prediction ltering observations time delayed processing smoothing observations 
settings density model observations required performance liability 
popular generative models sequential data hidden markov model hmm see gure necessarily appropriate attempting solve dicult problem learning full distribution require training data order achieve performance 
furthermore causal directed graphical model attempts capture full joint distribution outputs labels large rich nonlocal features observation stream 
appropriate approach model conditional distribution label sequences features 
related architectures emerged capturing dependence allow rich nonlocal features observations see gure 
mccallum 
proposed maximum entropy markov model memm learns observation dependent transition previous label current label 
memms improvement generative hmm su er label bias problem crudely speaking unfairly favours labels successors 
attempt address limitation la erty 
proposed conditional random field crf undirected model normalizes probability label sequence globally locally 
considerable investigation architectures representations input output sequence models discussion appropriate objective functions training architectures relatively absent 
focus question model capture conditional label sequence distribution sjx 

graphical models hmm memm crf described text 
new objective function derive ecient algorithms training markovian models demonstrate improved performance tasks 
motivated results introduce extension crf model hidden random fields 

new objective function practical applications modelling full conditional distribution labels observations unnecessary 
example identifying functional regions nucleotide chains goal minimize total number errors labelling 
interested modelling correlations functional regions occur respect 
similarly visual tracking tasks predicting object location accurately timestep may important sequence predictions probable trajectory 
cases interested marginal conditional distributions jx full joint conditional distribution sjx 
introduce cost function penalizes model quality single time marginal distributions 
standard objective function markovian sequence label models log probability entire label sequence observation sequence log sjx log jx notation fs optimizing objective training set attempts maximize probability correctly labelling entire observation sequence 
approach test time estimate labelling state sequence viterbi path 
accurately re ects task hand cost appropriate 
real criterion test time maximize expected number correct labels equivalently minimize number input sequences jointly labelled perfectly 
simple decomposition reveals crucial assumption underlying training standard objective function appropriate criterion 
chain rule conditional probability log js see models trained learning predict label observations correct labels point 
spirit discriminative learning consider new objective average single time prediction costs better suited goal log jx implicitly integrates model labels time predicting label time cost function concerned quality marginals jx 
test time generate labelling gamma path taken choosing best state marginal 
labellings may poor zero probabilities joint model sjx fewer single time errors 
course model joint distribution sjx sequence labels correct implies marginal distributions jx correct 
limited data dicult accurately model sophisticated distributions long range dependencies 
performance depends accurate estimation marginal distributions correctly modelling entire joint distribution may better learning quantities directly learning complex model 
analogous distinction generative discriminative learning methods classi cation generative methods learn joint model inputs class labels appeal conditional test time discriminative models attempt capture joint model conditional directly 

maxent random field models discuss new objective function ect behaviour performance memm crf extension 
memms label bias memm de nes conditional distribution labels features sjx js simplicity assume xed initial state 
feature entire observation stream individual conditional distributions parameterized maximum entropy logistic models js exp prede ned potentials describe depends learned weights determine contribution dependence local normalization constant 
feature unnecessary write function memms su er called label bias problem bottou 
didactic example revisit rib rob problem discussed la erty 

training data consists observation sequence rib labelled sequence rob labelled occur equal probabilities 
consider case simply observed letter time note training data contain observation corresponding transition 
accurately estimate transition probability term appear cost function see equation 
test time test training data model needs evaluate requires estimate 
general think equation way redistributing probability mass objective memm learn redistribution situations represented training corpus 
contrast new objective training implicitly integrate uncertain previous labels time step 
example memm forced estimate impossible transition training corpus 
see note marginal distribution training label time jx js jx shows training dependent possible previous states seen training set 
memm trained toy example symbol labelling error including label 
error rate roughly model errors labelling states corresponding observation longer errors section provide experimental results demonstrating improvement trivial new objective function signi cantly outperforms standard training memm architecture synthetic robot navigation problem real document labelling task 
sequential random fields training alleviate problem memm architecture argued problem model standard objective function crf la erty provides principled solution changing basic model 
ect crf infers label time observations inferred belief labels time contrast memm infers label time infers labels time fact previous example crf error rate 
consider di erence cost functions crf extension incorporating hidden states see gure 
conditional random fields crf de nes conditional distribution labels sjx exp simplicity assume xed initial state potentials hand crafted parameters learned data 
crucially crf joint global normalization factor allows circumvent label bias problem see la erty 
di erence objectives considerably subtle crf case memm case crf su er label bias problem 
set feature weights di erently consider simple situation particular transition observed 
feature nonzero transition model give feature nitely large negative weight 
real world violates modelling assumptions feature may useful improving marginal distributions expense worse joint distribution 
demonstrate di erence didactic example labels observations generative process follows observations labels observations labels alternate 
process switches modes equal probability spends average amount time mode 
minimal possible labelling error rate 
example sequence looks label observation consider impoverished model feature 
binary feature equal arguments 
note self transitions occur training set 
feature large negative weight 
nd resulting error rate close large weight forces incorrect labellings vice versa 
contrast feature large positive weight accurately captures marginal distributions nd resulting error rate roughly optimal 
example constructed demonstrate potential tradeo modelling joint vs marginal distributions 
section give experimental results toy hmm document labelling task 
hidden random fields conditional models crf memm designed able long range nonlocal features observations 
models assume markovian structure labels 
extending crf consider hidden random field model shown gure 
crf hidden markov model hidden states 
allows past information label history held hidden states features predict labels 
note hidden models longer share convexity properties maximum entropy models della pietra 
susceptible local optima problems training 
conditional distribution labels jx expf consider slightly powerful model adding links fs graphical model permits markov dependence directly labels 
crfs joint normalization factor 
note distribution integrates hidden states 
important distinction crf hidden states training 
hidden states integrated compute cost function 
model signi cantly unconstrained cost functions explicitly depend joint distribution hidden states 
give example section demonstrates obtain accurate marginal distributions additional freedom 

parameter estimation section derive algorithm eciently optimize crf 
algorithms memm derived similarly 
involved computing sucient statistics corresponding parameter 
various optimizers conjugate gradient iterative scaling type algorithms perform actual update case memm full step feasible 
sequence labels 
definition crf equation di erentiating respect obtain hf js hf jx hfi expectation distribution second term calculated belief propagation compute joint probabilities jx denotes value variable take 
expectations taken explicitly 
rst term similarly calculated quantities js passes network compute forward backward sums js js 

com results minka suggest conjugate gradient ecient iterative scaling 
time obs 
left blind robot problem shaded locations indicate proximity sensor detects wall 
right predicted distributions location robot models 
top observations direction movement wall 
square area blobs correspond probability location circle denotes true location 
puted forward recursion js intermediate values recursion 
similarly backward recursion js recursions ecient belief propagation updates compute constant 

results tested new cost function synthetic examples real document labelling task demonstrating superior performance various cases practicality associated training algorithms memms crfs 
consider training vanilla hmms di erent cost functions 
discussed lafferty discriminatively trained hmm equivalent crf trained table features 
similarly hmm discriminatively trained equivalent crf trained table features 
cases results 
possible consider training hmm naive extension log standard hmm cost function log 
naive extension performs particularly poorly due drastic tendency focus modelling entire observation sequence results appropriate comparison 
blind robot example blind robot example section highlights di erence optimizing ordinary cost function proposed cost memm 
robot moving grid world 
initially robot location uniformly distributed room 
step robot moves compass directions proximity sensor tells touching wall see gure 
assume noise system 
unknown starting location sequence movements robot wall sensor readings wish predict location robot 
compared models memm trained old cost function memm trained new cost function crf trained old cost function 
performance crf trained essentially identical performance shown 
memms trained full steps crf trained conjugate gradient 
full table parameterizations models 
right panel gure shows typical observation sequence world corresponding distribution locations labels predicted models 
expected able locate robot quickly performed worst 
notice time able determine possible locations time possible able predict true location move north wall move north finds wall 
features learned models 
location small square grid corresponds previous location robot blob squares corresponds location 
memms size blob describes probability transiting location crf related chance seeing locations 
quite accurately 
able probability mass previous location concentrated locations inconsistent observations time 
gure show parameters learned models observations right column left 
see gure 
parameters observations follow similar patterns 
expected learned sensible features current observations consistent previous location learn inconsistent 
due problem learn appropriate features inconsistent case see gure 
hand learns features predict location robot previous location inconsistent 
location normally consistent current observations closest robot previous location correct 
faq dataset tested viability new cost function frequently asked questions faq dataset introduced mccallum 

data consists les belonging usenet newsgroup faqs 
le consists header followed question answer pairs ends tail section 
task label lines le header question answer tail set features number begins question word indented see mccallum full list 
trained hmm optimizing full joint distribution labels observations features memms crfs objective 
zero mean gaussian prior variance imposed weights model weight decay 
faq performed leave evaluation training models le reserved testing 
table top shows prediction errors observation sequence test case 
prediction error calculated percentage lines labelled wrongly tests 
hmm performed worst expected 
training memm objective lowered error signi cant improvement due capturing marginal distributions labels accurately clear label bias problem role 
crf training gives substantial dramatic improvement possible crf performed worse memm due tting observe low training errors crfs 
shows histogram probabilities model assigned correct labels test set gure shows predicted distribution labels line faqs 
see drastic improvement 
table bottom show errors predicting label unusual case observations correct sequence labels time models trained performed better trained supports argument section objective minimizes error predicting correct label observations correct labels point see shows aspects joint distribution accurately captured 
memm memm crf crf 
histogram plots probability mass various models assigned correct label test set 
note assigns probabilities close signi cantly frequently memm crf test error training error hmm memm memm crf crf hmm memm memm crf crf table 
errors faq dataset various models test conditions 
top observations test time 
bottom unusual test condition observations correct labels point 
conditional random fields illustrative example consider training crf data generated simple state hmm labels 
states transition probability assume circular boundary conditions 
state emits observation probability emits equal probability 
full table parameterization models perform equally 
consider interesting variant corrupt feature 
addition feature fi jg feature takes positive value transition 
set transition value randomly state observation features similarly corrupted 
tested crf model performs variety corrupted features data sequences di erent 
shows objec percent error percent error 
vs error rates crf models corrupted table features see text 
region diagonal line outperforms tive outperforms informally results vary depending parameters amount data rarely nd performing worse results faq dataset show signi cant improvement hidden random fields nal demonstration di erent objectives trained simple hidden random field model synthetic example requiring labeler remember state information past 
task labels observations observation label label 
obviously model problems learning aspect 
addition resume observation repeats label observation 
bit memory required label correctly 
observation interrupt labels cycle inverting value label observation equal bit memory needed model joint label distribution interrupt due randomness possible distinguish interrupt sequence error rate 
standard crf clearly correctly label memory 
obtain minimal error rate required bit information resume mode 
bits information required capture joint distribution 
single bit hidden states forced chose modelling resume interrupt mode 
trained model model 
model appropriately uses memory chooses model interrupt mode error rate 
show hidden states observations label performance errors models runs 
document position ticks file boundaries 
faq line labelling results 
block horizontal strips shows log probability assigned labels answer question head tail various models 
horizontal position moves data tick marks top bottom denote le boundaries 
intensity strip corresponds log probability assigned label features line faq le 
blocks top bottom true labels 
hid state errors xxx xxxx xx observations errors hid state hid state errors xx observations errors hid state experimental results needed understand fruitful extension 
discussion limited data important match model training test conditions 
spirit discriminative training introduced objective function closely resembles test condition common interest 
simple idea cases test performance depends quality model marginal conditional distribution joint conditional distribution 
demonstrated signi cant improvements synthetic real world problems class markovian models 
drastic improvement memm 
nd new cost function provides novel interesting way deal tricky label bias problem 
crf solves problem principled manner clear crf preferable memm 
example faq experiment shows memm signi cantly outperforming models 
cases memm architecture captures important aspects underlying process provided way retaining bene ts architecture solving problem 
new objective function train memm crf models just eciently standard joint likelihood 
acknowledgments peter dayan helpful discussions andrew mccallum providing faq data reviewers useful comments 
bottou 

une approche th de apprentissage applications la reconnaissance de la parole 
della pietra della pietra la erty 

inducing features random elds 
ieee transactions pattern analysis machine intelligence 
la erty mccallum pereira 

conditional random elds probabilistic models segmenting labeling sequence data 
proc 
th international conf 
machine learning pp 

morgan kaufmann san francisco ca 
mccallum freitag pereira 

maximum entropy markov models information extraction segmentation 
proc 
th international conf 
machine learning pp 

morgan kaufmann san francisco ca 
minka 

algorithms maximum likelihood logistic regression 
techreport 
carnegie 
