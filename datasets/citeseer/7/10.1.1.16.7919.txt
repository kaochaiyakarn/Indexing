annals mathematics artificial intelligence 
kluwer academic publishers 
printed netherlands 
parameter learning object oriented bayesian networks department mathematical sciences norwegian university science technology trondheim norway mail math ntnu department computer science aalborg university fredrik vej dk aalborg st denmark mail hl cs auc dk describes method parameter learning object oriented bayesian networks oobns 
propose methodology learning parameters oobns prove maintaining object orientation imposed prior model increase learning speed object oriented domains 
propose method efficiently estimate probability parameters domains strictly object oriented 
attack type uncertainty special case model uncertainty typical object oriented domains 
keywords bayesian networks object orientation learning ams subject classification 
bayesian networks bns established powerful tool areas artificial intelligence including planning vision decision support systems robotics 
main obstacles create maintain large domain models 
remedy problem object oriented versions bn framework proposed literature 
object oriented bns oobns defined papers offer easy way creating bns problem assessing maintaining probability estimates remain conventional learning algorithms exploit domain object oriented learning 
propose learning method applied directly oobn specification 
proven learning method superior conventional learning methods object oriented domains method efficiently estimate probability parameters domains strictly object oriented proposed 
organized follows rest section create starting point analysis introducing oobns required notation assumptions 
section outline proposed learning method section propose framework learning domains approximately object oriented 
special case model uncertainty typical object oriented domains handled section conclude section 
parameter learning oobns 
object oriented bayesian networks small easy read pieces complex model applied technique constructing large bayesian networks 
instance introduces concept sub networks viewed edited separately different pieces network adds levels integration fragments analogy boolean circuits concerned combination fragments conditional noisy min 
frameworks representations called object oriented bayesian networks 
framework section foundation learning oobns 
oobns defined described way example adapted 
example illustrate proposed learning mechanism show works 
limit description framework parts relevant learning oobns details 
font describe classes instantiations classes described font font employed referring variables 
old mcdonald farm milk cows meat cows 
milk cow primarily produces milk meat cow primarily produces meat 
wants model stock oobn classes 
constructs generic cow shown 
knows cow eats mother influences milk meat produces 
wants mother food input nodes input node node outside class 
wants milk meat output nodes nodes class usable outside instantiations class 
dashed ellipses represent input nodes shaded ellipses represent output nodes see 
input output nodes form interface instantiation context instantiation exists 
nodes instantiation input output nodes termed normal nodes 
class may instantiated times different nodes having influence different instantiations input nodes number states input nodes known time specification cows different mothers 

generic cow class defined 
arrows links normal bns 
dashed ellipses input nodes shaded ellipses output nodes 
parameter learning oobns consults expert tells want get specifications milk cow meat cow agrees 
new cow specifications shown subclasses generic cow class generic cow top left class specifications 
class subclass class contains set nodes ensures instantiation oobn instantiation instantiation milk cow instantiation generic cow 
node subclass inherits conditional probability tables cpts corresponding node superclass parent sets differ modeler explicitly overwrites cpt 
sub superclass relation transitive antisymmetric avoid cycles required subclass class superclass class 
furthermore multiple inheritance allowed structure class hierarchy tree collection disjoint trees called forest 
trees class hierarchy forest arranged unique node superclass root nodes tree superclass parent 
tree called class tree 
continues constructing stock class representing live stock 
boxes instantiations cow instantiation class meat cow 

experts specification milk cow 
experts specification meat cow 
note input sets larger input set generic cow 

stock instantiations milk cow class instantiations meat cow class 
note input nodes referencing nodes 
parameter learning oobns indicated cow meat cow inside cow instantiation 
note input nodes output nodes visible part instantiation available encapsulating class stock 
double arrows links leaf link root link input node mother cow node daisy 
means node mother inside instantiation cow node daisy node 
subclasses class tree may larger set nodes superclass input set subclass larger input set superclass instantiation instantiation extra input nodes referencing node 
ensure nodes contain potential notion default potential introduced default potential probability distribution states input node input node referencing node 
default potential link specified reason subclassing 
mother nodes node default potential nodes associated cpt 
worth noting structure tree forest cycles links possible 
trees consist unique root leaf nodes layers structures case 
inference performed translating oobn multiply bayesian network see details translation constructing underlying bn 
underlying bn instantiation constructed algorithm assuming define legal oobn 
empty graph 

add node input nodes output nodes normal nodes 
add node input node output node normal node instantiations contained prefix name instantiation node name instantiation name node name 
instantiations contained instantiations 

add link normal link repeat instantiations 

tree merge nodes node 
node parents children normal links nodes tree family 
note root tree parents nodes node 
input node node normal node equipped default potential 
describes underlying bn instantiation stock class algorithm 
avoid confusion normal links model terms parent child referring links 
parameter learning oobns 
underlying bn instantiation stock class 

notation assumptions description important assumptions distance measure evaluate learning methods propose 
standard terminology learning community follow oobn terminology necessary 
domain interest modeled stochastic vector xm dimension wherex distributed unknown distribution function 
unknown vector parameters determining distribution 
vector sampled regularly observations stored database database size 
assume cases database identically independently distributed distribution assumed belong known parametric distribution family estimation problems boils estimating parameters distribution 
stated bn learning task assumption corresponds assuming structure bn known see description learning object oriented domains structure unknown priori 
denote estimate 
domain variable assumed discrete meaning xi takes values finite universe xi andx xm xm configuration probability distribution estimated samples denoted simply fn 
unknown true distribution function called framework discrete bayesian networks family distribution functions characterized fact takes form product conditional probability tables xi xi pa xi pa xi denotes xi parents bayesian network 
event pa xi takes particular configuration enumeration possible configurations denoted pa xi furthermore ij denote probability xi pa xi parameter learning oobns assume ij avoid trivial deterministic cases learning 
denote dimension parameter space meaning smallest possible number free parameters encode correctly 
sum sizes cpts encode distribution binary variable parameter furthermore calculated directly dimension bayesian network complete graph utilizes compact representation 
focuses maximum likelihood estimates parameters 
generate maximum likelihood estimates 
em algorithm particularly easy implement graphical models problematic issues regarding speed convergence convergence local sub optimal maximum likelihood function 
problems overcome different acceleration measures see second problem typically managed series random restarts iteration process convergence em algorithm :10.1.1.44.3555
described consider parameter priors learning algorithms 
reason want build theory asymptotic properties estimators find sample size maximum likelihood estimators constraining results bayesian estimators converge maximum likelihood estimators priors strictly positive parameter space see 
note find bayesian maximum posteriori estimators em framework 
note convergence estimators large sample distribution quite rapid examples focus asymptotic results constrain applicability results 
simplicity assume data missing completely random see 
informally means observability variable independent value variable missing observed 
note variables missing called hidden obey assumption 
extension missing random mar informally means assumption relaxed allow pattern depend values observed variables immediate 
extension left clarity exposition 
quality learned distribution measured respect kullback leibler divergence kl divergence estimated true distrib assumption simplicity exposition needed results valid 
learning speed domain deterministic nodes measured way learning speed domain deterministic nodes considered fixed 
including deterministic nodes gives tedious notation jeopardize underlying mathematics 
parameter learning oobns dn calculated fn log log 
expectation taken respect estimated distribution fn 
expectation calculated expanding sum equation see chapter 
arguments particular measurement calculating quality approximation see 
fact kl divergence bound maximum error assessed probability particular event proposition sup fn similar results maximal error estimated conditional distribution derived 
results kl divergence distance measure choice bayesian network learning see 
chosen empirical kl divergence fn fn finite probability simplifies asymptotic expansion 
results similar obtained fn bounded approximations divergence measure 
oobn learning meaningful initially assume domain fact object oriented cpts instantiation class identical corresponding cpts instantiation class 
call oo assumption 
section investigate happens assumption violated 

oobn learning described section class hierarchy definition forest containing trees classes subclasses parents tree 
class hierarchy data instantiations classes hierarchy want learn data 
way done described 
typical way learn data learn underlying bn take advantage object oriented specification probably violate oo assumption 
assumption instantiations class identical 
take advantage oobn specification learning method propose learns class specification instantiation 
means observation class instantiation treated virtual case class 
cpts represented class cpt different superclass exists 
example consider definition generic cow kl divergence distance measure mathematical sense hold general 
term everyday meaning phrase 
parameter learning oobns subclass milk cow shown 
cpts music state mind defined milk cow variables defined generic cow 
furthermore parent set metabolism different class specifications cpt metabolism specified generic cow milk cow 
cpts food mother milk meat need specified generic cow class 
possible food mother milk meat milk cow differ generic cow specification case cpts defined specifications 
scope cpt specification associated node xt defined follows 
ct class node xt defined time meaning xt defined superclass ct exists 
scope cpt xt substructure class tree ct root 
subclass ct member scope cpt overwritten subclass 
see example class tree 
set classes included scope 
subclasses members evaluated inclusion rule done recursively class tree 
subclasses ct included scope cpt specifications way 
easy see scopes cpts associated xt partition class tree substructures trees 
intersection scopes empty union scopes substructure variable cpt defined class tree rooted ct learning performed done cpts specified 
means learning cpt data instantiation class performed root substructure defined scope cpt 
example consider generic cow milk cow classes figures generic cow superclass milk cow subclass 
assume observed data instantiation milk cow class want update cpts milk metabolism 
scope 
class tree shows scope definitions cpt node xt classes defined marked filled circle 
cpt xt defined twice class tree non overlapping scope definitions partition class tree parts part cpt valid second valid node xt defined 
parameter learning oobns milk specification class tree equal tree assume milk overwritten subclass 
learning cpt milk performed root class tree generic cow class 
cpt metabolism overwritten milk cow specification learning metabolism performed milk cow class 
note learning performed instantiations update cpts underlying bn learning 
re compilation oobn cpts class specifications distributed instantiations described point underlying bn updated 
consequences subclass generic cow say meat cow updated learning performed milk cow 
class specification meat cow shown 
class cpts food mother milk meat generic cow assume overwritten meat cow 
data instantiation milk cow update milk change instantiations meat cow 
desirable cpts generic cow overwritten subclasses milk production milk cow different generic cow meat production different meat cows 
addition maintaining oo assumption proposed learning algorithm important effect 
cpts shared instantiation number parameters learn reduced 
desirable shown 

case missing data database complete missing values learning theory particularly easy 
recapitulate independent realizations distribution distribution function data complete find maximum likelihood approximation fn closed form equations applying iterative em algorithm 
test learning algorithms calculate kl divergence fn estimated distribution fn true distribution denote convergence distribution meaning infinite sequence write xn distribution functions fn xn converge distribution function continuity point wheref definition 
large sample theory easy see details verify unbiased estimator fn cram rao lower bound variance unbiased estimator defined chapter 
result fact complete data parameter learning oobns fn converges particular distribution fn size parameter space grows large easily interpretable relationship expected value kl divergence lim fn may formulated fn large surprisingly having fewer parameters increase expected learning speed measured empirical kl divergence 
object oriented learning reduces number parameters learn 
learning done class specification get fewer parameters estimate constraining existing parameters underlying bn identical 
define number parameters object oriented learning number free parameters object oriented model 
sum free parameters cpts class specifications instantiated oobn 
remember complete oobn instantiation class stock class 
number parameters instantiations counted forced identical parameters class definitions 
see equation valid object oriented learning property need class instantiations observing case instantiations class effect learning parameters object oriented model observing hypothetical cases class 
follows trivially asymptotic theory statistics outlined 
note suppress technicalities discussion notice smoothness strict positivity distribution functions quantities involved finite probability 
presentation particular chapter book 
current setting known maximum likelihood estimates asymptotically gaussian distributed mean variance 
fisher information matrix matrix defined iij log asymptotic variance maximum likelihood estimator defined fisher information certain regularity conditions fulfilled setting 
random variables distributed density respectively 
furthermore information denoted parameter learning oobns respectively 
information available sample called theorem log log 
maximum likelihood estimators asymptotically efficient section empirical kl divergence function parameter variances see information instantiations equals sum information imaginary cases class long missing data database 
fact equation valid object oriented learning follows 
test object oriented learning method consider example farm described section 
assume measures variables domain regularly stores database 
wishes estimate parameters domain uses conventional object oriented learning methods 
results displayed asymptotic values expected kl divergence methods function equation indicated 
conventional learning algorithm parameters estimate object oriented domain 
equation kl divergence conventional learning algorithm approximately times large object oriented learning large 
kl divergence learned networks true distribution function size training set network complete data 
results oo learning drawn solid line conventional learning results dotted 
large sample approximations equation drawn thick lines 
parameter learning oobns 
missing data learning missing data relation equation longer holds 
assume data missing completely random denote probability variable xi data vector missing 
small network sparsely connected argued conventional learning lim fn 
expected value fn approximately proportional number parameters asymptotically 
guarantee object oriented learning faster conventional learning data missing 
see problem consider simple example domain 
underlying bn oobn shown instantiations class framed 
follow include unknown probability parameters model 
probability parameters drawn filled circles empty circles domain variables 
assume data record domain observed 
common child 
missing data sample 
case get trouble want learn probability pieces information learning probability parameter correlated observed value influences 
parameter estimates dependent additivity information equation longer valid 
information matrix positive semi definite corollary follows information gain positive 

simple example instantiations class doing object oriented learning parameters constrained equal 
indicated dotted lines 
parameter learning oobns fact maximum likelihood estimators asymptotically efficient large parameter estimate denotes parameter variance obtained object oriented learning denotes variance conventional learning estimates 
object oriented learning worse conventional learning expectation measured empirical kl divergence 
grows large object oriented learning may better conventional 
test object oriented learning missing data assume time measure available information day 
day independently chooses measure variable probability skip day probability 
dataset missing completely random 
kl divergences achieves learning object oriented conventionally depicted different values object oriented learning conventional degrees missing data sample sizes 
results andq obtained random 
kl divergence learned networks true distribution function size training set 
object oriented learning offers kl divergence expectation small conventional learning data sizes degrees missing data 
parameter learning oobns restart em algorithm times graphs obtained run em algorithm 
data missing guarantee increased learning speed obtained case complete data 
method intuitively appealing loose information object oriented approach 
empirical results illustrated indicate object oriented learning strictly better large amount missing data 

violating oo assumption results figures show oobn approach works better conventional approach example network 
hardly surprise know instantiations identical object oriented learning simply takes account part learning bias 
interesting happens instantiations class slightly different 
may reasonable assume structure instantiations identical parameters may somewhat different 
papers parameter learning authors typically state learning probability parameters bn known structure hidden variables important problem structure easier elicit experts numbers similar line argument employed easy expert say instantiations identical structure 
cpts equal may differences small subtle due variables model differ individual instantiations difficult quantify 
case instance cows exactly alike due genetic differences 
propose relaxed oo parameter learning differences instantiations class penalized totally rejected 
note applying relaxed oo learning resulting network object oriented 
case object orientation merely help network design necessarily anticipated property network routine 
framework propose calculation bayesian model averaging bma see 
bma set competing statistical models mk 
model mk prior degree belief mk attached 
posterior degree belief database calculated standard bayesian way mk mk mk instantiations different domain expert oo assumption 
proper modeling imply subclasses fulfill oo assumption 
expect situation occur domain object oriented theory outlined instantiations different see discussion leading 
parameter learning oobns mk mk mk 
model parameters model mk integration performed parameter space property interest posterior distribution bma mk mk 
application event variable takes particular value configuration parents xi pa xi 
weuse ij denote parameter estimate ij xi pa xi object oriented learning ij case conventional learning 
bma estimate ij ij ij mo ij mc 
mo mc posterior belief object oriented conventional model respectively 
shown logarithmic scoring rule averaging models provide better average predictive ability single model mj conditioned set models considered 
typical problem implementing bma computational complexity 
set models grow large 
fortunately problematic case limit set models object oriented object oriented 
secondly integration equation may difficult perform 
cumbersome 
approximation may crudely approximate likelihood distribution degenerated maximum likelihood estimate 
equation posterior belief approximated mk mk mk mk 
note equation estimate likelihood data especially larger models 
conventional model contains parameters objectoriented know likelihood model large likelihood object oriented model 
tendency choosing complex model leads known problem fitting due higher flexibility complex model 
approximation log likelihood model penalized size 
approximation known bayesian information criteria bic log mk log mk log parameter learning oobns number free parameters model mk andn size data set 
shown asymptotic size error approximation increase bic earlier applied learning bayesian networks see 
equation modify likelihood calculations equation get mk mk mk posterior belief model mk 
problem bma defining model priors 
quite lot available generating model priors framework bayesian networks knowledge elicitation non informed methods :10.1.1.156.9918
experience domain experts find difficult assess priors competing models hand 
model initially developed object oriented believe oo assumption justified tends hold large belief object oriented model 
hand sufficiently detailed level truly object oriented real world domains rare confronted fact domain expert tends trouble belief quantified 
domain experts typically claim ignorant give uniform priors neutral choice little prior information relative plausibility models considered 
employ bma framework version domain object oriented knowledge cows hormones produce meat 
hormone treated cows meat cow milk cow 
effect hormone treatment model food quality issue treated cows produce significantly meat 
true probability distributions meat node changed cows 
rest domain unchanged 
milk cows identical anymore probability tables match meat node goes meat cows 
know treatment models stock object oriented way wants learn probability tables domain data 
feels oo assumption justified holds prior belief object oriented model 
results shown 
domain entirely object oriented similarity object oriented domain learning task example difficult 
number parameters conventional bn learning twice object oriented model 
equation give high posterior belief object oriented model observed data carrying strong evidence oo assumption node meat differs different instantiations 
larger model space describing intermediate cases specifically considering models type nodes xk different instantiations domain object oriented 
case learning method discovered violation oo assumption faster 
correct model parameter learning oobns 
empirical kl divergence versus size database displayed conventional learning object oriented learning bayesian model averaging 
object oriented learning better smaller data sizes data size gets larger conventional learner better oo assumption violated 
bma follows object oriented model small data sizes evidence oo assumption gets outspoken conventional model selected weight 
redundant parameters strongly penalized complexity 
employed enlarged model space calculations real world situations objects large fitting parameters models full enumeration extended model space computationally prohibitive 
hypothesis test check data indicate object oriented model 
test pearson asymptotic test employed 
problems regarding setting significance level interpretation large significantly large test statistics choose bma setup 
examine effect bma setup closely performed simple example class containing binary variable class instantiations instantiation defines difference instantiations 
note oo assumption violated long 
calculated degree belief model object oriented equation 
results shown different data sizes 
calculation scheme able detect oo assumption violated grows 
smaller values equation willing assume domain object oriented small data sizes preference object oriented model vanishes grows larger 
effect bma framework estimators instantiation borrows strength instantiations rejecting domain object oriented estimates parameter learning oobns 
posterior belief preposition domain object oriented calculated equation different values different data sizes robust 
data observed data clearly indicate oo assumption violated borrowing take place extent 
kind result obtained building hierarchical bayesian model 
setting model ij different instantiations random variables determined underlying distribution ij posterior variance ij determines equal instantiations classes see case study 

type uncertainty far assumed domain expert able unambiguously classify instantiation domain specific class 
may realistic real world applications 
able classify instantiation example called type uncertainty expert uncertain type class terminology instantiation 
example assume unable determine cowl cow meat cow 
able determine class cowl learn available data 
section devoted showing treat type uncertainty framework 
candidate classes instantiation oobn set si 
expert encodes prior beliefs class instantiation distribution si 
assume probability distributions different instantiations independent priori 
recall notation denote variable instantiation zi set nodes defined inside instantiation including input nodes instantiation nodes outside 
denote set instantiations oobn 
denote class instantiation denote classification instantiations domain 
classification induced classification instantiation weuse 
furthermore pa parameter learning oobns denote set parents class xi zi weuse ijk probability xi pa xi 
avoid problems overfitting assume instantiations allocated classes oobn model 
case penalization model complexity equation introduced 
denote variables contained underlying bn 
means fundamental factorization probability distribution encoded bn oobn get pa 
zi note choice classifications different oobn 
possible oobns structurally identical local models instantiations expert uncertain 
correct oobn unknown hold prior distribution possible candidates 
priori different oobn models conditionally independent classification 
model modeled object oriented version bayesian multinet bayesian multinets introduced 
goal employ learning algorithm learns parameters domain specifying class precisely prior distribution si 
done standard em algorithm 
denote estimate tth iteration em algorithm denote collection estimates time 
furthermore ijk collection probability parameter estimates classes tth iteration 
algorithm proceeds iterating update equations 
generate new estimates 
sum denominator taken possible classifications sum numerator restricted classifications classified class 
note easy calculate probability just product subset elements update estimates instantiation containing xi xi zi 
ij expected counts event xi pa xi 
distribution possible classification fit type uncertainty calculations oobn framework assume si nodes observed defined zi 
technically necessary implementation simplified 
classes meet requirement candidate classes removed 
parameter learning oobns instantiations conditional distributions missing values replaced expected values step em algorithm 
similarly ij ij expected counts event pa xi assumption 
estimates ijk class updated ijk xi zi ij xi zi 
ij equation natural extension update equation classification instantiations known 
case values fixed update rules identical 
iterating equations lead local maximum likelihood observed data 
spin algorithm equation generates posterior distribution possible classes instantiation 
task known classification rich body literature bn community see 
complexity performing parameter update steps exponential number instantiations expert classify certainty 
number unclassified instantiations large efficient implement generalized em algorithm likelihood data strictly increased iteration necessarily maximized 
interested classification parameters known type uncertainty task particularly easy computationally 
need 
empirical kl divergence versus size database displayed object oriented learning correct classification cow meat cow wrong classification cow milk cow results outlined method 
classification fairly random smaller data sizes data size gets larger correct class probability converging 
results correct classifier thin line hidden underneath results type uncertainty thick line 
parameter learning oobns perform calculations equation parameters known 
secondly input output sets classes si contain missing values required likelihoods classify calculated locally classes larger model instantiation embedded interest type uncertainty calculations 
example consider stock 
assume uncertain class cowl able correctly classify cows 
prior distribution class cowl classes equally data reported missing values 
result applying proposed learning algorithms equations displayed results consistently wrong classifier cowl assumed milk cow consistently correct classifier cowl assumed meat cow 
proposed method capable detecting correct class approximately cases larger data sizes results proposed method just consistently correct classifier 

proposed learning method learn parameters oobns 
proven learning method superior conventional learning objectoriented domains database complete shown long oo assumption holds proposed learning algorithm inferior conventional learning 
proposed bayesian model averaging estimate probability parameters domains strictly object oriented showed example methodology offers reasonable results 
method enables handle situations object oriented model completely specified described 
colleagues decision support systems group aalborg university interesting discussions 
particular thomas nielsen provided constructive comments earlier version 
abe warmuth takeuchi polynomial learnability probabilistic concepts respect kullback leibler divergence proceedings th annual workshop computational learning theory colt morgan kaufmann san mateo ca pp 

nielsen structural learning object oriented domains proceedings th international florida artificial intelligence research society conference flairs aaai press pp 


object oriented bayesian networks 
framework topdown specification large bayesian networks repetitive structures technical report cit department computer science aalborg university 
parameter learning oobns 
top construction repetitive structures representation bayesian networks proceedings th international florida artificial intelligence research society conference eds 
aaai press pp 

riva learning conditional probabilities longitudinal data working notes ijcai workshop building probabilistic networks numbers come 
aaai press montreal pp 

binder koller russell kanazawa adaptive probabilistic networks hidden variables machine learning 
cheng greiner comparing bayesian network classifiers proceedings th conference uncertainty artificial intelligence uai eds 
laskey prade morgan kaufmann pp 

cover thomas elements information theory wiley new york 
cowell dawid lauritzen spiegelhalter probabilistic networks expert systems statistics engineering information sciences springer new york 
cram mathematical methods statistics princeton university press princeton nj 
dasgupta sample complexity learning fixed structure bayesian networks machine learning 
dempster laird rubin maximum likelihood incomplete data em algorithm journal royal statistical society series 
friedman geiger goldszmidt bayesian network classifiers machine learning 
friedman yakhini sample complexity learning bayesian networks proceedings th annual conference uncertainty artificial intelligence uai morgan kaufmann san francisco ca pp 

geiger heckerman knowledge representation inference similarity networks bayesian multinets artificial intelligence 
green em algorithm penalized likelihood estimation journal royal statistical society 
heckerman tutorial learning bayesian networks learning graphical models ed 
jordan mit press cambridge ma 
heckerman geiger chickering learning bayesian networks combination knowledge statistical data machine learning :10.1.1.156.9918
available microsoft research technical report msr tr 
basu distinguishing missing random missing completely random american statistician 
madigan raftery bayesian model averaging tutorial discussion statistical science 
corrected version www stat washington 
edu www research online pdf 
jensen bayesian networks taylor francis london uk 
koller pfeffer object oriented bayesian networks proceedings th conference uncertainty artificial intelligence eds 
geiger shenoy morgan kaufmann san francisco pp 

lam bacchus learning bayesian belief networks approach mdl principle computational intelligence 
efficient parameter learning comparison large sample behaviour department computer science aalborg university 
available www cs auc dk research dss publications 
laskey mahoney network fragments representing knowledge constructing probabilistic models proceedings th conference uncertainty artificial intelligence eds 
geiger shenoy morgan kaufmann publishers san francisco ca pp 

parameter learning oobns lauritzen em algorithm graphical association models missing data computational statistics data analysis 
lehmann elements large sample theory springer texts statistics springer new york 
little rubin statistical analysis missing data wiley new york 
madigan raftery eliciting prior information enhance predictive performance bayesian graphical models communication statistics theory methods 
madigan raftery model selection accounting model uncertainty models occam window journal american statistical association 
ortiz kaelbling accelerating em empirical study proceedings th annual conference uncertainty artificial intelligence uai morgan kaufmann san francisco ca pp 

pearl probabilistic reasoning intelligent systems networks plausible inference morgan kaufmann san mateo ca 
pfeffer probabilistic reasoning complex systems ph thesis stanford university 
pradhan provan middleton henrion knowledge engineering large belief networks proceedings th conference uncertainty artificial intelligence morgan kaufmann san francisco ca pp 

schwarz estimating dimension model annals statistics 
spiegelhalter lauritzen sequential updating conditional probabilities directed graphical structures networks 
srinivas probabilistic approach hierarchical model diagnosis proceedings th conference uncertainty artificial intelligence morgan kaufmann san francisco ca pp 

thiesson accelerating quantification bayesian networks incomplete data proceedings st international conference knowledge discovery data mining aaai press menlo park ca pp 

van approximating bayesian belief networks arc removal ieee transactions pattern analysis machine intelligence 
whittaker graphical models applied multivariate statistics wiley chichester 
xiang jensen inference multiply bayesian networks extended shafer shenoy lazy propagation proceedings th conference uncertainty artificial intelligence uai eds 
laskey prade morgan kaufmann pp 

xiang poole multiply bayesian networks junction forests large knowledge systems computational intelligence 
