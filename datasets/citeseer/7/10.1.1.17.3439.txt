algorithmic approaches training support vector machines survey colin campbell department engineering mathematics bristol university bristol bs tr united kingdom campbell bris ac uk support vector machines svms increasingly popular tool machine learning tasks involving classification regression novelty detection 
exhibit generalisation performance reallife datasets approach motivated theoretically 
training involves optimisation convex cost function relatively free parameters adjust architecture experimentation 
tutorial survey methods training svms including model selection strategies determining free parameters new techniques active selection training examples 


support vector machines svms successfully applied number applications ranging particle identification face identification text categorisation engine knock detection bioinformatics database marketing 
approach systematic motivated statistical learning theory bayesian arguments 
training task involves optimisation convex cost function false local minima complicate learning process 
approach benefits example model constructed explicit dependence informative patterns data support vectors interpretation straightforward 
tutorial introduce subject emphasis issue training svms 
perspective statistical learning theory motivation considering binary classifier svms comes theoretical bounds generalisation error 
quote relevant theorem note important features 
firstly upper bound generalization error depend dimension space 
secondly error bound minimised maximising margin fl minimal distance hyperplane separating classes closest datapoints hyperplane 
consider binary classification task datapoints having corresponding labels sigma decision function sign delta margin perpendicular distance separating hyperplane hyperplane closest points support vectors 
examples support vectors opposite sign 
margin band region hyperplanes sides separating hyperplane 
dataset separable data correctly classified delta 
clearly relation invariant positive rescaling argument inside sign function define canonical hyperplane delta closest points side delta gamma closest 
separating hyperplane delta normal vector clearly jjwjj margin projection gamma vector see 
delta delta gamma means margin fl jjwjj maximise margin task minimise jjwjj subject constraints delta learning task reduced minimisation primal lagrangian delta gamma ff delta gamma ff lagrangian multipliers ff 
derivatives respect back primal gives wolfe dual lagrangian ff ff gamma ff ff delta maximised respect ff subject constraint ff ff dual lagrangian notice datapoints appear inside inner product 
get potentially better representation data map datapoints alternative space generally called feature space pre hilbert inner product space replacement delta oe delta oe functional form mapping oe need known implicitly defined choice kernel oe delta oe inner product hilbert space 
suitable choice kernel data separable feature space despite non separable original input space 
data parity spirals problem non separable hyperplane input space separated feature space defined rbf kernels giving rbf type network gammax jj oe choices kernel possible delta tanh fix delta defining polynomial feedforward neural network classifiers 
class mathematical objects kernels general includes example scores produced dynamic alignment algorithms 
binary classification choice kernel learning task involves maximisation lagrangian ff ff gamma ff ff subject constraints 
optimal values ff decision function sign ff bias feature dual formulation primal constraints gamma max gamma ff min ff confidence classification directly related magnitude maximal margin hyperplane feature space points lie closest hyperplane ff points support vectors 
points ff 
means representation hypothesis solely points closest hyperplane informative patterns data 
framework elaborated ways example multiclass classification 
number schemes multiclass classification outlined 
simplest schemes directed acyclic graph left learning task reduced binary classification node 
soft margins allowing training errors 
svm fit noise training data leading poor generalisation 
effect outliers noise reduced introducing soft margin remove effect outliers 
currently schemes possible 
error norm learning task box constraint ff second error norm learning task addition small positive constant leading diagonal kernel matrix control trade training error generalisation ability chosen means validation set 
novelty detection 
real world problems task classify detect novel abnormal instances 
novelty detection performed modelling support distribution finding function data lies approach find sphere minimal radius centre contains data novel test points lie outside boundary sphere :10.1.1.117.3731
training process effect outliers reduced slack variables allow datapoints outside sphere 
task minimise volume sphere number datapoints outside controls tradeoff terms 
chosen kernel learning task reduces maximisation ff ff gamma ff ff respect ff subject ff ff test point novel gamma ff ff ff equality training example ff bound ff approach developed scholkopf give different qp formulation estimating support provide experimental evidence favour approach highlighting abnormal digits usps handwritten character dataset 
regression 
approaches regression possible classification essential algorithmic task minimise convex function give sparse solution 
example ffl sv regression minimise jjwjj increase flatness penalise constraints gamma delta oe gamma ffl delta oe gamma ffl allowing deviation ffl eventual targets function delta oe modelling data 
learning task reduced maximisation dual gammaffl ff ff ff gamma ff gamma ff gamma ff gamma ff gamma ff delta subject constraint ff ff instance 

algorithmic approaches training svms tasks involve optimization quadratic lagrangian techniques quadratic programming applicable including quasi newton conjugate gradient primal dual interior point methods 
certain qp packages readily applicable minos loqo 
methods train svm rapidly disadvantage kernel matrix stored memory 
small datasets practical qp routines best choice larger datasets alternative techniques 
split categories techniques kernel components evaluated discarded learning working set methods evolving subset data 
category obvious approach sequentially update ff approach kernel ka algorithm 
binary classification soft margin bias simple gradient ascent procedure ff initially ff subsequently sequentially updated ff fi fi fi ff gamma ff fi heaviside step function 
optimal learning rate readily evaluated sufficient condition convergence jk 
decision function method easy implement give quick impression performance svms classification tasks 
equivalent hildreth method optimisation theory generalised case soft margins inclusion bias 
fast qp routines especially small datasets 
chunking decomposition 
sequentially updating ff alternative update ff parallel subset chunk data stage 
qp routine optimise lagrangian initial arbitrary subset data 
support vectors retained datapoints ff discarded 
new working set data derived support vectors additional datapoints maximally violate storage constraints 
chunking process iterated margin maximised 
course procedure may fail dataset large hypothesis modelling data sparse ff non zero say 
case decomposition methods provide better approach algorithms fixed size subset data ff remainder kept fixed 
decomposition sequential minimal optimisation smo 
limiting case decomposition sequential optimisation smo algorithm platt ff optimised iteration 
smallest set parameters optimised iteration plainly constraint ff hold 
remarkably parameters optimised rest kept fixed possible derive analytical solution executed numerical operations 
method consists heuristic step finding best pair parameters optimise analytical expression ensure lagrangian increases monotonically 
hard margin case easy derive maximisation respect additive corrections ff ff ff ff 
soft margin care taken avoid violation constraints leading bounds corrections 
smo algorithm refined improve speed generalised cover tasks classification regression estimating densities 
due decomposition learning task speed probably method choice training svms 
model selection 
apart choice kernel indeterminate choice kernel parameter oe 
kernel parameter cross validation sufficient data available 
model selection strategies give reasonable estimate kernel parameter theoretical arguments additional validation data 
attempt theorem stating test error bound reduced margin fl increased fl radius smallest ball containing training data general qp task 
optimum possible show fl ff ff values ff optimum 
rbf kernels data lie surface hypersphere oe delta oe 
estimate oe sequentially training svms dataset successively larger values oe evaluating ff case choosing value oe minimised 
method give reasonable estimate data spread evenly surface hypersphere poor data lie flat ellipsoid example radius influenced largest deviations 
refined estimates take account distribution data 
approach simply rescale data kernel space compensate uneven distributions 
rescaling achieved eigenvalues eigenvectors covariance matrix 
complex strategy lines proposed scholkopf leads algorithm performed practice small number datasets 
economical way training data leave cross validation procedure 
procedure single elements data set removed svm trained remaining gamma elements tested removed datapoint 
reasonable assumption set support vectors change possible derive tight bounds generalisation error 
examples model selection rules chapelle vapnik rule proposed haussler 
studies limited number datasets model selection strategies appear 
comparative study different techniques application wider range real life datasets needs undertaken establish fully practical approaches 
active selection 
far considered learning strategies data acquired passively 
svms construct hypothesis subset data containing informative patterns candidates active selective sampling techniques seek patterns 
suppose data initially unlabeled heuristic algorithm predominantly request labels patterns support vectors labels required patterns corresponding non support vectors eventual hypothesis 
active selection particularly important practical situations process labelling data expensive dataset large unlabelled 
process active selection information gained example depends position available information label unavailable information querying 
follow heuristic strategy maximise gain step 
firstly note querying point margin band guarantees gain label point 
gain querying point outside band current hypothesis predicts label incorrectly 
best points query points closest current hyperplane 
intuitively sense maximally ambiguous respect current hypothesis best candidates ensuring information received maximised 
strategy start requesting labels small initial set data successively querying labels points closest current hyperplane 
active selection works best hypothesis modelling data sparse comparatively support vectors find right 
left multi class classification problem reduced series binary classification tasks 
right generalisation error axis percentage versus number patterns axis random selection top curve active selection bottom curve 
active selection outperforms random selection especially hypothesis sparse dataset patterns support vectors 

learning task involves optimisation quadratic function svms provide unique solution 
approach general applied wide range machine learning tasks classification regression novelty detection generate possible learning machine architectures rbf networks feedforward neural networks choice kernel 
kernel substitution inner product powerful idea separate concept margins define types learning machines distinct svms 
svms perform practice consequently expected develop important tool applications machine learning 
burges 
tutorial support vector machines pattern recognition 
data mining knowledge discovery 
campbell cristianini smola 
instance selection support vector machines submitted machine learning 
chapelle vapnik 
model selection support vector machines appear advances neural information processing systems ed 
solla leen 
muller mit press 
cortes vapnik 
support vector networks 
machine learning 
cristianini campbell shawe taylor 
dynamically adapting kernels support vector machines advances neural information processing systems ed 
kearns solla cohn mit press 
cristianini shawe taylor 
support vector machines kernel learning methods cambridge university press appear january 
platt cristianini shawe taylor 
large margin dags multiclass classification advances neural information processing systems ed 
solla leen 
muller mit press 

cristianini campbell 
kernel algorithm fast simple learning procedure support vector machines 
th intl 
conf 
machine learning morgan kaufman publishers 
cf www com isabelle projects svm html haussler 
convolution kernels discrete structures uc santa cruz technical report ucs crl 
herbrich graepel campbell 
bayesian learning reproducing kernel hilbert spaces submitted machine learning 
haussler 
probabilistic kernel regression models proceedings conference ai statistics 
keerthi bhattacharyya murthy 
improvements platt smo algorithm svm classifier design 
tech report dept csa india 
luenberger 
linear nonlinear programming 
addison wesley 
alpaydin 
support vector machines multiclass classification proceedings international workshop artifical neural networks idiap technical report 
mika ratsch weston scholkopf 
muller 
fisher discriminant analysis kernels 
proceedings ieee neural networks signal processing workshop pages 
osuna girosi 
reducing run time complexity support vector machines scholkopf burges smola ed advances kernel methods support vector learning mit press cambridge ma 
platt 
fast training svms sequential minimal optimisation 
scholkopf burges smola ed advances kernel methods support vector learning mit press cambridge ma 
shawe taylor 
confidence estimates classification accuracy new examples 
ben david ed eurocolt lecture notes artificial intelligence 
scholkopf platt shawe taylor smola williamson estimating support high dimensional distribution 
microsoft research technical report msr tr 
scholkopf shawe taylor smola williamson 
support vector error bounds ninth international conference artificial neural networks iee conference publications 
smola scholkopf 
tutorial support vector regression 
neurocolt tr 
tax duin 
data domain description support vectors proceedings esann ed 
verleysen facto press brussels 
vapnik chapelle 
bounds error expectation svms submitted neural computation vapnik 
nature statistical learning theory springer 
vapnik 
statistical learning theory 
wiley 
watkins 
dynamic alignment kernels technical report ul royal holloway csd tr 
weston watkins 
multi class support vector machines proceedings esann ed 
verleysen facto press brussels 
