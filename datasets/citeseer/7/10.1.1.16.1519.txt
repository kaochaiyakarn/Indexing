journal arti cial intelligence research submitted published issues stacked generalization kai ming ting deakin edu au school computing mathematics deakin university australia 
ian witten cs waikato ac nz department computer science university waikato new zealand 
stacked generalization general method high level model combine lowerlevel models achieve greater predictive accuracy 
address crucial issues considered black art classi cation tasks stacked generalization wolpert type generalizer suitable derive higher level model kind attributes input 
nd best results obtained higher level model combines con dence just predictions lower level ones 
demonstrate ectiveness stacked generalization combining di erent types learning algorithms classi cation tasks 
compare performance stacked generalization majority vote published results arcing bagging 

stacked generalization way combining multiple models learned classi cation task wolpert regression breiman unsupervised learning smyth wolpert 
typically di erent learning algorithms learn di erent models task hand common form stacking rst step collect output model new set data 
instance original training set data set represents model prediction instance class true classi cation 
step care taken ensure models formed batch training data include instance question just way ordinary cross validation 
new data treated data learning problem second step learning algorithm employed solve problem 
wolpert terminology original data models constructed rst step referred level data level models respectively set cross validated data second stage learning algorithm referred level data level generalizer 
show stacked generalization classi cation tasks addressing crucial issues wolpert originally described black art resolved 
issues type attributes form level data ii type level generalizer order get improved accuracy stacked generalization method 
breiman demonstrated success stacked generalization setting ordinary regression 
level models regression trees di erent sizes linear ai access foundation morgan kaufmann publishers 
rights reserved 
ting witten regressions di erent number variables 
selecting single model works best judged example cross validation breiman di erent level regressors output values member training set form level data 
squares linear regression constraint regression coe cients non negative level generalizer 
non negativity constraint turned crucial guarantee predictive accuracy better achieved selecting single best predictor 
show stacked generalization reliably classi cation tasks 
output class probabilities generated level models form level data 
level generalizer version squares linear regression adapted classi cation tasks 
nd class probabilities crucial successful application stacked generalization classi cation tasks 
non negativity constraints necessary breiman regression irrelevant improved predictive accuracy classi cation situation 
section formally introduce technique stacked generalization describe pertinent details learning algorithm experiments 
section describes results stacking di erent types learning algorithms 
section compares stacked generalization arcing bagging methods employ sampling techniques modify data distribution order produce multiple models single learning algorithm 
section describes related ends summary 

stacked generalization data set yn xn ng yn class value xn vector representing attribute values nth instance randomly split data equal parts lj 
de ne lj test training sets jth fold fold cross validation 
learning algorithms call level generalizers invoke kth algorithm data training set induce model called level models 
instance xn lj test set jth cross validation fold kn denote xn 
entire cross validation process data set assembled outputs models prediction model yn ng level data 
learning algorithm call level generalizer derive data model function zk 
level model 
illustrates cross validation process 
complete training process nal level models derived data consider classi cation process uses models conjunction new instance models produce vector zk 
vector input level model output nal classi cation result instance 
completes stacked generalization method proposed wolpert breiman leblanc tibshirani 
level level issues stacked generalization mk mk cv gure illustrates fold cross validation process level level data set process produce level model situation described results level model considers situation output level models set class probabilities single class prediction 
model classify instance lj pki denote probability ofthe ith output class vector pk xn pki xn pki xn gives model class probabilities nth instance assuming classes 
level data assemble class probability vector models actual class cv yn kn ng denote level model derived contrast subsections describe algorithms level level generalizers experiments reported section 
level generalizers learning algorithms level generalizers decision tree learning algorithm quinlan nb re implementation naive bayesian classi er cestnik ib variant lazy learning algorithm aha kibler albert employs nearest neighbor method modi ed value di erence metric nominal binary attributes cost salzberg 
learning algorithms show formula estimated output class probabilities pi instance cases pi 
consider leaf decision tree instance falls 
mi number training instances class leaf suppose majority class ting witten leaf mi 
laplace estimator imi pi mi note pruned trees default settings experiments 
nb ijx posterior probability class instance pi ijx ip ijx note nb uses laplacian estimate estimating conditional probabilities nominal attribute compute ijx 
continuous valued attribute normal distribution assumed case conditional probabilities conveniently represented entirely terms mean variance observed values class 
ib suppose nearest neighbors denote ys xs pg instance 
experiments 
pi pp pp ys xs xs ys ifi euclidean distance function 
learning algorithms predicted class level model instance pi level generalizers compare ect di erent learning algorithms level generalizer ib nearest neighbors nb multi response linear regression algorithm mlr 
needs explanation 
mlr adaptation squares linear regression algorithm breiman regression settings 
classi cation problem real valued attributes transformed multi response regression problem 
original classi cation problem classes converted separate regression problems problem class instances responses equal class zero 
input mlr level data need consider situation model attributes probabilities separately model 
large value wolpert advice reasonable relatively global smooth level generalizers perform 
issues stacked generalization classes 
case attributes real valued linear regression class simply lr kx case classes unordered nominal attributes 
map binary values obvious way setting class instance zero linear regression 
choose linear regression coe cients minimize yn xn lj yn xn coe cients constrained non negative breiman discovery necessary successful application stacked generalization regression problems 
non negative coe cient squares algorithm described lawson hanson employed derive linear regression class 
show fact non negative constraint unnecessary classi cation tasks 
place describe working mlr 
classify new instance compute lr classes assign instance class greatest value lr lr section investigate stacking nb ib 

stacking nb ib stacked generalization 
experiments section show successful stacked generalization necessary output class probabilities class predictions mlr algorithm suitable level generalizer algorithms 
arti cial datasets real world datasets uci repository machine learning databases blake keogh merz 
details table 
arti cial datasets led waveform training dataset size respectively generated di erent seed 
algorithms experiments tested separate dataset instances 
results expressed average error rate repetitions entire procedure 
real world datasets fold cross validation performed 
fold cross validation training dataset models derived evaluated 
pattern recognition community calls type classi er linear machine duda hart 
ting witten datasets samples classes attr type led waveform horse credit vowel euthyroid splice abalone nettalk coding nominal binary continuous 
table details datasets experiment 
test dataset 
result expressed average error rate fold crossvalidation 
note cross validation evaluation entire procedure fold cross validation mentioned section internal operation stacked generalization 
set experiments 
section results model combination level models model selection method employing fold cross validation procedure 
note di erence model combination model selection level learning employed 
table shows average error rates obtained fold cross validation nb ib best selected fold crossvalidation 
expected classi er lowest error rate 
table shows result stacked generalization level model level data comprise classi cations generated level models level data comprise probabilities generated level models 
results shown level generalizers case 
lowest error rate dataset bold 
table summarizes results table terms comparison level model datasets 
clearly best level model derived mlr 
performs better datasets equally tenth 
best performing derived nb performs better datasets signi cantly worse waveform vowel 
regard di erence standard errors signi cant con dence 
standard error gures omitted table increase readability 
datasets shown order increasing size 
mlr performs signi cantly better largest datasets 
indicates stacked generalization give signi cant improvements predictive accuracy volume data large direct consequence accurate estimation cross validation 

note select classi er folds 
error rate equal lowest error rate classi ers 
issues stacked generalization datasets level generalizers nb ib led waveform horse credit vowel euthyroid splice abalone nettalk coding table average error rates nb ib best selected fold cross validation 
standard errors shown column 
datasets level model level model nb ib mlr nb ib mlr led waveform horse credit vowel euthyroid splice abalone nettalk coding table average error rates stacking nb ib 
level model level model nb ib mlr nb ib mlr win vs loss table summary table comparison ting witten level models performs signi cantly better rest euthyroid vowel datasets mlr performs selecting best performing level model better 
mlr advantage level generalizers model easily interpreted 
examples combination weights derives model appear table horse credit splice abalone waveform led vowel datasets 
weights indicate relative importance level generalizers prediction class 
example splice dataset table nb dominant generalizer predicting class nb ib predicting class generalizers worthwhile contribution prediction class 
contrast abalone dataset generalizers contribute substantially prediction classes 
note weights class sum constraint imposed mlr 
non negativity constraints necessary 
breiman leblanc tibshirani stacked generalization method regression setting report necessary constrain regression coe cients non negative order guarantee stacked regression improves predictive accuracy 
investigate nding domain classi cation tasks 
assess ect non negativity constraint performance versions mlr employed derive level model linear regression mlr calculated intercept constant weights classes constraints ii 
linear regression derived intercept constant weights classes constraints iii 
linear regression derived intercept constant nonnegativity constraints non negative weights classes 
third version results earlier 
table shows results versions 
indistinguishable error rates 
conclude classi cation tasks non negativity constraints necessary guarantee stacked generalization improves predictive accuracy 
reason idea employ non negativity constraints 
table shows example weights derived versions mlr led dataset 
third version shown column iii supports perspicuous interpretation level generalizer contribution class predictions 
dataset ib dominant generalizer predicting classes nb ib worthwhile contribution predicting class evidenced high weights 
negative weights predicting classes render interpretation versions clear 
issues stacked generalization horse credit class nb ib nb ib ib 
table weights generated mlr model horse credit datasets 
splice abalone waveform class nb ib nb ib nb ib table weights generated mlr model splice abalone waveform datasets 
led vowel class nb ib nb ib table weights generated mlr model led vowel datasets 
ting witten datasets mlr constraints intercept non negativity led waveform horse credit vowel euthyroid splice abalone nettalk coding table average error rates versions mlr 
ii iii class table weights generated versions mlr constraints ii intercept iii non negativity constraints led dataset 
issues stacked generalization dataset se majority mlr horse splice abalone led credit nettalk coding waveform euthyroid vowel table average error rates majority vote mlr model number standard error se worst performing level generalizers 
stacked generalization compare majority vote 
compare error rate derived mlr majority vote simple decision combination method requires cross validation level learning 
table shows average error rates majority vote mlr 
order see relative performances level generalizers ect methods number standard errors se error rates worst performing level generalizer datasets re ordered measure 
selects best performing level generalizer small values se indicate level generalizers perform comparably vice versa 
mlr compares favorably majority vote wins versus losses 
wins signi cant di erences exceptions splice led datasets losses horse credit datasets insigni cant di erences 
extra computation cross validation level learning paid interesting note performance majority vote related size se 
majority vote compares favorably rst datasets values se small 
se large majority vote performs worse 
indicates level generalizers perform comparably worth cross validation determine best result majority vote far cheaper signi cantly di erent 
small values se necessary condition majority vote rival su cient condition see example 
applies majority vote compared mlr 
mlr performs signi cantly better datasets large se values cases 
ting witten versus nb ib mlr win vs loss table versus generalizer summarized results table 
worth mentioning method averages pi level models yielding pi predicts class pi breiman method produces error rate identical majority vote 
stacked generalization best generated mlr 
shown stacked generalization works best output class probabilities class predictions mlr algorithm ib nb 
retrospect surprising explained intuitively follows 
level model provide simple way combining evidence available 
evidence includes just predictions con dence level model predictions 
linear combination simplest way pooling level models con dence mlr provides just 
alternative methods nb ib shortcomings 
approach form basis suitable pooling level models con dence independence assumption central naive bayes hampers performance datasets evidence provided individual level models certainly independent 
builds trees model interaction attributes particularly tree large desirable combining con dences 
nearest neighbor methods really give combining con dences similarity metric employed misleadingly assume di erent sets con dence levels similar 
table summarizes results table comparing level generalizer datasets 
clearly better label representation discretizing continuous valued attributes creates intra attribute interaction addition interactions di erent attributes 
evidence table nb indi erent labels con dences normal distribution assumption embodies case reason unsuitable combining con dence measures 
mlr ib handle continuous valued attributes better label ones domain designed 
summary summarize ndings section follows 
learning algorithms obtain model perform satisfactorily 
issues stacked generalization mlr best learning algorithms level generalizer obtaining model obtained mlr lower predictive error rate best model selected fold cross validation datasets experiments 
advantage mlr level generalizers interpretability 
weights indicate di erent contributions level model prediction classes 
model derived mlr non negativity constraints 
constraints little di erence model predictive accuracy 
non negativity constraints mlr advantage interpretability 
nonnegative weights support easier interpretation extent model contributes prediction class 
derived mlr model compares favorably majority vote 
mlr provides method combining con dence generated level models nal decision 
various reasons nb ib suitable task 

comparison arcing bagging section compares results stacking nb ib results arcing called boosting originator schapire bagging reported breiman 
arcing bagging employ sampling techniques modify data distribution order produce multiple models single learning algorithm 
combine decisions individual models arcing uses weighted majority vote bagging uses unweighted majority vote 
breiman reports arcing bagging substantially improve predictive accuracy single model derived base learning algorithm 
experimental results describe di erences experimental procedures 
results stacking averaged fold cross validation datasets waveform averaged repeated trials 
standard errors shown 
results arcing bagging obtained breiman averaged trials 
breiman experiments trial uses random split form training test sets datasets waveform 
note waveform dataset irrelevant attributes breiman version irrelevant attributes expected degrade performance level generalizers experiments 
cases training instances dataset test instances breiman 
arcing bagging done decision tree models derived cart breiman trial 
ting witten dataset samples stacking arcing bagging waveform glass ionosphere soybean breast cancer diabetes table comparing stacking arcing bagging classi ers 
results datasets table indicate methods competitive 
stacking performs better arcing bagging datasets waveform soybean breast cancer better arcing worse bagging diabetes dataset 
note stacking performs poorly glass ionosphere small real world datasets 
surprising cross validation inevitably produces poor estimates small datasets 
discussion bagging stacking ideal parallel computation 
construction level model proceeds independently communication modeling processes necessary 
arcing bagging require considerable number member models rely varying data distribution get diverse set models single learning algorithm 
level generalizer stacking level models 
suppose computation time required learning algorithm arcing bagging needs models 
learning time required ta hc 
suppose stacking requires models model employs fold cross validation 
assuming time needed derive level models level model learning time stacking ts results table ta ts 
practice learning time required level level generalizers may di erent 
users stacking free choice level models 
may derived single learning algorithm variety di erent algorithms 
example section uses di erent types learning algorithms bag stacking stacking bagged models ting witten uses data variation obtain diverse set models single learning algorithm 
case performance may vary substantially level models example nb performs poorly vowel euthyroid datasets compared models see table 
stacking copes situation 
performance variation member models bagging small derived learning algorithm bootstrap samples 
section 
heart dataset breiman omitted modi ed original 
issues stacked generalization shows small performance variation member models necessary condition majority vote employed bagging 
worth noting arcing bagging incorporated framework stacked generalization bagged models level models 
ting witten show possible way incorporating bagged models level learning employing mlr voting 
implementation test set bagged models derive level data cross validated data 
viable bootstrap sample leaves examples 
ting witten show bag stacking higher predictive accuracy bagging models derived nb 
note di erence adaptive level model simple majority vote employed breiman arcing bagging improve predictive accuracy learning algorithms unstable 
unstable learning algorithm small perturbations training set produce large changes derived model 
decision trees neural networks unstable nb ib stable 
stacking works 
mlr successful candidate level learning algorithms equally 
candidate neural networks 
experimented back propagation neural networks purpose slower learning rate mlr 
example mlr took seconds compare seconds neural network nettalk dataset error rate 
possible candidates multinomial logit model jordan jacobs special case generalized linear models mccullagh nelder supra bayesian procedure jacobs treats level models con dence data may combined prior distribution level models bayes rule 

related analysis stacked generalization motivated breiman discussed earlier leblanc tibshirani 
leblanc tibshirani examine stacking linear discriminant nearest neighbor classi er show arti cial dataset method similar mlr performs better non negativity constraints 
results section show constraints irrelevant mlr predictive accuracy classi cation situation 
leblanc tibshirani ting witten version mlr employs class probabilities level model induce linear regression 
case linear regression class lr kx ix ki ki implementation requires tting ki parameters compared parameters version see corresponding formula section 

schapire freund bartlett lee provide alternative explanation ectiveness arcing bagging 
ting witten versions give comparable results terms predictive accuracy version runs considerably faster needs fewer parameters 
limitations mlr known duda hart 
class problem divides description space convex decision regions 
region singly connected decision boundaries linear hyperplanes 
means mlr suitable problems unimodal probability densities 
despite limitations mlr performs better level generalizer ib nearest competitor deriving limitations may hold key fuller understanding behavior stacked generalization 
jacobs reviews linear combination methods mlr 
previous stacked generalization especially applied classi cation tasks limited ways 
applies particular dataset zhang mesirov waltz 
report results convincing merz 
di erent focus evaluate results just datasets leblanc tibshirani chan stolfo kim bartlett fan 
consider degenerate form stacked generalization crossvalidation produce data level learning 
level learning done training process jacobs 
approach level learning takes place batch mode level models derived ho 
researchers worked degenerate form stacked generalization cross validation learning level 
examples neural network ensembles hansen salamon perrone cooper krogh vedelsby multiple decision tree combination kwok carter buntine oliver hand multiple rule combination kononenko kovacic 
methods level majority voting weighted averaging bayesian combination 
possible methods distribution summation likelihood combination 
various forms re ordering class rank ali pazzani study methods rule learner 
ting uses con dence prediction combine nearest neighbor classi er naive bayesian classi er 

addressed crucial issues successful implementation stacked generalization classi cation tasks 
class probabilities single predicted class input attributes higher level learning 
class probabilities serve con dence measure prediction 
second multi response squares linear regression technique employed high level generalizer 
technique provides method combining level models con dence 
learning algorithms algorithmic limitations suitable aggregating con dences 
combining di erent types learning algorithms implementation stacked generalization achieve better predictive accuracy model selection cross validation majority vote competitive arcing bagging 
stacked regression non negativity constraints squares regression necessary guarantee improved predictive accuracy classi cation tasks 
constraints preferred increase interpretability level model 
issues stacked generalization implication successful implementation stacked generalization earlier model combination methods employing weighted majority vote averaging computations level learning apply learning improve predictive accuracy 
acknowledgment authors grateful new zealand marsden fund nancial support research 
conducted rst author department computer science university 
authors grateful ross quinlan providing david aha providing ib 
anonymous reviewers editor provided helpful comments 
aha kibler albert 
instance learning algorithms 
machine learning pp 

ali pazzani 
error reduction learning multiple descriptions 
machine learning vol 
pp 

blake keogh merz 
uci repository machine learning databases www ics uci edu mlearn mlrepository html 
irvine ca university california department information computer science 
breiman 

stacked regressions 
machine learning vol 
pp 

breiman 

bagging predictors 
machine learning vol 
pp 

breiman 

bias variance arcing classi ers 
technical report 
department statistics university california berkeley ca 
breiman friedman olshen stone 
classi cation regression trees 
belmont ca wadsworth 
cestnik 

estimating probabilities crucial task machine learning 
proceedings european conference arti cial intelligence pp 

chan stolfo 
comparative evaluation voting meta learning partitioned data 
proceedings twelfth international conference machine learning pp 
morgan kaufmann 
cost salzberg 
nearest neighbor algorithm learning symbolic features 
machine learning pp 

fan chan stolfo 
comparative evaluation combiner stacked generalization 
proceedings aaai workshop integrating multiple learned models pp 

hansen salamon 
neural network ensembles 
ieee transactions pattern analysis machine intelligence pp 

ting witten ho hull srihari 
decision combination multiple classi er systems 
ieee transactions pattern analysis machine intelligence vol 
pp 

jacobs 

methods combining experts probability assessments 
neural computation pp 
mit press 
jacobs jordan nowlan hinton 
adaptive mixtures local experts 
neural computation pp 

jacobs jordan 
hierachical mixtures experts em algorithms 
neural computation pp 

kim bartlett 
error estimation series association neural network systems 
neural computation pp 
mit press 
kononenko kovacic 
learning optimization stochastic generation multiple knowledge 
proceedings ninth international conference machine learning pp 
morgan kaufmann 
krogh vedelsby 
neural network ensembles cross validation active learning 
advances neural information processing systems tesauro leen editors pp 
mit press 
kwok carter 
multiple decision trees 
uncertainty arti cial intelligence shachter levitt kanal lemmer editors pp 
north holland 
lawson hanson 
solving squares problems 
siam publications 
leblanc tibshirani 
combining estimates regression classi cation 
technical report 
department statistics university 


voting ensembles classi ers extended 
proceedings aaai workshop integrating multiple learned models pp 

mccullagh nelder 
generalized linear models 
london chapman hall 
merz 

dynamic learning bias selection 
proceedings fifth international workshop arti cial intelligence statistics ft lauderdale fl unpublished pp 

oliver hand 
pruning averaging decision trees 
proceedings twelfth international conference machine learning pp 
morgan kaufmann 
perrone cooper 
networks disagree ensemble methods hybrid neural networks 
arti cial neural networks speech vision editor 
chapman hall 
quinlan 

program machine learning 
morgan kaufmann 
issues stacked generalization schapire 

strength weak learnability 
machine learning pp 
kluwer academic publishers 
schapire freund bartlett lee 
boosting margin new explanation ectiveness voting methods 
proceedings fourteenth international conference machine learning pages morgan kaufmann 
smyth wolpert 
stacked density estimation 
advances neural information processing systems 
ting 

characterisation predictive accuracy decision combination 
proceedings thirteenth international conference machine learning pp 
morgan kaufmann 
ting witten 
stacking bagged models 
proceedings fourteenth international conference machine learning pp 
morgan kaufmann 
weiss kulikowski 
computer systems learns 
morgan kaufmann 
wolpert 

stacked generalization 
neural networks vol 
pp 
pergamon press 
zhang mesirov waltz 
hybrid system protein secondary structure prediction 
journal molecular biology pp 


