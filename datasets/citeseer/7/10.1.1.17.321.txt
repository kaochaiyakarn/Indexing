minimum message length kolmogorov complexity wallace dowe computer science monash university clayton vic australia email csw cs monash edu au notion algorithmic complexity developed kolmogorov chaitin independently solomonoff notion algorithmic probability 
turing machine prefix algorithmic complexity string length shortest input cause output 
solomonoff probability probability random binary string result producing output having prefix 
attempt establish parallel restricted part version kolmogorov model minimum message length approach statistical inference machine learning wallace boulton explanation data string modelled part message part stating general hypothesis data second encoding details data implied hypothesis 
solomonoff model tailored prediction inference considers just explanation gives weights explanations depending posterior probability 
amount data increases typically expect explanation dominant weighting prediction 

study applications complexity body information followed streams independent conceptions concept 
motivations direction development streams different obvious similarities stream offer learn 
intent describe relationships different streams try clarify important differences assumptions development 
studies mentioning relationships appear section iv pp 
sections 
stream described initiated kolmogorov important developments martin lof chaitin 
stream body information usually assumed finite string binary digits write length prefix kolmogorov algorithmic complexity respect specified universal turing machine utm may defined length shortest binary string supplied causes output 
set inputs finite forms prefix set member set prefix 
prefix set may regarded prefix code set finite binary strings 
code decoded universality ensures strings difference complexities respect andt bounded constant independent length longer programs required imitate imitate 
write kolmogorov complexity respect utm second stream chronologically springs solomonoff involves idea length input string cause utm output string treated just binary string string represents binary code data real world 
intent measure complexity develop probability distribution set finite binary strings model probability data represented string true real world 
emphasizing shortest produce stream considers strings cause produce output having prefix proper prefix produce defines unnormalized probability 
sum may dominated term shortest input case may approximated note definition requires output precisely definition requires output begins general exceed length shortest stream input 
primary motivation development probability distribution inductive prediction 
known data represented strings representing possible events observations ande code relative probability ande computer journal vol 
minimum message length kolmogorov complexity estimated prob prob represents concatenation 
kolmogorov complexity solomonoff probabilities defined respect particular utm universality guarantees different choices utm affect odds events factor bounds independent ande 
third stream introduced wallace boulton similar independent development rissanen 
streams basis shannon theory information theory turing machines 
second stream regards string representation code data real world 
seek string part specifies hypothesis source normally selected limited set possible hypotheses second part encoding data code optimally efficient stated hypothesis true 
optimally efficient mean code huffman arithmetic code minimizes expected length coded string 
shannon theory shows length string coding event optimally efficient code log prob neglect rounding integer values length log conditional probability data hypothesis indicate stated hypothesis string states 
length specification hypothesis assumption prior probability distribution set possible hypotheses optimal code specification log 
resulting total length log log log 
aim stream find hypothesis leads shortest string may regarded shortest message encoding data reason technique termed minimum message length mml minimum description length mdl inference 
motivation aim mml hypothesis shown capture information relevant selection hypothesis method inference automatically embraces selection hypothesis appropriate complexity leading estimates free parameters hypothesis 
exists acceptable hypothesis data set considered possible data concluded random noise 
minimization shown equation equivalent maximization prob joint probability hypothesis data 
formally equivalent choosing hypothesis highest bayesian posterior probability note passing finite posterior probability maximize density chosen general posterior mode 
continuing shown shortest message achieved general devising coding hypotheses code strings defined countable subset possible hypotheses 
may call subset set usable hypotheses may stated string roughly speaking selection subset done way hypotheses subset similar available volume data expected distinguish reliably 
useful side effect subsetting real valued free parameters original hypothesis space may prior density distributions defined lead finite prior probabilities parameter values 
restriction countable set usable hypotheses allows non zero prior probability assigned usable vector parameter values allows construction optimal finite length binary code hypothesis strings 
evident primary motivation mml mdl stream inductive inference general hypothesis data prediction data 
course model data source lead predictions necessarily solomonoff sense weight possible hypotheses data 
sections attempt relate stream algorithmic complexity ac frameworks streams 
specifically attempt show set hypotheses considered stream expanded include computable probability functions set possible data mml selection hypothesis data equivalent selection shortest input cause certain utm output data 
correspondence stream stream frameworks clearly exact requirement frameworks inputs producing string permit interpretation stating hypothesis data part structure mml message 
introduce certain constraints acceptable input strings 
discussion follows read attempt define set constraints ac framework may bring correspondence mml mdl 
constraints suggest may sufficient wholly necessary undoubtedly needed establish correspondence rigorous basis proves possible 

technical details 
turing machine turing machine definitions kolmogorov complexity solomonoff probability conveniently computer journal vol 
wallace dowe taken couple special features 
finding input leaving output single tape assumed classical turing machine model machine way input tape may read advanced altered 
way output tape may written advanced written 
usual tapes 
input output tapes binary alphabet blank delimiter symbol addition zero 
cases speak machine reading input producing string mean machine started distinguished start state unspecified tape content input tape containing followed unspecified digits machine reads writes output tape proper prefix satisfies conditions 
stream machine required 
stream action unspecified may may write output read input 
argue comparison stream convenient require machine having output digit enter read state 
stream assume utm modified replacing states read states 
note stream input causes unmodified produce stream input causes modified produce may strings stream definition cause modified produce cause unmodified produce streams causes produce appropriate definition write assumed modified stream 

probability complexity solomonoff explicitly relates complexity probability 
solomonoff probability respect data represented string just probability infinite string random digits input produce output having prefix 
event produces finite string may considered contributing probability null string 
undecidability halting problem prevents exact computation normalization probability distribution finite strings welldefined concept clear probabilistic interpretation 
algorithmic complexity define unnormalized probability prob say 
probability correspond probability easily defined event solomonoff probability 
may reasonably considered conservative estimate little smaller 
shannon theory uniquely decodable code devised set mutually exclusive exhaustive events probability event expected length message announcing event minimized length code word log anda code constructed called optimal efficient 
properties efficient code provides exactly code word string possible event nonredundant 
utm may regarded decoder code finite strings code word input produces output code clearly efficient code words exist code highly redundant 
shortest code string code may fairly efficient unnormalized probability distributions strings defined 
particular shares efficient codes property words random senses described 

randomness informally random source binary digits provides infinite stream digits knowledge digits provides information digit produces ones zeros equal frequency 
property efficient shannon code sequence events randomly drawn probability distribution code efficient code words events concatenated resulting stream digits comes random source 
note statement randomness source process producing binary digits randomness finite string 
randomness finite string usually assessed asking standard statistical test cause reject hypothesis string came random source 
string came random source efficient binary coding event sequence test course result falsely rejecting randomness hypothesis just cases source happens chance give string exhibiting pattern 
complexity theory provides different definition randomness directly applicable finite strings 
string random utm wherec small constant chosen impose significance requirement concept non randomness larger values requiring lower complexity string accepted non random 
shown definition conflict ordinary notion randomness 
way stating definition random code defined provides coding significantly shorter thatis compressible note notion randomness incompressibility sense context redundant coding system 
code defined representations form copy computer journal vol 
minimum message length kolmogorov complexity program followed specification length followed 
length representation log 
shortest representation declared random 
code permits substantially shorter representation seen non random 
alternative representations exist redundant codes 
easily shown drawn random source earlier sense probability strings length equiprobable random source prefix free representations available length conversely ifs comes non random source early digits give hint values digits 
case may input comprising fixed length program compute probability usual sense nth digit function earlier digits decode shannon optimal code nth digit probability 
probability differs half optimal code able code digit bit average 
provided long input shorter declared non random universal follows shortest input produces random significantly shorter representation shorter representation constructed comprising short program followed program instruct decode remember resulting string act input 
shortest code word string code words shannon code random somewhat different sense 
similar remarks apply part mml code 
redundant data may encoded assertion usable hypothesis 
compressibility relevant concept mml chooses maximize compression 
incompressible regarded random having pattern expressible set possible hypotheses 

universal distributions interest ac means prediction arises special property universal universal 
particular effectively computable distribution finite strings ratio depends result follows fact exists finite length program enables compute finite decode shannon optimal code 
exceed log 
cq may taken argument applies 
suppose context observational program real world yields data strings randomly drawn distribution closely approximated 
suppose string observed interested relative probabilities alternative events representable respectively ratio probabilities approximately 
probabilities ratio bounded cq ratio may approximated 
approximation crude best definition negative power 
ac probability possibility exists greatly exceeds strings happen computationally simple form useful hard bounds placed approximation error 
wide class computable distributions strings high probability approaching long strings part input form described shortest inputs producing high probability approximation quite close 
universal ability complexity probabilities approximate probability ratios effectively computable distributions justifies prediction 

data hypotheses trying version ac assimilated version mml inference 
concepts data hypotheses play central roles concepts appear explicitly stream hypotheses formal role stream 
allow correspondence streams mml appear need impose specific interpretation meaning binary strings involved 
section attempt specify interpretation outline consequent formal constraints structure binary strings 

data string assume binary string representation data obtained observation real world phenomenon wish understand wish predict 
representation representation number facts extent concerned mapping facts observed binary strings 
propose assume consists concatenation sentences data language sentence records observed facts relating particular instance studied phenomenon 
sentences form prefix set string uniquely parsed sentences 
helpful assume sufficiently long string sentence prefix computer journal vol 
wallace dowe distinct sentences represent distinct sets facts 
assume non redundant 
assumption probably weakened cost elaboration argument 
important assumption sentence records data independent instance phenomenon 
independence mean believe facts recorded sentence causally statistically related facts recorded sentence way sentences record instances phenomenon 
believe sentences conditionally independent full knowledge nature phenomenon 
may objected data world gives beliefs irrelevant means 
objection valid principle real data real science invariably captured interpreted transformed human choice look instruments believe senses instruments observe 
similar beliefs independence instances inescapable nature may legitimately part understanding choices beliefs 
consequence independence assumption concatenation sentences order sentences may permuted affecting meaning data string permuted string may substituted 
hypotheses hypothesis phenomenon studied computable probability distribution sentences consider hypotheses context defined data language consider meaning hypothesis outside context 
probability hypothesis written 
independence sentences implies probability data string product 

note excluding hypotheses sequences data produced high order markov process expect probability observation sequence depend values previous observations 
regard record sequence single sentence data string comprises sentences read sequences produced independent runs markov process widely separated subsequences run 
conversely require facts recorded single sentence necessarily interdependent 
suppose example fact weight randomly selected cow sentences code sentence length followed weights 
respectively sentences sentence may sentence probably case believe weight independent instance cow weight believe meaning meaning hypothesis may hypothesize correlation weights sentence treat quite differently sentence forms 
course hypothesizes independence weights sentence presence length codes sentences general mean 

part encoding consider notion hypothesis encoding data string may embedded ac context 
approach described li vit anyi stream concept conditional prefix complexity 
conditional complexity string string defined length shortest input string cause utm produce access string tape 
considered representation hypothesis 
difficulty approach may regarded defining conditional probability probability correspond closely ordinary conditional probability obtaining data proposition true source 
particular may zero 
data falsify hypothesis sense 
string treated asserting truth hypothesis merely information may may compression data 
propose string representing hypothesis prefix input follows stream conventions 
recall assume utm state define input produce output written reads writes enters read state proper prefix satisfies conditions 
input regarded acceptable mml message encoding data string wheres sentences data language certain conditions met 
conditions reasons described 
requires encode condition requires compression data achieved regularity data discovered 
form empty string 
hypothesis determine data 
computer journal vol 
minimum message length kolmogorov complexity define tq turing machine equivalent state reading strings tq 
follows tq strings tq tq equivalently 
condition means reading producing permanently alter state remains equivalent tq 
intent ensure regularity exploited compress data expressed machine inference condition excludes incremental codes successive independent data items sentences refine distribution code item 
codes efficient lead parts capturing regularity data basis estimation 

condition requires say significant data 
condition strictly testable computable practice best known upper bound smaller 
suppose comprises sentences require tq condition forces separate sentences treated independent propositions hypothesis 
supposing comprise sentences exist equivalent data string comprises sentences comprises remainder correspondingly permuted input satisfies tq tq 
attempt require certain generality hypothesis 
means hypothesis says useful independent subsets available data 
strong require notation sentences 
proper prefix satisfying conditions merely means needed specification hypothesis 
collection conditions attempt specify conditions input plausible identify part input specification hypothesis requiring attempt understand course computation detailed inspection vein show division parts unique 
suppose contrary 
written string 

putting contrary proper prefix satisfying conditions supposition false 

interpretation part suggested conditions input intended ensure bear interpretation part message part states general hypothesis 
described section regard hypothesis probability distribution sentences data language ifi acceptable mml message causing produce data string need ask hypothesis represented part string effect converting equivalent turing machine tq 
machine decode certain strings sentences sequences sentences concatenation sentences condition sure tq decode set input strings sentences 
generally tq seen decoder code subset sentences subset contains sentence exists string tq code words code strings input tq cause output sentence sentences prefix set code defined prefix code necessarily non redundant 
may exist infinite strings word code may code words producing sentence 
propose interpreted specification relative probability distribution sentences shortest code word sentence code word prefix property code ensures 
propose respect interpreted specifying hypothesis tq decodes code 
mean hypothesis true code decoded tq minimizes expected length code word needed encode data sentence 
course code truly optimal hypothesis non redundant necessarily case 
note code non redundant equality reached inequality tq clearly universal 
capability decode non redundant prefix code sentences 
probability distribution defined hypothesis clearly computable 
hand code redundant may exist input strings commencing code word computer journal vol 
wallace dowe cause tq emulate utm 
probably undecidable case code words enumerated may correspond computable probability distribution necessarily mean incomplete hypothesis defined known code word lengths meaning useless 
simply mean hypothesis assigns computable probabilities sentences including sentences data possible sentences 
useful predicting relative probabilities potential sentences lie domain 
hypothesis complete specified propose part acceptable mml input string necessarily falsifiable show 
recall section data language assumed nonredundant 
sentences form complete prefix code data facts 
defines default probability distribution sentences 
acceptable mml input sentence tq code provides shorter coding complete code 
follows sentences tq code code word longer data add sentences data input able encode augmented data briefly longer acceptable hypothesis 

part versus part codes section developed model mml hypothesis set comprising computable distributions restricted form ac 
restricting conditions trivial question arises restricted form deviates far ac framework interesting correspondence exists 
essence question conditions expect shortest mml encoding data string length close ac complexity string 
defining length shortest acceptable mml input utm producing clear difference zero 
li vitanyi sections discuss question assuming string form conditional ac complexity 
gloss details treatment reached data string long typical realization recursive distribution distribution typical distribution recursive functions implied lengths shortest programs required compute difference small high probability hypothesis string probably approximate distribution data drawn 
treatment appears apply part form proviso independence assumptions data language enforced true data source 
theoretical results concerning complexity inductive inference powerful convergence results barron cover assumed part input form similar conditional complexity form section 
difference interesting interpretation 
parts shortest mml message 
length ac complexity respect distribution defined representing mml hypothesis may taken 
definition log 
treat ac probabilities 

ordinary probabilities follows log 
bayes theorem difference negative log posterior probability hypothesis data wherep plays role prior probability choosing minimize formally equivalent choosing hypothesis maximum posterior probability data 
follows hypotheses data posterior odds ratio favour prob prob shortest inputs respective hypotheses 

expected actual lengths experts ac see fundamental difference algorithmic mml approaches inference hypotheses concerned shortest encoding actual data respect turing machine considers shannon optimal codes giving shortest expected length encodings 
view mistaken 
mistake understandable different histories streams 
mml developed standard statistical inference framework probability models givens standard gaussian binomial distributions traditionally model data 
line thought went probabilities data strings code word lengths needed encode 
natural code transmitting data receiver informed inferred model shannon optimal code 
ac theory turing machine line thought naturally runs code word strings encode data string probability data string 
ac theory probability associated output string stream probability computer journal vol 
minimum message length kolmogorov complexity emit output random input 
suppose code words accepts form nonredundant code set output strings 
clearly universal 
ac definitions relative probability output just probability emit random input unique input producing zero produce probability distribution outputs exactly distribution code decoded shannon optimal 
decodes code possible outputs minimal expected code word length 
universal reasonable regard 
distribution near optimal decoder 
streams agree identify input string length negative log probability dealing input codes output strings close optimal terms expected length 
streams ac mml mdl shortest word code produce output worth noting stream development mml type codes section mention expected lengths occurs section interpretation hypotheses discussed 
expected length played role definition selection mml input string 
criterion actual length input required actual data 
expected code word lengths play role streams important consideration choice decoder choice determines complexities distributions may employed coding data 
part codes determines lengths shortest part string needed specify hypothesis distribution universal typically case practical applications mml mdl determines set possible hypotheses 
fact data choice limits usable hypotheses universal usable hypotheses complexity attempt inference finite body data relevant prior knowledge 
fact prior knowledge phenomenon studied scarcely possible obtain record information 
practice informal everyday inductions guided huge body knowledge world possess 
serious attempt inductive inference knowledge extent biasing inference hypotheses 
context algorithmic complexity means choosing provide short representations regularities thought data longer representations long shots 
important note choice knowledge genuinely prior knowledge source data way represented encoding concept arises transmission information agent storage information recall agent know forgotten 
case may reasonably assumed agent decode information receiver general prior information context information prior beliefs original source 
encoding reasonably chosen prior knowledge mind 
encoding chosen function information necessarily unintelligible receiver definition information attempts decode encoded form 
coding choices light information stated encoded form code decodable receiver information 
embodied context receiver 
choosing attempting model receiver just knowledge genuinely available prior receipt data clear mml framework stream approach actual encoding string shortest input cause produce difference approach lies choice traditional stream approach tended dismiss choice unimportant provided universal mml stream attempt specify model receiver agent informed relevant prior information 
inference problem prior knowledge suffices exclude limited set hypotheses need universal 
willing entertain hypothesis representable computable function mml universal chosen far practicable reflect relevant prior knowledge 
making choice addressed stream expected string lengths interest 
choice data obtained mml chooses encoding exactly stream school chooses shortest decode 

related confusion related confusion regard fundamental difference streams best illustrated example 
suppose data string generated random process emits digit independently gives probability 
set possible hypotheses contains family bernoulli processes probability parameter highly member decoding code encodes digit length log digit length log close provide shortest part encoding possible second part encoding happens digits 
suppose 
ac school inclined suggest shorter encoding possible clearly non random compressed published mml treatments bernoulli processes admit compression 
true provided universal long computer journal vol 
wallace dowe shorter encoding ac framework 
mml framework 
limited non universal machines hypothesis set envisaged mml analysis contain hypothesis family source data encoded optimal bernoulli code coded string generated bernoulli process probability 
corresponding family hypotheses mml parameter family hypotheses composed bernoulli decoders feeding 
hypothesis family mml provide shorter encoding single bernoulli hypothesis mml inference 
family available plain bernoulli hypothesis near inferred despite form mml method deals expected actual lengths mml analysis allow hypotheses capable representing particular regularity appeared data 
note non universal follows second part strings appear developed purely random process zeroth order markov model 
necessarily follows code words appear compressible turing machine section 
efficient code redundant available code words 

choosing decoder describe turing machine designed mml inference 
contributing stream mml mdl 
practice similar results differences way decoding machine selected way encode data 
describe mml construction offer comments mdl 

mml code described section mml inference normally concerned selection hypothesis model data source limited set possible hypotheses 
recasting notation section form making decoding machines assume possible hypotheses 
set may contain continua various dimensions spanned set real valued parameters 
prior probability function density continuum defined countable set possible data strings 
conditional probability likelihood function defined turing machine decodes optimal code distribution 
iff 
computable usually case code shannon optimal code requiring input length log 
computable may chosen minimize far practicable expected length shortest input cause produce expectation taken distribution 

select hypothesis machine best able compress data expected occur hypothesis true 
define 
sum replaced integral continuum 
marginal probability data construction proceed different ways 

strict mml partition non empty subsets 
subset define machine corresponding minimizes jh quantity weighted average lengths inputs needed machine produce averaged strings set machines 
set useful turing machines set corresponding hypotheses 
set useful hypotheses may encoding data 
turing machine capable imitating useful machines includes set useful hypotheses length input needed imitate minimized expectation distribution 
usual computable case needs decode shannon optimal code distribution log 
partition chosen consequent choice useful machines encoding minimize expected length part encoding string drawn marginal distribution useful machine subset containing discovering minimizing partition extremely difficult calculation simple regular functions 

properties resulting inference method described 
briefly shown method computer journal vol 
minimum message length kolmogorov complexity general invariant arbitrary measure preserving transformations statistically consistent efficient sense hypothesis giving shortest part encoding captures information relevant choice hypothesis 
widely bayesian point estimators maximum posteriori map selects mode posterior density mean posterior invariant sense 
estimate obtained methods depends model space parametrized 

practical mml construction infeasible simplest problems 
fortunately purposes inferring hypothesis data necessary construct encoding 
need able calculate sufficient accuracy length encoding hypothesis find hypothesis minimizes approximate length 
practical applications mml approximations 
result useful problems stated 
region dimensional continuum vector parameter prior density probability function exponential family length shortest encoding approximately log log log log log logs natural length units log bits fisher information probability function chosen minimize expression 
error expression corrections smaller known 
minimizing chosen full set possibilities restricted useful set need constructed 
minimizing taken mml inference 
emphasize construction outlined way introduces fundamental distinction mml minimizing part complexity hypothesis inference 
mml differently gives serious consideration choice machine appropriate data hand stream largely see section 
computable 

exactly construction derived aim producing bayesian point estimates high posterior probability information ac theory 

minimum description length mdl development differs respects mml approach practical applications similar results usually obtained 
mdl stated aim usually select single fully specified hypothesis select parametrized family called model class 
instance encoding set values set polynomial functions form plus gaussian noise term mdl emphasis selecting best order polynomial estimation coefficients considered different means 
consistent aim mdl defines stochastic complexity data respect model class length encoding data code optimized distribution obtained integrating parameters model class respect measure considered appropriate 
practice stochastic complexity approximated code length part encoding part names discrete set parameter vectors second codes data code distribution stated vector mml 
discrete set employed similar set useful models constructed mml reservations mdl emphasis model classes fully specified models 
loses ability mml parameter estimates superior maximum likelihood estimates typically conjunction mdl 
problems mixture modelling competing hypotheses formal structure differing parameter values may really represent models markedly different conceptual structure lumping class little different different model classes 
second mdl assumption prior probabilities 
codes encode parameter vector attempt avoid appeal priors relying natural enumeration vectors 
similarly integration performed parameters obtain exact stochastic complexity appeal natural measure parameter space 
course resulting code may interpreted code shannon optimal prior interpreted prior implied codes practice mdl usually alarm bayesian mdl school resists interpretations 
implied prior resembles jeffreys prior proportional square root fisher information inadmissible genuine bayesian prior depends properties observational protocol measure parameters genuinely prior beliefs values 
third development mdl attempt shorten second part part encodings 
fact hypothesis giving shortest encoding encoding data data strings possible probability distribution useful fact lead inference different hypothesis preferred 
putting way particular may coding strings coded code optimal 
include strings best hypothesis include strings coded optimal code 
proposed code stated part distribution 
computer journal vol 
wallace dowe truncated form distribution 
seen attempt approximate stochastic complexity better technique known complete coding may virtue 
light part coding destroys basis mdl selecting useful parameter vectors advantage codelength terms vectors possible 
fact shortest length obtained allowing data string hypothesis reducing length second part zero 
done minimal code length achieved depend hypothesis string mapped provided string non zero probability hypothesis 
construction provides basis selecting mapping useful estimator 
code length achieved complete version mdl data just log marginal data probability defined 
potential saving code length usually small 
shown expected saving regular model classes free parameters log large nit 
models exponential family greatest saving string exceeds expected saving bits 
regular model classes best case saving may greater model classes studied strings giving substantial savings rare marginal data distribution 
just results hold small samples 

terms order stream literature pays little attention choice utm analysing behaviour complexity 
reason usually difference complexities string relative length interpreter program independent string order 
consider dismissal order terms misguided potentially dangerous attempt apply stream theory real problems prediction estimation induction 
solomonoff approach stream consider terms order choice important 
deals relative probabilities small extensions long data string machine results may relatively insensitive choice theory advanced means discovering pattern randomness real finite data string choice may crucial 
note length interpreter making imitate whilst fixed independently data may nit log bits 
small 
length actual interpreter general purpose computer imitate quite similar computer typically thousands digits 
whilst length due speed considerations essential 
second results suggesting inductive ac rest derivations part input paradigm 
obtaining results indicating consistency convergence efficiency length part input plays crucial role 
balance lengths second parts prevents data string completely encoded part determines string random allows assert part captures regularities data leaving noise second part 
crucial length part 
precisely length interpreter imitate turing machine 
length precisely order dismissed unimportant selection non selection recall difference lengths part encodings data machine interpreted log posterior odds ratio hypotheses 
difference bits represents ratio approximately 
likelihood posterior ratio usually considered highly significant statistical inference length difference giving posterior odds billions overwhelming 
arbitrary unwise selection easily larger changes lengths needed encode hypotheses completely reverse seen convincing evidence 
complexity methods accepted practical reliable methods induction prediction terms order 
mdl school open weaker version criticism 
different equally natural choices parametrization enumeration model classes lead variations calculated complexity large compared convincing length differences 
mdl replaced calculation log fisher information important component part length gross approximation log number parameters model class data sample size 
error introduced easily exceed posterior odds poorly conditioned multiple linear regression problems regardless sample size 
similarly mdl neglected geometric factors arising spacing useful parameter vectors introducing smaller significant error length calculations 
mml considerable care minimizing errors 
largest uncertainty cases comes priors chosen mathematical convenience accurate representation reasonable prior beliefs 
computer journal vol 
minimum message length kolmogorov complexity 
prediction induction sense practical uses previous data predictive 
past experience guide behaviour predictions events 
reasonable ask inductive inference limited dictionary sense forming general theories specific data need play role intelligent reasoning 
predict data formulating theories source bother theories inevitably uncertain usually flawed 
solomonoff stream program offer just theory free prediction 
starting utm data allows inference relative probabilities events represented strings requiring commitment theory represented part part encodings streams 
part answer simple curiosity understand world inductively derived theories help 
pragmatic desire theories solomonoff program leaving aside questions computability capacity execute 
individuals society record full data guidance certainly afford computational effort computing previous data just predict high ball bounce needed acid spill 
general theories serve useful imperfect summaries relevant aspects previous data compact remember record simple give rapid calculation approximate probabilities events 
discussion theories represented parts part inputs turing machines encoded strings 
theory machines may prediction 
tq equivalent machine read part mml input data tq estimate relative probability data strings tq 
solomonoff shown machine constructed solomonoff probability ratio ratio summarizes known data 
general construction requires effectively contain design complete copy representation general case computation predictive probability ratios faster forgets learns construction intended existence proof practical means prediction 
pragmatic need compact theories implications drives scientific enquiry modest scale mml 
concede mml stream derived theory predict solomonoff probability ratios equivalent computable domains ratios conventional bayesian posterior densities bayesian decision theory 
commitment single theory brings great benefits allows deductions proceed easily 
accurate alternatives effect require possible theory consistent data weight prediction deductive reasoning data 
compromise possible 
analysis fails find single theory overwhelming superiority best theories may retained predictions weighted average various predictions 
examples compromise developed 

argued applied inference models theories data essential difference kolmogorov complexity minimum message length approaches 
differ choice turing machines attention choice 
stream universal machine regarded acceptable mml usually necessarily restricts machine non universal form interest computational feasibility 
mml attempts far possible choose machine complexities different theories relative machine reflect prior probabilities afforded theories intelligent agent knowing circumstances data obtained 
attention choice machine allows mml domains possible theories computable estimate complexities errors digits 
contrast significant terms order usually neglected stream theory 
result mml routinely applied confidence problems machine learning inductive statistical inference finite bodies real data comparison competing theories theory require overwhelming evidence data approximations theory safely ignored 
minimum description length close mml practice successful applications 
avoidance priors accurate length approximations may reliable mml 
techniques ideally suited predictive inference 
solomonoff algorithmic probability theory ideally offer better predictive performance account possible explanations known data 
practical application ideal solomonoff model appears difficult solomonoff proposed applicable model resource limited computations practical prediction real data best approach feasible may form weighted combination predictions best theories part stream mml methods suggestion solomonoff selecting prediction yielded shortest known stream encodings data selection favouring shorter inputs 
demonstrated experience authors complexity inference particularly computer journal vol 
wallace dowe mml past years led confidence properties 
problems number parameters estimated grows proportion data mixture modelling factor analysis neyman scott problem mml known theory seen practice give consistent results maximum likelihood akaike information criterion aic related classical techniques known fail 
mml give statistical estimators performance difficult distributions handle easily complex model selection problems inference causal nets 
mindful bayesianism inherent streams see sections experience led second author conjecture mml minimum expected kullback leibler distance closely related bayesian techniques general infer fully specified models statistical consistency invariance re parametrization 
gained discussions ray solomonoff vladimir vovk paul vitanyi jon oliver supported australian research council arc large 
barron cover 
minimum complexity density estimation 
ieee trans 
inform 
theory 
li vitanyi 
kolmogorov complexity applications nd edn 
springer new york 
rissanen 
modelling shortest data description 
automatica 
kolmogorov 
approaches quantitative definition information 
prob 
inform 
transmission 
chaitin 
lengths programs computing binary sequences 
assoc 
comput 
mach 
solomonoff 
formal theory inductive inference ii 
inform 
control 
wallace boulton 
information measure classification 
comput 

wallace boulton 
invariant bayes method point estimation 
classification soc 
bull 
boulton wallace 
program numerical classification 
comput 

boulton wallace 
information measure hierarchic classification 
comput 

boulton wallace 
information measure single link classification 
comput 

boulton wallace 
information content distribution 
theoret 
biol 
wallace freeman 
estimation inference compact coding 
statist 
soc 
wallace 
false oracles strict mml estimators 
dowe korb oliver 
eds information statistics induction science proc 
isis melbourne august pp 

world scientific singapore 
earlier version technical report june department computer science monash university australia 
wallace 
complexity strict minimum message length inference 
technical report department computer science monash university australia 
rissanen 
stochastic complexity statistical inquiry 
world scientific singapore 
wallace freeman 
single factor analysis mml estimation 
statist 
soc 
dowe wallace 
resolving neyman scott problem minimum message length 
computing science statistics proc 
th symp 
interface sydney australia pp 
technical report department computer science monash university 
wallace dowe 
mml estimation von mises concentration parameter 
technical report department computer science monash university 
wallace 
multiple factor analysis mml estimation 
technical report department computer science monash university 
lindley 
bayesian statistics review 
siam philadelphia pa bernardo smith 
bayesian theory 
wiley chichester 
dom 
mdl estimation small sample sizes application linear regression 
technical report rj ibm almaden research division ca usa 
allison wallace yee 
string string 
proc 
int 
symp 
artificial intelligence mathematics fort lauderdale fl 
wallace dowe 
mml mixture modelling multi state poisson von mises circular gaussian distributions 
proc 
comp 
sci 
stat th symposium interface sydney australia 
interface foundation north america 
oliver wallace 
inferring decision graphs 
ijcai workshop sydney 
oliver dowe wallace 
inferring decision graphs minimum message length principle 
proc 
th joint conf 
artificial intelligence hobart pp 

oliver 
decision graphs extension decision trees 
proc 
th int 
conf 
artificial intelligence statistics fort lauderdale miami usa pp 

baxter dowe 
model selection linear regression mml criterion 
technical report department computer science monash university 
earlier summary storer cohn 
eds proc 
th ieee data compression conf 
snowbird computer journal vol 
minimum message length kolmogorov complexity utah 
ieee computer society press los alamitos ca 
patrick wallace 
stone circle geometries information theory approach 

ed old world pp 

cambridge university press cambridge 
wallace 
improved program classification 
proc 
th 
comput 
sci 
conf 

australian comput 
sci 
commun 
solomonoff 
kinds probabilistic induction 
comput 

oliver hand 
averaging decision trees 
classification 
wallace georgeff 
general objective inductive inference 
technical report department computer science monash university 
wallace korb 
learning linear causal models mml sampling 
gammerman 
ed causal models intelligent data management 
springer verlag berlin appear 
dowe baxter oliver wallace 
point estimation kullback leibler loss function mml 
nd pacific asia conf 
knowledge discovery data mining proc 
pakdd melbourne april singapore 
springer verlag berlin 
computer journal vol 

