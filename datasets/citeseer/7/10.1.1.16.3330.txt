em algorithms pca spca sam roweis expectation maximization em algorithm principal component analysis pca 
algorithm allows eigenvectors eigenvalues extracted large collections high dimensional data 
computationally efficient space time 
naturally accommodates missing information 
introduce new variant pca called sensible principal component analysis spca defines proper density model data space 
learning spca done em algorithm 
report results synthetic real data showing em algorithms correctly efficiently find leading eigenvectors covariance datasets iterations hundreds thousands datapoints thousands dimensions 
em pca 
principal component analysis pca widely dimensionality reduction technique data analysis 
popularity comes important properties 
optimal terms mean squared error linear scheme compressing set high dimensional vectors set lower dimensional vectors reconstructing 
second model parameters computed directly data example diagonalizing sample covariance 
third compression decompression easy operations perform model parameters require matrix multiplications 
despite attractive features pca models shortcomings 
naive methods finding principal component directions trouble high dimensional data large numbers datapoints 
consider attempting diagonalize sample covariance matrix vectors space dimensions 
difficulties arise form computational complexity data scarcity 
computing sample covariance costly requiring np operations 
general best avoid altogether computing sample roweis cns caltech edu computation neural systems california institute tech 
data scarcity front data high dimensions sample covariance full rank careful employ techniques require full rank matrices 
complexity front direct diagonalization symmetric matrix thousands rows size extremely costly operation theta inputs 
fortunately techniques exist efficient matrix diagonalization leading eigenvectors eigenvalues required example power method 
covariance explicitly 
methods snap shot algorithm assuming eigenvectors searched linear combinations datapoints complexity 
note version expectation maximization em algorithm learning principal components dataset 
algorithm require computing sample covariance complexity limited operations number leading eigenvectors learned 
shortcoming standard approaches pca obvious deal properly missing data 
methods discussed accommodate missing values incomplete points discarded completed variety ad hoc interpolation methods 
hand em algorithm pca enjoys benefits em algorithms terms estimating maximum likelihood values missing information directly iteration 
pca model suffers critical flaw independent technique compute parameters define proper probability model space inputs 
density normalized principal subspace 
words perform pca data ask new data fit model criterion squared distance new data projections principal subspace 
datapoint far away training data near principal subspace assigned high pseudo likelihood low error 
similarly possible generate fantasy data pca model 
note introduce new model called sensible principal component analysis spca obvious modification pca define proper covariance structure data space 
parameters learned em algorithm 
summary methods developed provide advantages 
allow simple efficient computation eigenvectors eigenvalues working datapoints high dimensions 
permit computation presence missing data 
real vision problem missing information computed leading eigenvectors eigenvalues points dimensions hours matlab modest workstation 
small variation methods allow computation principal subspace complete gaussian probabilistic model allows generate data compute true likelihoods 
em pca 
principal component analysis viewed limiting case particular class models 
goal models capture covariance structure observed dimensional variable fewer free parameters required full covariance matrix 
linear gaussian models assuming produced linear transformation dimensional latent variable plus additive gaussian noise 
denoting transformation theta matrix dimensional noise covariance matrix generative model written cx latent cause variables assumed independent identically distributed unit variance spherical gaussian 
independent normal distributed assumed independent model reduces single gaussian model vectors column vectors 
denote transpose vector matrix notation determinant matrix denoted jaj matrix inversion gamma zero matrix identity matrix symbol means distributed 
multivariate normal gaussian distribution mean covariance matrix sigma written sigma 
gaussian evaluated point denoted sigma write explicitly gamma cc delta order save parameters direct covariance representation space necessary choose restrict covariance structure gaussian noise constraining matrix example shape noise distribution restricted axis aligned covariance matrix diagonal model known factor analysis 
inference learning central problems interest working linear gaussian models described 
problem state inference compression asks fixed model parameters said unknown hidden states observations 
datapoints independent interested posterior probability xjy single hidden state corresponding single observation 
easily computed linear matrix projection resulting density gaussian xjy yjx cx cc xjy fiy gamma fic fi cc gamma obtain expected value fiy unknown state estimate uncertainty value form covariance gamma fic 
computing reconstruction straightforward yjx cx computing likelihood datapoint merely evaluation 
second problem learning parameter fitting consists identifying matrices model assign highest likelihood observed data 
family em algorithms various cases restrictions follow similar structure inference formula step estimate unknown state choose restricted step maximize expected joint likelihood estimated observed zero noise limit principal component analysis limiting case linear gaussian model covariance noise infinitesimally small equal directions 
mathematically pca obtained limit lim ffl ffli 
effect making likelihood point dominated solely squared distance reconstruction cx 
directions columns minimize error known principal components 
inference reduces simple squares projection xjy fiy gamma fic fi lim ffl cc ffli gamma xjy gamma gamma delta ffi gamma gamma noise infinitesimal posterior states collapses single point covariance zero 
restriction merely save parameters covariance observation noise restricted way model capture interesting informative projections state restricted learning algorithm simply choose set covariance data trivially achieving maximum likelihood model explaining structure data noise 
remember model reduced single gaussian distribution better having covariance model equal sample covariance data 
recall theta rank left multiplication cc gamma appears defined cc invertible exactly equivalent left multiplication gamma intuition cc truly invertible directions invertible exactly project 
em algorithm pca key observation note principal components computed explicitly em algorithm learning 
easily derived zero noise limit standard algorithms see example section replacing usual step projection 
algorithm ffl step gamma ffl step new yx xx gamma theta matrix observed data theta matrix unknown states 
columns span space principal components 
compute corresponding eigenvectors eigenvalues explicitly data projected dimensional subspace ordered orthogonal basis covariance subspace constructed 
notice algorithm performed online single datapoint time storage requirements kp 
workings algorithm illustrated graphically 
gaussian input data non gaussian input data examples iterations algorithm 
left panel shows learning principal component data drawn gaussian distribution right panel shows learning data non gaussian distribution 
dashed lines indicate direction leading eigenvector sample covariance 
dashed ellipse standard deviation contour sample covariance 
progress algorithm indicated solid lines directions indicate guess eigenvector lengths indicate guess eigenvalue iteration 
iterations numbered number initial condition 
notice difficult learning right get stuck local minimum take iterations converge unusual gaussian data see 
intuition algorithm follows guess orientation principal subspace 
fix guessed subspace project data give values hidden states fix values hidden states choose subspace orientation minimizes squared reconstruction errors datapoints 
simple twodimensional example give physical analogy 
imagine rod pinned origin free rotate 
pick orientation rod 
holding rod project datapoint rod attach projected point original point spring 
release rod 
repeat 
direction rod represents guess principal component dataset 
energy stored springs reconstruction error trying minimize 
convergence complexity em learning algorithm pca amounts iterative procedure finding subspace spanned leading eigenvectors explicit computation sample covariance 
attractive small complexity limited iteration depends linearly dimensionality data number points 
methods explicitly compute sample covariance matrix complexities limited np methods snap shot method form linear combinations data compute diagonalize matrix possible inner products points limited complexity 
complexity scaling algorithm compared methods shown 
dimensionality random covariance matrix sigma generated points drawn sigma 
number floating point operations required find principal component recorded matlab flops function 
expected em algorithm scales favourably cases small large 
want eigenvectors methods 
standard convergence proofs em apply algorithm sure reach local maximum likelihood 
furthermore tipping bishop shown stable local extremum global maximum true principal subspace converges correct result 
possible concern number iterations required convergence may scale investigate question explicitly computed leading eigenvector synthetic datasets varying dimension recorded number iterations em algorithm required inner product current guess algorithm greater 
dimensions datapoints number iterations remains roughly constant mean 
ratios eigenvalues critical parameters controlling number iterations convergence example ratio 
complexity behaviour data dimensionality flops compute eigenvector eigenvalue slope slope slope snap shot method em algorithm sample covariance diag 
sample covariance convergence behaviour data dimensionality iterations em convergence time complexity convergence behaviour algorithm 
cases number datapoints times dimensionality left panel number floating point operations find leading eigenvector eigenvalue recorded 
em algorithm run exactly iterations 
cost shown diagonalization sample covariance uses matlab functions cov 
snap shot method show indicate scaling normally right hand panel convergence investigated explicitly computing leading eigenvector running em algorithm dot product guess true 
error bars show sigma standard deviation runs 
dashed line shows number iterations produce em algorithm curve left panel 
axis aligned covariance created eigenvalues drawn random uniform distribution positive range 
gamma points drawn dimensional zero mean spherical gaussian axes aligned space points 
missing data complete data setting values projections hidden states viewed missing information em 
step compute values projecting observed data current subspace 
minimizes model error observed data model parameters 
input points missing certain coordinate values easily estimate values fashion 
estimating value minimizes squared distance point reconstruction generalize step ffl generalized step possibly incomplete point find unique pair points lies current principal subspace lies subspace defined known information minimize norm gamma set corresponding column corresponding column complete exactly 
solution squares problem example qr factorization particular constraint matrix 
generalized step leading principal components datasets point missing coordinates 
sensible principal component analysis require multiple ffli identity matrix words covariance ellipsoid spherical take limit ffl model shall call sensible principal component analysis spca 
columns known principal components shown regular pca call scalar value ffl diagonal global noise level 
note spca uses pk gamma gamma free parameters model covariance 
inference done equation 
notice principal components spca pca mean posterior general point pca projection 
learning spca uses em algorithm 
finite noise level ffl spca defines proper generative model probability distribution data space gamma cc ffli delta possible generate data evaluate actual likelihood new test data spca model 
furthermore likelihood lower data far training set near principal subspace reconstruction error reported pca model 
em algorithm learning spca model ffl step fi cc ffli gamma fiy sigma ni gamma ffl step new sigma gamma ffl new trace xx gamma subtle points complexity important notice show learning spca enjoys complexity limited worse 
ffli diagonal inversion step performed efficiently matrix inversion lemma cc ffli gamma ffl gamma ffl gamma ffl 
second trace matrix step need compute full sample covariance xx compute variance coordinate 
relationships previous methods em algorithm pca derived probabilistic arguments closely related know sets algorithms 
power iteration methods solving matrix eigenvalue problems 
roughly speaking methods iteratively update eigenvector estimates repeated multiplication matrix diagonalized 
case pca explicitly forming sample covariance multiplying perform power iterations disastrous 
sample covariance fact sum outer products individual vectors multiply efficiently computing 
fact em algorithm exactly equivalent performing power iterations finding trick 
iterative methods partial squares algorithm doing trick regression 
singular value decomposition svd data matrix directly related way find principal subspace 
lanczos arnoldi methods compute svd resulting iterations similar em algorithm 
space prohibits detailed discussion sophisticated methods excellent general 
second class methods competitive learning methods finding principal subspace sanger oja rules 
methods enjoy storage time complexities em algorithm update steps reduce minimize cost typically need iterations require learning rate parameter set hand 
john hopfield fellow graduate students constant excellent feedback ideas 
particular am grateful erik significant contributions missing data portion dong provided image data try real problem carlos sanjoy mahajan 
zoubin ghahramani geoff hinton important motivation study 
chris bishop mike tipping pursuing independent unpublished virtually identical model 
comments anonymous reviewers visitors poster improved manuscript greatly 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society series 
everitt 
latent variable models 
chapman hill london 
zoubin ghahramani geoffrey hinton 
em algorithm mixtures factor analyzers 
technical report crg tr dept computer science university toronto feb 
zoubin ghahramani michael jordan 
supervised learning incomplete data em approach 
jack cowan gerald tesauro joshua alspector editors advances neural information processing systems volume pages 
morgan kaufmann 
gene golub charles van loan 
matrix computations 
johns hopkins university press baltimore md usa second edition 
sorensen yang 
arpack users guide solution large scale eigenvalue problems implicitly restarted arnoldi methods 
technical report www rice edu software arpack computational applied mathematics rice university october 
sirovich 
turbulence dynamics coherent structures 
quarterly applied mathematics 
michael tipping christopher bishop 
mixtures probabilistic principal component analyzers 
technical report ncrg neural computing research group aston university june 
michael tipping christopher bishop 
probabilistic principal component analysis 
technical report ncrg neural computing research group aston university september 
wilkinson 
algebraic eigenvalue problem 
press oxford england 
