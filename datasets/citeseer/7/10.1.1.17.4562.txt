self organizing multiple view representation objects shimon edelman weinshall center biological information processing dept brain cognitive sciences mit 
explore representation objects distinct views stored object 
demonstrate ability layer network thresholded summation units support representations 
unsupervised hebbian relaxation network learned recognize objects different viewpoints 
training process led emergence compact representations specific input views 
tested novel views objects network exhibited substantial generalization capa bility 
simulated psychophysical experiments network behavior qualitatively similar human subjects 
model object recognition involves definition comparison input image models different objects internal recognition system 
structure models depends information available input method comparing models images 
recognition methods lowe thompson mundy ullman avoid need recover depth input image rely models objects usually supplied independently range data hand coding 
psychophysical findings indicate human visual system tends represent familiar objects collections views single object centered descriptions tarr pinker edelman 
main difficulty faced computational recognition schemes representations infer appearance object novel viewpoint storing views 
algorithm level solutions offered ullman basri poggio edelman address problem implementation level constructing model human performance recognition subject constraints computational simplicity biological plausibility 
particular model relies unsupervised hebbian learning able generalize novel views extent subject tested stimuli generates turn testable predictions concerning human performance 
review psychophysical experiments results everyday objects readily recognized seen certain representative canonical viewpoints random viewpoints 
palmer 
canonical views commonplace objects reliably characterized criteria 
example asked form mental image object people usually imagine seen canonical perspective 
recognition canonical views identified quickly response times decreasing monotonically increasing subjective goodness 
dependency response time distance canonical view expected draws analogy recognition viewpoint normalization hand lowe ullman mental rotation shepard cooper 
existence canonical views may attributed tradeoff amount memory invested storing object representations amount time spent viewpoint normalization 
may preferred perspective exist familiar objects equally seen viewpoint 
evidence normalization effects recognition latency reflected existence preferred views disappear practice variety stimuli line drawings common objects random polygons larsen pseudo characters norman stick figures tarr pinker 
edelman 
investigated canonical views phenomenon novel wire frame objects looking effects object complexity familiarity variation response times error rates different views object 
results study indicate response times different views uniform practice subjects receive feedback correctness responses 
addition orderly dependency response time distance view characteristic canonical views phenomenon mental rotation tends disappear practice 
stimuli novel wire frame objects small nonzero thickness created displayed computer graphics system symbolics geometry environment 
objects created steps 
straight segment chain vertices 
second vertex displaced random amount distributed normally zero 
definition variance displacements determined complexity resulting wire 
third size resulting object scaled wires length 
novel objects generated procedure grouped average complexity sets served stimuli experiment 
evenly spaced images objects produced stepping increments latitude longitude 
basic experimental run objects complexity consisted blocks different object defined target recognition 
block phases training testing 
training phase preceded block tests subject shown views target twice natural succession target seen dimensional rotating space due kinetic depth effect 
testing phase subject static views shown time 
half views target fixed views spaced latitude longitude target 
half views rest objects current set 
subject asked determine view current target 
feedback correctness response 
experiment repeated sessions consisting blocks 
response time rt error rate er served measures recognition 
decrease mean rt brought subject increased proficiency task masked differential rt effects views coefficient variation rt different views defined ratio standard deviation rt mean rt measure prominence canonical views 
different perspective canonical views effect provided estimating dependency rt attitude object relative observer 
view yielded shortest rt object defined est view 
characterize rt function object attitude measuring dependency get view distance best view shown view 
regression analysis characterize rt er 
main findings experiment follows see figures 
stimulus complexity effect coefficient variation rt views little effect coefficient variation er 

stimulus familiarity reduced variation rt views 

initially rt particular view depended distance canonical view 
stimulus familiarity decreased dependency eventually making statistically significant 
possible interpretation findings terms theory recognition involves distinct stages normalization comparison ullman recognition alignment 
normalization stage image model brought common attitude visual buffer 
operation done process analogous mental rotation take time proportional attitude difference image model 
subsequently comparison 
time perform comparison depend object complexity attitude comparison stage contribute constant amount recognition time 
hand error rate recognition largely determined comparison stage 
practice views stimuli retained visual system resulting smaller average amount rotation necessary normalize input standard canonical appearance 
response times initially bad views determined normalization process decrease reducing variation rt views 
hand mean error rates bad views determined comparison process consequently variation er views change absence feedback subject 
rest demonstrate possibility alternative explanation experimental results edelman 
specifically show self organizing network model built provisions rotating arbitrary dimensional object representations may suffice account results 
constructing model testing experimental paradigm essentially stimuli projections vertices wire objects seen human subjects 
model structure structure network called clf conjunctions localized features appears 
input layer network feature map 
case features vertices wire frame objects local features edge elements suitable 
computer graphics system create wire frame objects marks vertex small square see 
isolate vertices thin image retaining object pixels neighbors 
side effect method crossings detected vertices 
unit feature layer connected units second representation layer 
initial strength unit decreases monotonically horizontal distance units inverse square law may considered approximation gaussian distribution 
simulations size layer units size layer units 
coordinates unit coordinates unit 
initial weight units point layer directly unit units representation layer interconnected lateral links initial strength zero 
connections form representations individual views object connections form associations different views object 
units may associated 
full connection matrix layer size 
operation training model sequence appearances object en coded locations concrete sensory features vertices list features 
presentation stimulus representation units active different strengths due initial gaussian distribution vertical connection strengths 
winner take employ simple winner take wta mechanism identify view input object active units subsequently recruited represent view 
wta mechanism works follows 
net activities units uniformly thresholded 
initially threshold high ensure activity layer suppressed 
threshold gradually decreased fixed multiplicative amount activity appears layer 
decrease rate threshold slow units remain active wta process 
implementation decrease rate 
cases winner emerged 
specifically flag set activity layer iteration tr global adjustable threshold net activity unit thresholded try 
increase likelihood obtaining single winner value learned smaller ratio activity second strongest unit eventual winner 
note wta obtained simple computation prefer stepwise algorithm natural interpretation biological terms 
interpretation requires postulating mechanisms operate parallel 
mechanism looks activity layer may thought high fan gate 
second mechanism performs uniform adjustable thresholding units similar global bias 
resemble feedback regulated global arousal networks thought limbic system brain kandel schwartz adjustment weights thresholds stage changes weights thresholds occur currently active units winners wta stage selectively responsive view input object 
enhancement connections active input units active units winners 
time thresholds active units raised presentation different input units respond recruited anew 
reason implement wta simple mechanism relaxation main functional requirement uniqueness winner 
existing wta algorithms koch ullman yuille approach require complicated arithmetics precisely weighted connections processing units 
advantages suggest increasing sophistication wta algorithms meet stringent functional requirements worthwhile revise theories incorporate wta models tolerate compromise wta performance 
employ hebbian relaxation enhance connections input layer active unit units specifically connection strength unit unit changes av rain cv aij 
aij unit wta upper bound connection strength parameter controlling rate convergence 
bounded hebbian relaxation rule weights updated correlation input output activities am aij activities ends link proportion current value weight correlation multiplied weight bounded 
threshold winner unit increased input specific result unit encodes spatial structure specific view responding selectively view presentations views association principle specific views object grouped temporal association new views object appear natural order corresponding succession arbitrary rotation object lateral connections representation layer modified time delay hebbian relaxation connection units represent successive views enhanced pro portion closeness peak activations time certain time difference ma aw am aj 
bounded hebbian relaxation weights correlation activities ends link different time instants weight bounded 
strength association views proportional coefficient am measures strength apparent motion effect ensue views succession human subject 
reason coefficient observation people tend perceive unfamiliar views belong object presentation induces apparent motion effect foster 
korte laws see ullman suggest am depend factors figural similarity views temporal proximity 
blurring followed correlation measure figural similarity views method appears biologically plausible finding perception dimensional structure motion human visual system appears compute minimal mapping ullman 
minimal mapping framework minimizing sum distances corresponding points equivalent maximizing correlation point sets suggested argument 
input pattern frame vat pattern frame motion sequence 
may recovered standard regularization looking ils ut smoothing operator see poggio 
assumed constant small patches image second term may dropped leaving ils ll patches covering image approximately constant 
reasonable assumptions equivalent 
cf 
mallot 
expression essentially maximal correlation frames 
signalling new object appearance new object explicitly signalled network different objects associated mechanism 
separation implicitly achieved forcing delay time units presentation different objects 
parameter decreases hi association stronger units activation closer time 
manner itt temporally associated view specific representations formed second layer object 
view specific representations form distributed multiple view representation object illustrates training sequence 
testing model subjected clf network simulated experiments modeled experi ments edelman 

novel wire frame objects low complexity set experiments served turn target 
task distinguish target non target objects 
network trained set projections target vertices evenly spaced viewpoints 
learning target hebbian relaxation described network tested sequence inputs half consisted familiar views target half views necessarily familiar objects 
presentation input layer activated units representation layer 
activation spread units connections see 
fixed number lateral activation cycles correlated resulting pattern activity footprints objects learned far 
object footprint yielded highest corre lation recognized definition 
experiment network recognized views session target previous targets rejected unfamiliar objects 
correlation measure closeness patterns 
choice may ified considering model decision making recognition units possibly different initial levels activation encode known entities unit entity cf 
morton ratcliff case units encode object 
input unit activation increased proportion similarity input concept unit represents 
decision threshold initially kept high minimize false alarms gradually decreased exceeded unit activation note similarity wta mechanism 
recognition latency scheme clearly depends activation induced input strongest represen tation unit 
scheme activation measured correlation actual footprint induced input prototypical memory trace footprint 
correlation serves analog response time 
representation scheme learning new view object amounts recruit merit new unit layer adjustment incoming connections threshold determine input specificity 
total initially available units little units necessary encode learned view objects network potential recognize correctly learned views 
recognition perfect views issue generalizing recognition novel views explored 
simulated psychophysical experiments recall analog response time simulations value correlation corr actual activation pattern layer ideal pattern recognized object 
able reproduce main results psychophysical experiments outlined section random initial choice parameters network model dependency coefficient variation corr views stimulus com plexity compare 
variation corr views significantly decreased practice compare 
analysis variance yielded dependence corr stimulus attitude diminished practice compare 
point involved computing regression coefficients corr distance shown view stimulus best highest corr view see section 
second order regression looked quadratic expression best approximated data 
real experiments revealed significant flattening regression curve practice 
simulated experiment difference sets regression coefficients corresponding sessions excluding intercept practically insignificant 
stage added enhancement lateral connections simul active units representation layer test phase simulated experiment enhancement training phase controlled equation 
winner take mechanism rarely came unit view 
result shortcuts lateral links spanning successive view object appeared footprints tended linear practice 
introducing shortcuts enhanced session effect increasing significance difference regression coefficients corr sessions variation corr stronger compare 
apparently ready session caused corr characteristics different views reach steady state values 
longer sessions flattening obvious see 
modeling variable association successive views simulated experiments described conducted apparent motion esti switched setting term am equation identically 
opportunity test apparent motion formulation correlation involved determin ing views association arose data subjects psychophysical experiments described section excluded final analysis reason 
subjects shown closely spaced views target object training phase views object subject trained mistake widely disparate views views object number testing stage 
significant dependency response time distance best view subject session 
replicate finding compared dependency corr performance mea sure model distance best view conditions 
control condition network trained views object tested views human subjects 
apparent motion condition views training testing 
expected dependency corr distance best view stronger control condition apparently influence am term equation accordance human performance analogous circumstances 
subject reported saw apparent motion training views 
save computation time simulated experiments far network exposed views training testing phases 
regression corr distance best view control condition regression apparent motion condition generalization novel views utility recognition scheme multiple view representation depends ability classify correctly novel views familiar objects 
assess generalization ability clf network tested views obtained rotating objects away learned views see 
classification rate better chance entire range rotation 
rotations close perfect decreasing chance level objects 
may compare result rock finding people difficulties recognizing imagining wire frame objects novel orientation differs familiar 
smoothness connections suffice network small deformations input objects caused shift viewpoint noise updating thresholds 
raising thresh olds implies training exact replica original input activate recruited unit 
partial solution difficulty provided observation units originally activated certain view object activated novel view chance simply raising input level turn correct unit committed unit 
uncommitted units situated periphery layer remained inactive provided decrease connection strength horizontal displacement larger increase input activity needed push correct unit threshold 
observation modified winner take mechanism follows 
learning winner units identified 
testing hand required total activity winner units exceed threshold equal fraction specifically long term running average activity layer 
wta step unit satisfied threshold requirement input activity layer boosted multiplied wta process repeated units activity exceeded threshold 
process correct unit cross threshold provided input sufficiently similar preferred pattern see 
connections smooth sense 
active unit causes activity layer peak shifting input small causes peak layer move small 
providing solution generalization problem biologically plausible framework solution generalization problem partial requires actual overlap positions features belonging novel view belong known views object 
boosting input enables network perform autoassociation activate representation view partial information position features 
blurring input prior application layer significantly extend model generalization ability 
performing autoassociation dot pattern blurred gaussian computationally equivalent finding th committed unit gives max iix vj number features points vertices input pattern coordinates layer coordinates layer th feature contributes th unit activity th feature detector layer weight connection fth feature th object unit cf 

width blurring gaussian small compared average distance ad change may rewritten max may considered correlation input set templates real ized gaussian receptive fields see 
turn appears related interpolation radial basis functions poggio girosi poggio edelman 
discussion notion visual objects represented conjunctions coincidences spatially localized feature occurrences traced far back mcculloch 
detection spatiotemporal coincidences proposed repeatedly general model brain function barlow damasio 
association modification wta mechanism require additional piece information 
network told current input pattern learned case layer activity artificially boosted pattern classified 
central characteristic clf model encodes object views coincidences organized features constructs complete object representations view specific representations cf 
perrett linking views natural order appearance object rotation 
discuss model details standpoint biological plausibility 
hebbian synapses correlation unsupervised learning adaptive system autonomous rely learning coincidence detecting correlation operations 
clf model incorporates correlation levels 
level weight adjustment correlation appears form hebbian rule equation see mcnaughton morris 
higher level correlation successive views object serves determine figural similarity strength association established representations layer 
model classifies unknown view choosing template familiar view maximally correlated input 
learning selective reinforcement clf model input layer fully connected representation layer 
reason model satisfies trivially availability requirement posed section input configuration units exists unit connected represent occurrence 
clf model learns represent recognize object selective reinforcement existing structures creating novel structures 
selection paradigm major structures case distinct input representation areas specified design details emerge self organizing fashion 
neurobiological support selection view learning may edelman finkel 
unit reinforced role wta clf model previously suggested learning schemes fukushima representation unit reinforced selected winner take process 
clf model flexible assume prior classification input features 
result different patterns may cause unit winner provided projections centroids layer coincide 
additional mechanism selective raising units thresholds necessary enhance representation selectivity 
lateral connections clf network differs layered models compute progressively complex topographic maps input reliance long range lateral connections representation layer 
perceptual phenomena modeled continuous maps topological proximity major consideration potentially holistic global phenomena recognition require conceptual proximity substituted topological yon der malsburg singer 
relatively long range lateral connections appear exist cortex may responsible nonlocal phenomena nonclassical receptive fields gilbert 
predictions clf scheme considered model human faculty object recognition gen specific predictions tested experimentally 
predicts people exhibit limited generalization capability novel views differ familiar ones 
psychophysical results date rock edelman appear support prediction 
second model predicts limited capability mental rotation outside range familiar views time dependence mental rotation effects range presentation sequence training 
third prediction arises reliance clf model localized features sensitive position object visual field occlusion 
restriction circumvented parallel recognition modules different feature object 
result model predicts subjects recognition performance depend fixation patterns training testing phases 
summary described layer network thresholded summation units capable developing multiple view representations objects unsupervised fashion fast hebbian learning 
simulated psychophysical experiments investigated phe canonical views mental rotation model performance closely paralleled human subjects model provisions rotating object representations fact employ representations 
indicates findings usually taken signify mental rotation may alternative tion 
footprints chains representation units created association training formed representation layer model provide hint substrate mental rotation phenomena may look 
time similarity model performance generalizing recognition novel views relevant psychophysical data supports notion recog nition tasks human visual system relies blurred template matching equivalently nonlinear view interpolation 
acknowledge nt poggio ullman useful discussions suggestions 
support research provided darpa onr cognitive neural sciences division sloan foundation 
se dw supported weizmann postdoctoral fellowships weizmann institute science 
barlow hb cerebral cortex model builder 
rose dobson editors visual cortex pp 
wiley new york damasio ar brain binds entities events activation convergence zones 
neural computation edelman gm finkel neuronal group selection cerebral cortex 
edelman gall cowan editors dynamical neocortical function pp 
wiley new york edelman hh weinshall stimulus familiarity determines recogni tion strategy novel objects 
memo ai lab mit edelman hh viewpoint specific representations object recog nition 
memo ai lab mit foster dh hypothesis connecting visual pattern recognition apparent motion 
kybernetik fukushima neocognitron hierarchical neural network capable visual pattern recognition 
neural gilbert cd neuronal synaptic organization cortex 
singer editors pp 
wiley new york time name disoriented objects 
memory cognition kandel er schwartz jh pr science 
elsevier new york koch ullman selecting simple network imple shifts selective visual attention 
human norman mental rotation visual familiarity 
ception larsen pattern matching effects size ratio angular difference orientation familiarity 
ception lowe dg ceptual visual recognition 
kluwer academic publishers boston mallot ha hh little jj neural architecture optical flow compu tation 
memo ai lab mit mcculloch ws brain behavior 
halstead wc ed comparative pay monograph vol pp 
calif press berkeley ca mcnaughton bl morris hippocampal synaptic enhancement infor mation storage distributed memory system 
mm jenkins wm tt rj cortical representation plasticity 
singer ed te pp 
wiley new york morton interaction information word recognition 
review palmer se rosch chase canonical perspective perception objects 
long baddeley ed attention ix pp 
erlbaum hillsdale nj perrett di aj aj visual neurones responsive faces 
trends neurosciences poggio edelman network learns recognize dimensional objects 
nature poggio girosi regularization algorithms learning equivalent multilayer networks 
science poggio torre koch computational vision regularization theory 
nature ratcliff parallel processing mechanisms processing organized information human memory 
anderson ja hinton ge ed parallel models associative 
erlbaum hillsdale nj rock case viewer centered object perception 
cognitive pay rock wheeler imagine objects look viewpoints 
cognitive ps shepard rn cooper la mental images transformations 
mit press cambridge ma tart pinker mental rotation orientation dependence shape recog nition 
cognitive ps thompson dw mundy jl dimensional model matching uncon strained viewpoint 
proceedings ieee conference robotics automation pp raleigh nc ullman interpretation visual motion 
mit press cambridge ma ullman aligning pictorial descriptions approach object recognition 
cognition ullman basri recognition linear combinations models 
menlo ai lab mit von der malsburg singer principles cortical network organization 
singer ed pp 
wiley new york yuille nm winner take mechanism presynaptic inhibition feedback 
neural legends examples wire objects 
shaded grey scale images similar wires stimuli experiments 
human subjects effects complexity familiarity 
coefficient variation rt views rs 
session complexity dot square triangle mark low middle high complexity respectively 
rt decreased session low medium high complexity groups 
effect session significant 
human subjects effect familiarity 
regression curves rt sec distance shown view best view dcg session 
difference regression curves sessions barely significant 
experiment sessions consisted exposures view object respectively 
apparently exposure level produce visible effect dependency rt cf 

human subjects effect familiarity 
regression curves rt ec distance shown view best view dcg session 
regression session session flatter curve highly significant 
experiment session consisted exposures view object 
error bars denote twice standard error mean corresponding points 
flattening curve signifies dependency rt interpreted weakening phenomenon related mental rotation see text 
network consists layers input feature layer representation layer 
small part projections shown 
network encodes input patterns making units layer respond selectively conjunctions features localized layer 
curve connecting representations different views object layer symbolizes association builds views result practice 
wire frame object model 
actual input network derived thinning operation 
note crossing segments original object detected vertices 
typically vertices detected 
snapshots activation patterns network different stages operations views object 
left right input array layer thresholding layer thresholding wta layer wta 
adjustment connections leftmost panel bottom row units activity visibly 
units previously recruited represent different view object active rest layer thresholding bottom row third panel left suppressed leaving black holes true distribution activity apparent 
note blurred version input shape 
wta rightmost panels remains usually just active unit 
winner may emerge happened second row 
left activation pattern layer produced object network trained objects 
right remembered ideal footprint object 
coefficient variation corr views sessions complexity shortcuts footprint see text 
compare 
regression corr distance best view session shortcuts footprint see text 
compare keeping mind high corr analogous low rt 
coefficient variation corr views sessions complexity shortcuts footprint see text 
regression corr distance best view session intro duction shortcuts footprint see text 
compare 
regression corr distance best view session tion shortcuts footprint exposures view session see text 
exposures necessary achieve disappearance dependency corr compare 
performance network novel orientations familiar objects mean objects bars denote variance 
broken line shows performance wta step implemented program simply chooses strongest unit fixed boost factor see text 
solid line shows performance iterative wta scheme adaptive boost factor 
recognition novel view vertex object clf network 
gaussian templates familiar views represented schematically hats centered units ti 
centers set vertex templates shown tik 
recognized view represented unit xv locations vertex distorted input recognized view 
zo address correspondence dr shimon applied institute science rehovot israel mail wisdom il distance best view distance best view input feature layer representa tion tion representation layer fo object view vi 
session 





distance 
session distance 




distance rotation away learned attitude tn 
