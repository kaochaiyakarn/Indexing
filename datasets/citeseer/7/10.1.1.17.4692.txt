parc extension shared memory parallel processing ben asher department mathematics computer science haifa university haifa israel dror feitelson ibm watson research center box yorktown heights ny larry rudolph institute computer science hebrew university jerusalem jerusalem israel summary parc extension programming language block oriented parallel constructs allow programmer express ne grain parallelism shared memory model 
suitable expression parallel shared memory algorithms conducive parallelization sequential programs 
addition performance enhancing transformations applied language resorting low level programming 
language includes closed constructs create parallelism instructions cause termination parallel activities enforce synchronization 
parallel constructs de ne scope shared variables delimit sets activities uenced termination synchronization instructions 
semantics parallelism discussed especially relating discrepancy limited number physical processors potentially larger number parallel activities program 
keywords parc language parallel programming semantics parallelism forced termination shared memory 
supported part fellowship ministry science technology israel supported part france israel bsf part quest parallel programming everyday activity developed easy quick learn parallel programming language referred parc 
parc block oriented shared memory parallel version popular programming language 
parc presents programmer model machine processors executing parallel access private shared variables 
parc constructs proposed best knowledge rst time brought cohesive form 
result language allows parallelism expressed natural simple manner 
programming languages designed support shared memory model computation making programming easier allowing users tap vast base pram algorithms 
addition users capitalize experience concentrating rst writing correct sequential program ecient parallelization parc extensions 
describe features semantics parc 
rest section explains motivation designing new language ect motivating forces design structure software environment surrounds 
section describes parallel constructs scoping rules 
exact semantics parallel constructs activities processors widely neglected literature 
discuss issue provide guidelines acceptable implementations 
describe innovative instructions forced termination analogies instructions break construct followed discussion synchronization mechanisms 
discussion programming methodology parc followed discussion experiences parc comparison parc parallel programming languages delayed described features 
motivation design guidelines parallel programming languages language extensions proposed implemented years 
languages di erent philosophies provide wide spectrum features 
parc belongs family provide explicit parallelism opposed functional logic languages 
addition conservative revolutionary data ow known widely language making ideal wide acceptance programming community 
supports shared memory model computation widely believed ease parallel programming enhance user productivity 
basic characteristics re ect belief language best vehicle research parallel algorithms implementation parallel architectures 
practical usefulness parc stems sources 
hand similar avor pseudo languages theoreticians describing parallel algorithms 
shared memory parallel algorithms directly coded parc 
hand conducive parallelization sequential programs written language promotes understanding problems inherent parallel code parallelization process allows various approaches solution expressed compared 
main feature parc parallelism incorporated block structure language 
encourages structured program design provides high level description program structure furnishing natural extension sequential control structures 
language intended completely general sense parallel structure expressed 
contrary structure means restricting programmer allowing constructs closed loops general goto instruction 
generalization idea parallelism results closed parallel constructs fork join 
addition structured language may include redundancy number similar constructs useful example provides di erent iterative constructs rst easily expressed third 
parc redundant parallel constructs redundant synchronization mechanisms 
parallelism expressed parc high level abstraction 
programmer need know exact number processors relative speeds processors processor executing parc code precompilation code compilation object code library maxi run time library linking executable multiprocessor simulator tasking system extension libraries network caching bn debugging monitoring library linking executable pc workstation parc software system 
activity 
results portable code frees programmer worrying low level details allowing concentrate getting parallelism correct 
hand applications resulting code may ecient 
cases may desirable hand tune code machine 
purpose parc supplies concise high level facilities express program mapped run time 
possible improve performance restructuring program changing relationship parallel sequential constructs 
note done context language need additional low level primitives 
tuning usually done application fully developed performance analysis shown problem spots 
system structure purpose parc system facilitate research design implementation parallel algorithms time provide testbed research parallel programming debugging run time systems context shared memory model 
environment composed parts precompiler translates parc code simulator simulates parallel execution unix workstation ibm compatible pc run time library supports execution parc programs research multiprocessor see 
parallel program represented single unix dos process little signi cance programmer 
process oriented unix system calls may parallel program 
unix dos available programmer program start operations 
operations serialized le descriptors shared parallel activities 
true simulator 
software layer system called maxi short runs intel kernel 
implementation parc constructs done partly code transformations precompiler partly simulator maxi library 
noted precompiler compiler preprocessor 
parallel extensions parc language extensions merely high level calls system functions 
precompiler recognize language keep symbol table order support scoping rules features 
simulator may linked libraries extend functionality 
set libraries emulates various hardware features certain network topology caching scheme 
libraries belong di erent versions parallel debugger 
libraries contain routines monitor program execution provide data graphical display facility 
additions simulator gives detailed performance prediction counting assembler instructions 
research multiprocessor contains single board computers connected ii 
board intel processor running mhz mathematical coprocessor mb memory 
processor access memory board bus takes longer local access 
non uniform memory access numa architecture 
additional elements bus include unix host terminal server peripherals interface card 
noted parc depend architecture 
fairly cost ective assemble small scale parallel machine single board computers connected shared bus 
instance architecture 
shared memory accessed crossbar multistage network 
parc assumptions architecture just requires ability share address space processors 
creating parallelism activities shared variables section describes essence parc parallel constructs scoping rules sharing variables special constructs related parallel processing 
parc superset programming language need review standard features 
parallel constructs parallel programs main sources parallelism loops recursion 
parallel constructs parc especially designed handle cases ecient structured manner 
parallel things generated parc called activities order avoid overloaded terms process task 
loop parallelism handled construct parallel version conventional loop 
syntax construct index stmt body loop stmt legal statement parc including compound statement possible nested parallel constructs function calls 
block compound statement declare private variables won known outside scope block iterations block elaborated 
bodies distinct iterates executed parallel meaning executed parallel discussed section entitled meaning parallelism 
distinct local copy loop index created iterate integer variable 
index variable modi ed ecting activities 
analogy serial loops number activities 
index variable activity initialized note variable de ned block exist outside scope 
loop expressions evaluated activities spawned 
illegal directly transfer control goto statement body construct 
lparfor lparfor construct important common case number iterates parallel loop larger number processors iterates independent data dependencies ecient chunk create activities processor 
chunked activities execute iterates serial loop total number iterates 
saves overhead associated spawning activities improves possibility ne grain parallelism 
parc special construct implements optimization called lparfor pre denoting lightweight 
lparfor gives compiler opportunity generate ecient code 
note construct add functionality useful optimizations 
implementation choose balance load activities statically allocating iterates dynamically chunked self scheduling 
optimization change semantics 
decided add special construct language just adding hint compiler commercial parallel languages semantic implications 
lparfor explicitly implies iterations independent 
correct program constructs may correct lparfor 
reverse true may replace lparfor ecting correctness just eciency 
parallelism recursion divide conquer algorithms handled construct stmt list stmt list general blocks possible 
construct general specialized construct divide conquer constituent activities coded individually 
note parallel constructs redundant lparfor may implemented example 
implementations bulky degrade code quality 
mapped constructs parallel constructs quali ed mapped keyword 
ect mapping activities physical processors way 
exact mapping implementation dependent reasonable choice round robin activity index mapping activity processor mod key feature instantiation mapped parallel construct th activity mapped processor 
activities locality relationship 
advanced programmers feature produce ecient code machines types 
gives programmer ability control allocation activities processors burden keeping track process ids explicit mapping 
helps code readable avoiding sync statements 
current implementation mapped lparfor construct supported 
nesting parallel constructs di erent parallel constructs may nested arbitrary ways mixed regular constructs 
cases activity executes parallel construct suspended constituent activities terminated 
nesting parallel constructs creates tree activities leaves executing internal nodes wait descendents 
note constructs allow user express spawning large number activities avoiding messy implementation mapping details 
details delegated runtime system 
general gotos allowed past boundaries parallel constructs 
restriction parc compatible int sorted void quicksort arr arr left right int splitter right left sorted left arr left return splitter arr left left right left arr splitter arr arr arr arr sorted splitter quicksort arr arr left quicksort arr arr right int sorted shared array void quicksort arr arr left right int splitter declaration right left sorted left arr left return splitter arr left left right lparfor left right arr splitter arr faa arr arr faa arr sorted splitter quicksort arr arr left quicksort arr arr right sequential parallelized versions quicksort 
faa atomic fetch add operation see section synchronization 
note parallel version shared activities lparfor recursive call create new copies copy array sorted shared activities 
example parallel quicksort example consider parallelization program implements quicksort algorithm left 
arrays alternately copy elements smaller larger rst element segment 
rst element copied output array location set smaller values set larger values 
recursion ends segment empty includes single element single element copied output array 
parallel version parc right 
lparfor compare elements splitting value parallel dividing larger smaller 
implies parallel access indices atomic fetch add faa instruction increment decrement 
perform recursive calls parallel 
scoping rules important feature programming language scoping rules 
parallel languages scoping replaced explicit declarations variables private shared 
nd explicit declarations unnecessary scoping naturally leads rich selection sharing patterns 
parc real parallel language just sequential language calls parallel run time support system precompiler understands program structure 
static int shared activities int private activity faa updated atomically private variables need protected variable declared static construct shared activities construct 
sharing patterns nested blocks scoping rules language come avors local procedure global 
procedure hierarchical scoping variables block code may contain data declaration section 
reminiscent hierarchical scoping pascal 
scope global variables limited just procedures le static statement 
parc allows parallel constructs freely nested inside parallel constructions 
preserving normal language scoping rules variables declared block matter block parallel sequential accessed nested statements parallel sequential 
particular nested statement parallel construct spawned activities share variables declared enclosing blocks 
global variables shared program activities static variables considered global 
general rule see 
activity see variable variable shared private 
variable parc program potentially shared 
private variable shared shared variable points 
local variables declared block allocated activity stack 
scoping rules imply logical cactus stack structure nested parallel constructs 
note case nested parallel constructs procedure 
code procedure access variables local procedure called passed arguments 
modify passed 
example hierarchical scoping rules parc 
index variable lparfor local activity speci cally right left distinct copies 
variables shared protected modi cation 
done atomic faa fetch add operation 
static declarations language designed early context writing unix operating system pdp 
remarkable features parc parallel machines ill ects 
major exception static declaration 
static variables persistent function invocations 
implementation allocate storage variables global heap space stack 
parc static variables declared parallel block shared activities private declaring block see 
forced termination parc activities logical structures implicitly speci ed parallel constructs consequently identi ed names ids 
impossible activity directly kill terminate activity 
activities parc generated block structured expressions parc allow limited control execution activities construct 
possible activity self destruct kill group related activities jumping parallel construct 
done sample code main void snapshot activity tree pb pf ect di erent instructions executed activity activity moment return pb pf pb return pb pf ect return issued parallel construct 
parc instructions semantics executed inside parallel construct analogies serial constructs deletes activity executes ect siblings 
analogous jump activity block code 
terminates activity siblings descendents ectively generated 
breaks construct analogous jump rst instruction construct 
return returns function call terminating activities generated function 
search arr int arr lparfor arr return return example return construct lparfor construct 
match activities terminated 
likewise setjmp longjmp allow return previous state number function calls 
pre required instructions unambiguously sequential iterative construct nested inside parallel construct 
example clari es ect highlights di erences instructions 
note subtree terminated root activity may resume constituent activities terminated 
start asynchronous mechanism cause subtree terminate extreme case just execute termination parallel root activities subtree access local variables root 
implied machine interrupted subtree terminated fast possible prevent situations new activities generated higher rate existing ones terminated 
code shows return statement construct code fragment parallel search elements large array 
match search terminated return instruction returns index matching cell 
lparfor construct reduce overhead size array may larger activities spawned loop elements 
synchronization closed parallel constructs sucient synchronization needs parallel programming 
wide spread agreement constitutes satisfactory high level synchronization mechanisms parallel languages 
researchers emphasize data abstraction mutual exclusion advocate monitors similar constructs 
emphasize relationship synchronization communication suggest mechanisms message passing ada rendezvous 
third group suggests events main mechanism process synchronization 
approach adopted parc choose speci high level primitive 
number lowlevel synchronization primitives di erent characteristics provided 
allows programmer create various synchronization schemes control program behavior resulting performance 
certainly possible high level primitives added language programming experience indicates useful large class applications 
basic synchronization mechanisms parc fetch add denoted faa exp atomic read modify write operation integers 
shown useful large number algorithms 
speci cally faa implement wait free interactions number activities operate shared data structures simultaneously having wait 
addition various synchronization schemes busy waiting 
example appears quicksort program 
faa allocate distinct cells array arr di erent activities increment index variables time 
atomic nature instruction alleviates need mutual exclusion prevents unnecessary serialization 
int int tmp init val sync tmp sync tmp sync int tmp lparfor init val lparfor tmp lparfor tmp examples initialization private variables 
rst sync statement embedded loop 
second parallel constructs clearly show sequentiality 
lparfor ecient large values requires temporaries de ned shared arrays 
lparfor left sync creates dependency iterations 
semaphores complement wait free capability faa providing interface activity suspend waiting event 
variables type semaphore may declared operations may applied usual semantics 
evident barrier synchronization useful primitive parallel programming 
added sync instruction parc 
instruction implements barrier involves activities spawned certain parallel construct 
activity executes sync suspended live siblings execute sync 
subset activities perform sync instruction loop forever wait semaphore deadlock ensue 
subset activities terminate remaining activities perform sync 
particular activities terminate activities waiting sync may proceed 
noted sync instruction redundant sense implemented semaphores 
common appropriate provide special instruction relieve users implementation details 
common ensure variables shared initialized accessed left 
example array initialized cell relaxed 
alternatively done separate parallel constructs emphasizing sequential nature operation right 
separate constructs requires shared arrays temporary values maintained constructs 
note passing doubtful machine support naive implementation code left gure construct large values require large number activities maintained 
additional instructions allow users uence execution activities processors bypassing high level abstraction provided language constructs 
example instructions allow programmer achieve ects similar routines 
yield activity executing instruction explicitly yields processor causing context switch ready activity 
switch switch allows user create activities 
change semantics parallel constructs de ned 
reasons option discussed section entitled meaning parallelism 
activity may preempted forced wait event 
happens activity performs sync instruction semaphore yield blocking command spawns additional activities 
note instruction guarantee mutual exclusion ect activities processors 
example illustrating instructions consider global time stamp mechanism useful program monitoring debugging 
idea initially create activities non preemptable activity accomplished switch construct increments clock variable nite loop regular activity starts parallel program 
interesting event occurs labeled current clock value 
provides full ordering events occur program execution 
parallel program terminates clock activity 
example yield instruction implement phase blocking mechanism ecient busy waiting 
implementation issues parc parallel programming language hopefully allow programmers get high performance parallel computers 
therefor important programmer idea parc implements constructs data structures get mapped 
hand goal easy language implies programmer need know lot implementation underlying machine 
section try give appropriate amount information meet con icting goals 
similarly important programmer understand meaning parallelism provided parc language gives programmer control activities executed 
explicit details runtime operating system needed 
mapping activities data structures architectures non uniform memory access important large part memory directed local memory 
programmer means coordinate mapping activities processors mapping data structures memory modules 
parc done scoping rules mapped lparfor construct memory allocation procedures 
threaded stack variables data structures declared activity known activities descendents declaring activity 
reasonable require variables reside memory module close processor runs activity 
variables allocated activity stack implies stack allocated memory module adjacent processor runs activity 
run time migration stack moved part activity context 
stack retain virtual address means page tables modi ed 
scoping rules implemented parc precompiler changing variables declared surrounding blocks indirect 
run time system chain stacks explicitly create full edged cactus stack search stack nd referenced variables 
direct pointers variables location typically activity stack available 
call resulting structure stacks mutual pointers threaded stack 
example 
activities share variable declared 
activity transformed precompiler procedure gets pointer argument 
function names arguments passed parameters function 
function part run time library 
called creates new activities di erent processors 
activity complete call stack 
activation record activity function constructed stack appropriate arguments copied 
activity commence behaves function called remote processor 
original code main int int int xxxxx precompilation main int int int int int run time parent activity stack frame main frame copy arguments activity stack pi frame activity stack frame implementation scoping rules threaded stacks 
persistent partitioned data structures dataparallel programming languages de ciency specifying control structure processor computes 
likewise control parallelism languages parc instance de cient expressing data distribution 
applications declaring local variables 
numerous parallel algorithms call large shared data structures partitioned set activities 
means activity owns part data structure computation relates part 
activities access parts belonging activities 
example parallel image processing application may partition large image blocks 
separate activity responsible block access neighboring blocks necessary 
problem local variables exist lifespan activity declared 
possible set activities deposit data set local data structures set activities subsequently retrieve data locality properties 
need ability partition data structures persistent pattern 
parc programmer overcome problems mapped lparfor construct malloc system call 
explained lparfor construct creates activities mapped variant corresponding activities distinct constructs mapped processor 
user wants create exactly activity processor prede ned global variable proc may 
persistent dimensional shared array partitioned processors may created shown code right 
elements referenced dimensional array ts notion vector pointers vectors 
global array pointers declared 
set activities spawned mapped lparfor 
activity allocates persistent local memory malloc function de nition parc guarantees malloc local heap 
addresses local memory blocks assigned global array 
recall heap local accessible activities 
note code works number processors mapped construct performs mapping activity processor 
implementation described optimal fact array access involves non local access global pointer array 
easily xed copying pointers local memory 
currently experimenting implementations partitioned data structures plan de ne new language constructs easier results 
meaning parallelism parc constructs identify activities may run parallel precise de nition required order fully de ne semantics giving indication performance characteristics constructs 
phrase execute parallel di erent ways parallel languages leave exact semantics implementation obtaining ad hoc operational de nition 
feel issue far important ignored 
ambiguities arise fact number physical processors machine limited fact mimd mode processors may execute varying speeds 
resolve characterize way parc activities scheduled 
de ne implementations give legal operational semantics 
problem especially severe shared memory model assumed parc 
computations functional languages example create tree activities 
due fact functional languages prohibit side ects interactions activities 
scheduling decisions may ect performance ect outcome computation 
computations explicit message passing run time system know process waiting 
shared memory hand interactions mediated side ects 
run time system know busy waiting process doing useful 
programs avoid undesired scenarios synchronization mechanisms supplied system regulate interactions 
important de ne happen synchronization omitted implemented programmer 
fairly common parallel machines adhere dictates sequential consistency order facilitate understanding parallel programs 
requirement states results parallel execution program results obtained instructions distinct shared memory stored stored shared memory stored local memories array declared inecient int mapped lparfor initialize mapped lparfor int owned array row jj partitioned array ecient int mapped lparfor int malloc sizeof int initialize mapped lparfor int owned array row jj left side gure illustrates usual dimensional array shared memory 
right side array partitioned processors memories 
achieved shown parc code 
elements referenced dimensional array ts notion vector pointers vectors 
mapped version lparfor ensures indexed activities mapped physical processor 
parallel activities interleaved executed serial ordering 
instructions execute parallel results executed ideal semantics parallel construct completely de ne interleaving 
lead deterministic execution shared memory parallel programs 
example require interleaving emulate priority crcw pram done executing instruction time activity cycle activities ordered lexicographically 
regrettably practical require semantics 
overhead involved enforcing far outweigh bene ts parallelism 
problem interleaving completely de ned program execution indeterminate 
consequence distinct executions program may lead di erent results di erent behaviors 
example execution may spawn activities execution may terminate result enters nite loop 
impossible specify exact semantics parc programs 
absence formal semantics speci cation set int example extreme case critical programmer understand underlying execution model 
nonpreemptive system code terminate 
rules guide implementation parc system 
noted manner interleaving instructions di erent activities may ect program termination addition obvious ect access shared variables 
implementation rules attempt reduce probability adverse ects deadlock non termination 
try intuitive understanding executing parallel means hopefully reducing chance unpleasant surprises 
show equivalent requirement fairness 
preemption executing activities multiprocessor processors activities execute time 
pool ready activities expected larger implementation rules de ne order activities join leave set currently executing activities 
options removal activity executing set voluntary letting run suspends requests removed explicitly terminates ii preemptive forcing relinquish processor certain time quantum 
preemption ensures degree fairness activities 
equivalent situation activities execute time asynchronous processors 
guaranteed delayed inde nitely proceed 
arguments preemption preemption user perceive activity virtual processor able create interactions interdependencies virtual processors 
extreme example code segment preemption code deadlock 
note software algorithms mutual exclusion 
correctness termination depend number processors available performance may ected 
user forced take account 
goal shift burden programmer compiler system 
example code segment ecient gang scheduling 
naive programmers expect system guarantees activities run completion 
example lack preemption guarantee mutual exclusion parallel systems 
minor changes adding print statements debugging cause activities preempted unexpectedly surprising users 
ects lead designers amoeba comment probably worst mistake design amoeba process management mechanisms decision threads run completion preemptable 
preemption int main void int int main void int examples codes support breadth rst execution left depth rst execution right 
case execution method lead explosion activities 
users knows going full control system 
system surprise user suddenly preempting task scheduling user able analyze performance program 
model asynchronous programmer explicit synchronization regulate execution 
code explicit synchronization need supported formal notion correctness 
preemption introduces additional overhead due context switching degrades cache performance 
preemption provide environment naive user annoy parallel programmer 
implementation rules parc compromise 
default rule preemption 
sophisticated users want full control may override rule yield switch switch instructions described section synchronization 
activity selection second question order activities join executing set 
number activities exceeds activities wait start execution 
pool waiting activities organized fifo queue lifo stack 
viewing tree activities generated choice determine tree traversed breadth rst depth rst order implications order execution subtle 
consider code segments shown 
argument breadth rst ordering code left 
code deadlock despite preemption depth rst ordering new activities generated time prevent execution activity terminate program 
scenario probably cause system failure due nite recursion 
argument depth rst ordering note abuse terminology branches tree traversed parallel case 
problems dealing deep recursion 
example code right require activities started done breadth rst done depth rst 
case breadth rst probably lead system failure 
examples lead ect generation huge numbers activities di erence 
code right looking trouble explicitly asking creation activities depth rst system optimization get trouble despite 
code left hand perfectly reasonable natural extension notion fairness motivates preemption 
extension requires fairness maintained independent activities branches tree activities 
words activity charged execution execution descendents 
certain activity descendents execute long branch preempted giving branches chance execute 
breadth rst approach advocated parc 
note fairness advocated practical matter theoretical issue 
extend done fairness nondeterministic systems csp claim important systems regrettably fairness impossible de ne semantics shared memory program 
cases fairness improve program behavior ease task parallel programming 
experience parc parc years hebrew university served vehicle annual course parallel algorithms 
section review experience strengths weaknesses language 
parc lived charter providing platform undergraduate students gain experience parallel programming 
parallel algorithms course successfully trained graduate undergraduate computer science students 
main exercises parallel programming usually accomplished identifying intensive inner loops application replace loop construct 
simple modi cation rarely leads signi cant speedups reveals challenges parallel processing 
immediately faced problem collecting partial results iteration 
faa primitive task easier 
example conjugate gradient method programmed achieved speedup processors simple method 
large variance speedup due sparsity input matrix di erent convergence speeds parallel sequential versions 
detailed examples parc simple quickly get working program reasonable performance rich allow knowledgeable programmer squeeze additional performance 
examples show performance enhancing transformations may applied parallel program 
main idea start simple parallelization re ne get better performance 
rst example simple parallelization possible parc gives performance extensive additional optimizations accrue limited bene ts 
second example simple parallelization additional needed improve memory locality 
cases easy express optimizations features parc 
int lg add simple ways sum numbers parc nal sum contained 
iterates independent lparfor construct 
int int private due scoping rules lg terminate top half add sync explicit barrier iteration optimization avoid explicit generation termination activities lg iterations 
systems unable maintain large number waiting activities 
vector summation example consider calculation sum elements array 
simplicity assume size array number processors powers basic parallel algorithm sum element pairs pairs pairs resulting tree structure 
easier express algorithm pairs taken adjacent elements elements half array apart 
code parc version procedure relaxed notation exponentiation 
formulation convenient includes implicit synchronization iteration new spawned 
costs overhead associated higher overhead synchronization 
lparfor construct reduce overhead 
potential transformation avoid creation termination activities iteration 
achieved reorganizing code putting sequential loop inside adding explicit synchronization iteration 
version shown 
note lparfor sync introduces dependency activities 
number activities halved iteration sync instruction knows number participating activities 
cases array larger number processors lparfor construct advisable chunk computation hand 
done spawning activities take care number values 
large values code execute activities system maintain 
code implements chunking optimization 
original lg iterations nondeterminism occurs certain language construct allows arbitrary choice number options 
constructs exist message passing languages select statement ada alt construct occam 
choices explicit program behavior change moment due di erences execution rates di erent processes 
may happen shared memory programs synchronize accesses shared variables 
parc provides number processors variable named proc 
short 
int proc number actual processors int private due scoping rules lg terminate top half lg iterations loop private chunk add lg shift lower part array rst phase sync explicit barrier iteration nal hand optimized code 
slightly better rst lparfor version 
time speedup number pes sequential sum sum lparfor sum table result adding numbers expensive add operation compensate memory con icts 
rst line time simple loop 
lines labeled sum code sum code 
split phases 
rst lg lg iterations activities array elements 
activity loops array elements assigned loop index 
assignment changes iteration partial sums created lower half array 
nal lg iterations number activities halved iteration assignment change previous example 
implemented variants parallel machine 
order compensate problems memory contention increased complexity addition operation dummy loop iterations overhead call add routine 
experiments consisted summing numbers 
second version run large number activities maintained 
results shown table 
see simplest version slow lparfor version came quite close sequential time processor processors got speedup times best sequential time 
hand optimized version chunking slightly better indicating lparfor save signi cant amount 
matrix multiplication example example consider matrix multiplication arrays partitioned local memories processors 
mapped version lparfor number arrays may manipulated parallel 
int void init int malloc int malloc int malloc mapped lparfor int int malloc int malloc int malloc init init initialization partitioned arrays matrix multiplication example 
mapped lparfor int sum sum sum sum mapped lparfor int sum rem int bot top bot top bot int malloc bot bot rem rem bot sum sum sum versions matrix multiplication 
rst assumes machine mapped lparfor construct 
second knows better exploit reuse local memory 
allocates just activity physical processor copies column local memory 
time speedup number pes mm lparfor mm lparfor table result matrix multiplication partitioned arrays 
rst row shows behavior simple coded version left side 
improved performance attained better local memory attempts avoid memory con icts right side 
note speedups relative execution single pe relative separate sequential version 
shows partitioned arrays allocated 
note mapped lparfor construct ensures partition local activity index matrices stored row major partition interpreted band rows 
matrix stored column major 
versions body matrix multiplication code 
rst assumes machine mapped lparfor construct 
results rst row table show improvement run time number processors increases levels speedup pes starts degrade 
reason accesses local accesses remote 
second version exploits reuses local memory allocating just activity real processor initialization similarly modi ed shown 
column array copied shared memory local array 
entire slice array local 
note code parametrized real number physical processors adjusts available parallelism 
signi cant improvements result seen second row table speedup processors 
applications students gotten signi cant speedups complex applications speedup processors fft application 
implementation fairly butter network simulated shared memory activity created nodes network 
message passing nodes done shared memory ags indicate presence data 
expected sorting programs popular students 
interesting variant sorting tried avoid bottlenecks shared bus 
parc program limited parallelism parallelism inherent system number processors 
processor radix sorted assigned data allowed distribute buckets processors 
mean time processors continued radix sorting buckets data transferred sorted 
surprisingly method noticeably better allowing processors access bus 
due fact local large 
interesting application parallelized usual fractal demonstration program 
di erent methods tried 
initially process generated strip 
due di ering amounts load balancing added program 
process calculated rate progress compared global average 
slow split remaining placed part global 
process nished take extra global 
scheme compared automatic balancing parc runtime system simply spawning new activity extra 
context switching overheads turned better load balancing program 
parc easy explore alternatives 
students nished working correct implementations 
continued investigate improvements 
improvements exploiting memory locality programming load balancing avoiding excessive process creation termination 
course attended computer science students stronger desire develop programming environmental tools parallel programming run applications 
computer scientists rarely care result program program execution holds interest output 
consequence debuggers runtime systems automatic load balancing systems developed parc environment 
alas debugging parc programs easy task believe features easier debug parc programs parallel programming languages message passing 
parc programs easily serialized simply replace statement statement 
bugs simple ones hard track students suspected subtle timing bugs strange parallel execution possibilities overlooking simple possible causes 
part common parc bugs confusion local global having loop global index variable inside parallel construct 
comparison languages parallel machines existence various ways programming 
parallel languages imperative style parallelism implicit way computation carried functional logic languages left challenge compiler :10.1.1.164.8106
parc imperative language explicit control parallelism compare languages type 
nutshell parc may characterized language providing shared memory mimd asynchronous model computation dynamic spawning parallel activities hierarchical scoping rules provide sense locality 
know language features 
paragraphs list various parallel languages point di erences parc 
programming model imperative parallel languages designed distributed architectures shared memory machines 
occam ada joyce concurrent cosmic languages example provide facilities synchronous asynchronous message passing parallel processes 
allows ecient implementations shared memory message passing architectures 
programs written languages utilize full possibilities orded shared memory architectures allow shared variables 
languages linda swarm provide shared data space parc lack scoping sense locality 
partly due fact languages support associative memory traditional location addressable shared memory parc 
split language supports non traditional shared memory model 
access shared address space mediated active messages speci cally asynchronous put get memory blocks pes 
accessing pe check busy wait local ag see put get completes 
assignment remote location split phase operation 
pc language uses mach task thread model threads task share address space access address spaces tasks 
gives tier memory 
concurrent allows users available shared memory uniprocessor shared memory multiprocessor implementations fear users refuse system prohibited 
language support scoping worse implementations lack shared memory lan execute programs rely feature 
fact languages designed explicitly shared memory mimd machines scarce 
installations simple thread packages allow multiple threads share address space 
hand exist number simd languages ectively allow data sharing pointers array indexes 
main limitation languages parallel computations identical proceed lockstep 
relaxed languages hpf computations loosely synchronous 
parallel languages tailored closely speci architecture 
especially typical languages designed vector machines desire usually high vector performance 
implies simple regular structures machine vector registers 
clarity portability ease programming sacri ced 
result ecient code loops general approach speci cally support parallelism divide conquer algorithms 
expressing sharing patterns noted parallel languages shared memory model allow activities access full address space 
means de ne sharing patterns 
examples include hpf dataparallel languages linda tuple space :10.1.1.113.9679
partitioning address space provided level task thread scheme pc 
threads created context task share task address space 
access address spaces tasks 
addition accessibility necessarily correlated locality 
split exposes locality providing special operations put get access remote memory 
number systems go step introduce special annotations tag variables private shared 
useful indication variable stored numa architecture 
example cedar system degree sharing dictates variable stored cluster memory global memory 
alternative approach initially allow variables shared execution limit access certain activities needed 
obviously practiced mutual exclusion talking having special constructs language 
example check check operations 
example construct jade :10.1.1.5.6610
scoping rules parc proposed programming nyu 
expressing parallelism desirable parallel programs express parallelism inherent algorithm physically available allow ecient implementations di erent architectures 
languages capable expressing massive parallelism 
done parallel loop constructs recursion 
surprisingly parallel languages place various limitations spawning parallel activities 
occam allow recursion making practically impossible code divide conquer algorithms 
languages parallel loop construct de ne scoping boundaries 
example occam provides closed parallel constructs similar parc support shared variables 
parallel construct typically assume interactions iterations parc lparfor 
languages low level unstructured support parallelism 
dijkstra observed closed parallel constructs provided parc promote structured style just closed control structures advocated sequential languages 
idea caught sequential languages goto disappeared parallel languages supply fork join primitives closed constructs examples include ada concurrent 
cases threads package mpc support parallelism limited direct interface run time system 
systems direct support expressing parallelism language assume copy program executed processor spmd paradigm 
example case concurrent pascal split required active messages depends fact nodes memory image 
systems provide capabilities somewhat similar parc pc split join model 
pc construct exactly syntax loop general parc 
price implementation rst execute control sequentially gure iterations spawns threads 
split join model takes opposite approach 
parallel constructs create activities constructs split existing activities groups di erent tasks 
total number activities constant limited number processors 
tasks nished groups re join create original larger group 
parc unique way deals shared memory integrates scoping synchronization forced termination block structure express parallelism 
note parc constructs part block structure language just serial loops conditionals 
activity just block code compound statement opposed languages parallelism created de ning processes modules ada joyce 
parc suitable expression ne grain parallelism 
course activities may quite large may call functions 
expressing synchronization synchronization mechanisms shared memory environments typically fall categories 
mechanisms associated speci access speci shared data structure order guarantee consistency 
examples include various mechanisms ensure mutual exclusion semaphores monitors locks check check 
category mechanisms delimit program phases known data races phase 
examples include barrier synchronization rendezvous join 
classi cation distinguishes synchronization mechanisms ect activity uses 
speci cally dichotomy mechanisms cause activity block allow activity just probe synchronization condition 
condition met activity proceed 
blocking mechanisms imported concurrent languages designed ease programming multitasking uniprocessor systems 
systems blocking reasonable alternative 
interest wait free primitives suitable truly parallel systems :10.1.1.164.8106
examples include various write instructions test set fetch add compare swap 
parc provides repertoire low level primitives cover di erent synchronization behaviors blocking semaphores barrier wait free fetch add 
high level primitives may added programming experience indicates certain constructs especially useful 
parc promising language extension coding shared memory parallel algorithms 
various macro packages just provide access lightweight processes supported system parc integrates parallelism block structure language 
results natural pattern shared private variables conventional scoping rules allows high level instructions terminate branches parallel program synchronize set activities 
summarize salient features parc closed parallel constructs freely nested 
control allocation mapping activities involve detailed knowledge underlying architecture 
explicit declarations shared variables variables shared parallel activities see 
high level primitive activity group termination 
ability exploit distributed shared numa memory mimd machines 
ideal semantics parc program activities execute parallel distinct processors 
number activities program may surpass number physical processors available 
activities blocked execution deadlock may ensue interdependencies executing blocked activities 
advocate preemption breadth rst execution activity tree provide degree fairness avoid situations 
basic features parc rst de ned simulator provides support historical reasons actual syntax parc recognized original compiler marginally di erent described 
system includes capacity link libraries code graphical debugger execution control exact time measurements 
run time system supports parc features described research multiprocessor operational 
research runtime systems programming platform parallel algorithms classes 
parc rst developed ben asher 
extensions added dana ron 
compiler simulator written ben asher marcelo 
initial port multiprocessor done mann metzger 
improved run time environment implemented dror feitelson moshe ben 
forced termination implemented yair friedman 
dror zernik danny wrote package monitoring graphical display program execution 
martin land kept hardware running 
larry rudolph supervised project inception 
lively discussions authors dror zernik moshe ben sharon members research group material de ning semantics various parallel constructs 
bal steiner tanenbaum programming languages distributed computing systems 
acm comput 
surv 

multithreaded languages scienti technical computing 
proc 
ieee 
henson system design parallel computing 
high perform 
syst 

fox applications parallel supercomputers scienti results computer science lessons 
natural arti cial parallel computation arbib robinson eds chap 
mit press 
ben asher haber rudolph improving parallel code parallel simulator 
manuscript institute computer science hebrew university jerusalem 
feitelson ben asher ben rudolph zernik issues run time support tightly coupled parallel processing 
symp 
experiences distributed multiprocessor systems iii 
rudolph feitelson zernik ben asher ben runtime support parallel language constructs tightly coupled multiprocessor 
technical report manuscript institute computer science hebrew university jerusalem 
performance cache memory shared bus multiprocessor experimental study conventional multi level designs 
master thesis institute computer science hebrew university jerusalem 
zernik rudolph animating time debugging parallel programs foundation experience 
acm onr workshop parallel distributed debugging 
ben asher feitelson performance overhead measurements 
technical report dept computer science hebrew university jerusalem 
polychronopoulos kuck guided self scheduling practical scheduling scheme parallel supercomputers 
ieee trans 
comput 

hummel schonberg flynn factoring method scheduling parallel loops 
comm 
acm 
ni trapezoid self scheduling practical scheduling scheme parallel compilers 
ieee trans 
parallel distributed syst 

segall siewiorek caplan lehr russinovich mpc multiprocessor language consistent shared data type paradigms 
nd ann 
hawaii intl 
conf 
system sciences 
gottlieb rudolph basic techniques ecient coordination large numbers cooperating sequential processes 
acm trans 
prog 
lang 
syst 

kruskal algorithms add 
intl 
conf 
parallel processing 
dijkstra operating sequential processes 
programming languages ed pp 
academic press 
frederickson jones smith synchronization control parallel algorithms 
parallel computing 
ousterhout scheduling techniques concurrent systems 
rd intl 
conf 
distributed computing systems 
hummel schonberg low overhead scheduling nested parallelism 
ibm res 
dev 

lamport multiprocessor computer correctly executes multiprocess programs 
ieee trans 
comput 

feitelson rudolph gang scheduling performance bene ts ne grain synchronization 
parallel distributed comput 

tanenbaum van renesse van staveren sharp mullender jansen van rossum experiences amoeba distributed operating system 
comm 
acm 
adve hill weak ordering new de nition 
th ann 
intl 
symp 
computer architecture conf 
proc 
mogul borg ect context switches cache performance 
th intl 
conf 
architect 
support prog 
lang 
operating syst 
hudak conception evolution application functional programming languages 
acm comput 
surv 

shapiro family concurrent logic programming languages 
acm comput 
surv 

kong re ned update :10.1.1.164.8106
languages compilers parallel computing gelernter nicolau padua eds mit press 
inmos occam programming manual 
prentice hall 
united states department defence manual ada programming language 
ansi mil std 
brinch hansen multiprocessor implementation joyce 
software pract 
exp 
gehani concurrent 
software pract 
exp 
seitz concurrent architectures 
vlsi parallel computation birtwistle eds chap 
morgan kaufmann publishers 
ahuja carriero gelernter linda friends 
computer 

roman cox implementing shared dataspace language message multiprocessor 
th intl 
workshop software speci cation design 
von culler goldstein schauser active messages mechanism integrated communication computation 
th ann 
intl 
symp 
computer architecture conf 
proc 
culler dusseau goldstein krishnamurthy von eicken yelick parallel programming split 
supercomputing 
canetti fertig malki pinter parallel pc programming language 
ibm res 
dev 

cmelik gehani experience multiple processor versions concurrent 
ieee trans 
softw 
eng 

rose language data parallel computation 
usenix papers 
hillis connection machine 
mit press 
milligan programming language 
software pract 
exp 
kuehn siegel extensions programming language simd mimd parallelism 
intl 
conf 
parallel processing 
high performance fortran 
ieee parallel distributed technology 
karp programming parallelism 
computer 
karp babb ii comparison parallel fortran dialects 
ieee software 
gelernter generative communication linda 
acm trans 
prog 
lang 
syst 

jordan force 
characteristics parallel algorithms jamieson gannon douglass eds mit press 
nichols siegel dietz data management control ow aspects simd spmd parallel language compiler 
ieee trans 
parallel distributed syst 

eigenmann hoe li padua restructuring fortran programs cedar 
intl 
conf 
parallel processing 
meier eigenmann parallelization performance conjugate gradient algorithms cedar hierarchical memory multiprocessor 
rd symp 
principles practice parallel programming 
hill larus reinhardt wood cooperative shared memory software hardware scalable multiprocessors 
th intl 
conf 
architect 
support prog 
lang 
operating syst 
lam rinard coarse grain parallel programming jade 
rd symp 
principles practice parallel programming 
rinard scales lam jade high level machine independent language parallel programming 
computer jun 
gottlieb kruskal rudolph snir teller wilson issues related mimd shared memory computers nyu approach 
th ann 
intl 
symp 
computer architecture conf 
proc 
cooper draves threads 
technical report cmu cs dept computer science carnegie mellon university jun 
george norton ster single program multiple data computational mode fortran 
parallel computing 
brinch hansen programming language concurrent pascal 
ieee trans 
softw 
eng 

valiant bridging model parallel computation 
comm 
acm 
brooks iii warren welcome split join message passing programming models bbn tc 
technical report id lawrence livermore national laboratory 
rudolph segall dynamic decentralized cache schemes mimd parallel processors 
th ann 
intl 
symp 
computer architecture conf 
proc 
implementation real time thread synchronization 
proc 
summer usenix technical conf 
wood chandra hill larus lebeck lewis mukherjee palacharla reinhardt mechanisms cooperative shared memory 
th ann 
intl 
symp 
computer architecture conf 
proc 
gupta fuzzy barrier mechanism high speed synchronization processors 
rd intl 
conf 
architect 
support prog 
lang 
operating syst 
casey distributed system software architecture 
ieee software 
nikhil papadopoulos arvind multithreaded massively parallel architecture 
th ann 
intl 
symp 
computer architecture conf 
proc 
andrews schneider concepts notations concurrent programming 
acm comput 
surv 

herlihy wait free synchronization :10.1.1.164.8106
acm trans 
prog 
lang 
syst 

burns mutual exclusion linear waiting binary shared variables 
sigact news 
stone high performance computer architecture 
addison wesley nd ed 


