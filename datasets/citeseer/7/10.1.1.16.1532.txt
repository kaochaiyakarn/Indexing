ranking algorithms named entity extraction boosting voted perceptron michael collins labs research florham park new jersey 
research att com id keywords named entity recognition machine learning boosting perceptron algorithm contact author michael collins research att com consideration conferences specify 

describe algorithms rerank top hypotheses maximum entropy tagger application named entity recognition corpus web data 
approach uses boosting algorithm ranking problems 
second approach uses voted perceptron algorithm 
algorithms give comparable significant improvements maximum entropy baseline 
voted perceptron algorithm considerably efficient train cost computation test examples 
ranking algorithms named entity extraction boosting voted perceptron id describe algorithms rerank top hypotheses maximum entropy tagger application named entity recognition corpus web data 
approach uses boosting algorithm ranking problems 
second approach uses voted perceptron algorithm 
algorithms give comparable significant improvements maximum entropy baseline 
voted perceptron algorithm considerably efficient train cost computation test examples 
statistical approaches parsing tagging begun consider methods incorporate global features candidate structures 
examples techniques markov random fields abney della pietra johnson boosting algorithms freund collins walker 
appeal methods flexibility incorporating features model essentially features useful discriminating bad structures included 
second appeal methods training criterion discriminative attempting explicitly push score probability correct structure training sentence score competing structures 
discriminative property shared methods johnson collins conditional random field methods lafferty :10.1.1.120.9821
previous collins boosting algorithm rerank output existing statistical parser giving significant improvements parsing accuracy wall street journal data 
similar boosting algorithms applied natural language generation results walker 
apply reranking methods named entity extraction 
state theart maximum entropy tagger generate possible segmentations input sentence probabilities 
describe number additional global features candidate segmentations 
additional features evidence reranking hypotheses maximum entropy tagger 
describe learning algorithms boosting method collins variant voted perceptron algorithm initially described freund schapire 
applied methods corpus words tagged web data 
methods give significant improvements maximum entropy tagger relative reduction error rate voted perceptron relative improvement boosting method 
contribution show existing reranking methods useful new domain named entity tagging suggest global features give improvements task 
stress contribution show new algorithm voted perceptron gives credible results natural language task 
extremely simple algorithm implement incredibly fast train testing phase slower means sluggish 
viable alternative methods boosting markov random field algorithms described previous 
background data period year words named entity data annotated 
data drawn web pages aim support question answering system web data 
number categories annotated usual people organization location categories frequent categories brand names scientific terms event titles 
data created training set sentences words test set sentences words 
task consider recover boundaries 
leave recovery categories entities separate stage processing 
evaluate different methods task precision recall 
method proposes entities test set correct entity marked annotator exactly span proposed precision method similarly total number entities human annotated version test set recall baseline tagger problem framed tagging task tag word start entity continuation entity part entity tags respectively cases 
baseline model maximum entropy tagger similar ones described ratnaparkhi borthwick mccallum 
maximum entropy taggers shown highly competitive number tagging tasks partof speech tagging ratnaparkhi recognition borthwick information extraction tasks mccallum 
maximum entropy tagger initial experiments forcing tagger recover categories segmentation exploding number tags reduced performance segmentation task presumably due sparse data problems 
represents serious baseline task 
feature set ffl word tagged previous word word 
ffl previous tag previous tags bigram trigram features 
ffl compound feature fields word start sentence word occur list words occur frequently lower case upper case words large corpus text 
type letter word type defined capitalized letter lower case letter digit 
example word animal seen start sentence occurs list frequent lower cased words mapped feature 
ffl word character mapped type 
example mapped animal mapped 
ffl word character mapped type repeated consecutive character types repeated mapped string 
example animal mapped aa mapped 
features inspired bikel hidden markov model gives excellent results named entity extraction 
tagger applied trained way described ratnaparkhi 
feature templates described create set binary features tag history context 
example feature word tagged parameters model ff defining conditional distribution tags history tjh ff ff parameters trained generalized iterative scaling 
ratnaparkhi include features occur times training data 
decoding beam search recover candidate tag sequences sentence sentence decoded left right top probable hypotheses stored point 
applying baseline tagger baseline trained model full sentences training data decoded sentences test data 
gave candidates test data sentence probabilities 
baseline method simply take probable candidate test data sentence calculate precision recall figures 
aim come strategies reranking test data candidates way precision recall improved 
developing reranking strategy sentences training data split sentence training portion sentence development set 
training portion split sections case maximum entropy tagger trained data decode remaining 
top hypotheses beam search log probabilities recovered training sentence 
similar way model trained sentence set produce hypotheses sentence development set 
global features global feature generator module describe section generates global features candidate tagged sequence 
input takes sentence proposed segmentation assignment tag word sentence 
output produces set feature strings 
tagged sentence running example section re aging flower child clueless gen day shot john playing dougherty arts center imagination example feature type simply list full strings entities appear tagged input 
example give features gen day shot john dougherty arts center stands entity 
section write features format 
start feature string indicates feature type case followed 
type generally words symbols separate symbol seperate module implementation takes strings produced global feature generator hashes integers 
example suppose strings gen day shot john dougherty arts center hashed respectively 
conceptually candidate represented large number features number distinct feature strings training data 
example take value features zero 
feature templates introduce notation describe full set global features 
assume primitives input candidate ffl th tag tagged sequence 
ffl th word 
ffl begins lower case letter 
description feature template entity string ws features entity ff fs fe features entity gf gs ge word entity lw indicates word lower cased le bigram boundary features words start entity bo gamma ws bo gamma gs bo gamma ws bo gamma gs bigram boundary features words entity ge ge trigram boundary features words start entity features total shown gamma gamma ws gamma gamma gs gamma ws gamma gs trigram boundary features words entity features total shown te gamma te gamma ge te gamma gamma te gamma gamma ge prefix features pf fs pf gs pf fs pf gs pf fs fe pf gs ge suffix features sf fe sf ge sf fe gamma sf ge gamma sf fe gamma fs sf ge gamma gs full set entity anchored feature templates 
features generated entity seen candidate 
take entity span words inclusive candidate 
ffl transformation transformation applied way final feature type maximum entropy tagger 
character word mapped type repeated consecutive character types repeated mapped string 
example animal mapped aa feature mapped 
ffl additional flag appended 
flag indicates word appears dictionary words appeared lower cased capitalized large corpus text 
example animal appears lexicon values aa respectively 
addition defined null features describe anchored entity boundaries candidate segmentation 
feature templates describe features 
example suppose entity seen words inclusive segmentation 
feature described previous section generated template applying template entities running example generates feature strings described previous section 
example consider template ff generate feature string entities candidate time values full set feature templates anchored entities see 
second set feature templates anchored quotation marks 
corpus entities typically long names seen surrounded quotes 
example day shot john name band appears running example 
define index double quotation marks candidate index matching double quotation marks appear candidate 
additionally define index word lower case letter upper case letter digit quotation marks 
set feature templates tracks values words quotes included features gamma prevent explosion length feature strings 
gamma gamma set feature templates sensitive entire sequence quotes tagged named entity 
define sequence words quotes tagged single entity 
additionally define number upper cased words quotes number lower case words 
templates qf qf point fully described representation input reranking algorithms 
maximum entropy tagger gives proposed segmentations input sentence 
candidate represented log probability tagger values global features section describe algorithms blend sources information aim improve strategy just takes candidate tagger highest score 
ranking algorithms notation section introduces notation reranking task 
framework derived transformation ranking problems classification problem freund 
related markov random field methods parsing suggested johnson boosting methods parsing collins 
consider set ffl training data set example input output pairs 
tagging training examples fs sentence correct sequence tags sentence 
ffl assume way enumerating set candidates particular sentence 
ij denote th candidate th sentence training data fx denote set candidates top outputs maximum entropy tagger set candidates 
ffl loss generality take candidate correct tags closest correct 
ffl probability base model assigns log log probability 
ffl assume set additional features features arbitrary functions candidates hope include features help discriminating candidates bad ones 
ffl parameters model vector parameters fw wm ranking function defined function assigns real valued number candidate taken measure plausibility candidate higher scores meaning higher plausibility 
assigns ranking different candidate structures sentence particular output training test example arg max 
take features fixed learning problem choose setting parameters convenience parts vector notation 
define vector fl hm ranking score written delta delta inner dot product vectors boosting algorithm algorithm consider boosting algorithm ranking described collins 
algorithm modification method freund 
method event multiple candidates get highest score candidate highest value log likelihood baseline model taken input ffl examples initial scores ffl arrays gamma gamma described section 
ffl parameters number rounds boosting smoothing parameter ffl 
initialize ffl set arg minw gammal ffl set fw ffl set gamma 
ffl set gammam ffl calculate gammam gamma gamma gammam fi fi fi fi gamma gamma fi fi fi fi repeat ffl choose arg max ffl set ffi log gamma ffl update parameter ffi ffl delta gammam gammaffi gamma gammam ffi delta gamma gamma gamma delta delta ffl gamma delta gammam ffi gamma gammam gamma ffi delta gamma gamma gamma delta delta ffl features values gamma changed recalculate fi fi fi fi gamma gamma fi fi fi fi output final parameter setting boosting algorithm 
considered greedy algorithm finding parameters minimize loss function loss gammaf delta theoretical motivation algorithm goes back pac model learning 
intuitively useful note loss function upper bound number ranking errors ranking error case incorrect candidate gets higher value correct candidate 
follows gammax define 
loss gamma 
note number ranking errors 
initial step set arg min gammal parameters set zero 
algorithm proceeds iterations usually chosen cross validation development set 
iteration single feature chosen weight updated 
suppose current parameter values single feature chosen weight updated increment ffi ffi 
new loss parameter update ffi gammam ffi gammah gamma 
boosting algorithm chooses feature update pair ffi optimal terms minimizing loss function ffi arg min ffi ffi update ffi define delta 
input examples feature vectors 
initialization set parameters argmax ij gamma gamma gamma gamma ij output sequence parameter vectors perceptron training algorithm ranking problems 
shows algorithm implements greedy procedure 
see collins full description method including justification algorithm fact implement update eq 
iteration 
algorithm relies arrays gamma gamma gamma gamma fk gamma gamma fk gamma gamma index features correct incorrect candidate pairs th feature takes value correct candidate value incorrect candidate 
array gamma similar index features examples 
arrays gamma reverse indices training examples features 
voted perceptron shows training phase perceptron algorithm originally introduced rosenblatt 
algorithm maintains parameter vector initially set zeros 
algorithm pass training set training example storing parameter vector parameter vector modified mistake example 
case update simple involving adding difference offending examples representations strictly speaking case smoothing parameter ffl 
define delta 
input set candidates sequence parameter vectors initialization set stores number votes argmax output arg max applying voted perceptron test example 
gamma gamma ij 
see cristianini shawe taylor chapter discussion perceptron algorithm theory justifying method setting parameters 
basic form perceptron parameter values taken final parameter settings output new test example candidates simply highest scoring candidate parameter values arg max delta freund schapire describe refinement perceptron voted perceptron 
training phase identical 
note parameter vectors stored 
training phase thought way constructing different parameter settings 
parameter settings highest ranking candidate arg max delta 
idea voted perceptron take parameter settings vote candidate candidate gets votes returned candidate 
see algorithm 
note reasons explication decoding algorithm efficient necessary 
example gamma preferable book keeping avoid recalculation arg max 
experiments applied voted perceptron boosting algorithms data described section 
features occurring distinct training sentences included model 
resulted distinct features 
methods trained training portion sentences training set 
development set pick best values tunable parameters algorithm 
case boosting main parameter pick number rounds ran algorithm total rounds optimal value measure development set occurred rounds 
case voted perceptron representation taken vector hm fi parameter influences relative contribution log likelihood term versus features 
testing values fi value fi give best results development set 
shows results methods test set 
reranking algorithms show significant improvements baseline relative reduction error boosting relative error reduction voted perceptron 
experiments voted perceptron algorithm considerably efficient training cost computation test examples 
attractive property voted perceptron kernels example kernels parse trees described collins duffy 
abney 
stochastic attribute value grammars 
computational linguistics 
bikel schwartz weischedel 

algorithm learns name 
machine learning special issue natural language learning 
borthwick sterling agichtein grishman 

exploiting diverse knowledge sources maximum entropy named entity recognition 
proc 
sixth workshop large corpora 
collins 

discriminative reranking natural language parsing 
proceedings seventeenth max ent boosting voted perceptron results maximum entropy boosting voted perceptron methods 
precision recall measure 
figures relative improvements error rate maximum entropy model 
ternational conference machine learning icml 
collins duffy 

convolution kernels natural language 
proceedings nips 
cristianini shawe 

support vector machines learning methods 
cambridge university press 
della pietra della pietra lafferty inducing features random fields 
ieee transactions pattern analysis machine intelligence april pp 

freund schapire 

large margin classification perceptron algorithm 
machine learning 
freund iyer schapire singer 

efficient boosting algorithm combining preferences 
machine learning proceedings fifteenth international conference 
johnson geman canon chi riezler 

estimators stochastic grammars 
proceedings acl 
lafferty mccallum pereira 

conditional random fields probabilistic models segmenting labeling sequence data 
proceedings icml 
mccallum freitag pereira 
maximum entropy markov models information extraction segmentation 
proceedings icml 
ratnaparkhi 

maximum entropy part ofspeech tagger 
proceedings empirical methods natural language processing conference 
rosenblatt 

perceptron probabilistic model information storage organization brain 
psychological review 
reprinted neurocomputing mit press 
walker rambow 

spot trainable sentence planner 
proceedings nd meeting north american chapter association computational linguistics naacl 

