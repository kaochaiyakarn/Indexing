evaluation gaussian processes methods non linear regression carl edward rasmussen thesis submitted conformity requirements degree doctor philosophy graduate department computer science university toronto fl copyright carl edward rasmussen evaluation gaussian processes methods non linear regression carl edward rasmussen thesis submitted conformity requirements degree doctor philosophy graduate department computer science university toronto march thesis develops bayesian learning methods relying gaussian processes rigorous statistical approach evaluating methods 
experimental designs sources uncertainty estimated generalisation performances due variation training test sets accounted 
framework allows estimation generalisation performance statistical tests significance pairwise comparisons 
experimental designs recommended supported delve software environment 
new non parametric bayesian learning methods relying gaussian process priors functions developed 
priors controlled hyperparameters set characteristic length scale input dimension 
simplest method parameters fit data optimization 
second fully bayesian method markov chain monte carlo technique integrate hyperparameters 
advantage gaussian process methods priors hyperparameters trained models easy interpret 
gaussian process methods benchmarked methods regression tasks real data data generated realistic simulations 
experiments show small datasets unsuitable benchmarking purposes uncertainties performance measurements large 
second set experiments provide strong evidence bagging procedure advantageous multivariate adaptive regression splines mars method 
simulated datasets controlled characteristics useful understanding relationship properties dataset performance different methods 
dependency performance available computation time investigated 
shown bayesian approach learning multi layer perceptron neural networks achieves better performance commonly early stopping procedure reasonably short amounts computation time 
gaussian process methods shown consistently outperform conventional methods 
ii acknowledgments radford neal geoffrey hinton sharing insights enthusiasm ph 
hope day similarly able inspire people wish past members visitors neuron delve groups committee particular drew van camp peter dayan brendan frey zoubin ghahramani david mackay mike revow rob tibshirani chris williams 
providing excellent meals times domestic life ebb 
lastly wish continued encouragement confidence 
studies toronto supported danish research academy university toronto open fellowship geoffrey hinton national sciences engineering research council canada institute robotics intelligent systems 
iii contents evaluation comparison generalisation 
previous approaches experimental design 
general experimental design considerations 
hierarchical anova design 
way anova design 
discussion 
learning methods algorithms heuristics methods 
choice methods 
linear model lin 
nearest neighbor models knn cv 
mars bagging 
neural networks trained early stopping mlp ese 
bayesian neural network monte carlo mlp mc 
regression gaussian processes neighbors large neural nets covariance functions 
predicting gaussian process 
parameterising covariance function 
iv contents adapting covariance function 
maximum aposteriori estimates 
hybrid monte carlo 
directions 
experimental results datasets delve 
applying bagging mars 
experiments boston price 
results kin pumadyn datasets 
implementations linear model lin 
nearest neighbors regression knn cv 
neural networks trained early stopping mlp ese 
bayesian neural networks mlp mc 
gaussian processes 
conjugate gradients conjugate gradients 
line search 
discussion 
vi contents chapter ability learn relationships examples compelling attracted interest parts science 
biologists psychologists study learning context animals interacting environment mathematicians statisticians computer scientists take theoretical approach studying learning artificial contexts people artificial intelligence engineering driven requirements technological applications 
aim thesis contribute principles measuring performance learning methods demonstrate effectiveness particular class methods 
traditionally methods learn examples studied statistics community names model fitting parameter estimation 
huge interest neural networks 
approaches taken communities differed substantially models studied 
statisticians usually concerned primarily interpretability models 
emphasis led diminished interest complicated models 
hand workers neural network field embraced complicated models unusual find applications computation intensive models containing hundreds thousands parameters 
complex models designed entirely predictive performance mind 
approaches learning begun converge 
workers neural networks rediscovered statistical principles interest non parametric modeling risen statistics community 
focus statistical aspects nonparametric modeling brought explosive growth available algorithms 
new flexible models designed particular learning tasks mind introduces problem choose best method particular task 
general purpose methods rely various assumptions approximations cases hard know met particular applications severe consequences breaking 
urgent need provide evaluation methods practical point view order guide research 
interesting example kind question long standing debate bayesian frequentist methods desirable 
unhappy setting priors claimed arbitrary 
bayesian theory accepted may considered computationally impractical real learning problems 
hand bayesians claim models may superior may avoid computational burden involved cross validation set model complexity 
doubtful disputes settled continued theoretical debate 
empirical assessment learning methods appealing way choosing 
method shown outperform series learning problems judged representative sense applications interested sufficient settle matter 
measuring predictive performance realistic context trivial task 
surprisingly tremendously neglected field 
seldom experimental backed statistically compelling evidence performance measurements 
thesis concerned measuring comparing predictive performance learning methods contains main contributions theoretical discussion perform statistically meaningful comparisons learning methods practical way novel methods relying gaussian processes demonstration assessment framework empirical comparison performance gaussian process methods methods 
elaboration topics follow 
give detailed theoretical discussion issues involved practical measurement predictive performance 
example statisticians uneasy fact neural network methods involve random initializations result learning unique set parameter values 
issues faced difficult give proper treatment 
discussion involves assessing statistical significance comparisons developing practical framework doing comparisons measures ensuring results reproducible 
focus chapter goal comparisons precise understand uncertainties involved empirical evaluations 
objective obtain tradeoff conflicting aims statistical reliability practical applicability framework computationally intensive learning algorithms 
considerations lead guidelines measure performance 
software environment implements guidelines called delve data evaluating learning valid experiments written research group headed geoffrey hinton 
delve freely available world wide web delve contains software necessary perform statistical tests datasets evaluations results applying methods datasets precise descriptions methods 
delve statistically founded simulation experiments comparing performance learning methods 
chapter contains discussion design delve implementational details provided rasmussen 
delve provides environment methods compared 
delve includes number allow easier comparisons earlier attempts provide realistic setting methods 
attempts making benchmark collections provide data preprocessed format order reproducibility 
approach misguided attempting measure performance achieved realistic setting preprocessing tailored particular method 
allow definitions methods delve include descriptions preprocessing 
delve provides facilities common types preprocessing default attribute encoding researchers primarily interested issues 
chapter detailed descriptions learning methods emphasize reproducibility 
implementations complicated methods involve choices may easily justifiable theoretical point view 
example neural networks trained iterative methods raises question iterations apply 
convergence reached reasonable amount computational effort example may preferable training convergence 
issues discussed thoroughly articles describing new methods 
furthermore authors may preliminary simulations set parameters inadvertently opening possibility bias simulation results 
delve web address www cs utoronto ca delve order avoid problems learning methods specified precisely 
methods contain parameters difficult set recognized having handicap heuristic rules setting parameters developed 
rules don practice may show comparative studies indicating learning method expected actual application 
naturally parameters may set initial trials training data case considered part training procedure 
precise level specification easily met automatic algorithms require human intervention application 
thesis automatic methods considered 
methods described chapter include methods originating statistics community neural network methods 
ideally hoped find descriptions implementations methods literature concentrate testing comparing 
unfortunately descriptions literature rarely detailed allow direct application 
frequently details implementations mentioned rare cases unsatisfactory nature 
example may mentioned networks trained epochs hardly principle applied universally 
hand proven extremely difficult design heuristic rules incorporate researcher common sense 
methods described chapter selected partially considerations difficult may invent rules 
descriptions contain precise specifications commentary 
chapter develop novel bayesian method learning relying gaussian processes 
model especially suitable learning small data sets computational requirements grow rapidly amount available training data 
gaussian process model inspired neal priors infinite neural networks provides unifying framework models 
actual model quite weighted nearest neighbor model adaptive distance metric 
large body experimental results generated delve 
neural network techniques statistical methods evaluated compared sources data 
particular shown difficult get statistically significant comparisons datasets containing cases 
finding suggests previously published comparisons may statistically founded 
unfortunately hard find suitable real datasets containing cases assessments 
attempt overcome difficulty delve generated large datasets simulators realistic phenomena 
large size simulated datasets provides high degree statistical significance 
hope realistic researchers find performance data interesting 
simulators allow generation datasets controlled attributes degree non linearity input dimensionality may help determining aspects datasets important various algorithms 
chapter perform extensive simulations large simulated datasets delve 
simulations show gaussian process methods consistently outperform methods 
chapter evaluation comparison chapter discuss design experiments test predictive performance learning methods 
large number learning methods proposed literature practice choice method governed tradition familiarity personal preference comparative studies performance 
naturally predictive performance aspect learning method characteristics interpretability ease concern 
predictive performance developed set directly applicable statistical techniques exist enable comparisons 
despite rare find compelling empirical performance comparisons literature learning methods prechelt 
chapter defining generalisation measure predictive performance discuss possible experimental designs give details promising designs comparing learning methods implemented delve environment 
generalisation usually learning methods trained goals identify interpretation data predictions unmeasured events 
study concerned accuracy 
statistical terminology called expected sample predictive loss neural network literature referred generalisation error 
informally define expected loss particular method trained data particular distribution novel test evaluation comparison case distribution 
formalism alluded thesis objective learning minimize expected loss 
commonly loss functions squared error loss regression problems loss classification considered 
noted formalism fully general requires losses evaluated case case basis 
disallow methods inputs multiple test cases predictions 
confinement fixed training sets single test cases rules scenarios involve active data selection incremental learning distribution data drifts situations test case needed evaluate losses 
broad class learning problems naturally cast framework 
order give formal definition generalisation need consider sources variation basic experimental unit consists training method particular set training cases measuring loss test case 
sources variation 
random selection test case 

random selection training set 

random initialisation learning method random initial weights neural networks 

stochastic elements training algorithm method stochastic hillclimbing 

stochastic elements predictions trained method monte carlo estimates posterior predictive distribution 
sources inherent experiments specific certain methods neural networks 
definition generalisation error involves expectation effects theta dx dt dd dr dr dr generalisation error method implements function trained training sets size loss function measures loss making prediction training set size test input true target generalisation loss averaged distribution training sets test points random effects initialisation training prediction 
assumed training examples test examples drawn independently unknown distribution 
simplifying assumption holds prediction tasks important exception time series prediction training cases usually drawn independently 
assumption empirical evaluation generalisation error problematic 
definition generalisation error involves averaging training sets particular size 
may argued unnecessary applications particular training set disposal 
current study empirical evaluations order get idea methods perform data sets similar characteristics 
unreasonable assume new tasks contain peculiarities particular training sets empirical study 
essential take effects variation account especially estimating confidence intervals evaluation difficult reasons 
function integrated typically complicated allow analytical treatment data distribution known 
real applications distribution data unknown sample distribution available 
sample large compared wish estimate real datasets find difficult situation trying estimate values far available sample size 
goal discussion sections design experiments allow generalisation error estimated uncertainties estimate allow performance methods compared 
ability estimate uncertainties crucial comparative study allows quantification probability observed differences performance attributed chance may reflect real difference performance 
addition estimating uncertainty associated estimated generalisation error may interest know sizes individual effects giving rise uncertainty 
example may interest know variability performance due random initialisation weights neural network method 
potentially large number effects estimated evaluation comparison estimate lot 
study focus types effects directly related sensitivity experiments 
effects general combinations basic effects eq 

general principles slightly modified experimental designs attempts isolate effects 
previous approaches experimental design section briefly describes previous approaches empirical evaluation neural network community 
severe shortcomings methodologies discussed remainder chapter attempt address 
common approach single training set chosen fraction total number cases available 
remaining fraction cases devoted test set 
cases additional validation set provided set fitting model parameters weight decay constants discussion considered part training set 
empirical mean loss test set reported unbiased consistent estimate generalisation loss 
possible common practice estimate uncertainty introduced finite test set 
particular standard error due uncertainty generalisation estimate falls number test cases gamma test unfortunately uncertainty associated variability training set estimated fact usually silently ignored 
simple approach extended way cross testing 
data divided equally sized subsets method trained gamma tested cases subset 
procedure repeated times subset left testing 
procedure frequently employed quinlan 
advantage won expense having train methods primarily number test cases increased size entire data set 
may suspect trained slightly differing training sets may able estimate uncertainty estimated 
kind analysis complicated fact training sets dependent training sets include training examples 
particular need model overlapping training sets introduce correlations performance estimates difficult 
general experimental design considerations book statlog project appeared michie 
large study sources data evaluating methods classification 
study single training test sets way cross testing 
authors discuss possible bootstrapping estimating performance 
attempt evaluate uncertainties performance estimates ignore statistical difficulties proposals entail 
elena project gu erin simple non paired analysis categorical losses considered 
scheme resembling way cross testing subsequent analysis failed take dependence training sets account 
remarked evaluated robustness holdout method trials considered minimum maximum error computing confidence intervals extrema 
obtained large confidence intervals measure hasn helpful comparisons 
approaches applicable addressing fundamental questions method generalises better data particular task provide ways estimating relevant uncertainties 
occasionally test significance difference larsen hansen prechelt particular training set pairing losses different methods test examples 
general experimental design considerations essence experimental design finding suitable tradeoff practicality statistical power 
practicality approach am referring number experiments required complexity terms computation time memory 
statistical power mean ability tests correctly identify trends small magnitude experiments 
obvious effects traded general may gain confidence repetitions progressively practical 
practicality approach subdivided issues computational time complexity experiments memory data requirements computational requirements statistical test 
learning algorithms require large amount computation time training 
cases fairly strong super linear dependency evaluation comparison available number training cases required amount computation time 
free determine number training cases experimental design regarded externally fixed particular interests 
main objective keep number training sessions low possible 
time needed making predictions method test cases may occasionally concern requirement scale linearly number test cases 
data memory considerations different causes give rise similar restrictions tests 
data requirement total number cases available constructing training test sets 
real datasets limited number cases limitation major concern 
cases artificial data generated simulator may able generate test cases desired large sets may impractical store individual losses tests necessary performing paired tests discussed chapter 
may wish limit tests results easily computed outcomes learning experiments 
analysis interesting experimental designs treated analytically approximate stochastic computation may needed order draw desired 
situations probably undesirable applications difficult ensure accuracy convergence methods 
cases people may find required computational mechanics suspect general convincing 
statistical power tests depends details experimental design 
general training sets test cases smaller effects detected reliably 
distributional assumptions losses importance 
issues easily clarified examples 
purely statistical point view situation simplest assume independence experimental observations 
extreme case may consider experimental design method trained times disjoint training sets single independently drawn test cases 
analysis losses case simple observed losses independent central limit theorem guarantees empirical mean follow unbiased gaussian distribution standard deviation scaling gamma learning methods may wish consider approach computationally prohibitively expensive real problems total amount data limited approach wasteful data amount information extracted case far small 
general experimental design considerations order attempt overcome previous design example may training set multiple test cases bringing total number required training sessions 
corresponds disjoint test sets individual test cases 
computationally lot attractive fewer training sessions required 
extract information training run performance method test cases 
losses longer independent common training sets introduce dependencies accounted analysis design 
persistent concern design requires disjoint training test sets may problem dealing real data sets limited size 
artificially generated large real datasets design may attractive properties discussed section name hierarchical anova design 
increase effectiveness data real learning tasks test trained methods available testing data carving test data disjoint sets 
doing testing able extract information performance method 
comes expense having deal complicated analysis 
losses dependent common training sets common test cases 
design discussed section title way anova design 
preferred design real data sets 
different requirement disk storage hierarchical way designs may importance 
methods tested need store individual losses order perform paired comparisons discussed detail section 
disk storage cheap requirement concern testing numerous methods large test sets 
respect hierarchical design superior losses test cases stored disk requirements 
attempts increase effectiveness terms data tests 
disjoint training sets may reuse cases training test sets 
widely way cross testing mentioned previous section example design 
longer independencies designs hard find reasonable justifiable assumptions performance depends composition training sets 
traditional way cross testing data split subsets attempt model effects subsets individually neglecting interactions may approximation may expect training cases interact quite strongly 
difficulties evaluation comparison mean means training sets individual losses ij schematic diagram hierarchical design 
case disjoint training sets disjoint test sets containing cases 
training test sets disjoint average losses training set independent estimates expected loss designs 
possible way overcoming difficulties certainly importance hopes able small datasets benchmarking 
noted way cross testing usually literature attempt estimate uncertainties associated performance estimates 
cases easy justify experiments 
hierarchical anova design simplest loss model consider analysis variance anova hierarchical design 
loss model learning algorithm trained different training sets 
training sets disjoint specific training case appears single training set 
associated training sets test set cases 
test sets disjoint disjoint training sets 
train method training sets training set evaluate loss case corresponding test set 
particular training set associated test cases referred instance task 
assume losses modeled ij ij ij loss test case test set method trained training hierarchical anova design set ij assumed normally independently distributed oe ij oe parameter models mean loss interested estimating 
variables called effects due training set model variability losses caused varying training set 
note training set effects include sources variability different training sessions different training examples stochastic effects training random 
ij variables model residuals include effects test cases interactions training test cases stochastic elements prediction procedure 
loss functions normality assumptions may appropriate refer section discussion 
analysis attempt evaluate individual contributions ij effects 
eq 
obtain estimated expected loss standard deviation error bars estimate sd oe oe ij hat indicates estimated value bar indicates average 
estimated standard error fixed values oe estimate losses 
introduce means ij ij ij mean squared error expectations ms gamma gamma ms oe oe ms gamma ij gamma ms oe anova models common minimum variance unbiased estimators oe values follow directly eq 
oe ms oe ms gamma ms unfortunately estimate oe may negative 
behaviour explained referring fig 

sources variation firstly variation due differences training sets secondly uncertainty due finitely evaluation comparison test cases evaluated training set 
second contribution may greater empirically eq 
may produce negative estimates variation values expected variation test cases 
customary truncate negative estimates zero introduces bias 
order compare learning algorithms model applied differences losses learning methods ij ijk gamma ijk ij similar normal independence assumptions eq 

case expected difference performance training set effect difference 
similarly ij residuals difference loss model 
noted tests derived model known paired tests losses paired training sets test cases 
generally paired tests powerful non paired tests random variation irrelevant difference performance filtered 
pairing requires training test sets method 
pairing readily achieved delve losses methods kept disk 
central objective comparative loss study get measure confident observed difference methods reflects real difference performance random fluctuation 
different approaches outlined problem standard test bayesian analysis 
idea underlying test assume null hypothesis compute probable observed data extreme data sampling distribution hypothesis 
current application null hypothesis models identical average performances 
may odd focus null hypothesis natural draw ij ij 
reasoning underlying frequentist test show get observed losses null hypothesis presumably confidence sign difference 
technically treatment composite hypothesis complicated simple hypothesis 
treated simple hypothesis exact analytical treatment unknown oe oe preferred may sight appropriate 
null hypothesis distribution differences losses hierarchical anova design partial means obtained eq 
giving ij oe oe oe oe variances unknown practical application 
different partial means independent observations gaussian distribution 
standard result dating back student fisher theory sampling distributions states independently normally distributed unknown variance statistic gamma gamma gamma sampling distribution distribution gamma degrees freedom gamma gammai perform test compute statistic measure null hypothesis obtain observed value extreme 
precisely value gamma gammat dt exist closed form expression numerically easily evaluated incomplete beta distribution rapidly converging continued fractions known abramowitz stegun 
notice test sided limits integral reflecting prior uncertainty method better 
contrast apriori true value negative sided test extending integral gamma getting value half large 
low values indicate confidence observed difference due chance 
notice failure obtain small values necessarily imply performance methods equal merely observed data rule possibility possibility sign actual difference differs observed difference 
fig 
shows example output delve comparing methods 
estimates performances estimated difference standard error estimate sd effects oe oe value test significance observed difference 
evaluation comparison estimated expected loss knn cv estimated expected loss lin estimated expected difference standard error difference estimate sd training sets stochastic training sd test cases stoch 
pred 
interactions significance difference test disjoint training sets containing cases disjoint test sets containing cases 
example applying analysis comparison methods lin knn cv squared error loss function task demo age std delve 
point may useful note standard error difference estimate sd computed fixed estimates standard deviations eq 
distribution gaussian 
uncertainty associated estimates standard deviations 
potentially obtain better estimates standard error difference interpreted confidence interval computationally may cumbersome requires evaluations eq 
little convenient 
reasonably large values differences small primary interest intervals values computed correctly 
alternative frequentist hypothesis test adopt bayesian viewpoint attempt compute posterior distribution observed data prior distribution 
bayesian setting unknown parameters mean difference variances oe oe box tiao likelihood fy ij gj oe oe oe gammai gamma oe oe gammai exp gamma gamma oe oe gamma ij gamma oe obtain posterior distribution multiplying likelihood prior 
may general easy specify suitable priors parameters 
circumstances possible dodge need specify subjective priors improper non informative priors 
simplest choices improper priors standard noninformative oe oe gamma oe oe gamma variances positive scale parameters 
cases resulting posterior proper despite priors 
setting priors hierarchical anova design lead proper posteriors singularity oe data explained acquire non vanishing likelihood oe effect prior density oe approach infinity oe goes zero 
inability non informative improper prior reflects real uncertainty analysis design 
small values oe likelihood independent parameter amount mass placed region posterior largely determined prior 
words likelihood provide information oe region 
alternative prior proposed box tiao setting oe oe gamma oe oe oe oe gamma prior somewhat unsatisfactory property effective prior distribution depends number test cases training set unrelated arbitrary choice experimenter 
positive side simple form posterior allows express marginal posterior closed form ij oe oe fy ij gj oe oe doe doe gammap incomplete beta distribution ij gamma gamma gamma fig 
posterior distribution shown comparison learning methods 
value frequentist test fig 
reasonably close posterior probability opposite sign observed difference calculated numerical integration 
styles analysis making statements different nature reason suspect produce identical values 
frequentist test assumes statement probability observed data extreme bayesian analysis treats random variable 
reassuring differ great extent 
reasons pursued bayesian analysis 
important reason primary concern find methodology adopted delve bayesian method appropriate 
firstly bayesian viewpoint met scepticism secondly analytical problems attempting priors eq 

promising approach proper priors numerical integration evaluate evaluation comparison performance difference posterior density ij posterior distribution comparing lin knn cv methods demo age std data squared error loss eq 

numerical integration mass lies positive values indicated hatched area 
eq 
investigate sensitive widening priors 
sampling approaches problem estimating posterior may viable open interesting possibilities able relax distributional assumptions underlying frequentist test 
extreme care taken attempting sampling methods simple gibbs sampling may hard ensure convergence may leave experiments open criticism 
way anova design experimental setup way design differs hierarchical design test cases training session gaining information performances fig 

efficient terms data may important number available cases small 
analysis model complicated 
loss model ij ij way anova design test cases average loss training sets average loss test cases training sets schematic diagram way design 
disjoint training sets common test set containing cases giving total losses 
partial average performances independent 
effects training sets effects test cases ij interactions noise 
case hierarchical design effects may different components attempt estimate individually 
assumptions independence normality previously oe oe ij oe analogy hierarchical design assumptions give rise expectation standard error sd oe oe oe ij introduce partial mean losses ij ij ij ij mean squared error expectations ms gamma gamma ms oe oe ms gamma gamma ms ioe oe ms gamma gamma gamma ij gamma gamma gamma gamma gamma delta ms oe evaluation comparison empirical values ms ms ms estimate values oe oe ms oe ms gamma ms oe ms gamma ms estimators uniform minimum variance unbiased estimators 
estimates oe oe guaranteed positive set zero negative 
substitute variance estimates eq 
get estimate standard error estimated mean performance 
note estimated standard error oe diverges single training set common practice 
effect caused hopeless task estimating uncertainty single observation 
training sets probably accurate estimates uncertainty achieved 
important question observed difference learning procedures shown significantly different 
settle question model eq 
time model difference losses models ijk gamma ijk ij assumptions 
question estimated mean difference significantly different zero 
test hypothesis quasi test uses statistic degrees freedom ssm ms ms ms ssm ij ssm ms ss ms gamma gamma ms ms ms gamma ms gamma result test value probability null hypothesis true get observed data extreme 
general low values indicates high confidence difference performance learning procedures 
unfortunately quasi test approximate assumptions independence normality met 
conducted set experiments clarify accurate test may 
purposes serious mistake normally termed type error concluding performances methods different reality 
experiments normally anticipate performance different methods exactly ensure discussion test rarely strongly rejects null hypothesis really true presumably rarer declare observed difference significant sign opposite true difference 
generated mock random losses null hypothesis eq 
oe various values oe oe unit oe simply sets scale loss generality 
computed value test repeated trials 
fig 
histograms resulting values 
ideally histograms ought show uniform distribution reason apart finite sample effects due approximation test 
prominent effects spikes histograms 
spikes great concern test strongly favor true null hypothesis 
may lead reduced power test type errors 
spikes occur directly concern 
test strongly rejecting null hypothesis leading infer methods differing performance fact 
effect strong case instances training set effect large 
instances shown problems vanished 
avoid interpretative mistakes fewer instances computed value result reported delve 
discussion may wonder happens tests described earlier sections assumptions rely violated 
independence assumptions fairly safe carefully designing training test sets independence mind 
normality assumptions may met 
example known squared error loss sees outliers accounting large fraction total loss test set 
cases may wonder squared error loss really interesting loss measure 
insist pursuing loss function need consider violations normality 
normality assumptions experimental designs obviously violated case loss estimation squared error loss functions guaranteed positive 
objection disappears comparative designs loss differences assumed normal 
known extreme losses may gaussian assumptions may inappropriate 
solution problem prechelt suggests evaluation comparison experiments training instances showing empirical distribution values bins obtained fake observations null hypothesis 
give standard deviations training set effect test case effect respectively 
discussion experiments training instances showing empirical distribution values obtained fake observations null hypothesis 
give standard deviations training set effect test case effect respectively 
evaluation comparison log losses tests removing outliers 
advocate approach 
loss function reflect function interested minimising 
isn concerned outliers choose loss function reflects 
removing outlying losses appear defensible general application 
method having smaller expected log loss method imply relation expected losses 
generally test test said fairly robust small deviations normality 
large deviations form huge losses occasional outliers turn interesting effects 
comparative loss models described previous sections central determining significance observed difference ratio mean difference uncertainty estimate oe eq 

ratio large confident observed difference due chance 
imagine situation oe fairly large select loss difference random perturb amount observe behaviour ratio increase oe gamma gamma large values tests tend significant magnitude outliers increase 
lucky outliers tend produce results appear significant merely reduce power test 
tendency may cases worrying proportions 
fact say testing methods doing significantly better 
losing method avoid losing face terms significance increasing loss single test case drastically 
impact mean smaller impact oe behaviour result large slightly worse performance bad method insignificant test results 
scenario contrived seen effects occasions shall see chapter 
somewhat unsatisfactory behaviour arises symmetry assumptions loss model 
losses model occasional huge values distribution loss differences assumed symmetric happen didn model huge loss insignificant result 
clearly exactly mind 
may cases assumptions reasonable situations methods may tend wild predictions conservative 
possible deficiencies overcome bayesian setting allowed discussion non gaussian skew distributional assumptions 
obvious great care taken designing scheme respect theoretical properties provisions satisfactory implementation required computations 
idea situation remedied allow winning method perturb losses losing method subject constraint losses losing method may lowered 
may cases alleviate problems situations plagued extreme losses competing method 
questions remain open respect approach 
sampling distribution obtained values null hypothesis 
unique simple way figuring losses perturb 
pursued ideas may worthwhile 
time underlines consider mean difference performance value test 
help reduce importance small values associated negligible reductions loss 
loss models considered chapter mainly developed continuous loss functions mind 
continuous loss functions outputs continuous tasks classification similarly handled access output class probabilities continuous 
quite common binary loss function classification 
quite obvious loss models binary losses 
clearly assumptions normality appropriate probably lead 
straightforward design appropriate models discrete losses allow necessary components variability 
empirical study tests difference performance learning methods binary classification appeared dietterich 
evaluation comparison chapter learning methods algorithms heuristics methods prerequisite measuring performance learning method defining exactly method may trivial statement detailed investigation reveals uncommon neural network literature find description algorithm detailed allow replication experiments see quinlan examples unusually detailed descriptions 
example article may propose part training data neural network validation set monitor performance training training minimum validation error encountered known early stopping 
refer description algorithm 
algorithm accompanied details implementation call heuristics order produce method applicable practical learning problems 
example heuristics include details network architecture minimization procedure size validation set rules determine minimum validation error reached appealing think performance comparisons terms algorithms heuristics 
example may wish statements linear models superior neural networks data domain 
case clearly talking algorithms argued empirical assessments supporting statements necessarily involve methods including heuristics 
hope cases exact details heuristics crucial performance method learning methods reasonable generalise results methods algorithm 
stressed experimental results involving methods objective basis subjective useful generalisations algorithms 
principled approach investigating sets heuristics algorithm extremely arduous address central issue attempting project experimental results novel applications 
focus attention automatic methods methods applied human intervention 
reason choice primarily concern reproducibility 
may argued practical problems allow human expert design special models take particular characteristics learning problem account 
rule usefulness automatic procedures aids expert 
may possible invent heuristics embody common sense expert turns extremely difficult endeavor 
approach try develop methods sufficiently elaborate heuristics method improved simple documented modification 
require methods automatic monitor progress algorithm take note cases heuristics break order able identify reasons poor performance 
primary target comparisons predictive performance methods 
reasonable completely ignore computational issues cpu time memory requirements 
algorithms may expect tradeoff predictive accuracy cpu time example training ensemble networks may expect performance improve ensemble gets larger 
wish study algorithms reasonably large amount cpu time disposal 
practical learning problems days cpu time fast computer typically excessive 
practical reasons limit computational resources hours task 
turns convenient practical point view develop heuristics particular amount cpu time algorithm choices amount time spent far example consider case training ensemble networks 
general reasonable heuristics problem difficult devise may hard determine long necessary train individual nets 
fixed time run algorithm may circumvent problem simply training nets equal amount time 
naturally may turn nets trained time may turn better choice methods train single net entire allowed period time trying ensemble nets 
convenient notion cpu time constraint methods may correspond realistic applications 
experiments algorithms tested different amounts allowed time performance measures usually possible judge algorithm perform better time 
choice methods thesis experiments carried different methods 
methods described remainder chapter methods relying gaussian processes developed chapter 
methods linear method called lin nearest neighbor method cross validation choose neighborhood size called knn cv rely simple ideas data modeling 
methods included base line performance give feel simple methods expected perform tasks 
versions mars multivariate regression splines method included 
method developed friedman supplied software 
method described detail thesis published friedman 
primary goal including methods provide insight neural network methods compare methods developed statistics community similar aims 
mlp ese method relies ensembles neural networks trained early stopping 
method included attempt thorough implementation commonly early stopping paradigm 
intention including method get impression accuracy expected widely technique 
mlp mc method implements bayesian learning neural networks 
software method developed neal 
method uses monte carlo techniques fit model may fairly computer intensive 
time may expect method predictive performance 
strong competitor 
learning methods obvious interest include promising methods study 
algorithms particular interest include evidence framework developed mackay methods relying weight decay pruning ideas le cun 
turned difficult design appropriate automating heuristics algorithms 
evidence methods quite sensitive initial values regularising constants may large networks 
algorithms difficult automatically select network sizes reasonable numbers training iterations 
motivations delve project enable advocates various algorithms heuristics algorithms tested delve environment 
avoids common problem people comparing highly tuned methods versions competing methods 
considerations motivated decision implement large number methods 
results constitute challenge designers methods 
sections contain descriptions discussions methods studied gaussian process methods discussed chapter 
general comments methods include detailed specifications kind preprocessing 
assumed default preprocessing delve applied 
datasets considered thesis binary values encoded values real inputs rescaled linear transformation training set zero median unit average absolute deviation median 
intended robust way centering scaling attributes 
problems scaling inappropriate relative scales inputs convey important information special cases considered 
performance measurements focus loss functions squared error loss absolute error loss negative log predictive loss 
evaluation loss type requires method produces predictive distribution loss negative log density test targets predictive distribution 
interesting aspect loss type method forced know size uncertainties predictions order able contrast situation commonly loss functions 
linear model lin linear model lin linear model popular models data analysis 
reason popularity conceptually computationally simple fit linear model resulting model easily intuitive representation 
prominent deficiency linear model strong assumption true relationship data data conform linear model predictions may inaccurate confident 
implementation linear model called lin 
details implementation appendix algorithm fitting linear model known heuristic necessary principled way handling situation linear system determine parameters model close singular 
simplicity initially assume targets scalar simple extension handle multiple outputs 
linear model defined model parameters wm bias inputs augmented xm take care bias 
model fit training data fx ji ng maximum likelihood assuming zero mean gaussian noise variance oe likelihood gamma fi fi oe delta exp gamma gamma gamma delta oe maximum likelihood estimate weights wml independent oe minimizing cost function gamma gamma delta respect model parameters 
notice gaussian noise assumption gives rise squared error cost function cost function regardless loss function choose evaluate linear model 
solution learning methods optimization problem known linear algebra solution unique gamma gamma delta wml gamma ii ill conditioned numerical evaluation eq 
may troublesome fairly accurate solutions obtained necessarily lead model predictions 
attempt define method high degree reproducibility produce results machine propose choosing directions input space insufficient variation training set ignored model 
conveniently computed singular value decomposition svd see example press 
decomposition diag orthonormal matrices vector length containing singular values regularised gamma computed setting small gamma diag exact criterion regularisation set gamma max close singular 
constant gamma chosen conservative estimate machine precision interfere conditioned 
condition number depends scale inputs procedure conjunction standard provided delve 
modified maximum likelihood weights computed wml gamma derive predictive distribution model 
simplicity derive predictive distribution fixed estimate noise account uncertainty predictions arising estimated noise inherent data uncertainty estimate weights 
noise estimated standard unbiased estimator oe gamma gamma wml gamma delta number parameters wml fit data minus singular value reciprocal set zero eq 

estimate may break training cases case compute predictive distribution 
assuming improper uniform prior weights linear model lin posterior weights proportional likelihood 
predictive distribution test case input gamma fi fi oe delta gamma fi fi oe delta gamma wjd oe delta gamma fi fi oe delta gamma fi fi oe delta exp gamma gamma gamma delta oe gamma gamma wml gamma wml gaussian integral solved exactly get gaussian predictive distribution mean variance gamma fi fi oe delta wml oe gamma optimal point prediction symmetric loss function wml 
consequently predictions absolute error squared error loss functions 
negative log density loss function compute log density targets predictive gaussian distribution log gamma fi fi oe delta gamma log gamma gamma tasks multiple outputs re decomposition eq 
set weights 
maximum likelihood weights inherent noise estimates eq 
target attribute point predictions obtained maximum likelihood weights 
log density predictions assume joint density targets gaussian diagonal covariance matrix density targets obtained summing log domain contributions form eq 

equivalent assuming residual errors outputs independent 
completes definition linear model recipe model uniquely defined data 
elaborations basic linear model exist pursued 
computational complexity involved fitting lin nm computing decomposing matrix respectively 
fairly large tasks usually considered trivial scales linearly learning methods nearest neighbor models knn cv nearest neighbor models popular non parametric memory models 
consider simple nearest neighbor model context delve called knn cv 
attempt define nearest neighbor method reproducible results depend heavily details implementation free parameters behaves reasonably broad variety conditions 
wish method applicable standard loss functions absolute error squared error negative log density loss 
simple nearest neighbor models require training 
algorithm making predictions involves searching training cases find inputs closest inputs test case kind weighted average targets neighbors prediction 
neighborhoods defined terms kernel gaussian supplies weighting factors terms number neighbors hybrids 
methods kernels intuitively appealing ability weight neighbors distance 
firstly involve somewhat arbitrary choice kernel shape secondly choice width kernel continuous equivalent discrete choice may plagued local minima width expressed terms distances neighbors 
procedure unsuitable reproducible base line nearest neighbor approach 
nearest neighbor approach uniform weighting neighbors predictions 
sophisticated methods certainly possible regression tasks common fit local linear models neighbors loess cleveland pursued 
need define metric measure closeness simple common choice euclidean distance 
extensions nearest neighbor algorithms exist attempt adapt distance metric lowe hastie tibshirani complicated algorithms pursued 
need resolve cases distance th nearest neighbor 
attempt algorithm sensitive round effects different floating point arithmetic implementations set small distance tolerance cases deemed tied distances 
cases defined ties squared distances test point differ gamma propose scheme making prediction test case nearest neighbor models knn cv 
sort cases distances placing ties arbitrary order 

find neighbors distance th case ordering including cases earlier list average targets 

include average ties final prediction weight equal number ties earlier list 
procedure small tolerance deciding cases ties help avoiding differences due finite precision arithmetic 
need find appropriate neighborhood size general expect optimal choice depend particular dataset number available training points 
number training cases increases nearest neighbor method consistent consistent estimator gets right answer limit infinitely large sample size grows 
appealing idea find best leave cross validation 
procedure leave training cases turn find nearest neighbors handling ties appropriately compute loss associated predictions neighborhood 
repeated gamma selected smallest average loss 
leave procedure gives unbiased estimate situation gamma training cases case fairly large close optimal current training set size 
neighborhood size estimated desired loss function 
squared error loss function average targets nearest neighbors predictions 
outputs multidimensional averages computed dimension 
absolute error loss median dimension prediction 
special problems negative log density loss 
form predictive distribution nearest neighbors 
simplest idea fit gaussian targets neighbors weighting neighbors uniformly 
mean gaussian chosen simply empirical mean nearest neighbors 
special cases arise estimating variance 
firstly leave procedure estimates estimate mean variance predictive distribution single nearest neighbor 
secondly neighbors exactly targets targets differ slightly naive empirical variance estimate undesirable leads unbounded losses 
problems learning methods addressed regularising variance estimate gamma gamma delta regulariser targets th nearest neighbor average targets nearest neighbors 
convenient value choose average squared difference targets corresponding nearest neighbors average extending training cases 
interpreted global estimate variance available modified local information 
regulariser act conjugate prior 
case local information variance available sum vanish eq 
leaving just global term 
grows amount local information increases importance regulariser decreases 
problem vanishing variance estimates addressed approach normalisation targets learning guarantees pathological case available training cases identical targets 
computational demands leave procedure training knn cv model depend implemented 
need leave cases turn compute distance neighbors takes total time 
need sort distances requiring log 
gamma possible values need compute mean variance median neighbors 
simple implementations computation values take estimates 
noted possible re partial results obtained values means variances updated constant time median heap data structure updated log 
total computational effort involved finding gamma logn delta simple implementation gamma log delta elaborate implementation 
current version knn cv uses simple implementation 
appropriate value predictions time gamma mn log delta test case prediction algorithm sorts training cases distance current implementation knn cv uses approach 
sorting attempt locate closest neighbors replace log previous expression clear best general expect optimal grow sub linear rate grows 
mars bagging mars bagging multivariate adaptive regression splines mars method friedman tested 
fairly known method non linear regression high dimensional data statistics community 
detailed description mars see friedman 
simplistic account mars gives flavor method 
input space carved overlapping regions splines fit 
fit built constructive phase introduces input regions splines followed pruning phase 
final model form sum products univariate splines continuous function continuous derivatives additive sets variables allowed interact 
friedman supplied fortran implementation mars version 
versions method tested 
original implementation delve name mars 
mars computationally demanding conjunction bagging procedure breiman 
method trains mars number bootstrap samples training set averages resulting predictions 
bootstrap samples generated sampling original training set replacement samples size original training set 
set predictions generated regardless loss function predictions absolute error loss function squared error loss function 
negative log density loss function applicable current version mars 
bagged version mars referred mars bag 
parameter settings mars bootstrap repetitions bagging procedure maximum number basis functions maximum number variables allowed interact 
computational cost applying method fairly modest 
training set inputs training cases bootstrap replications take total minutes mhz processor 
neural networks trained early stopping mlp ese method predictions average outputs multi layer perceptron neural networks trained training data time determined validation remaining 
networks identical architectures single learning methods hidden layer hyperbolic tangent units 
early stopping technique avoiding overfitting models iterative learning procedures 
uses model expected large capacity early stopping ensures model fit 
consequently approach helpful overcoming bias variance dilemma frequentist methods geman 
early stopping help avoid overfitting necessarily need accurate estimate required model capacity network large number hidden units 
fraction training cases held validation performance set monitored iterative learning procedure applied 
typically validation error initially decrease model fits data better better model begins fit validation error starts rising 
idea learning soon minimum validation error achieved 
neural networks great advantages early stopping simplifies difficult question long train model 
number details specified method practically applicable 
section contains discussion issues involved decisions implement method 
stress am claiming choices optimal sense tried define automatic method obvious documented way improved 
multi layer perceptron neural network single hidden layer hyperbolic tangent units linear output unit 
units biases 
network fully connected including direct connections inputs outputs 
network inputs hidden units implements function tanh gamma ih delta hidden output weights output bias direct input output weights hidden biases ih input hidden weights 
number hidden units chosen smallest number weights large total number training cases removal validation cases 
experimental evidence number hidden units important long large 
need decide large fraction training cases validation 
obvious conflicting interests want large validation set achieve estimate performance want large training neural networks trained early stopping mlp ese set able train complex model complex model supported entire training data set 
non linear models trained finite amounts data helpful theoretical results trade 
chose third training cases rounded necessary validation rest training 
question exact criterion training want merely small fluctuations validation error occur 
number complicated stopping criteria proposed literature convincingly shown outperform 
simple scheme training iteration smallest validation error lies backwards run 
way trained epochs necessary fairly confident minimum 
validation line search training run training conjugate gradients 
validation set half size training set validation requires forward pass validation typically slow training typically line search involved iteration requires average forward backward passes alarming 
compute efficient strategies employed validating longer intervals risk finding worse stopping times 
simpler method steepest descent fixed step size optimization technique probably sufficient validate longer intervals conjugate gradient technique involves line searches potentially take large steps 
idea including validation data training set continuing training appealing expected better performance achieved larger training sets 
problem additional training presents 
suggestion continue training combined set training error case early stopping achieved 
theoretical point view idea reasonable average case analysis expected validation cases slightly larger error initially training cases 
practical problems validation error lower case training error 
impossible achieve training error training data 
scenarios bad occur frequently possible generalisation performance decreased procedure 
including training data retraining effectively data training ensemble networks trained different validation set chosen random 
convenient way combining benefits averaging ensembles ability cases training 
different validation learning methods sets net choose networks include ones best validation error variation validation error due differing validation sets 
consequently include trained nets final ensemble 
complicated hybrid scheme networks trained validation set contemplated simplicity pursued 
additionally take care cases early stopping early late 
initial phases training conceivable validation error fluctuates quite wildly may cause stopping criteria fulfilled extra epochs may reliable indicator local minima 
rule scenario require training run uses total available training time iterations done minimization algorithm converges 
limit number possible members final ensemble probably adequate getting benefit averaging 
hand may validation error keeps decreasing stopping criterion satisfied 
recall expected behaviour validation error increase fitting sets guarantee happen particular run 
hand may simply takes large number epochs train network 
resolve dilemma terminate training uses total computational resources available final ensemble contain networks 
technically criteria conflicting actual implementation conditions deciding training particular net order precedence network converged allowed cpu time training net iterations training completed minimum validation error achieved backwards run total available cpu time net 
method run automatically specifications limit allowed amount cpu time specified 
default chosen allowed cpu time scale linearly number training cases 
default allows seconds training case means net training cases allowed train minutes 
bayesian neural network monte carlo mlp mc bayesian neural network monte carlo mlp mc bayesian implementation learning neural networks monte carlo sampling developed neal 
computation intensive method shown encouraging performance neal study datasets rasmussen 
full description method reader referred neal 
brief description algorithm heuristics employed 
feed forward multi layer perceptron neural network single hidden layer hyperbolic tangent units network fully connected including direct connections input output layer 
output units linear 
units biases 
network identical mlp ese method eq 

network parameters prior distributions specified terms hyperparameters 
predictions monte carlo samples posterior distribution weights 
network weights hyperparameters collectively termed 
posterior distribution bayes rule jd jx prior jx likelihood 
assume noise independent gaussian variance oe don know magnitude noise attempt infer data 
put gamma prior inverse variance oe gamma known precision 
gamma density gamma ff ff gamma exp gamma ff mean ff known shape parameter 
corresponding oe ff intended vague prior allowing noise variances gamma past unity 
density resulting prior depicted fig 

network weights assigned groups identity units connect 
groups weights output biases hidden biases input hidden weights hidden output weights direct input output weights priors network weights different group weights 
terms hierarchically specified distributions higher level hyperparameters shared weights group introducing dependencies weights 
output biases zero mean gaussian priors oe standard learning methods oe standard deviation noise oe non normalised prior noise level oe 
function oe plotted oe log scale allow usual interpretation probabilities proportional areas curve 
deviation oe 
targets training set normalised delve roughly zero mean unit variance prior accommodate typical values 
group hidden unit biases hierarchical prior consisting layers oe distribution oe specified terms precision oe gamma gamma form eq 
parameters ff 
attempted plot resulting prior weights obtained integrating dependencies weights common value difficult give faithful representation distribution 
hidden output weights similar layer prior parameters ff gamma number hidden units 
prior scaled number hidden units variance weights inversely proportional number hidden units 
scaling accomplishes invariance prior magnitude output signal respect number hidden units 
decoupling signal magnitude model complexity useful setting priors considering networks numbers hidden units tending infinity see section 
input hidden weights layer prior weight zero mean gaussian prior oe corresponding precision weights input unit gamma prior mean shape parameter ff gamma ff 
mean determined top level gamma distribution bayesian neural network monte carlo mlp mc mean number inputs shape parameter ff gamma ff 
prior variance weights scales proportional gamma done accordance subjective expectation inputs single important predictions 
scaled prior constant number inputs important large value oe number inputs increase 
direct input output connections prior 
mentioned layer prior incorporates idea automatic relevance determination ard due mackay neal discussed neal 
hyperparameters associated individual inputs adapt relevance input unimportant input grow large governed top level prior forcing oe associated weights vanish 
likelihood prior compute posterior distribution network weights 
order predictions integrate posterior 
predictive distribution target corresponding novel input gamma fi fi delta gamma fi fi delta jd gamma fi fi delta samples drawn posterior distribution 
neural network models integral handled analytically 
employ hybrid monte carlo algorithm duane obtain samples posterior approximate predictive distribution 
method combines metropolis algorithm dynamical simulation helps avoid random walk behavior simple forms metropolis essential wish explore weight space efficiently 
hyperparameters updated gibbs sampling 
sampling posterior weight distribution performed iteratively updating values network weights hyperparameters 
iteration involves components weight updates hyperparameter updates 
cursory description steps follows 
weight updates done hybrid monte carlo algorithm 
algorithm gp mc method section extensive explanation 
fictitious dynamical system generated interpreting weights positions augmenting weights momentum variables purpose dynamical system give weights inertia random walk behaviour avoided ex learning methods weight space 
total energy system sum kinetic energy function momenta potential energy potential energy defined exp gammae 
sample joint distribution exp gammae gamma marginal distribution posterior 
sample weights posterior obtained simply ignoring momenta 
sampling joint distribution achieved steps finding new points phase space near identical energies simulating dynamical system discretised approximation hamiltonian dynamics changing energy doing gibbs sampling momentum variables 
hamilton order differential equations approximated series discrete order steps specifically leapfrog method 
derivatives network error function enter derivative potential energy computed back propagation 
original version hybrid monte carlo method final position accepted rejected depending final energy necessarily equal initial energy discretisation 
modified version uses average probability domain window states neal 
step size discrete dynamics large possible keeping rejection rate low 
step sizes set individually heuristic approximations scaled parameter 
iterations window size step size simulations 
momentum variables updated modified version gibbs sampling allowing energy change 
persistence new value momentum weighted sum previous value weight value obtained gibbs sampling weight gamma horowitz 
form persistence momenta change approximately times slowly increasing inertia weights help avoiding random walks 
larger values persistence increase weight inertia reduce rate exploration advantage increasing weight inertia way increasing hyperparameters updated shorter intervals allowing adapt rapidly changing weights 
hyperparameters updated gibbs sampling 
conditional distributions hyperparameters weights gamma form efficient generators exist top level hyperparameter case layer priors weights inputs case conditional distribution complicated bayesian neural network monte carlo mlp mc method adaptive rejection sampling gilks wild employed 
network training consists levels initialisation sampling network weights prediction 
level initialisation hyperparameters standard deviations gaussians kept constant hyperparameters controlling output weights hyperparameters weights set zero 
weights allowed grow leapfrog iterations hyperparameters remaining fixed 
neglecting phase cause network get caught long time state weights hyperparameters small 
markov chain described invoked run long desired eventually producing networks posterior distribution 
initial nets discarded algorithm may need time reach regions high posterior probability 
networks sampled remainder run saved making predictions eq 

samples posterior approximate integral 
probably predictions get vastly different samples approximation avoided disk requirements storing network samples prohibitive 
output unit linear final prediction seen coming huge fully connected ensemble net appropriately scaled output weights 
size individual nets rule want network parameters training cases lower limit hidden units upper limit 
hope fitting region 
larger nets probably gain face limited training data avoided computational reasons 
runs parameter values 
check necessary rejection rate stays low say step size lowered 
runs reported adequate 
parameters concerning monte carlo method network priors selected intuition experience toy problems 
parameters need set user save specification allowable cpu time 
learning methods chapter regression gaussian processes chapter presents new method regression inspired neal neal priors infinite networks pursued williams williams rasmussen 
gaussian process gp models equivalent bayesian treatment certain class multi layer perceptron networks limit infinitely large networks 
gaussian process model large numbers network weights represented explicitly difficult task setting priors network weights replaced simpler task setting priors gp 
close relatives gp model appeared various guises literature 
essentially similar approach taken hagan hagan surprisingly spurred general interest 
thesis extends hagan adapting model parameters associated likelihood 
gaussian process models analysis computer koehler owen presumably applicable modelling noise free data 
undue restriction shown chapters gp models attractive modelling noisy data 
approach variational analysis regularised function approximation taken poggio girosi girosi related spline models studied wahba 
approaches cross validation generalised cross validation gcv estimate regularisation parameters 
approaches may regression gaussian processes viable large numbers regularisation constants required automatic relevance determination ard regularisation schemes 
gp models close correspondence variable metric kernel methods lowe 
exposition versions gp models formulated terms probabilistic model bayesian setting 
simplest approach parameters controlling model optimized maximum aposteriori map approach 
second version bayesian formalism taken allow integration parameters 
neighbors large neural nets covariance functions methods relying gaussian processes described chapter differ respects methods commonly data modeling consequently may easy get intuitive feel model works 
order help readers familiar conventional methods start discussion gp model analogy kernel smoother models 
help subsequent formal account 
simple calculation show neural networks gaussian priors weights imply gaussian process priors functions 
kernel smoother consists kernel function local model 
kernel function defines neighborhood giving weights training examples local model fit weighted examples predictions 
gaussian tri cubic functions common choices kernel functions 
kernel function function model inputs returns weighting training example fitting local model 
kernel functions depend euclidean distance gaussian kernel exp gammad oe distance kernel center input training case 
properties smoother controlled width kernel expressed terms numbers neighbors selected cross validation 
advanced methods flexible kernels referred variable metric models explored lowe 
computational arguments favour kernels put non zero weight small fraction training cases 
predicting output novel test input kernel centered test input weightings training cases determined 
common choice local model linear model fit weighted squares cleveland 
gp methods role kernel function local model integrated neighbors large neural nets covariance functions covariance function 
kernel function covariance function function model inputs returning weighting training case test input returns covariance outputs corresponding inputs 
kernel method gaussian exp gammad oe euclidean distance reasonable choice covariance function note name gaussian process refer form covariance function 
assuming mean output zero covariance outputs corresponding inputs defined 
inputs judged close covariance function outputs highly correlated quite similar 
gaussian process model assume outputs finite set cases joint multivariate gaussian distribution covariances covariance function 
predictions considering covariances test case training cases enable compute output test case 
fact obtain entire predictive distribution test output assumptions gaussian 
idea gaussian processes directly inspired investigations neal priors weights neural networks 
consider neural network inputs single output unit single layer tanh hidden units 
hidden output units biases network fully connected consecutive layers tanh gamma ih delta weights zero mean gaussian priors standard deviations hidden weights ih hidden biases oe oe respectively output weights output bias standard deviations oe oe sort prior suggested mackay 
consider distribution network output prior distribution weights specific input contribution hidden unit mean zero independent 
variance contribution hidden unit finite oe bounded 
setting conclude central limit theorem number hidden units tends infinity prior distribution converges zero mean gaussian variance oe hoe 
selecting value oe scales inversely obtain defined prior limit infinite numbers hidden units 
regression gaussian processes similar argument joint distribution inputs converges limit infinite multivariate gaussian means zero covariance gamma delta gamma delta oe hoe defines gaussian process crucial property joint distribution finite set function values gaussian 
attempt characterize covariance functions implied weight priors neural networks see neal 
preceding paragraphs meant motivate investigation models relying gaussian processes 
correspondences models play crucial role shown model data purely terms gaussian processes 
require reader accustomed thinking distributions functions terms covariances hopefully aided illustrations functions drawn various gaussian process priors 
predicting gaussian process formally gaussian process collection random variables fy indexed set finite subset joint multivariate gaussian distribution 
typical application gaussian processes field signal analysis random variables indexed time 
contrast case index random variables input space dimensionality gaussian process fully specified mean covariance function 
consider gp mean zero 
random variable model output gp model input covariance function inputs theta gamma gamma delta gamma gamma delta 
called covariance function matrix covariances pairs examples referred covariance matrix 
section shown parameterise covariance function consider form 
presentational convenience index training test cases superscripts brackets output associated input goal usual compute distribution jd scalar output test input set training points ji ng 
note distinguish model outputs training set targets predicting gaussian process random variables differ targets noise corrupted versions outputs 
introduce stochastic variables modeling function corresponding inputs note formulation different common frequentist model single random variable model conditional distribution tjx targets inputs 
example help clarify difference consider linear model coefficients vector fi linear model noise contributions fi noise 
frequentist framework single random variable possible model output deterministic function inputs parameters fi estimated example maximum likelihood 
bayesian approach contrary model parameters treated random variables model outputs obtained integration model parameters introducing stochasticity dependencies outputs requiring separate random variables 
fact gp model quantities modeled covariances outputs 
proceed assigning multivariate gaussian prior distribution variables gamma fi fi delta exp gamma sigma gamma sigma gamma delta note prior specifies joint distribution function values noisy targets inputs 
prior far targets training cases considered 
reasonable prior specify expect larger covariance function values corresponding nearby inputs function values corresponding inputs apart 
covariance functions discussed section 
convenient partition prior covariance matrix manner sigma contains covariances pairs training cases vector covariances test case training cases prior covariance test case 
regression gaussian processes likelihood relates underlying function modeled variables observed noisy targets noise assumed independent gaussian unknown variance gamma fi fi delta exp gamma gamma omega gamma gamma omega gamma gamma theta identity matrix 
gain notational convenience written joint distribution conditioning distribution depend variable 
formally vector equation elements observed targets vector simply augmented element value inconsequential 
bayes rule combine prior likelihood obtain posterior distribution gamma fi fi delta gamma fi fi delta gamma fi fi delta gamma fi fi delta constant proportionality equation normalisation constant independent variables 
note general necessary compute normalisation terms previous equations 
posterior gamma fi fi delta exp gamma sigma gamma gamma gamma omega gamma gamma exp gamma gamma ym theta sigma gamma omega gamma gamma ym ym vector posterior means 
posterior distribution gaussian inverse covariance sum inverse covariances prior likelihood 
posterior mean product takes maximum value consequently differentiation log gamma fi fi delta ym theta sigma gamma omega gamma gamma omega gamma interested distribution necessarily wish attempt evaluate expression 
notice posterior means ym necessarily coincide corresponding target values 
compute distribution need marginalise eq 
yields gaussian distribution mean variance gamma fi fi delta oe gamma oe gamma gamma predicting gaussian process gamma delta left panel shows standard deviation contour joint gaussian distribution single training case test point situation single training point 
target value training point observed condition value indicated dotted vertical line 
gives rise predictive distribution right 
vertical axes plots scale 
completes analysis model access desired predictive distribution 
depending loss function particular application optimal predictions making point predictions minimize expected loss 
notice order result invert matrix size theta slightly different view formalism may lead better intuitive understanding going 
starting middle line eq 
directly marginalise interested obtain delta delta delta gamma fi fi delta dy delta delta delta dy gamma fi fi delta exp gamma sigma gamma omega gamma indicates vector targets augmented condition observed values targets training set standard rule conditioning gaussians recover result eq 

conditioning illustrated fig 

regression gaussian processes parameterising covariance function previous section seen derive predictive distribution test cases covariance function 
section discusses various choices covariance functions 
possible choices prior covariance functions 
formally required specify function generate non negative definite covariance matrix set input points 
modeling point view wish specify prior covariances contain prior beliefs structure function modeling 
case bayesian modeling turns convenient specify priors terms hyperparameters values distributions specified priori adapted model fit training data 
example clarify idea covariance function extensively thesis sets covariance points gamma delta exp gamma gamma gamma delta log wm vector parameters playing role hyperparameters log applied elementwise 
note noise level included hyperparameters doesn appear covariance function treated way hyperparameters analysis 
covariance conceptually components 
part involving parameters controls scale bias linear contributions covariance 
understand form contributions consider linear function ff ff gamma coefficients ff considered random variables mean giving ff variables independent zero mean distributions finite variance compute covariances theta ff ff gamma gamma gamma gamma parameterising covariance function variance ff assumed remaining coefficients common variance pre processing data tests inputs mean median zero approximately drop term recover form linear contribution covariance eq 

may worry common variance inputs may dimensionally inconsistent inputs measured different units 
naturally may group variables common variance differently particular choice allow separate hyperparameter input dimension 
contributions linear terms covariance functions may large inputs quite distant bulk training examples 
probably useful mechanism extrapolating data points boundary training examples term covariance local interactions close zero prediction large distances close zero 
part covariance function eq 
expresses idea cases nearby inputs highly correlated outputs 
parameters multiplied coordinate wise distances input space allow different distance measures input dimension 
irrelevant inputs corresponding small order model ignore inputs 
characteristic lengths input directions gamma parameter gets large resulting function short characteristic length function vary rapidly corresponding axis indicating input high importance 
idea closely related automatic relevance determination ard idea mackay neal 
associated variable gives scale local correlations 
discussion term referred ard term 
covariance eq 
extensively experiments thesis 
choices may reasonable 
choices discussed briefly chapter 
example functions drawn random prior covariance function plotted fig 

theta mesh plots represent single sample drawn random dimensional joint gaussian distribution 
random samples obtained multiplying vector zero mean unit variance gaussian random numbers cholesky factor covariance matrix requiring mb memory minutes cpu time 
regression gaussian processes functions drawn random ard prior covariance functions 
input dimensions ard part covariance eq 
considered 
left plot ard hyperparameters corresponding characteristic lengths respectively 
plot right ard hyperparameters set 
expected functions generated prior smooth 
plots 
adapting covariance function far considered properties model fixed values hyperparameters 
discuss adapt hyperparameters light training data 
log likelihood hyperparameters higher level log dj log gamma fi fi delta gamma log det gamma gamma gamma log possible express analytically partial derivatives log likelihood form basis efficient learning scheme 
derivatives log gamma jx delta gamma trace gamma gamma gamma order compute parameters predictive distribution eq 
necessary invert matrix need partial derivatives likelihood 
covariance matrix guaranteed positive definite invert cholesky decomposition golub van loan requires multiply accumulate operations 
cholesky decomposition produces determinant matrix needed evaluate likelihood 
remaining computations involved evaluating likelihood partial derivatives vector matrix multiply evaluation trace product order 
main computational obstacle computation gamma certainly feasible workstation computers 
adapting covariance function larger values computations demanding example inversion takes minutes cpu time mhz processor 
learning schemes possible availability partial derivatives likelihood 
maximum likelihood implemented employing gradient descent 
may specify prior hyperparameters hyperprior compute posterior hyperparameters 
possible implementations covered sections conjugate gradient approach maximum aposteriori map estimation monte carlo method integrating hyperparameters 
discuss hyperprior 
priors map method monte carlo approach 
prior assumes training data normalised roughly zero mean unit variance 
priors log gaussian mean gamma standard deviation corresponding fairly vague priors 
prior log gaussian mean gamma standard deviation 
prior reasonable gives variance signal predicted ard term covariance function targets assumed normalized roughly unit variance expect hyperparameter log domain vicinity tasks low noise lower tasks high noise 
priors ard hyperparameters complicated 
wish introduce scaling prior number inputs small reasonable say parameters highly relevant larger probably somewhat inputs highly relevant 
consequently expect prior importance ard hyperparameters lower increasing numbers inputs 
derivations neal employ gamma prior mean scales number inputs inverse hyperparameters gamma priors gamma ff ff gamma ff gamma ff gamma exp gammaw gamma ff ff gamma mean gamma chose ff looking plots distributions see fig 
may difficult decide inputs thought relevant attempt top level hyperparameter set vague prior 
attempted current implementation 
resulting prior parameters log gamma gamma exp gamma gamma exp gamma naturally prior may appropriate tasks may tasks large number inputs relevant 
prior vague regression gaussian processes theta ard hyperparameter log scaling gamma prior ard hyperparameters 
theta plotted log eq 

note expected number hyperparameters corresponding important inputs cases larger values mass prior moves lower values indicating expect larger proportion inputs relevant 
hyperparameters may grow large values likelihood strongly suggests 
benefit putting pressure hyperparameters remain small may help learning procedures locating areas high posterior probability 
consider situation early learning hyperparameters time adapt hyperparameters random irrelevant ones take large values exponential eq 
may take small values 
tend attenuate partial derivatives likelihood turn may learning slow 
way saying initial search regions high posterior probability may slow prior likelihood different 
maximum aposteriori estimates maximum aposteriori map estimates conjugate gradient optimization technique discussed detail appendix locate local maximum posterior 
approach advantage reasonable approximation local maximum relatively function gradient evaluations 
preferred approach large number training cases hybrid monte carlo monte carlo computationally infeasible 
large number training cases generally expect posterior fairly narrow predictions map method may differ results integrating hyperparameters feasible 
risk conjugate gradient optimization technique may get stuck bad local maxima 
algorithm greedy get stuck shallow local maxima 
difficult say big problem current context 
attempt clarify trying multiple random restarts optimization pursued 
just single run allowing function gradient evaluations time likelihood changing slowly cases example hyperparameters 
noted resulting map estimates hyperparameters may useful interpretation data 
linear non linear parts function separated different sets hyperparameters small ard hyperparameters may carry linear contributions magnitude different ard hyperparameters may convey relative importance input attributes relation characteristic lengths 
initialisation hyperparameters fairly important may inappropriate initial values partial derivatives likelihood small creating problems optimization algorithm 
works set exp gamma 
hybrid monte carlo bayesian formalism multiply prior likelihood dj eq 
integrate resulting posterior 
unfortunately likelihood complex form analytical integration infeasible 
approximate integral markov chain sample posterior gamma fi fi delta gamma fi fi delta jd gamma fi fi delta regression gaussian processes samples posterior distribution 
note terms sum gaussian distributions means approximate predictive distribution mixture gaussians identical mixing proportions 
number samples grows approximate predictive distribution tend true predictive distribution 
note predictive distribution may complex form multi modal 
hybrid monte carlo hmc method duane promising application 
attempting sample complicated multidimensional distributions advantageous gradient information seek regions high probability cases gradients computed 
markov chain takes series small steps exploring probability distribution ensuring regions visited probability 
problems simple implementations idea distributions explored random walks poor exploratory properties 
hybrid monte carlo method avoids random walk behaviour creating fictitious dynamical system plays role position variables augmented set momentum variables oe 
combined system tends avoid random walks momentum introduce inertia hyperparameters tend keep system moving direction successive steps 
formally total energy system sum kinetic energy potential energy total hyperparameters kinetic energy function associated momenta oe oe particle mass potential energy defined way jd exp gammae 
sample joint distribution oe exp gammae gamma marginal distribution required posterior 
sample hyperparameters posterior obtained simply ignoring momenta 
sampling joint distribution achieved steps finding new points phase space near identical energies simulating dynamical system discretised approximation hamiltonian dynamics ii changing energy doing gibbs sampling momentum variables 
variation hmc due horowitz 
approach having defined parameters ffl ff transitions markov chain take place scheme 
starting current state oe perform leapfrog steps step size ffl re hybrid monte carlo state oe 

probability min exp oe gamma oe accept new state oe oe reject new state retain old state negated momenta oe gammaoe 

update total energy system perturbing momenta oe gamma ff drawn random zero mean unit variance gaussian 
hamilton differential equations govern evolution dynamical system fictitious time oe oe doe gamma gamma practice simulate equations exactly partial derivative respect complicated function 
leapfrog iterations approximate dynamics oe ffl oe gamma ffl gamma delta ffl ffl oe ffl oe ffl gamma ffl gamma ffl delta formal proof correctness approach sampling posterior context neural networks neal neal 
current implementation single leapfrog iteration find proposal states 
entirely clear longer trajectories standard algorithm better persistence see discussion neal 
step sizes ffl set value hyperparameters 
may fairly hyperparameters log domain step sizes implicitly scale magnitude underlying parameter 
step size chosen scale ffl gamma magnitude gradients typical point posterior expected scale roughly prior vague 
particle mass arbitrarily set constant proportionality step sizes chosen give low rejection rate ffl gamma typically gives rejection rates 
momenta updated persistence ff 
total energy change approximately times slowly persistence 
advantage regression gaussian processes persistence consecutive steps hyperparameter space tend direction momenta avoiding random walks 
particularly important posterior distribution highly correlated case exploration random walks extremely slow 
posterior distributions hyperparameters current context generally highly correlated open question 
reasonable cases introduce little bit persistence help distributions correlated slow progress distributions fairly spherical situations sampling fairly easy task 
predictions markov chain run long desired save states regular intervals predictions 
discard initial run chain converge posterior distribution 
samples evenly distributed remainder run predictions 
probably reasonable expect fewer samples suffice predictions gp mc needed mlp mc model effectively implicit integration weights 
predictions averaging predictive distributions samples posterior 
predictive distribution mixture gaussians 
noted making predictions may computationally intensive 
finding inverse covariance matrix posterior samples evaluate eq 
test point 
test points may considerable effort 
directions seen priors implied large neural networks gaussian weight priors directly modeling implementing neural network 
dubious useful attempt approximate gaussian processes bayesian neural networks large numbers hidden units 
may situations neural network parameterisation provide insight 
noted neural networks non gaussian priors necessarily define gaussian processes useful models may class networks 
commonly neural networks fairly small numbers hidden units modeling purposes 
case bayesian learning small numbers hidden units usually originally intended samples predictions due misunderstanding program interface got 
long run times programs redefined method re running 
directions chosen computational reasons non bayesian settings fear overfitting 
conceivable modeling problems network hidden units approximate true function suspect general case 
form covariance neural network determined shape activation function hidden units 
reasons tanh units particularly advantageous 
statistical point view clear reason expect gaussian processes bayesian neural networks perform differently 
issue may clarified empirical tests chapter 
different implementations schemes suggested may whichever model effective implementation preferred 
number possible extensions basic ideas may interest 
include complicated covariance functions implement additive models extensions computational nature allow gaussian processes applied large datasets extensions classification tasks 
possibilities discussed briefly 
possibility attempt model additive functions gaussian processes 
additive functions involve multiple components depend subset inputs 
components interact additively 
additive models popular statistics hastie tibshirani reasons computational convenience enable discovery interpretable structure data 
additive functions gp framework attained simply adding ard terms form exponential eq 
governed separate set hyperparameters identical 
multiple sets ard hyperparameters may able discover additive structure data certain number inputs considered relevant covariance contributions 
see neal discussion additive models context neural networks 
computational overhead additive models fairly modest iteration needs calculate partial derivatives likelihood search hyperparameter values may demanding space larger 
example functions drawn random additive covariance functions shown fig 

additive components depends inputs 
ard hyperparameters active inputs values fig 

severe limitations gaussian processes computational requirements training sets get large 
able handle training cases regression gaussian processes functions drawn random additive ard prior covariance functions 
input dimensions ard terms covariance function depends single input variable 
left hand plot ard parameters active inputs right hand plot 
plots 
requiring mb memory implementation stores theta matrices time efficiency hours training time requirements scale quadratically number training cases difficult handle significantly larger training sets 
noted algorithms exist matrix inversion time algorithms easily implemented modest values thousands computational benefits overwhelming 
possibility consider approximate techniques matrix inversion 
approaches promising 
attempt covariance sparse zeroing terms small magnitude best conjunction removal linear term covariance matrix fit standard linear model apply gp residuals linear fit 
sparse covariance matrix corresponds considering points high covariance probably yield approximations say considering points highest covariance training points 
may difficult find algorithms take advantage form covariance matrix 
approximate scheme employed long trajectories leapfrog steps final acceptance rejection exact calculation simply approximate computations current scheme abandoning guarantee eventual convergence exact posterior 
secondly noted need entire inverse covariance matrix computations quantities xq gamma det trace gamma vector matrix 
terms approximated efficiently conjugate gradient directions monte carlo methods skilling skilling approximation implemented gaussian processes gibbs mackay 
gaussian processes classification explored barber williams 
regression gaussian processes chapter experimental results chapter contains results discussions empirical tests learning methods 
tests focused measuring comparing predictive performance real simulated datasets varying computational constraints 
experiments carried delve environment particular statistical framework developed chapter extensively 
methods tested described detail chapters 
sets experiments 
experiment addresses question application bagging mars procedure advantageous 
studies comparing variants method common important tool model development 
second set experiments attempts rigorously compare different methods real dataset known boston housing study 
data widely benchmarking purposes incorporated delve 
experiments aimed clarifying large uncertainties performance measurements may real datasets limited size 
number simulated datasets extensive comparison methods 
aim test complicated bayesian neural networks gaussian processes able outperform simpler traditional methods 
particular attention paid computational considerations attempt clarify computation time needed gain possible advantages complicated experimental results methods 
datasets delve experiments chapter data different sources 
boston housing dataset known real dataset previously benchmarking purposes 
data source kin pumadyn data families generated realistic robot arm simulator specifically benchmarking purposes 
delve environment learning problems specified hierarchical scheme naming convention 
datasets top level contain list cases called examples attribute values rudimentary information ranges possible values 
top level mention learnt 
level called level list attributes inputs outputs case supervised learning cases specified 
task level sizes training test sets desired loss function specified 
task level information problem specified expected loss method defined 
bottom hierarchy task instances containing actual training test sets methods run 
similar characteristics loosely categorised families 
boston housing data collected connection study air quality affects housing prices harrison 
data set publicly available uci database murphy aha delve 
dataset benchmarking studies neal quinlan 
dataset total cases containing values attributes see fig 

object predict median house value attributes 
delve dataset called boston associated called boston price data randomly divided cases training single common test set cases way anova model discussed section 
way model analysis losses chosen total number cases fairly limited 
sizes tasks generated containing instances training cases instances training cases instances training cases 
datasets delve crim capita crime rate town zn proportion residential land lots sq 
ft proportion non retail business acres town charles river dummy variable tract bounds river nox concentration parts rm average number rooms dwelling age proportion owner occupied units built prior dis weighted distances boston employment centres rad index accessibility radial highways tax full value property tax rate pupil teacher ratio town bk gamma bk proportion blacks town lstat lower status population median value owner occupied homes attributes boston dataset kin dist pumadyn accel families contain datasets synthetically generated realistic simulations robot arm 
families generated zoubin ghahramani simulator developed corke 
short descriptions datasets details refer delve web site 
static state robot arm defined terms lengths twists ff offset distances joint angles links arm 
dynamic state arm described including forces second order time derivatives variables denoted dot notation angular velocity joint tasks kin dist family predict distance point link robot arm fixed point function various state parameters 
tasks static geometric nature 
tasks family inputs 
tasks inputs inputs joint angles parameters set fixed values 
uniform noise added inputs noise modeled outputs expected gaussian 
kin dist tasks inputs inputs ff joint angles corrupted additive uniform noise ff target distance corrupted multiplicative uniform noise meant emulate noisy measurements 
experimental results pumadyn accel family contains datasets synthetically generated realistic simulation dynamics puma robot arm 
tasks associated datasets consist predicting angular acceleration link robot arm joint angles joint velocities joint torques multiplicative mass changes deltam multiplicative length changes deltal multiplicative perturbations viscous friction joints deltab gaussian noise added dimensional tasks inputs joint angles velocities links torques joint joint angles velocities torques set zero fluctuations masses lengths 
tasks inputs parameters allowed vary 
flavour task learn dynamical properties arm 
data families give rise different tasks various characteristics 
tasks names pumadyn fh encode aspects tasks 
number name denotes input dimensionality 
denotes fairly linear tasks non linear tasks 
trailing denotes medium high levels unpredictability noise 
definitions features terms indexes 
index non linearity nl el ec el squared error loss best linear fit noise free data ec squared error loss corresponding constant fit 
note ec variance target nl minus proportion variance explained linear model 
datasets nl classified fairly linear nl classified non linear 
index unpredictability defined en ec en expected squared error loss noisy data best non linear fit expected value function 
practice en estimated follows case dataset targets generated fixed inputs average variance cases targets taken measure en data classified having medium noise tasks high noise 
dataset contains cases divided evenly training test set 
large number cases tasks set hierarchical model described section 
tasks different sizes created training sets containing cases respectively 
smallest tasks applying bagging mars instances corresponding test sets contain cases largest task instances corresponding test sets comprise cases 
advantages data families large test sets various controlled attributes input size degree non linearity degree non predictability 
may enable pinpoint characteristics certain learning methods excel perform poorly 
experiments conducted delve environment 
different loss functions squared error loss absolute error loss negative log density loss 
evaluated measuring negative log density targets test set predictive distribution model 
mars bag mlp ese methods produce predictive distributions evaluated error measure 
losses reported standardized delve intuitive sense losses knowing domain data 
squared error loss standardization obtained dividing loss variance targets test cases causes trivial method guesses mean data loss approximately 
absolute error loss standardization done division absolute deviation median 
negative log probability standardization done subtracting loss obtained gaussian predictive distribution mean standard deviation test set 
standardized losses negative log density loss function approximately zero simple methods negative better methods 
applying bagging mars delve framework compare methods differ fundamental principles testing small refinements methods 
example sort attempt clarify bagging breiman improves performance mars method friedman comparing methods mars mars bag 
mars computationally demanding may possible bagging 
possible disadvantage bagging may increased difficulty associated interpretation composite model 
methods mars mars bag described section tested kin pumadyn data families 
results squared error loss function experimental results training set sizes kin fh kin fm kin nh kin nm kin fh kin fm kin nh kin nm pumadyn fh pumadyn fm pumadyn nh pumadyn nm pumadyn fh pumadyn fm pumadyn nh pumadyn nm averages comparison plain mars bagged mars bag kin pumadyn data families 
task expected decrease squared error introduced bagging percent theta gamma expected squared error loss plain mars expected squared error loss bagged mars 
columns marked give values percent rounded nearest integer pairwise comparison methods 
tabulated fig 

table shows bagging leads improved expected performance tasks total 
tasks decrease seen performance difference statistically significant lowest value 
tasks small training sets relative improvement substantial average training sets containing cases 
larger training sets benefits bagging decline 
surprising training examples plain mars probably get performance close theoretical limit inherent noise level data leave scope improvement bagging 
conceivable bagging may degrade performance large training sets 
experiments strongly favour bagging mars 
example statistically significant decrease performance bagging size dataset rule possibility existence tasks 
noted easy show theoretically bagging improves certain methods possible example ensembles hansen salamon 
individual members bag expected performance model trained plain mars 
empirical evidence suggests benefits averaging outweigh expected decline performance individual models 
experiments boston price similar experiments provide insights possible utility elaborate versions methods investigation 
example weight decay selected leave oneout cross validation implemented efficiently linear model may broad conditions helpful 
possibility extension knn cv incorporate running line smoother cleveland 
experiments boston price performance different learning methods evaluated different loss functions boston price 
cases analysis losses done way anova model described section 
results displayed special plot conveys information average losses standard errors values pairwise comparisons fig 

upper half plots estimated expected losses indicated small horizontal lines intersected vertical lines indicating standard error 
axis gives standardized losses 
losses different task sizes grouped rectangles training set sizes indicated rectangle 
inside rectangles losses methods horizontal ordering methods vertical ordering axis 
symbol indicates method run 
upward arrow indicates loss graph 
axis matrix values pairwise comparisons obtained paired tests tests 
values rounded nearest number percent reported digit percent 
strongest value weakest reported value 
value larger considered significant reported 
values reported column winning method 
looking column wise matrix tells methods current method significantly performs 
looking row wise see methods significantly performed method row 
dots matrix simply included help guide eye 
matrices easily read row wise 
note possible causes absence entry row row method performed worse perform statistically significantly better 
presence entry row means row method significantly outperformed 
results tests boston price data fig 
squared error experimental results boston price lin knn cv mars mlp ese gp map gp mc mlp mc performances boston price data squared error loss 
horizontal ordering methods vertical ordering 
numerical values matrices axis rounded values pairwise comparisons percent 
values appear column winning method 
see section details 
loss absolute error loss negative log density loss respectively 
striking feature plots large error bars consequently small number significant results 
aspects important interpreting plots 
recall error bars computed separately method reflect effects pairing 
fig 
squared error loss training cases gp map gp mc methods significantly overlapping error bars value pairwise comparison favour gp mc 
pairing methods leads cancellation noise enabling stronger statement difference performance anticipated error bars 
different effect works opposite direction 
error bars computed single estimates variability due training sets test cases prone uncertainty accounted plots tests producing values take account 
example compare knn cv mars bag experiments boston price boston price lin knn cv mars mlp ese gp map gp mc mlp mc performances boston price data absolute error loss 
training cases fig 

error bars clearly separated graph pairwise test comes insignificant 
due uncertainty estimated effects training sets uncertainty large instances available size training set 
surprisingly difference gp map knn cv marginally significant example gp map larger error bars worse expected performance mars bag different outcome due effects pairing 
remember strongest possible value tests way anova instances discussed section 
test results strong 
lin knn cv lower expected loss methods gp map smallest tasks negative log density loss case differences significant 
general differences lin knn cv remaining methods considered significant 
comparisons better methods remain inconclusive mlp ese gp map win significantly 
note results different loss functions show similar tendencies 
clear usefulness boston price data severely limited large experimental results boston price lin knn cv mars mlp ese gp map gp mc mlp mc performances boston price data negative log density loss note mars bag mlp ese methods produce predictive distribution loss type applied 
uncertainties associated loss estimates 
differences expected losses large gamma declared significant 
differences magnitude important practical purposes 
may argued failure obtain statistically significant results caused sub optimal split data training test sets 
cases split roughly half half training test partitions possible increase sets factor leading expected reduction error bars order 
experiments larger sensitivity designed boston data insists disjoint training sets 
discussed section analysis experiments overlapping training sets require assumptions dependencies introduced overlapping training sets difficult 
route pursued enable meaningful assessment real datasets exceeding cases 
literature learning methods rare results smaller datasets results kin pumadyn datasets boston cases reported losses differing percent reported 
results suggest may wise exercise caution evaluating results tests small datasets 
general wary results consider uncertainties observed performances 
results kin pumadyn datasets extensive tests carried kin pumadyn families 
datasets larger boston set allowing reliable detection smaller differences performance 
methods tested experiments boston data 
results different task sizes plots fig 
pages 
reasons brevity results squared error loss function 
error bars plots quite small tendency tasks small training sets slightly larger error bars tasks large training sets 
tendency probably reflects larger training set effect small training sets performance methods sensitive details training sets 
general error bars small large number differences significant 
notice mlp mc method larger error bars methods tasks small training sets kin nm pumadyn nm pumadyn nh datasets 
may caused convergence problems method 
hard method find areas high posterior probability posterior peaked markov chain responsible integration posterior may take long time discover posterior peak spend time sampling big volumes prior 
effect seen estimated noise level plotted function iteration number 
noise estimate relatively high suddenly jumps lower value seldom returns high values 
tried reduce effect initially training network fixed values hyperparameters described section hope regions high posterior probability located rapidly conditions 
elaborate initialisation scheme advisable 
different tasks exhibit different behaviours 
performance kin nm task improves strongly number training cases increases 
contrast training set size importance kin fh knn cv method experimental results kin fm dist lin knn cv mars mlp ese gp map gp mc mlp mc kin fh dist lin knn cv mars mlp ese gp map gp mc mlp mc experimental performance comparisons squared error loss function kin tasks inputs fairly linear relationships medium high noise levels 
results kin pumadyn datasets kin nm dist lin knn cv mars mlp ese gp map gp mc mlp mc kin nh dist lin knn cv mars mlp ese gp map gp mc mlp mc experimental performance comparisons squared error loss function kin tasks inputs non linear relationships medium high noise levels 
experimental results kin fm dist lin knn cv mars mlp ese gp map gp mc mlp mc kin fh dist lin knn cv mars mlp ese gp map gp mc mlp mc experimental performance comparisons squared error loss function kin tasks inputs fairly linear relationships medium high noise levels 
results kin pumadyn datasets kin nm dist lin knn cv mars mlp ese gp map gp mc mlp mc kin nh dist lin knn cv mars mlp ese gp map gp mc mlp mc experimental performance comparisons squared error loss function kin tasks inputs non linear relationships medium high noise levels 
experimental results pumadyn fm accel lin knn cv mars mlp ese gp map gp mc mlp mc pumadyn fh accel lin knn cv mars mlp ese gp map gp mc mlp mc experimental performance comparisons squared error loss function pumadyn tasks inputs fairly linear relationships medium high noise levels 
results kin pumadyn datasets pumadyn nm accel lin knn cv mars mlp ese gp map gp mc mlp mc pumadyn nh accel lin knn cv mars mlp ese gp map gp mc mlp mc experimental performance comparisons squared error loss function pumadyn tasks inputs non linear relationships medium high noise levels 
experimental results pumadyn fm accel lin knn cv mars mlp ese gp map gp mc mlp mc pumadyn fh accel lin knn cv mars mlp ese gp map gp mc mlp mc experimental performance comparisons squared error loss function pumadyn tasks inputs fairly linear relationships medium high noise levels 
results kin pumadyn datasets pumadyn nm accel lin knn cv mars mlp ese gp map gp mc mlp mc pumadyn nh accel lin knn cv mars mlp ese gp map gp mc mlp mc experimental performance comparisons squared error loss function pumadyn tasks inputs non linear relationships medium high noise levels 
experimental results doing poorly 
non linear high dimensional version kin hard learn errors remain high largest training sets gp map mlp mc methods making improvements 
linear method usually fairly linear tasks beats sophisticated methods gp map gp mc mlp mc single occasion kin fh task 
lin pronounced difficulties smallest instances fairly linear high dimensional tasks 
suspect seeing effects fitting alleviated introducing regularisation linear method 
expected lin badly non linear tasks having performance close knn cv 
knn cv poor job 
noticeable fairly linear tasks methods perform 
may reason nearest neighbor methods rarely regression exclusively classification tasks 
simple extension method local linear models neighborhood loess cleveland may help 
spectacular difference performances seen pumadyn nm lesser degree pumadyn nh methods perform small amounts training data lin knn cv catch 
mlp ese method somewhat intermediate improving training cases short performance gp methods training data 
enormous difference performance coincides methods kind variable selection 
automatic relevance determination ard scheme gp map gp mc mlp mc helping focus relevant subset large number inputs 
examination hyperparameters gp methods reveal inputs considered significant 
mlp ese method form variable selection result large amounts training data needed method 
mars capable ignoring inputs method tasks 
order get clear overview performance tendencies function characteristics datasets produced plots average performances fig 

data families characterised binary characteristics domain input dimensionality linearity noise level 
figures separate tasks characteristic average performances characteristics 
graphs displaying averages tasks values results kin pumadyn datasets inputs inputs training set sizes methods geometric average squared error geometric average performances conditional number inputs 
approximate standard error error bars indicated vertical line segments 
methods grouped horizontally training set size 
group order lin knn cv mars bag mlp ese gp map gp mc mlp mc 
chosen characteristic 
methods grouped groups training set size 
groups ordering methods earlier performance plots lin knn cv mars bag middle mlp ese follows gp map gp mc mlp mc 
plots geometric average thought sensible usual mean performances different tasks differ substantially 
geometric average focuses attention relative differences absolute ones 
looking plots pay attention values performances focus performance particular method compares field plots 
plots show approximate standard error error bars 
geometric average expressed usual average log domain exp log 
assuming performance estimates individual tasks gaussian standard deviations smaller experimental results fairly linear non linear training set sizes methods geometric average squared error geometric average performances conditional degree linearity 
means distribution log approximately gaussian 
standard tools averaging independent performance measures log domain approximations transform back original domain 
fig note mlp ese difficulties tasks high dimensional input 
appears mlp mc difficulty high dimensional inputs small training sets 
fig 
similarly see mlp ese poorly non linear tasks 
confirms expectations lin non linear tasks 
plot fig 
interesting may give indication different data families 
results fairly similar exceptions mars bag badly kin data mlp ese badly pumadyn 
naturally extension delve incorporate data families essential tendencies bias respect effects reduced 
fig 
shows difference tasks medium high noise 
results kin pumadyn datasets kin family pumadyn family training set sizes methods geometric average squared error geometric average performances conditional data family 
strongest effect knn cv worse cases smaller noise expect fact seeing methods doing comparatively better case noise 
fairly surprising finding fig 
conventional neural network method mlp ese desirable non linear tasks high dimensional tasks mars bag 
prevailing folklore neural networks long time neural networks especially suitable high dimensional tasks non linear tasks 
experiments support view 
noted performance difference mlp ese mars bag correlates highly identity data family fig 

forces interpret results caution tests data families needed order enable firm 
cpu time consumed different methods table fig 

time needed lin knn cv mars bag methods mainly determined experimental results medium noise high noise training set sizes methods geometric average squared error geometric average performances conditional noise level 
size training sets depends slightly input dimensionality 
remaining methods iterative nature limit computation time 
mlp ese method linear scaling time number training cases allowing seconds training case 
linear scaling chosen compromise assumed requirements practical limitations compute time 
probably computational requirements grow roughly quadratically mlp ese time iteration scales quadratically number training cases number network weights grow 
difficult guess scaling behaviour required number iterations early stopping sets possibly increasing number iterations required larger training sets 
times method time largest tasks train members predictive ensemble 
consequently reasonable suppose reached point diminishing returns terms ensemble size 
smaller tasks linear scaling ensures method plentiful time 
results kin pumadyn datasets lin knn cv mars bag mlp ese gp map gp mc mlp mc approximate total training prediction cpu time minutes combinations learning methods training set sizes experiments kin pumadyn data families 
methods require large amounts computation time interest investigate trade time accuracy behaves different methods function training set sizes 
results fig 
computationally demanding methods 
approximate error bars computed approximation fig 

particular method performance gets better training time 
fixed time proportional number training cases occasionally better fewer training cases example gp mc method short run times worse cases cases 
due super linear time requirement method resulting lack convergence large instances 
longer runs larger training sets advantageous 
gp mc applied largest instances fairly obvious fail time constraints considered 
note results mlp mc gp mc methods reported earlier section seconds training case 
mlp ese seconds training case allowed probably adequate get benefit method 
time gp map method dictated cubic time dependency training set size limited small training sets 
interesting compare times mlp mc times method inventor neal 
study run times selected basis plots various quantities showing possibility equilibrium 
main simulations involved way cross test boston housing data seconds experimental results training cases cpu time seconds training case geometric average squared error loss training cases cpu time seconds training case geometric average squared error loss training cases cpu time seconds training case geometric average squared error loss training cases cpu time seconds training case geometric average squared error loss training cases cpu time seconds training case geometric average squared error loss mlp ese gp map gp mc mlp mc geometric average squared test errors kin pumadyn data families different training set sizes function training time computation intensive methods 
vertical line segments indicate approximate standard error error bars 
mlp ese gp map methods run single amount time mlp mc gp mc methods predictions intervals 
gp mc method run largest training set size 
plots scale 
note log time scale 
results kin pumadyn datasets training case training sets cases marginally slower machine 
current results orders magnitude computation time show mlp mc better mlp ese short times 
strong result time train mlp ese better mlp mc method converged 
obviously shorter times required methods appropriate 
mlp mc tendency smaller instances benefit longer runs larger instances may appear surprising may reasonably assume linear time required results 
may variety reasons behaviour 
firstly observation may artifact limited time scale performance larger instances may improve longer runs allowed study 
secondly bad results short runs small datasets may caused failure reach areas high posterior probability small training sets likelihood fairly weak markov chain may spend lot time sampling vast areas vague prior discovering peaks posterior 
indication behaviour fig 
poor performance large error bars largest instances probably indicate failure single instances 
large training sets likelihood strong avoid problem 
third reason may posterior hyperparameters broad multi modal small amounts data severely affect time required get sample posterior 
runs gp mc mlp mc smallest training set sizes extended factor sizes 
interestingly gp mc method converged seconds training case mlp mc improves time 
may wonder possible achieve convergence gp mc shorter times 
seconds training case markov chain perform roughly iterations smallest training sets 
large number iterations may suspect persistence iterations low 
scheme gradually increasing persistence run may decrease required run time 
long convergence times due high correlations posterior increase persistence certainly help hand problems caused markov chain infrequently switching multiple modes posterior simple measures may help 
theoretical point view may conjecture gp mc mlp mc perform nearly equally limit large amounts computation time experimental results approximately gaussian process priors integrate nuisance parameters 
course differences form covariance matrix exact shape hyper priors differences may great practical importance 
conjecture proven result clear reached convergence cases 
experimental evidence consistent conjecture 
agree conjecture choosing gp mc mlp mc methods done solely basis method efficient implementation 
small datasets gp mc implementation efficient roughly training examples standard implementation gp mc hard manage 
simple gp map method desirable properties 
large instances performs reasonable amounts computation time 
smallest instances fast beaten sampling methods orders magnitude computation time 
performance study 
naturally trade accuracy computational requirements depends details individual application 
current experiments allow strong 
characteristics data explored experiments methods implementing gaussian processes regardless characteristics data choice method determined primarily training set size 
training set small may worthwhile trying gp mc gp map nearly faster 
dataset large cases may better mlp mc efficient implementations gp methods may available 
chapter thesis provided rigorous statistical framework analysis empirical performance evaluations 
neural network statistics communities empirical evaluations methods play key role 
essential base experiments solid statistical theory 
hitherto field swamped proposals novel learning methods convincing empirical evidence 
success field depends heavily improving standards empirical tests 
experimental designs developed 
way design suitable datasets limited number cases single test set needed 
hierarchical design simpler enables exact tests method comparisons 
design requires multiple test sets suitable simulated data large numbers test cases may generated 
unfortunately fairly cumbersome comparisons 
development publicly available delve software environment ease burden 
delve includes results previously tested methods provide basis testing new methods 
performance tests delve somewhat laborious training instances tried firm drawn 
inherent property task 
pose problems testing compute intensive methods 
thesis allowed maximum hours computation time task instance standards regarded fairly modest 
total run time experiments thesis fairly large months 
methods relying direct manipulation gaussian process priors functions defined 
authors previously touched ideas remained largely unknown 
puzzling highly desirable properties 
gp map method relies simple optimisation fast small datasets 
methods small datasets numerous applications statistics 
direct applicability automatic relevance determination ard allows easy interpretation trained models 
complicated gp mc implementation attractive bayesian point view able integrate parameters model 
small datasets achieved reasonable amounts time 
reason neglect methods people failed notice modern computers easily invert large matrices 
inherent skepticism bayesian methodology played role 
hope thorough experimental evidence succeed bringing methods wide 
experimental results boston dataset show large error bars performance estimates 
argued may difficult design sensitive experiments datasets moderate size 
trend carries datasets implies great caution exercised interpreting experimental evidence literature 
presents problem real datasets benchmarking may really suitable 
solution problem may data realistic simulators benchmarking purposes 
datasets adequately large benefit datasets controlled characteristics may produced enabling determination strengths weaknesses methods depend characteristics data 
experiments failed confirm widely held beliefs community traditional neural network methods mlp ese especially suited tasks high degree non linearity tasks high input dimensionality 
additional experiments data families helpful corroborate findings 
bayesian treatment neural networks mlp mc may take long time converge perform better conventional method mlp ese allowed fairly short times run time comparable mlp ese corresponding orders magnitude time published results method 
contrasts commonly held belief bayesian methods slow practical importance 
applications involving small amounts data methods definitely tractable 
may argued single mlp ese method representing conventional neural network methods limited 
certainly true variants common including training weight decay networks multiple hidden layers networks radial basis functions networks trained constructive approaches evaluation methods relying principles obviously interest 
existence large number algorithms lack specification heuristics impossible benchmarking study attacks regarding choice methods details heuristics 
essential authors methods precisely define methods test methods publicly available datasets 
delve designed meet requirements 
researchers required test everybody methods 
researchers methods rigorously tested making constructive convincing contributions 
theoretical point view may reasons expect methods gp map gp mc mlp mc perform fairly similarly approximately implement modeling gaussian process priors functions 
performance methods quite close consistently better methods 
strength somewhat limited data families 
noted methods implementing gp regardless input dimensionality degree non linearity noise level identity data family 
method choose depends efficiency methods particular training set size 
small datasets gp mc performs best intermediate sizes gp map desirable faster large sets mlp mc performs best 
experimental results thesis allow strong methods preferable 
typical earlier benchmarking studies prechelt michie gu erin 
example prechelt able draw general methods tried different problems 
may reasons 
probably important reasons tested quite different methods large datasets 
biggest weakness experimental study small number data families 
near delve extended datasets allowing confidence experimental 
results empirical tests performed thesis available delve 
methods results represent important source people interested performance learning methods 
hopefully people report experimental results delve forming valuable source experimental evidence 
time show gaussian processes able stand competition 
appendix implementations appendix contains source listings important parts implementations methods described thesis mars mlp mc methods 
sake brevity interesting details reading example files included may delve web site 
source mlp mc available radford neal retrieved homepage www cs toronto edu radford 
linear model lin source straight forward implementation lin method pages 
usage program lin instance number program reads training examples file train test inputs test test targets targets instance number 
point predictions written files 
adequate number training cases number training cases exceeds input dimension losses negative log density loss function written implementations reads training examples train test inputs test targets targets 
produces point predictions conditioned cases inputs close linearly dependent 
include extern void svd real real int value decomposition main int argc char argv real tmp sigma sig mu file fp argc train num test num inp tar default unknown test inp tar df df sprintf df train argv name training file inp inp double size svd call tar inp real malloc size inp sizeof real sigma real malloc size tar sizeof real upper triangular matrix elements place symmetrically 
don tmp tmp construct matrix 
single target vector tmp tmp rows contain product remaining rows contain compute row time store lower half lastly compute inp invert sq max sv ratio sv large 
compute tmp tmp tmp inp produce point predictions test cases write fp argv file point predictions fprintf fp train num inp produce reasonable predictive train num inp training cases 
linear model lin tmp tmp sq train inp inp train tar predictions test cases sig test inp inp noise uncertainty fprintf fp tmp free free free free free free static real real real int length linear function bias real tmp routine adapted permission pascal implementation nash compact numerical methods computers 
rows contain product lower rows contain 
vector returns square singular values 
int vt sqrt vt sqrt vt warning reached maximum number sweeps svd routine implementations nearest neighbors regression knn cv source knn cv method included pages 
usage knn cv instance number program reads training examples file train test inputs test test targets targets instance number 
outer loop program leave training cases turn 
inside loop remaining cases sorted distance input left case losses standard loss functions squared error loss absolute error loss negative log density loss accumulated value neighborhood size 
optimal value loss echoed stderr predictions neighborhood sizes 
predictions written files named loss type 
nearest neighbors regression knn cv reads training cases train test inputs test test targets targets 
writes point predictions argument 
loss type selected leave cross validation 
include define tolerance define pi extern real median real int find median elements static int comp const void const void function qsort static void var est nn real glob var estimate variance nn loo est estimates loss functions value main int argc char argv char df df strings containing file names real glob var tmp temp argc fprintf stderr usage instance number argv exit train num test num inp tar default unknown sprintf df test argv name test file sprintf df train argv name training file dist tar train num tar dist tar train num avoiding check array boundaries looking ties targets train num tar real malloc size tar sizeof real estimates loss type columns loo est 
swap train inp train inp top swap train tar train tar top sort dist top train inp top find mean variance target tmp tmp fabs median targets train tar top loo est tmp loo est loo est loo est loo est printf loss types loo values fp argv find neighbors squared error loss fprintf fp fprintf fp median targets glob var tmp log pi fclose fp fclose fp fclose fp implementations contains euclidian distance input space loo case remaining training cases remaining tar columns contain static void sort dist int real inp dist tar train tar take care ties case average targets tied cases 
static int copy targets ties fix tmp targets tmp probability losses 
return array variances target 
int top train num var top leave example find neighbors var sq targets train tar top neural networks trained early stopping mlp ese neural networks trained early stopping mlp ese main portions source mlp ese appears pages 
usage program form mlp ese instance number cpu time seed program read training test sets files train test write predictions instance number 
allowed cpu time may optionally specified seconds negative values interpreted times training case 
default value gamma seconds training case 
seed random number generator may specified default chosen time date 
member ensemble training different random partition training validation sets 
members ensemble trained weights dumped file called training terminates networks file re read order produce ensemble predictions 
running diagnostics printed stderr 
main program uses conjugate gradient minimization procedure described appendix functions implementing neural network function function computing partial derivatives cost function respect weights network 
function randomly initialises network weights 
random weights drawn gaussian distribution zero mean standard deviation fan unit weight projects 
implementations ensemble neural networks trained early stopping 
member ensemble validated validation set picked random 
weights seconds training example 
running diagnostics printed stderr 
include define define define void void extern void struct timer int time extern int conj int iter int epoch int restart real struct train training examples wd array direct input output weights wo array hidden output weights wts total 
weights length time times milliseconds iter best iter current best iteration early stopping counts number nets trained far junk perf best perf current best performance early stopping struct valid test pred file file predictions dumped time seed chose seed time inp tar defaults unknown case arguments argc seed atoi argv seed test num train num inp tar default unknown test inp df null length length train num training time case valid num train num fprintf stderr open weight file writing bye df hid ceil train num inp tar inp tar hid wts fprintf stderr net train err valid err best iter max iter time length train num tmp train inp train inp train inp train inp tmp best best random net succ conj restart junk iter perf cost valid best perf succ timeout iter best iter fwrite best wts sizeof real append net neural networks trained early stopping mlp ese fprintf stderr cost train best perf best iter iter rewind compute ensemble predictions net test inp output fclose fprintf pred file pred fprintf pred file function net evaluates outputs inputs fully connected feed forward net single hidden layer tanh units 
copyright carl edward rasmusen 
include util extern real wi wd wo hidden output void net real real compute network output input compute activity hidden units tmp hidden tanh compute activities output units tmp wd struct ex int wc real tmp tmp ex tar return ex num implementations function returns batch cost set returns array partial derivatives weights fully copyright carl edward rasmusen 
extern real prior real dw int dw zero derivatives tmp wi hidden tanh compute output activities tmp wo tmp wd pat tmp direct weights dw hidden dw pat hidden dw pat functions allocate free memory fully connected neural networks single hidden layer tanh units size global magnitude parameter scaled root fan fo units project 
void real malloc size wts sizeof real wo wd hid wo wo hid wd wo inp wi wi inp output real malloc size tar sizeof real void define norm cos sqrt log int magnitude norm sqrt hid inp bayesian neural networks mlp mc bayesian neural networks mlp mc source method written radford neal publicly available web www cs toronto edu radford 
software experiments thesis release dated august 
quite long reproduce 
supply script written csh shell run simulations method 
detailed description options available refer documentation software 
net spec log model spec log real data spec log train rand seed log net gen log fix mc spec log repeat sample noise hybrid net mc log mc spec log sample sigmas hybrid negate net mc log net pred bn log test net pred bd log test net pred bp log test targets argument script contains instance number 
example network inputs single layer hidden units trained minutes cpu time 
number hidden units selected heuristic rules section 
net spec command specifies architecture network weight priors discussed section 
lines specify noise model tells program get data files 
setting seed random number generator hyperparameters initialized 
markov chain set perform iterations leapfrog steps step size fixed values hyperparameters 
iterations performed call net mc allow weights attain somewhat reasonable values 
line markov chain set remainder run 
sample sigmas command updates hyperparameters command updates momenta remainder line specifies hybrid monte carlo weights trajectories length window size step size 
call net mc allows minutes cpu time saves th iteration log file interval experimentation determined appropriate desired number samples minutes 
lines script generate predictions samples run standard loss types 
implementations gaussian processes section contains source code gaussian process methods thesis 
am currently rewriting software portable flexible allowing specification desired covariance function noise models command line 
new software available public domain delve 
source methods gp map gp mc 
code specific covariance function located cov file 
follows generic programs methods gp map gp mc optimization map method done conjugate gradient method described appendix leapfrog method hybrid monte carlo algorithm follows leapfrog file 
lastly generic prediction program gp pred listed followed matrix inversion code routine find medians median programs gp map gp mc store values hyperparameters momentum variables regular intervals called log files 
gp pred program predictions test cases information log files 
problems containing training cases design wasteful prediction program invert matrices inverted training program 
script csh shell run gp map method instance log files named log total cpu time seconds consists lines gp map log gp pred log grep log grep command simply returns number records written log file causing record predictions 
similarly script gp mc method instance number minutes cpu time seconds saving log records records run predictions gp mc log gp pred log note implementation supports continuation run specific log file simply re calling gp mc longer time specification feature generating data fig 

gaussian processes file contains code training gp mc prediction gp pred programs regression gaussian processes specific exp sum variance bias variance parameters linear exp exp exp hack ensure variances copyright carl edward rasmussen include include real ew exp convenience extern struct train void init set wts create initialise real malloc size wts sizeof real ew real malloc size wts sizeof real function returns log prior hyperparameters augments array derivatives effect prior 
prior gaussian 
real prior real dw mean sigma mu prior specification dw mu exp inp inp dw return static real trace prod real real static int supplied null likelihood real real dw real rr miscellaneous set upper triangle covariance rr train inp train inp compute rr inv train tar rr inv inp lastly noise scale inp 
dw inp trace prod tr inv dk dv dw inp rr input scales implementations dw trace prod dw rr ew inp rr rr rr dw inp rr rr rr dw inp rr prior dw augment prior return cases values fo hyperparameters 
uses globals ew wts train void pred real real struct test ew exp compute ew rr ew inp exp ew inp rr ew inp rr ew inp exp ew inp ew inp rr train tar ew inp ew inp ew inp rr ew inp gaussian processes copyright carl edward rasmussen 
include include include help variable inp input dimension tar number targets extern real dw dw arrays conj function extern int conj int iter int epoch int restart real extern void struct timer int time main argc argv int restart succ length mod iter long nexttime char logfile argc argc fprintf stderr parse length argv length mod sprintf logfile argv argv logfile exit train inp tar tar null init train num train num dw real malloc size wts sizeof real real malloc size wts sizeof real restart iter nexttime timer mod timer nexttime fprintf succ timeout iter length implementations copyright carl edward rasmussen 
include include include rand dw hyperparameter derivatives main matrices int number func grad evals 
far incremented tar number targets struct timer extern void init extern void leapfrog file df real rho real epsilon struct timer long current double long seed tm file argc argc argc seed atoi argv fprintf stderr rand seed int seed timer length limit compute time sprintf logfile touch argv argv system logfile fprintf stderr open log file writing bye logfile exit train num inp default don know init train num train num real malloc size wts sizeof real real malloc size train num sizeof real fscanf lf tm timer tm gaussian processes num leapfrog iterations save mod log file df 
num negative just keep going timeout set 
include include rand extern struct timer extern real prior real dw extern real real dw real prior real dw int reject epsilon sqrt real train num scale stepsize ow real malloc size wts sizeof real kin kin kin start num initial leapfrog step epsilon dw epsilon kin remaining leapfrog step epsilon dw kin exp old pot kin rand uniform reject rho sqrt rho rho rand gaussian kin kin kin mod mod mod timer nexttime pot kin real reject fprintf df df nexttime abs num mod set time save timeout exit timeout set caught implementations log record specified log file specified interval predictions squared error loss absolute error loss negative samples record 
negative log predictive loss width predictive distribution enlarged sum widths include include rand real hyperparameters int wts number hyperparameters inp input dimension struct train test extern real median real int main argc argv int argc double real means hlp tmp char logfile outfile file fp fprintf stderr parse range argv low high mod train inp tar tar null sprintf test argv sprintf logfile argv argv logfile exit low high range time upper limit fscanf fp ld lf lf lf tm low tm low low tm high rewind fp fprintf stderr train num train num mod low low high low mod fscanf fp lf fscanf fp lf pred means test fclose fp tmp tmp means fclose fp fprintf fp median fp argv write preds loss tmp sqrt means tmp tmp sq tmp tmp tmp fprintf fp log tmp gaussian processes free free free means free means free function looks elements diagonal 
computes substitution 
return upper diagonal matrix contains inverse 
see golub van loan matrix computations nd edition johns copyright carl edward rasmussen define real double int real malloc size sizeof real real malloc size sizeof real unit matrix backward substitution free free implementations routines find median array numbers expected linear time 
value 
algorithms mit press 
include static int partition real int odd return select select numbers elements rearranged function 
static int return select element 
returning elements indexes smaller equal index partition element values static int partition real int static int appendix conjugate gradients appendix describes utility function conj optimizes network weights conjugate gradient method 
function private function lns doing line searches 
conjugate gradient method minimizing function variables works iteratively computing search directions line search procedure minimizes function producing new approximation local minimum objective function 
iteration defined computation search direction line search 
epoch defined computation function value gradient rf 
function gradients computed pair 
iteration involve epochs line search 
number epochs indicative computational effort 
conjugate gradients iteration number position weight space value objective function gradient vector partial derivatives rf 
known version conjugate gradients fletcher computes new search direction old direction gamma current old gradients conjugate gradients gamma gammag gamma gamma gamma gamma gamma slope search direction denoted 
may happen function positive slope guarantee slope negative case direction steepest descent gammag single iteration 
gradient cost function computed back propagation rule 
conjugate gradient method shown quadratic convergence properties better originally proposed method steepest descent 
quadratic optimisation problem solved dimensionality problem iterations line searches exact 
cost function neural network typically quadratic weights procedure converge steps 
iterative manner 
algorithm restarts starting steepest descent iterations done implementation typically large version natural tendency automatically restart progress slow case gamma consequently gammag eq 

line search object line search minimize objective function direction dimensional minimization problems tractable multidimensional ones general possible find global minimum epochs may necessary achieve close approximations minimum 
case optimisation problem usually non quadratic sensible expend huge computational resources doing accurate line searches remember function evaluation requires full sweep entire training set 
consequently goal line search merely get significantly closer local minimum 
sided wolfe powell conditions fletcher determine new point significantly better current point 
wolfe powell conditions consists inequalities 
inequality requires absolute slope new point smaller absolute sense fraction magnitude slope current point 
general guarantee moved closer local extremum line search distance search direction cost function illustration wolfe powell conditions 
current point marked 
cost function tangent drawn current point 
intersections lines function defines region acceptable points 
non vanishing amount 
condition jf gamma oe 
fig 
condition implies acceptable points lie intersections lines second inequality requires substantial fraction decrease expected current slope achieved guaranteeing decrease function value avoiding huge steps negligible function decrease 
condition gamma ae gamma gamma gamma ae 
fig 
smaller values line labeled intersects acceptable 
final region acceptable points marked shown acceptable points exist continuous bounded ae oe 
line search iteratively improves guesses minimum acceptable point 
pre conditions guaranteed satisfied conjugate gradient routine current point computed guaranteed max undefined 
fig 
represents algorithm pseudo code 
additional constraint added code fig 
maximum epochs allowed avoid infinite loops numerical inaccuracies case line search fails best approximation far returned 
notation refers condition implied line fig 
point returns conjugate gradients initial guess compute loop violated max interpolate compute satisfied return success 
extrapolate max compute pseudo code line search algorithm violated satisfied 
solution bracketed max max defined 
initial step size guess current iteration computed step size previous iteration slope ratio method 
length previous step ffi gamma multiplied ratio previous current slopes maximum ffi min gamma ffi gamma ffi ffi jf limit factor relative step size introduced avoid errors numerically close zero 
simple heuristic slope ratio method practice 
typically epochs iteration needed indicating initial guess lies region acceptable points 
interpolation done finding minimum quadratic cubic polynomial fit 
cubic case ignored quadratic case far minimizer feared derivative may misleading 
interpolating polynomial local minimum inside interval bisection 
addition point interval length interval endpoint new guess moved distance order ensure exploration avoid stagnation repeated evaluation essentially identical points 
extrapolation interval done cubic fit 
cubic minimum minimum correspond extrapolation extrapolation times interval length point times interval length order prevent uncontrolled extrapolation 
discussion new point close current point interval length length ensure exploration 
conjugate gradient algorithm calls line search procedure directions computed 
entire minimization procedure terminates prespecified number function evaluations performed subsequent line searches failed usually happen reasons numerical inaccuracies close local minimum 
computations done double precision arithmetic 
discussion implementations conjugate gradient methods proposed 
unfortunately quite rare implementational details discussed 
extremely rare see empirical evidence performance optimisation procedures 
address issues mller 
presents implementation called scaled conjugate gradients scg avoids line search finite differences approximation hessian computing step size newton method 
scg uses epochs iteration function gradients needed current point gradient computed computing function value vicinity current point needed finite differences approximation hessian 
scg method version method modified require interpolation 
advantages method ffl guess slope ratio algorithm extra point needing evaluation local point finite differences method giving relevant information function 
ffl scg method ignores half function values computed discarding useful information 
ffl initial guess may accepted avoiding extra epochs conceivable scg benefit accurate new points 
typically epochs iteration needed opposed cgs 
conjugate gradients ffl interpolation typically done cubic scg quadratic polynomial 
ffl rigorous criteria ensure progress iteration 
reasons prefer implementation conceivable significant differences methods practice 
discussion include include util extern real weight vector extern real real dw evaluate function partial derivatives search direction real current function value real rho sig int ext int dw initialize point equal point max tighten bracket nan bisection int int bound solution away current update weights dw sig return success sqrt num 
error possible ok max ext max bisection max ext 
ext 
ext set 
limit max max int close max 
max int update weights int conj iter epoch restart restart restart restart cg steepest descent int miscellaneous counter real tmp slope extern int timeout set caught restart start direction steepest descent fun dw step slope set initial step size restart probably won want restart call iter cur iter iter dw dw dw dw dw dw tmp dw dw dw dw tmp swap derivatives slope slope dw dw slope line search failed ls failed break previous failed try steepest slope conjugate gradients bibliography abramowitz stegun 

handbook mathematical functions volume applied mathematics series 
national bureau standards washington 
barber williams 

gaussian processes bayesian classification hybrid monte carlo 
submitted nips 
box tiao 

bayesian inference statistical analysis 
john wiley sons breiman 

bagging predictors 
technical report department statistics university california berkeley california 
cleveland 

robust locally weighted regression smoothing scatterplots 
am 
statist 
assoc 
corke 

robotics toolbox matlab 
ieee robotics automation magazine 
dietterich 

proper statistical tests comparing supervised classification learning algorithms 
unpublished 
duane kennedy 

hybrid monte carlo 
physics letters 
fletcher 

practical methods optimization 
john wiley sons second edition 
friedman 

multivariate adaptive regression splines discussion 
annals statistics 
source code lib stat cmu edu general mars 
geman bienenstock doursat 

neural networks bias variance 
neural computation 
bibliography gibbs mackay 

efficient implementation gaussian processes 
available wol ra phy cam ac uk mackay 
gilks wild 

adaptive rejection sampling gibbs sampling 
applied statistics 
girosi jones poggio 

regularization theory neural networks 
neural computation 
golub van loan 

matrix computations 
johns hopkins university press second edition 
gu erin 

deliverable task benchmarks 
technical report elena enhanced learning neural architecture esprit basic research project number 
ftp ftp dice ucl ac pub neural nets elena benchmarks ps hansen salamon 

neural ensembles 
ieee transactions pattern recognition machine intelligence 
harrison jr 

hedonic housing prices demand clean air 
journal environmental economics management 
hastie tibshirani 

generalized additive models 
number monographs statistics applied probability 
chapman hall 
hastie tibshirani 

discriminant adaptive nearest neighbor classification regression 
touretzky mozer hasselmo editors advances neural information processing systems volume pages 
mit press 
ftp stanford edu pub hastie nips ps horowitz 

generalized guided monte carlo algorithm 
physics letters 
koehler owen 

computer experiments 
larsen hansen 

empirical generalization assessment neural network models 
ieee workshop neural networks signal processing boston 
le cun denker solla 

optimal brain damage 
touretzky editor advances neural information processing systems volume pages san mateo 
denver morgan kaufmann 


analysis variance experimental design 
springer verlag 
bibliography lowe 

similarity metric learning kernel classifier 
neural computation 
mackay 

bayesian interpolation 
neural computation 
mackay 

practical bayesian framework backpropagation networks 
neural computation 
michie spiegelhalter taylor editors 
machine learning neural statistical classification 
ellis horwood limited 
mller 

scaled gradient algorithm fast supervised learning 
neural networks 
murphy aha 

uci repository machine learning databases 
technical report department information computer science university california irvine ca 
www ics uci mlearn mlrepository html 
neal 

probabilistic inference markov chain monte carlo methods 
technical report crg tr department computer science university toronto 
neal 

improved acceptance procedure hybrid monte carlo algorithm 
journal computational physics 
neal 

bayesian learning neural networks 
springer verlag new york 
revised version ph thesis graduate department computer science university toronto 
hagan 

curve fitting optimal design regression 
journal royal statistical society 
discussion 
hagan 

bayesian inference volume kendall advanced theory statistics 
edward arnold edition 
poggio girosi 

networks approximation learning 
proceedings ieee 
prechelt 

proben set neural network benchmark problems benchmarking rules 
technical report fakultat fur informatik universitat karlsruhe 
doc pub papers techreports ps data pub neuron proben tar gz ftp ira uka de 
prechelt 

auf 
phd thesis fakultat fur informatik universitat karlsruhe 
german language 
bibliography prechelt 

study experimental evaluations neural network learning algorithms current research practice 
neural networks 
press teukolsky vetterling flannery 

numerical recipes cambridge university press second edition 
quinlan 

combining instance model learning 
machine learning proceedings tenth international conference amherst massachusetts 
morgan kaufmann 
rasmussen 

practical monte carlo implementation bayesian learning 
touretzky mozer hasselmo editors advances neural information processing systems volume pages 
mit press 
ftp ftp cs toronto edu pub carl ps gz 
rasmussen neal hinton van camp revow ghahramani tibshirani 

delve manual 
university toronto 
www cs utoronto ca delve 
skilling 

eigenvalues mega dimensional matrices 
skilling editor maximum entropy bayesian methods cambridge pages dordrecht 
kluwer 
skilling 

bayesian numerical analysis 
jr editors physics probability cambridge 
livingstone 

neural network studies 

comparison overfitting overtraining 
chem 
info 
comp 
sci 


review bayesian neural networks application near infrared spectroscopy 
ieee transactions neural networks 
wahba 

spline models observational data volume series applied mathematics 
siam philadelphia 
williams 

regression gaussian processes 
annals mathematics artificial intelligence 
appear ftp sparc server aston ac uk neural manna ps williams rasmussen 

gaussian processes regression 
touretzky mozer hasselmo editors advances neural information processing systems volume pages 
mit press 
ftp ftp cs toronto edu pub carl ps gz 
