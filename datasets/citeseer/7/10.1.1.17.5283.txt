appear aaai learning evaluation functions global optimization boolean satisfiability justin boyan andrew moore computer science department carnegie mellon university pittsburgh pa cs cmu edu describes stage learning approach automatically improving search performance optimization problems 
stage learns evaluation function predicts outcome local search algorithm hillclimbing walksat function state features search trajectories 
learned evaluation function bias search trajectories better optima 
positive results large scale optimization domains 
vlsi design engineering design boolean formula satisfaction bin packing medical treatment planning bayes net structure finding examples global optimization problem finding best possible configuration large space possible configurations 
formally global optimization problem consists state space objective function obj goal find minimizes obj 
large finding generally intractable problem specialized structure linear program 
general purpose local search algorithms attempt exploit obj structure locate approximate optima example hillclimbing simulated annealing tabu search 
imposing neighborhood relation states searching graph results guided obj 
local search likened trying find top mount everest thick fog suffering amnesia russell 
climber considers step consulting altimeter deciding take step change altitude 
suppose climber access altimeter additional senses instruments example current location slope ground copyright american association artificial intelligence www aaai org 
rights reserved 
current location trail 
additional features may enable climber informed evaluation take step 
real optimization domains additional state features generally plentiful 
practitioners local search algorithms append additional terms objective function spend considerable effort tweaking coefficients 
excerpt book vlsi simulated annealing wong leong liu typical clearly objective function minimized channel width crude measure quality intermediate solutions 
valid partition cost function ap av measures sparsity horizontal tracks measures longest path length current partition 
application authors hand tuned coefficients extra state features setting ap av 
show algo rithm learned assign negative value av achieved better performance 
similar examples evaluation functions manually configured tuned performance falkenauer 
question address extra features optimization problem incorporated automatically improved evaluation functions guiding search better solutions 
stage algorithm st ge analyzes sample trajectories automatically constructs predictive evaluation functions 
uses new evaluation functions guide search 
preliminary version st ge described boyan moore showed promising performance vlsi layout 
describes improved version stage superior results vlsi layout thorough results global optimization mains 
overview theoretical foundations stage consequence extension allowing stage accelerate non monotonic search pro walksat 
learning predict performance local search algorithm depends state search starts 
express dependence mapping starting states expected search result deal expected best obj value seen tra starts state follows local search method rr represents local search method hill climbing simulated annealing 
evaluates promise starting state example consider minimizing dimensional function obj cos domain depicted 
assuming neighborhood structure domain tiny moves left right allowed hillclimbing greedy descent search clearly leads suboptimal local minimum starting points 
quality local minimum reached correlate strongly starting position 
gathering data suboptimal trajectories function approximator easily learn predict starting near lead performance 
left obj dimensional minimization domain 
right value function predicts hillclimbing performance domain 
approximate function approximation model polynomial regression states encoded real valued feature vectors 
dis cussed input features may encode relevant properties state including original objective function obj 
denote map ping states features approximation 
training data supervised learning may readily obtained running rr different starting points 
algorithm rr behaves markov chain probability moving state matter visited states visited previously intermediate states simulated trajectory may considered alternate starting search training data 
insight enables get hundreds pieces training data trajectory sampled 
remainder section walksat set rr stochastic hillclimbing rejecting equi cost moves terminating soon fixed number patience consecutive moves pro duces improvement 
choice rr satisfies markov property 
simple linear quadratic regression fit training models incrementally extremely efficient time memory boyan 
predictions learned evaluation function evaluates promising starting point algorithms 
find best starting point optimize applying stochastic hillclimbing obj evaluation function 
run produces new data optimize obj retrain fitter produces new starting state 
optimize diagram main loop stage stage provides framework learning exploiting single optimization instance 
illustrated stage repeatedly alternates different stages local search running method rr obj running hillclimbing find promising new starting state rr 
stage viewed smart multi restart approach local search 
note smooth respect feature space surely bc wc represent simple model quadratic regression may give risc complex cost surface respect neighborhood structure existence state set features similar current state imply step state space take state 
hc obj stage effectively plots single long trajectory state space periodically switching tween original objective function obj newly learned evaluation function 
tory broken search phase accepts moves indicating local minimum evaluation functions 
occurs stage resets search random starting state 
illustrative example detailed illustrative example st ge operation 
provide results large difficult global optimization problems 
example comes practical np complete domain bin packing coffman garey johnson 
bin packing bin capacity list items having size ai goal pack items bins possible 
depicts example binpacking instance items 
packed optimally items fill bins exactly capacity 
small example bin packing instance apply local search define neighborhood operator moves single random item random new bin having sufficient spare capacity 
stage predicts outcome stochastic hillclimbing quadratic regression features state 
actual objective function obj bins 

vat variance fullness non empty bins 
feature similar cost function term intro duced falkenauer 
depicts iterations stage run example instance 
itera tion stage starting state obj vat local optimum obj vat 
training states predict outcome results flat function shown 
hillclimbing flat accepts moves stage resets initial state 
og lo feature obj number bins vpl iteration var obj hc obj hc obj feature obj number bins vpi second iteration hc obj hc obj hc vpi hc obj feature obj number bins stage working bin packing example second iteration stage new stochastic hillclimbing trajectory happens bet ter finishing local optimum obj vat 
training set augmented target values states new tory 
resulting quadratic signif structure 
note contour lines shown base surface plot correspond smoothed versions trajectories train ing set 
extrapolating predicts best starting points hillclimbing arcs higher vat 
stage learned try find starting point 
trajectory shown dashed line goes obj vat obj vat 
note search willing accept harm true objective function stage 
new starting state hillclimbing obj lead better local optimum obj vat 
iterations approximation refined 
continuing ate hill climbing obj hillclimbing stage man ages discover global optimum obj vat iteration 
results extensive experimental results table 
problems widely varying characteristics contrast performance stage multi start stochastic hillclimbing simulated annealing domain specific algorithms applicable 
hillclimbing runs accepted equi cost moves consecutive moves produced improvement 
simulated annealing runs successful modified lam adaptive annealing schedule parame ters hand tuned perform range problems exhaustively optimized individual problem instance 
instance algorithms held number total search moves considered run times 
bin packing set results item benchmark bin packing instance falkenauer 
table compares st ge perfor mance hillclimbing simulated annealing best fit randomized coffman garey johnson bin packing algorithm worst case performance guarantees 
stage significantly forms 
obtained similar results instances suite boyan 
channel routing problem manhattan channel routing important subtask vlsi circuit design 
rows labelled pins rectangular channel connect labelled pins placing wire segments vertical horizontal tracks 
segments may cross overlap 
objective minimize area channel rectangular bounding box equivalently minimize number different horizontal tracks needed 
small channel routing instance clever local search operators defined wong problem wong leong liu replace contrived objective function see equation natural objective function obj channel width wong additional ob function terms input features st ge function approximator 
results yk instance vertical tracks table 
methods allowed con sider moves run 
experiment shows multi restart hillclimbing finds quite poor solu tions 
experiment shows simulated annealing objective function wong leong liu considerably better 
surprisingly experiment better 
crude evaluation function obj allows long simulated annealing run effectively random walk ridge solutions equal cost time find hole ridge 
fact increasing hill climbing patience disabling restarts worked nearly 
stage simple linear quadratic regression models learning 
results show stage learned optimize improving performance hillclimbing trained finding better solutions average best simulated annealing runs 
true stage really design 
considered eliminated hypotheses 
stage alternates simple hillclimbing policy simply benefits having random exploration 
case tried search policy alternating hillclimbing steps random walk formance worse st ge 

function approximator may simply smooth ing obj helps eliminate local minima plateaus 
tried variant stage learned smooth obj directly learning produced improvement stage 
bayes network structure finding data set important data mining task identify bayes net structure best matches data 
search space acyclic graph structures nodes number attributes data record 
friedman yakhini evaluate network structure minimum description length score trades fit accuracy low model complexity 
stage features mean standard deviation conditional entropy score node std 

number parameters node probability table std 

number parents node number orphan nodes results large dataset records attributes shown table 
methods comparably solutions st ge performance slightly better average 
radiotherapy treatment planning radiation therapy method treating tumors 
linear accelerator produces radioactive beam mounted rotating gantry patient placed tumor center beam rota tion 
depending exact equipment beam intensity modulated various ways rotates patient 
radiotherapy treatment plan specifies beam intensity fixed number source angles 
map relevant part patient body tumor important structures labelled available 
known clinical forward models calculating treatment plan distribution radiation delivered patient tissues 
optimization problem produce treatment plan meets target radiation doses tumor minimizing damage sensitive nearby structures 
current practice sim annealing problem webb 
illustrates planar instance radiotherapy problem 
instance consists irregular shaped tumor sensitive structures eyes brainstem rest head 
treat ment plan objective function calculated summing terms penalty penalty structures 
subcomponents features stage learning 
objective function evaluations computationally expensive domain experiments considered moves run 
algorithms performed comparably stage solutions best average 
left radiotherapy instance 
right state area proportional electoral vote president 
design map boundaries deformed population density uniform entire map 
considered redraw ing map united states state area proportional electoral vote 
goal best meet new area targets minimally distorting states shapes borders 
represented map collection points space state polygon subset points 
search operator consisted perturbing random point slightly perturbations cause edges cross disallowed 
objective func tion defined obj penalizes states missing new area targets terms penalize states shaped differently true map 
stage represented configuration subcomponents obj 
learning new evaluation problem algorithm performance runs instance mean best worst bin packing hillclimbing patience opt simulated annealing 
best fit randomized 
stage quadratic regression 
channel routing hillclimbing patience 
yk opt simulated annealing obj 

simulated annealing obj 
hillclimbing patience 
stage linear regression 
stage quadratic regression 
hillclimbing random walk 
modified stage smooth obj 
bayes net hillclimbing patience 
adult simulated annealing 
stage quadratic regression 
radiotherapy hillclimbing patience simulated annealing stage quadratic regression hillclimbing patience simulated annealing stage quadratic regression satisfiability walksat noise cutoff tries 
par cnf opt walksat hillclimbing 
walksat 
stage walksat quadratic regression 
stage walksat markov linear regression 
table comparative results variety minimization domains 
problem algorithms allowed consider fixed number moves line reports mean confidence interval mean best worst solutions independent runs algorithm problem 
best results boldfaced 
function quadratic regression features stage produced significant improvement hillclimbing outperform simulated annealing 
satisfiability finding variable assignment satisfies large boolean expression fundamental original np complete problem 
years ingly difficult formulas solved walksat selman kautz cohen simple local search method 
walksat formula expressed cnf conjunction disjunctive clauses conducts ran dom walk assignment space biased minimizing obj clauses unsatisfied assignment obj clauses satisfied formula solved 
walksat searches follows 
step selects unsatisfied clause random satisfy clause flipping variable 
decide evaluates improvement obj result flipping variable 
best improvement positive greedily flips variable attains improvement 
flips variable worsens oh probability noise variable harms obj probability noise variable random clause 
best setting noise problem dependent mcallester kautz selman 
walksat effective rendered nearly obsolete archive benchmark problems collected dimacs challenge satisfiability selman kautz cohen 
archive largest parity function learning stances constructed kearns schapire hirsh crawford known solvable prin solvable walksat 
report results experiments instance par nf formula consisting clauses variables 
experiment run times allowed consider bit flips run 
experiment see table shows results best hand tuned parameter settings walksat 
best run left clauses unsatisfied 
additional walksat parameter effect flip worsen obj rejected 
normal extreme harmful moves accepted resulting tive form hillclimbing 
intermediate settings prohibiting destructive walksat moves harm performance cases improves 
stage learning variety potentially useful additional state features available clauses currently unsatisfied obj clauses satisfied exactly variable clauses satisfied exactly variables variables set naive setting stage observing walksat trajectories learn combine features usefully observing hillclimbing trajectories domains 
theoretically stage learn procedure proper guaranteed terminate marko 
walksat normal termination mechanism cutting pre specified number steps depends extraneous counter variable just current assignment 
despite technicality stage quadratic regression nearly completely solved problem satisfying clauses runs 
properly markovian cutoff criterion walksat terminating probability step linear quadratic regression stage im provement plain walksat 
results bit parity benchmark instances similar 
experiments walksat run noise full details may boyan 
pursue 
believe stage shows promise hard satisfiability problems maxsat problems near solutions useful 
transfer computational cost training function approximator learning trajectory cnf formula naive setting variable xi defined bc xi appears clauses xi xi appears clauses xi 
length squares linear regression features costs stage iteration quadratic regression costs 
ex periments previous section costs minimal typically total execution time 
stage extra overhead significant features sophisticated function approximators 
problems cost worth comparison non learning method better equally solution obtained computation 
cases computation stage method may preferable asked solve similar problems new channel routing problem different pin assignments 
hope computation invested solving problem pay second problems cause estimate 
call effect transfer extent occurs largely empirical question 
investigate potential transfer re ran stage suite problems channel routing literature 
table summarizes results gives coefficients linear evaluation function learned independently problem 
similarities easier see table normalized coefficients squares sum note search behavior evaluation function invariant linear transformations 
problem lower best learned coefficients instance bound stage yk table stage results problems chao harper 
similarities learned evaluation functions striking 
hand tuned cost function wong leong liu equation stage learned cost functions signed relatively large positive weight feature small positive weight feature hand tuned stage runs assigned negative weight feature similarity learned functions suggests transfer problem instances fruitful 
assignment negative coefficient surprising measures sparsity horizontal tracks 
correlates strongly positively objective function minimized term evaluation function ought pull search terrible solutions subnet occupies track 
positive coefficient cancels bias fact proper balance terms shown bias search solutions uneven distribution track sparsity levels 
characteristic mark high quality solution help lead hillclimbing search high quality solutions 
stage successfully discovered exploited predictive combination features 
discussion conditions stage 
intuitively stage maps attracting basins domain local minima 
coherent structure attracting basins stage exploit 
identifying coherent structure depends crucially user selected state features domain move operators regression models considered 
shown wide variety largescale problems simple choices features models useful structure identified exploited 
relevant investigation boese 
boese kahng gives reasons opti 
studied set local minima reached independent runs hillclimbing traveling salesman problem graph bisection problem 
big valley structure set minima better local minimum closer terms natural distance metric tended lo cal minima 
led recommend phase adaptive multi start hillclimbing technique similar stage 
similar heuristic chained local opti mization martin otto works alter greedy search user defined kick moves search nearby different attracting basin 
main difference authors hand build problem specific routine finding new starting states stage uses machine learning 
zhang dietterich explored way learning improve combinatorial optimization learn search strategy scratch line value iteration zhang 
contrast stage begins search strategy uses prediction learn improve 
zhang reported success transferring learned search control knowledge simple job shop scheduling instances complex ones 
stage offers directions ration 
currently tion reinforcement learning methods building efficiently algorithms robust transfer learned functions instances direct methods feature weighting 
acknowledgments author acknowledges support nasa fellowship 
boese kahng 
new adaptive multi start technique combinatorial global optimizations 
operations research letters 
boyan moore 
prediction improve combinatorial optimization search 
proceedings 
boyan 
learning evaluation functions global optimization 
ph dissertation carnegie mellon university 
chao harper 
efficient lower bound algorithm channel routing 
integration vlsi journal 
coffman garey johnson 
approximation algorithms bin packing survey 
hochbaum ed approximation algorithms np hard problems 
pws publishing 

visualizing human geography 
unwin eds visualization geographical information systems 
wiley 

falkenauer 
genetic algorithm bin packing line balancing 
proc 
ieee international conference robotics automation 
friedman yakhini 
sample complexity learning bayesian networks 
proc 
th conference uncertainty artificial intelligence 
martin otto 
combining simulated annealing local search heuristics 
technical report cs oregon graduate institute department computer science engineering 
mcallester kautz selman 
evidence invariants local search 
proceedings aaai 

synthesis high performance analog cells 
ph dissertation carnegie mellon university department electrical computer engineering 
russell norvig 
artificial intelligence modern approach 
prentice hall 
selman kautz cohen 
local search strategies satisfiability testing 
cliques coloring satisfiability second dimacs implementation challenge 
american mathematical society 

simulated annealing approach dimensional component packing 
asme journal mechanical design 
webb 
optimization simulated annealing threedimensional conformal treatment planning radiation fields defined 
phys 
med 
biol 

wong leong liu 
simulated annealing vlsi design 
kluwer 
zhang 
reinforcement learning job shop scheduling 
ph dissertation oregon state university 
