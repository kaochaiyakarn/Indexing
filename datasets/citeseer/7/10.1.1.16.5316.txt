em dd improved multiple instance learning technique qi zhang department computer science washington university st louis mo qz cs wustl edu sally goldman department computer science washington university st louis mo sg cs wustl edu new multiple instance mi learning technique combines em diverse density dd algorithm 
em dd general purpose mi algorithm applied boolean real value labels real value predictions 
boolean musk benchmarks em dd algorithm tuning significantly outperforms previous algorithms 
em dd relatively insensitive number relevant attributes data set scales large bag sizes 
furthermore provides new framework mi learning mi problem converted single instance setting em estimate instance responsible label bag 
multiple instance mi learning model received attention 
model training example set bag instances single label equal maximum label instances bag 
individual instances bag labels 
goal learn accurately predict label previously unseen bags 
standard supervised learning viewed special case mi learning bag holds single instance 
mi learning model originally motivated drug activity prediction problem instance possible conformation shape molecule bag contains low energy conformations molecule 
molecule active binds strongly target protein conformations inactive conformation binds protein 
problem predict label active inactive molecules conformations 
mi learning model formalized dietterich seminal developed mi algorithms learning axis parallel rectangles provided benchmark musk data sets :10.1.1.136.5563
significant amount research directed development mi algorithms different learning models 
maron raton applied multiple instance model task recognizing person series images labeled positive contain person negative 
technique learn descriptions natural scene images waterfall retrieve similar images large image database learned concept 
model data mining applications 
musk data sets boolean labels algorithms handle labels desirable real world applications 
example binding affinity molecule receptor quantitative real value classification binding strength preferable binary 
prior research mi learning restricted concept learning boolean labels 
mi learning real value labels performed extensions diverse density dd nn algorithms mi regression 
general purpose mi learning technique em dd combines em extended dd algorithm 
algorithm applied boolean real value labeled data results compared corresponding mi learning algorithms previous 
addition effects number instances bag number relevant features performance em dd algorithm evaluated artificial data sets 
second contribution new general framework mi learning converting mi problem single instance setting em 
similar approach ray page 
background dietterich algorithms learning mi model :10.1.1.136.5563:10.1.1.136.5563
best performing algorithm iterated discrim starts point feature space grows box goal finding smallest box covers instance positive bag instances negative bag 
resulting box expanded statistical technique get better results 
test data musk tune parameters algorithm 
parameters musk musk 
auer algorithm learns simple statistics find halfspaces defining boundaries target apr avoids potentially hard computational problems required heuristics iterated discrim algorithm 
wang zucker proposed lazy learning approach applying variant nearest neighbor algorithm nn refer citation knn bayesian nn 
ramon de raedt developed mi neural network algorithm 
builds heavily diverse density dd algorithm maron lozano erez :10.1.1.51.7638
describing shape molecule features view conformation molecule point dimensional feature space 
diverse density point feature space probabilistic measure different positive bags instance near far negative instances intuitively diversity density hypothesis just likelihood respect data target 
high diverse density indicates candidate true concept 
formally define general mi problem boolean real value la dd likelihood measurement originally defined extended real value labels :10.1.1.51.7638:10.1.1.51.7638
labeled data consists set bags fb bm labels 
bm bag fb ij ij denote th instance bag assume labels instances ij boolean labels real value labels maxf diverse density hypothesized target point defined dd pr pr pr pr pr pr pr assuming uniform prior hypothesis space independence pairs bayes rule maximum likelihood hypothesis hdd defined arg max pr arg max pr arg min gamma log pr label label correct hypothesis 
extended dd algorithm pr estimated gamma gamma label 
labels boolean formulation exactly cause estimator original dd algorithm 
applications influence feature label varies greatly 
variation modeled dd algorithm associating attribute unknown scale factor 
target concept really consists values dimension ideal attribute value scale value 
assumption binding strength drops exponentially similarity conformation ideal shape increases generative model introduced maron lozano erez estimating label bag hypothesis fh hn label max exp gamma ijd gamma scale factor indicating importance feature feature value dimension ijd feature value instance ij dimension gamma log pr denote negative logarithm dd :10.1.1.51.7638:10.1.1.51.7638
dd algorithm uses step gradient descent search find value minimizes maximizes dd :10.1.1.51.7638:10.1.1.51.7638
ray page developed multiple instance regression algorithm handle real value labeled data 
assumed underlying linear model hypothesis applied algorithm artificial data 
similar current em select instance bag multiple regression applied mi learning 
algorithm em dd describe em dd compare original dd algorithm 
reason mi learning difficult ambiguity caused knowing instance important 
basic idea em dd view knowledge instance corresponds label bag missing attribute estimated em approach way similar em mi regression 
em dd starts initial guess target point obtained standard way trying points positive bags repeatedly performs steps combines em dd search maximum likelihood hypothesis 
step step current hypothesis pick instance bag generative model responsible label bag 
second step step step gradient ascent search quasi newton search standard dd algorithm find new maximizes dd 
maximization step completed reset proposed target return step algorithm converges 
pseudo code em dd 
briefly provide intuition em dd improves accuracy computation time dd algorithm 
basic approach dd gradient search find value maximizes dd 
search step dd algorithm uses points bag maximum occurs equation computed 
prior diverse density algorithms softmax approximation maximum differentiable dramatically increases computation complexity introduces additional error parameter selected softmax 
comparison em dd converts multiple instance data single instance data removing point bag step greatly simplifies search step maximum occurs equation removed step 
removal softmax greatly decreases computation time 
addition believe em dd helps avoid getting caught local minimum major changes hypothesis switches point selected bag 
provide sketch proof convergence em dd 
note iteration set instances selected step step find unique hypothesis corresponding dd dd 
iteration dd dd algorithm terminate 
dd dd means different set instances selected 
iteration continue dd decrease monotonically set instances selected repeat 
finite number sets instances selected step algorithm terminate finite number iterations 
guarantee convergence rate em algorithms 
usually decreases dramatically iterations begins flatten 
empirical tests beneficial allow increase slightly escape local minima restrictive termination condition gamma dd delta dd number iterations greater 
modification reduces training time gaining comparable results 
modification convergence proof restricting number iterations 
experimental results section summarize experimental results 
reporting results musk benchmark data sets provided dietterich :10.1.1.136.5563
data sets contain feature vectors describing surface low energy conformations molecules musk molecules musk roughly half molecules known smell remainder 
musk data set smaller having fewer bags molecules fewer instances bag average musk versus musk 
prior highly tuned iterated discrim algorithm dietterich gave best performance musk musk 
maron lozano erez main partition fd fold cross validation gamma training data validation data pick random positive bags union instances selected bags instance em dd min return avg em dd fh hn initial hypothesis dimension bag step arg ij ij arg max step return pseudo code em dd indicates number different starting bags pr ij exp gamma ijd gamma :10.1.1.51.7638:10.1.1.51.7638
pr calculate gamma gamma pr linear model exp gamma gamma pr gaussian model pr ij pr ij 
summarize generally held belief performance reported apr involves choosing parameters maximize test set performance probably represents upper bound accuracy musk data set 
em dd tuning outperforms previous algorithms 
consistent way past results reported musk benchmarks report average accuracy fold cross validation value returned main 
em dd obtains average accuracy musk musk 
summary performance different algorithms musk musk data sets table 
addition data sets false negative errors em dd important drug discovery application final hypothesis filter potential drugs false negative error means potential drug molecule tested minimize errors 
compared standard dd algorithm em dd random bags musk random bags musk versus positive bags dd starting point algorithm 
results reported threshold tuned leave cross validation reported results threshold value tuned :10.1.1.51.7638:10.1.1.51.7638
importantly em dd runs times faster dd musk times faster applied musk 
table comparison performance musk musk data sets measured giving average accuracy runs fold cross validation 
algorithm musk musk accuracy accuracy em dd iterated discrim citation knn bayesian knn diverse density multi instance neural network addition superior performance musk data sets em dd handle real value labeled data produces real value predictions :10.1.1.51.7638:10.1.1.51.7638:10.1.1.136.5563
results real data set affinity real value labels artificial data sets generated technique earlier 
data sets starting points points bag highest dd value 
result shown table 
affinity data set features bags average points bag 
bags labels high considered positive 
gaussian version generative model obtained squared loss linear model performed slightly better loss 
contrast standard diverse density algorithm loss 
em dd gained better performance dd artificial data algorithms best result affinity data obtained version citation knn works real value data loss 
think affinity data set suited nearest neighbor approach negative bags labels actual predictions negative bags better citation knn 
study sensitivity em dd number relevant attributes size bags tests performed artificial data sets different number relevant features bag sizes 
shown table similar dd algorithm performance em dd degrades number relevant features decreases 
behavior expected scale factors initialized value features relevant adjustment needed algorithm succeed 
comparison dd em dd robust change number relevant features 
example shown number relevant features em dd dd algorithms perform correlation actual labels predicted labels 
number relevant features decreases correlation actual predicted labels dd em dd provide predictions labels 
intuitively size bags increases ambiguity introduced data performance algorithms expected go 
somewhat jonathan greene provided affinity data set 
due proprietary nature publicly available 
see description data sets 
table performance data real value labels measured squared loss 
data set rel 
features pts bag em dd dd affinity surprisingly performance em dd improves number examples bag increases 
believe partly due fact points bag chance bad starting point highest diverse density higher bags large 
addition contrast standard diverse density algorithm time complexity em dd go size bags increased instance selection step time complexities dominant step essentially data sets different bag sizes 
fact em dd scales large bag sizes performance running time important real drug discovery applications bags quite large 
directions avenues 
believe em dd refined obtain better performance finding alternate ways select initial hypothesis scale factors 
option result different learning algorithm starting point em dd refine hypothesis 
currently studying application em dd algorithm domains content image retrieval 
algorithm diverse density likelihood measurement believe perform applications standard diverse density algorithm worked 
addition em dd mi regression framework convert multiple instance data single instance data supervised learning algorithms applied 
currently working general methodology develop new mi learning techniques supervised learning algorithms em 
acknowledgments authors gratefully acknowledge support nsf ccr 
dan useful discussions 
jonathan greene provided affinity data set 
goldman zhang 

multiple instance learning real valued data 
proceedings th international conference machine learning pp 

san francisco ca morgan kaufmann 
auer 
learning mult instance examples empirical evaluation theoretical approach 
proceedings th international conference machine learning dd dd actual predicted actual predicted em dd em dd actual predicted actual predicted comparison em dd dd real value labeled artificial data different number relevant features 
axis corresponds actual label axis gives predicted label 
pp 

san francisco ca morgan kaufmann 
dempster laird rubin 

maximum likelihood incomplete data em algorithm 
journal royal statistics society series 
dietterich lathrop lozano erez :10.1.1.136.5563

solving problem axis parallel rectangles 
artificial intelligence 
maron 

learning ambiguity 
doctoral dissertation mit ai technical report 
maron lozano erez :10.1.1.51.7638:10.1.1.51.7638

framework multiple instance learning 
neural information processing systems 
cambridge ma mit press 
maron 

multiple instance learning natural scene classification 
proceedings th international conference machine learning pp 

san francisco ca morgan kaufmann 
press teukolsky vetterling flannery 

numerical recipes art scientific computing 
cambridge university press new york second edition 
ramon de raedt 

multi instance neural networks 
proceedings icml workshop attribute value relational learning 
ray page 

multiple instance regression 
proceedings th international conference machine learning pp 

san francisco ca morgan kaufmann 


learning single multiple instance decision trees computer security applications 
doctoral dissertation 
department computer science university turin torino italy 
wang zucker 

solving multiple instance learning problem lazy learning approach 
proceedings th international conference machine learning pp 

san francisco ca morgan kaufmann 
