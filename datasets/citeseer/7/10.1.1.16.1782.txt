craig boutilier dept computer science university british columbia vancouver bc cs ubc ca context specific independence bayesian networks nir friedman dept computer science stanford university stanford ca nir cs stanford edu provide qualitatively representing conditional independence properties distribution 
allows natural compact representation distribution eases knowledge acquisition supports effective inference algorithms 
known certain independencies capture qualitatively bayesian network structure independencies hold certain contexts specific assignment values certain variables 
propose formal notion context specific independence csi regularities conditional probability tables cpts node 
technique analogous separation determining independence holds network 
focus particular qualitative representation scheme tree structured cpts capturing csi 
suggest ways representation support effective inference algorithms 
particular structural decomposition resulting network improve performance clustering algorithms alternative algorithm cutset conditioning 
power bayesian network bn representations probability distributions lies efficient encoding independence relations random variables 
independencies exploited provide savings representation distribution ease knowledge acquisition domain modeling computational savings inference process 
objective increase power refining bn representation capture additional independence relations 
particular investigate independence certain variable assignments inference refers computation posterior distribution conditioned evidence 
moises goldszmidt sri international ravenswood way ek menlo park ca moises erg sri com daphne koller dept computer science stanford university stanford ca koller cs stanford edu exploited bns way independence variables exploited current bn representations inference algorithms 
formally characterize structured representation catalog number advantages provides 
bn directed acyclic graph node represents random variable interest edges represent direct correlations variables 
absence edges variables denotes statements independence 
precisely say variables independent set variables bn encodes statement independence random variable variable independent non descendants network state parents 
example network shown independent independence statements local statements read network structure polynomial time graph theoretic criterion called separation 
values variables addition representing statements independence bn represents particular distribution satisfies independencies 
distribution specified set conditional probability tables cpts 
node associated cpt describes conditional distribution different assignments values parents 
independencies encoded structure network joint distribution computed simply multiplying cpts 
naive form cpt encoded tabular representation assignment values parents requires specification conditional distribution example assuming binary need specify distributions parameters 
size representation exponential number parents 
furthermore representation fails capture certain regularities node distribution 
cpt example equal constant regardless values taken holds need consider context specific independence values clearly need specify distributions 
regularities occur known bn products microsoft bayesian networks modeling tool knowledge industries incorporated special mechanisms knowledge acquisition interface allow user easily specify corresponding cpts 
provide formal foundation regularities notion context specific independence 
intuitively example regularities cpt ensure independent context dependent con text 
assertion context specific independence csi restricted statements variable independence encoded bn structure 
show statements extend advantages variable independence probabilistic inference ease knowledge elicitation compact representation computational benefits inference 
certainly suggest extensions bn representation order capture additional independencies potentially enhance inference 
known examples include heckerman similarity networks related multinets asymmetric representations decision making poole probabilistic horn rules encode dependencies variables 
representation emphasize decision trees encode cpts 
intent formalize notion csi study representation part general framework propose methods utilizing representations enhance probabilistic inference algorithms 
section defining context specific independence formally introducing simple local transformation bn arc deletion csi statements readily determined separation 
section discusses detail trees represent cpts compactly representation exploited algorithms determining csi 
section offers suggestions speeding probabilistic inference advantage csi 
network transformations may reduce clique size clustering algorithms techniques csi associated arc deletion strategy cutset conditioning 
conclude discussion related notions research directions 
context specific independence arc deletion consider finite set discrete random variables variable may take values finite domain 
capital letters variable names lowercase letters denote specific values taken variables 
set values denoted val sets variables denoted boldface capital letters assignments values variables sets denoted boldface lowercase letters val obvious way 
definition joint probability distribution variables subsets conditionally independent denoted val val relationship holds val summarize statement values bayesian network directed acyclic graph nodes correspond random variables edges represent direct dependencies variables 
graph structure encodes set independence assumptions representing assertion node independent non descendants par ents statements local involve node parents statements arbitrary sets variables follow local assertions 
read structure graph theoretic path criterion called separation tested polynomial time 
bn represents independence information particular distribution require independencies encoded hold precisely said map distribution independence sanctioned separation holds bn required minimal map sense deletion edge network destroys network respect distribution describes 
bn permits compact representation distribution need specify variable conditional probability table cpt encoding parameter possible value variables 
see details 
graphical structure bn capture independence relations form indepen hold assignment values variables interested independencies hold certain contexts 
definition pairwise disjoint sets variables 
contextually independent context val denoted assertion similar equation evidence requires independence hold particular assignment easy see certain local statements form verified direct examination cpt example verify checking cpt value depend values 
section explores different representations cpts allow check local statements efficiently 
objective establish analogue principle separation computationally tractable method deciding validity non local statements 
turns problem solved simple reduction problem validating variable independence statements simpler network 
problem efficiently solved separation 
definition edge called vac uous context bn context define bn results deleting vacuous edges say csi separated context separated note statement local statement determined inspecting cpt decide csi separation transforming local statements delete vacuous edges separation resulting network 
show notion csi separation sound strong sense complete local independence statements 
network structure set local statements say csi map distribution independencies implied hold holds csi separated con text say perfect csi map implied independencies ones hold csi separated context theorem network structure set local independencies distribution consistent theorem establishes soundness procedure 
procedure complete 
procedure may independencies detect local independencies network structure 
shows sense procedure provides best results hope derive solely structural properties distribution 
csi map theorem network structure set local independencies 
exists distribution consistent perfect structured representations cpts context specific independence corresponds regularities cpts 
section discuss possible representations capture regularity qualitatively way bn structure qualitatively captures conditional independence 
representations admit effective algorithms determining local csi statements exploited probabilistic inference 
reasons space focus primarily tree structured representations 
general view cpt function maps val distributions compact representation cpts simply representation function exploits fact distinct elements val associated distribution 
compactly represent cpts simply partitioning space val regions mapping distribution 
generally represent partitions set mutually exclusive exhaustive generalized propositions variable set generalized simply truth functional combination specific variable assignments may par characterized generalized proposition proposition associated distribution representation fully general easily support probabilistic inference inference csi 
fortunately convenient representations type partitioning 
example canonical logical form minimal cnf 
classification trees known machine learning community decision trees popular function representation partitions state space induced labeling branches tree 
representations number advantages including fact vacuous edges detected reduced cpts produced linear time size cpt representation 
expected tradeoff compact cnf tree representation cpt larger exponentially larger worst case minimal representation terms generalized propositions 
purposes focus cpt trees tree structured cpts deferring discussion analogous results cnf representations graph structured cpts form discussed longer version :10.1.1.1.5124
major advantage tree structures naturalness branch labels corresponding sense rule structure see 
intuition particularly easy elicit probabilities directly human expert 
show subsequent sections tree structure utilized speed bn inference algorithms 
discuss trees amenable studied approximation learning network tree cpt tree representation tree methods 
section show admit fast algorithms detecting csi 
general operations wish perform context determine arc variable vacuous second determine reduced cpt condition operation carried set evidence reflect changes parents implied context specific independencies examine perform types operations cpt trees 
avoid confusion node arc denote nodes arcs tree opposed nodes arcs bn 
illustrate ideas consider cpt tree variable 
left arcs labeled true right arcs false 
representation relatively easy tell parents rendered independent context assume tree represents cpt context clearly remains relevant rendered dependent rendered independent intuitively distribution depend know path root leaf consis tent fails mention definition path cpt tree set arcs lying root leaf 
labeling path assignment variables induced labels path 
variable occurs path nodes path tests value path consistent context iff labeling path consistent assignment values theorem cpt tree parents 
context 
lie path consistent edge vacuous provides sound test context specific independence valid independencies discovered 
test complete cpt structures represented minimally tree 
instance suppose example 
context tell irrelevant inspection choice variable ordering prevents detecting criterion theorem 
test complete sense edge vacuous tree structure 
theorem cpt tree context 
occurs path consistent exists assignment parameters leaves vacuous shows test described fact best test uses structure tree actual probabilities 
similar spirit separation detects conditional independencies possible structure network detect independencies hidden quantification links 
conditional independence belief networks need soundness order exploit csi inference 
produce reduced cpt tree representing cpt conditioned context assume assignment variables containing certain parents cpt tree root immediate subtrees reduced cpt tree defined recursively follows label variables consists subtrees label subtree pointed arc labeled value reduced tree produced tree traversal time 
proposition variable labels node consistent occurs path implies appears deemed vacuous test described 
reduced tree determining list arcs deleted requires simple tree traversal reducing tree gives efficient sound test determining context specific independence parents exploiting csi probabilistic inference network representations distributions offer considerable computational advantages probabilistic inference 
graphical structure bn lays bare variable independence relationships exploited known algorithms deciding information relevant say query best summarized 
similar fashion compact representations cpts trees csi 
section describe csi exploited various bn inference algorithms specifically stressing particular uses clustering cutset conditioning 
space precludes detailed presentation provide basic intuitions 
emphasize means ways bn inference employ csi 
simple decomposition node cpt new node effective decomposition utilizing csi 
network transformations clustering compact representations cpts novel idea 
instance noisy distributions generalizations allow compact representation assuming parents independent casual contributions value distributions fall general category distributions satisfying causal independence 
distributions perform structural transformation original network resulting new network independencies encoded qualitatively network structure 
essentially transformation introduces auxiliary variables network connects cascading sequence deterministic nodes 
csi quite distinct causal independence similar ideas applied structural network transformation capture certain aspects csi directly bn structure 
transformations useful uses inference algorithm clustering 
roughly speaking clustering algorithms construct join tree nodes denote overlapping clusters variables original bn 
cluster clique encodes marginal distribution val set nodes cluster 
inference process carried join tree complexity determined largely size largest clique 
structural transformations prove worthwhile 
clustering process requires family bn node parents subset clique join tree 
family large set values val clique poor performance clustering algorithms 
transformation reduces number values family offer considerable computational savings clustering algorithms 
lead large order understand transformation consider generic node bayesian network 
parents remaining parents 
assume simplicity 
intuitively view value random variable outcome conditional variables value take true value take false 
conduct variables decided separately value revealed appropriate value chosen 
formally define random variable conditional distribution depends similarly define variable variable equal equal note variables set values perspective allows replace node network subnetwork illustrated 
node deterministic node call multiplexer node takes value depending value 
cpt 
generic node decomposition particularly useful 
thing total size new cpts exactly size original cpt resulting structure cycles admit effective decompositions cliques 
exhibits significant amount csi type transformation result far compact representation 
example assume depends true false 
parents 
variables binary new representation requires cpts entries plus single deterministic multiplexer node predetermined distributions 
contrast original representation single cpt entries 
furthermore structure resulting network may allow construction join tree smaller cliques 
transformation uses structure cpt tree apply decomposition recursively 
essentially node decomposed parent root cpt tree 
conditional nodes binary case cpt subtrees node cpt resulting conditional nodes decomposed recursively similar fashion 
example node corresponding decomposed node decomposed decomposition network tree 
nodes decomposed parents 
decomposition nodes possible beneficial cpts nodes unstructured complete tree depth 
clear procedure beneficial structure cpt node 
general want decomposition cpt node full tree 
note includes leaves special case 
structural transformation noisy nodes decomposition allow clustering algorithms form smaller cliques 
transformation nodes network order size cpt tree representations generally far fewer parents 
example describes transformation cpt tree 
transformation eliminated family parents introduced smaller families 
currently working implementing ideas testing effectiveness practice 
note large fraction auxiliary nodes introduce multiplexer nodes deterministic function parents 
nodes exploited clustering algorithm 
note reduction clique size resulting computational savings depend heavily structure decision trees 
similar phenomenon occurs transformation effectiveness depends order choose cascade different parents node 
case noisy graphical structure transformed bn capture independencies implicit cpts 
particular csi relations induced particular value assignments read transformed structure 
noisy analogue structurally represent node parents independent node observed false observed true 
cases csi relations captured deterministic relationships transformation node parents independent node set false 
multiplexer node value depends parent value selecting parent original variable known 
cutset conditioning noisy tree representations join tree algorithm take advantage fixed structural independencies 
static precompilation difficult algorithm take advantage independencies occur certain circumstances new evidence arrives 
dynamic algorithms cutset conditioning exploit context specific independencies effectively 
investigate cutset algorithms modified exploit csi representation 
cutset conditioning algorithm works roughly follows 
select cutset set variables instantiated render network singly connected 
inference carried reasoning cases case possible assignment variables cutset assignment instantiated evidence call polytree algorithm performs inference resulting network 
results calls combined give final answer 
running time largely determined number calls polytree algorithm val 
csi offers obvious advantage inference algorithms conditioning loop cutsets 
instantiating particular variable certain value order cut loop csi may render arcs vacuous cutting additional loops need variables 
instance suppose network solved cutset optimal strategy val large 
typically solve reduced singly connected network val val val times assignment val ues recognizing fact connections vacuous context need instantiate assign val val replaces network evaluations single evaluation 
instantiation longer ignored edges vacuous context 
capture phenomenon generalize standard notion cutset considering tree representations cutsets 
reflect need instantiate certain variables contexts order render network singly connected 
intuitively conditional cutset tree interior nodes labeled variables edges la fact heavily utilized algorithms targeted specifically noisy networks bn networks 
believe similar ideas applied compact cpt representations noisy 
valid conditional cutsets sets variable values 
branch tree corresponds set assignments induced fixing variable value edge 
tree conditional cutset branch tree represents context renders network singly connected set assignments mutually exclusive exhaustive 
examples conditional cutsets bn illustrated obvious compact cutset tree representation standard cutset fails exploit structure cpt requiring evaluation instantiation conditional cutset hand extension classical cutset inference fairly obvious 
consider assignment values variables determined branches tree instantiate network assignment run polytree algorithm resulting network combine results usual 
clearly complexity algorithm function number distinct paths 
crucial find heuristic algorithms constructing small conditional cutsets 
focus computationally intensive heuristic approach exploits csi existence vacuous arcs maximally 
algorithm constructs conditional cutsets incrementally fashion similar standard heuristic approaches problem 
discuss computationally motivated shortcuts near section 
standard greedy approach cutset construction selects nodes cutset heuristic value weight variable val degree net graph weight measures needed instantiate cutset degree vertex gives idea arc cutting potential incident outgoing edges mean larger chance cut loops 
order extend heuristic deal csi estimate extent arcs cut due csi 
obvious approach adding number arcs rendered vacuous averaging values reasonably straightforward unfortunately somewhat explain need set valued arc labels 
standard cutset algorithm weights required combine answers different cases obtained polytree computations 
assume network preprocessed legitimate cutsets selected easily 
see details 
myopic 
particular ignores potential arcs cut subsequently 
example consider family tree reflecting cpt adding cutset causes additional arcs cut heuristic value things equal 
clearly desirable choice value conditional cutsets produced subsequently small 
actual number arcs cut selecting node cutset consider expected number arcs cut 
considering children distinct distributions structured representation cpt child instantiation size reduced cpt 
log value expected number parents required child known fewer parents indicating potential arc cutting 
average number values may take sum expected number cut arcs children 
measure plays role cutset heuristic 
precisely size number entries fixed network size reduced cpt context assume parent 
define expected number parents ep parents val parents expected number arc deletions instantiated children val parents ep val gives reasonably accurate picture value adding conditional cutset network cutset construction algorithm proceeds recursively adding heuristically selected node branch tree structured cutset adding arcs cutset tree value val constructing new network instantiations reflects csi extending new arcs recursively selecting node looks best new network corresponding branch 
roughly sketch follows 
algorithm begins original network 
remove singly connected nodes leaving nodes remain return empty cutset tree 

choose node minimal 

val construct removing vacuous arcs replacing cpts reduced cpts 
return tree labels root arc emanates root node attached arc tree produced recursively calling algorithm network step standard 
step important realize heuristic value determined respect current network context established existing branch cutset 
step required ensure selection variable reflects fact part current context 
step emphasizes conditional nature variable selection different variables may selected different values steps capture key features approach certain computational implications turn attention 
algorithm exploits csi great degree requires computational effort greater normal cutset construction 
cutset structured tree representation standard cutset potentially exponentially larger full tree 
algorithm run online tree completely stored variables instantiated particular values conditioning selection variable 
conceptually amounts depth construction tree partial complete branch stored 
addition add optional step step detects structural equivalence networks say instantiations structural effect arcs representation reduced cpts need instantiations subsequently cutset construction 
step create new arc cutset tree labeled set 
reduces number graphs need constructed concomitant computations discussed 
completely unstructured settings representation conditional cutset size similar normal cutset 
apart amount information conditional cutset effort needed decide variables add branch heuristic component involved vertex degree 
unfortunately value fixed case involve single set prior computations recomputed step reflect variable instantiations gave rise current network 
part re evaluation requires cpts updated step 
fortunately number cpts updated assignment small children current graph need cpts updated 
done cpt reduction algorithms described efficient 
updates affect heuristic estimates parents spouses need value recomputed 
amount required large reduction number network evaluations usually compensate extra 
currently process implementing algorithm test performance practice 
directions currently order enhance algorithm 
involves developing ideal tractable methods conditional cutset construction 
example select cutset standard means considerations described order line variable instantiations cutset 
direction involves integrating ideas computation saving ideas standard cutset algorithms 
concluding remarks defined notion context specific independence way capturing independencies induced specific variable assignments adding regularities distributions representable bns 
results provide foundations csi representation role inference 
particular shown csi determined local computation bn specific mechanisms particular trees allow compact representation cpts enable efficient detection csi 
furthermore csi tree structured cpts speed clustering cutset style algorithms 
mentioned considerable extending bn representation capture additional independencies 
notion csi related heckerman calls subset independence similarity networks 
approach significantly different try capture additional independencies providing structured representation cpts single network similarity networks multinets rely family networks 
fact approach described decision trees closer spirit poole rule representations networks 
arc cutting technique network transformation introduced section reminiscent network transformations introduced pearl probabilistic calculus action 
semantics actions proposed viewed instance csi 
mere coincidence easy see networks representing plans influence diagrams usually contain significant amount csi 
effects actions decisions usually take place specific instantiation variables vacuous trivial instantiations realized 
testimony fact adding additional structure influence diagrams smith fung shachter boutilier decision trees represent cpts context markov decision processes 
number research directions needed elaborate ideas expand role csi compact cpt representations play probabilistic reasoning 
currently exploring different cpt representations decision graphs potential interaction csi causal inde noisy model 
deeper examination network transformation algorithm section empirical experiments necessary determine circumstances reductions clique size significant 
similar studies conducted conditional cutset algorithm section variants 
particular determine extent overhead involved conditional cutset construction 
currently pursuing directions 
csi play significant role approximate probabilistic inference 
cases may wish trade certain amount accuracy speed inference allow compact representation ease knowledge acquisition 
instance cpt exhibiting little structure little csi compactly represented may require full tree 
cases local dependence weaker circumstances 
consider tree suppose different reflecting fact ence relatively weak case true false 
case may assume entries approximating true cpt decision tree structure tree 
saving representation inference techniques comes expense accuracy 
ongoing show estimate error local cpts allowing practical greedy algorithms trade error computational gain derived simplification network 
tree representations turn particularly suitable regard 
particular show decision tree construction machine learning community construct appropriate cpt tree full conditional probability table pruning algorithms tree acquired directly user simplify cpt tree order allow faster inference 
structured representation cpts proven beneficial learning bayesian networks data 
due compactness representation learning procedures capable inducing networks better emulate true complexity interactions data 
represents starting point rigorous extension bayesian network representations incorporate context specific independence 
seen csi deep far ranging impact theory practice aspects probabilistic inference including representation inference algorithms approximation learning 
consider exploration development ideas promising avenue research 
dan geiger adam grove harada zohar yakhini useful discussions 
performed nir friedman moises goldszmidt rockwell palo alto science center daphne koller ley 
supported university california president postdoctoral fellowship koller arpa contract goldszmidt ibm graduate fellowship nsf iri friedman nserc research ogp boutilier 
becker geiger 
approximation algorithms loop cutset problem 
uai pp 

boutilier dearden goldszmidt 
exploiting structure policy construction 
ijcai pp 

bryant 
graph algorithms boolean function manipulation 
ieee transactions computers 
darwiche 
conditioning algorithms exact approximate inference causal networks 
uai pp 

friedman goldszmidt 
learning bayesian networks local structure 
uai 
fung shachter 
contingent influence diagrams 
unpublished manuscript 
geiger heckerman 
probabilistic reasoning 
uai pp 

koller 
constructing flexible dynamic belief networks order probabilistic knowledge bases 
ecsqaru pp 


heckerman 
probabilistic similarity networks 
phd thesis stanford university 
heckerman 
causal independence knowledge acquisition inference 
uai pp 

heckerman breese 
new look causal independence 
uai pp 

jensen andersen 
approximations bayesian belief universes knowledge systems 
uai pp 

lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
journal royal statistical society 
pearl 
probabilistic intelligent systems networks plausible inference 
morgan kaufmann 
pearl 
probabilistic calculus action 
uai pp 

poole 
probabilistic horn abduction bayesian networks 
artificial intelligence 
quinlan 
learning 
morgan kaufmann 
smith matheson 
structuring conditional relationships influence diagrams 
operations research 
srinivas 
generalization noisy model 
uai pp 

stillman 
heuristics finding loop cutsets multiply connected belief networks 
uai pp 

suermondt cooper 
initialization method conditioning bayesian belief networks 
artificial intelligence 
