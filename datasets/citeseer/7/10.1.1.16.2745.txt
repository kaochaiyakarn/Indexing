appart hybrid neural network adaptive resonance theory universal function approximation luis mart alberto luciano garc universit degli studi di udine dipartimento di matematica informatica udine italy universidad de la facultad de matem atica computaci la cuba 

appart adaptive resonance theory low parameterized neural model incrementally approximates continuous valued multidimensional functions noisy data biologically plausible processes 
appart performs higher order nadaraya watson regression interpreted fuzzy logic standard additive model 
describe appart dynamics training 
discuss approach hybrid neural systems deal theoretical foundations function approximation method 
benchmark problems solved order study appart application point view compare results ones obtained models 
modi cations appart formulation aimed improving appart eciency proposed tested 
neural networks successfully applied number problems particular problems deep knowledge phenomena methods tend fail 
approximation functions samples quite common real life class problem 
neural models eciently solve function approximation problems general terms particular problems classi cation pattern recognition clustering time series prediction 
success due models main characteristics particular inductive learning generation model training data generalization previously unseen data ability handle incoherent incomplete noisy data compact distributed knowledge representation 
years steadily increasing amount attention paid called hybrid neural systems wermter authors wish dipartimento di matematica informatica universit degli studi di udine support conception elaboration 
luis mart sun 
approaches aimed create knowledge systems preserve aforementioned features neural systems incorporate characteristics symbolic arti cial intelligence 
features systems example allow insertion priori problem domain knowledge form rules predicates automata language grammars network allow extraction formal interpretation network provide justi cation explanation network response incorporate network dynamics kinds learning deductive manna waldinger causal ge ner abductive kakas kowalski toni 
symbolic arti cial intelligence point view hybrid approaches allow system encode highly non linear relations dicult express symbolically improve models sensitivity noise typical lack generalization power robustness dealing partially corrupted incomplete information 
lot discussion possible create system uni es characteristics biologically plausible neural networks psychologically plausible symbolic arti cial intelligence paradigms unique arti cial knowledge system 
argued fodor pylyshyn neural networks lack paradigmatic features symbolic systems represent mental states combinatorial syntax semantics codings hold structure sensitivity higher mental processes 
arguments cast shadow doubt realization hybrid system share features computational models 
paradigms rely working hypotheses cognition modeled computation see mcculloch pitts 
theory system exist honavar 
years number hybrid connectionist models proposed 
provide degree realization relevant symbolic information processes variable binding sun browne sun predicate uni cation weber 
set results ful lls expectations created hybrid systems provide feasible foundation creation systems 
formulation systems repeatedly addressed diverse neural network approaches see wermter macintyre browne sun reviews 
neural models commonly foundation approaches ones universal function approximators multi layer perceptron mlp rumelhart hinton williams radial basis function networks rbf moody darken poggio girosi 
models deal called high parameterization 
high parameterization appart hybrid art network function approximation existence hard guess parameters number layers network number nodes layer form activation function node 
inconvenient hinders achievement adequate results problems recognizable model 
low parameterized model simpli es application optimization methods genetic algorithms goldberg simulated annealing kirkpatrick gelatt vecchi tabu search glover laguna order obtain optimal networks solving particular problem 
optimization methods shown discrete results applied models mlp rbf 
kind results partially caused exhaustive search computationally intense processes involved computation methods 
general regression neural network ler hartman low parameterized extension rbf networks successfully solved function approximation problems 
model critically su ers curse dimensionality bellman training set grows 
attempt improve model introducing simpli cations model providing enhanced training algorithm 
problem curse dimensionality remains properly handled 
rbf networks share localized nature input prototype layer nodes 
property networks suitable representing symbolic knowledge shown feldman 
smolensky sun 
neural networks perspective feature allows dynamic local modi cation network topology nodes weights training proceeds 
issue nding right network topology set weights problem repeatedly addressed various heuristic training approaches 
approaches giordana esp mill focused resolution issue general terms 
approaches focused solving speci formulations 
particular oriented nd optimum weights predetermined network architecture gori broomhead lowe moody darken orr wilson martinez generate classi cation pattern recognition network architecture corresponding weights fritzke hwang bang martinetz ahmed chan :10.1.1.43.6647
adaptive resonance theory art grossberg theory cognition recognized modern arti cial neural network theory achievements 
art networks features match stable learning intrinsic self organization appealing constructing hybrid system 
search process involved luis mart production network output art networks interest interpreted sort hypothesis testing mechanism carpenter grossberg 
features sensibly improve high parameterization problem associated mlp rbf networks 
orts dedicated art models addressed problem creating hybrid system problem universal function approximation 
focused development unsupervised learning clustering models art carpenter grossberg art carpenter grossberg art carpenter grossberg fuzzy art carpenter grossberg rosen gaussian art williamson supervised learning models capable learning multidimensional maps artmap carpenter grossberg reynolds fuzzy artmap carpenter grossberg reynolds rosen gaussian artmap williamson ram williamson art carpenter ross distributed artmap carpenter artmap ic carpenter fusion artmap carpenter grossberg lesher boosted artmap healy 
works deal art hybrid systems cascade artmap tan art function approximation marriott harrison cano ara opez cano omez opez cano ara opez cano 
describes appart mart garc art low parameterized neural model incrementally approximates multidimensional functions noisy data biologically plausible processes 
appart performs higher order nadaraya watson nadaraya watson regression interpreted fuzzy system 
appart allows line insertion fuzzy rules transformation network set fuzzy rules 
provides straight way justifying network response rules encoded explicitly network 
part discuss appart relations function approximation models appart theoretical power function approximation method 
propose modi cations original formulation asymmetric gaussian receptive elds appart input classi cation layer online estimation initialization value widths gaussian activated input layer nodes 
changes aimed rendering resulting model ecient 
rest rst describe appart general dynamics neural network 
deal interpretation hybrid neural system function approximation capabilities 
benchmark problems solved order study appart application point view compare results ones obtained models 
appart hybrid art network function approximation deal modi cations aforementioned analyze practical results yielded 
dynamics appart appart creates classes similar inputs similar outputs 
class desired output value assigned 
output network input weighted mean degree membership input classes stored desired output values assigned class 
match tracking mechanism induces creation speci classes prediction network di ers expected output degree 
appart see fig 
layer erent input nodes classi cation layer prediction layer output layer layer stores classes inputs 
activation combined measure similarity input prototype class size class 
layer dynamics gaussian art 
art models fuzzy art art task 
fuzzy art inputs features restricted interval 
usually inputs di erent range normalized scaled 
processes eventually destroy information contained input sample duda hart 
cases especially online learning situations exact range inputs guessed set samples 
models gaussian art art useful overcoming restrictions inputs fuzzy art 
art classi es inputs form nx category freeman 
property desirable circumstances suitable function approximation applications 
network output obtained propagating output layer layers design similar summation output layers respectively 
equations input input layer propagated layer 
nodes committed 
committed node models local density input space gaussian receptive elds mean standard deviation node activated satis es match criterion 
match function exp ji ji luis mart fig 

appart architecture 
input propagated 
activation measures degree membership de ned categories 
meet gf vigilance node committed 
prediction calculated meet go error criterion match tracking algorithm takes place 
greater vigilance parameter input strength node computed ji measure node priori activation probability 
activation node calculated normalizing node input strength appart hybrid art network function approximation activation interpreted conditional probability jjx node best encodes input prediction output layers calculate prediction network 
layer contains types nodes nodes features output vector node 
nodes calculate activations respectively weighted sums activation vector kj kj represents sum values output feature learned node active 
weight vector measure node learned 
nodes output layer connected connections corresponding node node 
outputs ak represent probable output value oj expected input 
performing simulation steps taken handle correctly di erence semantics zero activation prediction 
principal characteristics art neural models biological plausibility 
appart devised mind 
major arguable point lies gaussian receptive elds nodes 
shown poggio girosi implemented biologically plausible way 
error detection match tracking 
presentation input node active uncommitted node committed 
task detecting input su ciently coded accomplished gain control res committed nodes active 
signal max commit uncommitted node 
er don know answer non adaptive network 
luis mart incorrect predictions detected output gain control go go compares similarity prediction network desired network output relative measure error jo yj jyj output vigilance parameter absolute error approach jo yj choice method largely depends characteristics problem solved results expected 
go remains inactive learning takes place go res match tracking mechanism takes care rising vigilance base value minimum vigilance accepted 
purpose reset currently active categories interfering calculation accurate prediction 
straight approach setting vigilance value minimum activation min deactivating active node 
solution shot match tracking algorithm similar grossberg williamson exp ji ji ecacy approaches compared section 
learning 
art networks appart line learning neural network 
adaptation processes local rules 
updated learning rule gated steepest descent learning rule grossberg 
gated steepest descent dw ji dt ji learning law adaptive weight ji postsynaptic activity modulates rate ji tracks presynaptic signal 
appart hybrid art network function approximation equation discrete time formulation ji ji modifying obtain learning equations similar gaussian art 
constant change rate replaced represent cumulative category activation amount training taken place jth node 
equally weights inputs time intention measure sample statistics 
presynaptic signal substituted respectively learning rst second moments input ji ji ji ji standard deviation ji ji ji calculated williamson 
layer kj adapted represent corresponding cumulative expected output learned node 
di erential equation formulation process kj dt transformed discrete time formulation kj kj small constant 
weights node updated similar way tracking amount learning taken place node 
di erential equation similar dt discrete time version note 
luis mart appart initialized categories uncommitted 
category committed incremented 
new category indexed initialized learning proceed normal constant added ni set ni value direct impact quality learning 
larger slows learning corresponding input feature warranties robust convergence 
input features approximately standard deviation common common 
expected output kn kn learning appart summarized possible scenarios input node active 
activation calculated 
res node active causing commitment new node 
input nodes active 
activation vector propagated generating prediction active nodes learn 
input expected output learning interval node active 
new node committed case 
layer weights newly committed node set correctly predict expected output 
input expected output learning interval activating nodes match tracking needed 
prediction network generated res go allowing learning take place input expected output learning interval activating nodes match tracking needed 
prediction match vigilance criterion go vigilance risen deactivating nodes 
process prediction accuracy testing vigilance raising repeated prediction su ciently matches expected output network behaves case nodes active network acts case 
art models appart prediction accuracy depends training patterns presentation order 
voting strategy carpenter similar supervised mapping art models useful 
appart performs functional approximation prior existing voting strategy directly transferred voting committee kept way nal output chosen changes 
selecting popular output value calculated average outputs appart hybrid art network function approximation number committee members output committee member nal prediction 
standard deviation measure quality prediction 
symbolic knowledge representation appart decades neural networks evolved standard tool solving numerous complex real life problems 
facts neural networks priori domain knowledge give justi cation response stored knowledge easily translated human readable format limited application technologies core problems 
limitations led creation knowledge systems attempt combine neural networks symbolic arti cial intelligence systems 
hybrid neural systems attractive alternative traditional neural symbolic systems mentioned 
approaches subject 
approaches classi ed broad groups uni ed architectures ii transformational architectures iii modular architectures 
rst group consists systems implements processing dynamics connectionist way 
second group includes systems perform transformations symbolic neural representations neural symbolic representations 
group comprises systems neural symbolic modules interact certain degree coupling 
neural models hybrid approaches mlp rbf kohonen kohonen networks browne sun 
art systems started applied subject carpenter grossberg carpenter tan tan 
cascade artmap tan neural network probably relevant result eld 
neural network capable representing chained symbolic rules form capable performing multi level inference 
includes method inserting extracting symbolic rules 
appart addresses hybrid neural system issue represent fuzzy rules similar way fuzzy logic zadeh standard additive model sam kosko 
luis mart sam fuzzy function approximator stores fuzzy rules form xe ye assuming rules weighted formulated fuzzy membership corresponding fuzzy set membership set computed centroids volumes reformulating introducing convex weights convex sum part centroids 
appart extends sam model 
equations formulate appart fuzzy function approximator takes form hj node de nes fuzzy set activation interpreted membership function nodes activations plain membership functions 
composite measure membership degree volume corresponding rule weight relevance 
membership degree computed gaussian match function 
volume computed dividing multiplication deviations see appendix details 
doing appart primes active nodes represent smaller fuzzy sets working assumption event input belonging smaller particular class carries larger amount information input belongs general broader class 
cumulative category activation measure rule importance 
nodes jointly empirically approximate centroid volume membership function consequent rule 
rule insertion inserting fuzzy rule network restriction rule antecedent membership function approximated set gaussian functions 
equivalent say rule xe ye decomposable disjunctive form appart hybrid art network function approximation xe xe jl ye fuzzy set jl corresponding membership function jl gaussian formulation mean jl deviation jl disjunctive form insertion rule network straight process performed moment network training 
jl node index committed way explained see section mean deviation equal jl ones jl jl connections node layer nodes set assigning value centroid kn speci ed sam de nition range de ned rules weights general method translation rule node solution normalize allowing wh set maximum initial cumulative node activation 
rule inserted subjected adaptation process takes place rest network training process 
adaptation inhibited rule weight represented cumulative node activation set relatively large value 
action induce network pay attention rule 
sense desired rule changed adaptation process probably substantial evidence correctness 
rule extraction results interpretation showed section explicit way relationship appart architecture nodes corresponding nodes weights rules encoded 
straight method converting appart set fuzzy rules consists node create fuzzy rule 
antecedent rule de ned fuzzy set associated node match function 
luis mart consequent centroid volume constructed values weights connections node nodes 
particular kj kj weight rule calculated node cumulative activation volume antecedent fuzzy set 
total nodes weights jl converted normalized form order simplify calculations 
fuzzy rules generated appart network understand justify network response 
appart stores information localized way looking parts network active determine rules production response 
appart hybrid neural systems art learning state appart autonomous learning sam 
appart dynamics performs sam self adapts weights topology complexity problem solved 
shown self modi cation process directly interpreted incorporation rules knowledge system 
appart allows addition priori knowledge form fuzzy rules extraction rules system 
characteristics appart stand uni ed transformational classes hybrid systems 
compare appart art hybrid systems particular cascade artmap interesting points noted 
notorious fact encode discrete associations 
derived winner take activation cascade artmap 
cascade artmap inherits inecient coding fuzzy categories williamson related fuzzy artmap networks 
hand appart supports encoding chained rules 
inconvenience overcame introducing feed back connections output input nodes 
solution investigated depth 
art neural models hybrid neural systems 
discussed section related function approximation 
appart hybrid art network function approximation appart function approximation method appart hybrid design provides ground multiple interpretations 
section deal appart function approximation method comment theoretical capabilities relations methods 
problem function approximation formulated inverse problem de nition 
inverse problem set pairs nd function satis es conditions problem function approximation reformulated regression problem approximated conditional mean jx 
conditional mean formulated jx dy dy conditional probabilistic density function 
unknown guessed observations 
nadaraya watson nadaraya watson regression assumes form induced observations assuming continuous smooth derivative exp kx exp kx neural implementation bishop 
state appart extends stores standard deviation input feature generates needed amount inputs prototypes training set build network 
appart generates just needed amount gaussian basis nodes meet statistics training set 
appart generate compact code similar adapt statistics generated changing environment 
appart interpreted generalization 
appart certain parameters values behaves 
luis mart theorem 
appart common behaves 
proof 
currently stored network new category committed match tracking algorithm nd acceptable activation 
network built node non duplicate training samples adaptation take place 
set restriction activation nodes 
learning take place values standard deviation remain constant 
changes dynamics mimic network constructed training set category nodes bias output network categories standard deviation 
theorem allows express appart stable learning higher order neural implementation nadaraya watson regression 
viewed normalized radial basis function expansion bishop 
allows transitively apply appart important properties rbf networks universal approximation kowalski hartman keeler park sandberg best approximation girosi poggio properties 
rst ensures appart capability approximating function desired degree accuracy 
second guarantees existence con guration appart parameters approximates function desired degree accuracy 
regarding property noted shown mlps share girosi poggio knowledge shown property occurs art network 
fact appart possesses properties provides theoretical safety net solving practical problems 
connections art models compare appart art function approximation network particular interesting di erences spotted 
discussion details model scope concentrate commenting di erences 
marriott harrison network fuzzy artmap 
fuzzy artmap fuzzy art modules generates classes network inputs creates classes network outputs 
map eld binds classes inputs outputs discrete associations 
match tracking process correct wrong predictions generating re ned classes 
modi es fuzzy artmap changes formulation map eld collecting probabilistic information regarding association class inputs class outputs omitting appart hybrid art network function approximation match tracking process 
original formulation winner take activation fuzzy art modules 
modi cation allowed distributed activations 
cano fuzzy artmap 
changes way fuzzy art computes category choice introducing triangular shape fuzzy membership function weight centroid calculation operation 
cano introduced backpropagation algorithm aimed nding right values network parameters minimizing network complexity 
distributed activations ers solution similar appart giving match tracking mechanism looses main features artmap networks capacity create smaller particular classes inputs encode speci associations 
addresses appart formulation art fuzzy logic system 
keeps artmap winner take inter art map eld formulation 
inhibits exploiting generalization power lies distributed activations scheme 
backpropagation algorithm determine network parameters allows network trained line learning fashion introduces error learning de ciencies catastrophic forgetting convergence problems 
benchmarks focus solving benchmarking problems fth order chirp function approximation mackey glass equation approximation dna promoter recognition 
rst problems function approximation problems 
meant studying appart performance function approximating algorithm comparing neural models mlp rbf fuzzy artmap fam gaussian artmap gam 
cases appart fam gam applied voting strategy 
number voting runs set cent size training set 
tests appart output gain control uses absolute error measure 
mean squared error mse ml expected output network prediction respectively associated pattern size test set comparing results 
luis mart original fth order chirp function 
common mean squared error propagating test set nodes committed common mean squared error propagating test set nodes committed common mean squared error propagating test set nodes committed 
common mean squared error propagating test set nodes committed 
common mean squared error propagating test set 
nodes committed 
fig 

appart approximation fth order chirp function di erent values initial standard deviation common notable di erence obtained minimum activation shot match tracking mechanisms ered best results case shown 
third problem meant testing appart hybrid neural system 
results compared ones obtained cascade artmap kbann towell shavlik rule extraction algorithm towell shavlik fuzzy artmap mlp machine learning algorithms id quinlan nearest neighbor knn duda hart consensus pattern analysis neill 
fifth order chirp function approximation problem consists estimation fth order chirp function see fig 
formulated sin 
appart hybrid art network function approximation table 
mean squared errors propagating test set fth order chirp function approximation problem 
model mse obtained training epochs mlp rbf fam gam appart set samples generated having 
samples randomly extracted training set rest test set 
shows result propagating test set training appart di erent sets training parameters 
clearly noticeable common reduces quality prediction increases amount nodes created layer 
substantial di erence accuracy prediction noticed match tracking mechanisms described 
shot match tracking mechanism speed training process reduced fourth 
results obtained models summarized table 
appart performs best 
obtained results large amount training epochs 
mackey glass equation mackey glass equation see fig 
dx dt ax bx time delay di erential equation proposed model white blood cell production mackey glass 
constant values commonly set 
delay parameter determines behavior system 
system produces chaotic attractor 
simulations chosen input window time steps elements lawrence tsoi black 
generated luis mart fig 

mackey glass equation 
patterns set sets extracted pattern network training patterns test set 
input features equally distributed equally spaced time steps elements initial standard deviations input feature set common value common fig 
shows appart prediction errors 
clear ect initial standard deviation accuracy appart evidence match tracking algorithms exhibits better performance 
di erence convergence speed match tracking methods obvious fig 

shot match tracking performs generally twice fast minimum activation 
appart test set results types match tracking algorithms di erent values common appart speed training di erent training parameters match tracking algorithms 
fig 

appart test set results approximating mackey glass equation shot os minimum activation ma match tracking algorithms 
numbers right values initial standard deviation common appart hybrid art network function approximation applying neural models see table best con guration appart outperforms 
appart generates compact coding respect rest art networks appart created input prototypes minimum activation shot match tracking respectively fam generated gam 
dna promoter recognition promoter short dna sequences precede genes 
method recognition promoter dna sequences allows identi cation location genes large sequences dna 
promoter sequence experimentally detected protein called rna polymerase binds dna sequence 
area problems addressed di erent approaches diverse test inductive theory re nement inductive logic programming dynamic programming neural networks repeated turned problem standard problem model performance comparison 
particular case address promoter recognition data set blake merz consists patterns 
dna pattern consists position window position takes nucleotide values adenine 
imperfect domain theory neill comes companion data set 
domain theory applied directly classi es correctly half data set 
table 
test set mean square errors approximating mackey glass equation di erent neural models 
neural model mse obtained mlp rbf fam gam appart os appart ma luis mart table 
results di erent knowledge systems solving dna promoter recognition problem priori knowledge 
knowledge system rules nodes generated error cent id knn consensus sequences mlp fuzzy artmap kbann cascade artmap rules cascade artmap rules appart priori knowledge appart priori knowledge appart priori knowledge rules appart priori knowledge rules bigger data sets exist chosen applied related works towell shavlik tan allows direct comparison results 
table summarizes results obtained applying di erent knowledge systems training appart methodology tan order render results comparable 
di erent values appart training parameters tested 
best results shown 
comments emerge results 
notable improvement predictions rst inserting priori domain theory network appart priori knowledge outperforms rest models tested keeping compact set rules 
probably consequence appart distributed activation nodes enable appart greater generalization power 
noted sam built rules extracted appart network ers similar equal results ones obtained original network 
problem chosen mainly advantages test problem addressed diverse approaches importance practical point view 
eld bioinformatics eld biology related computational problems current core problems science 
application appart results deal appart taken tan 
appart hybrid art network function approximation fig 

evolution asymmetric widths gaussian receptive eld appart network solving approximation mackey glass equation 
notable training proceeds left side gaussian grows steadily right side shrinks 
studies related subject reveal interesting results 
improvements appart model section propose modi cations original appart model 
changes may improve appart performance obtaining better prediction accuracy compact input coding shorter training times 
modi cations consist asymmetric gaussian receptive elds improved selection initial widths gaussian receptive elds 
asymmetric gaussian receptive elds asymmetric gaussian receptive elds input classi cation layer di erent widths sides gaussian bells lead faster ecient description input space enrichment semantic capacity fuzzy rules encoded network 
luis mart modi cation implies equations rewritten 
particular equation reformulated exp ji ji ji ji ji ji ji order di erent values widths ji ji sides exponential 
learning equations need modi ed 
able cope tting ji ji mind introduce asymmetric learning rate modulate amount change second moment input side gaussian bell contains input time 
resulting equations follows ji ji ji ji ji case ji ji ji ji ji ji represent second moment inputs sides gaussian curve 
widths updated ji ji ji ji ji ji committing new node ni ni set initialization value training proceeds adapt separately 
choice value asymmetric learning rate direct implications learning process 
approaches learning widths get asymmetric modi cation reduces symmetric 
hand moves inputs lie side curve lesser impact width side 
appart hybrid art network function approximation lead potentially dangerous situations 
example training samples lie side curve side remain relatively untrained represent correctly nature input class modeled 
situations clearly avoided 
fig 
observed receptive eld modi es widths training proceeds 
section show choice di erent values uences prediction accuracy 
optimal initialization widths gaussian receptive elds obscure matters solving particular problem appart lies initialization widths radial basis gaussian activated nodes 
caused necessity understand problem statistics hand degree 
appart adapts values widths large small initialization value lead excessive overlapping input classes creation holes input domain remain uncovered classes generated 
problem closely related optimization widths hidden layer units rbf network addressed number times 
fundamental streams ghosh nag 
rst ts widths supervised training method backpropagation algorithm poggio girosi generalized cross validation orr 
provide method local adaptation widths 
fritzke moody darken :10.1.1.43.6647
second class methods suits best appart conception approach adheres 
kind methods layed moody darken 
node applied nearest neighbor algorithm discover nearest nodes 
setting widths node root mean square distances 
approach modi ed fritzke substituting nearest neighbor algorithm topological order supervised growing neural gas networks fritzke 
order apply methods appart adapted suit needs art related training characteristics 
approach nearest neighbor algorithm introduces unnatural computational overload builds set neighboring nodes match function measure distance order create means nodes active time ones take part estimation widths node created 
active nodes relatively near center new node forming sort neighborhood 
node uences computation width value match function 
luis mart table 
test set mean square errors mse nodes created training epochs needed appart asymmetric gaussian receptive elds di erent values asymmetric learning rate 
asymmetric learning rate mse nodes training epochs original appart formulation approach new node set ni min max ji ni max maximum width max prevents nodes committed earlier training get large widths initial values 
excessively wide node may able store details input space source noise computation 
analysis practical consequences described modi cations return problem mackey glass equation approximation 
aim compare results obtained original appart formulation modi cations introduced 
tests data sets training methodology section results obtained reproduced comparison reasons 
rst experiment asymmetric gaussian receptive elds described section 
table shown results obtained training appart di erent values asymmetric learning rate value searched optimal set training parameters 
results displayed best ones obtained value analyzing results comments emerge 
noticeable value approaches value prediction accuracy di ers signi best obtained 
number nodes needed archive accuracy larger ones created smaller appart hybrid art network function approximation values true amount iterations required network meet minimum error 
hand approaches resulting networks perform poorly worst original symmetric appart formulation 
results yielded introducing estimation initial widths shown table 
networks trained initial value widths optimized smaller test set mean square error compact coding input space 
notable combined modi cations ered accurate predictions compact codings 
concluding remarks appart incrementally approximates function degree accuracy noisy training samples 
incorporates features art models match line learning deep self organization order build autonomous learning neuro fuzzy system dynamically ts topology parameters weights complexity problem solved 
shown appart extends fuzzy logic standard additive model performing sort fuzzy inferencing 
straight methods allow line insertion extraction fuzzy rules generation explanation network response 
part shown appart universal best approximation features discussed appart relations similar neural models 
table 
test set mean square errors mse approximating mackey glass equation di erent neural models appart modi cations suggested optimization widths initialization asymmetric gaussian receptive elds 
neural model mse obtained nodes rbf original appart appart appart appart luis mart described modi cations asymmetric gaussian receptive elds estimation optimal initialization value widths gaussian receptive elds 
modi cations shown improve appart prediction accuracy input space coding compactness 
benchmark tests carried appart outperformed neural models tested generating compact knowledge representations 
representing fuzzy set volume nodes activation volume area fuzzy set represented dx xn dx dxn membership function substituting membership function nodes obtain exp ji ji dx dxn transforming get exp ji ji dx exp ji ji dx ji reformulate obtain ji assert multiplication standard deviations ji measure volume fuzzy sets de ned nodes 
carpenter grossberg lesher 

fusion artmap neural network architecture multi channel data fusion classi cation 
proceedings world congress neural networks vol 
pp 

hillsdale nj usa lawrence erlbaum associates 
appart hybrid art network function approximation bellman 

adaptive control processes guided tour 
princeton princeton university press 
gori 

learning local minima radial basis networks 
ieee transactions neural networks 
bishop 

neural networks pattern recognition 
oxford clarendon press 
blake merz 

uci repository machine learning databases 
giordana 

growing radial basis function networks 
proceedings fourth workshop learning robots 
karlsruhe germany 
broomhead lowe 

multivariable functional interpolation adaptive networks 
complex systems 
browne sun 

connectionist variable binding 
expert systems international journal knowledge engineering neural networks 
browne sun 

connectionist inference models 
neural networks 
cano ara opez 

new neuro fuzzy architecture incremental learning systems identi cation 
proceedings th world congress ifac vol 
pp 

san francisco 
cano ara opez 

matching error learning automatic generation fuzzy logic systems 
proceedings sixth ieee international conference fuzzy systems vol 
pp 

barcelona 
cano omez opez 

learning noisy information neuro fuzzy systems 
neural networks 
carpenter grossberg 

art stable self organization pattern recognition codes analog inputs patterns 
applied optics 
carpenter grossberg 

massively parallel architecture self organizing neural pattern recognition machine 
computer vision graphics image processing 
carpenter grossberg 

art hierarchical search chemical transmitters self organizing pattern recognition architectures 
neural networks 
carpenter grossberg 

self organizing neural network supervised learning recognition prediction 
ieee magazine 
carpenter grossberg 

arti cial intelligence neural networks steps principled 
honavar uhr eds arti cial intelligence neural networks steps principled integration 
san diego ca academic press 
luis mart carpenter grossberg reynolds rosen 

fuzzy artmap neural network architecture incremental supervised learning analog multidimensional maps 
ieee transactions neural networks 
carpenter grossberg reynolds 

artmap supervised real time learning classi cation non stationary data self neural network 
neural networks 
carpenter grossberg rosen 

fuzzy art fast stable learning categorization analog patterns adaptive resonance system 
neural networks 
carpenter 

artmap ic medical diagnosis instance counting inconsistent cases 
neural networks 
carpenter 

distributed artmap neural network fast distributed supervised learning 
neural networks 
carpenter ross 

art neural network architecture object recognition evidence accumulation 
ieee transactions neural networks 
carpenter tan 

rule extraction neural architecture symbolic representation 
connection science 


bear 
ai expert 
duda hart 

pattern classi cation scene analysis 
new york john wiley 
esp 

approximation continuous discontinuous mappings growing neural rbf algorithm 
neural networks 
feldman bailey narayanan regier stolcke 

lo rst years automated language acquisition project 
ai review 
fodor pylyshyn 

connectionism cognitive architecture critical analysis 
pinker mehler eds connections symbols pp 

cambridge ma mit press 
freeman 

neural networks algorithms applications programming techniques 
reading addison wesley 
fritzke 

fast learning incremental rbf networks 
neural processing letters 
fritzke 

growing cell structures self organizing network unsupervised supervised learning 
neural networks 
fritzke 

growing neural gas network learns topologies 
tesauro touretzky leen eds advances neural information processing systems pp 

cambridge ma mit press 
ge ner 

default reasoning causal conditional theories 
cambridge ma mit press 
appart hybrid art network function approximation ghosh nag 

radial basis function networks 
jain eds radial basis function neural network theory applications 
heidelberg physica verlag 
girosi poggio 

networks best approximation property 
biological cybernetics 
glover laguna 

tabu search 
reeves ed modern techniques combinatorial problems pp 

oxford blackwell 
goldberg 

genetic algorithms search optimization machine learning 
reading addison wesley 
grossberg 

brain build cognitive code 
review 
grossberg 

studies mind brain neural principles learning perception development cognition motor control 
boston reidel 
grossberg williamson 

self organizing system classifying complex images natural texture synthetic aperture radar cas cns tr 
boston ma boston university 
honavar 

symbolic arti cial intelligence numeric arti cial neural networks resolution dichotomy 
sun eds computational architectures integrating symbolic neural processes pp 

new york kluwer 


heuristic pattern correction scheme application speech recognition 
proceedings ieee workshop neural networks signal processing pp 

cambridge hwang bang 

ecient method construct radial basis function neural network classi er 
neural networks 
kakas kowalski toni 

abductive logic programming 
journal logic computation 
kirkpatrick gelatt vecchi 

optimization simulated annealing 
science 
kohonen 

self organized formation topologically correct feature maps 
biological cybernetics 
kosko 

fuzzy engineering 
new york prentice hall 
kowalski hartman keeler 

layered neural networks gaussian hidden units universal approximators 
neural computation 
lawrence tsoi black 

function approximation neural networks local methods bias variance smoothness 
bartlett williamson eds australian conference neural networks pp 

australian national university 
mackey glass 

oscillation chaos physiological control systems 
science 
luis mart manna waldinger 

logical basis computer programming vol 
deductive reasoning 
reading addison wesley 
marriott harrison 

modi ed fuzzy artmap architecture approximation noisy mappings 
neural networks 
mart garc 

appart art hybrid stable learning neural network universal function approximation 
abraham eds hybrid information systems pp 

heidelberg physica verlag 
martinetz 

neural gas network vector quantization application time series prediction 
ieee transactions neural networks 
mcculloch pitts 

logical calculus ideas nervous activity 
bulletin mathematical biophysics 
wermter macintyre 

hybrid neural systems single coupling fully integrated neural networks 
neural computing surveys 
mill 

learning ecient reactive behavioral sequences basic re goal oriented autonomous robot 
animals animats proceedings third international conference simulation adaptive behavior pp 

cambridge ma mit press 
moody darken 

fast learning networks locally tuned processing units 
neural computation 
ahmed chan 

training radial basis function classi ers 
neural networks 
nadaraya 

estimating regression 
theory probability application 
neill 

escherichia coli promoters consensus relates spacing class speci city repeat substructure dimensional organization 
journal chemistry 
orr 

regularization radial basis function centers 
neural computation 
orr 

optimising widths radial basis functions 
fifth brazilian symposium neural networks 
brazil 
park sandberg 

universal approximation radial basis function 
neural computation 
poggio girosi 

networks approximation learning 
proceedings ieee 
quinlan 

induction decision trees 
machine learning 
rumelhart hinton williams 

learning internal representations error propagation 
parallel distributed processing explorations microstructure cognition 
boston mit press 
appart hybrid art network function approximation ler hartman 

mapping neural network derived parzen window estimator 
neural networks 
smolensky 

proper treatment connectionism 
behavioral brain sciences 


general regression neural network 
ieee transactions neural networks 


learning generalization noisy mappings modi ed neural network 
ieee transactions signal processing 
sun 

variable binding connectionist networks 
connection science 
sun 

integrating rules connectionism robust commonsense reasoning 
new york wiley 
tan 

cascade artmap integration neural computation symbolic knowledge processing 
ieee transactions neural networks 
tan 

innovation art neural networks 
jain ed innovation art neural networks 
crc press 


modi ed general regression neural network new ecient training algorithms robust back box tool data analysis 
neural networks 
towell shavlik 

directed propagation training signals knowledge neural networks tech 
rep 
cs tr 
madison wi university wisconsin computer sciences department 
towell shavlik 

extracting rules knowledgebased neural networks 
machine learning 
towell shavlik 

knowledge arti cial neural networks 
arti cial intelligence 
healy 

boosted artmap 
proceedings international joint conference neural networks ijcnn vol 
pp 

alaska 
watson 

smooth regression analysis 
indian journal statistics 
weber 

connectionist unifying prolog 
albrecht reeves steele eds proceedings international conference arti cial neural nets genetic algorithms pp 

heidelberg springer verlag 
weber 

uni cation prolog connectionist models 
leong jabri eds proceedings fourth australian conference neural networks pp 

sydney nsw australia sydney univ electr 
eng 
wermter sun 

hybrid neural systems 
heidelberg springer 
williamson 

gaussian artmap neural network fast incremental learning noisy multidimensional maps 
neural networks 
luis mart williamson 

constructive incremental learning network mixture modeling 
neural computation 
williamson 

neural model self organizing feature detectors classi ers network hierarchy cas cns tr 
boston ma boston university 
wilson martinez 

improved heterogeneous distance functions 
journal arti cial intelligence research 
wilson martinez 

heterogeneous radial basis function networks 
proceedings international conference neural networks icnn vol 
pp 

washington dc 
zadeh 

fuzzy sets 
information control 
