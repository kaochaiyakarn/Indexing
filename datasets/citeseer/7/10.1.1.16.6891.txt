compression cache line compression extend physical memory fred douglis douglis research panasonic com matsushita information technology laboratory research way princeton nj usa january describes method trading computation disk network expensive line compression 
memory store data compressed format may possible fit working set large applications relatively small memory 
working sets large fit memory compressed compression provides benefit reducing bandwidth space requirements 
effectiveness compression cache depends application behavior relative costs compression measurements sprite decstation workstation local disk indicate memory intensive applications running compression cache run times faster unmodified system 
better speedups expected system greater disparity speed processor bandwidth backing store 
past decade processing power physical memory size typical computers increased dramatically 
workstation memory sizes increasing new technology trend pushing small memories mobile computers smaller desk top counterparts typically configured significantly memory 
application designers forced squeeze applications fit available memory may succeed 
general purpose mobile computer computers paging needed enable wider range applications run long performed efficiently 
difficulty paging mobile computers arises technology trend 
workstations normally connected relatively fast local area networks moderately fast disks mobile computers may communicate slower wireless networks run diskless small slower local disks 
time processors mobile computers steadily improving speed disparity processor speed speed appeared winter usenix conference january pp 

copyright retained author 
permission granted noncommercial reproduction complete educational research purposes 
appeared technical report mitl tr revised 
decstation trademark digital equipment 
compression cache great mobile computers workstations 
disparity suggests new technique managing memory exploits compression reduce compression widely reduce demand secondary storage networks 
suggest feasible compression reduce demand memory 
basic idea take memory normally directly application hold larger number pages compressed format 
call area compressed data compression cache 
pages touched process normally fit memory fit memory stored compression cache processor write page backing store local disk network computer 
pages written backing store compressing reduces amount data transferred 
potential benefits compression cache depend relationship speed compression bandwidth system compression ratio barely experiments reported 
cost compressing copying page negligible pages compressed compression cache give computer appearance having additional physical memory 
practice compressing copying costs associated benefit reducing traffic backing store offset overhead compression cache 
overhead comes compression additional page faults application experience memory compressed pages data structures support compressed pages 
furthermore mentioned applications compress poorly suited applications effort compress memory wasted degrade improve performance 
depending application hardware environment benefits reduced may outweigh costs compression additional faults viceversa 
configuring compression cache improve performance case staying way second case interesting difficult problem 
remainder organized follows 
section discusses related involving paging compression 
section elaborates tradeoffs involved compressed paging 
section describes design compression cache tradeoffs 
section evaluates performance compression cache sample applications 
section concludes 
related section discusses projects products goals similar compression cache 
fall general categories file systems virtual memory 
file systems number systems replaced individual users ad hoc techniques manual compression mechanism automatically compressing files 
cate gross compressed files level hierarchy accessed files uncompressed format ones compressed 
frequently files compressed compression performed background impact interactive performance delays due decompression minimal seconds user day 
time disk space requirements roughly halved 
burrows integrated compression sprite lfs primarily reduce disk space requirements :10.1.1.117.5365
argued lfs better vehicle compressing files traditional file systems files overwritten place change block file cause changes compressed data file 
multiple file blocks may winter usenix conference compression cache compressed unit providing better compression block compressed separately dynamic compression algorithm lzrw 
burrows line compression halved disk space requirements cate gross system delays incurred decompressing large file single unit 
system acceptable performance degradation compression performed software suited hardware compression 
addition family products personal computers line line compression purpose reducing disk space usage 
discussion products available omitted 
virtual memory focus systems disk space performance 
projects considered ways reduce disk space demands improve performance particularly area virtual memory 
described mechanism compressing binary executables personal computers reducing disk space requirements improving file system bandwidth 
compression executables performed line especially effective slower compression algorithm available 
bandwidth improved cost decompression offset reduction data transferred disk 
result performance program invocation improved 
atkinson xerox parc considered compression order reduce cost paging wireless links 
paging needed environment mobile computing devices small local disks tabs advocated weiser 
parc researchers concentrated read data executables space time overhead performing line compression 
executables stored transmitted compressed format cached mobile computer compressed format increase number executables cached 
compression performed line asymmetric compression algorithm give compression ratios correspondingly high overhead decompressing quickly 
researchers consider line compression resulting suggestion reported appel li pages compressed retained memory 
idea pursue extensively primary theme 
design considerations intuitively idea trading processing compression appealing large processors improving performance quickly devices especially disks 
compress pages occupy little memory permit process address space reside memory possible avoid backing store completely process execute correspondingly faster 
note technique fundamentally different writing dirty page file system compression disk compression driver level compressed pages go backing store 
compressed pages form intermediate level storage hierarchy uncompressed pages backing store 
paging network local disk issue 
environments efficient page mbps ethernet memory file server page local disk 
local area networks atm networks autonet provide bandwidth order magnitude greater ethernet 
mobile computers wireless networks expect disparity processing remain time 
winter usenix conference compression cache keeping compressed pages memory obviate need backing store 
possible collective address space running processes fit memory compression 
fit desirable move old pages backing store order memory available actively pages 
case pages transferred backing store compressed format reducing demand bandwidth 
technique similar paging file system disk compression 
differences reduced compression cache pages faulted written backing store 
pages longer need written 
variable memory allocation 
making compressed pages explicit part memory hierarchy system dynamically vary amount memory uncompressed pages compressed pages file blocks 
necessary avoid impacting applications need compress pages discussed section 
complexity space overhead 
better compression performed level backing store vm system 
assuming mapping vm pages file blocks transferring pages compressed requires vm system cluster multiple compressed pages smaller number file blocks 
vm system manage free space backing store granularity finer individual file blocks 
considering file system may compression regardless virtual memory extra overhead manage backing store may wasted effort wasted memory 
issue discussed 
regardless compression performed explicitly vm system implicitly pages transferred backing store effectiveness compressing vm pages depends factors compression speed 
compressing page decompressing significantly faster transferring backing store 
traditional paging compression 
compression ratio 
average pages compress significantly original size 
obviously compressing kbyte page bytes far useful compressing bytes 
page access patterns 
pages compressed retained memory fewer pages available uncompressed pages 
application take additional page faults accessing pages resident uncompressed standard system stored compressed format 
effect important compression cache degrade performance collective working set active processes fits physical memory need compress pages compression cache stay way 
implies size vary dynamically time demand memory changes 
memory overhead 
keeping pages uncompressed compressed format memory overhead associated keeping track state page pages stored disk 
away memory applications results additional page faults 
winter usenix conference compression cache compression ratio compression speed versus better compression faster bandwidth speedup transferring compressed pages backing store 
mean memory time speedup compression ratio compression speed versus better compression faster keeping compressed pages memory 
performance compressing pages modeled analytically 
speedups shown function compression ratio fraction bytes left compression speed compression relative decompression assumed twice fast compression roughly case algorithms lzrw 
regions speedup dark black areas top left show speedups go top scale fold improvement light areas show speedups relative compression darker areas right show data points slowdown result 
compression implementations 
compression cache allow software hardware compression 
ideally allow different compression algorithms different types data order get best compression rates throughput 
expect inverse relationship compression speed compression ratio faster page compressed compression required compression improve performance 
graphs speed paging backing store compressed format function compression bandwidth relative bandwidth backing store compression ratio 
shows speedup mean memory time function variables pages retained memory application sequentially accesses twice pages fit memory reading writing word page 
case pages compressed larger half original size average speedup due compression linear speed compression 
course pages compress compression faster performance worse compression 
systems possible application issue advisory operating system indicate lru page replacement behave poorly example half pages effectively pinned memory faults occurring half 
fast compression reducing factor inferior keeping pages compressed memory 
sharp leap speedup pages fit memory demonstrates potential difference compression cache system compresses pages en route backing store 
practice improvement fully realized access patterns winter usenix conference compression cache pathological 
performance sample applications description specific implementation compression cache sprite operating system 
design section describes design implementation compression cache sprite 
sprite largely compatible bsd unix virtual memory system interesting difference versions unix physical memory traded dynamically vm application processes file system buffer cache 
compression cache vary size dynamically sprite provides framework prototyping compression cache 
idea compression cache extend naturally unix mach systems fact mach external pager interface excellent foundation area 
target environment research consists mobile computers limited memory network bandwidth small local disks disks 
limited availability sprite compression cache prototyped workstation environment running decstation workstations paging local rz disk 
sprite kernel configurable boot time allow system variable amount physical memory mbyte machine behave little mbytes 
mbytes kernel code page tables forms tracing currently disabled 
overview compression cache forms new level memory management hierarchy 
general description technique follows 
ffl lru pages compressed room new pages 
compressed pages retained memory period time expectation accessed soon 
ffl pages fit memory compressed lru compressed pages written backing store 
ffl service page fault page uncompressed resident memory vm system checks see page compressed memory backing store 
backing store brought memory stored compression cache decompressed accessible faulting process 
compressed copy memory freed time copy backing store 
specific issues arise number areas 
vm system able vary amount physical memory allocated compression cache account demand uncompressed pages file system buffer cache 
second interface compression cache backing store complicated notion variable sized pages 
overhead managing compression cache adversely affect performance 
subsections discuss issues 
variable memory allocation initially compression cache implemented fixed size region physical memory 
done partly simplicity partly need vary size apparent 
version compression cache consisted number pages divided fragments experiments defined meaning blocks bytes pagesize unix registered trademark unix system laboratories winter usenix conference compression cache kbytes 
page compressed system allocated fragments hold compressed data 
fragments need allocated contiguously compression performed contiguous buffer compressed data scattered multiple fragments 
satisfy page fault fragments page copied contiguous buffer decompressed 
fixed size implementation simple unused fragments linked list fragmentation kept minimum 
implementation suitable applications paged heavily compression cache fit compression cache excessive traffic backing store 
example machine mbytes memory available user processes setting aside mbytes compressed pages cause mbyte process page performance 
hand compression mbyte process probably fit mbytes available 
case compression cache cache mbytes better second case mbytes compressed pages eliminate traffic backing store 
compression cache redesigned vary memory usage time 
considered extension previous design fixed size page fragments problem approach 
reclaim physical page compression cache uncompressed vm page file block fragment page copied written backing store 
page fragments contain small piece different pages physical page transferred directly backing store resulting multiple os read fragments particular page page fault different pages transferred backing store order free physical page 
addition overhead doing scatter gather contiguous compression buffer page fragments unnecessary 
memory compression cache treated variable sized circular buffer 
physical pages mapped kernel virtual address space eventually wrapping start range addresses compression cache 
notion oldest physical page added cache longest time ago new pages added may contain data 
physical pages added queue normally removed 
may removed middle clean pages oldest 
vm pages compressed compressed directly unused region compression cache page added cache 
page small header describes page size compressed contains dirty data link page cache information 
shows simplified view compression cache 
pages states clean page modified compressed pages written backing store 
page clean contains compressed pages brought backing store satisfy page faults 
dirty page modified data backing store 
free slot compression cache physical page associated 
new page containing data 
new pages exist tail queue 
pages may reclaimed dynamically compression cache 
oldest page cache unmodified data unmapped returned kernel pool free physical pages 
winter usenix conference compression cache clean clean dirty dirty dirty dirty dirty free clean new older state compression cache 
physical pages may states 
separate array page descriptors stores mapping slots compression cache physical pages keeps track state page 
patterns represent distinct vm pages compression cache 
user page small descriptor just indicating state 
lighter pages clean darker ones contain modified data 
white areas contain current data 
kernel thread writes oldest dirty data compression cache attempt keep pool physical pages clean ready reclamation 
rate pages cleaned function number completely free pages system number clean pages size compression cache 
method choosing grow shrink compression cache similar algorithm sprite trading memory file system vm system 
sprite compares age file block age lru vm page reclaims older modulo adjustment favor retaining vm pages longer 
penalty file system helps improve interactive performance preventing large file flushing process address space completely memory 
compression cache adding third collection pages third consumer memory tradeoffs complicated 
current implementation allocation types memory file system cache blocks uncompressed vm pages compressed pages requires comparison ages oldest pages types 
system biases ages favor compressed pages uncompressed pages file cache blocks 
winter usenix conference compression cache system favors compressed pages larger compression cache tend grow periods heavy paging low bias bias favor uncompressed pages compression cache degenerates buffer compressing decompressing pages memory backing store 
interestingly single penalty vm file system works wide range applications optimal penalty compression cache application dependent 
application exhibits great deal locality pages uncompressed possible compression cache serve just buffer backing store expected eliminate completely 
large application exhibits little locality faults rarely satisfied compression cache benefit large cache 
application characteristics cause hit compression cache benefit large cache 
examples application appear section 
interface backing store unmodified sprite system size vm page integral multiple kbyte file system block 
compression cache prototyped vm page kbytes discussion assumes mapping file blocks vm pages 
page written backing store written swap file corresponding segment containing page offset corresponding location page segment 
fixed mapping pages file blocks trivial locate page backing store 
number ways transfer variable sized compressed pages backing store especially appealing 
ideally system keep compressed page location swap file compression cache transfer just amount data occupied compressed page 
unfortunately exception block file file system enforces transfers multiples file system block 
part block written file system reads old contents overwrites part just written writing block back disk 
words page compressed kbytes kbytes kbyte write result kbyte read kbyte write expected kbyte write 
furthermore request read kbytes kbyte block result file system reading kbytes copying just part requested requesting process buffer 
changing internal structure sprite file system writing page file significant overhead way avoid reading minimum kbytes satisfy page fault 
unfortunate effect reducing usefulness compression cache applications read large number pages unpredictable order page fault require full kbyte read decompression 
possible solutions extra overhead partial writes described ffl partial solution issue operation write entire block writing kbytes issuing disk read 
benefit having performed compression 
environment pages written backing store unimportant applications fit memory compressed 
ffl possibility modify file system overwrite part file system block disk reading remainder block 
case disk bandwidth improve suffer having independent small os small number large os 
note possible page sprite lfs winter usenix conference compression cache provides higher bandwidth coalescing small writes single larger transfer lfs suffers restriction kbyte transfers :10.1.1.117.5365
ffl solution implemented attempts transfer exactly amount data page occupies compressed merging compressed pages smaller number file blocks 
reduces fragmentation corresponding reduction bandwidth needs disk space usage 
merging compressed pages problems 
scheme loses mapping offsets swap file pages segment 
necessary store location page explicitly 
second page written backing store faulted back memory modified written may written location 
problem writing partial file blocks occur 
necessary perform garbage collection backing store keep track blocks contain copy page blocks contain obsolete data 
compressed pages written arbitrary locations block keeping track location size page bookkeeping nightmare 
third pages allowed span file blocks necessary read blocks satisfy page fault 
kbyte read kbyte 
page accesses exhibit sufficient locality retrieving kbytes compressed pages satisfies additional page faults spanning pages disadvantageous long run locality system pay performance penalty 
version compression cache implemented sprite pads compressed page uniform fragment size currently kbyte writes set fragments spanning file blocks single operation 
currently kbytes compressed pages written 
system parameterized determine pages allowed span file block boundaries fragmentation increases effective bandwidth writes backing store correspondingly decreases 
overhead compression cache adds overhead terms memory usage 
kernel sets aside static buffer lzrw algorithm hash table 
hash table relatively large order mbyte improves compression cost memory relatively small 
system measured hash table kbytes 
addition difference code sizes unmodified system system compression cache additional kbytes 
segment created enlarged page table essentially extended bytes kbyte page compression cache 
overhead pages resident memory information resident pages non resident pages unmodified system stores just bytes page compression cache 
example collective virtual memory running processes mbytes kbyte pages page overhead compression cache total kbytes 
overhead space managed compression cache 
kernel uses bytes page range addresses compression cache occupy shown 
overhead determined boot time maximum possible size cache 
kernel allocates byte header physical page frame mapped cache overhead byte header virtual page compressed placed cache 
overheads occur compression cache data offset savings memory usage due compression 
winter usenix conference compression cache performance showed improvement due compression depends speed compression amount compression obtained number backing store completely eliminated 
consider maximum possible performance improvement performance applications 
maximum possible improvement possible estimate maximum possible improvement particular configuration compression algorithm running program contrived thrash vm system 
cycles linearly working set reading optionally writing word memory page time working set 
system uses lru algorithm page replacement working set fit memory takes page fault page access 
modifying pages accesses system write page time room page faulted 
unmodified sprite system uses regular files backing store perform disk seeks fault write page retrieve page faulted 
reading page seek necessary pages close swap file equivalently close address space 
compression cache fault page fault satisfied decompression compression pages modified disk os 
memory compressed pages increase fault rate faulting new page anyway ratio compression speed speed determines speedup 
working set fit memory compressed fault may require read backing store possibly write room 
clustering compressed pages transfers effectively smaller multiple pages obtained single read backing store 
reduce average number seeks page fault considerably 
note sprite lfs alleviate problem seeks grouping multiple pages single segment 
clear paging lfs desirable heavy paging load 
lfs requires significant memory buffers lfs clean segments containing swap files copy live blocks types data :10.1.1.117.5365
shows access time function working set size machine configured mbytes mbytes available user processes 
lines std rw unmodified sprite system sequentially reading writing page 
cc rw sprite system compression cache sequentially reading writing page 
std ro unmodified sprite system sequentially accessing page modification cc ro sprite system compression cache sequentially accessing page modification gives average time access page gives speedup compression cache relative original system 
winter usenix conference compression cache size address space mbytes average page access time ms std rw cc rw std ro cc ro average page access cost 
size address space mbytes speedup cc ro cc rw speedup relative original system 
compression cache performance thrashing 
unmodified system large number pages fit memory measurable page fault overhead system starts thrashing pays disk operations page access 
compression cache pages compress roughly 
compression reduces average access time considerably especially compressed pages fit memory need disk total address space mb 
larger address spaces mbytes upward resulted disk fewer transfers fewer seeks unmodified system 
measurements taken decstation approximately mbytes available user processes paging local rz disk page size kbytes 
compression performed williams lzrw algorithm 
labels explained text 
application speedup speedup gives upper bound performance improvement real applications experience compression cache 
takes number page faults memory set aside compressed pages memory accesses compresses extremely 
real applications may compress exhibit degree locality significantly increases page fault rate allocated memory 
section reports performance sample applications 
summary results table 
example application exhibits substantial improvement compression cache program computes sequence modifications change file 
compare useful transferring diffs entire files changes versions file minimal 
lopresti implemented file differencing dynamic programming algorithm refer lipton lopresti description 
application uses dimensional array wide stripe diagonal accessed 
works way array direction reverses direction goes linearly back 
elements diagonal recurrence relation causes frequent repetitions values turn suggests data array extremely compressible 
lzrw pages compress compression algorithms pages winter usenix conference compression cache application time std time cc speedup compression ratio pages compare isca sort partial gold create gold cold sort random gold warm table application speedups 
measurements taken decstation paging local rz disk page size kbytes 
benchmarks run approximately mbytes available user processes 
compression performed williams lzrw algorithm 
applications high fraction pages compress threshold applications greatest speedup compressed pages 
times minutes seconds 
compress better 
example application benefits compression cache cache simulator cpu intensive memory intensive 
sample run isca experienced improvement execution time pages compressed execution averaged compression ratio 
despite examples applications compression ratios consequently performance compression cache applications necessarily compress especially performance suffers accordingly 
considered application performs quicksort file containing approximately mbytes text numerous copies word usr dict words 
text completely unsorted sort random minimal repetition strings individual kbyte page sort program ran significantly slowly compression cache unmodified system primarily pages compressed threshold keeping compressed format 
time compress pages wasted effort 
sake comparison sort heap compressed better input file contained frequent repetitions words example input file minor permutation sorted copy file substrings complete words repeated page memory sort partial 
case compression ratio application ran faster unmodified system slower 
expect main memory database benefit compression cache fits memory compressed 
accesses data tends remain uncompressed warm data frequently cold data stay compressed 
access compressed data incur overhead decompression subsequent compression page modified disk hit rate uncompressed data lower hit rate system compression cache memory compressed pages regular virtual memory 
poorer compression ratio greater penalty 
database index engine gold compresses slightly worse runs slowly compression cache unmodified system 
partly due poor compression partly due high fraction nonsequential page accesses encounters requires full kbyte read backing store 
ideally winter usenix conference compression cache compression cache system permitted kbyte read satisfy page fault case gold applications benefit generally compression 
table lists runs gold gold create benchmark creates new index scratch 
high degree write accesses degradation suffers reading kbyte blocks partly offset writing compressed pages backing store 
pages compress average rest 
program runs slowly compression cache 
gold cold benchmark performs sequence queries existing gold index engine index engine having just started 
index engine writes pages reading 
runs slowly 
gold warm lastly benchmark performs set queries gold cold executed 
index data established address space index engine faulted read fashion 
small number pages modified program operates 
benchmark runs slowly 
obviously applications run slowly compression cache varying amount memory insufficient prevent degradation 
possible disable compression completely poor compression obtained 
compression reduce amount backing store possibly eliminating completely 
operations necessary compressed pages require bandwidth 
depending cost compression cost compressibility memory pages technique improve performance factors best case 
real applications generally obtain degree improvement number reasons ffl locality causes application take faults compressed pages accessible unmodified system ffl poor compressibility results reduction amount effort ffl restricted causes larger transfers performed necessary 
example application obtain significant speedup file comparison program compresses sequential passes large dimensional array susceptible increase fault rate 
applications vary moderate improvements performance slight substantial degradations 
compression gets faster relative range applications benefit compressed paging improve 
happen ways hardware compression improve disparity compression speeds rates faster processors thing software compression slower backing stores wireless networks 
better interface backing store help 
winter usenix conference compression cache note techniques virtual memory potentially applied areas 
instance systems physical memory sprite lfs practical consider combining compressed sprite lfs compression cache techniques system keep part file buffer cache compressed format order improve cache hit rate 
redesign specific applications databases keep data structures compressed format application specific techniques compressing data managing choice data compress 
experiences compression cache clear success scheme uses compression improve performance depend great deal relative speeds compression compressibility data data access patterns 
brian marsh rafael alonso contributed design initial implementation compression cache helped formalize expected performance 
ponamgi helped implementation backing store 
steven johnson dan lopresti karin petersen provided programs help evaluate compression cache 
rafael alonso lisa daniel barbar brian bershad haber hank korth kai li dick lipton dan lopresti brian marsh ponamgi sreedhar marvin theimer provided helpful feedback earlier drafts helped improve content presentation 
lastly andrew appel mike burrows ram mike jones karin petersen jonathan sandberg comments suggestions 
andrew appel kai li 
virtual memory primitives user programs 
proceedings fourth international conference architectural support programming languages operating systems pages santa clara ca april 
russ atkinson dan greene bryan lyles marvin theimer 
applying compression techniques virtual memory paging 
xerox parc internal memorandum 
daniel barbar chris clifton fred douglis hector garcia molina stephen johnson ben kao sharad mehrotra jens walsh 
gold 
th international conference data engineering vienna april 
appear 
burrows lampson mann 
line data compression logstructured file system 
fifth international conference architectural support programming languages operating systems pages 
acm october 
vincent cate thomas gross 
combining concepts compression caching level filesystem 
fourth international conference architectural support programming languages operating systems pages 
acm april 
leblanc 
adjustable block size coherent caches 
proceedings th annual international symposium computer architecture pages gold coast australia may 
acm 
david golub richard draves 
moving default memory manager mach kernel 
proceedings nd mach symposium pages november 
winter usenix conference compression cache lipton lopresti 
comparing long strings short systolic array 
moore mccabe urquhart editors systolic arrays pages 
adam boston 
nelson welch ousterhout 
caching sprite network file system 
acm transactions computer systems february 
nelson 
physical memory management network operating system 
phd thesis university california berkeley ca november 
available technical report ucb csd 
ousterhout douglis nelson welch 
sprite network operating system 
ieee computer february 
mendel rosenblum john ousterhout :10.1.1.117.5365
design implementation log structured file system 
acm transactions computer systems february 
appears proceedings th symposium operating systems principles october 
schroeder birrell burrows murray needham satterthwaite thacker 
autonet high speed self configuring local area network point point links 
ieee journal selected areas communications october 
mark 
compressed executables exercise thinking small 
proceedings usenix summer conference 
mark weiser 
computer st century 
scientific american pages september 
ross williams 
extremely fast ziv lempel data compression algorithm 
data compression conference pages april 
winter usenix conference 
