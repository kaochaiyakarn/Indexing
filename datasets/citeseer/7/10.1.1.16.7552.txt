overcomplete bss geometric algorithm theis lang institute biophysics university germany fabian theis mathematik uni de dept architecture computer technology university granada spain 
independent component analysis ica algorithm geometric considerations decompose linear mixture sources sensor signals 
proposed step approach separation learn mixing matrix recover sources maximum likelihood approach 
efficient method matrix recovery step mimicking standard geometric algorithm generalizing method 
independent component analysis ica random vector goal find statistically independent components 
solve blind source separation bss problem mixtures underlying independent sources separate mixed signals recovering original sources 
geometric ica algorithms received attention due relative ease implementation 
standard ica algorithms generally assume sensor signals underlying source signals provided 
overcomplete ica sources mixed signals 
olshausen fields put idea overcomplete representations coding theory information theoretic context decomposing natural images overcomplete basis 
prager independently olshausen connection sparse coding ica quadratic case 
basics mat vectorspace real matrices gl mat det general linear group general case linear blind source separation bss random vector composed sensor signals originates independent random vector composed source signals mixing mixing matrix mat 
denotes fixed probability space 
sensor signals known task recover mixing matrix source signals ai aei denote columns ei unit vectors 
assume mixing matrix full rank different columns ai aj linearly independent note quadratic case invertible 
gl 
recover sources sensors bss problem said easily reduced quadratic bss problem selecting sensors applying sophisticated preprocessing pca 
separation problem mainly interested overcomplete case sensors sources 
problem stated ill posed restrictions 
step approach separation quadratic case sufficient recover mixing matrix order solve separation problem sources reconstructed inverting overcomplete case finding similar fashion quadratic ica matrix recovery step sources chosen dimensional affine vector space solutions suitable boundary condition source recovery step 
algorithm follow step approach separation sources mixtures step approach proposed delta distributions 
contrasts single step separation algorithm lewicki sejnowski steps fused minimization single complex energy function :10.1.1.164.7690
show approach resolves convergence problem induced complicated energy function reflects quadratic case special case obvious way 
matrix recovery step step mixtures goal find matrix mat full rank pairwise linearly independent columns exists independent random vector assume component gaussian known equivalent consider matrices mat equivalent written bp invertible diagonal matrix scaling matrix gl invertible matrix unit vectors row permutation matrix gl 
overcomplete case uniqueness theorem known believe restrictions true anyway 
geometric matrix recovery generalization geometric ica algorithm 
restrict case dimensional mixture spaces illustrative purposes mainly 
high dimensional problems geometrical algorithms need samples practical arbitrary 
independent dimensional lebesgue continuous random vector describing source pattern distribution density function denoted independent factorizes 
xn 
xn marginal source density functions denote vector sensor signals mixing matrix 
assumed full rank pairwise linearly independent columns 
interested dealing scaling factors assume columns euclidean norm 

denote columns write 

geometric learning algorithm symmetric distributions simplest form goes follows pick starting elements 
wn unit sphere wi opposite wi 
wi pairwise linearly independent vectors rm wi called neurons resemble neurons clustering algorithms kohonen self organizing maps 
usually takes unit roots wi exp 
furthermore fix learning rate 
iterate step appropriate abort condition met choose sample rm distribution pick new note case happens probability zero probability density function assumed continuous 
project unit sphere yield 
wi neuron closest respect euclidean metric 
set wi wi sgn wi rm denotes projection dimensional unit sphere rm wi 
neurons moved iteration 
similar quadratic case algorithm may called absolute winner takes learning 
resembles kohonen competitive learning algorithm self organizing maps trivial neighbourhood function neighbour algorithm modification step size direction sample depend distance learning process takes place source recovery step results assume estimate original mixing matrix 
left problem reconstructing sources sensor signals estimated matrix full rank equation yields dimensional affine vectorspace solution space problem ill posed assumptions 
assumption derived maximum likelihood approach shown :10.1.1.164.7690
problem source recovery step formulated follows random vector matrix find independent vector satisfying assumption 
considering neglecting additional noise imagined determined probability observing 
bayes theorem posterior probability probability event knowing samples standard approach reconstructing maximum likelihood algorithm means maximizing posterior probability knowing prior probability samples find probable 
terms representing observed sensor signals basis ai called probable decomposition terms overcomplete basis columns posterior sources obtain estimate unknown sources solving relation arg max asp arg max asp arg max asp 
equation fully determined trivial 
note course maximum constraint necessarily unique 
assumed laplacian si exp get arg minx vi denotes norm 
show solution unique 
note may unique norms example considering supremum norm perpendicular affine vectorspace unique 
general algorithm source recovery step maximization constraint 
linear optimization problem tackled various optimization algorithms 
assume laplacian prior distribution characteristic sparse coding observed sensor signals 
case minimization nice visual interpretation suggests easy perform algorithm source recovery step consists minimizing norm constraint samples 
norm vector pictured length path parallel steps axes call search shortest path decomposition show represents shortest path rm lines matrix columns ai aei experimental results section give demonstration algorithm 
calculations performed amd athlon ghz computer matlab took minute 
mixed speech signals sensor signals shown left side middle 
iterations mixing matrix satisfactorily small minimal column distance original matrix source recovery calculate correlation estimated original source signals error cor 
right estimated source signals shown 
see resemblance original sources error high 
fig 

example sources left mixtures middle recovered signals right 
speech texts peace love 
signal kurtosis 
suggest fundamental problem source recovery step knowledge probabilistic approach improved 
explore aspect performed experiment source recovery algorithm recover laplacian signals mixed cos cos sin sin started algorithm correct mixing matrix 
compared error cor correlation matrix recovered signals original ones different angles 
result nearly independent angle sense show shortest path algorithm invariant coordinate transformations 
experiment indicates general border sources recovered overcomplete settings 
step approach overcomplete blind source separation 
original mixing matrix approximated geometry mixture space similar fashion geometric algorithms quadratic case 
sources recovered usual maximum likelihood approach laplacian prior 
research issues dealt 
hand geometric algorithm matrix recovery improved tested especially higher mixing dimensions currently experiment overcomplete generalization quadratic algorithm histogram geometric algorithm looks stable faster 
hand source recovery step analyzed detail especially question natural information theoretic barrier data recovered overcomplete settings 
amari cichocki yang 
new learning algorithm blind signal separation 
advances neural information processing systems 

blind separation sources mixtures sparsity short time fourier transform 
proc 
ica pages 
chen donoho saunders 
atomic decomposition basis pursuit 
technical report dept stat stanford univ stanford ca 
comon 
independent component analysis new concept 
signal processing 
prager 
development low entropy coding recurrent network 
network 
jung theis lang 
histogram approach linear geometric ica 
proc 
ica pages 
lewicki sejnowski :10.1.1.164.7690
learning overcomplete representations 
neural computation 
olshausen 
learning linear sparse factorial codes 
technical report aim 
olshausen field 
sparse coding natural images produces localized 
technical report 
prieto 
adaptive geometrical procedure blind separation sources 
neural processing letters 
prieto jutten alvarez ortega 
separation sources geometry procedure reconstruction valued signals 
signal processing 
