discriminative training hidden markov models kapadia downing college march dissertation submitted university cambridge degree doctor philosophy declaration dissertation result includes outcome done collaboration stated 
submitted part degree university 
length thesis including footnotes appendices exceed words 
contents declaration vi abbreviations vii notation viii hidden markov models definition 
hmm modelling assumptions 
hmm topology 
finding best transcription 
setting parameters 
summary 
objective functions properties maximum likelihood estimators 
maximum likelihood 
maximum mutual information 
frame discrimination 
differential learning 
summary 
optimisation constraints 
derivatives 
steepest ascent 
line methods 
conjugate gradients 
quick prop 
line quick prop 
ii contents iii manhattan optimisation 
extended baum welch 
line search 
cubic fitting 
interval location 
interval refinement 
summary 
previous literature theoretical comparisons 
nn hmm hybrids 
hmm studies 
isolet experiments isolet database 
isolet preprocessing 
previous results 
comparison optimisation methods 
temporal evolution mmi training 
temporal evolution fd training 
isolet results 
connex set experiments connex set database 
connex set preprocessing 
previous results 
connex set results 
review 

list figures block diagram speech recognition system 
hmm topologies 
isolet recognition hmm 
connex set recognition hmm 
convergence newton algorithm quadratic 
convergence steepest ascent quadratic 
convergence stochastic approximation regular maximum 
convergence conjugate gradients quadratic 
intervals guaranteed bracket maxima 
isolet preprocessor 
word entropy epoch mmi training different algorithms 
word accuracy epoch mmi training different algorithms 
frame entropy epoch fd training different algorithms 
frame accuracy epoch fd training different algorithms 
log probability epoch ml training different algorithms 
word accuracy epoch ml training different algorithms 
temporal evolution word accuracy mmi training 
temporal evolution word entropy mmi training 
temporal evolution log probability mmi training 
temporal evolution frame accuracy mmi training 
temporal evolution frame entropy mmi training 
temporal evolution word accuracy fd training 
temporal evolution word entropy fd training 
temporal evolution log probability fd training 
temporal evolution frame accuracy fd training 
temporal evolution frame entropy fd training 
connex set preprocessor 
iv list tables hcode mel scale triangular bins 
speaker independent isolet results 
multi speaker isolet results 
training set isolet results 
speaker independent connex set results 
training set connex set results 
modern speech recognition systems hidden markov models 
despite widespread properties understood 
aims increase understanding training hidden markov models classification 
examine question best measure hidden markov model fitness 
research shows principle motivated previous studies incorrect model sub optimality maximum likelihood wrong 
show identity best hidden markov model fitness measure objective function depends outside factors model flexibility size training set 
factors control point training set performance test set generalisation limiting factors 
conjecture major effect controlling test set generalisation confusion environment state probability density functions trained 
idea introduce new class hidden markov model frame discriminative hidden markov model 
focus zero memory frame discriminative hidden markov models having generalisation ability maximum likelihood hidden markov models better training set performance 
study optimisation frame discrimination objective function 
comparison traditional learning techniques modern machine learning ones shows machine learning ones considerably faster 
show possible increase speed learning incorporating extra knowledge hessian structure fitness surface 
ideas obtain general purpose reasonably fast training algorithm line manhattan quick prop 
apply zero memory frame discriminative objective function line quickprop alphabet recognition tasks 
experimental results provide empirical ver ification training set test set performance frame discriminative training 
re sults show compared maximum likelihood hidden markov models produce considerable reductions model size frame discriminative hidden markov models maintaining accuracy 
lastly ideas 
keywords speech recognition connex isolet set alphabet speech recognition hidden markov model maximum likelihood discriminative training classification merit maximum mutual information frame discrimination baum welch optimisation steepest ascent conjugate gradients newton algorithm manhattan learning quick prop 
vi abbreviations clam classification merit 
id frame discrimination 
imm hidden markov model 
learning vector quantisation 
mi maximum likelihood 
mmi maximum mutual information 
minimum mean square error 
nn neural network 
pdi probability density function 
vii notation represents identity matrix 
vii element vector element matrix transpose vector matrix 
represents expected value cov represents variance cov represents covariance represents generic objective function 
represents different hmm parameter vectors 
size vectors column vector derivatives hessian matrix number training utterances 
depending context transcription synthesised hmm th training utterance 
illustrates synthesised training hmm 
represents complete set preprocessed training utterances 
parameterised speech frames th training utterance 
speech frame occurring time training utterance 
number frames training utterance 
represents task recognition hmm 
illustrates recognition hmms dissertation 
total number hmm states 
ooo viii notation ix qi represents state indicates ranges possible full state paths synthesised hmm indicates ranges possible full speech frame gaussian pairings synthesised hmm indexes speech frame gaussian pairings returns state time construction start state associated hmm 
st state time indexes speech frame gaussian pairings returns gaussian time gt gaussian time aid transition probability moving state state represents number gaussians associated emitting state 
mi represents mean gaussian state ci represents covariance matrix gaussian state wi represents weight gaussian state true probability event event hmm 
estimated probability event event hmm 
estimator parameter vector 
chapter seen explosion interest automatic speech transcription 
explosion causes including 
availability cheaper faster processors 

increased computers untrained public 

growth voice carrying telecommunications network 
resulting increase research led steady improvements transcription accuracy 
compared humans current recognisers fragile inaccurate 
ease humans decode speech deceptive 
causes variability speech decoding hard process computers 
mapping acoustic signal transcription 
non determinism due factors including 
anatomical build speaker 
instance men lower fundamental cies women 

dialect differences due different social economic backgrounds 

differences speaking rate 

intonation changes 

alternations loudness 

grammatical semantic context word 

background sounds 

reverberation 

electronic noise recording 

homo phones word boundary ambiguity 
chapter 
acoustic model speech acoustic preprocessor hmm classifier transcription block diagram speech recognition system surprising best transcription systems date simple model hidden markov model hmm 
illustrates structure typical hmm recogniser including thesis 
broken main components 
acoustic preprocessor black box aims transform incoming speech form modelled accurately 
aims reduce data rate signal reduce computational complexity rest recognition system 
ideal preprocessor able produce speech frames contain information corresponding transcription original waveform 
theory large amount training data flexible classifier model speech waveform directly discard preprocessor 
practice complexity classifier limited amount training data preprocessor essential 
standard preprocessors designed utilising knowledge features speech waveform important recognition 
attempt dissertation improve preprocessors 
sections describe detail 
acoustic model black box categories speech segments 
knowledge encoded model known hmm 
described detail chapter 
aims improve component 
language model module imposes grammatical constraints transcriptions produced recogniser 
eliminating impossible transcriptions transcriptions performance recogniser significantly boosted 
component discussed chapter 
classifier process integrates input components outputs best matching transcription 
see chapter detail 
chapter 
hmm parameters determined 
process commonly known training 
requires elements 

training database consisting speech recordings associated transcriptions 

objective function training database measure hmms fitness 
optimisation procedure maximise objective function 
training optimisation procedure search hmm parameter vector high fitness 
problem determining hmms parameters essentially optimisation process 
thesis shall limit study training acoustic model 
assume rest system fixed 
particular shall investigate nature objective function methods optimisation 
standard hmm training process maximum likelihood ml training described chapter 
chapter study properties ml detail 
chapter show hmm classifier ml hmm learns irrelevant fact better noise 
essence ml hmm suboptimal flexibility 
chapter shall describe alternative objective functions maximum mutual information mmi frame discrimination fd 
compared ml mmi fd better hmms flexibility 
chapter number optimisation algorithms suitable mmi fd 
chapter compares contrasts objective functions alternatives ml previously suggested 
show alternatives theoretically suboptimal 
chapters presents experimental results standard speech tasks 
experiments show case comparisons attempted mmi fd better ml 
addition vast majority cases fd hmms performed better mmi hmms 
chapter concludes study summarising discovered suggesting avenues study 
chapter hidden markov models speech recognition system thesis combines acoustic language knowledge sources structure hmm 
chapter describes type hmm 
definition name suggests component part hmm markov chain 
markov chains consist set states set transition probabilities 
system states distinguished start state 
full paths start state finish state 
states collectively known emitting states 
associated probability density function pdf 
case mixture multivariate gaussian distributions 
popular name hidden markov model concisely describes nature generating process 
usually see output pdfs corresponding state sequence hidden 
hmm simple admittedly accurate model speech generation process 
associate state configuration vocal tract 
state associated pdf determines probability particular segment speech signal produced 
motion vocal tract modelled transition probabilities 
complete state path represents sequence vocal tract configurations implication transcription 
model give probability sequence parameterised speech vectors came particular series words 
specified order define hmm 

number states loss generality starting state numbered state 
set transition probabilities ai ai represents probability moving state state 
mean vectors mi mi represents mean gaussian state 
covariance matrices ci 
ci represents covariance matrix gaussian state chapter 
hidden markov models 
mixing proportions wi wi represents weight gaussian state definition quantities satisfy constraints ai ai wi wi number gaussians associated state transpose vector matrix joint probability state sequence st qn th observation sequence px 
st qn px 
st qn px st qn bi exp mi tc mi st state time qi represents state ai mi ci wi hmm parameter vector 
speech frame occurring time th training utterance 
number frames th training utterance 
dimension speech vector definition dimension means covariances 
reader notice point term probability quantities involved probability densities 
writer feels justified order focus attention important aspects maintain notational compatibility current literature 
rigorous derivations require term 
terms cancel classification statistic leaving classifier unchanged 
chapter 
hidden markov models hmm modelling assumptions seen model highly simplified 
particular assump tions 

probability state dependent current state 
words independent acoustic signal state sequence 

acoustic signal dependent current state 
addition delta delta delta elements see sections provide local context dependency 

acoustics stationary state modelled mixture gaussians 
assumptions incorrect systems order reduce number parameters estimated 
necessary parameters lead unreliable estimates training data limited 
addition reducing number parameters reduces computational complexity 
problem serious additional parameter limiting heuristics successful recognisers 

pdf distinct states markov chain may tied share pa rameters 
obviously useful represent similar acoustics 

covariance matrix assumed diagonal 

number gaussians varied achieve best balance modelling flexibility complexity 

having separate hmm hypothesised transcription systems build building block hmms 
joined transition model start 
building block represents fundamental speech unit word syllable phone 
smaller inventory fundamental speech units lower number parameters 
resulting embedded non emitting states removed obvious markov chain equivalences 
shorten presentation assumed done 
actual implementation efficient leave non emitting states place adjust system compensate 
results different combinations heuristics 
order build effective hmm system number choices 
descriptive purposes shall decompose choices 
topology give hmms 

efficiently find best matching hmm 

set hmm parameters ai chapter 
hidden markov models hmm topology hmms evaluated tasks 
alphabet recognition isolet database set recognition connex database 
standard widely available databases results previously reported 
state hmms represent letter emitting plus non emitting states 
final phone set members similar states models tied 
state set models shared gaussian mixture weight state 
isolet database set members connex set database set members tying substantially alter equations thesis 
modifications involved typically consist extra summation calculating posterior probabilities partial derivatives members tied set 
pre post letter silence modelled state hmm 
emitting non emitting states 
hmms typical composite transcription hmm illustrated 
topologies chosen light previous experience instance attempt optimise 
finding best transcription known transcription minimises probability error true probability transcription speech 
bayes rule obtain iw decomposition yields computable factors 
lw probability observation sequence transcription recogniser value modelled synthesised hmm priori probability transcription thesis part problem specification kept fixed 
priori probability preprocessed speech 
independent ignored classification 
unknown approximate estimates computed hmm 
chapter 
hidden markov models transition non emitting state emitting state key silence hmm letter hmm silence letter silence synthesised transcription hmm hmm topologies chapter 
hidden markov models start isolet recognition hmm approximate hmm estimate viterbi approximation equation 
lw px lw sew max ii indicates ranges possible full state paths synthesised hmm tests carried showed performance obtained identical 
approximation allows obtain probability estimate best state path 
direct calculation quantity feasible number possible state paths exponential 
fortunately reduce computation required 
firstly note max px slr ser hmm represents possible transcriptions parallel 
hmms suitable recognition isolet connex set databases illustrated 
seen applied graph factoring techniques reduce number silence models 
junction equal probability 
tasks type graph reduction produces huge computational savings 
alphabet recognition tasks savings minor maximises savings produced optimisations 
reduce computational burden viterbi recursion 
chapter 
hidden markov models start connex set recognition hmm recursion allows find state sequence linearly 
mx ai bj vn ax ai arg ax ai ax ai equations represents joint probability maximised partial state sequences st state time full state path 
final computational saving comes beam width pruning 
calculated reachable active state time 
active states states close maxk vk 
heuristic reduces computation little loss performance margin chosen carefully 
setting parameters section describe widely learning algorithm hmms maximum likelihood ml learning baum welch algorithm 
section serves emphasise notation provides basic results needed calculate derivatives hessians 
chapter examines rationale ml alternatives 
ml training involves maximisation probability speech straightforward knew speech frame gaussian pairings training data 
unknown closed form solution rely iterative techniques 
chapter 
hidden markov models baum welch algorithm uses initial estimates parameters ai mi ci obtain tentative speech frame gaussian pairings 
obtain re estimates having property px probability training utterances calculated parameters takes place algorithm reapplied 
process continues ad hoc convergence criterion met 
shall see algorithm maintains required sum positivity positive definiteness constraints 
central presentation baum welch algorithm auxiliary function 
derivation 
training data consists independent identical trials log ex log ex lw synthesised hmm th training utterance 
introduce example speech frame gaussian pairing expression probability th training utterance 
represents connected sequence gaussians potentially produced 
log px iw log px lw lo lo log ex iw mlo log adding instances obtain lw mw mlo log px ml indicates ranges possible full speech frame gaussian pairings synthesised hmm 
left hand side reduces log px 
log px chapter 
hidden markov models mlo mw px mlo mlo mw noted point derivation valid particular consider equality og px og easily proved og px og px define parameter re estimates maximise function 
prove fixed point transformation corresponds critical point likelihood 
oh px log lw oq chapter 
hidden markov models equivalent oa proceed calculate transition probability re estimate 
px mlo lw lw gs gs log log log ivs ms cs indexes speech frame gaussian pairings returns state time indexes speech frame gaussian pairings returns gaussian time order impose sum constraint introduce lagrange multiplier ias shall see positivity constraint automatically met 
examine equality satisfied re estimate iss oq pi zi fi oi px mlo zz aid ix st st jlo px st st jlo bi chapter 
hidden markov models re estimate satisfies mlo mw ai st bi adding instances obtain eu 
st ai st zu 
proceed calculate gaussian weight re estimate 
order impose sum constraint introduce lagrange multiplier cq 
positivity constraint automatically met 
require solution oq cq chapter 
hidden markov models gt gaussian time derive mean re estimation equation 
require solution oq oi mw klo eu ett st gt klo mi eu ett px st gt klo covariance re estimate solve oq oci chapter 
hidden markov models shall see positive definiteness constraint automatically satisfied ci log ii oi ci mi mi tci eu 
px st gt klo eu 
px derive known efficient recursions forward backward recur sions quantities needed re estimation equations 
idea iteratively calculating probability state successive px lw px st px st px io st equated px st px 
io 
st 
st st chapter 
hidden markov models modelling assumptions ai jbj initial values 
similarly aj ibi final values ai 
statistics easily compute various useful probabilities 
lw px st st llo px st st ai 
lw px st st jlo lw px st gt klo aj iwi bi lw st px lw chapter 
hidden markov models equation reduce computation required calculation 
equation applied st floor value 
floor smaller floating point accuracy computer results exact 
floor value 
baum welch re estimation formulas 
ai ci tt ai jbj tt 
ai 

aj iwi summary chapter reviewed theory baseline modelling approach 
firstly described modelling technique hmms 
showed hmms viterbi algorithm classify speech 
simple derivation baum welch algorithm ml learning 
techniques ones successful speech recognition systems 
provide baseline shall evaluate new methods 
chapter alternatives ml objective function 
addition show viterbi algorithm decoding remains valid 
alternative objective functions require new training algorithms described chapter 
chapter objective functions chapter reviews depth theoretical justifications various objective functions 
chosen selective coverage 
objective functions described considered effective important due widespread theoretical value 
firstly review theory ml estimators 
show justified making incorrect assumptions optimal occasions 
describe examples ml estimators traditionally confusingly named ml maxi mum mutual information mmi frame discrimination fd 
shall describe non ml estimator currently popular classification merit cfm 
properties maximum likelihood estimators section discuss ml estimators abstractly 
represent event probability maximising symbol training set consisting examples represents joint event example speech literature ml event domain mmi event domain fd event st domain st 
way prove properties shared ml estimators 
past studies emphasised role ml estimators finding confidence intervals correct models 
context known method extremely justification 
instance 
ml estimator function sufficient statistic sufficient statistic exists 
statistic said sufficient parameter conditional distribution observations value statistic independent ml arg mx chapter 
objective functions eit eit vi equation shows sufficient statistic contains information parameter original observations 
equation shows ml function information 

large sample case ml estimator distribution normal unbiased covariance matrix reaching cramer rao lower bound covariance unbiased estimator 
shall prove sufficiency ml 
function training data estimator identity true 
sufficient words independent written 
gives op elt tells ml estimator solution equivalent ml estimator function derive asymptotic distribution ml estimate need prove properties distribution ogp purposes proofs shall assume regularity conditions necessary manipulations valid 

expected value ogp zero 

covariance matrix 
large sample case approximately normally distributed 
ol chapter 
objective functions see oh normally distributed sum independent variables og may invoke central limit theorem 
prove og 
og 

oe prove definition oa oh pe log log oa og ogp igp op igp log preliminary results find sampling distribution ml estimate log mu evaluated mu true value definition zero vector 
taylor theorem quantity approximately equal log ogp 
ogp 
chapter 
objective functions aml oa 
large sample case replace observed value expected value 
om sampling distribution follows expected value ml covariance matrix log tends normally distributed large samples 
estimator converge rue parameters data 
asymptotic desirable proper estimator 
shall prove unbiased estimator lower dispersion ml 
nw matrix unbiased estimator ov cov ox ov represents identity matrix cov represents variance mog represents covariance covariance matrices positive semi definite follows ir log px semi positive definite 
order better understand result consider linear combination estimated parameters ct weighting factors 
unbiased estimator ct unbiased estimator ca 
variance cov result obtain oov oa gives cov ct log px oa chapter 
objective functions asymptotic dispersion ml estimate unbiased estimator 
fields model true generator data sufficiency efficiency powerful justification ml method 
sufficiency allows reduce memory requirements training procedure storing raw training data 
addition efficiency gives peace mind comes knowledge ml estimator larger variance large class estimators class unbiased estimators 
hmm true generator speech fact true generator unknown 
clearly results applied speech recognition field 
stated speech researchers ml suboptimal 
reasoning statement flawed 
assume model correctness order prove sufficiency efficiency 
allow say ml suboptimal model incorrect 
contrary absence information properties ml estimators remain biased 
fact extend analysis provide intuitive justification small sample inexact model case 
firstly obvious ml invariant 
ml ml estimate ml ml estimate 
property ensures front representational abilities model effect 
addition write ml parameters aml arg mx log arg mx log log understand take limit ml log px quantity minimised ku back er distance probability esti mates supplied hmm mode px unknown true source distribution ml method seeks minimise sampled ku back er distance px 
consistency ml easy understand 
distance minimum zero matches 
important explicit feature ml method rarely 
measured ml objective function value model depends model estimate 
models value depend model estimate incorrect classes 
means px needs calculated adjusted training 
chapter 
objective functions model estimate probabilities irrelevant ignored 
results computational saving training process 
show ml unique having property locality known 
training set consisting require objective function 
maximum probability estimates close true probabilities 
occurs locality model valued 
tt tt limit require equality 
max order prove ml unique displaying property locality show solution equation occurs 
log 
order find maximum probabilistic constraints introduce auxiliary variable 
differentiating lagrangian obtain op op arbitrary constant integration 
summarising shown large sample exact model optimality computationally efficient distribution approximation outside region viewed heuristic justification ml 
maximum likelihood ml commonest objective criteria hmm training 
complete dominance result empirically performance availability fast globally convergent baum welch training algorithm described section 
chapter 
objective functions reader note term ml mathematical literature refers method maximises model likelihood 
speech literature taken mean notation section event 
inappropriate nomenclature coupled incorrect assertion ml requires model correctness assumption lead widely accepted view ml justified 
fact alternatives proposed traditional ml estimators 
differ ml synthesised transcription hmms component larger model 
instance mmi training directly optimise bayes classification statistic 
accident history mmi named ml 
name ml taken mmi invented 
writer hopes notion fundamentally flawed proved founded 
exhibit bad attributes 
limited sample case models degenerate condition occur 
mean gaussian coincides observation vector corresponding variance tends zero increase limit 
parameters irrelevant leads poor generalisation novel test data 
wish increase likelihood allow achieve degenerate maximum 
fields insufficient data problem traditionally solved bayes smoothing techniques 
commonly case speech recognition field rely joint effect initial parameter values suboptimal optimisation techniques avoid problem 
initial point chosen acoustic observations map gaussian 
practical experience shown hill climbing procedures fall pathological regions parameter space 
experience shown produces accurate systems 
rest thesis search bad attributes seek answer question better 
maximum mutual information mmi training objective function chapter 
objective functions expression easily derived expression empirical mutual information mi name 
mi log lp lo logp maximisation mi hmm parameters second term ignored 
reader may wondering mi worthy study 
number reasons 
comparing bayes classification criterion see statistic 
ensures optimality test set training set size large 
comparing mmi see maximises conditional probability observation model mmi maximise conditional probability transcription 
px lw correct optimal properties ml estimator 
particular produce estimates asymptotically unbiased normal limitations assumed source distributions minimum dispersion possible 
shall show asymptotic dispersion ml estimate lower asymptotic dispersion mmi estimate 
lw logp lo arbitrary constant vector og lw og lo ct px oa obtain identity obtain ct og px lo oa chapter 
objective functions strict inequality ax ax positive definite 
marginal distribution ignored mmi provides information fom equation obtain yields dispersion ml estimate equal mmi estimate strict inequality px provides information notice px calculated ml models mmi models 
mmi places demands representational abilities models ml 
case instance px lo correct model px lw correct asymptotically mmi favoured 
incorrect model case approximating px available parameters profitably model conditional probabilities needed bayes decoding rule 
behaviour extends independent test set achieve improved recognition rate 
hope finding mmi better ml 
vital difference ml mmi methods model different source distributions 
mmi minimises empirical sampled distance probability estimate supplied model px lo true posterior probability word acoustic observations io 
ml minimises empirical sampled distance px lw lw 
point key motivation thesis 
worth emphasising 
ml training hmms trained model lw 
classification estimate px indirectly compute classification statistic px identity px iw px zw improve performance problem transcriptions explicitly allocate resources 
instance increase number gaussians correct synthesised hmm chapter 
objective functions hmms confused 
mmi training directly model lo 
see parameters hmms adjusted simultaneously 
mmi training process effectively allocates resources acoustic difficulty 
mmi uses parameters efficiently 
recap mmi objective function lo ogp ogp lw logp og px vw see term identical quantity ml training 
calculated manner 
second term obtained applying baum welch algorithm recognition hmm vw tasks computationally expensive step mmi training 
calculation mmi objective function requires recognition pass training utterances 
previously mentioned classification mmi hmm equation calculated alternative yields maximum chosen 
reader note denominator independent ignored 
addition calculate numerator viterbi approximation reduce recognition process standard ml approach 
frame discrimination section presents class novel objective functions 
objective functions shall explain potential better training set performance ml better tion mmi 
recap mmi objective function lo lw logp lr looking expression experienced practitioners identify possible source problems 
terms lw lr dominated chapter 
objective functions paths 
paths belong achieve rating 
paths different aim correct path 
restrict attention specimen observation may equivalently say aim separate correct incorrect sets state distributions associated match states irrelevant 
matters worse confusions time removed improvements times due state sequence coupling 
danger training set confusions exist learned 
consider independent test set 
model rating test set depends training set coverage states 
smaller training set equivalently greater hmms flexibility worse test set performance 
modelling flexibility hmm large training data limited may profitable increase number confused states 
course know states confused novel data 
situation 
best hope introduce extra confusions invalid confusions swamp valid ones 
possible approach introducing extra confusions replace recognition model hmm especially constructed increase number confusions 
new hmm allow superset state sequences allowed substituting obtain fd objective function log log original mmi calculating value requires forward backward passes training example 
synthesised transcription hmm second confusion hmm greater number allowed alignments underestimate equivalently training difference transcription hmm recognition hmm probabilities larger 
cases mean models trained higher training set accuracy 
balancing mentioned may generalise better 
addition tasks handling language model major share computational complexity 
tasks second term expensive compute third term 
objective function similar fd objective function hmm neural network nn hybrids 
manipulate order illustrate clearly 
log ex ln sco chapter 
objective functions null markov model px ls constant reduce fd objective function ml constructed allows superset state paths ls ls allows simplify sw px sin term equation objective function hmm nn hybrids 
term represents static transcription prior compensation needed hybrids align data 
hmm nn hybrids training procedures require separate alignment fitting stages 
recognition calculate equation alternative pick produces maximum 
px ln independent ignored 
addition viterbi assumption recognition process reduces standard ml 
easy show mmi fd estimator variance larger ml estimator 
letting du value fd objective function th training example relation log px lw log log px ln yields simple manipulation lw du ln reasoning equations obtain oa cts oa dispersion ml estimate equal fd estimate 
strict inequality px log px ox positive definite 
provides information dissertation report results experiments zero memory markov chain qi qi bi author derived relative mmi fd 
chapter 
objective functions experiments thesis set state priors qi constant independent test experiments showed limited parameter models slight performance improvement utilising memory 
statistically significant tasks 
utilising form obtain log sin ii stl ogp comparing fd ml see ml state pdfs approximate adjusts state pdfs approximate 
approximation model classification profitably extra freedom improve training set performance 
expect zero memory fd generalise independent test set ml 
cases assume worst case confusion environment ml assumption modelling separately leads worst case confusion environment estimate 
hand shall see chapter discriminative objective functions difficult optimise 
theoretical advantage enjoyed fd stress negated lack ability find better regressions 
differential learning approximating probability distributions differential learning attempts maximise training set accuracy 
training set accuracy approximated continuous differentiable function 
allows efficient gradient optimisation techniques 
approximation classification merit cfm suggested lo lo transcription excluding maximal probability utterance calculated parameters words correctly recognised second ranked transcription top ranked transcription 
continuous differentiable approximation step function parameter controls steepness 
suitable form chapter 
objective functions easily verified closer estimate training set accuracy 
downside graph develop vertical cliffs horizontal plateaus 
features hard optimise 
practice initially set small value 
increased neighbourhood optimum 
directly maximising training set accuracy definition result best training set performance possible limitations model 
model representational flexibility directed aim 
modelling aspects 
easily seen comparing particular training utterance 
involves probability observing transcriptions 
transcriptions correct best alternative irrelevant 
involves alternatives 
addition yields decreasing marginal rewards penalties increasingly correct incorrect classification 
law large numbers asymptotically large training sets guarantees test set performance 
performance achieved model correctness assumptions 
noted direct training set error minimisation double edged sword 
little information training set 
hmms approximately correct training data limited wasteful 
understand approach may ideal limited training sets imagine situation 
assume recogniser operates storing element observation vector associated transcription 
clearly training set size essentially infinite system perfect training set performance 
hand test set performance better chance 
behaviour detectable measure computer value cfm objective function 
practice question performance limited training data hmm model answered experiment 
seriously differential learning strong assumptions final models 
cfm move task differing prior probabilities errors penalised differently cfm useless 
methods learn probability distributions suffer problem 
summary chapter studied number objective functions 
showed ml estimators enjoy number provably optimal properties 
examples ml estimators speech literature ml mmi fd 
objective functions enjoy different advantages 
speech literature ml easily optimised baum welch algorithm generalises independent test set training data limited fd better training set performance ml ml generalises mmi yields optimal performance training database extremely large 
described non ml objective function cfm 
performance measure final system unweighted string error rate large training database demonstrably optimal 
objective function part recipe training procedure 
chapter 
objective functions element just important optimisation algorithm 
studied chapter 
chapter optimisation chapter various objective functions reasons possible utility 
chapter describe algorithms maximise functions 
examined baum welch algorithm production ml models 
algorithm theoretically guaranteed converge stationary point 
experimentally convergence fast 
factors ml models trained algorithm 
unfortunately baum welch algorithm applied alternative objective functions wish investigate 
fact algorithm known gives global convergence speed 
heuristic methods 
important source heuristics study quadratic function 
optimisation solve quadratic function 
equivalent solving set linear equations 
study important number reasons 
quadratic function simplest maximised 
linear functions constant unbounded 
addition taylor theorem tells functions approximated locally quadratic function 
taylor theorem certain assumptions satisfied objective function vector approximated xo xo xo differentiating obtain necessary condition maximum chapter 
optimisation convergence newton algorithm quadratic xo full rank yields assume starting initial approximation reach optima step 
method known newton algorithm illustrated 
algo rithm recommended 
calculating oj requires order time order space 

update suggested may large approximation invalid 

presence minima points inflection sufficiently near iteration unstable 
may circumstances step infinity converge unwanted points increase number iterations required 

may full rank may 
update directly usable quadratic model form solution heuristic basis algorithms 
important concept minimising quadratic approximation conjugacy 
full rank shown exists full rank matrix pt xo diagonal matrix 
infinite number choices instance columns eigenvectors transformation ko py chapter 
optimisation yt dy maximum maximising respect element separately 
identical sequentially maximising vectors pn pj column directions known conjugated set directions 
procedure property quadratic termination 
method guaranteed find optima quadratic iterations 
considerations size optimisation problem relative calculating function gradient hessian information serve restrict choice optimisation algorithm 
quadratic approximation contains independent terms 
altering alter position maximum 
expect find exact maximum calculated information 
recognition tasks tens thousands 
chance finding exact maximum 
best hope come close resources run 
heuristic maximise rate calculate information 
optimisation experiments search algorithm effective utilising information 
immediately rule methods compute hessian directly 
requires order space impossible store 
shall see gradient information free function value calculated 
sense restrict optimisation study gradient methods 
constraints maximisation maintain constraints 
experiments mmi objective function constraints ignored 
removes original probabilistic interpretations resulting transitions mixture weights viterbi decoding algorithm remains valid 
relaxation leaves prob constraints classification statistic intact 
initial experiments showed maintaining constraint produce consistent change performance 
fd objective function constraint needs imposed 
fd objective function unbounded 
mmi case main experiments ignored constraint 
probabilistic nature estimate require 
consistent change performance constraint imposed 
number methods impose constraints 
ruled stage constraints approximately satisfied 
addition involve complicated extra stages complexity limited process 
method failings transformation 
constraint satisfaction main thrust limited technique 
transform method maximising respect variables ai mi ci wi chapter 
optimisation maximise respect mi domains related constraint imposed 
constraint imposed 
ci easily verified ai wi resulting ai wi mathematically satisfy constraints 
ci ensures non negative definiteness 
tools corporate command line option controls addition positive definite diagonal matrix ci reestimate 
similar command line options exist floor ai wi command line options required experiments 
derivatives section expressions derivatives objective functions 
consider mmi objective function 
oi oa ogp lo log px ir lw lr derivative consists terms corresponding transcription recognition models 
shall drop dependency reader remember correct hmm calculating required quantities 
addition require derivatives constraint equations 
sum constraint imposed oai ai ai pai ow wi chapter 
optimisation hand sum constraint oai ai jsp ow wi ksp addition need observation derivatives necessary components results 
sum constraints imposed sum constraint oai aip st io px st gt plo ow wi pn chapter 
optimisation derivatives respect observation parameters st gt klo mid 
log bid oq ci st gt klo gt klo mi mi ci derivative equations involve types compo nents 
type contains current hmm parameters ai mi ci wi 
obviously immediately available computation 
second type contains ment expected values 
immediately available px calculated baum welch algorithm 
hmms unusual probability derivatives calculated little extra 
steepest ascent algorithm uses update xk xk effect replaced hessian term heuristic equation identity matrix 
learning rate fixed user line search see 
small method traces ascending path parameter space line greatest slope 
practice algorithm displays number faults 
ignoring second higher order derivatives region validity update equation small 
line greatest slope locally appealing may lead small improvements 
successive gradients exact line searches orthogonal conjugate 
general steps may introduce gradient components searched directions 
illustrated 
method quadratically convergent line searches method globally convergent 
line methods objective functions wish study consist sum similar terms 
case redundancy training set 
additional marginal chapter 
optimisation convergence steepest ascent quadratic information reduces term added 
symbolically situation log fu log fu duplicate training data times required equation increases factor 
clearly better updating times 
process taken extreme update training pattern leads algorithm known stochastic approximation 
update equation gradient th randomly chosen training example 
shown algorithm converges stationary point probability satisfies lim oz unfortunately real world terminate training finite amount time 
may find update strategy may best 
chapter 
optimisation convergence stochastic approximation regular maximum effectiveness line methods depends close maximum wish come data redundancy objective function model 
compares optimisation algorithms examined experimentally 
behaviour stochastic approximation quadratic function illustrated 
conjugate gradients major failing steepest ascent parameter updates may introduce gradient components searched directions 
notion conjugate directions avoids difficulty see page 
conjugate gradient method seeks find directions efficiently 
pure gradient search direction conjugate gradients removes components conjugate previous search directions 
search directions di ex ex zi jdj 
di search direction point ai 
coefficients zi chosen assuming constant di satisfies conjugacy conditions 
dt dj applying wr obtain ai dt chapter 
optimisation know substituting gives maximisation previous search directions exact dk allows prove result allows final expressions zi equivalent quadratic function perfect line searches 
practice values differ 
obtain zi xi xi rl 
li li known beale sorenson expression 
alternatively substituting obtain zi ki ki xl 
li chapter 
optimisation convergence conjugate gradients quadratic polak expression 
obtain fletcher reeves expression forj 
summarising conjugate gradient algorithm di zo lai zi ldi 
ai ai zi cq chosen maximise direction di 
quadratic function exact arithmetic line searches algorithm exhibits quadratic termination 
error surfaces tasks properties 
hope method better steepest ascent 
behaviour algorithm quadratic function illustrated quick prop algorithm replace hessian term finite difference diagonal approxi mation 
nli element matrix chapter 
optimisation substituting approximation vii element vector heuristic iterative optimisation algorithms impose restrictions update rule ensure stability 
suggested ensure step proceeds uphill step small trusted 
di growth factor parameter set user 
value greater 
equation violated finite difference hessian component positive 
situation update equation attempts move critical point center nearby minimum 
quadratic model suggests correct course action move large distance gradient 
equation violated suggests limit growth step circumstances default update 
sign bootstrap algorithm iteration steepest ascent algorithm introduced section manhattan learning 
algorithm abandoned hope finding global maximum exceptional cases 
attempt progress possible resources exhausted 
line quick prop view speeding convergence highly redundant training sets quick prop transformed line algorithm 
brief experimentation scheme chosen 
training data split approximately equal sized blocks 
differing block sizes compensated dividing function values gradients number frames block 
update occurred block processed 
better convergence obtained hessian approximation involved block data 
calculated chapter 
optimisation ensure stability necessary reduce maximum growth factor equation 
xn available updates normal quick prop algorithm updates 
rest original quick prop algorithm remains unchanged 
manhattan optimisation section learning algorithm uses manhattan update rule 
contrast methods described previously update involve gradient magnitude 
uses fixed update size chosen hopefully move maximum 
obtain update step number approximations 
expressions easily realisable hopefully useful 
approximations 
calculating update step replace objective function log probability 
hope shape functions similar 

nd order taylor series calculate update 
newton method 

assume constant segmentation calculating hessian 

assume hessian diagonal 

various points replace sample statistics related quantities ignore 
newton update involves second order derivatives 
analogy approximation quantities log px presentation shall split parameter vector various component types ai mi cj reader reminded ai cj introduced order remove parameter constraints 
order simplify implementation ignored transition weight sum constraints assume diagonal covariance gaussians 
simplifications deemed acceptable attempting approximate second order training information gross approximations 
shall derive manhattan update ai differentiating obtain oai chapter 
optimisation analogously obtain jn differentiating obtain simplify manhattan update oi 
differentiating obtain manhattan update mi ignore mi term assumption ml means variances approximately correct optima new objective function uv tt px st gt klo mi oi uu tt lo manhattan mean update px st gt klo chapter 
optimisation obtain practice steps large 
actual updates small fraction expressions 
summarising updates wl wi mi em sign parameter dependent learning rates 
full covariance case require suitable update diagonal covariance terms 
unfortunately appear simple derivable expression 
selected analogy 
sign 
simplicity expressions striking 
updates independent differing variance occupancy scale effects 
apart sign gradients dynamically calculated statistics involved 
method candidate line strategy 
extended baum welch studies mmi estimation extended baum welch algorithm popular 
theoretical guarantees ml baum welch 
guarantee convergence stationary point 
practice shown useful 
derivation extended baum welch ml baum welch long infor 
reader referred details 
shall limit chapter 
optimisation presentation final re estimation equations 
reader note variance update extended full covariance case 
experiment extension shown useful 
algorithm applied fd objective function change 
update equations ai di ai wi di vi fmi mi wi mi km wi jn 
jn 
ew max klo px gt st gt parameter chosen method suggested 
briefly large equation produced positive definite re estimates 
set small chapter 
optimisation value 
doubled resulting ci positive definite 
final value doubled provide safety margin 
line search section consider problem maximising generic objective function line 
maximise line ai ai current point di search direction 
resulting algorithm component steepest ascent conjugate gradient algorithms 
chosen algorithm assumptions form particular chosen guaranteed converge local maximum matter uncooperative assurance achieved embedded intervals guaranteed contain maxima 
continuity considerations tells interval contain maxima ki ai ooi ki ai xi adi xi bdi situations illustrated 
algorithm stages interval location interval refinement 
major part stages bounded cubic interpolation extrapolation step 
subsections describe cubic fitting interval location interval refinement steps detail 
cubic fitting process approximates ai function cq cubic 
cubic chosen value derivative match xi endpoints current interval symbolic terms wish approximation cl oq oq oq ki cq cq reader note chosen form cubic aim simplifying proofs 
setting cq obtain ai adi ki ai cl ooi chapter 
optimisation intervals guaranteed bracket maxima substituting setting cq obtain solving cs obtain xi adi xi bdi ai ooi ai ooi chapter 
optimisation critical points cubic setting zero solving cq 
ai ai answer roots maximum examining sign ai 
ai take negative sign 
cubic maxima interval location purpose stage find interval satisfies 
input stage starting point line search ai expected length interval 
interval length satisfies conditions stage returns 
new interval tested 
left hand boundary comes old right hand chapter 
optimisation boundary 
new right hand boundary chosen place extrapolated cubic maxima center new interval 
extrapolated cubic maxima subject restrictions right old interval new interval length grow factor 
interval refinement purpose stage accurately locate maxima respect iteratively examining new points bracketing interval uses results produce new smaller interval 
iterations current interval satisfies interval guaranteed contain maximum 
trial points chosen cubic interpolation restriction trial points lie interval 
search terminated user specified tolerance number iterations reached 
summary chapter obtained ml mmi fd derivatives 
described derivative optimisation algorithms steepest ascent conjugate gradients quick prop online quick prop manhattan optimisation 
addition described non derivative optimisation algorithm heuristically extended baum welch algorithm 
proceed experimental evaluation new objective functions optimisation algorithms 
chapter shall look objective functions 
time shall comment selection objective functions proposed previous literature 
chapter previous literature study hmm discriminative training large body literature 
rapidly developing fields presentation disjoint contradictory 
section aims review align studies 
theoretical comparisons differential learning strategy probabilistic learning strategy compared 
task classifying data produced spherically symmetric normal distributions linear classifier bias 
cases probabilistic learning strategy performed best 
difference increased training set size decreased 
authors theoretical proofs empirical tests illustrating optimality cfm objective function 
unfortunately theoretical proof limited domain applicability 
valid probabilities associated distinct observations unconnected 
smoothness feature space 
clearly case attempt probabilistic objective function match observation probabilities suboptimal due smoothness model 
experimental results extremely encouraging 
tests involve parameterised models large training sets 
conditions ideal differential learning 
simple class normally distributed problem studied 
show cfm optimal training set error 
provided simple example cfm distinguish bad test set classifiers 
treats case classification model arbitrary mixture domains 
domain priors loss matrices 
non uniform priors loss matrices cfm longer optimal 
errors different weights 
domain mixture gives rise different set weights 
value model expected value classification domains results probabilistic objective function 
differentially trained acoustic models speech recognition systems hard justify acoustic models put unknown 
theoretical proofs showed number widely objective functions belong class probabilistic objective functions 
functions chapter 
previous literature examined mmi ml minimum mean square error mmse 
order shows question probability density model important objective functions 
authors derived weightings ml mmi mmse training utterances 
showed ml weighted training examples equally 
mmi gives low weight training examples correctly classified far decision boundary 
mmse hand gives low weight training examples correctly classified misclassified far decision boundary 
shows incorrect model case correct model case ml optimal see chapter training data mmse focusing marginal decisions exhibit lower training set error 
question model value measure differs error rate tackled 
argument problem dependent 
nn hmm hybrids presents integration feed forward neural network nn markov model 
nn works trained produce posterior probabilities state occupancies 
achieved combination probabilistic objective function conditional maximum likelihood viterbi labelling training data 
results reported encouraging 
systems similar fd systems apart differences 
systems massive nn explicit normalisation 
system uses local pdfs relies fact normalisation required classification 
global system contains parameters implement large vocabularies 
traditional hmm counterpart codebook rare 
disadvantage systems savings produced beamwidth pruning reduced 
entire nn active times systems 
works utilised stage optimisation process alignment followed function fitting 
purely computational viewpoint suboptimal line optimisation methods 

hmm nn hybrids works trained binary viterbi labelling speech frames 
elaboration works known remap 
showed binary viterbi segmentation replaced soft probabilistic segmentation 
computational terms remap model training competitive compared fd 
simplified time invariant nn algorithm slower 
requires forward propagation probabilistic segmentation back propagation derivatives accumulated 
current implementation fd training requires forward propagation 
cost derivative accumulation negligible pruning posterior probabilities see page 
pruning techniques forward propagation example gaussian selection increase factor chapter 
previous literature advantage 
discriminative hmm systems advantage initialised baum welch algorithm shortening training time 
authors learning vector quantisation lvq hmm hybrid 
lvq usual vector quantisation aim increase codebook accuracy state classification 
latest altered basic model attempt moderate loss information caused binary classification codebook 
system closest match lvq hmm hybrid differentially scored zero memory fd system 
may view recognition process integration local decision problems mentioned learning mixture decision problems leads probabilistic objective function 
differential fd scoring seen suboptimal 
recurrent nn interpolative probability estimating function 
hope recurrent net encode state units useful temporal information 
system roughly corresponds fd trained hmm utilising memory 
advantage recurrent nn approach memory discovered part training process 
disadvantages 
firstly recurrent nn run recognition 
secondly assertions thesis correct contextual information mean danger training data performance improved expense generalisation 
important depends size training set model flexibility 
hmm studies designer differential learning hmms currently popular 
earlier studies objective function objective function utilises experimental observation lo close zero intermediate values rarely seen 
conditions approximates training set error 
introduces different objective function yd lo log oo chapter 
previous literature positive constant chosen control number incorrect hypothesis contribute positive constant controls smoothness total number alternative transcriptions recognised system 
larger values produce increasingly better approximations training set error 
objective functions described chapter theoretical justification 
design guided creator experience 
seek differential learning robust incorporating confusion statistics 
sense considered intermediate differential learning mmi 
mean lose optimality properties differential learning mmi 
studies designer differential learning reported improved results compared ml baseline 
reported higher training set improvements compared test set improvements 
authors compared ml mmi 
studies encouraging results showed mmi training increase performance ml seed models 
showed training performance improved test set performance 
training set results especially interesting 
advantage better training algorithm training set accuracy achieved 
case optima mmi coincided differential learning optima 
probability distribution approximation property mmi shared differential learning expect test set performance mmi better differential learning task 
chapter isolet experiments chapter test methods isolet database 
preprocessing language model acoustic model standard ones optimised way 
independent variables objective function optimisation algorithm acoustic model complexity test database 
section compare speed optimisation algorithms 
best ones experiments 
sections attempt examine properties mmi fd objective function surfaces 
aim verify number theoretical predictions 
important mmi provably asymptotically optimal realistic classification task fd generalises better 
section conventional tables results show effect altering acoustic model complexity test set objective function 
isolet database isolet speech database contains examples english alphabet spoken isolation 
examples letter recorded subjects 
data collected ogi speech recognition laboratory 
laboratory tile floor standard office wall board drop ceiling 
room sun workstations disk drives 
prompt seconds speech recorded hmd noise cancelling microphone 
low pass filtered khz bit sampled khz 
uttered letter isolated semi automatic procedure surrounding silence removed 
remaining recording average seconds length 
average signal noise ratio calculated db 
isolet experiments ogi suggested partitions 
training set consists recording speakers 
multi speaker test set consists second recording training speakers speaker independent test set consists recordings remaining speakers 
chapter 
isolet experiments isolet preprocessing isolet database standard mel frequency cepstral mfcc preprocessing produced htk hcode 
production illustrated steps described greater detail 
digitised speech blocked ms segments ms 
samples offsets samples 


blocks passed order filter 
nth speech sample block 

hamming window applied block 
cos 

block padded elements trailing zeros 
discrete fourier transform applied produce complex spectral domain values 

magnitudes complex spectral values binned term zero direct current term ignored 
bins implemented triangular bandpass filters 
lower cutoff middle knot upper cutoff frequencies indexes table 
spacing chosen approximate mel pitch scale log frequency mels frequency hertz 

binned values logged 

values discrete cosine transform calculated 
tri mj cos 
mj logged value mel bin 
thirteenth element energy current frame appended 
chapter 
isolet experiments khz speech blocking pre emphasis hamming window pad fft mel bin log bins cosine transform append energy append normalise energy mfcc vectors isolet preprocessor chapter 
isolet experiments bin lo cutoff center hi cutoff mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz mel hz table hcode mel scale triangular bins 
elements appended 
delta delta delta coef 
deltas 
approximate rate change base cepstral energy coefficients calculated delta deltas ao approximate acceleration base cepstral energy coefficient calculated applying formula deltas 

recording log energy profile scanned low energy values replaced floor db peak value 
energy profile scaled peak value 
chapter 
isolet experiments previous results extensive experiments isolet database published oregon graduate institute 
database description report speaker independent speaker dependent performance 
recogniser developed described greater detail 
baseline classifier nn inputs hidden units output units 
input features nn chosen combination phonetic knowledge experiment aim yielding classification performance 
trained extended training set consisting normal training multi speaker partitions recogniser yielded performance 
adding specialised set classifier post processor initial set choice raised augmented training set speaker independent performance 
comparison optimisation methods section shall experimental comparison optimisation algorithms described chapter 
experiments carried isolet database see section standard hmms see section standard isolet feature extraction see section 
initial ml hmms trained baum welch algorithm see section results obtained speaker independent test set 
interpreting results reader bear mind number points 
discriminative training algorithms guarantee monotonicity 
steepest ascent conjugate gradient algorithms line search algorithms monotonic line search exact 
exactness requires line search function evaluations close parameter space 
practical computational cost 
author experimentally confirmed drastic decrease objective function occurred algorithms managed quickly recover 
improvements measured objective function imply improvements measures 
instance increasing likelihood score hmm necessarily increase mutual information fd score 
improvements linked little need 

training test set improvements linked 
guaranteed linkage requires training test sets identical 
occasions expect training set size increases equivalently modelling flexibility decreases performance difference training test set reduce 
graphs plots log conditional probability frame ev lo eu empirical word entropy epoch training algorithms 
user settable parameters chosen brief experimental trials order maximise speed convergence 
set follows chapter 
isolet experiments hne manhattan quick prop manhattan quick prop manhattan learning extended bw quick prop conjugate gradient steepest ascent epochs word entropy epoch mmi training different algorithms steepest ascent line search terminated step size ck equation tolerance 
usually required function evaluations 
steepest ascent curve include overhead 
comparing curve curves quick prop extended baum welch manhattan learning manhattan quick prop line manhattan quick prop reader bear mind algorithms hidden cost line search epoch 
conjugate gradients line search terminated step size ci equation tolerance 
plot assumes line search cost zero 
plot illustrates polak equation 
update rules similar graphs 
quick prop update uses steepest ascent rule 
updates quick prop maximum growth rate equation 
growth rates tried results fundamentally different 
extended baum welch user settable parameters implementation 
manhattan learning learning rate equations set iteration 
manhattan quick prop update manhattan learning 
updates quick prop rule floor imposed update size 
floor set size manhattan update global learning rate 
chapter 
isolet experiments quick prop manhattan quick prop manhattan learning 
xt ended bw 
ii op conjugate gradient steepest ascent epochs word accuracy epoch mmi training different algorithms line manhattan quick prop experiments updates epoch 
maximum growth rate set 
minimum step size set manhattan update initial global learning rate exponentially decaying epoch 
updates stayed floor value 
seen fastest algorithms close speed 
plots training set accuracy epoch 
illustrates little choose fastest algorithms 
fastest ones achieve accuracy epochs 
base choice training algorithm final performance better 
base choice speed convergence room improvement 
changes examine algorithm convergence fd objective function 
illustrated 
plots log conditional probability frame plots performance zero memory confusion hmm plot sequences identical classifications merged 
graphs show line manhattan quick prop algorithm better 
possible explanation effect mmi training training set errors little training set redundancy exploited 
hand fd objective functions training set partition provides estimate entire training set value gradient 
notice achieve performance 
know achievable accuracy possible algorithms substantially better 
figures illustrate remarkable robustness training algorithms 
manhattan quick prop algorithm fails epoch manages epoch fully recover 
chapter 
isolet experiments line manhattan quick prop manhattan quick prop epochs frame entropy epoch fd training different algorithms 
line prop correct online qu kc prop accuracy manhattan quick prop accuracy epochs frame accuracy epoch fd training different algorithms chapter 
isolet experiments baum welch line manhattan quick prop manhattan quick prop epochs log probability epoch ml training different algorithms indicates behaviour may due independence individual parameter updates 
relatively high classification performance epoch shows parameters reasonable values 
training remain unaffected period badly adjusted parameters recover 
line search procedures ensure drastic reductions objective function cost line search robustness instance quick prop 
try gauge extent training algorithms may suboptimal com pared baum welch see section 
plots normalised training set log proba bility epoch baum welch manhattan quick prop line manhattan quick prop 
initial models roughly initialised ml hmms variances set value tied 
seen line manhattan quick prop superior manhattan quick prop slightly inferior baum welch 
see difference line manhattan quick prop baum welch small 
author opinion improved discriminative training algorithm substantially alter reached dissertation 
plots training set accuracy epoch experiment shows trends 
drop performance epoch line manhattan quick prop epoch manhattan quick prop indicator likelihood metric lacks vital information 
basis results fd experiments done line manhattan quick prop algorithm mmi experiments done manhattan quick prop line manhattan quick prop 
chapter 
isolet experiments baum welch line manhattan quick prop manhattan quick prop epochs word accuracy epoch ml training different algorithms temporal evolution mmi training graphs display temporal evolution various metrics mmi training 
number training epochs increased presentation section 
addition line manhattan quick prop optimisation algorithm training 
factors ensure unusually wide area peak fitness surface sampled 
features note 
graphs large difference training set test set improvements 
addition evidence training optimisation progresses 
graph plot normalised log probability epoch gentle downward trend 
flexibility single mixture diagonal hmm model maintain likelihood conditional likelihood values 
graphs plot frame classification performance entropy epoch 
see word level entropy accuracy increases frame entropy accuracy decreases 
ml estimate scores state pdfs independently produces zero memory estimate 
test set performance dependent training set confusion coverage 
mmi training progresses hmm parameters move away ml values frame performance suffers 
decreasing test set performance provides evidence generalisation limiting factor 
chapter 
isolet experiments training set si test set epochs temporal evolution word accuracy mmi training temporal evolution fd training graphs displays temporal evolution various metrics fd training 
attempted sample wide area peak fitness surface manhattan quick prop increasing number training epochs 
see graphs plot word level classification performance entropy epoch 
improvement fd training progresses 
addition difference training test set performances reduced compared mmi training 
drop performance middle experiment due natural variation different performance measures 
verify training repeated experiment mixture hmm 
test zero memory fd performance initial ml performance 
order test conjecture generalisation performance fd ml attempted sample peak likelihood fitness surface 
done baum welch algorithm convergence properties 
applied manhattan quick prop ml objective function baseline single mixture hmms 
observe peak likelihood performance variation 
graph shows normalised log probability decreases training progresses 
distinct lines graph consist coincident curves making total curves 
curve possible pairing speaker independent test set training set transcription model recognition model cases chapter 
isolet experiments training set sl test set epochs temporal evolution word entropy mmi training 
si test set model si test set model training set model training set model epochs temporal evolution log probability mmi training chapter 
isolet experiments training set correct si test set correct training set accuracy si test set accuracy epochs temporal evolution frame accuracy mmi training training set si test set epochs temporal evolution frame entropy mmi training chapter 
isolet experiments training set epochs temporal evolution word accuracy fd training dramatic decrease model likelihood 
suggest parameters change substantially order improve fd metric 
graph illustration likelihood metric suboptimal classification 

graphs plot frame classification performance entropy epoch 
expected frame recognition entropy improves fd training progresses 
isolet results isolet results tables 
column describes type hmm involved experiment 
globally tied diagonal variance hmm experiments labelled grand 
diagonal mixture hmm experiments labelled diag 
single mixture full covariance experiments labelled full 
column headings describe source hmm objective function 
columns labelled ml give fiat start ml results 
columns labelled mmi give ml seed mmi training results 
columns labelled fd give ml seed fd training results 
fd mmi column give fd seed mmi training results 
looking ml results see obvious trends 
increasing model complexity increases training set performance diag experiment speaker independent multi speaker results 
theory predicts trend increasing hmm flexibility improves probability predictions hmms 
improvement continues parameters estimate reliably 
second trend occurs chapter 
isolet experiments training set sl test set ro epochs temporal evolution word entropy fd training si test set model si test set model training set model training set model epochs temporal evolution log probability fd training chapter 
isolet experiments 
si te set correct epochs temporal evolution frame accuracy fd training 

ning set set epochs temporal evolution frame entropy fd training chapter 
isolet experiments type params 
ml mmi fd fd mmi grand 
diag 
diag 
diag 
diag 
diag 
diag 
full 
experiment deemed worthwhile training set performance high 
table speaker independent isolet results type params 
ml mmi fd fd mmi grand 
diag 
diag 
diag 
diag 
diag 
diag 
full 
experiment deemed worthwhile training set performance high 
table multi speaker isolet results type params 
ml mmi fd fd mmi grand 
diag 
diag 
diag 
diag 
diag 
diag 
full 
experiment deemed worthwhile training set performance high 
table training set isolet results chapter 
isolet experiments different test sets 
training performance highest followed multi speaker performance lowest speaker independent performance 
trend predicted theoretically 
increasing mismatch training set multi speaker speaker independent test sets 
examine mmi experiments results labelled mmi 
striking aspect training set accuracy 
implies cfm empirical error variants better mmi 
fact expect mmi better attempt model data 
speaker independent test sets mmi training cases improve results 
occurs mmi model models aspects needed classification 
expected theoretical discussion page improvement decreases model complexity increases 
training larger models large variations performance iteration iteration occasions performance drop ml baseline 
moving fd column see cases results better ml ones 
predicted theory 
fd ignores values modelled ml needed classification 
comparing results mmi see grand experiment mmi produced better hmms 
rest fd better 
result effects 
grand case fd hmm flexibility model confusions introduced relaxed recognition model occur real speech serve mask ones test set performance suffers 
trained mmi hmms successfully models true training set confusions 
results better test set performance 
larger hmms adequately model extra confusions introduced fd models fd better mmi 
prolonged fd training performance remained ml baseline 
column labelled fd mmi applies mmi objective function fd trained models 
hope experiments performance improvement engendered fd training maintained removing training set confusions mmi training 
relying fact model flexibility mmi training substantially alter hmm parameters see graph 
see improvement obtained smaller models impressive 
provides evidence better accuracy obtained fd training algorithm improved 
larger models performance improvement impressive 
fact performance drop fd baseline 
chapter connex set experiments experiments chapter undertaken order provide independent check results chapter 
author attempted avoid sub conscious bias deliberately restricting number experiments 
apart preliminary ml tuning experiments reported 
connex set database british telecom set database contains examples british english set spoken isolation 
approximately examples letter different speakers 
speakers males females 
bad examples discarded 
database subset larger british telecom connex database 
known faulty files connex database 
set database corrections 
data collected booth 
prompt seconds speech recorded high quality headset microphone 
speech bit sampled khz bandwidth hz khz 
uttered letter isolated semi automatic procedure surrounding silence removed 
connex set experiments standard partitions 
speakers divided disjoint groups 
group contained roughly balance ages genders 
test group contained recordings training group contained recordings 
connex set preprocessing connex set database preprocessed produce mfcc coefficients 
production differed scheme described section 
process illustrated steps described greater detail 
standard channel filter bank bank applied digitised speech 
filters approximately mel spaced produced bit log energy values ms intervals 
chapter 
connex set experiments khz speech bank cosine transform normalise coefficients append mfcc vectors connex set preprocessor 
mfcc values calculated discrete cosine transform 
ogi case zero th coefficient included 
fori cos mj logged output bank channel 
mfcc coefficients scaled shifted truncated database integer range 

elements appended 
delta delta delta coefficients calculated equation 
previous results authors achieved baseline performance 
baseline hmm states states shared models 
input features mfccs differences 
author linear discriminant analysis algorithm best performance authors achieved 
hmm topology input features baseline model subspace representation full covariance hmm 
discriminative dimensions input dimensions retained 
authors reported results experiments carried full connex database containing letters alphabet 
hmm classifier states mixtures common diagonal covariance matrix model 
input feature vector elements 
outputs preprocessor normalised chapter 
connex set experiments type params 
ml mmi fd fd mmi diag 
diag 
diag 
diag 
diag 
diag 
full 
experiment deemed worthwhile training set performance high 
table speaker independent connex set results type params 
ml mmi fd fd mmi diag 
diag 
diag 
diag 
diag 
diag 
full 
experiment deemed worthwhile training set performance high 
table training set connex set results subtracting average value component 
th feature vector element frame energy 
classifier achieved accuracy 
authors results human recognition experiments full connex database 
removing faulty recordings tests gave performance 
connex set results tables give connex set results 
experiments carried order provide independent confirmation results obtained section recognition hmm illustrated 
comparing tables tables respectively reader easily confirm trends exist 
reader read section explanation trends 
chapter speech recognition research concentrates finding better models stage speech recognition process 
author opinion approach correct stages improved 
listening speech re synthesised preprocessed frames know preprocessing discards useful information 
hmm true acoustic source 
prediction accuracy language models far lower human 
shortcomings reduced speech recognition robust accurate 
dissertation adopted approach 
accepted hmm acoustic model 
attempted improve performance better training 
section reviews summarises 
section suggest obvious avenues study 
review chapter discussed theoretical properties various estimators 
proved widely known large sample exact model properties ml estimators sufficiency eft ciency 
importantly speech recognition task small sample incorrect model properties 
case ml estimators enjoy properties invariance probabilistic fitting locality 
authors stated ml estimation justified inexact model case 
contention view wrong 
chapter continued description ml estimators 
known speech literature ml 
described mmi fd 
training speech literature ml produces hmms approximate 
state pdfs resulting hmms produce estimate zero memory 
estimate observed random zero memory process 
estimate key understanding excellent generalisation observed ml 
ignoring acoustic context ml training data state confusions novel test data state confusions 
training mmi produces hmms approximate 
modelling reduce stress hmm 
increases training set formance 
test set generalisation suffers estimate extrapolate chapter 
training set state confusions 
dissertation concentrated member fd class estimators zero mem ory fd 
hmms trained zero memory fd produce estimates zero memory hmms extrapolation ability ml hmms 
addition compared ml modelling st directly ignore increasing training set performance effectively decreasing training set noise 
chapter ended discussion differential learning 
type learning attempt fit probability distribution 
making strong assumptions recogniser optimise application specific performance directly 
downside course recogniser application specific 
instance cfm optimal unweighted string error rate 
optimisation objective functions described chapter 
obtained objective function derivatives discussed variety optimisation algorithms 
optimisation algorithms evaluated chapter 
quickprop update hessian information step size manhattan learning line strategy improved speed 
experiments reported vast majority best results obtained epochs 
chapter sampled peaks various objective functions 
empirically verified zero memory fd generalised better mmi 
addition experiments supported notion zero memory fd generalised ml 
lastly final part chapter chapter considered effect changing model complexity 
cases peak mmi performance zero memory fd performances better ml 
large training data set size limit mmi best cases zero memory fd better 
previously poor generalisation mmi hmms attributed training 
training epochs model parameters 
experimental confirmation major cause poor generalisation failure mmi reward extrapolation training set state confusions 
success discriminative training relies speed accuracy learning algorithm 
huge literature optimisation evaluate 
particular study promising line learning techniques great depth 
hmm nn hybrids pruning technique called posterior phone pruning 
theory type pruning probabilistically trained hmm 
interesting experimentally verify hypothesis 
zero memory fd hope posterior phone pruning effective 
compared ml hmms increased effectiveness occur noise distribution modelled 
compared hmm nn hybrids increased effectiveness occur possibility gaussian selection reduce cost calculating st 
current successful large vocabulary systems rely speaker adaption achieve high recog chapter 
nition accuracy 
attempted study methods 
lastly large vocabulary speech recognition systems large numbers states gaussians 
contemporary hmm nn hybrids provide proof large vocabulary implementations fd possible 
addition zero memory fd enjoy advantages 
implemented cache friendly manner pruning techniques example gaussian selection 
bibliography aczel 
remarks measurement subjective probability information 
september 
amari 
theory adaptive pattern classifiers 
ieee transactions electronic computers ec 
etienne barnard 
performance generalization classification merit criterion function 
ieee transactions neural networks 
etienne barnard 
optimization training neural nets 
ieee transactions neural networks march 
roberto battiti 
second order methods learning steepest descent newton method 
neural computation 
leonard baum 
inequality associated maximization technique statistical estimation probabilistic functions markov processes 
inequalities pages 
leonard baum 
inequality applications statistical prediction functions markov processes model ecology 
bull 
amer 
math 
oc 
leonard baum ted petrie george soules norman weiss 
maximisation tech nique occurring statistical analysis probabilistic functions markov chains 
annals mathematical statistics 

vector quantization efficient computation continuous density 
icassp proceedings volume pages 
enrico george doddington 
frame specific statistical features speaker independent speech recognition 
ieee transactions acoustics speech signal processing assp 
bourlard 
speech pattern discrimination multilayer perceptrons 
computer speech language 
herv bourlard christian 
links markov models multilayer perceptrons 
ieee transactions pattern analysis machine intelligence 
bibliography peter brown 
acoustic modelling problem automatic speech recognition 
phd thesis ibm thomas watson research center yorktown heights ny august 
gis yves normandin 
inter word coarticulation modelling mmie training improved connected digit recognition 
icassp proceedings volume pages 
chou 
lee 
juang 
minimum error rate training inter word context dependent acoustic model units speech recognition 
international conference spoken language processing pages 
mark lc ron cole 
speaker independent english alphabet recognition experiments set 
international conference spoken language processing volume pages november 
mark lc ronald cole 
spoken letter recognition 
richard lippmann john moody david touretzky editors advances neural information processing systems pages 
ron cole 
isolet spoken letter database 
technical report cse department computer science engineering oregon graduate institute science lc technology von neumann drive beaverton 
mail cole cse ogi edu march 
cox bridle 
simultaneous speaker normalisation utterance labelling bayesian neural net techniques 
icassp proceedings pages 
cox johnston 
performance humans isolated word speech recognition task 
proceedings institute acoustics volume pages 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal statist 
soc 
ser 
methodological 
paul dwyer 
applications matrix derivatives multivariate analysis 
journal american statistical association pages june 
scott fahlman 
empirical study learning speed back propagation 
technical report cmu cs carnegie mellon university september 
marco maltese 
language model acoustic model information probabilistic speech recognition 
icassp proceedings pages 
fischer 
inequality pif pi pif qi 

bibliography horacio franco michael cohen nelson morgan david rumelhart victor 
context dependent connectionist probability estimation hybrid hidden markov net speech recognition system 
computer speech language 
fritsch 
bucket box intersection algorithm fast approximative evaluation diagonal mixture gaussians 
icassp proceedings volume pages 
stuart geman elie bienenstock ren doursat 
neural networks bias variance dilemma 
neural computation 
herbert gish 
probabilistic approach understanding training neural network classifiers 
icassp proceedings pages 
gopalakrishnan nahamoo 
generalisation baum algorithm rational objective functions 
icassp proceedings pages 
gopalakrishnan dimitri arthur david nahamoo 
inequality rational functions applications statistical estimation problems 
ieee transactions acoustics speech signal processing january 
hampshire ii 
differential theory learning efficient statistical pattern recognition 
phd thesis carnegie mellon university 
hampshire ii vijaya kumar 
shooting search optimal strategy training connectionist pattern classifiers 
moody hanson lippmann editors advances neural information processing systems pages 
morgan kauffman 
hampshire ii vijaya kumar 
differential learning leads efficient neural network classifiers 
icassp proceedings volume pages 
hampshire ii pearlmutter 
equivalence proofs multilayer perceptron classifiers bayesian discriminant function 
touretzky elman sejnowski hinton editors proceeding connectionist models summer school pages 
john hampshire ii vijaya kumar 
error measures sub optimal training neural network pattern classifiers 
ieee international joint conference neural networks volume pages 
john hampshire ii alexander waibel 
novel objective function improved phoneme recognition time delay neural networks 
ieee transactions neural networks 
konig bourlard nelson morgan 
remap recursive estimation maximisation posteriori probabilities 
application transition connectionist bibliography speech recognition 
technical report tr international computer science institute icsi eecs department university california berkeley california march 
hochberg les foote harvey silverman 
hidden markov model neural networks training techniques connected speech recognition 
icassp proceedings pages 
holmes 
channel 
proc 
iee pt 

huang jack 
semi continuous hidden markov models speech recognition 
computer speech language 
huang jack 
unified modeling vector quantization hidden markov model semi continuous hidden markov models 
icassp proceedings pages 
mcdermott 
speaker independent large vocabulary word recognition lvq hmm hybrid algorithm 
icassp proceedings pages 
mcdermott 
hybrid speech recognition system hmms lvq trained codebook 
icassp proceedings pages 
jelinek mercer 
interpolated estimation markov source parameters sparse data 
gelsema kanal editors pattern recognition practice pages 
new york north holland 
fitzgerald 
optimization schemes neural networks 
cal report cued infeng tr cambridge university engineering department street cambridge cb pz october 
kapadia young 
mmi training continuous phoneme recognition timit database 
icassp proceedings volume pages 
hochberg robinson 
incorporating context dependent classes hybrid recurrent network hmm speech recognition system 
technical report cambridge infeng tr cambridge university engineering department trump street cambridge cb pz 
kai fu lee hsiao hon 
speaker independent phone recognition hidden markov models 
ieee transactions acoustics speech signal processing 
louis 
maximum likelihood estimation multivariate observations markov sources 
ieee transactions information theory september 
rabiner 
estimation hidden markov model parameters minimizing empirical error rate 
icassp proceedings pages 
bibliography ronny meir 
empirical risk minimization versus maximum likelihood estimation case study 
neural computation 
john miller rod goodman padhraic smyth 
loss functions minimise conditional expected values posterior probabilities 
ieee transactions information theory july 
john moody joachim 
principled architecture selection neural networks application corporate bond rating prediction 
john moody steve hanson richard lippmann editors advances neural information processing systems pages 
john moody 
effective number parameters analysis generalization regularization nonlinear learning systems 
john moody steve hanson richard lippmann editors advances neural information processing systems pages 
morgan bourlard 
continuous speech recognition multilayer perceptrons hidden markov models 
icassp proceedings pages 
nelson morgan herv bourlard 
neural networks statistical recognition continuous speech 
proceedings ieee may 
arthur 
hidden markov chains forward backward algorithm initial statistics 
ieee transactions acoustics speech signal processing april 
les harvey silverman bush 
neural networks maximum mutual information training maximum likelihood training 
icassp proceedings pages 
yves normandin 
hidden markov models maximum mutual information estimation speech recognition problem 
phd thesis department electrical engineering mcgill university montreal march 
yves normandin renato de mori 
high performance connected digit recognition maximum mutual information estimation 
ieee transactions acoustics speech signal processing april 
yves normandin 
mmie training large vocabulary continuous speech recognition 
international conference spoken language processing pages 
yves normandin salvatore 
improved mmie training algorithm speaker independent small vocabulary continuous speech recognition 
icassp proceedings pages 
bibliography stefan thorsten hermann ney 
fast likelihood computation methods continuous mixture densities large vocabulary speech recognition 
icassp proceedings volume pages 
padmanabhan bahl nahamoo de souza 
decision tree quantization feature space speech recogniser 
icassp proceedings volume pages 
thomas parsons 
voice speech processing 
mcgraw hill book 
douglas paul 
algorithms optimal search linearizing search stack decoder 
icassp proceedings pages 
steve renals nelson morgan herv bourlard 
probability estimation feed forward networks continuous speech recognition 
ieee workshop neural networks signal processing october 
steve renals nelson morgan herv bourlard michael cohen horacio franco 
connectionist probability estimators hmm speech recognition 
ieee transactions speech audio processing 
michael richard richard lippmann 
neural network classifiers estimate bayesian probabilities 
neural computation 
tony robinson frank fallside 
phoneme recognition timit database recurrent error propagation networks 
technical report cued infeng tr cam bridge university engineering department street cambridge cb pz march 
tony robinson mike hochberg steve renals 
ipa improved phone modelling recurrent neural networks 
icassp proceedings volume pages 
tony robinson mike hochberg steve renals 
recurrent neutral networks continuous speech recognition 
chin hui lee frank paliwal editors automatic speech speaker recognition advanced topics chapter 
kluwer academic publishers 
david rosen 
cross entropy vs squared error vs misclassification relationship loss functions 
technical report center biomedical modelling research university nevada reno may 
david rosen 
scoring forecaster mean resulting payoff distribution decision problems 
editor maximum entropy bayesian methods 
proceedings thirteenth international workshop 
kluwer dordrecht netherlands 

rt alphabetic database connex project 
technical report technical report rt bt laboratories bt laboratories heath near england 
bibliography charles 
decision estimation classification 
john wiley lc sons 
kari torkkola 
new ways lvq codebooks hidden markov models 
icassp proceedings volume pages 
joachim john moody 
selecting neural network architectures prediction risk application corporate bond rating prediction 
international conference artificial intelligence applications wall street 
webb david lowe 
comparison nonlinear optimisation strategies feed forward adaptive layered networks 
memorandum royal signals radar establishment procurement executive ministry defence july 
wilks 
mathematical statistics 
new york wiley 
woodland young 
benchmark darpa rm results htk portable hmm toolkit 
proceedings darpa workshop september 
philip woodland david cole 
optimising hidden markov models discrim output distributions 
icassp proceedings pages 
young 
general tying phoneme hmm speech recognisers 
icassp proceedings volume pages 
