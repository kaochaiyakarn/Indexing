implementing projection pursuit learning ying zhao christopher atkeson mit artificial intelligence laboratory department brain cognitive sciences ne technology square cambridge ma bbn com cga ai mit edu examines implementation projection pursuit regression ppr context machine learning neural networks 
propose parametric ppr direct training achieves improved training speed accuracy compared nonparametric ppr 
analysis simulations done heuristics choose initial projection directions 
comparison projection pursuit learning network hidden layer sigmoidal neural network shows grouping hidden units projection pursuit learning network useful 
learning robot arm inverse dynamics example problem 
projection pursuit nonparametric statistical technique find interesting low dimensional projections high dimensional data sets 
nonparametric curve fitting smoothing data analytic purposes 
projection pursuit learning network projection pursuit regression ppr friedman stuetzle possesses structure similar hidden layer sigmoidal neural network 
hidden layer sigmoidal feedforward neural network approximates function vector structure cr sigmoidal function direction parameters ii ay py function parameters 
projection pursuit learning network ridge function approximation network approximates function structure current address fawcett street bbn systems technologies cambridge ma layer neural network 
projection pursuit learning network 
oj holds meaning previously 
corresponding function parameters implicit ridge functions gj arbitrary smooth function learned data 
sigmoidal functions replaced general functions projection pursuit learning networks viewed generalization hidden layer sigmoidal feedforward neural networks 
projection pursuit exciting multivariate data analysis meth ods statistics potential reduce difficulties due curse dimen nonparametric statistics working low dimensional linear projections 
original projection pursuit approach called exploratory projection pursuit looked interesting low dimensional projections higher dimensional data numerically maxi projection index 
projection pursuit techniques originally proposed kruskal related ideas go back wright 
friedman tukey implemented early version projection pursuit 
fried man stuetzle extended ideas exploratory projection pursuit proposed projection pursuit regression friedman stuetzle projection pursuit classifica tion friedman projection pursuit density estimation friedman stuetzle schroeder 
friedman implemented ting algorithms perform projection pursuit regression effectively 
review discussion projection pursuit huber 
projection pursuit regression explored theoretically diaconis shahshahani 
studied necessary sufficient conditions functions exactly represented linear combination nonlinear ridge functions 
donoho johnstone demonstrated duality projection pursuit regression kernel regression dimensions 
projection pursuit re behaves functions angular smoothness oscillating slowly angle kernel regression behaves functions laplacian smooth ness oscillations averaging locally 
hall showed explicit formulae bias variance approximation estimator 
chen showed ap conditions rate convergence projection pursuit regression independent dimensionality projection pursuit introduced context learning networks barron barron 
intrator applied exploratory projection pursuit feature extraction speech recognition 
hwang studied dimensional examples compare projection pursuit sion hidden layer sigmoidal neural networks 
moody compared generalization properties different network representations including projection pursuit network 
comparisons projection pursuit sigmoidal neural networks cherkassky 
discusses techniques implementing projection pursuit 
goal investigate implementation issues projection pursuit re 
apply projection pursuit learning networks learn inverse dynamics robot arms 
robot arm inverse dynamics provides useful test functions dynamics complex know true functions idealized case 
noise free data generated idealized model dynamics 
convincing examples ppr works higher dimensional spaces 
robot arm example explore performance ppr high dimensional space 
joint robot example inputs position velocity acceleration joint dimensional input space 
section reviews traditional training algorithm implementation application robot arm problem 
section proposes new parametric representation direct method train 
method replace traditional ppr achieve improved training speed accuracy 
section discusses heuristics choose initial projection directions 
section compares sigmoidal neural network show grouping hidden units useful 
learning joint arm inverse dynamics backpropagation training neural network backfitting friedman stuetzle train projection pursuit learning network 
learns directions node functions iterative search procedure data points xi xi 
backfitting trains hidden units time backpropagation 
algorithm described follows 

initialization gi 
optimize hidden unit fixed hidden units 
optimize gj fixed oj rj xi yi lj gl xi 
find dimensional function gj oj 
xi best fits rj xi particular projection direction optimize fixed gj min oj min rj xi gj oj 
xi 
go convergence stops decreasing 
smoothers optimization methods built blocks algorithm 
smoothers kernel smoothers spline smoothers friedman stuetzle 
optimization done levenberg marquardt conjugate gradient optimization procedures 
interested training robot arms follow trajectories 
example robot arm able generate correct torque command receiving position velocity acceleration command 
shows idealized joint arm degrees freedom 
link masses ml moments inertia lengths locations center mass relative proximal joint cx cx cy cy gravity joint torques rl idealized joint arm model functions desired joint position joint velocities tl joint accelerations 
dynamics 
lg lg cos sin sin 


cos sin function example function learned projection pursuit regression backfitting 
random training points random test points taken uniformly tl schematic diagram backfitting 
learns node functions direction parameters alternatively node node 
fits projection input data particular direction dimensional smoother 
joint arm 
table joint arm dynamics training errors test errors 
methods conjugate global local levenberg rms errors gradient random search marquardt training error test error backfitting steps smooths table joint arm dynamics projection pursuit directions fl 
torques rl learned separately 
data scaled 
normalized rms error training set test set defined combine torques ei tli tli ei li table shows results conjugate gradient global local random search huber levenberg marquardt optimization methods projection pursuit context 
shows learned node functions gj zl levenberg marquardt optimization cubic spline smoother wahba 
methods give comparable results 
table show directions learned backfitting zl projection pursuit learning learn dynamics functions reasonably 
model detect irrelevant variables 
example irrelevant predicting zl direction oo projection direction projection oo oo direction projection direction projection oo direction projection dimensional node functions learned oo direction projection direction projection direction projection direction projection dimensional node functions learned 
table joint arm dynamics projection pursuit directions irrelevant predicting 
reflected directions learned model corresponding direction components equal zero see table 
surprising random search optimization procedure give results problem 
qualitative criterions manually selecting number hidden units 
assume underlying function smooth dimension reduction done original variables number hidden units number dimensions original variables 
data points provided high dimensions model selected simple possible order generalize 
manually experimented different numbers hidden units chose network architecture best performance 
number hidden units obtained automatically cross validation 
choice smoothing parameters backfitting algorithm critical 
cross validation friedman 
su basically running line smoother simplified cross validation operations avoid overfitting underfitting 
simulations comparison cu bic spline smoother 
versions uses manual bandwidth selection mbs uses automatic bandwidth selection abs 
columns table show results learning joint arm dynamics example 
performance worse cubic spline smoother column table computationally cheaper cubic spline smoother 
surprising automatic bandwidth selection cross validation leads worse performance 
table training errors test errors cubic spline smoother mbs abs heuristic training errors test errors smooths heuristic choosing smoothing parameter large smoothing pa rameter reduce gradually backfitting 
smoothing parameter proportional noise signal ratio dimensional scatterplot 
initial stage backfitting noise signal ratios dimensional scatterplots large structure data emerged 
backfitting goes noise signal ratio gets smaller converges zero noise free data small number noisy data 
av training error empirical formula choosing right smoothness backfitting initialize 
shrinking term training error avoids overfitting final backfitting stages 
column table shows result selecting span heuristic converges consistent result manual selection 
cubic spline smoother unbiased underlying function linear 
hoped higher order spline smoother reduce bias function curved 
result trained spline smoother training error test error better result cubic spline smoother 
higher order spline smoothers stable lower order spline smoothers 
conclude increasing order splines improve performance ppr example 
parametric ppr pointed smoothers optimizers important blocks traditional projection pursuit regression 
directly affect speed result learning process 
choice smoothing parameter smoother delicate part process 
complications choosing smoothing parameter intrinsic problem backfitting algorithm ppr 
example cubic spline smoother backfitting theoretically converge solution min yi gj xi dx 
smoothing parameter 
shown solving equivalent minimizing ak symmetric nonnegative definite matrix 
solution sy tk buja 
corresponding smoother backfitting procedure 
smoother shrinking sense sy 
backfitting converges strictly shrinking buja 
rate convergence inversely proportional second largest eigenvalue smoother timid tmi second smallest eigenvalue large small chosen backfitting converges quickly slowly 
large small chosen estimation function overfits data 
training test error large test error larger training error overfitting 
generalization cases 
optimal provided estimated function biased noise free data 
joint arm dynamics example data noise free backfitting training slow test error achieves 
true smoothers corresponding smoothing parameters 
consequently conclude backfitting algorithm converges slowly large bias noise free data provided 
propose direct method replace backfitting solving problem 
observe nonlinear directions fixed node functions fy evaluated data points directly backfitting 
suppose cubic spline smoother directions 
smoother direction sj equation backfitting sl fl direct unique solution buja inverse exists 
norm 
fl sls sl common sufficient condition ll ll general directions equation backfitting 


equation nn nn system equations direct solution 
large solving practical 
observe process simplified 
projection direction dimen sional smoother leads moving average solution dimensional node function ti different weight functions correspond different smoothers 
tl projections data points xl direction 
smoother working dimensional scatterplot necessary project dimensional data direction order obtain structure direction 
possible small number parameters parameterize function direction 
approximation achieved cig ti data projections approximately range scaling dimension ci equispaced knots ti simplifies procedure choosing partial data projected direction 
propose parametric representation projection pursuit re oj ti direction parameters ti fixed equispaced knots directions dimensional weight functions usually take symmetric form tl 
ideal weight function proved silverman shape close weight function cubic spline smoother 
suppose ti local density weight function cubic spline asymptotic form silverman large kernel function exp sin local bandwidth satisfies graph 
shows hidden unit projection pursuit network compared sigmoid hidden unit neural network 
kernel function 
comparison hidden unit functions projection pursuit network sigmoidal network dimensions 
kernel node function 
node function neural network 
multiple units aligned direction 
multiple units aligned direction 
units different directions 
units different directions 
general data convolved fixed width kernel function scaling parameter varies sample 
done eliminating condition notice dependence local bandwidth density low negative power parametric representation np linear parameters cij nd nonlinear direc tion parameters 
hidden layer neural network node function shaped 
grouped nodes directions 
fixed location parameters ti properties original idea projection pursuit 
trained directly sigmoidal neural network 
show train representation efficiently 
continue problem predicting joint arm dynamics example 
number directions torque torque 
take direction 
tried search procedure alternated linear nonlinear param eters 
essentially backfitting strategy 
direct search parameters better initial guess parameters provided 
propose direct algorithm train algorithm 
treat linear nonlinear parameters set parameters optimize simultaneously nonlinear squares method nl sol dennis converges example function evaluations 

linear squares fit fixed step 
adaptive nonlinear squares algorithm maintains secant approximation second order part squares hessian adaptively decides approximation 

oj step cij step initial parameters go iterations 
simulations show algorithm works 
execution step obtains better cij changes initial guess local search step stuck 
shows combined test errors torques algorithm random initial direction parameters 
number iterations test errors vs iteration torque 
final nonlinear directions obtained backfitting initial nonlinear direction parameters direct training final combined test error torques iterations 
achieve better accuracy training parametric representation projection pursuit direct search nonparametric ppr backfitting 
choose directions section propose heuristics choose initial directions measurements derivatives function 
duan li li proposed sliced inverse regression principal hessian directions estimate projection directions projection pursuit regression 
hessian approach li similar described 
theoretically smooth function variables best projection directions 
suppose function variables exactly represented sum ridge functions independent directions ii gj oj 
assume 
singular 
changing variable represented terms new variables xx dx oy xj yj xx ox 
diagonal matrix 
diagonal matrix solve exactly recover directions exactly 
unique singular value decomposition svd eo usv orthogonal matrices diagonal matrix 
vs obtained singular vectors sin values matrix 
equation see ae furthermore ute left side equation singular value decomposition matrix rd singular vector matrix rd algorithm obtain known 
algorithm compute 
svd decomposition vs 
svd decomposition fu 
compute 
directions obtained normalizing column 
table show directions obtained algorithm torque torque derivatives symbolically known functions numerically data 
dimensionality reduction directions torque torque obtained 
step unique exchanges columns 
caused singular values step da step 
example il table directions rl algorithm 
table directions algorithm dtd 
permutation matrices indices il ik jl jl respectively 
singular vectors unique respectively 
suppose singular vectors vp step 
pts pa dp singular vectors da step su step 
permutation matrix final direction set unchanged permutation step 
nonsingular matrix singular multiplication nonsingular matrix preserve rank 
rank rank rank 
rank rank rank 
nonzero columns corresponds nonzero directions 
dimensionality reduction step throwing away columns small norms normalizing columns acquire see number effective ridge functions rank best directions eigenvectors corresponding nonzero eigenvalues suggests directions orthogonal obtained eigenvectors corresponding nonzero eigenvalues matrix directions shown table 
test errors directions torque torque worse obtained directions obtained algorithm 
approach orthogonal matrix respect metric 
vsv la words orthogonal matrix 
analysis shows directions related function derivatives 
practically estimation derivatives underlying function difficult estimating function method difficult implement 
notice backfitting table directions rl eigenvectors table directions eigenvectors choose initial directions time may overcome difficulty searching large unknown parameter space trapped local minima 
complexity smoothers algorithm essentially determines speed accuracy algorithm simple smoother backfitting obtain approximate estimates directions 
direct method achieve better accuracy 
comparison hidden layer sig neural network described previously parametric projection pursuit learning network structure similar hidden layer sigmoidal neural network 
projection pursuit architecture corresponds grouping hidden units traditional hidden layer sigmoidal neural network small number distinct directions 
grouping important 
analyze traditional hidden layer sig neural network 
proposed classification networks special function layer traditional hidden layer sigmoidal neural network partition input space cells 
layer groups cells form decision regions makhoul 
example node hidden layer network specifies planes divide dimensional input space cells shown 
accuracy resolution decision regions completely spec ified size density cells determined number placement layer hyperplanes input space 
number cells formed hyperplanes space obeys recursion makhoul layer 
obtain divisions dimension ud hidden units layer 
depending accuracy needed representing regression regions interest help estimate needed number nodes layer 
hyperplanes partition dimensional input space cells 
sets parallel hyperplanes partition dimensional input space cells 
number parameters network approximately pd parametric projection pursuit learning network 
suppose hyperplanes grouped directions 
corresponds having sets parallel hyperplanes input space 
assume parallel hyperplanes direction distributed evenly direction 
see cells formed sets hyperplanes 
recursion holds sc shown asymptotic approximation easy see regular dimensional grid 
words grouping hidden units affect number cells input space 
number parameters network reduced number cells partitioned hyperplanes determines training errors complexity network determines test errors parametric projection pursuit learning network achieves training accuracy fewer parameters hidden layer sigmoidal neural network 
easily trained especially second order method better generalization properties hidden layer sigmoidal neural network 
test error learning torque joint arm hidden layer sigmoidal neural network sigmoidal hidden units 
test error projection pursuit learning network number sigmoids grouped directions 
number parameters sigmoidal net projection pursuit net respectively 
projection pursuit net fewer parameters easier train better generalization test set 
interesting note idea counting number cells formed hy dimensional input space explain sigmoidal neural network compact representation compared learning networks 
ud means number hidden units net grows linearly dimension accuracy determined division dimension fixed 
curse dimensionality reduced sense complexity network grows exponentially dimensionality problem 
hand example number centers radial basis function approach grows exponentially dimension 
projection pursuit learning network learn simple arm dynamics reasonably 
parametric projection pursuit regression direct training method replace non parametric projection pursuit regression achieve better accuracy training speed 
para metric projection pursuit advantage achieving better accuracy fewer param eters hidden layer sigmoidal neural network 
direction parameters related derivatives underlying functions obtained backfitting 
acknowledgments describes research done department brain cognitive sciences center biological computational learning artificial intelligence laboratory massachusetts institute technology 
support provided air force office scientific research afosr siemens atr human information processing research laboratories 
support cga provided national science foundation presidential young investigator award 
barron barron 
statistical learning networks unifying view 
computing science statistics proceedings th symposium interface 
ed wegman editor american statistical association washington 
buja hastie tibshirani 
linear smoothers additive models discussion 
annals statistics 
chen 
estimation projection pursuit type regression model 
annals statistics 
cherkassky lee lari 
self organization networks regression efficient implementation comparative evaluation 
international joint conference neural networks 
vol 
pp 

seattle wa july 
dennis gay 
adaptive nonlinear squares 
cm transactions mathematical software vol 

diaconis shahshahani 
nonlinear functions linear combina tions 
siam journal scientific statistical computing 
donoho johnstone 
projection approximation du ality kernel methods 
annals statistics 
duan li 
slicing regression link free regression method annals statistics 
vol 
pp 

friedman 
stuetzle 
projection pursuit regression 
journal american statistical association 
friedman 
tukey 
projection pursuit algorithm ex data analysis 
ieee transactions computers 
friedman 
ue schroeder 
projection pursuit density estimation 
journal american statistical association 
friedman 

variable span smoother 
department statistics 
stanford university report lcm 
friedman 

smart user guide 
department statistics stanford uni versity report lcm 
friedman 

classification multiple response regression pro pursuit 
dept statist stanford university report lcm 
hall 
projection pursuit regression 
annals statistics 
huber 
projection pursuit discussion 
annals statistics 
huber 
algorithms projection pursuit technical report de mathematics massachusetts institute technology 
hwang lay martin regression modeling back propagation projection pursuit learning unpublished manuscript 
hwang li martin 
projection pursuit learning networks regression 
engineering application artifical intelligence vol 
pp 

hwang li martin com parison projection pursuit neural network regression modeling moody hanson 
lippmann ed 
advances neural information processing systems 
san mateo ca morgan kaufmann 
hwang li martin 
learning parsimony projection pursuit back propagation networks 
conference record fifth asilomar conference signals systems computers vol 
nov pacific grove california 
intrator 
neural network feature extraction 
touretzky ed 
advances neural information processing systems 
san mateo ca morgan kaufmann 
intrator 
feature extraction unsupervised neural network 
touretzky sejnowski hinton editors proceedings con models summer school pp 

morgan kaufmann san mateo ca 
intrator 
exploratory feature extraction speech signals 
mann moody touretzky ed 
advances neural information processing systems 
san mateo ca morgan kaufmann 
intrator 
feature extraction unsupervised neural network 
neural computation pp 
kruskal 
practical method helps uncover structure set multivariate observations finding linear transformation optimizes new index condensation statistical computation 
milton nelder eds 
academic new york 
kruskal 
linear transformation multivariate data reveal clustering 
multidimensional scaling theory applications behavioral sciences ory 
seminar press new york london 
li 
sliced inverse regression dimension reduction 
journal statistical association vol 
pp 
li 
principal hessian directions data visualization dimension re duction application stein lemma 
journal american statistical asso ciation 
vol 
pp 
martin hwang 
projection pursuit learning networks regression proc 
nd interna tional ieee conference tools artificial intelligence november 
makhoul schwartz partitioning capabilities layer neural networks ieee transactions signal processing pp 
gallant 
estimating lyapunov exponent chaotic system nonparametric regression 
journal american statistical association vol 

moody 
networks learned unit response functions moody hanson 
lippmann ed 
advances neural information processing systems 
san mateo ca morgan kaufmann 

learning forgetting perception action projection pursuit density adaptive approach 
dissertation 
computer information science department university pennsylvania 

exploring regression structure nonparametric functional estimation 
tr sloan school management massachusetts institute technology 
silverman 
aspects spline smoothing approach non parametric regression curve fitting journal royal statistical society pp 

wright 
numerical classification applied certain ja 
mathematical geology 

numerical classification 

plenum new york 
wahba 
spline models observational data 
cbms nsf regional conference series applied mathematics 

fortran package cross spline smoothing differentiation advances engineering software 

