noisy channel model document compression document compression system uses hierarchical noisy channel model text production 
compression system automatically derives syntactic structure sentence discourse structure text input 
system uses statistical hierarchical model text production order drop non important syntactic discourse constituents generate coherent grammatical document compressions arbitrary length 
system outperforms baseline sentence compression system operates simplifying sequentially sentences text 
results support claim discourse knowledge plays important role document summarization 
single document summarization systems proposed date fall classes extractive summarizers simply select user important sentences text see mani maybury marcu mani comprehensive overviews methods algorithms accomplish 
headline generators noisy channel probabilistic systems trained large corpora headline text pairs banko hal iii daniel marcu information sciences institute university southern california admiralty way suite marina del rey ca marcu isi edu berger mittal 
systems produce short sequences words indicative content text input 
sentence simplification systems mahesh carroll grefenstette jing knight marcu capable compressing long sentences deleting unimportant words phrases :10.1.1.42.2394
extraction summarizers produce outputs contain non important sentence fragments 
example hypothetical extractive summary text shown table compacted deleting clause win 
headline summaries shown table usually indicative text content informative grammatical coherent 
repeatedly applying sentence simplification algorithm sentence time compress text outputs generated way incoherent contain unimportant information 
summarizing text sentences dropped altogether 
ideally build systems strengths classes approaches 
document compression entry table shows grammatical coherent summary text generated hypothetical document compression system preserves important information text deleting sentences phrases words subsidiary main message text 
obviously generating coherent grammatical summaries produced hypothetical document compression system table trivial conflicting type hypothetical output output output output summarizer contains coherent grammatical important info extractive john doe secured vote summarizer democrats constituency win 
support ground 
headline mayor vote constituency generator sentence mayor looking re election 
john doe simplifier secured vote democrats constituency 
ground 
document john doe secured vote democrats 
compressor ground 
table hypothetical outputs generated various types summarizers 
goals deletion certain sentences may result incoherence information loss 
deletion certain words phrases may lead ungrammaticality information loss 
mayor looking re election 
john doe secured vote democrats constituency win 
support grounds 
document compression system uses hierarchical models discourse syntax order simultaneously manage conflicting goals 
compression system automatically derives syntactic structure sentence discourse structure text input 
system uses statistical hierarchical model text production order drop non important syntactic discourse units generate coherent grammatical document compressions arbitrary length 
system outperforms baseline sentence compression system operates simplifying sequentially sentences text 
document compression document compression task conceptually simple 
document goal produce new document dropping words order achieve goal number systems outputs extractive summarizers repair improve coherence duc duc 
unfortunately flexible produce shot summaries simultaneously coherent grammatical 
extent noisy channel model proposed knight marcu 
system compressed sentences dropping syntactic constituents applied entire documents sentence basis 
discussed section adequate resulting summary may contain compressed sentences irrelevant 
order extend knight marcu approach sentence level need glue sentences tree structure similar sentence level 
rhetorical structure theory rst mann thompson provides glue tree depicts rst structure text 
rst discourse structures nonbinary trees leaves correspond elementary discourse units internal nodes correspond contiguous text spans 
internal node rst tree characterized rhetorical relation 
example sentence text provides background information interpreting information sentences contrast relation see 
relation holds adjacent non overlapping text spans called nucleus satellite 
exceptions rule relations list contrast multinuclear 
distinction nuclei satellites comes empirical observation nucleus expresses essential writer purpose satellite 
system able analyze discourse structure document syntactic structure sentences 
compresses document dropping syntactic discourse constituents 
noisy channel model document want find summary text maximizes bayes rule flip maximizing left modelling probability distributions probability document summary probability summary 
assume discourse structure document syntactic structures 
intuitive way thinking application bayes rule noisy channel model start summary add noise yielding longer document noise added model consists words phrases discourse units 
instance document john doe secured vote democrats add words word generate john doe secured vote democrats choose add entire syntactic constituent instance prepositional phrase generate john doe secured vote democrats constituency examples sentence expansion previously knight marcu 
system ability expand core message adding discourse constituents 
instance decide add discourse constituent original summary john doe secured vote democrats contrasting information summary uncertainty regarding support governor yielding text john doe secured vote democrats 
support governor ground noisy channel application parts account build complete document compression system channel model source model decoder 
describe 
source model assigns string probability probability summary english 
ideally source model ungrammatical sentences documents containing juxtaposed sentences 
channel model assigns document summary pair probability models extent expansion instance mayor looking re election mayor looking re election 
secure vote democrats major looking re election 
sharks sharp teeth expect higher expands elaboration shifts different topic yielding incoherent text 
decoder searches possible summaries document summary maximizes posterior probability parts described 
source model job source model assign score compression independent original document 
source model measure english summary independent compression 
currently bigram measure quality trigram scores tested failed difference combined non lexicalized context free syntactic probabilities context free discourse probabilities giving better lexical ized context free grammar possible decoder 
channel model channel model allowed add syntactic constituents stochastic operation called constituent expand discourse units stochastic operation called edu expand 
operations performed combined discourse syntax tree called ds tree 
ds tree text shown 
suppose start summary mayor looking re election constituent top npb dt nn mayor vp vbz advp rb vp vbg pp looking npb nn punc 
re election john doe secured vote democrats constituency win 
support ground 
discourse full syntax partial tree text 
expand operation insert syntactic constituent year syntactic tree constituent expand operation add single words instance word added looking yielding mayor looking re election probability inserting word syntactic structure node inserted 
knight marcu describe detail noisy channel model explains short sentences expanded longer ones inserting expanding syntactic constituents words 
constituent expand stochastic operation simply knight marcu model focus 
refer reader knight marcu details :10.1.1.42.2394
addition adding syntactic constituents system able add discourse units 
consider summary john doe secured vote democrats sequence discourse expansions expand summary reach original text 
complete discourse expansion process occur starting initial summary generate original document shown 
follow sequence steps required generate original text summary operation project increase depth tree adding intermediate nuc span node 
projection adds factor nuc span nuc span nuc span probability sequence operations shown arrow 
able perform second operation expand expand core message contained adding satellite evaluates information expansion adds probability performing expansion called discourse expansion probabilities example discourse expansion probability written nuc span nuc span sat eval nuc span nuc span reflects probability adding evaluation satellite nuclear span 
rest shows remaining steps produce original document step labeled appropriate probability factors 
probability entire expansion product listed probabilities combined appropriate probabilities syntax side things 
order produce final score document summary pair multiply expansion probabilities path leading estimating parameters discourse models rst corpus wall street journal articles penn treebank obtained ldc 
documents corpus range size words average words document 
document paired discourse structure john doe secured vote democrats constituency mayor looking re election 
john doe secured vote democrats constituency nuc span nuc span nuc span john doe secured vote democrats constituency win 
support ground 
nuc span nuc span sat evaluation nuc span nuc span root sat background nuc span root nuc span john doe secured vote democrats constituency john doe secured vote democrats constituency win 
win 
support nuc span nuc contrast nuc span nuc contrast nuc span nuc contrast nuc contrast sat nuc span nuc contrast nuc span ground 
john doe secured vote democrats constituency john doe secured vote democrats constituency win 
win 
sequence discourse expansions text probability factors 
ally built style rst 
see carlson details concerning corpus annotation process 
corpus able estimate parameters discourse pcfg standard maximum likelihood methods 
furthermore document corpus paired extractive summaries edu level 
human annotators asked important suppose example annotators marked second fifth starred ones 
stars propagated discourse unit descendent considered important considered important 
annotations deduce compress nuc contrast children nuc span sat evaluation drop evaluation satellite 
similarly compress nuc contrast children sat condition nuc span dropping discourse constituent 
compress root deriving sat background nuc span dropping sat background constituent 
keep counts examples collected normalize get discourse expansion probabilities 
decoder goal decoder combine get vast number potential compressions large ds tree nuc span nuc contrast nuc contrast nuc span nuc contrast ground 
efficiently pack shared forest structure described detail knight marcu 
entry shared forest structure associated probabilities source syntax pcfg source discourse pcfg expansion template probabilities described section 
generated forest representing possible compressions original document want extract best best trees account expansion probabilities channel model bigram syntax discourse pcfg probabilities source model 
generic extractor built langkilde 
purposes extractor selects trees best combination lm expansion scores performing exhaustive search possible summaries 
returns list trees possible length 
system system developed works pipelined fashion shown 
step pipeline generate discourse structure 
decision discourse parser described marcu discourse structure send edu syn discourse parser achieves score edu identification identifying hierarchical spans nuclearity identification relation tagging 
input document discourse syntax parser parser decoder length chooser forest generator output summary pipeline system components 
tactic parser collins 
syntax trees merged discourse tree forest generator create ds tree similar shown 
ds tree generate forest subsumes possible compressions 
forest passed forest ranking system decoder langkilde 
decoder gives list possible compressions possible length 
example compressions text shown respective log probabilities 
order choose best compression possible length rely log probabilities lest system choose shortest possible compression 
order compensate normalize length 
practice simply dividing log probability length compression insufficient longer documents 
experimentally reasonable metric compression length divide log probability job length chooser enabled choose single compression document evaluation 
compression chosen length selector italicized shortest results testing began sets data 
set drawn wall street journal wsj portion penn treebank consists documents containing words 
second set drawn collection stu tends case short documents compressions get sufficiently long length normalization effect 
dent compositions consists documents containing words 
call set mitre corpus hirschman 
liked run evaluations longer documents 
unfortunately forests generated relatively small documents huge 
exponential number summaries generated text decoder runs memory longer documents selected shorter original documents 
wsj mitre data evaluation wanted see performance system varies text genre 
mitre data consists short sentences average document length mitre sentences quite constrast typically long sentences wall street journal articles average document length wsj sentences 
purpose comparison mitre data compressed systems random drops random words word chance dropped baseline 
hand hand compressions done human 
concat sentence compressed individually results concatenated knight marcu system comparison 
edu system described 
sent syntactic parsers tend parsing just clauses system merges leaves discourse tree sentence proceeds described 
wall street journal data evaluated systems additions 
correct discourse trees known data thought wise test systems human built discourse trees automatically derived ones 
systems pd edu edu perfect discourse trees available rst corpus carlson 
theory text words possible compressions 
len log prob best compression mayor looking 
mayor looking win 
mayor looking support ground 
mayor looking support ground 
mayor looking re election support ground 
mayor looking win 
support ground 
pd sent sent perfect discourse trees 
human evaluators rated systems metrics 
evaluators grammaticality coherence third separately summary quality 
grammaticality judgment english compressions coherence included compression flowed instance anaphors lacking antecedent lower coherence 
summary quality hand judgment compression retained meaning original document 
measure rated scale worst best 
draw evaluation results shown table average compression rate cmp length compressed document divided original length 
clear genre influences results 
mitre data contained short sentences syntax discourse parsers fewer errors allowed better compressions generated 
mitre corpus compressions obtained starting discourse trees built sentence level better compressions obtained starting discourse trees built edu level 
wsj corpus compression obtained starting discourse trees built sentence level grammatical coherent compressions obtained starting discourse trees built edu level 
choosing manner discourse syntactic representations texts mixed influenced genre texts interested compress 
run system mitre data perfect discourse trees hand built discourse trees corpus 
possible compressions text 
wsj mitre cmp grm coh qual cmp grm coh qual random concat edu sent pd edu pd sent hand table evaluation results compressions obtained starting perfectly derived discourse trees indicate perfect discourse structures help greatly improving coherence grammaticality generated summaries 
surprising see summary quality affected negatively perfect discourse structures statistically significant 
believe happened text fragments summarized extracted longer documents 
discourse structures built specifically short text snippets different 
component designed handle cohesion expected compressions contain dangling 
systems outperformed random baseline concat systems empirically show discourse important role document summarization 
performed tests results wall street journal data differences score concat sent systems grammaticality coherence statistically significant level difference score summary quality 
mitre data differences score concat sent systems grammaticality summary quality statistically significant level difference score coherence 
score differences grammaticality coherence summary quality systems baselines statistically significant level 
results table assessed inspecting compressions show spite success far away human performance levels 
error system dropping complements dropped phrase re election complement looking 
currently experimenting lexicalized models syntax prevent compression system dropping required verb arguments 
consider methods scaling decoder handling documents realistic length 
partially supported darpa ito nsf iis usc dean fellowship hal iii 
kevin knight discussions related project 
michele banko mittal michael witbrock 

headline generation statistical translation 
proceedings th annual meeting association computational linguistics acl pages hong kong october 
adam berger mittal 

query relevant summarization faqs 
proceedings th annual meeting association computational linguistics acl pages hong kong october 
lynn carlson daniel marcu mary ellen 

building discourse tagged corpus framework rhetorical structure theory 
proceedings nd workshop discourse dialogue eurospeech aalborg denmark september 
john carroll minnen canning devlin john tait 

practical simplification english newspaper text assist aphasic readers 
proceedings aaai workshop integrating artificial intelligence assistive technology 
doran srinivas bangalore 

motivations methods text simplification 
proceedings sixteenth international conference computational linguistics coling copenhagen denmark 
michael collins 

generative lexicalized models statistical parsing 
proceedings th annual meeting association computational linguistics acl pages madrid spain july 
proceedings document understanding conference duc new orleans la september 
proceedings second document understanding conference duc philadelphia pa july 
gregory grefenstette 

producing intelligent telegraphic text reduction provide audio scanning service blind 
working notes aaai spring symposium intelligent text summarization pages stanford university ca march 
hirschman light breck burger 

deep read reading comprehension system 
proceedings th annual meeting association computational linguistics 
jing 

sentence reduction automatic text summarization 
proceedings annual meeting north american chapter association computational linguistics naacl pages seattle wa 
kevin knight daniel marcu 

statistics summarization step sentence compression 
th national conference artificial intelligence aaai pages austin tx july th august rd 
irene langkilde 

forest statistical sentence generation 
proceedings st annual meeting north american chapter association computational linguistics seattle washington april may 
mahesh 

hypertext summary extraction fast document browsing 
proceedings aaai spring symposium natural language processing world wide web pages 
inderjeet mani mark maybury editors 

advances automatic text summarization 
mit press 
inderjeet mani 

automatic summarization 
william mann sandra thompson 

rhetorical structure theory functional theory text organization 
text 
daniel marcu 

theory practice discourse parsing summarization 
mit press cambridge massachusetts 
