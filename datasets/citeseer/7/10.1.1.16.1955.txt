new ranking algorithms parsing tagging kernels discrete structures voted perceptron michael nigel yat labs research florham park new jersey 
research att com ave building palo alto ca 
cs ucsc edu id keywords parsing tagging machine learning kernels perceptron algorithm contact author michael collins research att com consideration conferences specify 

introduces new learning algorithms natural language processing perceptron algorithm 
show algorithms efficiently applied exponential sized representations parse trees subtrees dop representation described bod representation tracking sub fragments tagged sentence 
give experimental results showing significant improvements tasks parsing wall street journal text named entity extraction web data 
new ranking algorithms parsing tagging kernels discrete structures voted perceptron id introduces new learning algorithms natural language processing perceptron algorithm 
show algorithms efficiently applied exponential sized representations parse trees subtrees dop representation described bod representation tracking sub fragments tagged sentence 
give experimental results showing significant improvements tasks parsing wall street journal text named entity extraction web data 
perceptron algorithm oldest algorithms machine learning going back rosenblatt 
incredibly simple algorithm implement shown competitive learning methods support vector machines see freund schapire application image classification example 
describe perceptron voted perceptron algorithms parsing tagging problems 
crucially algorithms efficiently applied exponential sized representations parse trees subtrees dop representation described bod representation tracking tagged sentence 
paradoxical able efficiently learn apply model exponential number features 
key algorithms kernel trick cristianini shawe taylor discuss kernel methods length 
describe see goodman efficient algorithm dop model discuss section 
inner product feature vectors representations calculated efficiently dynamic programming algorithms 
leads polynomial time algorithms training applying perceptron 
kernels describe related kernels discrete structures haussler lodhi 
previous collins duffy showed improvements pcfg parsing atis task :10.1.1.19.8980
show method scales far complex domains 
parsing wall street journal text method gives relative reduction error rate model collins 
second domain detecting named entity boundaries web data show relative error reduction improvement measure state art model maximum entropy tagger 
result derived new kernel tagged sequences described 
results rely new approach incorporates log probability baseline model addition features 
feature vector representations parse trees tagged sequences focuses task choosing correct parse tag sequence sentence group candidates sentence 
candidates enumerated number methods 
experiments top candidates baseline probabilistic model model collins parsing maximum entropy tagger named entity recognition 
choice representation central polynomial number training examples size trees sentences training test data 
features evidence choosing candidates 
function denote dimensional feature vector represents tree tagged sequence possibilities 
obvious example parse trees component rule contextfree grammar underlies trees 
representation stochastic contextfree grammars 
feature vector tracks counts rules tree encoding sufficient statistics scfg 
representation structures inner product structures defined delta idea inner products feature vectors central learning algorithms support vector machines central ideas 
intuitively inner product similarity measure objects structures similar feature vectors high values delta 
formally observed algorithms implemented inner products training examples direct access feature vectors 
see crucial efficiency learning certain representations 
support vector machine literature call function objects kernel shown inner product feature space algorithms notation section formalizes idea linear models parsing tagging 
method related boosting approach ranking problems freund markov random field methods johnson boosting approaches parsing collins 
consider set ffl training data set example input output pairs 
parsing training examples fs sentence correct tree sentence 
ffl assume way enumerating set candidates particular sentence 
ij denote th candidate th sentence training data fx denote set candidates ffl loss generality take correct candidate 
ffl candidate ij represented feature vector ij space parameters model vector output model training test example argmax delta 
key question having defined representation set parameters discuss method setting weights perceptron algorithm section 
perceptron algorithm shows perceptron algorithm applied ranking task 
method assumes training set described section representation parse trees 
algorithm maintains parameter vector initially set zeros 
algorithm pass training set updating parameter vector mistake example 
parameter vector update simple involving adding difference offending examples representations gamma ij 
intuitively update effect increasing parameter values features correct tree parameter values features competitor 
see cristianini shawe taylor discussion perceptron algorithm including overview various theorems justifying way setting parameters 
briefly perceptron algorithm guaranteed find hyperplane correctly classifies training points hyperplane exists data separable 
number mis find hyperplane algorithm run training set repeatedly mistakes 
algorithm includes just single pass training set 
define define delta 
ff delta gamma ij delta initialization set parameters initialization set dual parameters ff argmax ij argmax ij gamma ij ff ij ff ij output test sentence output test sentence argmax 
argmax perceptron algorithm ranking problems 
algorithm dual form 
takes low providing data separable large margin translates guarantees method generalizes test examples 
freund schapire give theorems showing voted perceptron variant described generalizes non separable data 
algorithm dual form shows equivalent algorithm perceptron algorithm call dual form perceptron 
new algorithm store parameter vector storing set dual parameters ff score parse defined dual parameters ff delta gamma ij delta contrast delta score original algorithm 
spite differences algorithms give identical results training test examples see verified ff gamma ij training 
important difference algorithms lies analysis computational complexity 
say size training set take dimensionality parameter vector algorithm takes td time 
vectors sparse taken number non zero elements assuming takes time add feature vectors non zero elements take inner products 
follows calculated member training set calculation involves time 
say time taken compute inner product examples running time algorithm 
follows algorithm number non zero dual parameters bounded calculation takes nk time 
dual algorithm efficient cases nk 
case naively expected time calculate inner product delta vectors 
turns highdimensional representations inner product calculated better time making dual form algorithm efficient original algorithm 
voted perceptron freund schapire describe refinement perceptron algorithm voted perceptron 
give theory suggests voted perceptron preferable cases noisy data 
training phase dual form algorithm unchanged change comes method applied test examples 
algorithm considered build series hypotheses defined ff delta gamma ij delta scoring function algorithm trained just training exam phi phi phi np john vp phi phi saw np phi phi man np phi phi man np man np phi phi np phi phi man example parse tree 
sub trees np covering man tree contains subtrees 
ples 
output model trained examples sentence arg max 
training algorithm considered construct sequence models test sentence functions return parse tree 
voted perceptron picks tree occurs set fv note easily derived gamma identity gamma ff delta gamma tj delta 
voted perceptron implemented number kernel calculations roughly computational complexity original perceptron 
tree kernel consider representation tracks subtrees seen training data representation studied extensively bod 
see example 
conceptually enumerating tree fragments occur training data note done implicitly 
tree represented dimensional vector th component counts number occurences th tree fragment 
define function number occurences th tree fragment tree represented 
note huge tree number subtrees exponential size 
aim design algorithms computational complexity independent key efficient representation dynamic programming algorithm computes inner product examples polynomial size trees involved time 
algorithm described collins duffy completeness repeat :10.1.1.19.8980
define set nodes trees respectively 
define indicator function subtree seen rooted node 
follows 
step efficient computation inner product property delta delta define delta 
note delta computed efficiently due recursive definition ffl productions different delta 
ffl productions pre terminals delta 
ffl productions pre terminals delta nc delta ch ch nc number children tree productions nc nc 
th child node ch 
see recursive definition correct note delta simply counts number common subtrees pre terminals nodes directly words surface string example symbols 
rooted cases trivially correct 
recursive definition follows common subtree formed production choice child simply nonterminal child common sub trees child 
delta child child possible choices th child 
note similar recursion described goodman goodman goodman application conversion bod model bod equivalent pcfg 
clear identity delta delta recursive definition delta delta calculated jn time matrix delta values filled summed 
tree fragments larger size say depth versus depth sense contribution larger tree fragments kernel 
achieved introducing parameter modifying base case recursive case definitions delta respectively delta delta nc delta ch ch 
corresponds modified kernel delta size size number rules th fragment 
roughly equivalent having prior large sub trees useful learning task 
tagging kernel second problem consider tagging word sentence mapped finite set tags 
tags represent part speech tags named entity boundaries base noun phrase chunks structures 
experiments consider named entity recognition 
pessimistic estimate runtime 
useful characterization runs time linear number members theta productions 
data number nodes identical productions approximately linear size trees running time close linear size trees 
lou gerstner chairman ibm lou gerstner lou tagged sequence 
example fragments tagged sequence tagging kernel sensitive counts fragments 
tagged sequence sequence word state pairs fw th word tag word 
particular representation consider similar sub trees representation trees 
tagged sequence fragment subgraph contains subsequence state labels label may may contain word 
see example 
tagged sequence represented dimensional vector th component counts number occurrences th fragment inner product representation calculated dynamic programming similar way tree algorithm 
define set states tagged sequences respectively 
state associated label associated word 
define indicator function fragment seen left state node 
follows 
simple algebra shows delta delta define delta 
state define state right structure analogous definition holds 
delta computed dynamic programming due recursive definition ffl state labels different delta 
ffl state labels words different delta delta 
model words sentences lr lp cbs cbs cbs vp model words sentences lr lp cbs cbs cbs vp results section wsj treebank 
lr lp labeled recall precision 
cbs average number crossing brackets sentence 
cbs cbs percentage sentences crossing brackets respectively 
model collins 
vp voted perceptron tree kernel 
ffl state labels words delta theta delta 
couple useful modifications kernel 
introduce parameter penalizes larger substructures recursive definitions delta delta delta delta respectively 
gives inner product size size length number state labels ith fragment 
useful modification follows 
define sim words 
define sim share word features 
example sim defined capitalized case sim looser notion similarity exact match criterion sim definition delta modified ffl labels different delta 
ffl delta sim sim theta theta delta words respectively 
inner product implicitly includes features track word features better sparse data 
experiments parsing wall street journal text data set described collins 
penn wall street journal treebank marcus training test data 
sections inclusive sentences training data section final test set 
training sentences train perceptron 
remaining sentences development data tuning parameters algorithm 
model collins parse training test data producing multiple hypotheses sentence 
order gain representative set training data training sentences parsed sentence chunks chunk parsed model trained remaining sentences prevented initial model unrealistically training sentences development sentences parsed model trained training sentences 
section parsed model trained sentences 
representation incorporates probability original model subtrees representation 
introduce parameter fi controls relative contribution terms 
log probability tree original probability model feature vector subtrees representation new representation fil inner product examples delta fil delta 
allows perceptron algorithm probability original model subtrees information rank trees 
expect model original probabilistic model 
algorithm applied problem inner product delta definition 
algorithm runs approximately quadratic time number training examples 
somewhat expensive run algorithm training sentences pass 
broke training set chunks roughly equal size trained separate perceptrons data sets 
advantage reducing training time quadratic dependence training set size easy train models parallel 
outputs runs test examples combined voting procedure described section 
shows results voted perceptron tree kernel 
parameters fi set respectively tuning development set 
method shows absolute improvement average precision recall sentences words relative reduction error 
named entity extraction period year words named entity data annotated 
data drawn web pages aim support question answering system web data 
number categories annotated usual people organization location categories frequent categories brand names scientific terms event titles 
result created training set sentences words test set sentences words 
task consider recover boundaries 
leave recovery categories entities separate stage processing 
evaluate different methods task precision recall 
problem framed tagging task tag word start entity continuation entity part entity 
baseline model maximum entropy tagger similar described ratnaparkhi 
maximum entropy taggers shown highly competitive number tagging tasks part speech tagging ratnaparkhi named entity method proposes entities test set correct entity marked annotator exactly span proposed precision method similarly total number entities human annotated version test set recall recognition borthwick 
maximum entropy tagger represents serious baseline task 
feature set included current previous word previous tags various capitalization features word tagged full feature set described submission 
baseline trained model full sentences training data decoded sentences test data beam search keeps top hypotheses stage left right search 
training voted perceptron split training data sentence training set sentence development set 
training set split portions case maximum entropy tagger trained data decode remaining 
way training data decoded 
top hypotheses beam search log probabilities recovered training sentence 
similar way model trained sentence set produce hypotheses sentence development set 
parsing experiments final kernel incorporates probability maximum entropy tagger delta fil delta log likelihood tagging model delta tagging kernel described previously fi parameter weighting terms 
free parameter kernel determines quickly larger structures 
running training runs different parameter values testing error rates development set best parameter values fi 
shows results test data baseline maximum entropy tagger voted perceptron 
results show relative improvement measure 
relationship previous bod describes quite different parameter estimation parsing methods dop rep max ent perc 
imp 
results maximum entropy voted perceptron methods 
imp relative error reduction perceptron 
precision recall measure 
resentation 
methods explicitly deal parameters associated subtrees sub sampling tree fragments computation manageable 
bod method left huge grammar bod describes grammar sub structures 
method requires search probable derivations grammar beam search presumably challenging computational task size grammar 
spite problems bod gives excellent results method parsing wall street journal text 
algorithms different flavor avoiding need explicitly deal feature vectors track subtrees avoiding need sum exponential number derivations underlying tree 
note goodman gives polynomial time conversion dop model equivalent pcfg size linear size training set 
method uses similar recursion common sub trees recursion described earlier 
goodman method leaves exact parsing model intractable need sum multiple derivations underlying tree gives motivated approximation finding probable tree computed efficiently 
theoretical point view difficult find motivation parameter estimation methods bod see johnson discussion 
contrast parameter estimation methods strong theoretical basis see cristianini shawe taylor chapter freund schapire statistical theory underlying perceptron 
bod 

grammar experience theory language 
csli publications cambridge university press 
bod 

minimal set fragments achieves maximal parse accuracy 
proceedings acl 
borthwick sterling agichtein grishman 

exploiting diverse knowledge sources maximum entropy named entity recognition 
proc 
sixth workshop large corpora 
collins 
head driven statistical models natural language parsing 
phd dissertation university pennsylvania 
collins 

discriminative reranking natural language parsing 
proceedings seventeenth international conference machine learning icml 
collins duffy 

convolution kernels natural language 
proceedings neural information processing systems nips 
cristianini shawe 

support vector machines learning methods 
cambridge university press 
freund schapire 

large margin classification perceptron algorithm 
machine learning 
freund iyer schapire singer 

efficient boosting algorithm combining preferences 
machine learning proceedings fifteenth international conference 
san francisco morgan kaufmann 
goodman 

efficient algorithms parsing dop model 
proceedings conference empirical methods natural language processing emnlp pages 
haussler 

convolution kernels discrete structures 
technical report university santa cruz 
johnson geman canon chi riezler 

estimators stochastic unification grammars 
proceedings th annual meeting association computational linguistics 
johnson dop estimation method biased inconsistent 
appear computational linguistics 
lodhi shawe taylor watkins 

text classification string kernels 
appear advances neural information processing systems mit press 
marcus santorini marcinkiewicz 

building large annotated corpus english penn treebank 
computational linguistics 
ratnaparkhi 

maximum entropy part ofspeech tagger 
proceedings empirical methods natural language processing conference 
rosenblatt 
perceptron probabilistic model information storage organization brain 
psychological review 
reprinted neurocomputing mit press 

