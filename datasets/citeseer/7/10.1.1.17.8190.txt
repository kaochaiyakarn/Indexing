htk hidden markov model toolkit design philosophy sj young cued infeng tr 
september cambridge university engineering department street cambridge cb pz sj 
cam 
uk htk integrated suite software tools building manipulating continuous density hidden markov models hmms 
consists set library modules set tools programs 
written ansi runs mainly unix systems run modern os 
currently speech laboratories world teaching number universities 
htk recogniser included arpa september resource management evaluation november wall street journal csr evaluation cases performance comparable systems developed main arpa contractors see section 
htk designed general purpose platform research benchmark testing product development young 
libraries provide effective programming environment implementing new algorithms tools allow variety recognisers built quickly efficiently 
htk general purpose encourages particular approach building speech recognition systems 
firstly htk restricted continuous density systems preference discrete systems purposes research number mathematically desirable properties purposes practical application believed robust 
secondly parameter tying regarded essential requirement htk provides generalised mechanism allows tying levels 
thirdly htk fosters incremental approach model building system hmms refined number stages involving interleaved model manipulation model re estimation 
htk acknowledges building complex hmm system involves manipulating diverse range data including speech transcriptions dictionaries 
provides rich set integrated tools facilitate activities 
htk fully described user programmer manuals 
address underlying concepts design direct attempt explain philosophy 
aim address just issues 
remainder organised follows 
begins section general overview htk followed section discussion core methods represent hmms internally externally 
section discusses htk incremental model building philosophy section describes design recognition tools 
user interface design described section 
htk designed support large scale system building optimisation important concern 
section describes optimisations built htk training recognition 
section discusses relevant software design aspects htk section describes number applications systems built htk 
includes summary performance results obtained major recognition benchmarks phoneme recognition timit database word recognition resource management database word recognition wall street journal database 
overview htk toolkit htk toolkit designed facilitate construction systems continuous den sity gaussian mixture hmms juang juang bahl 
consists number tools programs comprehensive set library interface modules 
library modules ensure tools behave uniform way simplify development new tools 
htk library htk library consists modules provide fixed interface tools outside world provide variety useful support functions 
library modules listed table 
name module function training token database interactive graphics interface label file additional math support memory management hmm definitions grammar support operating system interface signal processing routines speech data file table htk library modules illustrates way modules typical training tool reads set existing hmm definitions training data produces new hmm files 
input output hmm definitions performed converts external textual representation hmm internal memory representation 
speech data input 
module read waveform parameterised data standard data formats 
perform automatic parameter conversions 
example second difference coefficients automatically appended fly reduce external data storage requirements minimal computational overhead 
module gives efficient access speech segments tools need cyclically process training data examples handling data transcription files performed 
provides command line argument handling facilities enable htk tools uniform manner range operating systems hem provides facilities efficient memory allocation de allocation module supplies simple event driven graphics interface 
converts set syntax rules including word pair grammars simple loops optional terms network representation primarily recognition tools 
htk tools tools current htk distribution table lists selection 
way tools build hmm systems discussed detail section varies course depending type hmms required 
brief example construction continuous speech sub word recogniser involve steps 
hmm definition file file label htk file tool arguments graphics display hmm definition file typical htk tool library interactions name tool function perform forced alignments hcode speech analysis lpc mfcc batch mode dictionary editor embedded baum welch re estimation batch mode hmm editor isolated unit segmental means model initialisation batch mode label file editor list contents data file isolated unit baum welch re estimation results analysis simple interactive label file editor lab convert snor orthography label files net convert snor dictionary network generate data hmm statistical source viterbi decoder isolated connected table main htk tools firstly speech data parameterised hcode phone level transcription files prepared 
may derived dictionary set snor format orthographic transcriptions lab subsequently modified label editor 
necessary dictionary may need editing 
initial monophone context independent models created small amount transcribed bootstrap data tools 
alternatively transcribed data available possible fiat start procedure models initial parameters embedded training 
htk build philosophy core hmm editor embedded re estimation tool 
starting initial monophone models hmms repeatedly edited re estimated required level model complex ity performance reached 
typical edit operations include cloning tying adding removing transitions modifying input data vector sizes mixture compo nent incrementing 
main training tool 
provides embedded training facility need time aligned transcription sequence models training utterance required 
facility training optional units allowing models direct transition non emitting entry state non emitting exit state 
allow optional silence words 
designed support large training databases includes pruning forward backward passes parallel operation number machines 
allows different types tied structures trained 
dictionary multiple pronunciations new label transcriptions generated reasonable models obtained models re estimated new accurate transcriptions 
suitable word pronunciation network generated automatically snor format dictionary realignment performed efficiently network yield pronunciation training file 
trained models tested 
general purpose viterbi recognition tool recognises word sequences syntax 
hvi features include beam search pruning mechanism support word recognition sub word units pronunciation graphs defined network bigram support phone word level control insertions fixed penalty grammar scale factor number computational optimisations shared parameters 
label files produced scored uses dynamic programming string matching procedure compatible standard nist scoring software 
provides number output formats options including statistics confusion matrices speaker analyses 
generate receiver operating characteristic roc information merit fom scores word spotting applications 
representation hidden markov models htk section discusses way hidden markov models represented htk externally internally 
intrinsic representations mechanism standard normal orthographic representation nist 
parameter tying 
described detail including associated re estimation process 
hmm parameter set hmm htk consists arbitrary number states entry state exit state non emitting confluent states 
output distribution associated state may depend statistically independent streams probability independent stream exponentially weighted set stream weights 
ost input observation vector dimensionality stream time bjs ost probability vector state total probability bj composite input vector ot time state ii individual stream probability bjs ost represented mixture gaussian mixture components stream cjs weight mixture stream state denotes multivariate gaussian mean covariance addition state output distribution parameters number ancillary parameters 
firstly transition probabilities states represented single matrix aij 
secondly state may duration probability distribution associated represented parameter set dk 
complete model may duration probability distribution associated represented parameter set dl 
note htk prescribe form parameters allowing named poisson gamma standard hmm tools support 
limits htk imposed available memory 
example model number states number components mixture 
general formulation output probability distributions designed support number different modelling paradigms flexible satisfy needs 
htk fully supports standard single stream multiple stream mixture gaussians multiple stream tied mixture systems 
hmm representation encompasses features directly supported supplied tools included wish htk basis research 
example multiple streams model disparate sources data state stream weights combine sources discriminatively 
alluded earlier various forms durational model incorporated state level model level 
covariance matrices full diagonal non square 
form provided allow arbitrary sub space transformations 
external representation hmms full set parameters constitute hmm definition arranged hierarchy shown fig top hierarchy left descending leaves right 
hmm model state stream mix mean var mix mean vat stream state dk dur stream mt mean var mix mean transition aij matrix hierarchy hmm parameters externally hmm definitions stored text files simple formal language structure mirrors hierarchy 
full syntax semantics language defined htk manual sufficient just give flavour means examples 
shows hmm definition file represents single stream single mixture diagonal covariance left right hmm states emitting keywords indicated surrounding angle brackets definition keywords notation meant easily readable humans whilst efficient parse machine 
general structure global parameters followed parameters state 
example hmm states vector size diagonal covariance duration parameters data parameterisation consisting mel frequency cepstral coefficients fcc 
data state consists dimensional mean vector followed dimensional variance vector 
keyword introduces transition matrix 
referring back fig seen layout definition file external definition single gaussian state hmm follows parameter hierarchy diagram middle stream mixture layers omitted defaults single stream single gaussian distributions assumed 
shows fragment complex hmm definition options 
notice necessary number mixture components distribution 
state number mixture components stream keyword 
example state mixture components stream components stream 
notice stream weights may set individually stream number streams fixed states models 
examples give general flavour hmm definition language demonstrate flexibility 
major feature htk hmm definition language left explain 
referring back fig large black dot diagram represents parameter tie point 
pair similar dots joined entire sub structure right dots shared 
mechanism implemented external hmm definitions simple macro mechanism shared sub structure macro name defined separately macro definition file 
time shared sub structure actual definition file macro name 
macro name consists type identifier followed user defined name 
type identifiers consist tilde followed single mnemonic character various kinds shown fig 
example mechanism fig shows hmm definition fig variance vectors shared single shared variance vector defined separate macro definition file called macros 
hmm definition recognition tool behaves variance replicated definition file 
hmm re estimated training tool behaviour 


external definition mixture gaussian multiple stream hmm 
case data re estimate individual variance pooled variances remain identical 
values simply replicated definition file different re estimation 
parameter tying mechanism underlies htk operation discussed 
note macro definitions contain macro definitions provided nested definitions defined macro file 
practice hmm definition files macro definition files written hand 
usually generated htk tools part incremental build process outlined section 
rarely need practice understand detailed rules construction hmm macro definition files 
tying sub structures hmms htk allows hmms tied mechanism different 
hmm htk logical physical name stored hmm list 
logical physical names commonly need 
logical hmms share physical name effectively tied re estimation training data logical hmm tied set pooled yield just set physical hmm parameters 
internal representation hmms parameter estimation internally hmms stored memory hierarchical organisation external definitions important difference parameter tying macros file macros external definition showing macro create tied variance achieved macros explicit internal representation 
parameter set hierarchy referenced parent address pointer 
parameter tied multiple parents referencing 
state aij state internal representation showing tied variance vector illustrates greatly simplified form example fig previous section stored internally 
seen state points unique mean vector points shared variance vector 
note viewed top difference tied non tied system 
operations tying transparent includes parameter estimation 
htk uses baum welch re estimation estimating hmm parameters training data 
parameter estimates procedure weighted averages 
simple case single stream single gaussian hmm parameter belonging state weight observation time likelihood li state time example re estimation formula variance ett lj ot ij ot ij lj depends current hmm parameters formula applied iteratively required convergence achieved 
clear re estimate parameter storage needed accumulate numerator re estimation formula separately accumulate denominator 
numerator dimensions parameter denominator effectively just counter 
htk accumulators attached directly parameter re estimated way baum welch procedure works properly independently parameter tied young 
fig illustrates process 
shareable vector htk space allocated head vector store pointer accumulator 
re estimation htk manual describes actual internal representation terms data structures 
vector fact new estimate existing estimate 
existing estimate significant difference arise practice 
shared parameter vector numerator denominator 
attached accumulator parameter re estimation involving tied parameter vector shared parameter accessed different parents 
time accessed increments numerator denominator added accumulators 
differing parents know care contributing parameter 
data processed numerators denominators update parameter vectors attached 
means simple storage organisation accumulators needed baum welch re estimation attached directly parameter vectors generalised tying facilities provided htk effectively transparent re estimation tools 
furthermore simple show tying process affect con vergence properties baum welch algorithm bellegarda nahamoo 
htk model build philosophy noted previously htk adopts incremental philosophy construction systems representations models selected give flexibility needed support 
noted htk continuous discrete density distributions improved robustness desirable mathematical properties 
parametric mixture distributions advantage precision individual distribution easily changed simply adding removing mixture components 
htk component added mixture simply making copy component largest weight perturbing means standard deviations halving weights 
mechanism referred mix ture crude followed immediately baum welch re estimation works 
mixture splitting coupled generalised tying mechanism enables complexity hmm system adjusted carefully balanced amount available training data 
general approach model building htk illustrated fig 
input prototype hmm definition function specify topology global characteristics hmm 
prototype set initial hmms created small amount bootstrap data model unit boundaries marked hand 
set initial models retrained training data set perform embedded training sequence units prototype hmm definition bootstrap data intermediate models training data fully trained model set hmm construction procedure training utterance need known 
set fully trained models produced refined hmm editor 
example small vocabulary word recognition limited training data single gaussian hmms built 
insufficient data estimate individual state variances variances tied models globally 
data available units mixture components added variances units 
number states recognition accuracy vs number tied states phoneme recognition task timit database large vocabulary continuous speech recognition necessary model contextual effects context dependent sub word units 
example commonest unit triphone phone model specific left right 
typical phone set potential triphones cient data robustly estimate parameters 
furthermore experimental evidence shows mixture gaussian distributions achieve adequate modelling accuracy creating need training data 
problems solved tying mixture component incrementing 
particular state ty ing effective woodland young hwang huang young woodland hwang huang 
procedure firstly set single gaussian triphones trained corresponding states set clustered tied 
clustering process ensures resulting tied states sufficient data estimate accurate mixture gaussian distributions 
aim process find optimal balance modelling contextual effects accurately modelling underlying distributions 
shows recognition performance varies function number tied states standard phoneme recognition task timit database right context dependent hmms lee hon 
system differing number tied states number mixture components incremented maximum performance achieved 
far left graph corresponds case context independent far right corresponds conventional context dependent 
seen clear peak performance state tying 
timit experiments data driven agglomerative clustering procedure 
possible phonetically driven tree clustering procedure decision tree node question left right contexts bahl odell kannan odell 
complex procedure advantage built decision trees find appropriate state distributions triphones triphones examples occur training data 
clear key component htk toolkit 
operates loading complete set hmms applying sequence edit commands writing new set definitions 
edit commands consist character command name plus arguments typically item list 
defines set similar items parameter hierarchy target operation 
example command tie variance vectors mixture component states hmm called 
resulting macro called vat 
ti var 
state mix cov general ti command simply ties items item list item list exemplar 
mean vectors tied exemplar formed average covariances tied exemplar formed maximum items 
hmm definitions syntax item list mirrors hmm parameter hierarchy explicit stream indicator optional oren unity 
item list specification may model names match zero characters may match single character 
example context dependent models htk naming convention ph ph phone name ler context right context 
command apply state clustering procedure described state phone ih tc ih ih ih ih state argument clustering threshold second root name macro example actual macros tied state called ih ih ih tied state hmm system created simply executing command state 
special case mentioned item list word mix component indices denotes set pdfs 
pdfs specified ti command pdfs joined tied result tied semi continuous system huang jack bellegarda nahamoo paul 
example commands jo ti mix state mix indicate pdfs states models joined 
case mixture components items specified list sorted order mixture weight 
command sets total number tied mixtures required 
mixture components discarded split required number obtained mixture weights normalised subject floor specified second argument command 
macros called mix created pdfs share components 
tied mixture systems common special notation provided represent externally various computations internal htk optimised see section 
section outlined philosophy model building htk 
notion incremental building successive refinement primarily interleaved executions hmm editor embedded re estimation tool 
allows model complexity balanced amount available training data experience gained far approach suggests effective 
speech recognition htk provides recognition tools 
firstly viterbi tool connected word recognition continuous speech sub word recognition word spotting 
secondly derivative specially configured perform alignments state level 
tools gain flexibility fi om syntax definition language provided sparse 
syntax definition function viterbi decoder find sequence hmms likelihood unknown speech maximum sequence 
practice extremely useful able prescribe allowable sequence models order constrain recogniser operate desired way 
example voice control interface limited commands may allowed 
commands may conveniently defined grammar 
similarly sub word recognition different pronunciations word allowed grammar specify 
htk uses extended backus naur form ebnf grammar notation specify recognition constraint networks wirth 
ebnf context free grammar notation htk variables defined constraining grammar regular 
full definition network definition language manual 
brief network definition consists optional set variable definitions followed expression defines actual network 
name expression meaning expressions model variable sequence alternatives parallel factoring zero repetitions repetitions context sensitive loop see text define variable equivalent table network definition constructs variable name hmm 
right hand side variable definitions network expression built constructs listed table 
cmd turn left right cmd sil network construction ebnf constructs maps equivalent network construct fairly obvious way 
simple example fig shows simple network definition file allows sequences form turn left turn right silence command 
shows network definition translated actual network 
sub word recognition special names mark pronunciation 
example fig shows syntax definition defines simple word recogniser word follow word 
notice name word attached symbols wd wd abdomen wd abdomen ae ax max wd abdomen wd ax bay wd wd aa ih sh wd abdomen defining word pronunciations output recogniser constituent phone model names 
iy ih eh ch dh en el sil pau ae ix ax ah dx ng em nx eng er axr vcl vcl sil ax defining context sensitive phone loop designing htk network definition language try straightforward define networks common tasks 
alternative provide language define finite state network directly rejected basis cases tedious 
lack direct control form network constructed limitations 
example possible define triphone loop way consistent triphone sequences allowed 
similarly word level possible define word pair grammars 
solve specific problems syntax network definitions may contain context sensitive loops 
example syntax fig defines network consisting set context dependent phone models parallel 
context broad class defined variable value list phones 
resulting network model iy iy iy linked back models contexts match context definitions 
example link back vowels right context iy viterbi decoding token passing time synchronous viterbi decoders find sequence hmms maximum likelihood generating unknown speech sequence conforms syntax constraints specified recognition network described 
recognition network defined explicitly user input speech files 
network constructed ot fi input speech file supplied transcription 
finds interpretation input file subject constraints recognition network forced recognition find optimal alignment hmm states speech data 
htk viterbi decoding implemented model concept state alignment path explicit young young 
model node recognition network corresponding hmm contains single hmm 
hmm instance contains storage sufficient hold hmm state 
time token state hmm instance represents best partial match observation sequence ol ot sequence hmms state model initially possible hmm entry states contain token partial path probability log states hold token partial path probability log 
time frame token copied possible successor states probabilities updated transition output probabilities 
process leave multiple tokens state best token discarded 
input frames consumed exit states hmms utterance examined token highest log probability identifies optimal sequence 
order find sequence propagation tokens accompanied house keeping operations 
history token route network recorded efficiently follows 
addition partial path probability token carries single pointer called 
token propagated exit state hmm entry state transition represents potential hmm boundary 
record called record generated stores identity hmm token just emerged current value token link 
token actual link replaced pointer newly created link record 
illustrates process 
unknown speech processed link records attached link best matching token token highest log probability traced back give best matching sequence models 
time positions word boundaries extracted required 
recognition process link records generated lie optimal path 
active tokens periodically traced back find useful link records remainder garbage collected 
describes essence token passing paradigm 
simple powerful conceptual model extended number ways 
example allows additional bigram language model super imposed top syntax constraint network model model word word transition probability added path probabilities 
word pronunciations defined id id constructs link records generated word transitions need know positions identity word internal transitions 
token holds record input speech frames stayed state 
allows trace back token history recorded state level 
combination general syntax definition mechanism facilities best token came link record generation token propagation word pronunciations ability specify bigram language model standard htk recognition tools powerful flexible easy 
typical recognition tasks handled include isolated connected digits phoneme recognition simple commands phone word recognition upto words various forms word spotting 
similarly flexibility syntax definition mechanism multi purpose 
syntax definition aligns sequence models corresponding exactly transcription 
useful research purposes underlying state sequence required 
syntax definition contains word pronunciations corresponding transcriptions orthographic find optimal pronunciation word utterance 
extremely useful building phone word recogniser transcriptions training data available 
user interface design htk tools typically invoked typing command form arg arg spl mfc sp mfc name tool followed zero optional arguments followed file names 
form interface chosen preference interactive wimp style interface htk portable operating systems ensure simple execute htk tools fi om scripts 
user convenience typing name htk tool arguments prints summary invoke tool 
option names consist single character shown example 
improve consistency upper case options meaning tools 
example option sets trace level tools 
htk tools typically process kinds file hmm definitions speech files transcriptions label files 
transcriptions consist sequence labels optional start boundary times 
boundary times ignored embedded training commonest case 
general speech file corresponding transcription 
example argument generic htk tool hmm list 
file contains list hmm definitions load 
files spl mc sp mc typically names speech files train test set hmms 
speech file spn mfc htk expect find corresponding label file name spn lab 
options exist change various extensions directories searches hmm definitions label files carried 
cases training testing hmm system involve thousands hmm definitions thousands speech files 
accommodate situations htk number additional mechanisms avoiding problems scale arise 
operating systems upper limit number files specified command line 
htk allows file name arguments command written separate file 
example running embedded re estimation tool common approach list training files file called rain 
scp execute 
train scp command behaves contents train scp appended command line 
simple facility useful keeping permanent record files train system 
need separate transcription speech file causes efficiency problems 
transcriptions contain bytes information storing separate file inefficient 
secondly commonly case speech files may require identical transcriptions 
example training isolated digit recogniser examples say require transcription containing just labels silence 
increase efficiency htk provides concept master label file mlf 
single file contains number transcriptions stored sequentially 
transcription preceded pattern terminated period 
pattern complete file name case behaviour identical case transcription stored separate file 
pattern contains speech files transcription simple pattern matching 
example mlf contained lab silence silence speech file name form nn mfc expect corresponding transcription name form 
lab 
matches pattern preceding transcription silence silence speech files transcription 
master label files redirect search transcription directory allowing htk tool access label files dispersed large database 
similar facilities exist storing set hmm definitions single master model file 
optimisations htk designed support construction large scale hmm systems reason important reduce computational burden possible 
htk includes number optimisations implementation standard baum welch viterbi decoding algorithms 
parameter tying major load training recognition continuous density hmms computation output probabilities 
parameter tying allows robust estimation sparse data allows certain cases efficient computation output probabilities 
basic mechanism htk exploit parameter tying simple 
shared single gaussian shared stream distribution shared complete state distribution evaluated value stored owner attempts re evaluate quantity pre computed value 
special common case mixture component gaussians tied states models order create called tied 
key point form tying gaussian mixture components computed globally individual output distribution formed weighted sum gaussians 
individual gaussians low probability observation vector worth summing gaussian probabilities fall beam relative 
set gaussians sorted rank order top list beam output probability calculations 
pruning implementation baum welch re estimation formulae requires forward probabilities backward probabilities jj computed states time frames case embedded training performed composite model consisting sequence hmms corresponding speech file transcription matched utterance may frames long 
case extremely wasteful compute probabilities forward backward beam search illustrated fig 
backward probabilities computed 
starting speech frame hmm sequence activated values computed backwards time 
time beam active models extended nearer start jj computed states active models 
process maximum value say recorded 
active models processed time beam beam input frames forward backward pruning shrunk models beam state og og user defined threshold 
values known values calculated similar way 
case values known possible calculate actual state occupation likelihoods product models beam state logp log jj logp total log likelihood calculated fixed threshold 
beam depends actual state occupation likelihoods threshold set quite precisely beam narrower lies beam 
order memory computation storage values created demand output probabilities calculated demand 
forward pass stored actual re estimation formulae numerator denominator sums updated step 
viterbi decoder implemented uses beam search similar principle pass described 
case number extra complications 
firstly simple sequence hmms assumed re estimation replaced arbitrary network hmms recognition 
secondly phone word recognition pruning applied word phone level 
solved keeping list active hmm instances beam global array 
inter model token propagation modified output tokens active models scores lie beam passed successor models 
receiving model inactive activated 
cycle active models checked model containing tokens beam deactivated 
beam calculated relative maximum partial path probability state time 
case recognition networks constructs second list maintained active word ends second independent beam applied 
software engineering general structure htk outlined section noted htk intended serve just ready toolkit development environment users create new tools 
consequence htk distributed source form care taken engineering 
htk written ansi standard main data structures represented explicit data types 
example hmm definitions represented type associated set ancillary types defining lower level structures hmm parameter hierarchy 
order support types set routines provided accessing manipulating 
types routines associated particular logical function bundled separate library module 
header file module defines interface rest system 
example definitions support routines type defined odel odel modules htk designed modern principle data abstraction 
data types 
far fi om htk data types concrete 
full definition type visible outside module defines program uses type free manipulate 
software engineering perspective construction htk unsafe easy external agent corrupt internal operation module 
furthermore necessary external agent detailed understanding library module data type order effectively 
type provides example type represents large hierarchical structure htk tools need traverse manipulate 
access manipulate structure directly complex kind operation prone error 
reasons htk constructed 
firstly importantly hard isolate complex data structure secure whilst time allowing tools efficiently 
main htk tools invariably compute bound additional overheads incurred accessing truly data types thought unacceptable 
secondly functions htk provide programming environment support research speech recognition expected individual researchers wish direct access htk internals level detail 
thirdly ansi selected implementation language widely available portable 
principled data types demanded language gave explicit support data abstraction 
time htk inception language stable portable efficient widely available 
htk uses floating point arithmetic re estimation decoding gorithms log arithmetic representations preference explicit scaling 
parameter storage accumulators re estimation tools single precision order conserve memory 
double precision calculation forward backward re estimation tools accumulation log probabilities recognition tool order ensure significant loss numerical accuracy processing long observation sequences 
htk generalised parameter tying mechanism requires ability share structures arrays number owners attach additional storage structures 
accommodate shareable objects field called hook store pointer field called nyse record current number owners 
nearly htk data structures allocated dynamically nyse field necessary know safe dispose shared structure 
applications systems noted htk licensed speech laboratories world wide variety applications 
final section examples cambridge speech group described illustrate flexibility performance achieved 
simplest applications htk word recognition 
basic approach define prototype model word train isolated word training tools 
training data consist isolated examples continuous speech word points known 
alternatively training data continuous speech orthography known embedded training 
type application flexibility syntactic constraint network allows various recognition modes silence noise models inserted network required 
type set extensively study noise compensation parallel model combination pmc 
speech models trained clean speech combined hmm trained examples prevailing background noise form compensated set hmms parameters approximate obtained speech models trained directly noisy speech 
tested isolated connected digit recognition tasks performed variety background noises typical performance db snr improves fi om worse gales young gales young gales young 
development timit acoustic phonetic database possible phoneme recognition standard test acoustic accuracy speech recogniser lee hon 
htk test training corpus phonetically tran scribed utterances estimate parameters left right state phoneme hmms 
take account articulation effects context independent models cloned form right context dependent models 
state tying described section reduce total number states single gaussian distributions incremented component mixture gaussians 
context sensitive phone loop context independent bigram computed phone recognition rate correct accuracy achieved young woodland young woodland 
htk build speech recognition systems vocabularies words variety grammatical constraints including word pair bigram language models 
example tied state word internal triphone recognition systems built word resource management task woodland young woodland young word credit card corpus young word wall street journal task woodland 
case recognition accuracy achieved compared favourably systems developed specifically tasks 
specially designed decoder htk build word word recognisers wall street journal task include full cross word triphones bigram trigram language models 
arpa november wsj evaluation htk recogniser produced lowest error rate word bigram word trigram word bigram second lowest error rate word trigram woodland 
addition building speech recognition systems htk build word spotting systems james young speaker separation systems wang young face recognition systems 
research environment studies discriminative training woodland kapadia hybrid hmm neural net systems young valtchev prosody jones woodland 
years developing htk number people substantial contributions design implementation 
foremost phil woodland played major role improving extending aspects htk developer author 
members cued speech group past assisted debugging htk suggesting improvements contributing code 
acknowledge contribution particular note valerie beattie authored early versions isolated word re estimation tools kapadia helped tune embedded re estimator wrote source generator julian odell developed tree clustering software shed valtchev wrote label editor 
contributed htk years include david cole mark gales matthew jones chris gordon 
contributions arose side effect developing research software htk base 
software substantial innovative day may find way public version htk 
mean time existence evidence author achieved goals producing htk system 
browning browning russell downey phoneme decision tree con struction automatic speech recognition 
dra memorandum defence research agency wore 
bahl bahl lr brown pf de souza pv mercer rl 
speech recognition continuous parameters hidden markov models 
computer speech language vol pp 
bahl bahl lr de souza pv gopalakrishnan ps nahamoo ma 
conte dependent modeling phones continuous speech decision trees 
proc darpa speech natural language processing workshop pp pacific grove calif feb bellegarda nahamoo bellegarda jr nahamoo tied mixture continuous pa rameter modeling speech recognition 
ieee trans assp vol pp 
gales young gales young sj 
improved approach hidden markov model decomposition noise 
proc icassp pp san francisco march 
gales young gales young sj 
cepstral parameter compensation hmm recognition noise 
speech communication vol pp 
gales young gales young sj 
hmm recognition noise parallel model combination 
proc eurospeech pp berlin sept huang jack huang xd jack ma 
semi continuous hidden markov models speech signals 
computer speech language vol pp 
hwang huang hwang huang modeling markov states 
proc icassp vol pp san francisco 
hwang huang hwang huang shared distribution hidden markov mod els speech recognition 
ieee trans speech audio processing vol pp 
james young james young sj 
fast lattice approach vocabulary independent 
icassp adelaide 
jones woodland jones woodland pc 
exploiting variable width features vocabulary speech recognition 
proc icassp vol pp minneapolis 
juang juang levinson se sondhi mm 
maximum likelihood estima tion multivariate mixture observations markov chains 
ieee trans information theory vol pp 
juang juang 
maximum likelihood estimation mixture multivariate stochastic observations markov chains 
att technical vol pp 
kapadia kapadia valtchev young sj 
mmi training continuous parameter recognition timit database 
proc icassp minneapolis 
kannan ostendorf rohlicek jr maximum likelihood clus tering gaussians speech recognition 
ieee trans speech audio processing july 
lee hon lee hon 
speaker independent phone recognition hid den markov models 
ieee trans assp vol pp 
la maximum likelihood estimation multivariate observa tions markov sources 
ieee trans information th vol pp 
maxwell woodland maxwell ba woodland pc 
hidden markov models shared vector linear predictors 
proc eurospeech berlin 
odell odell jj 
decision trees context sensitive phoneme mod 
thesis cambridge university engineering department sept odell odell young sj woodland pcw 
ee state clustering vocabulary speech recognition 
ieee conf image speech neural nets hong kong 
paul paul db 
lincoln tied mixture hmm continuous speech recogniser 
proc darpa speech natural language workshop pp hidden valley pennsylvania june 
face segmentation identification hidden markov models 
proc th british machine vision conference springer verlag 
valtchev valtchev kapadia young sj 
recurrent input hidden markov models 
proc icassp minneapolis 
wang young wang mq young sj 
speech recognition hidden markov model decomposition general background speech model 
proc icassp san francisco march 
wirth wirth algorithms data structures programs 
prentice hall series automatic computation englewood cliffs new jersey 
woodland woodland pc 
vector linear prediction hidden markov mod els 
proc icassp san francisco march 
woodland young woodland pc young sj 
benchmark darpa rm results htk portable hmm toolkit 
proc darpa continuous speech recognition workshop stanford sept woodland young woodland pc young sj 
htk continuous speech recog 
proc eurospeech pp berlin sept woodland woodland pc odell valtchev young sj 
large vocabulary continuous speech recognition htk 
icassp adelaide 
young young sj 
general tying phoneme hmm speech recog 
proc icassp san francisco march 
young young sj russell nh thornton 
token passing simple conceptual model connected speech recognition systems 
technical report cued infeng tr cambridge university engineering dept young young sj russell nh thornton 
syntax multiple alternatives voice operated database inquiry system 
computer speech language vol pp 
young young sj 
competitive connectionist approach discrim hidden markov models 
proc iee part vol pp 
young woodland young sj woodland pc 
state tying contin uous speech recognition 
proc eurospeech pp berlin sept young woodland young sj woodland pc 
state clustering con speech recognition 
submitted computer speech language 
young young sj woodland pc byrne wj 
htk version user refer ence programmer manual 
publ 
entropic research laboratories washing ton dc 
young young sj woodland pc byrne wj 
spontaneous speech recognition credit card cor htk toolkit 
submitted ieee trans audio speech processing special issue robust processing 

