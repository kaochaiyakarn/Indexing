efficient memory learning robot control andrew william moore trinity hall dissertation submitted degree doctor philosophy university cambridge 
october dissertation application machine learning robot control 
system initial model robot world dynamics able construct model data received sensors approach formalized ab state action behaviour control cycle 
method learning experiences lifetime robot explicitly remembered 
experiences stored manner permits fast recall closest previous experience new situation permitting quick predictions effects proposed actions goal behaviour permitting fast generation candidate action 
learning take place high dimensional non linear control spaces real valued ranges variables 
furthermore method avoids number shortcomings earlier learning methods controller trapped inadequate performance improve 
considered system resistant noisy inputs adapts environmental changes 
founded mechanism choosing actions introduced solves experiment perform dilemma domain adequate computational efficiency fast convergence goal behaviour 
dissertation detail ab control cycle integrated low high complexity tasks 
methods algorithms evaluated numerous experiments real simulated robot 
final experiment illustrates compound learning task structured hierarchy simple learning tasks 
supervisor william clocksin initial motivation help advice 
am grateful mary lee thomas vogel provided valuable detailed comments drafts dissertation 
due thomas clarke barney pell useful inspiring discussions 
am grateful chris atkeson introducing number important pieces related 
declaration declare dissertation result explicitly stated text contains outcome done collaboration 
part dissertation currently submitted degree diploma qualification university 
dissertation dedicated parents 
contents ion learning motivations 
sab learning system 
dissertation 
robotic tasks 
conventional robot control 
robot control difficulties 
learning robotic tasks birth learning control 
learned 
important issues learning control 
learning robot control 
investigation 
sab learning ab learning 
perceived state transition function 
mountain car example 
sab control cycle 
nearest neighbour quick cheap learning 
nearest neighbour generalization 
alternative generalizations 
class learnable functions 
accuracy learning 
nearest discussion 
kd trees cheap learning nearest specification 
naive nearest 
kd trees 
nearest search 
theoretical behaviour 
empirical behaviour 
kd tree operations 
sab trees coping disorder sab relations representation 
resisting noise 
adapting changing environment 
updating relation 
sab tree garbage collection 
sab control making control decision 
control decision analysis 
learning strength 
learning perform task goal behaviours come 
controlling ice puck easy 
low abstraction tasks 
middle abstraction tasks 
compound tasks 
benefits learning 
experimental results ab learning 
sab learning joint arm 
trajectory tracking experiments 
arm experiments 
juggling ball 
task 
experimental results 
extensions albus cmac kd trees 
reinforcement learning dynamic programming 
summary 
contributions 
extensions 
concluding remarks 
format graphs nearest neighbour polynomially learns continuous functions estimating directed behaviour attainable chapter starts simple example robot control problem 
moti learning control briefly reviewed statement areas subject addressed 
guidance structure rest dissertation 
dissertation robots autonomously develop models world 
shows robot looking hand 
times controller choose joint torques cause perceived position hand behave way helps achieve task moving cross hairs 
control problem solved robot knows related perceived state location image hand perceived speed direction 
raw action signals sent motors 
perceived behaviour change perceived position velocity occurs 
relationship composition relationships 
torque joint depend signal joint 
torque affect angular acceleration joint particular configuration 
configuration affect perceived hand position 
example central problem low level robotics need compute relationship number variables related robot state environment 
relations termed world models 
include kinematic models hand eye coordination models dynamic models spatial models 
models obtain mathematically reasons described 
appealing arguably practical way obtain learning 
direct drive robot arm controller camera image robot looking torque controlled arm 
learning control motivations system improve aesthetically pleasing thing 
related motivation designing learning robots understand learning occurs biological organisms 
successful automatic learning system provide indication animals people learn 
robot learning particularly place motor learning basic form learning behaviour organisms 
guaranteed engineering solution provides complete biological explanation 
analogy flight engineered solution differs markedly biological solution 
important motivation practical 
conventional control cope sorts interesting autonomous machines substantially different industrial machinery 
main reason difficulty precoding sufficiently general world model accurately take account eventualities 
world complex analytic models simple components relationship joint angles velocities accelerations torques complex 
true highly idealized simplified component models 
sab learning system dissertation practical efficient fast robust method obtain robot world models learning 
word robot conciseness applicable dynamic control problems need multivariate models world 
method called sab learning system 
acronym denotes components dynamic world model state action behaviour 
principal aims ab learning listed practical 
strongly motivated desire avoid micro world problem 
concerned learning complex high dimensional state control spaces real valued ranges variables 
efficient 
time update world model new knowledge model learned sufficiently fast realistically occur robot operates 
attained means computationally efficient algorithms 
fast 
learning method fast performance improves quickly 
achieved partly means powerful generalization primarily shot learning method presentation piece data required information stored 
case seen number times time perturbing world model representation lessens error 
robust 
learning method cope disorder environment form noise form gradual change sudden unpredictable change 
robust respect internal parameters chosen minimal kind relationship learned 
method hard get stuck repeat error 
investigation explains demonstrates learning world models sufficient transform design robot controllers simpler design problems 
simple controllers trajectory followers pick place additional effort 
compound tasks learned world models keep controller design process entirely level renders design problem easy human automatable 
dissertation dissertation begins techniques problems robot modelling control earlier current field learning control 
formalizes behaviour learned model controller discusses world models represented 
explains detail chosen representation expected meet goals turn practicality efficiency speed robustness 
features require special attention dealt chapters 
time issues uncovered including curse dimensionality search useful diversity experience 
problems explained dealt algorithm called ab action chooser 
discussion best world model accomplish tasks 
main body dissertation variety experiments conducted evaluate method performance 
experiments include learning hand eye coordination real jointed arm 
learning visually observed trajectory simulated torque controlled arm wide variety conditions 
learning movement control wide variety trajectories arm 
learning juggle 
learning simulated ball simulated bucket 
discussion additional investigations relating new method implementing albus cmac experiments variable resolution dynamic programming 
chapter robotic tasks 
chapter serves simple issues robotic control 
begins introducing giving examples disciplines robot mod ii robot control iii robot intelligence 
discusses problems conventional robot modelling affect higher levels control 
conventional robot control section provides brief tasks facing designer robot controller 
listing problems need solved increasing level abstractness 
robot modelling conventionally modelling achieved analytically 
successful universally applied method branches engineering 
set primitive axioms model behaviour primitive components physical world combined mathematical analysis model complex systems 
perception 
order achieve task necessary objects real world observed observations obtain real world positions orientations 
example method perception vision mapping image real world position orientation required 
kinematics 
usually necessary robot controller obtain positions tations particular links joints different frames real world 
computation takes input fixed data robot topology link lengths ii vector current joint positions 
joint position typically joint angle joint revolute joints joint length joint prismatic 
output computation location links world coordinates 
conversely necessary take input target position coordinate frame real multi jointed robot ma 
world obtain set joint positions produce target position 
computation robot kinematics robot inverse kinematics 
dynamics 
robot dynamic behaviour determined forces acting 
forces gravity control 
forces torques supplied robot joints 
dynamics problem compute behaviour affected forces acting robot 
formalize problem notion system state 
state representation col lection values contains sufficient information predict principle behaviour system provided external internal forces known 
particularly convenient state representation robotic manipulator consists vectors represent current set joint positions velocities 
state change time derivative current state determined internal external forces arm 
current state time derivative position component state calculated trivially velocity component derivative velocity joint accelerations vector behaviour dependent joint torques calculation dynamics problem 
inverse dynamics problem converse 
current state target joint acceleration compute set joint torques achieve target 
robot control trajectory tracking 
trajectory temporal sequence states qo qo ql ql tracked sequence joint accelerations ii ql qi li time step typically th th second 
known sampling frequency 
inverse dynamics model determine sequence joint torque vectors vl 
cause ideal accelerations 
method implements open loop control result sequence torque vectors precomputed prior trajectory execution 
closed loop control current state monitored actual torque vector modified actual current state 
advantage closed loop control behaviour manipulator differ predicted dynamic model tracking error compensated 
variety reasons predicted behaviour inaccurate discussed section 
compensation function error signal 
field control theory graham provides selection schemes generating modification provides mathematical tools analyse stability modification strategy 
common examples proportional positional control adds basic precomputed torque component proportional current position error tending cancel 
derivative velocity control adds component proportional current velocity error 
example required state stationary torques supplied opposite direction current movement 
integral control modification varies local accumulation errors 
different controllers combined additively 
example pd control chosen torque mow defined mow ri kp kv iti precomputed inverse model feedback control called feedforward control 
requires inverse dynamics model precompute necessary torques general control theoretic mathematical tools 
particular feedback matrices kp kv equation called gains determined analytically 
chosen provide stable response short time 
generally mathematical analysis requires assumption local linearity dynamic model 
error monitored reduced acceptable performance occur model simple approximation 
extreme case inverse dynamics computed joint torque computed entirely current position velocity errors 
joint actuator independent variable control system continually tries track input signal output signal means linear feedback control 
speeds low extreme approach results large trajectory tracking errors 
scheme computed torque control fu cation directly inverse dynamics model 
ideal acceleration take back trajectory computed torques achieve acceleration computed inverse dynamics 
current state meant ith state trajectory attempt apply acceleration low iii kp qi iti trajectory controlled fashion accelerations linear system defined section 
shown provided gains high converge accurate tracking trajectory 
computed torque control computationally expensive inverse dynamics computed real time 
extent model correct affects accuracy stability trajectory tracking 
cost accuracy simpler model robot dynamics 
example model took gravitational forces account 
summary trajectory tracking achieved suitable inverse dynamics model 
possible methods open loop control precomputed inverse dynamics 
check errors 
closed loop control precomputed inverse dynamics modifications function error signal 
requires extra mathematical models analysis 
closed loop control dynamically computing ideal current accelerations turn torques achieve accelerations 
balancing 
typically remaining static position easy application closed loop control error signal simply difference current state ideal state 
state changed lessen error 
dynamic situations error decreasing joint torques may available 
occur joint actuators insufficiently strong provide required torque 
systems actuator joint 
case global strategy returning goal required local adjustments insufficient 
stability problem 
perfect knowledge kinematics dynamics sufficient solve directly intelligence needed develop strategy stability 
cart pole problem 
classic example pole balancing problem described michie chambers 
depicted 
cart moved left right bounded track 
base pole fixed cart revolute joint actuator 
control angle pole indirect means thrusts cart 
balance non local states prevent eventual disaster state vector moved goal state 
robot intelligence obstacle avoidance 
move manipulator static configuration controller track straight line uniform speed trajectory joint space 
transition arm sweeps volume space coincide obstacles 
obstacle avoidance involves finding trajectory intermediate configurations cause collision 
obstacle avoidance specification shape robot required defined mapping joint angles space dimensional solids 
spatial model implemented combining standard dimensional geometrical algorithms robot kinematics 
candidate trajectory tested spatial model ensure qi trajectory maps solid intersects obstacle 
possible method solution obtain trajectories generate test procedure 
search valid trajectory guided evaluation function 
function scores different configurations badly points near obstacles points far away 
trajectory search follows directions gradient function 
autonomy 
highly sophisticated robot controller require abilities moment available biological systems 
include planning inter agent communication failure management 
abilities require knowledge environment obtained learning 
study problems restricted robotics form general field artificial intelligence 
example considers design autonomous robot assist building space station 
bandwidth communication robot low unreliable time delay 
result practical human task fixed program specification cover details wide range tasks robot expected achieve 
robot relatively task specifications left compute locally means achieve 
discussion examples tasks facing robotic designers 
include actuator modelling control languages trajectory planning optimization 
examples roughly increasing order complexity generally earlier tasks solution tasks 
robot control difficulties previous section considered issues addressed designer robot control system 
explore difficulties conventional solutions 
discussion split sections 
discusses problems robotic mathematical modelling second considers effect deficiencies models robot control 
mathematical modelling robot world problems mathematical model built set axioms physical behaviour world newton laws motion 
low level system components treated atomic having ideal properties required fit axioms 
result extent model correct depends extent low level components fact match requirements axioms 
example joints arm may modelled rigid links rotating common frictionless axis 
reality friction joint mismatch idealized assumption world lead general model inaccuracies 
friction modelled treated growing linearly angular velocity 
approximation designer check experimentally inaccuracies sufficiently small 
mathematical model describes behaviour terms explicit system variables angles velocities large number system parameters 
system parameters include features mass moments inertia link link lengths coefficients friction sensor locations relationship actuator signals torques produced 
values parameters obtained theory measured 
measurements directly computed observations features system computations turn mathematical models set problems 
accuracy parameters affect model accuracy critically 
short term dynamic changes system state included dynamic model 
changes system time 
include gradual changes performance components due wear tear example joint stiff 
gradual changes theoretically incorporated practice techniques model wear 
changes result unpredictable system perturbations example camera 
possible model 
system designer assume small changes little impact model accuracy regularly inspect equipment ensure significant changes occurred 
generally differential equations resulting models analytically soluble need approximated computed numerically case issues numerical stability arise 
generation mathematical models evidently requires great deal expertise 
designer needs apply knowledge axioms physical world behaviour 
necessary judge aspects important model affect accuracy 
large degree mathematical dexterity required combine com ponents model solve resulting equations 
aspects system automatable algebraic manipulation software reduce hearn generally requires time expensive highly skilled experts 
mathematical models require enormous computational power 
greatly increases expense calculating real time dynamics inverse dynamics tasks trajectory tracking 
increasingly powerful computers eventually able deal problem complex dynamic systems 
discussion robot control argument section high level robotic problems search problems 
examples general problems particularly fields optimization artificial intelligence ai 
modelling problems 
caused inaccuracies mathematical modelling 
argument supported considering tasks discussed previous section 
trajectory tracking poor open loop model marginally inaccurate 
control closed loop error caused inaccurate model reduced typically consists actual state lagging quite reaching desired trajectory 
problem computed torque feedforward control trajectory tracking enormous amount computation required 
modelling problems difficulties designing trajectory tracker 
balancing achieved trying track static trajectory 
harder case system state serious imbalance attainable trajectory back safety deduced entirely general way obtain trajectory analytically mathematical model 
strategy obtained automatically search techniques applied 
obstacle avoidance relies spatial kinematic model 
goal set known obstacles model entirely correct able prescribe suitable trajectory 
solution search problem dimensional space 
autonomy depends representation world general problem solving abilities proposed investigated variety fields ai 
abstracting aspects world placing uniform coordinate system modelling problem 
aspects autonomy ai domain 
dissertation concentrates solution modelling problems 
designer high level control confronted search problem alternatives 
perform search planning tasks manually encapsulate knowledge program 

best automatic techniques optimization ai fields 
robotic design tasks alternatives entirely adequate 
design tasks may domains restricted allow knowledge expressed planning take place realistic time 
majority dissertation focus reliably accurately learn models world 
chapter learning robotic tasks chapter surveys related literature learning control 
begins early introduces important distinction learns ac tion maps learns world models 
particularly important aim survey show earlier field usually regarded learning relationship state action behaviour 
important issues learning control surveyed 
particularly relevant re cent summarized 
chapter description dissertation fits issues discussed 
birth learning control mechanical governor invented james watt device regulates speed rotating shaft 
rotation increases pair balls move apart 
movement transmitted mechanically engine producing rotation causes hard example closing gas 
conversely rotation ideal balls closer similarly causes engine increase output 
elegant system controls automatically sensing performance making adjustments bring performance closer ideal 
systems common mechanical engineering century 
called closed loop control systems 
century analogue electronics allowed flexible approach systems 
measurements performance encoded electrical signals mechanical positions 
sophisticated forms closed loop control possible 
signals encoded processed digitally allowing increase flexibility amount information available 
large amount information obtained system leads question best 
approach manner earlier generations controllers 
provides robust mundane control 
example approach servo driven robotic manipulators 
joint moved specifying desired position joint simple closed loop control joint independently continually adjustments position joint required 
interesting possibility information received sensors autonomously improve performance learning world 
benefits summarized 
optimality 
control complex systems manipulators quicker accurate 
self programming 
complex systems need analysed modelled expensive human experts 
discover abilities 
model intelligence 
system improves performance provides possible model behaviour biological systems 
disorder 
system monitors performance able cope noisy non stationary environment 
difficulty large amount information 
trade offs available reliance standard control techniques ii intensive processing dynamic information 
trade information automatically adjustments controller high level 
discussed subsection 
adapting higher levels controllers standard closed loop controller adjustments low level 
servo controlled arm tries follow trajectory continually monitors current error position velocity 
addition adjustments higher level control 
attempted trajectory completed data collected adjust advance torques supplied joint time step time trajectory attempted 
idea basis adaptive control phan el 
different example idea making adjustments higher levels abstraction task level learning el 
example throwing ball target 
conventional modelling control throw ball fails due modelling errors adjustment hard ball thrown 
fallen short thrown harder time thrown hard 
idea applied successfully real bat ball juggler lower levels built mathematical models system components 
task level learning ball usually dropped hits 
task level learning time failure substantially increased typically hits 
basis learning control basis learning control established consolidated expository fu 
summarize 
fu cites variety possibilities learning control 
classification teacher 
possibility learn classify states accord ing action best applied 
drew emerging field pattern classification duda hart 
sample states known optimal actions pattern classifier learned state original sample produces action predefined criterion best agrees sample points 
possible criteria included assumption optimal control actions linearly separable case learning controller tried find hyperplane partition state space manner agreed sample points 
method information environment improve performance relied teacher tell correct 
reinforcement learning 
action applied part information environment signal saying ideal action signal improve performance 
reinforcement learning 
state space partitioned different regions 
region relative probabilities attempting alternative actions modified reinforcement signal 
reinforcement signal increases autonomy learning tricky requirement systems having global goal find hard specify local goals lead global performance 
stochastic automata 
stochastic automaton finite number internal states traversed control cycle 
input automaton finite number reinforcement signals world 
output automaton finite number actions applies 
state transition function stochastic matrix modified scheme designed increase probability causing outputs produce higher levels reinforcement 
fu overview mentions issues date developed field 
include problems learning world change unpredictably non stationary environment problems slow learning rate question learning controllers linked 
history issues discussed section explain different approaches learning control developed seeds 
learned 
fundamentally different things learning control system attempt acquire 
action map state action world model interpreted form state action behaviour learned inverse world model state behaviour action inverse form world model dangerous learn may function 
current state desired behaviour contexts actions achieve behaviour contexts multiple actions 
successful learning inverse world model requires extra domain knowledge problems occur 
remainder chapter surveyed interpretation learned mappings action maps world models relate state action behaviour 
subsections describe early examples 
subsection contrasts approaches 
michie chambers boxes early classic example action map learning boxes pole balancing system michie chambers 
system controlled pole balanced cart 
cart thrust left right control cycle permitted actions 
state system consists values position velocity cart angle angular velocity pole observed control cycle 
goal prevent pole falling specification learning system 
systems learns action map xct left right state action representation mapping dimensional array indexed quantized state variables 
example coordinates cart ins ins left cart considered behaviourally equivalent 
grades position distinguished 
total entries array entries boxes system named 
contents boxes affect action chosen system state coincide box 
contains statistics previous decisions box entered 
identify mean survival time left action taken mean survival time action taken 
decision direction choose biased expected longer life 
learning continues decisions lead disaster gradually eliminated lead immediate disaster 
system displays desirable property local reinforcement signal required learning done simply final outcome trial 
system manage learn balance pole typically balancing attempts 
raibert parameterized method raibert raibert raibert learned control real torque driven robot ma 
learned inverse world model joint angles joint velocities joint acc ns joint torques state behaviour action represented quantized multi dimensional array 
state space dimensional quantization levels cells hash coded save memory 
cell corresponds simple local model behaviour robot 
learning system uses domain knowledge form equations motion robot arm 
equations parameters vary state space assumed constant box 
values parameters estimated cell recording real world experiences robot inverting known linear form local model obtain parameters 
method estimating local parameters provides accurate model disadvantage needing assume certain form dynamic equations motion 
learning system task prespecified trajectory 
learned model precompute necessary torques 
execution trajectory feedback 
despite open loop control performance improved learning reach perfect behaviour 
convergence took approximately trials 
raibert carried tests illustrating important features learning control 
firstly tested performance environment changed unpredictably time 
experiment consisted period normal learning unknown controller dynamic behaviour arm changed adding weight joint 
second experiment similarly attached spring joint 
arm adapted slowly changes 
second extra experiment learn trajectory try executing nearby trajectory 
see successful generalizing abilities learning system 
results showed trajectory learned quickly nearby trajectory previously learned 
learning action maps world models 
subsequent investigations learning control differed approaches adopted 
survey distinguish clearly piece examines kind mapping learned 
large impact applicability expected performance utility learning system concerned 
trade usefulness learned expected ease speed learning 
learning action maps provides useful product learned controller knows state 
disadvantage action map generally hard learn 
clear current performance improved improve 
simple example armed bandit problem described fully system state possible actions optimal solution known 
despite serious difficulties action map learning attempted suc cess michie chambers barto kaelbling simons gordon grefenstette 
michie chamber pole balancer barto implemented learning controller balances pole considerably quickly delayed reinforcement signal remember pole controller gets told formance pole falls 
improvement learning immediate reinforcement signal 
reinforcement signal scores action decision moves pole state superior state bad moves inferior state 
relative qualities states estimated record expected time failure starting state 
precise definition value hard pin 
trying estimate expected time failure current state optimal controller estimating means expected time failure current controller 
unfortunately recursive definition multiple solutions necessarily defined 
practice jordan jacobs ambiguity cause problem 
investigation kaelbling kaelbling thoroughly area learning embedded systems 
learning world models easier objective observations world 
performance inadequate observed behaviour differs predicted behaviour 
case clear world model updated reduce eliminate error 
reason model learning control systems popular raibert miller mel atkeson sutton 
sacrifice relative ease learning world may modelled necessarily clear model 
investigations mentioned deal problem variety ways 
weak ai optimization 
christiansen controller learns fiat block behaves tray lying tilted robot 
experiments involve real visually observed robot 
world model learned start pos orientation tilt angles pos orientation behaviour representation means quantized array 
robot goal position orientation 
current position orientation observed 
generally action immediately produce desired goal standard search carried learned world model find sequence actions achieve goal 
tray tilting explores learning control issues discussed shortly 
examples model learning search mel performs best search produce obstacle avoiding positional trajectory reach visual goals sutton uses dynamic programming learned model plan simple maze paths goal 
perform non task 
robotic tasks sufficiently concrete learn world model 
prime example trajectory tracking task studied raibert similar tasks atkeson atkeson miller 
model pre programmed controller 
logical extension previous approach 
problem design model controller achieve problem 
model controllers simple unsophisticated 
example controller visual tracker designed miller guided program expressed short set decisions feedback rules 
top level programs sufficiently simple form may mechanism generate automatically 
learn evaluation function 
learning world model evaluation function world states learned 
evaluation function mapping form ef state conventionally lower ef better state world model conjunction evaluation function choose actions 
current state set possible actions consulted candidate action evaluation computed predicted resultant state 
action chosen minimizes predicted evaluation 
efficient provided number possible actions large 
method utgoff balance pole case world model learned estimated single previous state transition 
mapping learned states expected time disaster 
learning evaluation functions domains puzzle game learning world model trivially available need learned samuel rendell 
important issues learning control curse dimensionality realistic systems learning action maps models world able cope dimensions approximately zero eighteen eighteen direct drive jointed arm twelve dimensions state space dimensions action space 
restricted learning task dimensions zero 
see learning representations convergence times exponentially worse increasing dimensionality 
approaches possible actions similarly blow increasing dimensionality action space 
problems compounded usually case variables state space action space continuous 
example generally known advance level safe quantize quantization levels vary 
problem dimensionality rarely directly addressed learning control liter 
generally dealt ways 
assume control spaces small denumerable 
assumption stochastic automata fu systems quantizations state spaces michie chambers barto christiansen commonly reinforcement learning research kaelbling sutton 
similarly action spaces small example classic pole balancer actions 
reasonable approach initial investigations aspects learning control doubt useful point take initial approaches bigger problems 
assume underlying discoverable structure problem 
generalize way essential assumption form 
strength assumption vary greatly 
parameterized mapping learners described section include polynomials neural nets strong form assumption minsky papert jordan jacobs 
chosen fore knowledge structure world model guarantee possible set parameters produce mapping adequately model data 
decision tree classifiers quinlan weaker assumption domain split fairly small number large regions classification constant 
feature common approach salzberg aha learn nearest generalization assumption classification regions characterized small number chosen example points exemplars 
learn task 
state space dimensional trajectory required behaviour world need learned dimensional strand 
approach adaptive controllers robot arms phan 
miller 
dimensionality model brought 
learn small sub areas task 
natural extension previous approach learns small regions domain small dimensional strand 
entirely repetitive task usually important know behaviour close solution task solution task 
order compensate unpredictable deviations 
tactics learning task try keep experiences clustered fairly low dimen sional task specific subspace 
miller goal stated 
idea demonstrated clocksin moore hand eye coordination relation learned jointed arm learning biased explore dimensional subspace sufficient reach observed positions 
variable resolution ability concentrate particularly important areas control space requires suitable choice mapping representation 
aim particularly important previous subsection defence curse dimensionality assuming extra domain knowledge concentrate task specific sub areas 
issue considered simons array boxes recursively partitioned increase resolution necessary utgoff learned balance pole needing quantize state variables 
modularization technological professions large systems broken smaller components typically hierarchy 
desirable achieve learning control systems 
result group simultaneous learning controllers controllers making concrete controllers 
issues hierarchy organized 
organization achieved automatically 
question mentioned places fu sutton discussed detail 
exception architecture proposed albus 
second problem interesting difficult addressed hierarchical structures modules complex learning controllers 
disorder environment consequence disordered environment individual observations may reliable 
reasons environment may disordered 
noisy environment 
controller perceives randomly perturbed ally happens 
case solution local averaging data implementation depends entirely mapping represented 
representations mappings discussed chapter 
non deterministic environment 
simple approach problem treat non determinism noise 
interesting forms non determinism inadequate variation probability distribution mapping learned valuable information controller 
tray tilting robot christiansen learns actions minimal non determinism preference unreliable actions 
non determinism learning action maps considered kaelbling sutton 
non stationary environment 
problem investigated raibert miller moore case world modelled perturbed requiring quantitative change control strategy 
harder problem discussed sutton control strategy undergo qualitative change 
sutton dyna system described section 
state identification action maps world models need able detect state system 
point worth recalling definition system state 
imagine detect certain amount information current configuration system 
information combined proposed sequence actions principle sufficient determine behaviour system information representation system state 
field assumes information provided system sufficient determine state requiring certain small amount world knowledge system de signer 
partial solution case important aspects state unknown proposed simons farmer 
approach suggested vogel 
experimenting controller learning needs generate diversity experience 
methods achieve fall categories 
teacher 
role teacher simply tell system done perform task guide system areas state space judged profitable explore 
common form teacher naive servo linear feedback controller atkeson miller miller el directs experience areas state space lie close solution 
extra domain knowledge structure world model required provide teacher 
randomness 
common approach gaining experience 
exam ples mel 
estimate utility information gain 
investigated thor kaelbling kaelbling 
uses statistical heuristic called interval estimation choose actions achieve reward avoid getting stuck repeated application known mundane action superior actions little experience available 
investigates algorithms immediate reinforcement ii delayed reinforcement choice action motivated state states 
copes non deterministic environments 
choosing actions heuristics clude benefits information gain investigated christiansen el sutton 
inductive learning discussion literature reviewed section considering design controller perform environment 
goals learning important environment 
argued achieved truly complex tasks require human intervention 
furthermore argued michie michie learning systems accepted commercially decisions understood human users 
learning robot control utgoff variable resolution pole balancer utgoff evaluation function learned classic pole balancing problem described section 
evaluation function cart poe state desirability state mentioned section evaluation function conjunction world model provide functionality action map experiment world model 
evaluation state action repeated obtained 
obtained behaviour action previous state guide alter current state 
predicted evaluation decline alternative action automatically predicting consequences 
evaluation function represented explicit record experienced states lated shepard method described section 
evaluation function updated relationship subsequent states ad hoc analysis states prior collapse pole 
appropriate choice parameters quickly learn balance pole 
miller learning world models cmac miller colleagues miller miller cmac albus albus model world 
model conjunction teacher form simple linear feedback controller improve performance 
investigations reported literature 
learning track dynamic trajectories 
learned inverse world model joint angles joint velocities joint accelerations joint torques behaviour simulated jointed robot arm 
goal trajectories defined joint space coordi 
main experiments repetitive trajectory taught fixed gain controller 
results showed quick improvement performance feedback controller 
experiments carried trajectories learned changes environment 
cmac behaviour discussed particular problems small underlying memory 
kinematic visual tracking moving objects 
jointed robot arm held camera pointing downwards conveyor belt 
motivated advantages able world models coordinate system task learned 
case task coordinates visually sensed position orientation plastic disposable razor sensed joint angles arm 
arm controlled requesting joint velocities obtained independent servo motors joint 
forward inverse world models learned 
forward model relationship current joint angles current observed position razor requested joint velocities sent motors resulting change image position 
image position value obtained image processing consisted values coordinates center razor image orientation 
forward model joint angles image position joint velocity image change state action behaviour inverse model joint angles image position image change joint velocity behaviour action task keep image razor fixed meant arm move keep camera relative moving razor 
experience provided means teacher fairly complex position feedback controller 
required substantial amount domain knowledge feedback joint space tracking error image space 
model 
control cycle forward model predict razor appear cycle desired image position change computed 
backward model obtain joint velocity achieve desired image position change feedback control signal added 
results final average error approximately quarter obtained feedback control 
learning typically took trials razor placed identically start trial 
random initial razor configuration learning considerably slower accurate 
mel murphy mel mel learned vision kinematic control real jointed planar arm 
interesting feature investigation ecological approach visual observations kept raw sensed form binary array 
control learned world model joint angles raw image action behaviour world model forward 
mel explains generally valid direction learn inverse model usually definable function 
true domains workers particularly true kinematics redundant manipulator 
manipulator redundant multiple ways move gripper desired position desired position orientation 
world model learned period random arm 
processed represented kd tree algorithm behaviour neural net chapter introduces describes evaluates kd trees 
world model learned plan sequences incremental joint modifications reach target positions avoiding visually observed obstacles 
mel stresses importance planning place learned model requiring real execution 
plan modified best search visual distance heuristic 
aided second learned world model inverse differential kinematics joint angles hand position joint angle state behaviour action defined function mel explains rectify 
learning successful computationally expensive expense due processing images 
atkeson memory control atkeson atkeson learns control simulated dynamic robot arm follow trajectory learns corrections foot placement model simulated hopping robot 
experiment model learned inverse world model joint angles joint velocities joint accelerations joint torques beh representation explicit set data points 
variety generalizations tried 
nearest 

local regression 

local fitting quadratic surface 
task specified trajectory joint angles experience gained means teacher linear pd controller 
convergence generally successful quick simple nearest neighbour generalization gets stuck states performance improved 
reason incompletely learned inverse model actions known failed 
problem discussed section 
rate learning final accuracy seen improve increasing complexity method generalization 
foot placement task learned state ste 
foot placement state behaviour action learning model directly learned adjustment simple analytic world model 
difference mapping expected smoother direct model easy learn 
aspects foot control achieved non learning methods 
results show marked improvement simple world model stuck states problem 
block real robot learns effects block pushing gripper robot 
block position horizontal surface observed visually smooth straight line robot movement 
world model learned relative pos orientation block relative movement state action change relative pos orientation behaviour interesting feature experiment goal simply undirected aim obtain knowledge 
representation mapping decision tree able produce concise human comprehensible description mapping 
experimentation means random movements 
investigation issues discussed 
decide variables dependent 

quantize range continuous variables 

possible methods automating 
sutton dyna architecture sutton small experiment leads uncertainty approach scale 
discussion number interesting important issues learning control 
dyna framework assumes specification tasks reward reinforcement signals 
assumption learning problem difficult example forbid feedback controllers teachers 
dyna learning achieved system great deal autonomy 
system learns world model generates evaluation function estimates relative qualities states 
evaluation function computed world model dynamic programming graham 
practice avoid periods wasted time robot planning gaining extra knowledge world model dynamic programming occurs incrementally parallel task execution 
example simple maze domain discrete states discrete actions 
controller receives reinforcement zero moves special goal state 
reaches goal state reset starting state 
tactics maximum reward repeatedly trace shortest path start goal tactics dyna advance 
learning optimal behaviour occur quickly run 
experimentation random biased favour actions predicted produce improved reward 
time progresses level decreases 
cope changing environment current best set actions change sutton suggests adding exploration bonus reinforcement encourages rarely visited states occasionally visited case possible improvement 
demonstrated maze wall sections occasionally appear disappear 
investigation briefly mention aspects field described chapter investigated dissertation 
am concerned making robot learning practical world model learning preference action map learning 
important topic tackled curse dimensionality specifically stated goal pieces 
part approach learning method variable resolution profound consequences representation mapping 
initial representation examined ideal variable resolution learning rate learning efficiency poor noise tolerance brittle performance non stationary environment 
method serious weakness cope problems modifications developed lose primary advantages 
second critical aspect avoiding dimensionality problems increasing rate learning nature experimentation method developed estimates utility information gain 
order compensate relative weakness autonomy world model learner compared action map learner investigation focuses complex tasks modularized hierarchy learning tasks 
learns model performing entirely perceived world 
robustness aim robust learning method system get situation achieve goal improvement occur 
learning system situation called stuck 
identified features cause sticking insufficiently general class learnable models 
discussed parametric methods section shown avoided section 
inverse world model 
discussed section 
failing provide adequate experimentation 
discussed chapter 
blame assignment errors hierarchical learning systems 
discussed chapter 
issues addressed important issues addressed investigation 
case considered loss feature requiring system designer generally critical autonomy system 
issues state identification learning non deterministic world model providing inductive simple explanation world model 
issue left system designer scaling state variables discussed section 
chapter sab learning chapter introduces sab learning main idea developed course dissertation 
chapter begins robot problems state introduces concept perception function 
extends dynamic state perceived state transition function 
sequence actions taken sab learner described sab control cycle 
ab learning considering full dynamic control robots examine simpler problems percep tion geometry 
example problem hand eye coordination 
point arm readily identifiable image processing 
example simple implementation subtract images arm obtained moving gripper perform trivial statistics thresholded image 
obtains perceived coordinates hand coordinates hand image 
computer receives values send signals arm 
consist number values sent joint 
specifies encoder units angle prismatic joint length joint take 
signals arrive joint angles automatically slowly adjusted independent specified values achieved 
hand eye coordination action set requested joint angles 
typical hand eye coordination task move perceived hand position target perceived position 
specified directly showing goal controller 
clear achieve task controller needs knowledge relationship perceived image coordinates raw action signal sends 
interesting note necessarily need know relationships absolute real world coordinate system 
example general problem controller needs relationship raw actions meant supply perceived behaviour 
cases domain bibliography drucker atkeson 
task level robot learning juggling tennis ball accurately 
ieee international conference robotics automation 
aha aha kibler albert 
instance learning algorithms 
appear machine learning 
albus albus 
new approach manipulator control cerebellar model ulation controller cmac 
journal dynamic systems measurement control pages september 
albus albus 
data storage cerebellar model articulation controller cmac 
journal dynamic systems measurement control pages september 
albus albus 
brains behaviour robotics 
byte books mcgraw hill 
atkeson 
model control robot manipulator 
press 

connection neural network learning multivariate non linear squares estimation 
neural networks january 
atkeson atkeson 
associative content addressable memories control robots 
memo artificial intelli gence laboratory november 
atkeson atkeson 
local models control movement 
proceedings neural information processing systems conference november 
barto barto sutton anderson 
neuronlike adaptive ele ments learn difficult control problems 
ieee transactions systems man cybernetics 
bentley bentley 
multidimensional divide conquer 
communications cm 
bib breiman breiman friedman olshen stone 
classification regression trees 
wadsworth 
graham graham 
control theory including optimal control 
ellis horwood 

course mathematical analysis 
cambridge university press 
christiansen christiansen mason mitchell 
learning reliable manipulation strategies initial physical models 
ieee conference robotics automation pages 
cleveland cleveland 
locally weighted regression approach regression analysis local fitting 
journal american statistical association september 
cleveland cleveland 
robust locally weighted regression smoothing scatter plots 
journal american statistical association december 
clocksin moore clocksin moore 
experiments adaptive state space robotics 
proceedings th aisb conference brighton 
morgan kaufman april 
utgoff connell utgoff 
learning control dynamic physical system 
proceedings american association artificial intelligence conference 
diederich diederich 
explanation component connectionist inference system 
aiello editor proceedings ecai th european conference artificial intel pages august 
duda hart duda hart 
pattern classification scene analysis 
john wiley sons 
farmer farmer 
predicting chaotic dynamics 
kelso editors dynamic patterns complex systems 
world scientific new jersey 
teleoperation robotics scenarios 
artificial intelligence space station automation pages 
nasa advanced technology advisory committee 
franke franke 
scattered data interpolation tests methods 
mathematics computation january 
bib friedman friedman bentley finkel 
algorithm finding best matches logarithmic expected time 
acm transactions mathematical software september 
fu fu gonzalez lee 
robotics control sensing vision intelligence 
mcgraw hill 
fu fu 
learning control systems review outlook 
ieee transactions automatic control pages april 
gordon grefenstette gordon grefenstette 
explanations empirically derived reactive plans 
proceedings th international conference machine learning june 
gottschalk gottschalk turney mudge 
efficient recognition partially visible objects logarithmic complexity matching technique 
international journal robotics research december 
grosse grosse 
loess multivariate smoothing moving squares 
approxi mation theory vi 
academic press 
hearn hearn 
reduce user manual 
technical report university utah march 
hoel hoel 
mathematical statistics 
wiley international 
holland holland booker goldberg 
classifier systems genetic algorithms 
technical report university michigan cognitive science machine intelligence laboratory 
jordan jacobs jordan jacobs 
learning control unstable system forward modeling 
technical report 
kaelbling kaelbling 
learning functions dnf reinforcement 
proceed ings th international conference machine learning june 
kaelbling kaelbling 
learning embedded systems 
phd 
thesis 
technical report 
tr stanford university department computer science 
kibler kibler aha albert 
instance prediction real valued attributes 
technical report university california irvine 
langley langley bradshaw simon 
rediscovering chemistry bacon system 
machine learning artificial intelligence approach 
morgan kaufmann 
bib 
empirical algorithms non euclidean nearest neigh bout searching 
document cambridge university computer laboratory december 
mel mel 
murphy robot learns doing 
technical report uiucdcs university illinois urbana champaign january 
mel mel 
murphy connectionist approach vision robot motion planning 
technical report university illinois urbana champaign june 
michie chambers michie chambers 
boxes experiment adaptive control 
machine intelligence 
oliver boyd 
michie michie 
personal models rationality 
journal statistical planning inference 
published turing institute technical report 
miller miller kraft 
application general learning algorithm control robotic manipulators 
international journal robotics research 
miller miller 
real time application neural networks sensor control robots vision 
ieee transactions systems man cybernetics july 
minsky minsky 

press 
moore moore 
acquisition dynamic control knowledge robotic manipulator 
proceedings th international conference machine learning june 
murray murray 
numerical methods unconstrained optimization 
academic press 
omohundro omohundro 
efficient algorithms neural network behaviour 
tech nical report university illinois urbana champaign april 
phan phan juang longman 
developments learning control system identification robots structures 
dynamics space structures conference cranfield institute technology june 
poggio girosi poggio girosi 
regularization algorithms learning equivalent multilayer networks 
science 
preparata shamos preparata shamos 
computational geometry 
springer verlag 
bib quinlan quinlan 
learning efficient classification procedures application chess games 
michalski carbonell mitchell editors machine learning artificial intelligence approach 
tioga publishing palo alto 
raibert raibert 
model sensorimotor control learning 
biological cy 
raibert raibert 
motor control learning state space model 
phd 
thesis 
rendell rendell 
new basis state space learning systems successful im plementation 
artificial intelligence pages 
rumelhart mcclelland rumelhart mcclelland 
parallel distributed processing explorations microstructure cognition 
press 
salzberg salzberg 
exemplar learning theory implementation 
technical report aiken computation laboratory harvard university 
michie michie 
controlling black box simulation spacecraft 
technical report turing institute october 
samuel samuel 
studies machine learning game checkers ii progress 
memo stanford artificial intelligence project june 
simons simons van brussel de 
self learning automaton variable resolution high precision assembly industrial robots 
ieee transactions automatic control october 
waltz stanfill waltz 
memory reasoning 
commu cm december 
sutton sutton 
integrated architecture learning planning reacting approximating dynamic programming 
proceedings th international conference machine learning june 
utgoff utgoff 
incremental induction decision trees 
machine learning 
valiant valiant 
theory learnable 
november 
communications cm vogel vogel 
phd 
thesis proposal year report university cambridge computer laboratory may 
bib whitley whitley 
applying genetic algorithms neural network learning 
pro ceedings th aisb conference brighton 
morgan kaufman april 

learning autonomous agent pushing domain 
machine intelligence appear shortly 
oliver boyd 
bib 
