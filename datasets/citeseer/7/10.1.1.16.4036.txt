support vector machine active learning applications text classification simon tong simon tong cs stanford edu computer science department stanford university stanford ca daphne koller koller cs stanford edu computer science department stanford university stanford ca 
support vector machines met significant success numerous real world learning tasks 
machine learning algorithms generally applied randomly selected training set classified advance 
settings option pool active learning 
randomly selected training set learner access pool unlabeled instances request labels number 
introduce new algorithm performing active learning support vector machines algorithm choosing instances request 
provide theoretical motivation algorithm notion version space 
experimental results showing employing active learning method significantly reduce need labeled training instances standard inductive transductive settings 
keywords active learning selective sampling support vector machines classification relevance feedback abbreviations svm support vector machine tsvm transductive support vector machine 
supervised learning tasks labeling instances create training set time consuming costly finding ways minimize number labeled instances beneficial 
usually training set chosen random sampling instances 
cases active learning employed 
learner actively choose training data 
hoped allowing learner extra flexibility reduce learner need large quantities labeled data 
pool active learning introduced lewis gale learner access pool unlabeled data request true class label certain number instances pool 
domains fl kluwer academic publishers 
printed netherlands 
ml long tex simon tong daphne koller reasonable approach large quantity unlabeled data readily available 
main issue active learning finding way choose requests queries pool 
examples situations pool active learning employed ffl web searching 
web wishes search web particular types pages pages containing lists people publications employs number people hand label web pages create training set automatic classifier eventually classify rest web 
human expertise limited resource wishes reduce number pages employees label 
labeling pages randomly drawn web computer requests targeted pages believes informative label 
ffl email filtering 
user wishes create personalized automatic junk email filter 
learning phase automatic learner access user past email files 
interactively brings past email asks user displayed email junk mail 
user answer brings email queries user 
process repeated number times result email filter tailored specific person 
ffl relevance feedback 
user wishes sort database website items images articles personal interest ll know see type search 
computer displays item user tells learner item interesting 
user answer learner brings item database 
number queries learner returns number items database believes interest user 
examples involve induction 
goal create classifier works unseen instances 
third example example transduction 
learner performance assessed remaining instances database totally independent test set 
new algorithm performs pool active learning support vector machines svms 
provide theoretical motivations approach choosing queries experimental results showing active learning svms significantly reduce need labeled training instances 
shall text classification running example 
task determining pre defined topic text document belongs 
text classification important role play especially ml long tex support vector machine active learning applications text classification 
simple linear support vector machine 
svm dotted line transductive svm solid line 
solid circles represent unlabeled instances 
explosion readily available text data 
approaches achieve goal rocchio dumais sebastiani 
furthermore domain svms shown notable success joachims dumais interest see active learning offer improvement highly effective method :10.1.1.11.6124
remainder structured follows 
section discusses svms terms induction transduction 
section introduces notion version space section provides theoretical motivation methods performing active learning svms 
section experimental results real world text domains indicate active learning significantly reduce need labeled instances practice 
conclude section discussion potential significance results directions 

support vector machines 
svms induction support vector machines vapnik strong theoretical foundations excellent empirical successes 
applied tasks handwritten digit recognition object recognition text classification 
shall consider svms binary classification setting 
training data fx vectors space labels fy gamma 
simplest form svms hyperplanes separate training data maximal margin see fig 
vectors lying side hyperplane labeled gamma vectors lying side labeled 
training instances lie closest hyperplane called support vectors 
ml long tex simon tong daphne koller generally svms allow project original training data space higher dimensional feature space mercer kernel operator words consider set classifiers form ff satisfies mercer condition burges write phi delta phi phi delta denotes inner product 
rewrite delta phi ff phi implicitly projecting training data different higher dimensional feature space svm computes ff correspond maximal margin hyperplane choosing different kernel functions implicitly project training data spaces hyperplanes correspond complex decision boundaries original space commonly kernels polynomial kernel delta induces polynomial boundaries degree original space radial basis function kernel gammafl gammav delta gammav induces boundaries placing weighted gaussians key training instances 
majority assume modulus training data feature vectors constant training instances phi fixed quantity phi constant radial basis function kernels assumption effect kernel 
phi constant polynomial kernels require kx constant 
possible relax constraint phi shall discuss section 

svms transduction previous subsection worked framework induction 
labeled training set data task create classifier performance unseen test data 
addition regular induction svms transduction 
set labeled unlabeled data 
learning task assign labels unlabeled data accurately possible 
svms perform transduction finding hyperplane maximizes margin relative labeled unlabeled data 
see example 
transductive svms text classification joachims attaining improvements precision recall breakeven performance regular inductive svms 
ml long tex support vector machine active learning applications text classification 
version space duality 
surface hypersphere represents unit weight vectors 
hyperplanes corresponds labeled training instance 
hyperplane restricts area hypersphere consistent lie 
version space surface segment hypersphere closest camera 
svm classifier version space 
dark embedded sphere largest radius sphere center lies version space surface intersect hyperplanes 
center embedded sphere corresponds svm radius proportional margin svm training points corresponding hyperplanes touches support vectors 

version space set labeled training data mercer kernel set hyperplanes separate data induced feature space call set consistent hypotheses version space mitchell 
words hypothesis version space training instance label gamma 
formally definition 
set possible hypotheses ae delta phi kwk oe parameter space simply equal version space defined ff ng notice set hyperplanes bijection unit vectors hypotheses redefine fw kwk delta phi ng note version space exists training data linearly separable feature space 
require linear separability training ml long tex simon tong daphne koller data feature space 
restriction harsh 
feature space high dimension cases results data set linearly separable 
second noted shawe taylor cristianini possible modify kernel data new induced feature space linearly separable exists duality feature space parameter space vapnik herbrich shall take advantage section points correspond hyperplanes vice versa 
clearly definition points correspond hyperplanes intuition converse observing training instance feature space restricts set separating hyperplanes ones classify correctly 
fact show set allowable points restricted lie side hyperplane formally show points correspond hyperplanes suppose new training instance label separating hyperplane satisfy delta phi 
viewing normal vector hyperplane think phi normal vector hyperplane delta phi delta phi defines half space furthermore delta phi defines hyperplane acts boundaries version space notice version space connected region surface hypersphere parameter space 
see example 
svms find hyperplane maximizes margin feature space way pose optimization task follows maximize min fy delta phi subject kwk delta phi having conditions kwk delta phi cause solution lie version space 
view problem finding point version space maximizes distance min fy delta phi duality feature parameter space phi phi unit normal vector hyperplane parameter space 
constraints delta phi hyperplanes delimit version space 
expression delta phi regarded theta distance point hyperplane normal vector phi done redefining training instances positive regularization constant 
essentially achieves effect soft margin error function cortes vapnik commonly svms :10.1.1.15.9362
permits training data linearly non separable original feature space 
ml long tex support vector machine active learning applications text classification want find point version space maximizes minimum distance delineating hyperplanes 
svms find center largest radius hypersphere center placed version space surface intersect hyperplanes corresponding labeled instances 
normals hyperplanes touched maximal radius hypersphere phi distance delta phi minimal 
original dual view regarding unit normal vector svm phi points features space see hyperplanes touched maximal radius hypersphere correspond support vectors labeled points closest svm hyperplane boundary 
radius sphere distance center sphere touching hyperplanes delta phi support vector 
viewing unit normal vector svm phi points feature space distance delta phi theta distance support vector phi hyperplane normal vector margin svm divided radius sphere proportional margin svm 

active learning pool active learning pool unlabeled instances 
assumed instances independently identically distributed underlying distribution labels distributed conditional distribution 
unlabeled pool active learner components 
component classifier gamma trained current set labeled data possibly unlabeled instances 
second component querying function current labeled set decides instance query 
active learner return classifier query online learning fixed number queries 
main difference active learner passive learner querying component brings issue choose unlabeled instance query 
seung approach queries points attempt reduce size version space possible 
need definitions proceed ml long tex simon tong daphne koller definition 
area surface area version space occupies hypersphere kwk 
definition 
active learner denote version space queries 
th query define gamma fw gamma delta phi fw delta phi gamma denote resulting version spaces query labeled gamma respectively 
wish reduce version space fast possible 
intuitively way doing choose query halves version space 
formally lemma motivate instances query lemma 
suppose input space finite dimensional feature space induced kernel parameter space suppose active learner queries instances corresponding hyperplanes parameter space halves area current version space 
active learner 
denote version spaces queries respectively 
denote set conditional distributions sup area sup area strict inequality exists query ig halve version space gamma proof 
proof straightforward 
learner chooses query instances halve version space 
area area matter labeling query points 
denote dimension feature space dimension parameter space denote surface area unit hypersphere dimension conditional distribution area sr suppose query instance halves area version space 
number queries chooses query point halve current version space gamma correspond labeling cause larger half version space chosen 
loss generality assume area gamma area gamma 
note area gamma area sr area gamma sr ml long tex support vector machine active learning applications text classification consider conditional distribution gamma ae distribution area gammak gamma area gamma sup area sup area lemma says number queries minimizes maximum expected size version space maximum taken conditional distributions suppose unit parameter vector corresponding svm obtained known actual labels data pool 
know lie version spaces oe oe denotes version space queries 
shrinking size version space possible query reducing fast possible space lie 
svm learn limited number queries lie close willing assume hypothesis lying generates data generating hypothesis deterministic data noise free strong generalization performance properties algorithm halves version space shown freund 
example show generalization error decreases exponentially number queries 
discussion provides motivation approach query instances split current version space equal parts possible 
unlabeled instance pool practical explicitly compute sizes new version spaces gamma version spaces obtained labeled gamma respectively 
ways approximating procedure 
ffl simple margin 
recall section data fx labels fy svm unit vector obtained data center largest hypersphere fit inside current version space position version space clearly depends shape region approximately center version space 
test unlabeled instances ml long tex simon tong daphne koller 
simple margin query 
simple margin query 

maxmin margin query svms margins gamma shown 
ratio margin query svms margins gamma shown 
pool see close corresponding hyperplanes come centrally placed closer hyperplane point centrally placed version space bisects version space 
pick unlabeled instance pool hyperplane comes closest vector unlabeled instance shortest distance hyperplane vector simply distance feature vector phi hyperplane easily computed jw delta phi results natural rule learn svm existing labeled data choose instance query instance comes closest hyperplane presents illustration 
stylized picture flattened surface unit weight vector hypersphere appears 
white area version space bounded solid lines corresponding labeled instances 
dotted lines represent unlabeled instances pool 
circle represents largest ml long tex support vector machine active learning applications text classification radius hypersphere fit version space 
note edges circle touch solid lines just dark sphere meet hyperplanes surface larger hypersphere meet surface 
instance closest svm choose query ffl maxmin margin 
simple margin method rough approximation 
relies assumption version space fairly symmetric centrally placed 
demonstrated theory practice assumptions fail significantly herbrich 
careful may query instance hyperplane intersect version space 
maxmin approximation designed somewhat overcome problems 
data fx labels fy svm unit vector center largest hypersphere fit inside current version space radius hypersphere proportional size margin radius indication size version space vapnik 
suppose candidate unlabeled instance pool 
estimate relative size resulting version space gamma labeling gamma finding svm obtained adding labeled training data looking size margin gamma perform similar calculation relabeling class finding resulting svm obtain margin want equal split version space wish area gamma area similar 
consider min area gamma area 
small area gamma area different 
consider min gamma approximation choose query quantity largest 
maxmin query algorithm follows unlabeled instance compute margins gamma svms obtained label gamma respectively choose query unlabeled instance quantity min gamma greatest 
figures show example comparing simple margin maxmin margin methods 
ffl ratio margin 
method similar spirit maxmin margin method 
gamma indications sizes gamma shall try take account fact current version space may quite elongated pool gamma may small simply shape version space 
ease notation loss generality shall assume constant proportionality radius equal margin 
ml long tex simon tong daphne koller look relative sizes gamma choose query min gamma gamma largest see 
methods approximations querying component halves version space 
performing number queries return classifier learning svm labeled instances 
margin indication version space size irrespective feature vectors constant modulus 
explanation maxmin ratio methods holds constraint modulus training feature vectors 
simple method training feature vectors constant modulus motivating explanation longer holds svm longer viewed center largest allowable sphere 
simple method alternative motivations proposed campbell 
require constraint modulus 
inductive learning performing number queries return classifier learning svm labeled instances 
transductive learning querying number instances return classifier learning transductive svm labeled unlabeled instances 

experiments empirical evaluation methods real world text classification domains reuters data set newsgroups data set 

reuters data collection experiments reuters data set commonly collection newswire stories categorized hand labeled topics 
news story number topic labels corn wheat corporate acquisitions 
note topics overlap articles belong category 
articles modapte split data considered top frequently occurring topics 
learned different binary classifiers distinguish topic 
document represented stemmed tfidf weighted word frequency vector 
vector unit modulus 
list common words words occurring documents ignored 
representation document vectors dimensions 
obtained www research att com lewis 
rainbow www cs cmu edu mccallum bow text processing 
ml long tex support vector machine active learning applications text classification random simple maxmin ratio labeled training set size test set accuracy full ratio maxmin simple random random simple maxmin ratio labeled training set size precision recall breakeven point full ratio maxmin simple random 
average test set accuracy frequently occurring topics pool size 
average test set precision recall breakeven point frequently occurring topics pool size 
table average test set accuracy top frequently occurring topics frequent topic trained labeled documents 
boldface indicates place 
topic simple maxmin ratio equivalent random size earn sigma sigma sigma acq sigma sigma sigma money fx sigma sigma sigma grain sigma sigma sigma crude sigma sigma sigma trade sigma sigma sigma interest sigma sigma sigma ship sigma sigma sigma wheat sigma sigma sigma corn sigma sigma sigma compared querying methods inductive learning setting 
test set consisted documents 
topics performed 
created pool unlabeled data sampling documents remaining data removing labels 
randomly selected documents pool give initial labeled training set 
document desired topic document topic 
gave learner unlabeled documents labeled documents 
fixed number queries asked learner return classifier svm ml long tex simon tong daphne koller table ii 
average test set precision recall breakeven point top frequently occurring topics frequent topic trained labeled documents 
boldface indicates place 
topic simple maxmin ratio equivalent random size earn sigma sigma sigma acq sigma sigma sigma money fx sigma sigma sigma grain sigma sigma sigma crude sigma sigma sigma trade sigma sigma sigma interest sigma sigma sigma ship sigma sigma sigma wheat sigma sigma sigma corn sigma sigma sigma polynomial kernel degree learned labeled training documents 
tested classifier independent test set 
procedure repeated times topic results averaged 
considered simple margin maxmin margin ratio margin querying methods random sample method 
random sample method simply randomly chooses query point unlabeled pool 
method reflects happens regular passive learning setting training set random sampling data 
measure performance metrics test set classification error stay compatible previous reuters corpus results precision recall breakeven point joachims :10.1.1.11.6124
precision percentage documents classifier labels relevant really relevant 
recall percentage relevant documents labeled relevant classifier 
altering decision threshold svm trade precision recall obtain precision recall curve test set 
precision recall breakeven point number summary graph point precision equals recall 
figures average test set accuracy precision recall breakeven points topics vary number queries permitted 
horizontal line performance level achieved svm trained labeled documents comprising pool 
reuters corpus active learning methods perform identi svm transductive svm learning joachims svmlight www ai informatik uni dortmund de thorsten svm light html 
ml long tex support vector machine active learning applications text classification labeled training set size test set accuracy full ratio random balanced random random simple ratio labeled training set size precision recall breakeven point full ratio random balanced random 
average test set accuracy frequently occurring topics pool size 
average test set precision recall breakeven point frequently occurring topics pool size 
labeled training set size test set accuracy active pool active pool random labeled training set size active pool active pool random 
average test set accuracy frequently occurring topics pool sizes 
average breakeven point frequently occurring topics pool sizes 
cally little notable difference distinguish 
method appreciably outperforms random sampling 
tables ii show test set accuracy breakeven performance active methods asked just labeled instances initial random instances seen labeled instances 
demonstrate active methods perform similarly reuters data set queries maxmin ratio showing slight edge performance 
columns table interest 
show approximately instances needed ml long tex simon tong daphne koller random achieve level performance ratio active learning method 
instance passive learning average requires times data achieve comparable levels performance active learning methods 
tables indicate active learning provides benefit infrequent classes particularly measuring performance precision recall breakeven point 
observation noted previous empirical tests mccallum nigam :10.1.1.13.8629
noticed approximately half queries active learning methods asked tended turn positively labeled regardless true proportion positive instances domain 
investigated gains active learning methods regular random sampling due biased sampling 
created new querying method called randomly sample equal number positive negative instances pool 
obviously practice ability randomly sample equal number positive negative instances having label entire pool instances may may reasonable depending domain question 
figures show average accuracy breakeven point method compared ratio active method regular random method reuters dataset pool instances 
ratio random curves shown figures 
maxmin simple curves omitted ease legibility 
method better precision recall breakeven performance regular random method matched outperformed active method 
classification accuracy method initially extremely poor performance worse pure random guessing consistently significantly outperformed active method 
indicates performance gains active methods merely due ability bias class instances queries 
active methods choosing special targeted instances approximately half instances happen positive labels 
figures show average accuracy breakeven point ratio method different pool sizes 
clearly random sampling method performance affected pool size 
graphs indicate increasing pool unlabeled data improve accuracy breakeven performance active learning 
quite intuitive active method able take advantage larger pool potential queries ask targeted questions 
investigated active learning transductive setting 
queried points usual method simple random returned transductive svm trained labeled remaining unlabeled data pool 
breakeven point tsvm computed ml long tex support vector machine active learning applications text classification inductive passive transductive passive inductive active transductive active labeled training set size precision recall breakeven point transductive active inductive active transductive passive inductive passive 
average pool set precision recall breakeven point frequently occurring topics pool size 
gradually altering number unlabeled instances wished tsvm label positive 
re learning tsvm multiple times computationally intensive 
setting transduction performance classifier measured pool data separate test set 
reflects relevance feedback transductive inference example 
shows tsvm provides slight advantage regular svm querying methods random simple comparing breakeven points 
graph shows active learning provides notably benefit transduction tsvm random querying method needs queries achieve breakeven performance regular svm simple method seen labeled instances 

newsgroups data collection experiments second data collection ken lang newsgroups collection comp groups discarding usenet headers subject lines 
processed text documents exactly resulting vectors dimensions 
placed half documents aside independent test set repeatedly randomly chose pool documents remaining instances 
performed runs topics averaged results 
test set accuracy measure performance 
contains learning curve averaged results comp topics active learning methods obtained www cs cmu edu 
ml long tex simon tong daphne koller random simple maxmin ratio labeled training set size test set accuracy full ratio maxmin simple random ratio maxmin simple random labeled training set size test set accuracy full ratio maxmin simple random 
average test set accuracy comp topics pool size 
average test set accuracy comp sys ibm pc hardware pool size 
random sampling 
horizontal line indicates performance svm trained entire pool 
appreciable difference maxmin ratio methods newsgroups comp sys ibm pc hardware comp os ms windows misc simple active learning method performs notably worse maxmin ratio methods 
shows average learning curve comp sys ibm pc hardware topic 
fifteen cent runs newsgroups simple method misled performed extremely poorly instance achieving accuracy training instances worse random guessing 
indicates simple querying method may unstable methods 
reason simple method tends explore feature space aggressively active methods ignoring entire clusters unlabeled instances 
simple method takes queries considers instance unlabeled cluster maxmin ratio query point unlabeled cluster immediately 
maxmin ratio appear stable computationally intensive 
large pool instances require svms learned query 
computational cost incurred number queries asked large 
cost training svm grows size labeled training set training svm costly minute generate th query sun workstation pool documents 
number asked queries small large pool size maxmin ratio fairly fast ml long tex support vector machine active learning applications text classification ratio hybrid simple labeled training set size ratio hybrid simple 
simple example querying unlabeled clusters 
macro average test set accuracy comp os ms windows comp sys ibm pc hybrid uses ratio method queries simple rest 
table iii 
typical run times seconds active methods newsgroups dataset query simple maxmin ratio hybrid seconds query training svm fairly cheap 
interestingly queries simple suffer lack aggressive exploration 
motivates hybrid method 
maxmin ratio queries simple method rest 
experiments hybrid method show maintains stability maxmin ratio methods allowing scalability simple method 
compares hybrid method ratio simple methods newsgroups simple method performed poorly 
test set accuracy hybrid method virtually identical ratio method hybrid method run time simple method indicated table iii 
ml long tex simon tong daphne koller training set size test set accuracy svm simple active mn algorithm labeled training set size test set accuracy svm simple active svm passive lt algorithm winnow active lt winnow passive 
average test set accuracy comp hierarchy 
average test set accuracy top reuters categories 

related studies active learning classification 
query committee algorithm seung freund uses prior distribution hypotheses 
general algorithm applied domains classifiers specifying sampling prior distribution natural 
probabilistic models dagan engelson specifically naive bayes model text classification bayesian learning setting mccallum nigam :10.1.1.13.8629
naive bayes classifier provides interpretable model principled ways incorporate prior knowledge data missing values 
typically perform discriminative methods svms particularly text classification domain joachims dumais :10.1.1.11.6124
re created mccallum nigam experimental setup newsgroups corpus compared reported results algorithm mn algorithm 
algorithm require positive negative instance seed classifier 
seeding active svm positive negative instance give active svm unfair advantage active svm randomly sampled documents queries 
virtually guaranteed training set contained positive instance 
active svm proceeded query instances actively simple method 
line experimental setup queries asked time achieved picking instances closest current hyperplane 
mn algorithm permitted perform active querying query 
compares mccallum nigam reported results 
despite ml long tex support vector machine active learning applications text classification naive initialization policy active svm despite fact mn algorithm uses em dempster take advantage unlabeled documents pool graph indicates active svm performance significantly better mn algorithm 
alternative committee approach qbc explored liere tadepalli 
algorithm lt algorithm lacks theoretical justifications query committee algorithm successfully committee active learning method winnow classifiers text domain 
produced emulating experimental setup reuters data set compares reported results 
active svm initialized number randomly selected documents permitted perform active learning 
graph shows active svm accuracy significantly better lt algorithm 
lewis gale introduced uncertainty sampling applied text domain logistic regression companion decision trees lewis catlett :10.1.1.52.2415
simple querying method svm active learning essentially uncertainty sampling method choose instance current classifier uncertain provided substantially justification algorithm effective 
noted performance uncertainty sampling method variable performing quite poorly occasions 
studies campbell schohn cohn independently developed simple method active learning support vector machines provided different formal analyses 
campbell cristianini smola extend analysis simple method cover soft margin svms cortes vapnik linearly non separable data :10.1.1.15.9362
schohn cohn note interesting behaviors active learning curves presence outliers 

introduced new algorithm performing active learning svms 
advantage duality parameter space feature space arrived algorithms attempt reduce version space possible query 
shown empirically techniques provide considerable gains inductive transductive settings cases shrinking need labeled instances order magnitude cases reaching performance achievable entire pool having seen fraction data 
furthermore larger pools unlabeled data improve quality resulting classifier 
ml long tex simon tong daphne koller main methods simple method computationally fastest 
simple method unstable approximation witnessed performed poorly newsgroup topics 
asking query expensive relative computing time maxmin ratio may preferable 
cost asking query relatively cheap emphasis placed fast feedback simple method may suitable 
case shown methods learning substantially outperform standard passive learning 
furthermore experiments hybrid method indicate possible combine benefits ratio simple methods 
leads directions interest 
studies noted gains computational speed obtained expense generalization performance querying multiple instances time lewis gale mccallum nigam :10.1.1.13.8629:10.1.1.16.3103
viewing svms terms version space gives insight approximations may provide guide multiple instances better query 
instance suboptimal query instances version space hyperplanes fairly parallel 
simple method blindly choosing query instances closest current svm may better query instances close current svm hyperplanes version space fairly perpendicular 
similar tradeoffs ratio maxmin methods 
bayes point machines herbrich approximately find center mass version space 
simple method point svm point version space may produce improvement performance stability 
monte carlo methods estimate version space areas may give improvements 
way viewing strategy choosing halve version space essentially placed uniform distribution current space consistent hypotheses wish reduce expected size version space fast possible 
maintaining uniform distribution consistent hypotheses plausible addition prior knowledge hypotheses space may allow modify query algorithm provided better strategy 
furthermore framework introduced mcallester considers effect prior knowledge generalization bounds approach may lead theoretical guarantees modified querying algorithms 
ratio maxmin methods computationally expensive step unlabeled data instances learn svm possible labeling 
temporarily modified data sets differ instance original labeled data set ml long tex support vector machine active learning applications text classification envisage learning svm original data set computing incremental updates obtain new svms poggio possible labelings unlabeled instances 
hopefully able obtain efficient implementation ratio maxmin methods allow active learning algorithms scale larger problems 
supported darpa information assurance program subcontract sri international aro daah muri program integrated approach intelligent systems 
burges tutorial support vector machines pattern recognition 
data mining knowledge discovery 
campbell cristianini smola query learning large margin classifiers 
proceedings seventeenth international conference machine learning 
poggio incremental decremental support vector machine learning 
conference advances neural processing systems 
cortes vapnik support vector networks 
machine learning 
dagan engelson committee sampling training probabilistic classifiers 
proceedings twelfth international conferenceon machine learning 
pp 
morgan kaufmann 
dempster laird rubin maximum likelihood incomplete data em algorithm 
journal royal statistical society 
dumais platt heckerman sahami inductive learning algorithms representations text categorization 
proceedings seventh international conference information knowledge management 
acm press 
freund seung shamir tishby selective sampling query committee algorithm 
machine learning 
herbrich graepel campbell bayes point machines estimating bayes point kernel space 
international joint conferenceon artificial intelligence workshop support vector machines 
pp 

joachims text categorization support vector machines 
proceedings european conference machine learning 
springer verlag 
joachims transductive inference text classification support vector machines 
proceedings sixteenth international conference machine learning 
pp 
morgan kaufmann 
lewis catlett heterogeneous uncertainty sampling supervised learning 
proceedings eleventh international conference machine learning 
pp 
morgan kaufmann 
ml long tex simon tong daphne koller lewis gale sequential algorithm training text classifiers 
proceedingsof seventeenth annual international acm sigir conferenceon research development information retrieval 
pp 
springer verlag 
liere active learning committees approach efficient learning text categorization linear threshold algorithms 
oregon state university ph thesis 
liere tadepalli active learning committees text categorization 
proceedings aaai 
pp 

mcallester pac averaging 
computational 
mccallum nigam employing em pool active learning text classification 
proceedings fifteenth international conference machine learning 
morgan kaufmann 
mitchell generalization search 
artificial intelligence 
rocchio relevance feedback information retrieval 
salton ed smart system experiments automatic document processing 
prentice hall 
schohn cohn active learning support vector machines 
proceedings seventeenth international conference machine learning 
sebastiani machine learning automated text categorisation 
technical report iei istituto di dell informazione 
seung opper sompolinsky query committee 
proceedings fifth workshop computational learning theory 
pp 
morgan kaufmann 
shawe taylor cristianini results margin distribution 
proceedings twelfth annual conference computational learning theory 
pp 

vapnik estimation dependences empirical data 
springer verlag 
vapnik statistical learning theory 
wiley 
ml long tex 
