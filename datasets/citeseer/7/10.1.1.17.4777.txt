svm boosting class gunnar bernhard sch kopf sebastian mika klaus robert gmd str 
berlin germany research street cambridge uk university potsdam am potsdam mika klaus 
grad 
de bsc scientist 
corn november show equivalence mathematical programs support vector sv algorithm translated equivalent boosting algorithm vice versa 
exemplify translation procedure new algorithm class leveraging starting class support vector machines svm step unsupervised learning boosting framework 
building called barrier methods known theory constrained optimization returns function written convex combination basis hypotheses characterizes test point generated distribution underlying training data 
simulations class classification problems demonstrate usefulness approach 
boosting methods successfully applied classification problems drucker lecun maclin opitz schwenk bengio bauer kohavi dietterich regression estimation duffy helmbold bennett 
high accuracy ease implementation wide applicability placed standard toolbox machine learning neural networks kernel learning methods support vector machines svms vapnik 
aims point equivalence mathematical programs underlying boosting svms formalizing correspondence hinted researchers boosting community schapire :10.1.1.31.2869
show hypothesis set boosting corresponds choice particular kernel svms vice versa 
illustrate correspondence interesting application class classification trains unlabelled data trying assess test point belong distribution underlying training data cf 
scholkopf tax duin 
problem unsupervised learning thought simplified version problem density estimation 
class classification far studied context svms scholkopf tax duin campbell bennett boosting 
potential advantage boosting techniques prior knowledge choice base hypotheses weak learners 
easier incorporate prior knowledge weak learner kernel function 
specifically context class classification advantageous interpretable simple base hypotheses decision stumps linear cuts lets extract simple interpretable rules decision boundary cf 
section 
section shows connection boosting svms 
relation derive linear programming formulation class problem section develop boosting technique section building results connects boosting barrier optimization techniques 
experimental section shows validity approach 
brief follows 
svms boosting subsections start review basic ideas corresponding optimization problems svms boosting methods 
show relation approaches discuss different norms svms boosting 

xv denotes training sample size set 
see appendix summary notation 
support vector machines consider dimensional feature space subset spanned mapping support vector sv setting corresponds mercer kernel implicitly computing dot product goal svms find separating hyperplane described vector feature space fw finding hyperplane casted quadratic optimization problem min subject yn 

simplicity omitted bias offset 
selects hyperplane minimal vc capacity vapnik case achieved maxi defined minimum distance margin 
margin training point separating hyperplane 
generally margin example defined distance pattern separating hyperplane 
positive margin corresponds correct classification positive margin greater confidence schapire schapire singer vapnik learned classifier cor rect :10.1.1.31.2869
connection margin pattern term 
theorem mangasarian 
point plane 
denotes distance plane measured respect dual norm gq maximizing minimal gq training patterns sep hyperplane equivalent solving optimization problem mangasarian max subject 
conditions theorem 
equivalent solving problem 
case shown vapnik solution expressed linear combination training patterns positive coefficients xn 
coefficients sparse hold weight vector feature space 
boosting consider boosting algorithms 
particular focus algorithm breiman 
adaboost freund schapire similar connections see discussions 
class base hypotheses 

boosting seeks linear combination ej vector hypothesis weights 
done calling base learner selects hypothesis ht iteration ii computing corresponding hypothesis weight cf 
freund schapire 
base learner training set set weights ida 
dtv updated iteration starting uniform distribution 
ideally finds hypothesis minimizes weighted training error cf 
weighted minimization cf 
weighted minimization breiman 
algorithm hypothesis weights normalized sum 
details weights wt computed adaboost arc gv cf 
freund schapire breiman respectively 
arc gv shown asymptotically breiman find linear combination solves linear optimization problem lp commonly stated max wer subject 
denotes non negative half space 
solution sparse base hypotheses combined eq 
bennett mangasarian chen bradley 
number bounded number patterns independently size usually connection boosting svms common folklore statement boosting svms essentially way measure margin way optimize weight vector svms norm boosting employs norm 
think solely influences imposed regularization 
connection precise explicit show unique strategies handle high infinite dimensional spaces 
svms need norm implicitly compute scalar products feature space help kernel trick 
norm expressed terms scalar products 
boosting contrast performs computation explicitely feature space 
known prohibitive solution sparse feature space high infinite dimensional depending size base hypothesis set 
norm sparseness inducing regularization functional 
mangasarian mandatory 
boosting relies fact hypotheses necessary express solution boosting tries find iteration 
basically boosting considers salient dimensions feature space spanned believed true adaboost problem separable cf 
freund schapire 
added constraints context boosting 
assumes complementation closeness hypothesis set adding constraints change solution 
roughly speaking reason induced sparseness fact vectors far coordinate axes larger respect norm respect norms 
example consider vectors 
norm ii norm note norm regularizer optimal solution vertex solution expressed tends sparse 
hypotheses efficient 
level mathematical programs see relation boosting svms clearly similar 
explicit note hypothesis set implies mapping hi 
hj kernel hj hj 
hypothesis set spans feature space furthermore feature space spanned mapping corresponding hypothesis set constructed hj pj pj denotes projection th dimension feature space 
class svms class lps class svms goal class svm svm approach scholkopf find hyperplane separates unlabeled training data origin threshold estimates function fv decides pattern belongs class fv find threshold quadratic program min wl er pe subject 
xn 

optimization problem incorporates intuition large fraction training patterns satisfying fv having small sv type regularization term vapnik 
parameter controls trade meaning clearer section 
different approach called support vector data description svdd taken tax duin seeking hyperplane hypersphere contains possible training data keeping radius small 
radial basis function rbf kernels shown equivalent current approach scholkopf 
linear programming approach intend consider high dimensional feature spaces prohibitive non sparse solution vectors explicitly carrying computations want forced take dimensions account decide new pattern belongs class 
propose modify regularization term weight vector gl norm feature space gl norm induces sparsity see discussion section 
obtain optimization problem min vp vc cr pcr subject fix objective contra linear optimal solution trivial unbounded 
note exists version uses 
shown possess solution set provided optimal solution satisfies 
furthermore worthwhile observe solutions shown sch kopf proposition 
assume solution satisfies statements hold upper bound fraction outliers points ii lower bound fraction points inside outside estimated region points 
proof analogous scholkopf 
ratsch 

emphasize particular unsupervised learning important interpretable regularization model selection case poses difficult problem 
practice parameter selected knowledge problem hand see section 
similar algorithm proposed campbell bennett finds additional term objective 
holds case derivations boosting algorithm sections adapted easily approach 
eq 
mapping induces hypothesis space hi 

easily replace constraints 
furthermore assume hypothesis space complementation closed enforce changing problem get linear optimization problem min vp pe 
looks similar solved gv cf 
eq 

leveraging approaches section obtained linear programming formulation class problem serve basis deriving boosting algorithm 
despite fact algorithm pac boosting property schapire algorithms described adaboost algorithm works similar adaboost 
confuse terms leveraging 
principally leveraging approach selects iteratively hypothesis time updates weight vector implemented different ways 
essentially alternatives ideally solves optimization problem hypotheses selected iterations proposed grove schuurmans kivinen warmuth bennett 

greedy approach original adaboost arc gv algorithm ii updates weight hypothesis selected minimizing exponential cost function breiman friedman mason 
shown relates coordinate descent methods barrier optimization techniques bregman algorithm bregman censor zenios 
section start considering examples categories illustration develop new method 
arc column generation adaboost note formulation eq 
similar problems underlying generation adaboost cg adaboost bennett 
difference simply label appears additional factor left hand side constraints cf 
eq 
available unsupervised learning 
approach arc observation arc gv algorithm maximizes minimum margin 
ideas 
rain max problem max 

advantage readily de fined soft margin sense 
left hand side inequalities 
optimal values chosen property holds cf 
proposition max fw margin con arc gv algorithm real margin 
soft margins arc gv computes weighting training set obtain hypothesis computes weight minimizing certain error function 
arc gv exploited optimization tool solve min max problem large hypothesis classes 
theoretically completely clear approach converges optimal solution 
column generation approach adaboost bennett starts dual problem linear program lp similar uses technique known optimization community column generation 
just briefly recapitulate algorithm apply modified problem 
dual cf 
appendix derivation max en xn 
dj constraint hypothesis box constraints di simplex 
algorithm iteratively selects hypothesis hj column constructs subsets hi 
hi solves problem exactly simplex method 
original boosting gorithm iteration generates pattern weighting find hypothesis 
easily seen hypothesis progress largest called edge supervised learning breiman 
clearly hypothesis added problem solved 
due smart selection rule sparseness solution convergence algorithm faster point hypothesis left change current solution 
holds en lem 
note careful definition primal problem case infinite hypothesis spaces regression setting considered 

far new approaches boosting techniques supervised learning 
approach advantage easy implement guarantee convergence optimal solution 
second approach guaranteed convergence empirically fast performance bennett relies availability lp solver 
sections propose new method 
barrier optimization techniques combine algorithms advantages 
class barrier algorithm briefly recall basic statements barrier optimization 
details see 

goal barrier optimization ease notation denote ht hypothesis selected th iteration hj th hypothesis hypothesis set notation corresponding weights wt respectively 
find optimal solution problem min ci convex function non empty convex set feasible solutions 
problem solved called barrier function 
exp barrier function cf 
particular useful choice purposes exp cid denotes penalty parameter 
finding sequence unconstrained minimizers sequence ts shown minimizers converge global solution original problem 
conditions convergence relaxed shown proposition 
assume ci differentiable convex functions 
st minimizer exp ci xt ll limit point xt tc solution 
apply exp barrier technique problem 
get barrier functional vp en exp zj xn 
omitted terms corresponding constraints maintain outside 
order reduce number variables optimized find optimal slack variables minimizing setting ff solving obtain closed form solution log exp log expected satisfies lira max sn 
note difficult find expression optimal independent see exists simple way problem 
computing proving convergence algorithm substitute wj ii ih implement restricting sech space 
cle section 
usually barrier algorithm optimize parameters directly desired precision reached cf 
proposition 
requires know hypotheses advance 
propose leveraging algorithm finds new hypothesis ht weight wt wh iteration 
parameter wt determined iteration 
proving convergence exploits needs minimizer 
turn base learner helps estimate norm gradient 
selecting hypothesis gradient respect wj computed fw xn hj xn tm note maximum duality gap primal problem dual non positive see appendix details reduce ez iteratively fixed may choose hypothesis ht argmax dh argmax wn denotes weight hypothesis complementation closeness 
selection rule implements gauss method luenberger full problem details proof theorem 
furthermore finds hypothesis responsible size duality gap 
note supervised boosting techniques corresponds minimizing training error pattern weighting order find best hypothesis unsupervised setting scalar product weight vector hypothesis weight needs maximized 
finding weights hypothesis ht weight wt computed essentially works adaboost argmin wes uses optimal values 
space chosen constraints fulfilled st wt details see lemma proof theorem 
th unit vector 
wt dimensional subset dimensional probability simplex st defined freely change coefficient current hypothesis takes care constraints 
may chose larger minimization implemented efficiently 
may improve convergence speed practice 
analysis consider simpler case 
convergence complete algorithm summarized algorithm pseudo code 
theorem shows convergence proposed algorithm solution class lp theorem 
suppose complementation closed bounded finite hypothesis set base learner called algorithm algorithm returns hypothesis fulfills 
beta function lim 
output algorithm asymptotically converge optimal solution 
sketch proof idea proof follows show base learner selects hypothesis corresponds weight largest gradient 
convergence properties coordinate descent methods particular gauss algorithm conclude gradient respect hypothesis weight decreases fixed iff gradient small decreased iteratively get 
apply proposition limiting case shown desired convergence 
special care needs taken additional constraints implement iw iw elementwise absolute value 
get objective exp xn 
derivatives optimal wl easily com puted lemma 
oe iw lll sign wj fw xn hj algorithm class leveraging algorithm argument sample xl 
xn 
iterations returns convex combination function rage set set ht dt ht xn fw xn argmin ez wc pt argmin endfor return function set return tp exp exp proof see appendix duality arguments cf 
appendix edge combined hypothesis greater equal best single hypothesis 
complementation closeness hypothesis exists hypothesis edge opposite sign 
selecting hypothesis choosing weight maximal gradient non positive 
shows algorithm fixed implements coordinate descent method variables objective lw lll 
iteration coordinate wj maximal gradient coordinate optimized 
standard convergence properties algorithms cf 

cf 
luenberger norm gradient respect vanish asymptotically smaller finite number iterations 
sense base learner helps estimate norm remaining gradient size duality gap 
condition algorithm asymptotically decreased iff gradient gets small 
constructed sequences fulfill proposition eventually proven convergence 
examples base learner section briefly look base learners algorithm 
solving certain problem depends course problem hand 
important note approach general specialized hypothesis classes designed particular problems performance interpretability desired 
consider base learners linear combinations fixed functions kq functions kq kernel functions kq xq centered training patterns 
find optimal hypothesis weighting term needs maximized cf 

needs bounded kq 
need bounded may maximize const paragraphs consider special cases particular simple solutions exist 
sparse combinations case minimization simple solution 
shown el maxq gq qq solution maximum gq maxq gq unique 
implies case eventually useful optimize linear combination functions kq 
selected anyway 
simpler kq 
hypothesis set obtains solution 
sin el 
concept active kernels proposed known moving centers known moving centers poggio girosi optimizing centers kernel functions computed 
hypothesis space infinite dimensional needs deeper treatment primal dual problems convergence corresponding optimization algorithms cf 
el 

restrict finite hypothesis case 
squares assumption ll cons 
pointed bennett solved minimizing error weight vector hypothesis output 
usually assumption hold 
may solve argmin regularizer effectively bounds norm cf 
cf 
smola 
problem simple solution aq kq 
case particular interesting neural networks linearly combine units output layer 
structure network may implement prior knowledge problem hand 
decision stumps simpler highly interpretable hypotheses class consider decision stumps frequently boosting community slightly different form hc set possible cuts 
dim slope parameter 
hypothesis soft decision stump considering coordinate splitting position essentially linear combinations hypotheses describe boxes input space subspaces cut directions coordinate axes interpreted easily 
experiments experiments illustrate algorithms detecting outliers 
conduct experiments evaluate result objective measurement outliers detected normal patterns classified outliers need labels evaluation purposes 
toy data toy experiment show basic properties algorithms 
data cf 
generated uniform distributions differently sized supports 
leads large density patterns center lower density periphery 
toy model considered outliers 
decision stumps form described section 
shows result barrier algorithm cf 
algorithm cg algorithm described section 
illustrates parameter top fl fl lp 
class approach tanh decision stumps finding outliers toy problem different values top line middle line bottom line different values class barrier algorithm columns class lp right column 
points initially unlabeled points outside region estimated algorithm marked asterisks 
fraction outliers algorithms 
bottom controls fraction marked asterisks 
value algorithm shrinks region small outliers fraction resulting decision boundary superposition soft decision stumps yielding readily interpretable result translated simple rules characterizing normality novelty outlier exceptional event 
furthermore show snapshots barrier algorithm certain levels 
shows output barrier algorithm converges output linear problem solver cplex right 
note cases result barrier algorithm describes data intuitively better case looks bit appropriate lp solution 
conjecture due implicit regularization properties barrier approach 
experiment usps database third experiment utilized usps database handwritten characters 
ii 
centered base hypothesis set rbf kernel functions exp training patterns shown useful independent studies cf 

cf 
scholkopf 
shows plot outputs training dual barrier optimization problem appendix sees forcing distribution vector small relative entropy uniform distribution cf 

details 
note differently cg approach favoring sparse patterns weights barrier approach tries avoid sparseness exponential form cf 


experiments postal service ocr data set 
recognizer digit output histogram exemplars training test set test exemplars digits 
axis gives output values argument sign decision function 
left get outliers consistent proposition true positive test examples false positive class digits 
right get outliers 
case true positive rate improved false positive rate increases 
threshold marked graphs 
plots show parzen windows density estimate output histograms 
reality examples sit exactly threshold value 
test sets postal service database handwritten digits 
database contains digit images size constitute test set 
fed algorithm training instances digit 
testing done digit digits 
shown leads false positive learning machine seen non training correctly identifies non recognizing digits test set 
higher recognition rates achieved smaller get correct recognition digits test set fairly moderate false positive rate 
similar experiments done scholkopf 
svm similar results 
model selection serious problem class approach select model 
note commonly model selection algorithms cross validation easily applicable class problem 
find appropriate base learner 
smaller problem derive specialized algorithm certain problems rbf kernels ocr 
furthermore find optimal parameter due meaning may able infer optimal problem hand set estimated fraction training data 
cases fraction unknown 
case propose simple heuristics may help find appropriate idea follows class classifier find decision irrespectively quality separation class patterns 
intuitively separation achieved separation clear distance classes large 
propose exactly intuition criterion model selection 
measures average output classifier fraction outliers digit data set data set digits randomly sampled digits class lp clas digits different fractions axis applied 
separation distance left error randomly sampled digits rate misclassification labeled test set right different val dashed graph shows shown 
maximum separation distance coin lected maximum separation minimum test error 
criterion 
solid graph shows minimizes test error 
classes selects separated classes best 
illustrate idea conduct experiment usps data set cf 
section separate digit rest 
training digits additionally include different fractions tt 
digits 
data sets compute class lp different values compute separation distances fv respective number patterns classes chose largest separation distance 
exam ple maximum distinct cf 
left coincides minimum test error cf 
right 
cases characterized small gap observed maximum erroneously achieved 
expected model selection heuristic assumes clear separation 
studied correspondence svms boosting methods 
essentially high dimensional feature spaces 
differ deal algorithmic problems cause 
think boosting sv approach high dimensional feature space spanned basis hypothesis 
problem tractable effectively norm regularizer 
induces sparsity really full space small subspace 
vice versa think svm boosting approach high dimensional space 
svms kernel trick optimization packages cplex useful feature allows change coefficients objective function case obtain updated solution low computational effort 
explicitly feature space 
svms get away having norm regularizers kernel allows computation norm feature space 
methods lead sparse solutions sample coefficient space svms feature space boosting methods adapted algorithmically exploit form sparsity produce 
providing insight correspondence concrete practical benefits designing new algorithms 
employed devise leveraging algorithm novelty detection 
new algorithm combines ideas benefits arc column generation algorithm boosting 
converges asymptotically solution linear program similar svm program 
experiments shown promise new research direction boosting unsupervised learning 
focus contribution seen theoretical conceptual side 
practical side conducted experiments toy data real world ocr data demonstrate proof concept 
interesting real world class applications challenging splice site detection problem dna cf 
salzberg jagota 
theoretical research dedicated incorporate prior knowledge obtain class algorithms eventually faster better general easier understand interpret 
particular extension algorithms infinite hypothesis classes derivation theoretically motivated model selection methods class classification problem challenging 
acknowledgments valuable discussions manfred warmuth alex smola bob williamson demiriz 
partially funded dfg contract ja ja mu eu neurocolt project 
furthermore gr anu uc santa cruz warm hospitality 
proof lemma proof 
holds exp exp fw xn 
oe kon ow ow exp exp get plugging yields sign wj ll fw second line ii sin iw line set exp primal dual problem duality gap start problem set corresponding lp min qn dn qn hj xn simplicity 
lagrangian problem logd logq el setting derivatives respect primal variables zero yields non linear constraint sets exp exp 
plugging logq dt dt pl tp dt qt get dual program exp stated 
duality gap difference primal dual objective optimal feasible max gap dn qn dn qn max 


notation notational conventions ii counter number patterns counter number hypotheses dimensionality counter number iterations input space space non negative real numbers training pattern label set base hypotheses element feature space hypothesis weight vector slack variable pattern combined hypothesis weighting weighting training set quantile parameter determines number outliers margin barrier penalty parameter norm scalar product feature space bauer kohavi 

empirical comparison voting classification algorithm bagging boosting variants 
machine learning 
bennett demiriz shawe taylor 

column generation algorithm boosting 
unpublished manuscript submitted special issue machine learning 
bennett demiriz shawe taylor 

column generation algorithm boosting 
langley 
ed th icml pp 
san francisco 
morgan kaufmann 
bennett mangasarian 

robust linear programming discrimination linearly inseparable sets 
optimization methods software 
bradley mangasarian rosen 

parsimonious norm approximation 
computational optimization applications 
bregman 

relaxation method finding common point convex sets application solution problems convex programming 
ussr computational mathematics mathematical physics 
breiman 

prediction games arcing algorithms 
technical report statistics department university california 
campbell bennett 

linear programming approach detection 
advances neural information processing systems proceedings conference 
mit press 
appear 
censor zenios 

parallel optimization theory algorithms 
numerical mathematics scientific computation 
oxford university press 
chen donoho saunders 

atomic decomposition basis pursuit 
tech 
rep department statistics stanford university 


stable exponential penalty algorithm superlinear convergence 

dietterich 

experimental comparison methods constructing ensembles decision trees bagging boosting randomization 
machine learning 


interior proximal algorithm exponential multiplier method semidefinite programming 
siam 
drucker schapire simard 

boosting performance neural networks 
international journal pattern recognition artificial intelligence 
duffy helmbold 

leveraging regression 
proc 
colt pp 
stanford 
morgan kaufmann 
freund schapire 

game theory line prediction boosting 
proc 
colt 
morgan kaufman 
freund schapire 

decision theoretic generalization line learning application boosting 
eurocolt european conference computational learning theory 
lncs 
friedman hastie tibshirani 

additive logistic regression statistical view boosting 
tech 
rep department statistics sequoia hall stanford 
grove schuurmans 

boosting limit maximizing margin learned ensembles 
proceedings fifteenth national conference artifical intelligence 
hayton scholkopf tarassenko 

support vector detection applied jet engine vibration spectra 
proc 
nips 
appear 
kivinen warmuth 

boosting entropy projection 
proc 
colt 
lecun jackel bottou cortes denker drucker guyon simard vapnik 

comparison learning algorithms handwritten digit recognition 
gallinari 
eds proc 
icann vol 
ii pp 
france 
luenberger 

linear nonlinear programming second edition 
addisonwesley publishing reading 
reprinted corrections may 
maclin opitz 

empirical evaluation bagging boosting 
proc 
aaai 
mangasarian 

mathematical programming data mining 
data mining knowledge discovery 
mangasarian 

arbitrary norm separating plane 
operation research letters 
mason baxter bartlett frean 

functional gradient techniques combining hypotheses 
smola bartlett scholkopf schuurmans 
eds advances large margin classifiers pp 

mit press cambridge ma 


penalty barrier multiplier algorithm semidefinite programming 
optimization methods software 
press 
demiriz bennett 

sparse regression ensembles infinite finite hypothesis spaces 
special issue machine learning 
submitted 
jagota 

splice site recognition support vector machines 
preparation 
onoda 

soft margins adaboost 
machine learning 
neurocolt technical report nc tr 
sch kopf smola mika onoda 

robust ensemble learning 
smola bartlett scholkopf schuurmans 
eds advances large margin classifiers pp 

mit press cambridge ma 
warmuth mika onoda 

barrier boosting 
proc 
colt pp 
stanford 
morgan kaufmann 
salzberg 

method identifying splice sites translational start sites eukaryotic mrna 
bioscience 
schapire 

strength weak learnability 
machine learning 
schapire freund bartlett lee 

boosting margin new explanation effectiveness voting methods 
proc 
th international conference machine learning pp 

morgan kaufmann 
schapire singer 

improved boosting algorithms predictions 
proc 
colt pp 

scholkopf 

support vector learning 
oldenbourg verlag 
sch kopf platt shawe taylor smola williamson 

estimating support high dimensional distribution 
tr microsoft research redmond wa 
appear neural computation 
schwenk bengio 

neural networks 
gerstner nicoud 
eds proc 
icann vol 
lncs pp 

springer 
smola 

learning kernels 
ph thesis technische berlin 
tax duin 

data domain description support vectors 
verleysen 
ed proc 
esann pp 
brussels 
facto press 
poggio girosi 

extensions theory networks approximation learning dimensionality reduction clustering 
tech 
rep aim mit 
vapnik 

nature statistical learning theory 
springer verlag new york 

