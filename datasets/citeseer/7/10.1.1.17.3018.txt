reversing smoothing multinomial naive bayes text classifier juan hermann ney iti universitat de valencia cam de vera valencia spain aj upv 
es lehrstuhl informatik vi aachen university technology aachen 
rwth aachen 
de 
naive bayes text classifier long core technique information retrieval emerged focus research machine learning 
concerned naive bayes text classifier multinomial model instantiation 
model equivalent reversed version proposed interpreted statistical framework log linear modelling 
reversed version provides alternative way parameter estimation broad sense main issue considered 
pa large extent devoted study effects parameter smoothing document length normalization 
purpose parameter smoothing consider standard smoothing method text classification alternative techniques context statistical language modelling speech recognition 
empirical results provided comparing techniques effect length normalization multinomial model reversed version 
key words log linear modelling text classification naive bayes multinomial smoothing length normalization naive bayes text classifier long core technique information retrieval emerged focus research machine learning 
area focused probabilistic analysis accordance identify basic instantiations classifier bernoulli model multinomial model 
bernoulli model works documents represented vectors binary features word occurrence bits 
assumed binary features class conditional independent pattern class modelled multivariate bernoulli distribution 
supported spanish de ciencia tic 
multinomial model uses vectors integer features word counts repre sent documents 
case assumed class described multinomial distribution 
empirical comparison models shown multinomial model usually superior bernoulli model 
concerned multinomial naive bayes model text classifier 
model interpreted statistical framework log linear modelling categorical data analysis providing clear connection interesting ideas framework particular maximum entropy 
framework assuming class priors equal propose equivalent model parameters reversed class conditional word probabilities transformed word conditional class probabilities 
idea parameter reversing provide alternative way parameter estimation broad sense main issue studied 
shown conventional relative frequencies arc approximate solutions log probability criterion multinomial model reversed version 
importantly large extent devoted study effects parameter smoothing document length normalization 
purpose parameter smoothing wc consider standard smoothing method text classification alternative techniques arc context statistical language modelling speech recog nition 
empirical results arc provided comparing techniques effect length normalization multinomial model reversed version 
organized follows 
section presents basic log linear model 
multinomial model reversed version arc described sections respectively 
section devoted parameter smoothing section discusses document length normalization 
empirical results arc section 
log linear modelling denote class variable word variable text documents represented dimensional vectors word counts known bag words representation 
text document xz assume order multiplicative decomposition class posteriors eli xd oc parameter acd introduced order capture effect word count xd class order log linear model easily extended include zero order class independent parameters simply assuming empty word text document 
inclusion second higherorder parameters possible 
instance include second order parameter word pair dd capture interaction effect word counts xd xa class case obtain second order multiplicative decomposition class posteriors da cd cdd dd general setting bigram trigram counts added text document representation model 
noted log linear modelling natural interpretation maximum entropy framework 
fact model identical dual maximum entropy model proposed text classification pl definitions fi fag ai log aj focus model relates multinomial naive bayes classifier reversed version :10.1.1.43.7517
consider inclusion class independent parameters optional extension 
assumed extension great impact decision rule associated arg fax xa log aa arg fax log ad multinomial naive bayes model closely related popular naive bayes text classifier multinomial event model instantiation 
classifier naive bayes assumption probability word event occurrence document independent word context position document 
document represented vector word counts class conditional probability multinomial distribution set words lc 
hp denotes length document 
note principle multinomial distribution possible document length 
purpose keeping number parameters assumed document lengths independent class probabilities word occurrence independent document length 

xl hp xd application rule obtain expression class posteriors hp hp identical model definitions normalization constraints estimate set parameters best fits sequence training data xn cn consider maximization log probability function cd na number occurrences word training documents class maximization subject equality constraints 
lagrange multipliers obtain equivalent unconstrained maximization problem objective function determine optimal parameter values fi take partial derivatives respect parameter acd lagrange multiplier zc equate zero og dl ocd cd 
obtain closed form solution equations wc approximation depend straightforward manipulations wc conventional estimates class priors class conditional word probabilities na worth ha estimates solution problem maximizing log likelihood function logp na log aa subject constraints 
reversed multinomial naive bayes stated section assume inclusion class independent param eters priors model great impact associated decision rule 
uniform priors assumption rewrite expression class posteriors follows ip xd ilp xd lq cld lq ld lp hp cld hp ld identical definitions cd cld normalization constraints ccd call reversed multinomial naive bayes equivalent uniform priors assumption words classes way round 
estimate word conditional class probabilities class conditional word probabilities 
goes saying reversing variables leads different parameter estimation problem 
preceding section parameter estimation carried maxi mization log probability function case constraints 
corresponding lagrangian function rt rt rtca necessary conditions optimality solving lagrange multipliers find zero constraints superfluous derive exact closed form solution 
obtain approximate closed form solution arrive conventional relative frequencies previous section approximate solution log probability criterion exact solution log likelihood criterion maximize subject constraints 
parameter smoothing parameter smoothing required counteract effect statistical variability particularly number parameters relatively large comparison amount training data available 
case effect significant estimation class priors accordance 
poses severe problem estimation class conditional word prob abilities word conditional class probabilities 
instance experiments reported section face problem estimating class conditional word word conditional class probabilities non zero 
result sole occurrence rare word test document introduce dominant underestimated terms decision rule may certainly cause classification error 
section consider smoothing methods class conditional word probabilities 
methods applied class probabilities simply interchanging roles played words classes 
note smoothing class conditional word probabilities entails separate smoothing problems class conditional probability distribution set words 
avoid confusion follows take arbitrary fixed class 
baseline smoothing method consists simply adding pseudo count ned count ned default value 
method referred laplace smoothing 
default smoothing technique popular bow toolkit text classification 
laplace smoothing effective way prevent zero probabilities think may effective guess appropriate redistribution probability mass set possible events words 
fact problem extensively studied context statistical language modelling speech recognition addition constant possible event preferred smoothing methods 
inspired results achieved context consider alternative smoothing techniques text classification 
idea alternative techniques known absolute discounting 
artificial pseudo counts gain free probability mass discounting small constant count associated seen event positive count 
gained probability mass distributed events accordance generalized distribution uniform distribution unigram distribution ned generalized distribution tries reliable estimation class independent word probabilities 
alternative smoothing techniques text classification differ set events receives gained probability mass technique know backing distributes mass unseen events nca na dl ocd na nca gained probability mass discount restricted interval avoid zero probabilities satisfy normalization constraint 
second technique distributes gained probability mass events max vo md 
technique ed lion 
contrast need generalized 
document length normalization multinomial naive bayes text classifier reversed version biased correctly classifying long documents expense misclassifying short ones 
due unrealistic assumption probabilities class conditional word occurrence independent document length see 
assumption estimates clearly dominated word counts come long documents 
remedy difficulty normalize word counts document respect length convenient normalization constant average document length simply 
distributions fractional counts arc ill defined derivations arc extensible normalized counts estimates arc applicable 
decision rule invariant length normalization test documents classified prior length normalization 
important note multinomial naive bayes classifier reversed version equivalent combined laplace smoothing number training documents class uniform priors assumption 
see rewrite conventional classifier reversed version ned arg max xt log arg max xa log na na arg max xa log minor obstacle fractional counts longer valid perform absolute discounting backing interpolation respectively 
easy overcome obstacle redefining na na ocd nd dtl min ned nod 
easily checked equivalent unnormalized integer word counts 
similarly substituting get appropriate redefinition absolute discounting interpolation 
detail take account context reversed clas distinction uniform unigram generalized distributions classes number training documents 
case unigram distribution uniform ul dl empirical results section provides empirical results effects parameter smoothing document length normalization multinomial naive bayes classifier reversed version 
results newsgroups data set 
newsgroups data set collection approximately news group documents partitioned nearly evenly different newsgroups 
original version data set provided doc ument headers discarded subject header fields retained 
preprocessing carried ra consisted steps elimination uu encoded segments formation tokens contiguous alphabetic characters stemming stoplist removal 
preprocessed documents sorted posting date file name oldest documents class total documents selected training 
remaining documents held testing 
resulting training vocabulary words singletons words occurring 
average documents words length 
nice feature newsgroups task uniform priors assumption certainly holds need take care interpreting results 
advantage task vocabulary selection required multinomial naive bayes classifier performs best full vo 
noted authors remove singletons preprocessing 
despite advantages newsgroups task proven difficult previously reported classification error rates fall 
occurs tasks face difficult problem data sparseness sm class conditional word word conditional class probabilities estimated amount training data available relatively small 
concretely different ncd counts non zero greater greater shows error rate function smoothing parameter multinomial naive bayes classifier combined laplace smoothing absolute discounting unigram backing absolute discounting unigram interpolation 
left plot refers conventional classifier right plot contains results documents normalized average length 
seen absolute discounting particular absolute discounting unigram interpolation gives better results laplace smoothing 
length normalization helps flattening curves laplace smoothing unigram interpolation surprisingly help 
note laplace smoothing performs better values smaller usual 
results discounting uniform backing interpolation slightly worse obtained unigram backing interpolation omitted sake clarity 
shows results reversed multinomial naive bayes classifier structure 
contrast conventional classifier length normalization provides significantly better results 
fact results obtained length normalization probably estimation word conditional distributions classes highly influenced bias long documents 
concentrate results achieved length normalization 
discussed previous section laplace smoothing achieves results conventional classifier backing interpolation depend choice generalized distribution uniform 
worth noting results absolute discounting consistently better obtained conventional classifier length normalization 
fact reversed classifier interpolation length normalization best option tested 
table provides summary classification error rates selected cases 
table results reported figures may conclude things 
hand length normalization helps getting better results case conventional classifier flattens error rate curves 
authors reported significant improvements case may due differences preprocessing removal singletons hand absolute discounting gives marginal consistent improvements error rates achieved laplace smoothing 
concluding remarks effects parameter smoothing document length normalization studied multinomial naive bayes text classifier reversed laplace smoothing unigram backoff gram backoff unigram interpolation fig 

left error rate function smoothing parameter multinomial naive bayes classifier combined laplace smoothing absolute discounting unigram backing absolute discounting unigram interpolation 
right results documents normalized length 

laplace smoothing uniform backoff uniform interpolation laplace smoothing uni form gram backoff uni form gram interpolation fig 

left error rate function smoothing parameter reversed multinomial naive bayes classifier combined laplace smoothing absolute discounting uniform backing absolute dis counting uniform interpolation 
right results documents normalized length 
table 
summary classification error rates 
length norm conventional classifier laplace laplace best absolute discounting reversed classifier laplace laplace best absolute discounting version 
purpose parameter smoothing considered laplace smoothing absolute discounting backing interpolation 
empirical results shown length normalization helps getting better results case conventional classifier flattens error rate curves 
regard parameter smoothing absolute discounting gives marginal consistent improvements error rates achieved laplace smoothing 

lewis naive bayes independence assumption information retrieval 
proc 
ecml 

nigam comparison event models naive bayes text classification 
aaai icml workshop learning text categorization 

agresti categorical data analysis 
wiley 
ney martin wessel statistical language modeling leaving 
young eds corpus methods language speech processing 
kluwer academic publishers 
nigam lafferty maximum entropy text classification 
ijcai workshop machine learning information filtering 

bow toolkit statistical language modeling text retrieval classification clustering 
www cs cmu edu mccallum 
original newsgroups data set 
www ai mit edu 
