learning predict leave error kernel classifiers tsuda gunnar sebastian mika klaus robert gmd 
berlin germany university potsdam am potsdam aist computational biology research center ku tokyo japan tsuda mika klaus gmd de 
propose algorithm predict leave loo error kernel classifiers 
achieve goal computational efficiency cast loo error approximation task classification problem 
means need learn classification training sample left data set misclassified 
learning task simple data dependent features proposed inspired geometrical intuition 
approach allows reliably select model demonstrated simulations support vector linear programming machines 
comparisons existing learning theoretical bounds span bound various model selection scenarios 
numerous methods proposed model selection classifiers support vector machines svms linear programming machines ill try find reasonably estimate generalization error select proper hyperparameters :10.1.1.28.8322
data dependent loo error principle ideal selecting hyperparameters learning machines unbiased estimator true generalization error 
computation unfortunately practical cases prohibitively slow 
attempts approximate leave error closed form svm classifiers :10.1.1.28.8322
example new type bound proposed relies span svs empirically perform best learning theoretical bounds 
approximations limited special learning machine svm difficult provide useful approximation valid general class classifiers 
introduce learning approach approximating loo error general kernel svms linear programming machines lpm 
propose geometrical features cast leave error approximation problem kernel classifiers fast solvable classification problem 
loo error approximation problem reduces learn classification training sample left data set misclassified 
task referred meta learning problem try learn learning machine 
meta classification task data rich large number training patterns generated sorts different classification problems 
note features meant reflect local difficulty complexity meta learning problem large set possible data 
experiments show approach works svm lpm 
reviewing popular learning theoretical loo error bounds describe features inputs meta learning problem solve classification approach 
subsequently perform simulations showing usefulness loo error approximation scheme comparison bounds conclude remarks 
reviewing svms selected loo bounds learning svms seeking coefficients linear combination kernel functions xi ik lri lr done solving type optimization problem min ei li 
lip norm feature space svms norm coefficient space respectively 
data labels denoted ii respectively 
regularization parameter 
solving usually introduces lagrange multipliers ai 
constraints zero constraint active 
svms turn equal lpm correspondence 
review bounds loo error svms proposed 
complete presentation 
sample size pattern defined zi xi yi 
furthermore define fp zp fp decision obtained learning th sample left training set 
loo error defined xp 
xp positive fp commits error xp average number patterns misclassified left 
support vector count svms useful properties exploited loo prediction patterns support vectors svs change decision surface correctly classified 
consider svs lo procedure lo error easily bounded sv sv number svs 
rough estimate svs misclassified removed training set 
holds defines svs patterns expansion coefficients corresponding lagrange multipliers ai non zero 
jaakkola haussler bound jaakkola haussler proposed tighter bound eq epk xp xp xp weight coefficient svm kernel function 
span predictions sophisticated way predicting loo error span support vectors proposed 
assumption set svs change loo procedure loo error exactly rewritten xp psp sv denotes set support vectors geometric value called span support vector 
unfortunately practice assumption satisfied experimentally shown ap proximation works 
computing span needs solve optimization problem expensive number svs high 
meta learning predicting generalization error empirical data outline learning framework predict lo error 
lo error depends data set parameters learning machine 
assume lo errors measured data sets various combinations zm 
meta learning machine trained predict unseen data appropriate features extracted 
predicting loo error classification problem recall loo error represented loo result rp xp zp zp sgn xp 
predict loo error sufficient predict result loo procedure error certain pattern left 
meta learning problem binary classification problem 
kernel classifiers learning scheme designed fig coefficient attached training sample xi 
features extracted left sample xp ap neighboring samples 
neighbors included affect loo result shown fig 

neighbors care support vectors 
features include information non support vectors span bound derived support vectors 
left sample neighbors zl feature extractor meta estimate lo result learning machine zp fig 
learning scheme predicting loo result 
features meta classification include local geometry left sample features loo extracted follows left sample gp weight ap dual weight lpm margin additionally calculate quantities nearest neighbors xk xp distance input space distance feature space xk weight dual weight lpm margin di maximum distances training samples input space feature space respectively 
features gp form vector vp label zp learn loo error prediction neighbor neighbor fig 
consider case non sv near sv left panel 
leave procedure boundary re estimated sv non sv takes part boundary change significantly 
lo boundary show large difference close non sv neighbor right panels 
vp zp patterns 
built meta classifier linear programming machine polynomial kernel degree employs norm regularization feature space leading sparse weight vectors 
turns beneficial meta lpm automatically selects relevant features 
experiments motivation experiment answer questions loo error predictor learn data set generalize unseen data sets 
prediction reliable model selection 
class benchmarks study considered data sets twonorm ringnorm heart 
twonorm ringnorm quite similar input dimensionality number training samples data sets 
heart quite different data set input dimensionality number training samples 
evaluation meta classifier experimental setup data set considered realizations train test splits averaging obtaining error bars 
tion trained svms wide ranges regularization constant rbf kernel parameter exp iix 
training sample extracted features described section 
features corresponding labels computed actual loo procedure training testing classifier 
learned data sets tested third 
evaluate performance model selection error kernel parameter regularization constant data sets incl 
training test splits loo results obtained ida 
gmd 
de sch 
ringnorm heart twonorm heart twonorm ringnorm twonorm ringnorm heart jh jh jh fig 
model selection errors svm 
labels denote opt optimal choice test error loo actual leave calculation span span bound jh bound svc support vector count pred method 
data sets training test error assessed third 
selected predicted loo error minimized 
model selec tion error defined classification error test set chosen parameters 
results svm lpm shown fig 
respectively 
results span bound jh bound available respective bounds simply exist 
experiments show cases loo predictor performs actual highly expensive loo calcu lation 
compared bounds observe methods achieve similar performance note method applied 
comparing cases fig 
method performs slightly worse heart testing comparison cases 
shows tendency loo error prediction works data similar characteristics contained training set 
indicates statistics feature set varies considerably different data sets 
surprising simple features show generalization performance benchmark problems 
multiclass example application approach consider multi class problems 
solving class problems common svms single class classi trained discriminate class classes 
hyperparameters svm properly set model selection process 
clearly takes prohibitively long perform leave procedures svms possible hyperparameter settings 
cope problem leave procedure performed respect classification problems meta classifier trained ringnorm heart twonorm heart twonorm ringnorm opt loo svc pred opt loo svc pred twonorm ringnorm heart opt loo svc pred fig 
model selection errors lpm 
result 
hyperparameters vms efficiently selected loo error predictions meta classifier 
performed experiment dobj data set classes samples samples class 
task choose proper value kernel width cr prespecified set values 
sample randomly divided training test samples obtaining error bars results 
class chosen training remaining classes testing 
cr 
computed loo error predictions meta classifier 
shows model selection errors svms 
method performed actual loo calculation svms 
task model selection multiclass problems appears suitable dataset added ida website 
svm lpm opt loo span jh pred opt loo pred fig 
model selection errors multiclass problem 
method data sets training testing considered similar statistical properties 
train learning machine learns generalization behavior set learning machines appealing idea introduces meta level reasoning 
goal obtain effective meta algorithm predicting loo error past experience variety data sets 
adding twist casting meta learning problem specific classification task allowed achieve accurate fast empirical estimate loo error svm lpm 
crucial point simple geometrical features classifying training sample misclassified left 
reliable loo error estimate easily model selection 
careful simulations approximately cpu years mhz pentium iii obtaining actual loo error show meta learning framework compares favorably conventional bounds 
apparently heuristic geometrically motivated features generalization ability different data sets 
speculate features provide new ways improve bounds way integrating particularly meaningful features meta learning problem new learning theoretical bounds 
research dedicated exploration meta learning idea learning machines gaining better learning theoretical understanding findings 
acknowledgments valuable discussions jason weston olivier chapelle bernhard 
partially funded dfg contract ja mu 

bennett mangasarian 
robust linear programming discrimination linearly inseparable sets 
optimization methods software 

chapelle vapnik 
choosing kernel parameters support vector machines 
personal communication mar 

jaakkola haussler 
probabilistic kernel regression models 
proceedings conference ai statistics 


mika tsuda 
kernel learning algorithms 
ieee transactions neural networks 
press 

opper winther 
gaussian processes svm mean field 
smola bartlett schuurmans editors advances large margin classifiers pages cambridge ma 
mit press 

vapnik 
estimation dependences empirical data 
springer verlag berlin 

vapnik 
nature statistical learning theory 
springer verlag new york 

vapnik chapelle 
bounds error expectation support vector machines 
neural computation september 

lin zhang 
generalized approximate cross validation support vector machines way look quantities 
technical report tr dept statistics university wisconsin april 
