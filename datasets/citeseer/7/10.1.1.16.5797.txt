journal arti cial intelligence research submitted published optimizing dialogue management reinforcement learning experiments njfun system satinder singh cs colorado edu capital new york ny diane litman litman cs pitt edu department computer science university pittsburgh pittsburgh pa michael kearns cis upenn edu department computer information science university pennsylvania philadelphia pa marilyn walker walker research att com labs research florham park nj designing dialogue policy spoken dialogue system involves nontrivial choices 
presents reinforcement learning approach automatically optimizing dialogue policy addresses technical challenges applying reinforcement learning working dialogue system human users 
report design construction empirical evaluation njfun experimental spoken dialogue system provides users access information fun things new jersey 
results show optimizing performance reinforcement learning njfun measurably improves system performance 

advances spoken language understanding possible develop dialogue systems applications 
role dialogue manager systems interact natural way help user complete tasks system designed support 
typically expert designs dialogue management policy hand nontrivial design choices 
dicult assess rami cations choices performance dialogue policy depends factors user population robustness automatic speech recognizer asr task diculty kamm litman walker danieli gerbino 
applies reinforcement learning rl automatically learn design choices optimize system performance chosen performance measure levin pieraccini eckert walker narayanan 
ai access foundation morgan kaufmann publishers 
rights reserved 
singh litman kearns walker welcome njfun 
may help 
nd um morning 
asr nd morning 
say interested 

say want go morning 

near open morning 
poor richard 
system 
please give feedback saying bad 

goodbye nice day 
example dialogue njfun 
consider spoken dialogue system named njfun implemented provide telephone access database activities new jersey 
sample dialogue njfun shown system utterances labeled si user utterances labeled ui 
dialogue starting open ended greeting may help system lets user take initiative providing information activity interested 
user responses cases may relatively unconstrained 
contrast system take initiative saying restrictive phrase please tell location interested constraining user provide information location activity 
contrasting choices user system initiative superior may depend strongly properties underlying imperfect asr population users dialogue far 
choice initiative occurs repeatedly dialogue example class dicult design decisions 
main previous research treated speci cation dialogue management policy iterative design problem versions system created version uses single dialogue policy intuitively designed expert dialogue corpora collected human users interacting di erent versions system number evaluation metrics collected dialogue di erent versions statistically compared danieli gerbino sturm den os cremers kamm walker litman kamm 
due costs experimentation handful policies usually explored experiment 
thousands reasonable dialogue policies typically possible 
njfun example search space potential dialogue polices detailed 
suggested dialogue policy designed formalisms markov decision processes mdps reinforcement learning rl biermann long levin walker singh kearns litman walker walker standard approach ai problems involve agent learning improve performance interaction environment sutton optimizing dialogue management barto kaelbling littman moore 
speci cally mdp rl formalisms suggest method optimizing dialogue policies sample dialogue data features suited problem dialogue design 
features include fact rl designed cope gracefully noisy sensors asr stochastic behavior environment case user population delayed rewards typical spoken dialogue systems 
main advantage approach potential computing optimal dialogue policy larger search space relatively small number training dialogues 
rl approach data ecient evaluates actions function state traditional iterative method evaluates entire policies 
unfortunately practical application rl area spoken dialogue management presents technical challenges 
theory rl quite advanced applications limited exclusively problems control operations research game playing crites barto tesauro 
dialogue management represents di erent type problem mdp models working system interaction population human users rl optimize system performance 
type application amount training data severely limited requirement human interact system 
furthermore need exploratory data balanced need functioning system choice system available particular context sense context user perspective 
presents detailed methodology rl optimize design dialogue management policy limited interactions human users experimentally demonstrates utility approach context njfun system 
high level rl methodology involves choice appropriate performance criteria reward measures estimates dialogue state deployment initial training system generates deliberately exploratory dialogue data construction mdp model user population reactions di erent action choices system optimal dialogue policy learned estimated model 
section describes dialogue policy choices dialogue manager 
section explains reinforcement learning optimize choices dialogue system human users 
section describes architecture njfun system section describes njfun optimizes dialogue policy experimentally obtained dialogue data 
section reports empirical results evaluating performance njfun learned dialogue policy demonstrates approach improves njfun task completion rate chosen measure performance optimization 
section presents results establishing veracity learned mdp compares performance learned policy performance standard hand designed policies literature 
results provide empirical evidence properly applied rl quantitatively substantially improve performance spoken dialogue system 

explored non rl learning methods immediate kinds rewards chu carroll brown walker rambow 
singh litman kearns walker policy dialogue database tts asr user block diagram representation spoken dialogue system 
user gains access database speaking system natural language automatic speech recognition system asr 
system talks back user text speech tts system 

dialogue management spoken dialogue systems typical spoken dialogue system shown block diagram form user speaks system real time telephone microphone free form natural language order retrieve desired information back database 
user speech interpreted automatic speech recognizer asr system natural language responses conveyed user text speech tts component 
dialogue manager system uses dialogue policy decide system say rl terminology action take point dialogue 
purposes asr viewed imperfect noisy sensor adjustable parameter language model grammar tuned uence types speech recognition mistakes 
addition perceived matches utterance asr returns score typically related log likelihood hidden markov model giving subjective estimate con dence matches 
score important interpreting asr results 
concentrates automating important types decisions faced dialogue policy design heavily colored asr facts 
rst type decisions seen example initiative system allow user system point prompt user relatively open ended manner referred user initiative relatively restrictive manner system initiative 
second type choice investigate conservative system con rming understanding user 
applied asr user utterance obtained value attribute interest instance town system decide con rm perceived utterance user 
user response example njfun decide explicitly con rm understanding utterances 
njfun simply continue dialogue explicitly con rm user wants nd 
posit con rmation unnecessary high values asr con dence necessary low values proper de nitions high low ideally determined empirically current state instance depending diculty previous exchanges depend measure system success 
optimizing dialogue management detailed njfun system identi ed di erent dialogue states wanted learn take user system initiative prompt 
similarly identi ed di erent dialogue states wanted learn con rm asr perceived user utterance con rm 
note genuine debate choices initiative con rmation dialogue system designers walker whittaker danieli gerbino haller mcroy smith walker 
simple example users enjoy systems con rm frequently unnecessarily provides con dence system understanding user 
understood choices prevailing consensus precisely wish automate principled way process making choices basis empirical data 

reinforcement learning dialogue policy design section describe methodology propose apply rl dialogue policy design 
section describe detail instantiation methodology njfun system 
order apply rl design dialogue policy necessary de ne state representation dialogues 
simply mean information dialogue far relevant deciding action system take contained single summarizing entity called state 
obvious impractical choice state transcript system log entire dialogue include audio far utterances matched asr language models con dence scores returned asr quantities 
practice need compress state possible representing states values small set features losing information necessary making decisions 
view design appropriate state space application dependent task skilled system designer 
choices state features system designer think terms state space appropriate actions take state 
de ne dialogue policy mapping set states state space set actions states proper action take may clear instance greeting user start state querying database informational attributes instantiated 
states called choice states may multiple reasonable action choices choices initiative con rmation 
mapping choice states particular action distinct dialogue policy 
typically system designer uses intuition choose best action take choice state 
rl approach choices learning particular dialogue system explores action choices systematic way learn optimize behavior interacting representative human users 
system converses human users perform set representative tasks dialogue domain 

learned obviously types dialogue policy decisions system results database queries litman pan walker 
singh litman kearns walker dialogue interaction scalar performance measure called reward calculated 
resulting dialogue corpus construct markov decision process mdp models users interaction system 
approach problem learning dialogue policy reduced computing optimal policy choosing actions mdp system goal take actions maximize expected reward 
computation optimal policy learned mdp done eciently standard dynamic programming algorithms bertsekas tsitsiklis sutton barto 
dicult predict actions states rewards advance build desired mdp sample dialogues 
singh 
view dialogue trajectory chosen state space determined system actions user responses 
indicates ith exchange system state executed action received reward state changed experiments terminal dialogue states nonzero rewards 
dialogue sequences obtained training data empirically estimate transition probabilities js denoting probability transition state system state took action reward function denoting expected reward obtained system state took action 
example estimate transition probability simply number times dialogues system took arrived divided number times system took regardless state 
estimated transition probabilities reward function constitute mdp model user population interaction system 
hopefully captures stochastic behavior users interacting system 
note order con dence model sample dialogues system tried possible actions possible states preferably times 
words training data exploratory respect chosen states actions 
try allowed action state expect know value action state 
straightforward way ensuring exploratory training data take actions randomly approach take njfun requires exceptionally careful designing actions allowed choice state order guarantee random choices result dialogue sensible human users 
keep mind exploration non choice states appropriate action known xed system designer 
approaches generating exploratory data possible 
mdp expected cumulative reward value action state calculated terms values successor states 
discuss various choices reward measure experiments reward quantity directly obtainable experimental set user satisfaction task completion 

course random exploration possible practice explore states equally 
states occur 
net ect states occur actions tried states occur rarely transition probabilities frequent potentially important state action pairs accurate transition probabilities infrequent state action pairs 
optimizing dialogue management db access utterances reward state state semantic tags dialogue policy asr db user utterances log likelihood estimated users estimator state dialogue system viewed mdp 
population users correspond environment state things de ned outputs automatic speech recognition asr system database db 
dialogue policy de nes agent state estimator de nes agent sensors database actions possible set tts utterances de ne agent action set 
recursive equation watkins sutton barto js max js estimated transition model estimated reward model 
discount factor set value discount rewards obtained time 
njfun policy learned insensitive reasonable choices discounting experiments reported 
values de ned equation estimated desired threshold value version standard value iteration algorithm bertsekas tsitsiklis iteratively updates estimate current values neighboring states stops update yields di erence threshold 
value iteration completed optimal dialogue policy estimated model obtained selecting action maximum value dialogue state 
extent estimated mdp accurate model user population optimized policy maximize reward obtained users 
approach theoretically appealing cost obtaining sample human dialogues crucial limit size state space minimize data sparsity problems retaining information state learn accurate model 
sample data nite idealized state include dialogue far derived features asr results log likelihood scores representing asr con dence semantic analysis results database queries 
state small number features yield enormous state space 
proposed simulating user interactions obtain training data levin young approach directly small carefully designed estimated state space singh shown 
minimal state representation approximate true state amount data required learn optimal dialogue policy learned mdp value iteration greatly reduced 
singh litman kearns walker contribution empirically validate practical methodology reinforcement learning build dialogue system optimizes behavior human computer training dialogue data 
nutshell proposed approach 
choose appropriate reward measure dialogues appropriate representation dialogue states design dialogue policy maps state set reasonable actions 
states may reasonable action 

build initial state training system creates exploratory data set tries times choice state actions choose 
despite exploratory system provide desired basic functionality 

training dialogues build empirical mdp model state space 
transitions mdp modeling user population reactions rewards various system actions 

compute optimal dialogue policy learned mdp 

reimplement system learned dialogue policy 
section details methodology design njfun system 

njfun system njfun real time spoken dialogue system provides users information things new jersey 
njfun built general purpose platform spoken dialogue systems levin pieraccini eckert narayanan support modules automatic speech recognition asr spoken language understanding speech tts database access dialogue management 
njfun uses watson speech recognizer stochastic language understanding models trained example user utterances levin levin pieraccini tts system concatenative diphone synthesis sproat olive 
mixed initiative dialogue manager built scripting language levin 
njfun database populated nj online webpage contain information activity types amusement parks historic sites museums parks 
njfun indexes database attributes activity type location time day assume values morning afternoon evening 
informally njfun dialogue manager sequentially queries user regarding activity location time attributes respectively 
njfun rst asks user current attribute possibly attributes depending initiative 
current attribute value obtained njfun asks attribute possibly attributes 
njfun obtain value njfun moves attribute 
njfun successfully obtains value con rm value move attribute 
njfun nished acquiring attributes queries database wildcard attribute value 
binding attributes may multiple database matches returned optimizing dialogue management prompt type grammar open directive restrictive doesn sense system initiative user initiative mixed initiative de nition initiative system prompts 
user 
length njfun dialogues ranges user utterances database query 
njfun dialogues fairly short njfun asks attribute twice information acquisition part dialogue similar complex tasks travel planning danieli gerbino 
discussed methodology reinforcement learning optimize dialogue policy requires potential actions state speci ed 
recall states easy human correct action choice don want system able say goodbye initial state simulations levin 

obvious dialogue policy choices advance learning optimize dicult choices walker 
njfun restricted action choices type initiative asking attribute con rm attribute value obtained 
optimal actions may vary dialogue state subject active debate literature 
action choices available njfun shown figures 
types initiative system uses de ned combination wording system prompt open versus directive kamm type grammar njfun uses asr restrictive versus non restrictive 
examples show njfun ask user rst attributes types initiative 
njfun uses open question non restrictive grammar user initiative 
non restrictive grammar user initiative prompt choice restrictive grammar sense case 
njfun uses directive prompt restricted grammar system system initiative 
system calls asr user utterance grammar recognizes particular attribute mentioned prompt 
njfun uses directive question non restrictive grammar mixed initiative allows user take initiative supplying extra information 
non restrictive grammar designed recognize attribute explicitly mentioned directive prompt information ered attributes 
rows gure show njfun uses system initiative third attribute point user provide time day 

support continuous system functionality extended number ways larger live database support followup questions users 

ways de ning initiative walker whittaker chu carroll brown operationalization commonly applied spoken dialogue systems levin 

greet equivalent asking rst attribute 
singh litman kearns walker action prompt prompt type grammar welcome njfun 
please say activity name say list activities list activities know 
directive restrictive welcome njfun 
may help 
open know amusement parks historic sites museums parks 
please say activity name list 
directive restrictive please tell activity type 
tell location time 
directive ask please say name town city interested 
directive restrictive ask please give information 
open please tell name town city interested 
directive restrictive please tell location interested 
tell time 
directive ask time day want go 
directive restrictive want go morning afternoon evening 
directive restrictive initiative choices available njfun 
rst column speci es names actions corresponding prompts second column 
third column speci es prompt type fourth column speci es type grammar 
actions taken state grouped 
njfun vary actions con rming attribute shown 
njfun asks user explicitly verify attribute explicit con rmation expconf location exempli ed 
explicit con system initiative restrictive grammar generated templates 
example prompt con rm time attribute say want go replaced perceived value time attribute morning afternoon evening 
njfun generate con rmation prompt con rmation noconf action 
solely purposes controlling operation opposed learning discuss moment njfun internally maintains representation dialogue state operations vector variables 
variables track system greeted optimizing dialogue management action prompt template prompt type grammar expconf say interested going 
directive restrictive noconf expconf say interested directive restrictive noconf expconf say want go directive restrictive noconf con rmation choices available njfun 
rst column speci es names actions corresponding prompts second column 
third column speci es prompt type fourth column speci es type grammar 
prompt noconf con rmation action empty 
feature values explanation greet system greeted user attribute attribute worked con dence con rmed low medium high asr con dence 
explicitly con rmed rmed value value obtained current attribute tries times current attribute asked grammar non restrictive restrictive grammar history trouble previous attribute state features values 
user attribute system currently attempting obtain 
attributes variables track system obtained attribute value value system con dence value obtained number times system asked user attribute type asr grammar ask attribute 
formal state space maintained njfun purposes learning simpler operations vector due data sparsity concerns discussed 
dialogue state space contains variables summarized 
computed operations vector hand designed algorithm 
greet feature tracks system greeted user 
attribute speci es attribute njfun currently attempting obtain verify activity location time done attributes 
con dence con rmed represents con dence njfun obtaining value attribute 
values represent singh litman kearns walker lowest middle highest asr con dence values 
values set asr hears con rmation question 
value tracks njfun obtained value attribute 
tries tracks number times njfun asked user attribute 
grammar tracks type asr grammar language model obtain attribute non restrictive restrictive 
history represents njfun trouble understanding user earlier part conversation bad 
omit full de nition example njfun working second attribute location history variable set njfun activity activity con dence value needed queries obtain activity 
note state representation interests keeping state space small deliberately ignores potentially helpful information dialogue far 
example state feature explicitly tracking average asr score user utterances far keep information raw feature values previous states 
mentioned goal design small state space critical distinctions support learning 
reduces number states supports construction mdp model sparse respect limited training data 
state space utilize minimal allows initiative decisions success earlier exchanges con rmation decisions asr con dence scores grammars suggested earlier danieli gerbino walker litman walker kearns 
state space action choices precisely de ned detail policy class explored experiment de ned set deterministic mappings states system choice particular xed choice 
state action mapping representing njfun dialogue policy class exploratory initiative con rmation shown 
choice state list choices actions available 
action choices boldface ones eventually identi ed optimal learning process discussed detail 
choice states action choices total number unique policies class keeping rl methodology described goal compute implement approximately optimal policy large class basis rl applied exploratory training dialogues 
policy class obtained allowing choice system user initiative system needs ask attribute allowing choice con rming simply moving attribute system just obtained value attribute 
example initial state user greeted user greet value system choice uttering system initiative 
utterance asr output includes recognized string associated acoustic con dence score 
data obtained system development de ned mapping raw con dence values approximately equally populated partitions 

discussed system uses operations vector store information actual values previous attributes eventual database query 
uence dialogue policy way stored state features 

refers states occur dialogue 
example greet possible initial dialogue state 
states occur 
optimizing dialogue management choice states action choices noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf ask ask ask ask noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf noconf expconf policy class 
de nitions state features 
prompt please say activity name say list activities list activities know user initiative prompt may help 
example choices con rmation available states value feature 
states system con rm attribute value obtained asr accept current binding move attribute 
singh litman kearns walker execute particular policy policy class njfun chooses randomly actions choice state maximizing exploration minimizing data sparseness constructing mdp model 
note due randomization action choice prompts figures designed ensure coherence possible action sequences 
state action turn reward noconf expconf expconf tell generating dialogue 
illustrates dialogue policy class generates dialogue 
row indicates state njfun action executed state corresponding turn reward received 
initial state represents njfun rst attempt obtain attribute 
njfun executes shown possible generating rst utterance 
user response state represents njfun greeted user obtained activity value high con dence non restrictive grammar 
njfun chooses noconf action attempt con rm activity causes state change prompt generated 
third state represents njfun working second attribute location value high con dence location obtained activity user rst utterance dialogue history 
time njfun chooses expconf action con rms attribute second njfun utterance state changes 
processing time similar location leads njfun nal state performs action tell corresponding querying database presenting results user asking user provide reward 
note njfun reward terminal state shown column 
illustrates njfun dialogue generated policy class 
note dialogues figures instantiate di erent dialogue policies policy class 
example njfun begins dialogues rst state njfun executes dialogue 

experimentally optimizing policy collected experimental dialogues training testing system 
obtain training dialogues implemented njfun dialogue policy class described 
recall current attribute features state 
operations vector contains information regarding previous attributes 
optimizing dialogue management welcome njfun 
please say activity name say list activities list activities know 
visit historic site morning 
asr output zoo historic sites historic say interested going zoo 

know amusement parks historic sites museums parks 
please say activity name list 
visit historic site 
asr output visit historic sites say interested going historic site 

please give information 
user says please tell location interested 
tell time 
visit historic 
asr output historic time day want go 

asr output want go morning afternoon evening 
morning 
say want go morning 

historic sites new jersey open morning 
rst 
hear 
system 
please give feedback saying bad 
bad 
example dialogue njfun 
section 
dialogues build empirical mdp computed optimal dialogue policy mdp described section 
section describe experimental design learned dialogue policy 
section results testing learned policy show improves task completion rates performance measure chose optimize 
experimental subjects employees associated njfun project 
subjects training testing 
subjects distributed training testing pools balanced gender english rst language expertise spoken dialogue systems 
training subjects informed experiment njfun change behavior experiment set web instructions see appendix 

subsequent analyses indicated system performance depend signi cantly factors 
singh litman kearns walker task 
bored home morristown rainy afternoon 
njfun nd museum go 
task 
live cape may want take friends evening cruise 
njfun nd options 
task 
lived years managed visit historic sites 
today feeling 
njfun nd see morning 
task 
feel thirsty want morning 
close house 
task 
hard day florham park relax evening theatre 
njfun nd possible see show near florham park 
task 
live jersey city want spend afternoon enjoying nature weather beautiful 
parks nearby 
task scenarios 
training testing subjects carried free form conversations njfun complete application tasks 
example task executed user task 
subjects read task description going separate web page task accessible main experimental web page called njfun oce phone 
task njfun asked feedback experience utterance 
users hung phone lled user survey web shown 
possible responses questions shown 
answers rst question bad mapped respectively 
remaining questions users indicated strength agreement point likert scale jack foster responses strongly agree somewhat agree agree disagree somewhat disagree strongly disagree mapped respectively 
dictated step rl methodology described section rst built training version system state space action choices outlined preceding section random exploration 
mean state speci ed choice system actions training system chose randomly allowed actions uniform probability 
emphasize fact allowed choices designed way ensured dialogue generated exploratory training system intuitively sensible human user permitted successful completion task system intended perform 
important note multiple calls system training users may ectively experienced multiple dialogue policies induced random exploration test users experienced single xed deterministic policy 
optimizing dialogue management please repeat give feedback conversation 
bad complete task get information needed 
conversation easy nd place wanted 
conversation knew say point dialogue 
conversation njfun understood said 
current experience njfun njfun regularly nd place go away computer 
user survey 
training phase experiment resulted complete dialogues subjects completed tasks njfun logged sequence states corresponding executed actions 
shortest longest dialogues obtained user utterances respectively 
training set number samples state initial ask choices ask ask ask ask data illustrates random action choice method exploration led fairly balanced action distribution state 
similarly small state space fact allowed action choices state prevented data sparseness problem 
important optimal dialogue policy obtained rl unreliable infrequently visited states 
rst state initial state dialogue frequently visited state visits 
states occur near dialogue visited times 
logged data construct empirical mdp 
mentioned measure chose optimize binary reward function strongest possible measure task completion called binary completion takes value njfun queries database exactly attributes speci ed task description 
system logs matched tasks user attempting possible directly compute system logs user completed task 
completed mean binding attributes activity type location time day exact values speci ed task description associated web page 
way training dialogue automatically labeled case completed task 
note de nition task completion guarantees user heard database entries matching task speci cations 
relaxations reward measure types measures reward measure discussed section 
computed optimal dialogue policy learned mdp value iteration cf 
section 
action choices constituting learned policy boldface singh litman kearns walker 
note choice xed states meaning values identical value iteration 
learned policy njfun chooses randomly certain action pairs 
intuitively learned policy says optimal initiative user initiative back mixed system initiative attribute 
note speci backo method di ers attribute system initiative attribute generally mixed initiative attribute 
respect con rmation optimal policy mainly con rm lower con dence values 
point con rmation unnecessary di ers attributes con dence level attribute lower levels attributes depends features state con dence grammar history 
asr con dence dialogue policy sophisticated previous approaches 
kobayashi litman pan 
njfun learn ne grained distinctions optimal policy comparison possible exploratory policies 
initiative con rmation results suggest dialogue problematic njfun 
example dialogue optimal policy 

experimentally evaluating optimized policy testing phase njfun reimplemented deterministic learned policy 
test subjects performed tasks training resulting complete test dialogues 
primary empirical test proposed methodology course extent statistical signi cance improvement allegedly optimized measure binary task completion training test populations 
fact task completion measured binary completion increase training testing 
sections devoted analysis test related tests 
comparing learned policy training policy table summarizes training versus testing performance njfun various evaluation measures 
recall training dialogues njfun randomly chosen policies policy class 
testing dialogues njfun single learned policy 
learned policy optimized task success measure binary completion types measures evaluate dialogue systems task success dialogue quality eciency usability danieli gerbino kamm 
evaluate performance learned policy respect original reward measure number potential reward measures optimize test system 
important results summarized rst rows table 
rst row summarize performance binary completion reward measure discussed preceding section 
average value reward measured dialogues generated randomized training system recall range average value measure dialogues learned test system improvement value standard optimizing dialogue management evaluation measure train test 
value binary completion weak completion asr web feedback easy say njfun understood reuse table train versus test performance various evaluation measures 
rst column presents di erent measures considered see text detail second column average value measure obtained training data third column average value obtained test data fourth column shows di erence test average train average positive number win negative number loss fth column presents statistical signi cance value obtained standard test 
sample test subject means 
result corresponds improvement completion rate training dialogues completion rate testing dialogues 
second row table shows performance improves training test closely related measure weak completion weak completion relaxed version task completion gives partial credit attribute values correct wildcards value sum correct number attributes 
attribute wrong user says system hears morristown value 
motivation re ned measure reward indicates information desired contained database entries user non negative reward means information desired buried larger set irrelevant items smaller values reward 
training dialogue average weak completion range test dialogue average 
large improvement time signi cant level 
note policy dictated optimizing training mdp binary completion implemented test system policy dictated optimizing training mdp weak completion implemented similar minor di erences action choices 

conventionally value considered statistically signi cant values considered indicative statistical trend 

emphasize improvement weak completion system designed optimize binary completion single test system examined performance changes di erent evaluation measures reward measure 
singh litman kearns walker measure third row asr variation binary completion 
evaluating task success asr evaluates dialogue quality 
particular asr approximates speech recognition accuracy database query computed adding correct attribute value wildcard 
task go near morning system queries database activity new jersey morning binary completion weak completion asr 
table shows average value asr increased training testing range signi cant improvement 
improvement occurred learned policy testing optimized asr 
measures considered far objective reward measures sense reward precisely de ned function system log dialogue computed directly log 
examine performance changes training test set subjective usability measures provided human user dialogue considered 
recall dialogue task accompanied web survey 
measure web feedback obtained rst question survey recall range 
measures easy say njfun understood reuse obtained questions recall range 
optimize subjective measures priori expectations improvement degradation 
rows table shows fact nd statistically signi cant changes mean direction measures 
observed curious move middle ect smaller fraction users extremely positive extremely negative things say test system training system 
shows entire distribution values train test systems subjective measures shows optimizing test system task completion measure consistently shifted weight away tails subjective measures intermediate values 
rm explanation phenomenon consistency occurs varying degree subjective measures noteworthy 
sum empirical results demonstrated improvement optimized task completion measure improvement non optimized related objective measures 
contrast results show statistically signi cant changes number non optimized subjective measures interesting move middle ect 
ect expertise addition task independent performance changes training testing policy just discussed task dependent performance changes 
example signi cant interaction ect policy task evaluated binary completion 
believe ect user expertise system previous suggests novice users perform comparably experts tasks kamm 
learned policy 
experimental design consisted factors group factor policy groups factor task 
way analysis variance anova compute interaction ects policy task 
optimizing dialogue management train test train test train test train test train test distributions subjective measures 
web feedback 
easy 
say 
njfun understood 
reuse 
singh litman kearns walker interaction ects task policy 
bar charts show binary completion rate tasks order test train policies 
test policy performance better tasks train policy performance better rst tasks providing evidence learned test policy slightly optimized expert users 
tasks user possible learned policy slightly optimized expert users 
explore hypothesis divided corpus dialogues novice tasks expert tasks users 
learned policy fact lead large signi cant improvement binary completion experts increasing number completed dialogues training testing train test lower train means rst tasks higher tasks 
appropriate system repeat usage case system primarily novice users system need retrained 
comparison hand designed policies results far indicate improvement training testing potential limitation set policies class may best baseline comparison learned policy 
standard alternative comparison best hand designed xed policy 
agreement literature authors best hand designed policy 
natural ask optimized system compares systems employing dialogue policy picked human expert 
implementing number hand picked policies gathering dialogues comparing learned system time consuming expensive fact exactly kind repeated sequential implement test methodology attempting replace training system provides convenient mathematically sound proxy 
section show optimizing dialogue management performance learned policy better standard xed policies computing reward trajectories empirical mdp consistent alternative policy 
alternatives handful consistent trajectories mdp section analysis mdp accuracy 
training dialogues generated making random choices dialogue training set consistent policy policy class provides unbiased monte carlo trial 
consistent mean random choices dialogue agree dictated 
average rewards consistent training dialogues obtain unbiased estimate return 
policy 
emp 
avg 
mdp value value test rm rm rm rm mixed table comparison standard policies 
compare test policy standard policies monte carlo method 
rst column presents di erent policies considered see text detail second column shows number consistent trajectories training data third column shows empirical average reward consistent trajectories fourth column shows estimated value policy learned mdp fth column shows statistical signi cance value policy loss respect test policy 
table compares performance learned test system binary completion reward measure xed policies class common choices dialogue systems literature suggested dialogue system designers 
rm policy uses system initiative con rms rm policy uses system initiative con rms rm policy uses user initiative con rms rm policy uses user initiative con rms mixed policy varies initiative dialogue 
rm policy test policy better signi cance near level di erence rm signi cant 
surprisingly xed rm policy fared best comparison similar policy learned 
addition optimizing large class policy choices considerably re ned typical reinforcement learning approach outperforms number natural standard policies 
singh litman kearns walker goodness mdp ask estimate state estimate simply fortunate mdp poor predictor value actions happened chosen policy chance 
closing evidence view er results simple experiment randomly generated deterministic policies policy class 
policy training dialogues consistent compute unbiased monte carlo estimate expected binary completion return exactly done hand picked expert policies table 
estimate paired value start state learned mdp 
mdp perfect model user population responses system actions monte carlo estimate simply noisy estimate correlation quantities signi cant course dependent number samples monte carlo estimate best linear relationship simply slope intercept normally distributed noise variable adjustable mean variance decreasing number consistent trajectories increases 
extreme mdp relation user population responses system actions uncorrelated best terms linear slope intercept ignore simply model noise 
results summarized table indicate closer case 
random policies generated correlation positive rejected null hypothesis variables uncorrelated level signi cance furthermore squares linear gave slope coecient close intercept close predicted idealized case 

discussion practical methodology applying reinforcement learning problem optimizing dialogue policy design spoken dialogue systems 
methodology takes relatively small number exploratory dialogues directly computes apparent optimal policy space thousands policies performing sequence implementations handful particular policies 
method construct training version njfun spoken dialogue system empirically demonstrated improved performance njfun optimization 
controlled experiment human users njfun veri ed signi cant improvements reward measure optimization performed 
showed signi cant improvements objective reward measures test policy optimized measures improvements set subjective measures despite interesting change distributions 
showed learned policy better non deterministic policy class better xed choices proposed literature 
results demonstrate application reinforcement learning allows empirically optimize system dialogue policy searching larger search space explored traditional methods 
optimizing dialogue management 
policies corr 
coe value slope inter 
table test mdp accuracy 
generated deterministic policies randomly 
policy computed pair numbers estimated value mdp value trajectories consistent training data 
number consistent trajectories varied policy 
rst row policies second row policies consistent trajectories row policies consistent trajectories 
reliability empirical estimate policy increases increasing number consistent trajectories 
third column presents correlation coecient empirical mdp values 
fourth column presents statistical signi cance correlation coecient 
main result hypothesis sets values uncorrelated soundly rejected 
columns slope intercept resulting best linear sets values 
reinforcement learning applied dialogue systems previous approach di ers previous respects 
biermann long test reinforcement learning implemented system experiments levin 
utilized simulated user model 
walker 
methodology similar testing reinforcement learning implemented system human users 
walker 
explore initiative policies policies information presentation spoken dialogue system accessing email phone 
explored policy choices states dialogue conceivably explored traditional methods compared choice states explored 
note learned policy dialogue decisions asr con dence conjunction features varied initiative con rmation decisions ner grain previous learned policy standard policy investigated dialogue system literature 
example predicted complex interesting back policy respect initiative attribute 
system experiments begun address challenges spoken dialogue systems prevailing theory application rl balancing competing concerns random exploration user experience training system keeping state space small possible order learning data ecient retaining information necessary decision making 
challenges remain addressed 
provide general methodology reducing state space manageable size 
furthermore learned mdp model best approximation may introducing problem hidden state partial observability singh litman kearns walker problem choosing optimal actions state 
situations hidden state richer pomdp model appropriate kaelbling 
roy pineau thrun currently exploring pomdp style approach yield mdp speeds spoken dialogue system robot state represent user intentions system state 
wish understand aforementioned results subjective measures explore potential di erence optimizing expert users novices automate choice state space reward dialogue systems methodology assumed investigate learned reward function walker explore informative non terminal rewards 
acknowledgments authors fan jiang substantial ort implementing njfun wieland eckert esther levin roberto pieraccini technical help julia hirschberg comments draft david mcallester richard sutton esther levin roberto pieraccini helpful conversations 
appendix experimental instructions njfun new jersey place go recommender general description njfun experimental spoken dialogue system allows access database things new jersey telephone conversation 
asked call njfun di erent tasks 
try task eciently 
note speaking di erent version njfun phone call njfun vary behavior single phone call 
instructions calling njfun task scenario 
please read instructions calling 
rare occasions may get apparently dead line call 
indicates lines busy 
occurs hang call 
please speaker phone 
task asked say bad order provide feedback phone call njfun 
please hang phone providing feedback 
hang phone brief questions answer 
njfun aborted complete task please finish survey continue task 
nished tasks opportunity provide comments 
problems experiment call diane satinder 
participating experiment 
optimizing dialogue management task scenarios tasks try experiment 
task time prescribed order 
nish task provided feedback hang phone nish survey task 
nished tasks please provide nal comments 
click try task click try task click try task click try task click try task click try task click provide nal comments bertsekas tsitsiklis 

neuro dynamic programming 
athena scienti biermann long 

composition messages speech graphics interactive systems 
proceedings international symposium spoken dialogue pp 

chu carroll brown 

tracking initiative collaborative dialogue interactions 
proceedings th annual meeting association computational linguistics pp 

crites barto 

improving elevator performance reinforcement learning 
proceedings nips pp 

danieli gerbino 

metrics evaluating dialogue strategies spoken language system 
proceedings aaai spring symposium empirical methods discourse interpretation generation pp 

haller mcroy 

special issue computational models mixed initiative interaction part 
user modeling user adapted interaction 
haller mcroy 

special issue computational models mixed initiative interaction part 
user modeling user adapted interaction 
jack foster 

intelligent dialogues automated telephone services 
international conference spoken language processing icslp pp 

singh litman kearns walker kaelbling littman moore 

reinforcement learning survey 
journal arti cial intelligence research 
kamm 

user interfaces voice applications 
roe 
eds voice communication humans machines pp 

national academy press 
kamm litman walker 

novice expert ect tutorials user expertise spoken dialogue systems 
proceedings international conference spoken language processing icslp 
levin pieraccini 

generation 
proc 
arpa spoken language systems technology workshop austin texas 
levin pieraccini eckert 

stochastic model human machine interaction learning dialog strategies 
ieee transactions speech audio processing 
levin pieraccini eckert narayanan 

spoken language dialogue theory practice 
proc 
ieee workshop automatic speech recognition understanding 
litman pan 

predicting adapting poor speech recognition spoken dialogue system 
proc 
seventeenth national conference arti cial intelligence aaai 
litman pan walker 

evaluating response strategies webbased spoken dialogue agent 
proceedings sixth annual meeting association computational linguistics pp 

litman walker kearns 

automatic detection poor speech recognition dialogue level 
proceedings seventh annual meeting association computational linguistics pp 

kobayashi 

dialog control strategy reliability speech recognition 
proc 
international symposium spoken dialogue pp 

roy pineau thrun 

spoken dialog management robots 
proceedings th annual meeting association computational linguistics 
sturm den os cremers 

evaluation timetable information system developed arise project 
interactive voice technology telecommunications applications pp 

singh kearns litman walker 

reinforcement learning spoken dialogue systems 
proc 
nips 
optimizing dialogue management smith 

evaluation strategies selectively verifying utterance meanings spoken natural language dialog 
international journal human computer studies 
sproat olive 

approach text speech synthesis 
paliwal 
eds speech coding synthesis pp 

elsevier 
sutton barto 

reinforcement learning 
mit press 
tesauro 

temporal di erence learning td gammon 
communications acm 
walker 

application reinforcement learning dialogue strategy selection spoken dialogue system email 
journal arti cial intelligence research 
walker narayanan 

learning optimal dialogue strategies case study spoken dialogue agent email 
proceedings th annual meeting association computational linguistics coling acl pp 

walker litman kamm 

evaluating spoken dialogue agents paradise case studies 
computer speech language 
walker rambow 

spot trainable sentence planner 
proceedings north american meeting association computational linguistics 
walker whittaker 

mixed initiative dialogue investigation discourse segmentation 
proc 
th annual meeting acl pp 

young 
probablistic methods spoken dialogue systems 
philosophical transactions royal society series pp 

watkins 

models delayed reinforcement learning 
ph thesis cambridge university 

