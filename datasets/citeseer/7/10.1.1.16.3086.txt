dynamically adaptive parallel programs michael voss rudolf eigenmann purdue university school electrical computer engineering 
dynamic program optimization recourse optimizing compilers machine program parameters necessary applying optimization technique unknown runtime 
movement portable parallel programs facilitated language standards openmp optimizations developed high performance machines longer applied prior runtime potential performance degradation 
alternative propose dynamically adaptive programs programs adapt runtime environment 
discuss key issues successfully applying approach show examples application 
experimental results dynamically adaptive programs seek eliminate redundant runtime data dependence tests select optimal tile size tiled loops serialize loops profit parallelism 
today known program optimization techniques applied prior runtime assume detailed knowledge target machine program available 
unfortunately information may unknown program execution 
information technique requires available optimization applied applied possibly incorrect assumptions architecture application 
assumptions significantly limit performance improvement cause degradation 
context parallel programs availability portable parallel languages openmp api exacerbates problem 
portability attractive reasons ease distribution new computing paradigms network computing kf llm metacomputing fk 
portability requires advanced knowledge target configuration 
loss information means done high performance codes done portable programs 
performance key issue programs simply discard optimizations statically performed 
propose alternative dynamically adaptive programs 
programs perform optimizations dynamically usefulness correctness evaluated 
supported part army contract dabt nsf award asc nsf career award 
necessarily representative positions policies army government 
michael voss rudolf eigenmann discuss scheme highlight key performance issues 
demonstrate scope applying diverse example problems 
case studies apply dynamic optimization reduce redundant runtime data dependence testing see adaptive code outperform original sequential code 
scheme re evaluates loop classification serial parallel necessary executes faster program performs dependence test slower program executes loop advanced knowledge data independence 
apply dynamically adaptive programming approach tiling show matrix multiplication kernel automatically selects best tile size improves version 
dynamic scheme best possible statically optimized program advanced knowledge best tile size 
show dynamically adaptive programs automatically generated polaris parallelizing compiler bde bef detect loops amortize overheads associated parallel execution serialize leading reductions parallel execution time large processors origin 
average programs loops serialized profiling showed larger improvement 
overview dynamically adaptive programs dynamically adaptive programs seek apply optimizations usefulness correctness evaluated prior program execution 
runtime programs modify behavior techniques applied evaluated 
optimizations re evaluated parameters ecting usefulness correctness change 
dynamically adaptive programs written schemes 
shows state transition diagrams binary selection ary selection iterative modification approaches 
binary selection optimization turned code section 
done simply verifying correctness usefulness optimization runtime 
optimized code shown correct better performance unoptimized code 
choice re evaluated environmental parameters ecting correctness usefulness change 
approaches dynamic data dependence testing dynamic serialization binary selection scheme 
cases determined loop executed parallel sequentially turning parallelism 
approach tiling uses ary selection 
try di erent tile sizes select yields shortest execution time 
tiling iterative modification discuss reason choosing ary selection section 
dynamically adaptive parallel programs verify verify correctness usefulness start optimization safe useful run execute configurable code section env changes optimization unsafe useful optimization binary selection setup setup code configuration start optimization monitor execute monitor code section env changes run execute best configuration configurations tried ary selection start optimization monitor execute monitor code section env changes run execute current configuration iterative modification optimal optimal modify modify configuration fig 

schemes dynamically adaptive programs 
case study runtime dependence tests runtime data dependence tests traditional compile time dependence tests determine safe execute loop parallel smc rp rp 
common case traditional tests fail array accessed subscripted subscript 
case access form loop index subscript array 
compiler determine values held array cross iteration data dependence conservatively assumed 
runtime test loop speculatively run parallel accesses array tracked 
loop completes determined gathered information cross iteration dependencies exist 
dependencies loop re executed sequentially 
ensure correctness variables may modified copied prior speculative execution parallel loop restored loop parallel 
obvious downside runtime test overhead introduces program 
variables may modified copied prior loop execution variables may dependencies tracked loop executes gathered information evaluated michael voss rudolf eigenmann loop completes 
loop cross iteration dependencies overheads compounded need restore modified variables re execute loop sequentially 
fortunately subscript array may remain constant program may change infrequently 
phenomenon called schedule reuse law may exploited minimize incurred overheads 
dependence test need applied values subscript array modified previously determined classification parallel parallel valid 
example loop statically parallelized polaris loop perfect benchmark dyfesm 
loop parallelized due array mx accessed subscripted subscript 
loop executed correctly parallel 
runtime test applied loop show loop parallel 
noted subscript array responsible access pattern remains constant program execution 
su ce perform runtime test loop verifying run parallel simply execute parallel 
runtime data dependence testing fits dynamically adaptive scheme 
optimization performed loop parallelization 
executing program decide optimization beneficial loop loop fact parallel 
done binary selection classifying loop parallel parallel runtime data dependence test 
classification re evaluated conditions determining applicability change subscript array modified 
experiment hand modified dyfesm incorporate scheme 
test loop executed test flag set false 
point program subscript array modified test flag set true 
time loop executed classification loop parallel serial valid test false 
test true dependence test performed 
executed hand modified version dyfesm processors ultrasparc enterprise 
ran version performed runtime test ran parallel testing 
versions expressed parallelism openmp api 
shows versions loop approach linear speedup 
overhead due critical section enclose reduction statements parallel versions reductions statements form 
performing runtime test improves sequential version 
dynamic approach retesting done necessary improvement seen 
fixed parallel version having advanced knowledge loop data independence shows slight improvement dynamically adaptive version 
dynamically adaptive program able perform optimal statically optimized program 
dynamically adaptive parallel programs fig 

speedup processors ultrasparc enterprise versions loop dyfesm 
test version refers program runtime data dependence test applied execution loop 
dynamic version loop subscript array modified 
fixed parallel version refers parallel version testing performed 
assumes loop parallel 
case study tiling optimization commonly enhance temporal locality parallel loop nests tiling 
loop tiled partitioning iteration space regions chosen size shape known tiles 
iterations tile executed processor begins executing new tile 
temporal reuse tile reused locations remain resident processor cache accesses 
original iteration order evicted data item cache large number accesses reuse 
common example demonstrate applicability tiling matrix multiplication 
shows matrix multiplication tiling 
note element reused iteration loop 
cache large hold entire array iteration loop cache misses accesses cache large entire array need reloaded cache iteration loop 
tiled version matrix multiplication shown 
smaller tiles array operated iteration loop 
choosing small tile size ensure portion remain cache iterations loop 
tile size small unnecessary overheads introduced added iterations outer loops 
di culty arises choosing tile size 
decision requires size matrices known size cache known 
writes program portable di erent machines size cache assumed 
size arrays may statically determinable 
dynamic scheme determine proper tile size 
michael voss rudolf eigenmann enddo enddo enddo kk blk jj blk kk min kk blk jj min jj blk enddo enddo enddo enddo enddo tiled fig 

matrix multiplication original tiled sized tiles 
demonstrate applicability adaptive programs problem hand modified matrix multiplication kernel automatically select tile size ary selection 
kernel performs matrix multiplications matrices 
element matrices byte real number 
approach try possible tile sizes choose yields minimum execution time 
time multiplication run executed tiling time recorded 
execution multiplication performed tile size number times nest executed 
tile sizes 
tried 
continues tile size reached assumed smaller tile sizes ine cient machines 
step tile size yielding minimum time recorded 
tile sizes tried producing minimum time subsequent multiplication 
iterative modification select tile size 
approach loop ary selection approach run smaller tile sizes subsequent execution loop 
iterative modification scheme soon configuration yielded larger execution time previously configuration 
point assumed overhead associated smaller tile size begun set benefit 
unfortunately approach lead selection local minimum chose cient ary selection technique 
ran hand modified kernel ultrasparc enterprise direct mapped mbyte external data caches direct mapped kbyte internal data caches sparcstation direct mapped kbyte external caches internal data caches 
machines ran processors 
ran kernel tile size fixed various sizes multiplication original form 
shows dynamic scheme shows performance close best fixed tile size machines 
dynamically adaptive parallel programs machines dynamic scheme chose correct tile size able perform best statically optimized program 
fig 

speedup matrix multiplications 
case study dynamic serialization structured parallel applications executed current shared memory machines may run slower serial counterparts 
programs slow contain parallel regions amortize overheads associated parallel execution 
important recognize situations parallel execution code section perform original serial version undo parallelization 
overheads responsible poor performance include factors fork join costs communication overheads added memory latency accessing remote data 
quantities target specific 
addition performed program region depends input may vary program runs 
impact parallel overheads function program program input machine configuration 
portable program parameters known runtime 
deciding parallel code region profitable benefit dynamically adaptive runtime scheme 
added pass polaris parallelizing compiler automatically generate programs include dynamic scheme serialization 
loop classified dynamically serial parallel state transition diagram shown 
scheme binary selection data dependence test 
warmup test states collapsed verify state serial parallel states refer run state 
initially loop classified parallel starts warmup state 
loop allowed execute iterations parallel timing michael voss rudolf eigenmann done 
done timing loop influenced cold misses cache 
warmup run warmup parallel loop test state run time parallel loop state test start state loop parallel test passes update iterations state parallel loop parallel test fails update iterations state serial loop serial serial state run serial loop parallel state run parallel loop iterations iterations state warmup loop parallel state warmup loop parallel fig 

state transition diagram dynamic serialization 
test classify loop serial parallel sequential profile program base machine 
average iteration time loop recorded base machine fed polaris compiler adaptive program generated 
runtime small kernel embedded program run timed determine scaling factor scale profiled timings target machine 
test compare iteration scaled sequential time measured parallel time 
parallel time longer loop classified serial 
model basis proof concept 
optimizations done improve model 
example decision re test refined reduce unnecessary testing loop executes quickly serialized may need re tested iteration count increases little 
may note loops execute may correctly classified 
loops slow usually small loops important executed frequently 
evaluate technique dynamic serialization generated parallel versions benchmark programs perfect spec benchmarks suites arc flo mdg hydro swim tomcatv 
programs generated polaris parallelism expressed dynamically adaptive parallel programs ing openmp 
timed original versions processors origin 
loop timings collected runs generate version program loops executed quickly processor serialized 
view profile program best possible statically optimized program serialization avoid overheads 
generated versions program dynamically adaptive scheme outlined previous section 
base timings dynamic scheme collected sparcstation 
dynamic scheme profile approach uses profile profile scaled allows program run machine just machine profiling done 
ran various versions program processors origin 
shows dynamic scheme able improve performance programs 
normalized execution times adaptive programs improved average 
profile scheme likewise improved performance programs able improve performance average 
fig 

normalized execution time measured benchmark programs 
noticeably poor performance arc explained 
arc benchmark shows large degradation adaptive program profile program 
due fact profiles loops execute quickly sequential version serializing parallel version causes degradation application 
attributed cache ects induced serialization 
example consecutive parallel loops access regions array loop loads data cache second loop sees misses array 
loop serialized loops incur misses 
michael voss rudolf eigenmann dynamically adaptive programs outperform original unoptimized programs cases 
addition average optimal profile programs show better performance dynamic scheme 
related techniques proposed adapt program dynamically input runtime environment 
bdh develop multiversion loops loop parallelization parallel serial version selected runtime 
multiversion loops dynamic serialization scheme 
gb gupta bodik develop framework runtime application loop fission loop fusion loop alignment loop reversal 
gupta bodik focus modifying parameters alter execution code sections generate code 
similar technique perform tiling dynamically 
approach useful generating infeasible impractical 
techniques developed apply dynamic optimization specific optimization problems 
saavedra park dynamically determine amount software prefetching distance prefetch instructions improve performance parallel programs networks workstations sp 
hall martonosi propose system dynamically adjusts number processors compiler parallelized programs improve throughput hm 
dr rinard dynamic feedback automatically select best synchronization policy parallel object programs 
rinard briefly outline idea dynamic feedback generic approach optimization 
alternating sampling production phases user defined lengths 
contrast approach re evaluates configurations automatically environmental factors ecting previous decisions change 
parallel programming technique thought discussing runtime techniques runtime data dependence testing 
smc rp rp runtime tests performed uncover parallelism undetectable compile time 
authors discuss schedule reuse phenomenon exploited reduce number times test need applied 
apply techniques runtime data dependence example show fit dynamic optimization framework 
schemes discussed forms dynamic optimization 
goal solve particular problem schemes develop generic scheme dynamic optimization show applicability wide range problems 
aim combining integrating techniques discussed literature cited new techniques generic framework 
dynamically adaptive parallel programs shown dynamically adaptive programs ective means applying optimizations information required determine usefulness correctness unavailable runtime 
discussed key features programs issues addressed cient implementations 
described binary selection ary selection iterative modification approaches 
demonstrated dynamic approach remove redundant runtime data dependence tests yield decrease execution time compared performing tests 
performance program tested loop explicitly parallelized 
second experiment tile size automatically selected matrix multiplication kernel 
resulting code showed improvements large code best statically tiled version 
third dynamic serialization scheme automatically applied polaris compiler 
showed decreases parallel execution time large 
average profile approach showed larger gain 
dynamically adaptive programs new paradigm promises overcome severe limitations optimizing compilers dependence availability compile time information 
general framework possible defer optimization decisions runtime inspect state program machine environment timers hardware monitors evaluate optimizations multi version code parameterized code dynamic recompilation runtime tuning 
may yield truly performance portable programs lead new generation optimizing compilers 
bde william blume ramon rudolf eigenmann john jay thomas lawrence lee david padua paek bill pottenger lawrence rauchwerger peng tu 
advanced program restructuring high performance computers polaris 
ieee computer pages december 
bdh davies wolfe 
multiple version loops 
international conf 
parallel processing pages august 
bef blume eigenmann lee lawrence padua paek petersen pottenger rauchwerger tu 
restructuring programs high speed computers polaris 
icpp workshop challenges parallel processing pages august 
dr pedro rinard 
dynamic feedback ective technique adaptive computing 
proc 
acm sigplan conf 
programming language design implementation las vegas nv may 
michael voss rudolf eigenmann fk ian foster carl 
globus metacomputing infrastructure toolkit 
international journal supercomputing applications january 
gb rajiv gupta bodik 
adaptive loop transformations scientific programs 
ieee symposium parallel distributed processing pages san antonio texas october 
hm mary hall margaret martonosi 
adaptive parallelism code 
proc 
nd suif compiler workshop august 
kf kapadia jose fortes 
design demand network computing system purdue university network computing hubs 
proc 
ieee symposium high performance distributed computing pages chicago il 
law thomas lawrence 
implementation run time techniques polaris fortran 
master thesis univ illinois urbanachampaign center supercomputing res 
dev 
llm litzkow livny mutka 
condor hunter idle workstations 
proc 
th int conf 
distributed computing systems pages june 
rp lawrence rauchwerger david padua 
privatizing doall test run time technique doall loop identification array privatization proceedings th acm international conference supercomputing manchester england pages july 
rp rauchwerger padua 
lrpd test speculative run time parallelization loops privatization reduction parallelization 
proceedings sigplan conference programming languages design implementation june 
smc saltz crowley 
run time parallelization scheduling loops 
ieee transactions computers may 
sp saavedra park 
improving ectiveness software prefetching adaptive execution 
proc 
conf 
parallel algorithms compilation techniques boston ma october 
michael voss rudolf eigenmann 
reducing parallel overheads dynamic serialization 
proc 
nd merged symposium ipps spdp th international parallel processing symposium th symposium parallel distributed processing appear san juan puerto rico april 
