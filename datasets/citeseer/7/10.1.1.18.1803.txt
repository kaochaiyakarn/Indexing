lifelong robot learning sebastian thrun tom mitchell university bonn institut ffir informatik ill 
bonn germany school computer science carnegie mellon university pittsburgh pa usa 
learning provides useful tool automatic design autonomous robots 
research learning robot control predominantly focussed learning single tasks studied isolation 
robots encounter multitude control learning tasks entire lifetime opportunity transfer knowledge 
order robots may learn invariants individual tasks environments 
task independent knowledge employed bias generalization learning control reduces need real world experimentation 
argue knowledge transfer essential robots learn control moderate learning times complex scenarios 
approaches lifelong robot learning capture invariant knowledge robot environments 
approaches evaluated hero mobile robot 
learning tasks included navigation unknown indoor environments simple find fetch task 
learning 
traditionally assumed robotics research accurate priori knowledge robot sensors important environment available 
assuming availability accurate models world robot kind environments robots operate consequently kind tasks robots solve limited 
limitations especially arise factors knowledge bottleneck 
human designer provide accurate models world robot 
engineering bottleneck 
sufficiently detailed knowledge available making computer accessible hand coding explicit models robot hardware sensors environments require unreasonable amounts programming time 
tractability bottleneck 
early recognized realistic robot mains complex handled efficiently schwartz canny available technical report iai tr university bonn dept computer science ill march 
bylander 
computational tractability turned severe ob designing control structures complex robots complex domains robots far reactive 
precision bottleneck 
robot device precise accurately execute plans generated internal models world 
research autonomous robots changed design autonomous agents pointed promising direction research robotics see exam ple brooks papers maes 
reactivity real time operation received considerably attention example optimality completeness 
approaches dropped assumption perfect world knowledge available systems operate extreme domain specific initial knowledge available 
consequently today robots facing gradually unknown hostile environments orient explore environments autonomously recover failures solve families tasks 
robots lack initial knowledge environments learning inevitable 
term learning refers variety algorithms characterized ability replace missing incorrect world knowledge experimentation observation generalization 
learning robots collect parts domain knowledge improve time 
dependent human instructor provide knowledge 
learning robot architecture typically flexible deal class environments robots tasks goals 
consequently internal prior knowledge available weak solve concrete problem line 
order reach goal learning robots rely interaction environment extract information 
different learning strategies differ mainly aspects way experimentation exploration way generalize observed experience type amount prior knowledge constrains space internal hypothesizes world 
optimal general learning technique autonomous robots learning techniques characterized trade degree flexibility size gaps domain knowledge amount observations required filling gaps 
generally speaking universal robot learning architecture experimentation expect robot take learn successfully vice versa 
advantage applying learning strategies autonomous robot agents obvious learning robots operate class initially unknown environments compensate changes re adjust internal belief environment 
empirically learned knowledge grounded real world 
long recognized ai learning key feature making autonomous agents capable solving complex tasks realistic environments 
research produced variety rigorous learning techniques allow robot acquire huge chunks knowledge self 
see example mel moore pomerleau tan robot control learning problem 
robot agent able perceive state environment sensors change effectors actions 
reward function maps sensations rewards measures performance robot 
control learning problem problem finding control policy generates actions reward maximized time 
mahadevan connell lin kuipers byun 
exactly concrete problem addressed robot learning 
outline general definition learning robot control 
assume robot acts environment world 
time step environment certain state state mean sum quantities environment may change lifetime robot 
robot able partially perceive state environment sensors 
able act effectors shown defines set possible sensations set actions robot execute 
actions including zero actions robot change state world 
environment understood mapping states actions states 
example imagine autonomous mobile robot task keep floor clean 
worlds robot acting buildings including obstacles surrounding robot floors dirt floors humans walk 
appropriate sensors include camera mounted robot arm set sensations space camera images 
actions go forward turn switch vacuum lift arm 
order define goals robot assume robot reward function maps sensations scalar reward values 
reward evaluates success robot solve tasks simple form reward positive say tee robot reaches goals negative tee robot fails zero 
positive reward corresponds pleasure negative reward represents pain 
mobile robot domain example reward may positive dirt floor negative reward may received robot collides furniture runs battery power 
control learning problem problem finding control function generates actions reward maximized time 
formally control learning problem described way control learning problem tm su maximizes time formulation robot control learning problem extensively studied field reinforcement learning 
sutton barto 
far approaches emerged field studied robot learning minimal set assumptions robot able sense able execute actions actions effect sensations pre reward function defines goals robot 
goal reinforcement learning maximize reward time 
note general definition lacks specifications robot hand environment kind reward functions robot expected face 
approaches able adaptively solve robot control learning problem general learning techniques 
restricting assumption approaches reinforcement learning robot able sense state world reliably 
case suffices learn policy function sensation action control policy purely reactive 
barto pointed barto problem learning control policy attacked dynamic programming techniques bellman 
turns robots access complete state descriptions environments learning control complex robot worlds large state spaces practically feasible 
takes experimentation acquire knowledge required maximizing reward fill huge knowledge gaps robot hardware slow 
argue better learning techniques invented decrease learning complexity general 
certainly space better learning generalization techniques gradually decrease amount experimentation required techniques applicable complex real world robot environments sparse reward difficult tasks 
complexity knowledge acquisition inherent formulation problem 
real world experimentation central bottleneck general learning technique utilize prior knowledge robot environment 
natural agents humans learn better artificial agents 
better question ask learning problem faced natural agents simpler artificial ones 
attempt give general answers general questions 
point importance knowledge transfer lifelong learning problems order robots learn complex control 
necessity lifelong agent learning humans typically encounter multitude control learning problems entire lifetime robots 
henceforth interested lifelong learning problem faced robot agent learn collection control policies variety related control tasks 
control problems vi involves robot set sensors effectors may vary particular environment vi reward function defines goal states problem 
example industrial mobile robot face tasks shipping packages delivering mail supervising critical processes guarding place night 
scenario environment tasks reward function varies 
alternatively window cleaning robot able climb fronts buildings move arbitrarily walls windows single task cleaning windows 
face multiple fronts windows environments lifetime 
lifelong learning problem different environment reward function robot agent find different control policy fi 
lifelong learning problem agent corresponds set control learning problems lifelong learning problem fi maximizes ti time course agent approach lifelong learning problem handling control learning problem independently 
opportunity agent considerably better 
control learning problems defined terms potentially agent able reduce difficulty solving th control learning problem knowledge acquired solving earlier control learning problems 
example robot learn deliver laser printer output collect trash environment able learned task simplify learning second 
problem lifelong learning offers opportunity synergy different control learning problems speed learning lifetime agent 
viewing robot learning lifelong learning problem motivates research bootstrapping learning algorithms transfer learned knowledge learning task 
bootstrapping learning algorithms start low complexity learning problems gradually increase problem solving power harder problems 
similar humans robots learn simple tasks low level navigation hand eye coordination successful draw attention increasingly difficult complex learning tasks picking robot experiments described wheeled hero robot manipulator gripper 
equipped sonar sensors top robot directed rotating mirror give full sweep values wrist rotated manipulator give sweep 
sonar sensors return approximate echo distances 
sensors inexpensive noisy 
delivering laser printer output 
example singh singh lin lin demonstrate learning related control tasks increasing complexity result remarkable synergy control learning tasks improved problem solving power 
experiments simulated mobile robots able solve complex navigation tasks robots simpler related learning tasks 
report systems unable learn complex tasks isolation pointing importance knowledge transfer robot learning 
reminder organized follows 
sections briefly approaches lifelong learning problem 
approaches implemented evaluated hero mobile robot manipulator shown 
approach called explanation neural ebnn assume environment agent stays control learning tasks 
allows robot learn task independent action models 
learned action models provide means transferring knowledge control learning tasks 
ebnn action models explain analyze observations 
section drop assumption static environments 
describe mobile autonomous robot solve episode starting initial state action sequence observed produce final reward 
robot agent uses action models capture previous experiences environment explain observed episode bias learning 
see text explanation 
task different related environments 
demonstrate robot learn transfer environment independent knowledge captures tics sensors invariant characteristics environments 
section review related approaches robot learning utilize previously learned knowledge 
see types techniques grouped categories 
concluded section explaining observations terms prior experiences defined previous section lifelong learning problem faced learning robot agent problem learning collections tasks families environments entire lifetime 
section draw attention particular subtype lifelong learning problems lifelong learning problem robots spend life exclusively environment 
restricted class lifelong learning scenarios plays important role autonomous agent research 
prospective robot agents housekeeping robots industrial robot arms artificial insects face variety learning problems environment 
scenarios knowledge environment reused environment stays task 
explanation neural network learning algorithm ebnn section transfers task independent knowledge learned action models learned empirically problem solving 
learning action models robots observe environments effects actions 
words time action performed robot may sensors sense way world changed 
assume simplicity robot able accurately sense state world 
pointed previous section learning control reduces learning reactive controller 
environment sufficiently predictable neural network learning techniques back propagation training procedure rumelhart model environment 
specifically time action performed previous state denoted action state form training example action model network denoted action models functions type assume environment tasks action models capture invariants allow transferring knowledge task 
individual control learning problem requires different policy learning control function employed agent maximizes corresponding reward general difficult learn directly 
ideas reinforcement learning samuel sutton barto watkins decompose problem learning problem learning evaluation function qi defined states actions 
qi qi expected cumulative reward achieved executing action state assume moment agent learned accurate evaluation function easily function select optimal actions maximize reward time 
state agent finds computes control action considering available actions determine produces highest qi value qi qi measures cumulative reward optimal action qi correct 
problem learning policy reduced problem learning evaluation function 
agent learn evaluation function qi see training data learning qi obtained consider scenario depicted 
robot intentionally avoid complex problem incomplete noisy perception perception algorithm section kind orthogonal research issues 
see bachrach mozer chrisman lin mitchell mozer bachrach rivest schapire tan whitehead ballard approaches learning incomplete perception 
see example barto jordan munro thrun approaches learning action models neural networks 
agent begins initial state performs action sequence action receives reward example indicates action sequence considerably successful 
glance episode agent derive training examples evaluation function qi associating final reward state action pair inductive learning method back propagation training procedure training examples learn qi 
number training examples grows agent internal version qi improve resulting choosing increasingly effective actions 
notice control learning problem vi agent learn distinct qi reward differs different tasks 
explanation neural network learning algorithm agent previously learned knowledge neural network action models guide learning evaluation function qi pointed earlier interested lifelong learning problem 
assume section environment individual control learning problems neural network action models capture important domain knowledge independent particular control learning problem hand 
explanation neural network learning algorithm ebnn agent uses action models bias learning control functions 
simplification notation assume reward received episode 
ebnn applied arbitrary reward functions 
see mitchell thrun details 
note sophisticated learning schemes learning evaluation functions developed 
dissertation watkins watkins describes learning scheme learning evaluation function recursively 
learning train ing patterns derived maximum possible value state sk ak max sk 
experiments applied linear combi nation watkins recursive scheme non recursive scheme described 
combination strongly related sutton td algorithms sutton 
exact procedure essential ideas omit details 
second extension widely discount reward time 
actions chosen number actions minimal reward typically discounted discount factor reward distant reward 
see mitchell thrun thrun mitchell detailed description issues 
fitting slopes target function examples known 
points learner generate hypothesis 
output input derivatives known learner better ebnn works follows 
suppose robot faces control learning problem number learn evaluation function qi 
learning scheme described provides training values desired evaluation function 
repetitive experimentation allows robot collect data learn desired qi 
process utilize knowledge represented action models 
assume agent learned accurate action models model effect actions state environment 
course action models approximately correct learned inductively finite amount training data 
ebnn agent employs action models explain analyze generalize better observed episodes 
done steps 
explain 
explanation post facto prediction observed state action sequence dejong mooney mitchell mitchell thrun 
starting initial state action pair agent post facto predicts subsequent states final state neural network action models 
action models approximately correct predic tions deviate observed states 

analyze 
having explained episode explanation analyzed extract information useful learning evaluation function particular agent analyzes small change states features affect final reward value evaluation function 
done slopes target function respect observed states episode agent computes partial derivative final reward respect final state notice reward function including derivative assumed agent 
slopes propagated backwards chain action model inferences 
neural network action models represent differentiable functions 
chain rule differentiation agent computes partial derivative final reward respect preceding state multiplying partial derivative reward derivative neural network action model 
process iterated yielding partial derivatives final reward episode denotes neural network action model 
reward state slopes analyze importance state features final reward 
state features believed action models irrelevant achieving final reward partial derivatives zero large derivative values indicate presence strongly relevant features 

learn 
analytically extracted slopes approximate slopes target evaluation function qi 
illustrates importance slope information target function 
suppose unknown target function function depicted suppose training examples 
arbitrary continuous function approximator example neural network hypothesize function shown 
slopes points known resulting function better illustrated 
ebnn uses combined learning scheme utilizing types training information 
target values equation learning qi generated observation target slopes 
om extracted analyzing observations domain knowledge acquired previous control learning tasks 
sources training information target values target slopes update weights biases target network 
consequently domain knowledge bias generalization 
bias knowledgeable partially replace need real world experimentation accelerate learning 
simard colleagues pointed back propagation algorithm extended fit target slopes target values simard 
algorithm incrementally updates weights biases neural network value slope error simultaneously minimized 
accommodating imperfect action models initial experiments ebnn simulated robot navigation task showed significant speedup learning robot agent access highly accurate action models mitchell thrun 
action models sufficiently accurate robot performance seriously suffer analysis 
extracted slopes wrong mislead generalization 
observations raises essential question research lifelong agent learning knowledge transfer robot agent deal incorrect prior knowledge 
clearly agent lacks training experience inductively learned bias poor misleading 
worst case learning mechanism employs previously learned domain knowledge take time learning control learning mechanism utilize prior knowledge 
learner avoid damaging effects arising poor prior knowledge 
ebnn malicious slopes identified influence gradually reduced 
specifically accuracy extracted slopes estimated observed prediction error action models 
example action models perfectly post facto predict observed episode estimated accuracy slopes 
likewise action models chain model derivatives produced inaccurate state predictions corresponding estimated accuracy close zero 
accuracies slopes training target network 
tangent prop allows weight training pattern individually estimated accuracies determine ratio value learning slope learning weighted learning target concept 
specifically ebnn step size weight updates multiplied estimated slope accuracy learning slopes 
illustrated mitchell thrun weighting slope training accuracies successfully reduce impact malicious slopes resulting inaccurate action models 
evaluated ebnn different sets action models trained different amounts training data 
trained action models simulated robot domain speedup observed 
increasing inaccurate action models performance ebnn approached standard reinforcement learning knowledge transfer 
experiments ebnn degraded gracefully increasing errors action models 
results intriguing indicate feasibility lifelong learning algorithms able benefit previously learned bias bias poor 
concrete example learning pick cup initial results obtained hero robot 
far predominantly investigated effect previously learned knowledge robot uses manipulator sonar sensor sense distance cup pick 
learning speed new control learning tasks 
trained action models manually collected training data 
robot agent learn policy approaching grasping cup 
robot hand shown hand mounted sonar sensor observe environment 
sonar data preprocessed estimate direction distance closest object world state description 
robot actions forward inches urn degree grab 
positive reward received successful grasps negative reward unsuccessful grasps losing sight cup 
experiment actions modeled separate networks action 
networks parameterized actions forward inches turn degrees predicted distance orientation cup hidden layer units model grab predicted probability pre open loop grasping routing manage pick cup hidden units 
action models pre approximately training episodes containing average steps manually collected 
shows example action model grab action 
particular action model modeled probability success grasping routine reward function simply identity mapping constant derivative 
evaluation function modeled distinct networks action hidden units 
learning action models training episodes evaluation networks shown provided human teacher controlled robot 
applied version td sutton watkins learning watkins grab model deg action model action model grab 
axis measures angle distance cup 
axis plots expected success grasp action probability grasping succeeds 
experience replay lin learning control 
illustrates learned function grab action right row left row employing action models ebnn 
initial stage learning little data available generalization bias pre action models apparent 
functions converged functions learned ebnn shape correct guessed solely observed training points initial knowledge 
example presenting episodes plain learning procedure predicts positive reward solely angle cup combined ebnn method learned grasping fail cup far away 
information represented training episodes single example attempt grasp cup far away 
slopes model copied target evaluation function 
illustrates effect slopes ebnn evaluation functions learned ebnn discovered correlation distance cup success grab action neural network action model 
action models learned earlier control learning tasks significant synergy effect 
ebnn results section initial 
indicate task independent knowledge learned successfully transferred ebnn learning grasping task 
evaluated ebnn real robot domain results mitchell thrun thrun mitchell 
far collect training episodes forward deg deg deg angle training episodes learning control labeled 
horizontal axis measures angle cup relative robot body vertical axis measures distance cup logarithmic scale 
successful grasps labeled unsuccessful 
notice episodes included forwarding turning 
training data learn complete policy approaching grasping cup method 
research characterize synergy full course learning convergence 
experiment families related tasks approaching grasping different objects including cups lie side 
ebnn lifelong robot learning lesson ebnn teach context lifelong agent learning 
section restricting assumption control learning problems robot agent play environment 
case type models robot environment promising candidates transferring knowledge individual control learning problems 
ebnn task independent information represented neural network action models 
models bias generalization learning control process explaining analyzing observed episodes 
ebnn method lifelong agent learning learns re uses learned knowledge independent particular control learning problem hand 
initial experiments described fully demonstrate point ebnn able efficiently replace real world grab function analytical training episodes analytical training episodes deg de episodes episodes deg de evaluation functions action cj alv presenting upper row lower row training episodes left column right column action model biasing generalization 
experimentation previously learned bias 
related experiments simulated robot mitchell thrun observed significant speed factor pre action models employed 
important mention expect ebnn efficient dimension state space higher 
single observation ebnn extracts dimensional slope vector environment dimension input space 
conjecture accurate domain theories generalization improvement scales linearly number instance features expect order magnitude improvement network input features derivative extracted ebnn viewed roughly summarizing information equivalent new training example input dimensions 
conjecture roughly consistent experiments simulated robot domain 
relative performance improvement increase higher order derivatives extracted case ebnn 
important notice quality action models crucial performance ebnn 
inaccurate action models ebnn perform better purely inductive learning procedure 
surprise 
approaches lifelong learning problem efficient non transferring approaches robot solves control learning problem life 
desired synergy effect occurs agent learned appropriate domain specific bias mature 
lifelong learning multiple environments previous section focussed certain type lifelong robot learning problems problems deal single environment 
focus general type lifelong learning scenarios environments differ individual control learning tasks 
pointed section quite robot learning scenarios robot learn control families environments 
example vacuum cleaning robot learn clean different buildings 
alternatively autonomous vehicle learn navigation types terrain 
multi environment scenarios provide possibilities transferring knowledge 
glance transferring knowledge hard environment control learning tasks 
type problems invariants may learned bias 
key observation lifelong learning scenarios involve robot effectors sensors deal variety environments control learning tasks 
approaches type lifelong learning problems aim learning characteristics sensors effectors robot invariants environments 
principle learning transferring task independent knowledge previous section just type knowledge transferred differs 
far little systematic research general type lifelong robot learning scenarios 
remainder section describe general learning mechanism particular approach learning characteristics robot sensors environments 
hero robot example demonstrate inverse sensor models represent knowledgeable bias independent particular environment hand 
robot explores unknown environment 
note obstacle middle laboratory 
lab causes malicious sonar values hard testbed sonar navigation 
example chairs absorb sound completely hard detect sonar 
learning interpret sensations kind invariants learned multiple environments 
observations crucial approach described 
robot sensors environment 
second regularities environ ments learned 
section describe neural network approach learning characteristics robot sensors typical indoor environments 
task mobile hero robot explore unknown buildings thrun 
facing new indoor environment laboratory environment depicted robot wander sensors avoid collisions 
exploration task robot uses sensors rotating sonar sensor mounted head robot shown 
robot monitors wheels encoders detect stuck slipping wheels 
negative reward received collision detected wheel encoders 
positive reward received entering regions robot 
initially robot possess knowledge sensors environments face life 
sensations uninterpreted dimensional vectors floats sonar scans single bit encodes state wheels 
order simplify learning assume robot access coordinates task independent knowledge sensor interpretation network confidence network global frame measures orientation robot 
navigation unknown environments clearly lifelong robot learning problem 
initially robot experience collisions initial knowledge suffice prevent 
collisions penalized negative reward 
order transfer knowledge robot learn interpret sensors order prevent collisions 
knowledge re environment robot face lifetime 
initial experimentation robot able maneuver new unknown world successfully avoiding collisions obstacles 
describe pair networks learn sensor specific knowledge re multiple environment 
sensor interpretation function denoted maps sensor information case sonar scan reward information 
specifically evaluates arbitrary locations close robot probability collision single sonar scan 
shows 
input network vector sonar values coordinates query point ax ay relative robot local coordinate frame 
output net interpretation predicts collision point network predicts free space 
function standard supervised learning algorithms robot keeps track sensor readings locations collided collide 
robot operates constructs maps label occupied regions free space form training examples sensor interpretation network 
usual robot uses back propagation team 
order prevent precious robot hardware real world collisions robot wheels perfect sc position calculated internally dead reckoning 
robot hand precise minutes operation real coordinated usually deviate significantly internal estimates 
approach compensating control errors briefly described thrun 
sensor interpretation confidence feet capturing domain knowledge networks 
sensor interpretations confidence values shown examples 
hallway 
hallway open door 

hallway human walking 
comer room 
comer obstacle 
obstacles 
lines indicate sonar measure ments distances region darkness represents expected collision reward surrounding areas dark values indicate negative reward confidence level dark values indicate low confidence 
designed robot simulator generating training patterns interpretation network 
simulator robot environment known training examples map sensations occupancy information extracted easily 
minutes simulated robot explored simulated world collecting map building raw noisy sensor input exploration path measurements 
resulting model corresponding lab shown lb lab doorway bottom left 
path robot plotted right left demonstrating exploration initially unknown lab 
steps robot pass door lab explore hallway 
total training examples 
examples sensor interpretation shown la circle center represents robot lines orthogonal robot represent distances sensed sonars 
probability negative reward predicted displayed circular regions robot darker region higher probability collision 
seen network successfully learned interpret sonar signals 
sonar values small meaning sonar signal bounced back early network predicts obstacle nearby 
likewise large value readings interpreted free space 
network learned invariants training environments 
example typical sizes walls known 
example robot predicts long obstacle certain width obstacle predicts considerably low probability collision 
prediction surprising sonar sensors see obstacles 
training environments regions walls happened free fairly explains ray predictions network 
provides clear evidence sensor interpretation network represent knowledge robot sensors knowledge certain invariants environments hand 
lab floor model confidence map 
building maps motivate need second network called confidence network related 
bee seen la single sonar scan build small local map robot 
robot moves takes readings resulting interpretations combined form map world 
points world interpretations multiple sensations disagree 
main reasons conflicting interpretations sonar sensors sensor noisy devices 
fail detect certain objects smooth surfaces objects absorbing surfaces office chairs 
second sonars blind areas obstacles 
interpretations regions obstacle inaccurate truly reflect average probability occupancy prior 
observation necessary design mechanism resolves conflicts different predictions 
approach problem explicitly modeling reliability interpretations reliability weighting factor combining multiple interpretations 
specifically consider testing phase interpretation network 
testing examples manage predict target value closely 
significant residual error see moravec elfes related approaches map building robot navigation 
map compiled aaai autonomous robot competition san jose july 
map reflects knowledge robot stage competition 
previous stage robot find approach visually marked poles avoiding collision obstacles 
lines mark boundary competition area 
map inaccurate built separate days locations robot obstacles quite identical 
compensated inaccuracies decaying confidence time 
reasons 
error train second network called confidence network 
input input 
target output normalized prediction error 
training predicts expected deviation sensor interpretations denoted aa ay 
confidence interpretation ac ac 
multiple sensor interpretations combined individual interpretation values averaged weighted confidence value 
lb shows confidence values interpretations showed la dark regions correspond low confidence 
likewise light regions indicate high confidence 
seen examples confidence regions obstacles generally low 
low confidence predicted boundary regions free space obstacles surprise sonar values detect objects cone 
tell cone object 
consequently fine structure objects hard predict 
similar sensor interpretation network dence network represents knowledge sensors environments robot transferred environments 
important notice networks represent learned knowledge independent particular environment hand 
networks act bias modeling process robot constructs internal model new environment 
figures show example maps obtained different environments 
networks find models lab 
simple non adaptive algorithm exploration basically planned plans closest unexplored region 
models lab hallway shown figures 
networks trained simulation successfully prevented robot colliding obstacles 
networks autonomous robot competition held aaai conference july san jose 
task environment differed environments robot seen previously environment large arena filled boxes task find navigate visually marked poles 
robot information location obstacles poles 
explore model environment 
final implementation named robot far complex described 
navigation map adaptive map building procedure component control 
robot employed networks generated simulator tested experiments lab 
total minutes operation separate stages competition robot produced map shown 
networks produced maps accurate protect robot collision obstacle 
allowed robot find close optimal paths goal locations 
sensor interpretation lifelong robot learning contribution approach problem lifelong agent learning 
course learning sensor interpretation necessarily viewed method learning bias method means general learning scheme lifelong learning problems 
purpose characterize map building terms lifelong robot learning 
obviously new environment robot clearly learn control 
initially robot faces new unknown environment knowledge clearly suffice generate actions maximize reward 
robot gradually learns policy action generation step step internal maps provide freedom learning knowledge gaps filled course interaction world 
building internal dimensional occupancy maps static planning routine generates action maps learning control 
learning neural network sensor interpretations offers promising perspective research lifelong robot learning multiple environments 
environments goals differed individual robot control tasks experiments demonstrated knowledge transfer drastically reduce real world experimentation 
robot know collisions realworld environments having experienced solely previously learned knowledge 
believe methods acquire utilize models sensors effectors demonstrated invariants environments promising candidates efficient robot learning complex lifelong learning scenarios 
related approaches knowledge transfer lifelong learning problems viewed techniques acquiring function approximation bias 
various researchers noted importance learning bias transferring knowledge multiple robot control learning tasks 
roughly grouped categories 
learning models 
action models straightforward way learn transfer task independent knowledge individual control learning tasks deal single environment 
approaches utilize action models differ type action models employ way action models bias learning control 
sutton sutton presents system learns action models ebnn 
uses models synthesizing hypothetical experiences refine control policy 
experiments tremendous speedup learning control action models 
ebnn differs approach uses action models explaining real world observations provides mechanism recover errors action models 
lin lin describes mechanism past experience mem repeatedly replayed learning control 
collection past experience forms non generalizing action model 
lin reports significant speedups replay mechanism 
mentioned experience replay neural network training ebnn 
far little research learning models act bias lifelong learning problems multiple robot environments 

learning behaviors abstractions 
second way learning transferring knowledge tasks behaviors 
behaviors controllers policies low complexity term behavior refers reactive controllers 
reinforcement learning example viewed technique learning behaviors 
behaviors form action spaces basic actions robot replaced action invoking behavior 
appropriate behaviors action spaces formed hierarchies actions identified 
learning behaviors accelerates learning control restricting search space possible policies mainly reasons 
num ber behaviors smaller number actions 
second behaviors typically selected longer periods time 
argument usually important provides stronger bias learning control 
singh singh singh reports technique learn complex tasks learning controllers simple tasks reinforcement learning 
controllers represent reactive behaviors 
high level policy learned action space formed low level behaviors 
order represent high level controller purely reactive function singh restricting assumptions type tasks sensor information 
doctoral thesis lin lin describes related scheme learning action hierarchies abstraction 
assumes human instructor teaches robot set elemental behaviors suffice tasks robot face lifetime 
singh approach guarantee optimal controller learned limit 
dayan hinton dayan hinton proposed system uses pre hierarchical decomposition learn control different levels abstraction 
level abstraction differs grain size sensory information resulting differently specialized controllers 
system learns reactive controllers level reinforcement learning algorithms unclear type problems procedure learn successfully essential information may missed providing incomplete sensor information purely reactive controllers 

learning inductive function approximation bias 
straightforward approach learning transferring knowledge learn inductive bias function approximators learning control directly 
atkeson atkeson presents scheme learning distance measures instance local approximation schemes 
algorithm scaling factors learned allows weight different input features differently 
sutton sutton reports family learning schemes allow learn inductive bias similar kalman filters kalman 
describe methods context learning control research motivated transferring knowledge multiple control learning tasks 

learning representations 
representations inductive bias determine way function approximator generalizes examples 
researchers focussed learning appropriate representations order learn bias 
example pratt pratt describes approaches allow re learned representations hidden units neural networks 
empirically demonstrate transfer significantly reduce number training epochs required convergence back propagation algorithm occasional improvements generalization 
sim ilar technique reported sharkey sharkey sharkey sharkey 
researchers studied knowledge transfer tasks learned simultaneously 
example suddarth suddarth demonstrated multiple learning tasks certain types success fully guide improve generalization 
approach gives hints neural networks form additional output units learn closely related task 
hints constrain internal representation developed network 
general way caruana caruana proposed learn collections tasks parallel shared internal representation 
conjectures multi task learning neural network learning algorithms scale complex learning tasks 
approaches lifelong learning problem described fall category 
ebnn robot learns action models bias generalization evaluation functions 
sensor interpretation functions inverse models sensors environments robot 
robot exploration tasks robot learns models typical aspects environments bias construction individual maps 
pointed earlier action models suited environment learning tasks sensor models appropriate lifelong agent learning multiple environments 
lifelong learning perspective autonomous robots 
propose study robot learning problems isolation context multitude learning problems robot face lifetime 
lifelong robot learning opens opportunity transfer learned knowledge 
knowledge may bias learning control 
control learning methods allow transfer knowledge complex algorithms solve isolated control learning problems robot learning easier 
robots memorize transfer knowledge rely real world experimentation learn faster 
previously learned knowledge may act knowledgeable bias may partially replace pure syntactic bias inductive learning algorithms 
demonstrated concrete approaches potential synergy effect knowledge transfer 
approaches addressed main types lifelong agent learning scenarios defined single environment 
strongly believe knowledge transfer essential scaling robot learning algorithms realistic complex domains 
exploiting previously learned knowledge simplifies learning control 
results support fundamental claim learning easier embedded lifelong learning context 
acknowledgment cmu robot learning group team cmu invaluable discussion contributed research 
university bonn robot rhino manufactured real world interface map approximately meters constructed rhino aaai robot competition technique described 
invaluable help refining ebnn 
research sponsored part avionics lab wright research development center aeronautical systems division afsc air force afb oh contract arpa order siemens 
views contained document authors interpreted representing official policies expressed implied government siemens addendum submitted ebnn successfully applied variety real world learning tasks 
mitchell thrun thrun thrun result applying ebnn mobile robot navigation cmu xavier robot reported 
ebnn applied robot perception mitchell sullivan appear object recognition thrun mitchell game chess thrun 
thrun mitchell definition lifelong learning problem context supervised learning 
approach interpreting sonar sensors building occupancy maps reported sect 
slight modifications successfully employed university bonn entry rhino aaai mobile robot competition buhmann appear 
currently maps routinely built large indoor areas 
atkeson christopher atkeson 
locally weighted regression robot learning 
proceedings ieee international conference robotics automation pages sacramento ca april 
bachrach mozer jonathan bachrach michael mozer 
connectionist modeling control finite state systems partial state infor mation 

barto andrew barto richard sutton chris watkins 
learning sequential decision making 
technical report coins department computer science university massachusetts ma ber 
barto andrew barto richard sutton chris watkins 
learning sequential decision making 
gabriel moore editors learning computational neuroscience pages cambridge massachusetts 
mit press 
barto etal andrew barto steven bradtke satinder singh 
real time learning control asynchronous dynamic programming 
technical report coins department computer science university massachusetts ma august 
bellman bellman 
dynamic programming 
princeton university press princeton nj 
brooks rodney brooks 
robot walks emergent behaviors carefully evolved network 
neural computation 
buhmann appear joachim buhmann wolfram burgard armin cre mers dieter fox thomas hofmann frank schneider se thrun 
mobile robot rhino 
ai magazine appear 
bylander tom bylander 
complexity results planning 
proceedings pages sydney australia 
ijcai canny john canny 
complexity robot motion planning 
mit press cambridge ma 
richard 
multitask learning knowledge source inductive bias 
paul utgoff editor proceedings tenth international conference machine learning pages san mateo ca 
morgan kaufmann 
chrisman chrisman 
reinforcement learning perceptual alias ing perceptual distinction approach 
proceedings aaai conference menlo park ca july 
aaai press mit press 
dayan hinton peter dayan geoffrey hinton 
feudal reinforce ment learning 
moody hanson lippmann editors advances neural information processing systems san mateo ca 
morgan kaufmann 
dejong mooney gerald dejong raymond mooney 
explanation learning alternative view 
ma ine learning 
elfes alberto elfes 
sonar real world mapping navigation 
ieee journal robotics automation ra june 
jordan michael jordan 
generic constraints underspecified target tra 
proceedings international joint conference neural networks washington dc san diego 
ieee tab neural network com 
kalman kalman 
new approach linear filtering prediction problems 
trans 
asme journal basic engineering 
kuipers byun benjamin kuipers yung tai byun 
robot ration mapping strategy semantic hierarchy spatial representations 
technical report department computer science university texas austin austin texas january 
lin mitchell long ji lin tom mitchell 
memory approaches reinforcement learning non markovian domains 
technical report carnegie mellon university pittsburgh pa 
lin long ji lin 
self improving reactive agents reinforcement learning planning teaching 
machine learning 
lin long ji lin 
self supervised learning reinforcement artificial neural networks 
phd thesis carnegie mellon university school computer science pittsburgh pa 
maes pattie maes editor 
designing autonomous agents 
mit press elsevier cambridge ma 
mahadevan connell sridhar mahadevan jonathan connell 
scaling reinforcement learning robotics exploiting subsumption architecture 
proceedings eighth international workshop machine learning pages 
mel bartlett mel 
murphy neurally inspired connectionist approach learning performance vision robot motion planning 
technical report center complex systems research beckman institute university illinois 
mitchell thrun tom mitchell sebastian thrun 
explanation learning comparison symbolic neural network approaches 
paul utgoff editor proceedings tenth international conference machine learning pages san mateo ca 
morgan kaufmann 
mitchell thrun tom mitchell sebastian thrun 
explanation neural network learning robot control 
hanson cowan giles editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 
mitchell thrun tom mitchell sebastian thrun 
learning ana inductively 
mitchell editors mind matters tribute allen newell 
lawrence erlbaum associates publisher 
mitchell tom mitchell rich keller kedar 
explanation generalization unifying view 
ma ine learning 
mitchell tom mitchell joseph sullivan sebastian thrun 
explanation learning mobile robot perception 
workshop robot learning eleventh conference machine learning 
moore andrew moore 
efficient memory learning robot control 
phd thesis trinity hall university cambridge england 
moravec hans moravec 
sensor fusion certainty grids mobile robots 
ai magazine pages summer 
mozer bachrach michael mozer jonathan bachrach 
discovering structure reactive environment exploration 
technical report cu cs dept computer science university colorado boulder november 
munro paul munro 
dual backpropagation scheme scalar reward learning 
ninth annual conference cognitive science society pages hillsdale nj 
cognitive science society lawrence erlbaum 
sullivan appear joseph sullivan tom mitchell sebastian thrun 
explanation neural network learning mobile robot perception 
ikeuchi manuela veloso editors symbolic visual learning 
oxford university press appear 
pomerleau dean pomerleau 
alvinn autonomous land vehicle neural network 
technical report cmu cs computer science dept carnegie mellon university pittsburgh pa 
pratt lori pratt 
discriminability transfer neural net works 
moody hanson lippmann editors advances neural information processing systems san mateo ca 
morgan kaufmann 
rivest schapire ronald rivest robert schapire 
diversity inference finite automata 
proceedings foundations computer science 
rumelhart david rumelhart geoffrey hinton ronald williams 
learning internal representations error propagation 
rumelhart mcclelland editors parallel distributed processing 
vol 
mit press 
samuel samuel 
studies machine learning game checkers 
ibm journal research development 
schwartz jacob schwartz micha john hopcroft 
plan ning geometry complexity robot motion 
ablex publishing norwood nj 
sharkey sharkey noel sharkey amanda sharkey 
adaptive generalization transfer knowledge 
proceedings second irish neural networks conference belfast 
simard patrice simard bernard yann lecun john denker 
tangent prop formalism specifying selected invariances adaptive network 
moody hanson lippmann editors advances neural information processing systems pages san mateo ca 
morgan kaufmann 
singh satinder singh 
efficient learning multiple task sequences 
moody hanson lippmann editors advances neural systems pages san mateo ca 
morgan kaufmann 
singh satinder singh 
transfer learning composing solutions elemental sequential tasks 
machine learning 
suddarth steven suddarth 
rule injection hints means improving network performance learning time 
proceedings eurasip workshop neural networks portugal feb 
eurasip 
sutton richard sutton 
temporal credit assignment reinforcement learning 
phd thesis department computer information science university massachusetts 
sutton richard sutton 
learning predict methods temporal differences 
machine learning 
sutton richard sutton 
integrated architectures learning planning reacting approximating dynamic programming 
proceedings seventh international conference machine learning june pages san mateo ca 
morgan kaufmann 
sutton richard sutton 
adapting bias gradient descent incre mental version delta bar delta 
proceeding tenth national conference artificial intelligence aaai pages menlo park ca july 
aaai aaai press mit press 
tan ming tan 
learning cost sensitive internal representation rein learning 
proceedings eighth international workshop machine learning pages 
thrun mitchell sebastian thrun tom mitchell 
integrating neural network learning explanation learning 
proceedings france july 
ijcai thrun mitchell sebastian thrun tom mitchell 
learning thing 
technical report cmu cs carnegie mellon university pittsburgh pa september 
thrun sebastian thrun 
role exploration learning control 
david white donald editors handbook intelligent control neural fuzzy adaptive es 
van nostrand reinhold florence kentucky 
thrun sebastian thrun 
exploration model building mobile robot domains 
proceedings icnn pages san francisco ca march 
ieee neural network council 
thrun sebastian thrun 
lifelong learning perspective mobile robot control 
proceedings ieee rsj gi international conference intelligent robots systems september 
thrun sebastian thrun 
approach learning mobile robot navigation 
robotics autonomous systems 
press 
thrun sebastian thrun 
learning play game chess 
tesauro touretzky leen editors advances neural information processing systems san mateo ca 
morgan kaufmann 
appear 
watkins christopher watkins 
learning delayed rewards 
phd thesis king college cambridge england 
whitehead ballard steven whitehead dana ballard 
learning perceive act trial error 
machine learning 
