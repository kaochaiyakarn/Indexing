switching kalman filters kevin murphy august show different variants switching kalman filter models represented unified way leading single general purpose inference algorithm 
show find approximate maximum likelihood estimates parameters em algorithm extending previous results learning em non switching case dro gh switching fully observed case ham 
dynamical systems assumed linear subject gaussian noise 
model called linear dynamical system lds model defined gamma hidden state variable time observation time independent gaussian noise sources 
typically parameters model theta assumed time invariant estimated data em gh 
main advantages model efficient algorithm performing inference computing belief state jy known kalman filter generalization offline case rauch tung smoother computing jy observed data 
unfortunately systems linear subject non gaussian noise 
approach problem discretize hidden state variables resulting dynamic bayesian networks dw gha hidden markov model hmm rab simplest example :10.1.1.131.2084
resulting system general belief state exponential number hidden state variables resulting intractable inference 
addition may large number parameters resulting inefficient learning lot data needed 
approach take bank different linear models switch take linear combination 
consider case dynamics piecewise linear 
discrete switch variable specifies matrix time assume markovian dynamics transition matrix initial distribution 
model shown 
expected time spend state mode switching 
change distribution length linear segment explicitely modeling long spend state rab 
switching auto regression switching observations switching dynamics switching observations factored state switching kalman filter models represented bayesian networks pea 
square nodes discrete oval ones gaussian 
shaded nodes observed clear nodes hidden 
observed know apply submodel segmentation known hidden weighted combination sub model weights pr 
called soft switching 
resulting system thought mixture kalman filters 
example interested tracking maneuvering airplane 
basic models cover horizontal vertical motion represent turns convex combination 
shown bsl give superior performance online methods input estimation problems 
consider case specifies observation matrices time model non gaussian observation noise approximating mixture gaussians 
example take covariance observation process broad covariance approximately uniform 
prior probability reflects expect outliers occur 
widely technique making linear regression robust see pg modelling sensor failure wil 
gh proposed model shown :10.1.1.30.5334
switching observations interpretation different 
switch variable case thought selecting sub processes pass output variable choosing permutation matrix apply model fact uncertain process causes observation ss called data association ambiguity bsf 
course dynamics observation model dependent separate markov chains 
general case assume rest 
concerned special case get observe directly see 
called switching auto regression model 
sar models computationally simpler approximations necessary inference see hidden node discrete 
course practice need deal fact number objects tracking number measurements receive time step constant 
inference fundamental problem belief state grows exponentially time 
see suppose initial distribution mixture gaussians value different equations value mixture gaussians 
general time belief state jy mixture gaussians possible model history general approaches dealing exponential growth sm ffl collapsing approximate mixture gaussians mixture gaussians 
called generalized pseudo bayesian algorithm order gpb see bsl kim 
approximate mixture gaussians single gaussian moment matching shown lau best kl sense single gaussian approximation 
collapse gaussians differ history steps ago general similar gaussians differ history 
interacting multiple models imm algorithm bsl approximation gpb cost filters see smoothing 
surprisingly history keep better approximation sm 
see comparison gpb gpb imm filtering algorithms 
ffl selection keep high probability paths tree model histories 
technique widely filtering data association ambiguity called multiple hypothesis tracking bsf 
ffl iterative sample missing values mcmc methods collect averaged statistics ck bmr 
simply alternate picking segmentations sequence viterbi algorithm hmms doing inference fixed segmentation 
alternatively weighted combinations matrices best matrix step sm 
ffl variational essentially break vertical links model introduce new variational parameters couple tight way possible 
em model maximize lower bound likelihood see gh details 
focus collapsing approximation 
worry errors introduced time step approximating posterior accumulate time leading poor performance 
shown bk bk stochasticity process ensures true distribution spreads high probability overlaps approximate distribution able prove error remains bounded 
delving case warm considering simpler case sar model perform exact inference 
switching ar model define inference computing posterior probabilities pr jjx sequence observations 
passes 
forwards pass proceed follows 
pr jjx gamma pr js gamma pr jjx gamma pr js gamma pr jjs gamma gamma pr gamma ijx gamma pr gamma ijx gamma normalization constant pr jx gamma gamma jt gamma gamma likelihood innovation prediction error time model backwards pass pr jjx pr jjs pr kjx pr jjs pr kjx pr jjx pr pr kjx pr kjx line marked follows effect evidence blocked observing children 
practical note computing conditional probability pr jx opposed joint probability pr hmm need worry underflow need scaling rab :10.1.1.131.2084
switching kalman filter model follows review gpb algorithm gpb imm need reason consecutive time steps calculate cross variance term needed em 
start defining notation 
tj jy gamma tj jy tj jy called filtered statistics called smoothed statistics called predicted statistics 
notice superscript inside brackets value switch node time subscript value superscript left value gamma right need subtle distinctions handle cross variance terms correctly 
define 
tj cov jy gamma cov gamma jy gamma cov gamma jy gamma gamma tj pr gamma jjy tj pr jjy pr jy gamma gamma jt gamma gamma jt gamma gamma gamma gamma filter filter tjt tjt tjt tjt gamma gamma gamma merge tjt tjt gamma jt gamma gamma jt gamma gamma jt gamma gamma jt gamma gamma gamma gamma gamma gamma gamma filter filter filter filter tjt tjt tjt tjt tjt tjt tjt tjt bn theta theta theta theta theta theta theta theta gamma gamma gamma merge merge tjt tjt tjt tjt gamma jt gamma gamma jt gamma gamma jt gamma gamma jt gamma merge gamma jt gamma gamma jt gamma gamma jt gamma gamma jt gamma filter filter tjt tjt tjt tjt gpb gpb imm algorithms 
filters takes extra input returns extra output shown clarity 
likelihood innovation time current model 
filtering perform steps sequence 
tjt tjt gamma jt filter gamma jt gamma gamma jt gamma gamma tjt pr gamma jjy gamma jt gamma gamma jt gamma tjt gamma tjt ijj pr gamma ijs gamma tjt tjt tjt tjt collapse tjt tjt ijj definitions filter smoother collapse operators appendix derivation see bsl derivation cross variance term see dro mode update equation follows 
pr gamma jjy gamma pr gamma jy gamma pr js gamma gamma pr gamma jjy gamma pr js gamma gamma pr jjs gamma gamma pr gamma gamma gamma jt gamma normalization constant gamma jt gamma initial conditions follows 
set predicted mean variance evidence js cov js sigma set smoothing perform steps sequence 
tjt tjt tjt smooth jt jt tjt tjt jt tjt jjk pr jjs tjt tjt jt jjk jt tjt jt kjj pr jt tjt tjt tjt collapse tjt tjt kjj tjt tjt collapse tjt tjt tjt jt jy jt tjt jt tjt tjt jjk tjt jy tjt jjk tjt jt tjt tjt jt line marked standard approximation kim derived follows 
pr jjs pr jjs pr jjy pr pr approximation conditionally independent evidence approximation bad provided evidence contain information contained learning start considering simpler sar case extending case 
switching ar model interested finding maximum likelihood estimate parameters 
knew segmentation model apply time step solve linear regression 
values unobserved shall em ham 
complete data log likelihood log log jx gamma log pr js gamma log js log pr jjs gamma jx gamma exp gamma gamma gamma gamma gamma gamma gamma delta gamman jq gamma best way see terms directed graphical model 
recall node conditionally independent non descendants parents pea 
independent past evidence parent gamma children need observed block effect evidence observing arrow points wrong way 
ar model children observed case general model observed 
js sigma em iteratively maximize parameters expected value parameters old complete data log likelihood st old log fs tg jx log jx gamma delta delta delta log jx gamma delta delta delta weights pr jjx computed inference step fact jx gamma node independent non descendants parents 
maximize equation take derivatives set intuitively derivative kill terms summation resulting weighted version standard formulas see appendix details 
assuming iid sequences indexed find gamma gamma gamma gamma gamma gamma gamma sigma gamma gamma gamma gamma pr gamma jjy gamma formulas hmm rab formulas sigma mixture gaussians see bis xj formulas fact special cases linear regression :10.1.1.131.2084
equations case restrictions form matrices 
typically know entries known value 
shown constrained mle obtained computing unconstrained mle setting constrained entries correct values projecting allowable subspace 
example estimate covariance matrix constrained diagonal compute set diagonal entries 
written formula sigma usual form form easier compute incremental fashion nh sufficient statistics :10.1.1.33.2557
remember functions 
achieve parameter pool expected sufficient statistics parameter equivalence class 
example replace estimating known problem mixtures gaussians models non dynamic case covariance matrix easily singular 
hamilton ham ham suggests wishart prior deg regularize problem 
particular suppose prior gamma ff ff equivalent sample size precision matrix map estimate ff gamma gamma setting ff nt imagine seen data ff works quite practice 
switching kalman filter model case complete data log likelihood log gamma gamma gamma gamma gamma delta gamma log jr gamma gamma gamma gamma gamma gamma gamma delta gamma log jq gamma gamma sigma gamma gamma gamma log sigma gamma log log log gamma quantity maximise js jy fs tg jy log jx gamma delta delta delta log jx gamma delta delta delta delta 
approximation arises jy jy exponential number vectors segmentation 
advantage formula form equation sar case modulo leading constant factor redefinitions pr jjy tjt tjt tjt gamma gamma gamma jt tjt gamma jt course computed terms tjt gamma 
practice doesn difference advantage don need collapse cross variance terms twice compute gamma equations section 
course new equation terms involving maximizing respect gives see appendix derivation gamma gamma deterministic annealing em notorious getting stuck local minima 
especially common models kind considering possible segmentations 
solution deterministic annealing em un suggested gh 
replace posterior pr pr pr hidden variables observed values pr fi pr fi fi inverse temperature parameter 
fi infinite temperature posterior uniform 
fi low temperature posterior regular em posterior 
applying principle case suggest fi fi appendix details inference algorithm filter filter operator tjt tjt gamma jt filter gamma jt gamma gamma jt gamma defined follows 
compute predicted mean variance 
tjt gamma fx gamma jt gamma tjt gamma fv gamma jt gamma compute error prediction innovation variance error kalman gain matrix likelihood observation 
gamma hx tjt gamma hv tjt gamma tjt gamma gamma update estimates mean variance cross variance 
tjt tjt gamma tjt gamma tjt gamma tjt gamma gamma gamma jt gamma fv gamma jt gamma smoother smooth operator tjt tjt tjt smooth jt jt tjt tjt jt tjt defined follows 
compute predicted quantities pass filtering stage jt tjt jt tjt compute smoother gain matrix 
tjt gamma jt update estimates mean variance cross variance 
tjt tjt gamma jt gamma jt delta tjt tjt gamma jt gamma jt delta tjt tjt gamma jt gamma jt delta gamma jt tjt alternative way compute smoothed estimates cross variance terms gamma jt require corresponding filtered terms ss gh 
smooth operator tjt tjt gamma jt smooth jt jt tjt tjt tjt gamma jt gamma defined gamma jt tjt gamma tjt gamma tjt gamma boundary condition gamma jt gamma kt ht ft gamma jt gamma collapse consider random variables conditional means xjs js cross variance cov js mixing coefficients pr 
compute unconditional moments follows 
procedure called moment matching 
cov js gamma gamma js gamma gamma gamma gamma js gamma gamma js gamma gamma gamma gamma introduce shorthand operation 
define collapse shown lau gaussian moments closest possible gaussian kl distance original mixture distribution 
derivation step follow gh generalize switching case 
simplicity notation consider single sequence 
standard identities xa xa xa xb ab ln jxj gamma system matrix identity gamma theta gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma gamma system noise covariance identities gamma gamma theta gamma gamma gamma gamma gamma theta gamma gamma gamma gamma gamma new value fact symmetric find gamma gamma gamma gamma gamma gamma gamma gamma gamma observation matrix identity gamma theta gamma gammac gamma observation noise covariance identities find gamma theta gamma new estimate def gamma gamma gamma initial mean covariance standard derivation mixture gaussians model see ham bis xj 
initial state probability transition matrix constrained maximization problem probabilities sum solved lagrange multipliers see rab ham derivation 
bis bishop 
neural networks pattern recognition 
clarendon press 
bk boyen koller 
approximate learning dynamic models 
neural info 
proc 
systems 
bk boyen koller 
tractable inference complex stochastic processes 
proc 
conf 
uncertainty ai 
bmr robert 
bayesian estimation switching arma models 
technical report crest paris 
bsf bar shalom fortmann 
tracking data association 
academic press 
bsl bar shalom li 
estimation tracking principles techniques software 
artech house 
ck carter kohn 
markov chain monte carlo conditionally gaussian state space models 
technical report univ new south wales graduate school management 
deg degroot 
optimal statistical decisions 
mcgraw hill 
dro digalakis rohlicek ostendorf 
ml estimation stochastic linear systems em algorithm application speech recognition 
ieee trans 
speech audio proc 
dw thomas dean michael wellman 
planning control 
morgan kaufmann 
gh ghahramani hinton 
parameter estimation linear dynamical systems 
technical report crg tr dept comp 
sci univ toronto 
gh ghahramani hinton 
switching state space models 
technical report crg tr dept comp 
sci univ toronto 
gha ghahramani 
learning dynamic bayesian networks 
giles gori editors adaptive processing temporal information lecture notes artificial intelligence 
springerverlag 
appear 
ham hamilton 
analysis time series subject changes regime 
econometrics 
ham hamilton 
quasi bayesian approach estimating parameters mixtures normal distributions 
business economic statistics 
kim 
kim 
dynamic linear models markov switching 
econometrics 
reese haussler 
generalized hidden markov model recognition human genes dna 
international conf 
intelligent systems molecular biology 
appear 
lau lauritzen 
graphical models 
oup 
nh neal hinton :10.1.1.33.2557
new view em algorithm justifies incremental variants 
jordan editor learning graphical models 
kluwer 
pea pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
pg pena guttman 
bayesian approach kalman filter 
editor bayesian analysis time series dynamic models 
marcel dekker 
rab rabiner :10.1.1.131.2084
tutorial hidden markov models selected applications speech recognition 
proc 
ieee 
sm smith makov 
bayesian detection estimation jumps linear systems 
jacobs davis dempster harris parks editors analysis optimization stochastic systems 

ss shumway stoffer 
dynamic linear models switching 
am 
stat 
assoc 
tw weigend 
modeling volatility state space models 
intl 
neural systems 
un ueda nakano 
deterministic annealing em algorithm 
neural networks 
wil willsky 
survey design methods failure detection dynamic systems 
automatica 
xj xu jordan 
convergence properties em algorithm gaussian mixtures 
neural computation 

