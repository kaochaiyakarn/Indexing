stochastic gradient boosting jerome friedman march gradient boosting constructs additive regression models sequentially fitting simple parameterized function base learner current pseudo residuals squares iteration 
pseudo residuals gradient loss functional minimized respect model values training data point evaluated current step 
shown approximation accuracy execution speed gradient boosting substantially improved incorporating randomization procedure 
specifically iteration subsample training data drawn random replacement full training data set 
randomly selected subsample place full sample fit base learner compute model update current iteration 
randomized approach increases robustness base learner 
gradient boosting function estimation problem system consisting random output response variable set random input explanatory variables fx delta delta delta xn training sample fy known values goal find function maps joint distribution values expected value specified loss function psi minimized arg min psi boosting approximates additive expansion form fi mh am functions base learner usually chosen simple functions parameters fa delta delta deltag 
expansion coefficients ffi mg parameters fam jointly fit training data forward stage wise manner 
starts initial guess delta delta delta fi am arg min fi psi fm gamma fm fm gamma fi mh am csiro locked bag north nsw stat stanford edu gradient boosting friedman approximately solves arbitrary differentiable loss functions psi step procedure 
function fit squares am arg min ae im gamma aeh current pseudo residuals yim gamma psi fm gamma am optimal value coefficient fi determined fi arg min fi psi fm gamma am strategy replaces potentially difficult function optimization problem squares followed single parameter optimization general loss criterion psi 
gradient tree boosting specializes approach case base learner terminal node regression tree 
iteration regression tree partitions space disjoint regions predicts separate constant value lm lm rlm im mean region lm parameters base learner splitting variables corresponding split points defining tree turn define corresponding regions fr lm partition mth iteration 
induced top best manner squares splitting criterion friedman hastie tibshirani 
regression trees solved separately region rlm defined corresponding terminal node mth tree 
tree predicts constant value region lm solution reduces simple location estimate criterion psi fl lm arg min fl rlm psi fm gamma fl current approximation fm gamma separately updated corresponding region fm fm gamma delta fl lm lm shrinkage parameter controls learning rate procedure 
empirically friedman small values lead better generalization error 
leads algorithm generalized boosting decision trees algorithm gradient treeboost arg min fl psi fl yim gamma psi fm gamma gamma terminal node tree im fl lm arg min fl rlm psi fm gamma fl fm fm gamma delta fl lm lm endfor friedman specific algorithms template loss criteria including squares psi gamma absolute deviation psi jy gamma huber psi gamma jy gamma ffi ffi jy gamma gamma ffi jy gamma ffi classification class multinomial negative log likelihood stochastic gradient boosting bagging procedure breiman introduced notion injecting randomness function estimation procedures improve performance 
early implementations adaboost freund schapire employed random sampling considered approximation deterministic weighting implementation base learner support observation weights essential ingredient 
breiman proposed hybrid bagging boosting procedure adaptive bagging intended squares fitting additive expansions 
replaces base learner regular boosting procedures corresponding bagged base learner substitutes bag residuals ordinary residuals boosting step 
motivated breiman minor modification gradient boosting algorithm incorporate randomness integral part procedure 
specifically iteration subsample training data drawn random replacement full training data set 
randomly selected subsample full sample fit base learner line compute model update current iteration line 
fy entire training data sample random permutation integers delta delta delta ng 
random subsample size fy stochastic gradient boosting algorithm algorithm stochastic gradient treeboost arg min fl psi fl rand perm fig gamma psi fm gamma gamma terminal node tree fl lm arg min fl rlm psi gamma fm gamma fl delta fm fm gamma delta fl lm rlm endfor introduces randomness causes algorithm return result algorithm 
smaller fraction random samples successive iterations differ introducing randomness procedure 
value roughly equivalent drawing bootstrap samples iteration 
delta reduces computation factor making value smaller reduces amount data available train base learner iteration 
cause variance associated individual base learner estimates increase 
simulation studies effect randomization gradient tree boost procedures depend particular problem hand 
important characteristics problems affect performance include training sample size true underlying target function distribution departures 
order gauge value estimation method necessary accurately evaluate performance different situations 
conveniently accomplished monte carlo simulation data generated wide variety prescriptions resulting performance accurately calculated 
important characteristics problem affecting performance true underlying target function 
nature target function vary greatly different problems seldom known evaluate relative merits randomized gradient tree gradient boosting variety different targets randomly drawn broad realistic class functions 
procedure generate random functions described friedman 
simulation studies randomly generated target functions friedman 
performance average absolute error derived estimate approximating target ex jf gamma estimated large independent test data set 
performance comparisons different estimates absolute error relative best performer target functions best method arg min fa receives value receive larger value fr particular method best smallest error target distribution target functions point mass value 
regression section effect randomization huber treeboost procedure investigated 
regression procedures derived friedman treeboost best performance considered method choice 
break parameter set default value ff 
small data sets shrinkage parameter algorithm set 
larger ones set 
best regressions trees terminal nodes base learner 
compare various levels randomization terms performance target functions different error distributions 
data sets fy generated represents randomly generated target functions 
study errors generated gaussian distribution zero mean variance adjusted ej ex jf gamma giving signal noise ratio 
second study errors generated slash distribution delta 
scale factor adjusted give signal noise ratio 
slash distribution thick tails extreme test robustness 
gaussian errors compares performance treeboost different degrees randomization small training data sets 
degree randomness controlled fraction randomly drawn observations train regression tree iteration 
fraction randomly sampled error min error distributions absolute approximation error relative best training different fractions randomly selected observations full training sample iteration small data sets gaussian errors 
shown distributions fr targets values largest value corresponds deterministic boosting algorithm successively smaller values introduce increasing degrees randomness 
distributions summarized boxplots 
shaded area boxplot shows interquartile range distribution enclosed white bar median 
outer hinges represent points closest plus minus interquartile range units upper lower quartiles 
isolated bars represent individual points outside range outliers 
sees fig 
randomization improves performance substantially 
values extreme distribution absolute error relative best closer minimum value corresponding distribution deterministic algorithm 
averaged target functions best value sampling fraction approximately typical improvement absolute error sampling 
represents improvement squared error scale 
sampling data iteration gives considerable improvement sampling corresponding computational speed factors respectively 
sampling fractions close optimal dispersion distributions smaller 
means nearly targets produced best close best average absolute errors 
sampling fractions farther optimal value dispersion distributions 
targets produced best close best results badly 
illustrates relative performance depend strongly particular target encountered 
example target function produced lowest error 
distribution indicates sampling fraction produces improvements sampling range absolute error median 
shows similar comparison target functions error distribution fraction randomly sampled error min error distributions absolute approximation error relative best training different fractions randomly selected observations full training sample iteration moderate sized data sets gaussian errors 
moderate sized training data sets 
sees similar dramatic pattern 
note vertical scale fig 
half fig 

optimal sampling fraction closer typical improvements deterministic algorithm absolute error median 
increase accuracy associated random sampling dramatic larger data sets speed increase meaningful 
compares performance small data sets treeboost different sized regression trees base learner 
left panel shows distribution target functions absolute error relative best deterministic algorithm 
right panel show corresponding distributions sampling 
cases optimal tree size averaged targets 
increasing capacity base learner larger trees degrades performance fitting 
sees applying randomization effect fitting 
increase median relative error distribution going sampling third deterministic algorithm 
shows effect different perspective 
distribution ratio error deterministic algorithm sampling shown function tree size 
terminal node trees deterministic treeboost typically seen worse absolute error random sampling 
node trees ratio typically close 
random subsampling effective reducing error larger trees mitigate eliminate effect capacity base learner 
slash errors gaussian error distribution behaved ideal seldom realized practice 
check extent effect randomization depends error distribution applied terminal nodes error min error terminal nodes error min error distributions absolute approximation error relative best different sized regression trees base learner small training samples 
left panel training full sample 
right panel random subsampling iteration 
treeboost target functions slash distributed errors 
slash distribution represents opposite extreme gaussian thick tails outliers 
shows performance comparisons analogous fig 
fig 
slash gaussian errors 
distributions relative error sample sizes resemble large sample gaussian errors fig 

particular small sample size performance gain random sampling roughly half achieved gaussian errors 
larger sample improvement relatively insensitive sampling fraction range 
classification consider bernoulli distributed output variable gamma pr exp gamma 
appropriate loss criterion deviance twice binomial negative log likelihood psi log exp gamma serves continuous surrogate misclassification error 
data generated target functions 
target median computed 
data set trial taken fy sign gamma equal numbers classes bayes error rate zero 
decision boundaries induced fairly complex 
shows distribution error rate relative best values sampling fraction left panel shows distributions small samples right panel shows larger ones 
sees behavior similar identical regression case 
randomization beneficial average universally regression 
distributions corresponding values close optimal higher medians larger spreads especially terminal nodes error error improvement ratio distributions ratio error deterministic algorithm sampling different sized regression trees base learner small training samples 
fraction randomly sampled error min error fraction randomly sampled error min error distributions absolute approximation error relative best training different fractions randomly selected observations full training sample iteration slash errors 
left panel small data sets right moderately sized data sets 
fraction randomly sampled error min error fraction randomly sampled error min error distributions error rate relative best training different fractions randomly selected observations full training sample iteration binary valued output variable 
left panel small data sets right moderately sized data sets 
larger sample 
indicates assurance randomization improve error rate individual situations 
fact sampling smaller error sampling fractions targets 
discussion results previous section indicate accuracy gradient boosting substantially improved introducing randomization simple expedient training base learner different randomly selected data subsets iteration 
degree improvement seen depend particular problem hand terms training sample size true underlying target function distribution gaussian slash bernoulli capacity base learner 
reason randomization produces improvement clear 
fact effective small samples high capacity base learners suggest variance reduction important ingredient 
smaller subsamples causes variance individual base learner estimates iteration increase 
correlation estimates different iterations 
tends reduce variance combined model effect averages base learner estimates 
apparently averaging effect dominates surprisingly small subsamples 
phenomenon known bagging bootstrap sampling produces random subsamples effective size roughly half full training sample 
stochastic gradient boosting viewed sense boosting bagging hybrid 
adaptive bagging breiman represents alternative hybrid approach 
results obtained suggest original stochastic versions adaboost may merit implementation convenience 
deterministic adaboost weights assigned observation recomputed successive iterations emphasize observations currently difficult correctly predict 
weighting stochastic adaboost draws random unweighted samples replacement full training sample probability observation selected proportional currently computed weight 
injects random component procedure tends reduce correlation solutions successive iterations 
usual prescription randomly drawn sample size original training data set 
results stochastic gradient boosting suggest stochastic adaboost accuracy improved increasing randomness drawing smaller samples iteration 
produces computational savings 
acknowledgments helpful discussions leo breiman gratefully acknowledged 
partially supported csiro mathematical information sciences australia department energy contract de ac sf dms national science foundation 
breiman 

bagging predictors 
machine learning 
breiman 

adaptive bagging regressions 
technical report dept statistics university california berkeley 
freund schapire 

experiments new boosting algorithm 
machine learning proceedings thirteenth international conference 
friedman 

greedy function approximation gradient boosting machine 
technical report dept statistics stanford university friedman hastie tibshirani 

additive logistic regression statistical view boosting 
technical report dept statistics stanford university 

