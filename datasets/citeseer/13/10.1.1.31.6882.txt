generative learning structures processes generalized connectionist networks honavar department computer science iowa state university leonard uhr computer sciences department university wisconsin madison technical report january department computer science iowa state university ames ia generative learning structures processes generalized connectionist networks honavar department computer science iowa state university leonard uhr computer sciences department university wisconsin madison massively parallel networks relatively simple computing elements offer attractive versatile framework exploring variety learning structures processes intelligent systems 
briefly summarizes popular learning structures processes networks 
outlines range potentially powerful alternatives pattern directed inductive learning systems 
motivates develops class new learning algorithms massively parallel networks simple computing elements 
call class learning processes generative offer set mechanisms constructive adaptive determination network architecture number processing elements connectivity function experience 
generative learning algorithms attempt overcome limitations approaches learning networks rely modification weights links fixed network topology slow learning need priori choice network architecture 
alternative designs extensions refinements generative learning algorithms range control structures processes regulate form content internal representations learned networks examined 

pattern recognition learning recognize patterns central attributes intelligent entity 
learning defined informally process enables system absorb information environment 
central powerful types learning ability construct appropriate internal representations environment learner operates 
learning build internal representations perception cognition utilize 
pattern directed inductive learning see massively parallel networks relatively simple computing elements focus 
section enumerates research partially supported air force office scientific research afosr national science foundation ccr university wisconsin graduate school iowa state university college liberal arts sciences 
desiderata learning systems motivate structures processes developed sections defines essential terminology discussion follow 

desiderata learning systems learning structures processes developed motivated considerations rapid learning ability adapt changes environment 
robustness presence noisy misleading data large number observations samples able undo mistakes resulting poor data 
ability construct efficient internal representations environment 
resolution stability plasticity dilemma grossberg able modify internal representations response changes environment tasks performed minimal interference performance previously learned tasks 

pattern directed inductive learning definitions issues connectionist learning networks developed date broadly characterized inductive learners 
inductive learning typically involves learning assign patterns appropriate classes categories 
worth defining terminology associated inductive learning pattern directed inductive learning massively parallel networks relatively simple computing elements forms focus 
patterns dimensional dimensional dimensional patterns typically specified terms list attribute values 
reasons clear call patterns dimensional denote vectors attribute values 
dimensional pattern may specified number attributes specify patterns embody inherent spatial temporal spatio temporal structure 
scheme specification patterns rich preserve structure temporal ordering may implicit sequence measurements set attributes time spatial relationships implicit visual image 
dimensional pattern linearly ordered sequence dimensional patterns number measurements sequence temporal resolution sequence time analogy dimensional pattern dimensional array dimensional patterns spatial resolutions dimensions indexed respectively 
explicitly 
simple matter extend scheme specification patterns arbitrary dimension worth pointing scheme specification arbitrary spatial temporal spatio temporal patterns far expressive typically feature vector connectionist symbol processing approaches machine learning implicitly encodes spatial temporal spatio temporal distribution attributevalue measurements iconic picture representations 
representational commitment important consequences design performance learning structures processes 
classes categories responses partition universe patterns denote set patterns universe patterns associated environment perceiver learner operates 
practice environment perceiver learner operates may subset practical situations perceptual learning categorization impossible know exact value obvious reasons denotes cardinality set 
learner ability accomodate new categories needed encounters new class objects environment 
classes categories subsets universe denote set classes 
practice small subset may relevant task perceiver learner perform 
pattern recognition involves making appropriate response class assignment sampled pattern define pattern recognition task producing particular response class assignment pattern perceiver learner samples environment 
universe dimensional patterns defined attributes take possible values contains patterns 
generally universe dimensional patterns defined attributes capable values resolution th dimension contains patterns universe dimensional visual patterns encoded binary input array upper bound number patterns worst case complexity recognition clearly np complete garey johnson tsotsos 
practice patterns universe interest exhibit regularities connectedness compactness judiciously exploited recognizer potentially expected case complexity recognition patterns computationally tractable tsotsos 
inductive learning defined task inductive learner learn set mappings yy assign pattern drawn universe appropriate class es drawn particular interest special case typically assumed machine learning tasks elements mutually disjoint classes 
interest particular structures induced classes may organized hierarchy classes level disjoint conceptual clustering michalski stepp may form complex 
set classes may determined external agent say teacher perceptron rosenblatt may adaptively developed learner internal criteria grouping patterns classes metric similarity patterns art networks grossberg id algorithm quinlan concept learning systems cls hunt cobweb fisher 
inductive learning guided feedback teacher environment 
feedback modify yy necessary 
typical inductive learning scenario typically subset relatively small subset viz 
training set train sampled system learning 
functions yy map set patterns set classes induced sampled training set train cases external agent provides correct class assignment possible define performance measure train yy subset correct train training set train correct class assignments system particular yy 
train yy train correct train performance system categorizing subset patterns encountered viz 
test set test measure ability generalize 
test train test yy test correct test note situations test yy insufficient evaluate merit system 
poor performance test set consequence choice poor inadequate training set 
variations scenario possible teacher may provide sample patterns particular classes desired order starting simpler patterns classes gradually introducing patterns classes increasing complexity 
combinatorial space inductive learner contend intractable worst case gold 
characterization mappings feasibly learnable important area research machine learning valiant 
noted earlier universe dimensional patterns defined attributes capable values resolution th dimension contains patterns simplest cases possible classes number different ways assigning patterns classes problem discovering correct yy assigns patterns respective classes exhaustively examining possible class assignments clearly intractable 
major lesson complexity results inductive learner order effective explore useful regions combinatorial space 
order efficiently absorb information training set 
constrained feedback structures processes learn useful yy map patterns sub universe appropriate classes network responses limited resources tasks perform 
learning structures processes developed constitute steps direction 

approaches pattern directed inductive learning dominant research paradigms computational approaches learning symbol processing systems newell connectionist networks rosenblatt feldman ballard rumelhart hinton mcclelland 
learning sp systems soar laird newell rosenbloom typically involves modification stored data structures rules procedures 
space permit review current machine learning sp paradigm carbonell michalski mitchell dietterich michalski michalski kodratoff provide reviews 
learning techniques symbolic representations suffer high sensitivity noisy data primarily rigid inflexible inference categorization strategies 
years seen tentative moves probabilistic fuzzy inference mechanisms alleviate problem noise sensitivity brittleness sp approaches machine learning michalski kodratoff aha kibler 
source difficulty computational approaches learning choice inappropriate knowledge representations feature vector representation data available road map iconic picture representation appropriate order available learning algorithms 
known knowledge representation chosen influences ease computational complexity learning boolean concepts expressed dnf disjunctive normal form literals disjunct feasibly learnable concepts expressed cnf conjunctive normal form literals conjunct valiant 
need range learning algorithms architectures exploit strengths available representations efficiently transform representations necessary 
networks relatively simple computing elements cn offer attractive versatile framework exploring variety learning structures processes intelligent systems variety reasons massive parallelism computation potential fault noise tolerance 
remainder section describes cn extensions cn leading may called lack better term generalized connectionist networks gcn honavar uhr 
gcn offer number potentially powerful alternative learning mechanisms including generative learning discussed detail 

connectionist networks cn connectionist network rosenblatt feldman ballard rumelhart hinton mcclelland consists directed graph nodes compute relatively simple functions inputs receive nodes network weighted input links external environment transmit results nodes environment weighted output links 
cn typically specified terms function computed individual nodes topology graph linking nodes network adaptive algorithm modify weights links 
network structures implement learning algorithm control structures necessary perform variety functions synchronizing nodes switch learning processes generally left unspecified 
node computes single scalar output value simple function numeric valued inputs 
node inputs dimensional weight vector 
weight associated input link 
input node node associated node represents computational steps involved calculation output node examples calculations shown figures 
form nonlinearity necessary making decisions necessary categorization nilsson 
threshold function discrete decision making device 
classifies input patterns sharply distinguished sets 
logistic function continuous version threshold function 
graded response attractive variety purposes including noise tolerance contrast enhancement existence derivative entire range function property required learning algorithms generalized delta rule rumelhart hinton williams error back propagation 
necessary compare input pattern stored template 
various versions match function occasionally accomplish task 
particular class functions shown 
functions attractive variety reasons output maximum perfect match stored weight vector input monotonically decreases increasing mismatch 
rate decrease response governed functional form tuned varying explicit measure mismatch design variety learning algorithms see 
bias associated node customary treat bias weight associated constant input 
output node commonly define linear threshold logistic node functions linear threshold logistic linear threshold logistic functions output node typical choices positive integer suitable normalization term tunable parameter 
straightforward specialize expression yield normalized euclidian distance vectors hamming distance vectors binary 
typical choices positive integer alternatively functions belong family radial basis functions 
various match functions learning cn modifies weights links learning feed forward cn involves induction functions yy map cn input patterns desired outputs 
accomplished modifying weight vectors associated nodes linked priori fixed topology hinton 
weight modification schemes correlations activation values associated connected nodes mechanism proposed hebb 
various estimates error desired actual network outputs 
error estimates may extremely specific feedback desired network output provided teacher input pattern perceptron algorithm rosenblatt error backpropagation algorithm rumelhart hinton williams werbos faster variants fahlman 
alternatively may obtained specific feedback reward punishment signal reinforcement learning barto anandan may internally derived estimate output necessary node produce desired behavior network competitive learning grossberg 
consider cn node receives inputs node feedback node estimate desired output node current output node typical rule modifying weight takes form dw constant proportionality called learning rate function compute weight modification function error feedback current output 
popular weight modification schemes form perceptron algorithm network layer modifiable links rosenblatt generalization network multiple layers modifiable links werbos rumelhart hinton williams 
major limitation single layer perceptron inability learn correctly partition universe patterns linearly separable nilsson minsky papert 
primarily due limited representational power single layer perceptrons 
multi layer architectures potentially overcome limitation appropriate set learning processes nilsson multi layer networks learn generalized delta rule rumelhart hinton williams demonstrate fact 
fixed network topology set training patterns problem determining set weights correctly map set patterns set pre defined categories network outputs assuming weights exist called loading problem np complete judd 
appears reasonable conjecture schemes cn suffer variant loading problem 
difficulty compounded problems plague gradient descent strategies particular local minima 
little known overcome difficulties multi layer cn learning algorithms 
empirical question particular cn architectures efficiently learn effectively perform particular tasks pattern recognition remains largely open 
generative learning algorithms adaptively acquire new representational primitives function experience see offer range alternatives overcoming limitations single layer perceptrons 

generalized connectionist networks gcn general definition honavar uhr uhr clear today cn architectures extended potentially useful ways 
gcn graph linked nodes particular topology gg 
total graph partitioned functional sub graphs gg behave act sub graph gg ll evolve learn sub graph gg coordinate control sub graph 
motivation distinguishing functions clear 
nodes gcn compute different type functions behave act ll evolve learn coordinate control 
gcn gg ll today cn specified typically partially follows graph structure gg sub net behaves today total graph including entire sub graphs needed handle learning control usually left unspecified 
complete description entire graph gg necessary completely specify cn realizations architectures 
node function weight vector associated node define functions computed behave cycle 
typically node function nodes network 
single function ll compute changes weight vectors learning cycle actual sub net structures needed compute changes 
network behavior typically initiated sending values input sensing nodes 
resulting performance set values sent output acting nodes 
net behavior completely determined topology values originally associated links functions computed nodes links plus modifications learning 
potentially powerful extensions today cn including powerful structures processes behaving control learning suggest leading systems characterized generalized connectionist networks gcn generalized systems gns honavar uhr 
focus learning structures processes 
powerful learning structures processes learning today cn handled processes change associated node reduce error desired output node actual output 
compelling reason restrict set learning processes 
variety potentially powerful alternatives learning modifies node functions associated processing elements gcn learning alter steepness sigmoid function equivalently threshold element learning involve systematically ranging number alternative node functions logical logical changing threshold 
learning modifies weight matrices local templates associated processing element gcn involve weight modification strategies currently cn widrow hoff forms weight modification may appropriate corresponding node functions node matches input stored weight matrix produces measure match gaussian match node suitable weight modification strategy blur weight matrix adding small fraction sufficiently matched input 
learning modifies connectivity network gg addition indicated necessary space deletion links nodes viz generative learning honavar uhr related approaches ash diederich fahlman lebiere hanson gallant nadal marchand 
suitable set generative learning mechanisms gcn adaptively search assume connectivity appropriate particular tasks possibly certain predefined topological constraints imposed locally linked multi layer converging diverging networks honavar uhr 
learning modifies control structures processes gcn particular learning alter controls regulate particular types learning parameters learning rate weight modification initiation termination plasticity manner different sorts learning weight modification generative learning coordinated 
examine examples mechanisms 
learning modifies learning structures processes ll changes weight modification strategies changes node generation strategies 

motivations study generative learning gcn section motivates generative learning enables gcn adaptively acquire necessary network connectivity functions yy necessary effectively classifying patterns universe recruiting nodes growing links needed 
adaptive determination network connectivity cn learn necessary yy entirely changing weights links far commonly learning mechanism initialized contain sufficient number appropriately linked nodes 
way guarantee kind network nodes necessary links provide advance priori knowledge guess necessary number substantially larger number nodes links safe side 
way completely safe worst case estimates extremely impractical problems networks handle real time pattern recognition np complete see minsky papert discussion time versus memory complexity pattern matching 
generative learning enables gcn modify topology nodes connections appears offer partial solution difficult problem choosing fixed network connectivity 
mechanisms generate network gradually grow sufficient topology implicit guidance input patterns feedback environment network learning structures processes necessary number nodes links 
rapid learning generalization cn learns necessary yy changing weights network fixed size connectivity solve intractable loading problem judd 
learning algorithm simply adds new units necessary rapidly build network essentially random access memory look table represent arbitrary set functions baum 
unfortunately look table store sample pattern inefficient memory minsky papert 
furthermore incapable generalizing correctly sample patterns stored look table 
networks supplemented mechanisms enable network generalize correctly sample patterns 
number reasons cn nodes limited fan node receives inputs small subset nodes layer see honavar uhr example networks motivations restricted connectivity 
fan limited manner necessary multiple layers enable computation complex functions entire input pattern 
networks get deeper weight modification schemes error backpropagation perform poorly error signal gets weaker reaches layers output nodes 
generative learning potentially offer way problem constructively building multi layer networks 
cn learn weight modification potentially support generalization rumelhart hinton williams essentially function approximators 
observed cn far nodes excess number needed learning task exhibit poor generalization cases learn overfit training set rumelhart 
considerations suggest generative learning algorithms combine ability add nodes links needed ability change weights yield networks learn rapidly sacrificing ability generalize correctly sample patterns 
contrast schemes start large network discard links improve generalization le cun denker solla mozer smolensky hanson pratt 
constructive estimation expected case complexity successful generative learning provide constructive empirical estimate expected case complexity task perceiving learning perceive everyday objects estimates extremely difficult obtain means tasks usually ill defined 
neurobiological considerations animal learning appears involve growth new synapses life addition tuning synaptic strengths bailey 
gcn combine generative learning tuning node functions weight matrices encode synaptic strengths appear provide potentially useful framework simplified models brain development honavar 
integration symbolic sub symbolic approaches learning generative learning adaptively constructively build gcn network structures conceptually analogous knowledge reformulation techniques symbol processing approaches machine learning chunking soar laird newell rosenbloom constructive induction pagallo haussler 
generated nodes shall see encode potentially useful sub patterns analogous chunks soar 
combined ability fine tune acquired knowledge structures weight modification strategies sort cn provides basis integration symbolic sub symbolic approaches machine learning gcn 

generative learning structures processes gcn general form generative learning viewed process acquisition masks partial templates weight matrices form encode potentially useful information input patterns generating creating recruiting pool uncommitted nodes node instantiate function acquired incorporating generated node network 
follows assume generation encodes information form weight matrices 
simple matter extend encodings form non numeric patterns symbol structures encoding non numeric patterns numeric binary code nodes match non numeric patterns 
basic generative learning algorithm generative learning scheme shown skeletal form 
definition suggests generative learning requires sub tasks performed preferably local gcn deciding generate new node multi layer networks deciding generate new node opposed say continued modification existing nodes choosing pattern sub pattern encoded generated node weight matrix includes choosing inputs generated node choosing nodes receive output node inputs corresponding weights choosing weight modification strategy appropriate node functions particular choices generation strategy choosing particular node link evaluation de generation network pruning network reorganization strategies different design decisions subtasks accomplished gcn lead different variants generative learning strategy 

choosing pattern vector encode generation alternative strategies may choose pattern associated generated node discussed section 
choosing pattern vector random weight vector may chosen node random encoding random pattern vector 
typically strategy mechanisms modify weights available weight modification strategies error back propagation 
strategy dynamic node creation dnc algorithm ash cascade correlation algorithm fahlman lebiere 
choosing pattern vector mutation crossover network may initialized nodes generated encode pattern vectors random described encode small subset prespecified patterns 
additional patterns encoded obtained applying mutation crossover inversion operations akin ones evolutionary learning algorithms fogel owens walsh holland koza pattern vectors encoded network 
clear parallels biological evolution genesis immune system animals farmer packard perelson 
choosing pattern matrix extraction input patterns network sees learning experience carry potentially useful information 
desirable generate nodes weight matrices response functions chosen generated node tuned respond optimally sub pattern input pattern abstraction pattern represented outputs nodes network 
technique extracts weight matrix input abstraction input pattern 
generation extraction process detecting potentially useful sub pattern sampled input pattern generating gcn node suitable weight matrix tuned respond sub pattern 
attempts encode potentially useful information patterns network environment usable form random encoding 
mechanisms accomplishing explained detail 
performance criterion met sample stimulus pattern train compute network output reason modify network error unacceptable criteria generation satisfied choose suitable pattern add node initialize weight matrix grow links chosen nodes output layer initialize weights 
high level outline generative learning algorithm performance criterion evaluated periodically testing network patterns sampled number essential details criteria generation choosing suitable pattern encoded generated node initialization weight matrix associated generated node left unspecified 
generating node extraction network gaussian match nodes sampled input binary pattern 
extraction sub pattern denotes don care 
example node function gaussian match node weight vector set weight vector equal extraction physical interpretation case absence link corresponding input node generated node generated node case responds output network receives input pattern output falls monotonically mismatch input increases 
generation gaussian match node encodes sub pattern extracted input pattern abstraction input pattern 
simple example generation extraction gcn learning universe dimensional patterns shown 
section shows extraction increased refining process extraction 
generation extraction novel sub patterns novel sub pattern currently encoded see nodes share receptive field network 
receptive field node defined set nodes receives inputs 
associated set nodes receive inputs receptive field novelty detector extractor call set nodes field influence novelty detector extractor see 
number nodes constitute field influence extractor circuit denote input pattern network 
denote sub pattern input abstraction seen receptive field denote output response activated produces generate signal sub pattern sufficiently novel see 
conditions novelty detector extractor circuits activated discussed 
novel sub pattern detected receptive field 
translating functional specification 
predefined tunable novelty threshold 
novelty detection involves identifying sub patterns current input pattern abstraction node network sufficiently strong response greater novelty threshold 
novelty detector extractor novel sub pattern receptive field uncommitted gcn node pool nodes available locally network initializes receptive field node growing links nodes recruited node adds set nodes initializes weight matrix tuned respond novel subpattern see just detected resets generate signal 
operation novelty detector extractor illustrated 
network generates novelty driven extraction initialized novelty detector extractor distinct receptive field determines field influence 
alternatively node generated unrepresented receptive field need look novel sub pattern recruit novelty detector extractor uncommitted pool circuits perform extraction novel subpatterns appear receptive field 
increasing information content extraction generation extraction novel subpatterns increases likelihood generated node encodes potentially useful information 
strategy conjunction novelty detection increase information content extraction discussed 
novelty detector extractor receptive field input nodes field influence uncommitted node recruited encode novel sub pattern uncommitted nodes operation novelty detector extractor 
choosing novel extraction random system recruit pool candidates generation encoding different extraction input pattern 
depending locus generation network entire pool situated single network layer may constituted nodes different layers 
presentation sample patterns continues candidates generation pool get trained evaluated parallel 
candidates pool compete added network 
procedure allows phase evaluation usefulness potential extraction network 
simple strategy evaluating information content potential extraction correlate output corresponding node residual error intended reduce high correlation indicative high information content 
different versions strategy honavar uhr fahlman lebiere 
alternatively evaluation information content potential extraction measure mutual information shannon output candidate generation nodes network candidate extractions informative desired network output ones encoded generated nodes 
alternatives selection sub set candidate nodes addition network suggested techniques optimal feature sub set selection developed statistical pattern classification see fukunaga discussion selection algorithms 
generalizing extracted sub patterns generation extraction novel sub patterns described essentially fixes weight matrix associated generated node single extracted subpattern 
universe patterns contains patterns contain similar sub patterns 
sub patterns sufficiently similar novel ones encoded network novelty detector extractor ensure new nodes generated 
similarity sub patterns potentially meaningful 
similarity generalize limited sense previously encoded extraction follows suppose activated novelty detector extractor fails find novel sub pattern receptive field 
case exist nodes set output novelty threshold means subpattern appearing receptive field sufficiently similar sub pattern encoded nodes outputs greater similarity driven generalizer generalizes weight matrix associated nodes follows hx small positive constant 
effect moving sub pattern obtaining optimal response node little bit closer current input node 
additional schemes generalization exist 
possible view task generalization surface fitting interpolation problem set encoded patterns wolpert 
suggests variety interpolation strategies constructing generalizers conjunction generative learning 

establishing output connectivity generated node alternative designs choosing set nodes receive outputs generated node possible 
different design choices establishing output connectivity node different implications terms weights output links get initialized get changed weight modification 
alternative designs discussed 
generated node linked nodes output layer network initialized random heuristically chosen weights 
design recognition cones combine generation weight modification honavar uhr dnc algorithm ash cascade correlation architecture fahlman lebiere 
alternatively generated node linked selectively network output node indicated feedback provided training sample generation 
variations basic designs possible 

weight modification strategies mechanisms generalizing extracted sub patterns described constitute form weight modification 
different form weight modification strategy suggested links connect generated nodes desired input nodes output nodes 
simple perceptron rule variant candidate purpose 
course prevent network learning non linearly class partitions input representation augmented additional representational primitives capture non linear interactions inputs generative learning 
doing avoid necessity back propagation error signals multiple layers 

deciding generate alternative designs micro circuits control decision generate new nodes gcn possible generate nodes certain predetermined tunable rate time perpetual generation corresponding de generation generate feedback indicates network produced incorrect output error driven generation generate feedback differs network output performance network unsatisfactory sufficiently long sequence training samples error driven minimal generation combinations variations 
examine detail error driven minimal generation generative learning algorithms examined empirically honavar uhr 
error driven minimal generation error driven minimal generation motivated need strike suitable balance tuning nodes network limited generalization extractions discussed forms weight modification generation new nodes network determines current topology incapable attaining desired performance 
network inadequate number nodes restricted learning tuning weight matrices may able attain desired performance 
hand new nodes generated indiscriminately result unduly large inefficient network 
network effective processes modify weights tune nodes evaluate nodes usefulness discard nodes links useless 
general strategy error driven minimal generation follows continue tune existing nodes weight modification long network continues progress desired performance criterion initiate generation improvement performance leveled see details 
requires keep track desired performance measure sliding window time sequence presentations training sample 
particular example strategy recognition cone networks combine generation weight modification learning honavar uhr specified detail 
case sufficiently novel see extraction obtained misclassified training pattern sequence patterns generation initiated 
somewhat different versions strategy dnc algorithm ash cascade correlation algorithm fahlman lebiere 
time measured number samples class seen network node extracted sample pattern class current time 
classification accuracy sample patterns class time time interval performance improvement measured 
desired classification accuracy learning 
generate parameter controls degree leveling learning curve sufficient initiate generation 
generation initiated current sample pattern 
generate 

example error driven minimal generation 
deciding generate discussion various aspects generation explicitly address generation multi layer networks details carry case exception extractions higher layers occur abstractions input pattern sampled input pattern 
obvious layers take precedence potential loci generation 
precedence built generative process influences structure network assumes result learning 
alternative designs considered simplest strategy initiates generation layer network criteria generation see satisfied 
strategy employed recognition cone networks learn combined generation weight modification honavar uhr 
different strategy suggested assumption complex features input encoded simple features 
suggests biasing system tends fill lower layers generation start filling higher layers network 
variants strategy possible layer filled pre determined capacity generation layer initiated probability candidate generation added network falls monotonically proceed lower layers higher ones probability candidate generation added network falls monotonically time measured terms number patterns sampled learning layer gets filled generated nodes 
additional alternatives exist networks employ encodings input multiple spatio temporal resolutions honavar uhr may biased generation lower resolution preferred higher resolution reasons representational parsimony 
networks regular topological structure local receptive fields multi layer convergence raise additional possibilities generated node replicated spatial location done recognition cones honavar uhr node may added network location sub pattern extraction number intermediate schemes lie extremes 

node link evaluation network pruning network reorganization necessary eliminate nodes evaluated useless see room new potentially useful extractions 
evaluation may estimated information content node weights output links mutual information estimates redundancy information encoded different nodes results competitive interaction nodes encode patterns environment 
task elimination nodes instance optimal feature sub set selection problem statistical pattern recognition fukunaga 
gcn implementations techniques optimal feature sub set selection adapted network pruning 
number network pruning schemes proposed literature le cun denker solla mozer smolensky hanson pratt reduce size networks potentially improving generalization elimination excess degrees freedom network function approximation 
network de generation strategies fruitfully integrated network generation strategies generative learning 
slower time scale generative learning processes reorganize previously generated nodes structures trees groups independent functional modules may number purposes network compact replacing nodes functionally equivalent single node eliminate excessive redundancy replacing nodes encode information single node introduce fault tolerance replacing single node functionally equivalent distributed cluster nodes 

generative learning search useful internal representations environment cn learn weight modification search useful internal representations environment space weight matrices fixed network topology 
gcn combine generative learning weight modification extend search representations space network topologies addition space weights 
necessitates incorporation additional control structures processes networks constrain search promising regions space wasteful exhaustive search avoided 
techniques guiding search investigated include built general topological constraints suggested problem domain local connectivity converging diverging multi layer structures vision honavar uhr honavar uhr 
controls regulate learning network forced generate new nodes links fails improve performance error driven minimal generation discussed novelty detectors help choose potentially information rich extractions 
section explore additional control structures constrain search development appropriate internal representations environment 
empirical exploration usefulness controls interactions topic research 
vigilance control vigilance node determines sensitive small degrees mismatch pattern optimally tuned input pattern 
behaving phase vigilance determines extent network generalizes previously unseen stimuli 
learning phase vigilance affect generalization previously learned extractions influence potential extraction meets novelty criteria see grossberg example vigilance control learning 
consider gaussian match node output response input pattern vigilance parameter normalization term 
high value vigilance network sensitive mismatch vigilance globally regulated distributed control structures entire network influence behavior learning network placed new way critical environment guiding robot negotiating dangerous terrain desirable increase vigilance 
representational parsimony controls desirable network learn parsimonious internal representations environment 
requirement translated corresponding control structures processes network 
controls generation induce network build parsimonious internal representations environment discussed 
novelty detection minimal feedback guided generation supplementing generation weight modification way contributes parsimony internal representations acquired system learning 
example scheme efficient learning multi resolution spatial temporal spatiotemporal patterns honavar uhr 
parsimony requirement situations may conflict need resolved requirement distributed redundant representations ensure damage resistance requirement rapid learning 
network plasticity control plasticity controls regulate number frequency generations initiated 
plasticity dynamically varied increase network placed new environment suggested instance number novel extractions learned period time learn rapidly 
forms controlled regulation plasticity control nature internal representations learned controlling locus node generation plasticity introduced lower layers gradually extended higher layers network lower layers get filled tuned nodes 
parallels development visual cortex mammals honavar 
representational bias control generation extraction constructively build multi layered networks provides means learning successively higher order interactions features input stimuli 
cases desirable shallow networks 
cases desirable construct networks nodes small fan cases may necessary account physical constraints limit number nodes layer 
ways introducing various forms representational biases networks 
gcn implementations various controls regulatory controls generative learning translated gcn structures implement distributed fashion local 
trivially true gcn turing equivalent describable function realized possibly bizarre inefficient gcn 
see honavar uhr examinations reasonably efficient alternative control structures processes gcn 
worth pointing regulatory controls outlined interesting right contribute powerful learning complex real world tasks perceptual recognition complex objects may unnecessary learning tasks fairly simple 

summary discussion generative learning structures processes examined offer potentially powerful learning capabilities gcn enabling networks adaptively modify connectivity meet needs tasks perform 
variety regulatory controls potentially influence form content internal representations environment learned networks 
learning today cn involves search usually gradient descent error surface represents error desired actual performance network minimize error space weights network topology fixed priori 
initial choice number nodes network connectivity inappropriate network fails attain desired performance 
generative learning potentially find adequate network connectivity constructively learning 
multilayer networks generative learning discover successively higher order relationships attributes input patterns 
networks potentially learn arbitrarily complex mappings universe input patterns desired categories class descriptions 
cn learn weight modification risk getting caught local minimum shallow trough error surface 
generation discarding nodes thought providing means dynamically altering terrain gradient descent performed incidentally offers way potentially climbing local minimum 
generative learning incremental learning technique potentially enable system adapt learn rapidly non stationary rapidly changing environments 
gcn learn minimal generation extraction information rich novel sub patterns environmental stimuli possess direct consequence design potential resolve stability plasticity dilemma grossberg ability respond novelty environment minimal disruption knowledge structures acquired past learning 
range alternative designs generative learning discussed obviously quite large 
far examined small subset designs empirically feedback guided minimal generation extraction novel sub patterns multi layer converging diverging networks perceptual learning dimensional visual patterns honavar uhr simple extensions facilitate efficient learning multi resolution spatial temporal spatio temporal patterns honavar uhr 
results application techniques similar generative learning quite encouraging fahlman lebiere 
large variety alternative generative learning approaches remain explored 
aha kibler 

noise tolerant instance learning algorithms 
proceedings international joint conference artificial intelligence 
san mateo ca morgan kaufmann 
ash 

dynamic node creation backpropagation networks 
connection science journal neural computing artificial intelligence cognitive research 
barto anandan 

pattern recognizing stochastic learning automata 
ieee transactions systems man cybernetics 
baum 

proposal powerful learning algorithms 
neural computation 
carbonell michalski mitchell 

overview machine learning 
michalski carbonell mitchell 
eds 
machine learning artificial intelligence approach 
palo alto ca tioga 
diederich 

knowledge intensive recruitment learning 
technical report tr 
berkeley ca international computer science institute 
dietterich michalski 

comparative review selected methods learning examples 
michalski carbonell mitchell 
eds 
machine learning artificial intelligence approach 
palo alto ca tioga 
fahlman 

faster learning variations back propagation 
hinton sejnowski touretzky eds 
proceedings connectionist models summer school 
san mateo ca morgan kaufmann 
fahlman lebiere 

cascade correlation learning architecture 
advances neural information processing systems vol 

touretzky ed 
san mateo ca morgan kaufmann 
farmer packard perelson 

immune system adaptation machine learning 
physica 
feldman ballard 

connectionist models properties 
cognitive science 
fisher 

knowledge acquisition incremental conceptual clustering 
machine learning 
fogel owens walsh 

artificial intelligence simulated evolution 
new york wiley 
gallant 

perceptron learning algorithms 
ieee transactions neural networks 

garey johnson 

computers intractability guide theory np completeness 
san francisco ca freeman 
gold 

language identification limit 
information control 
bailey 

anatomy memory convergence results diversity tests trends neuroscience 
grossberg 

adaptive pattern classification universal recoding parallel development coding neural feature detectors 
biological cybernetics 
grossberg 

adaptive pattern classification universal recoding ii feedback expectation illusions 
biological cybernetics 
grossberg 

brain build cognitive code 
psychological review 
grossberg 

competitive learning interactive activation adaptive resonance 
cognitive science 
hanson pratt 

comparisons constraints minimal network construction back propagation 
touretzky ed 
neural information processing systems vol 

san mateo ca morgan kaufmann 
hanson 

networks 
touretzky ed 
neural information processing systems vol 

san mateo ca morgan kaufmann 
hebb 

organization behavior 
new york ny wiley 
hinton 

connectionist learning procedures artificial intelligence 
holland 

adaptation natural artificial systems 
ann arbor mi university michigan press 
honavar uhr 

network neuron units learns perceive generation reweighting links 
hinton sejnowski touretzky eds 
proceedings connectionist models summer school 
san mateo ca morgan kaufmann 
honavar uhr 

brain structured connectionist networks perceive learn 
connection science journal neural computing artificial intelligence cognitive research 
honavar uhr 

generation local receptive fields global convergence improve perceptual learning connectionist networks 
proceedings international joint conference artificial intelligence 
san mateo ca morgan kaufmann 
honavar 

perceptual development learning behavioral neurophysiological morphological evidence computational models 
technical report 
madison wi university wisconsin computer sciences dept honavar uhr 

coordination control structures processes possibilities connectionist networks 
journal experimental theoretical artificial intelligence 
honavar uhr 

efficient learning multi resolution representations spatial temporal spatio temporal patterns 
proceedings conference neural networks press 
honavar uhr 

symbol processing systems connectionist networks generalized connectionist networks 
technical report 
ames iowa iowa state university department computer science 
hunt 

concept formation information processing problem 
new york wiley 


antibodies learning selection versus instruction 
schmitt eds 
neurosciences study program 
new york ny university press 
judd 
neural networks complexity learning 
cambridge ma mit press 
koza 

genetic programming paradigm genetically breeding populations computer programs solve problems 
report stan cs 
stanford ca stanford university department computer science 
laird newell rosenbloom 

soar architecture general intelligence 
artificial intelligence 
le cun denker solla 

optimal brain damage 
touretzky ed 
neural information processing systems vol 

san mateo ca morgan kaufmann 
michalski stepp 

learning observation conceptual clustering 
michalski carbonell mitchell 
eds 
machine learning artificial intelligence approach 
palo alto ca tioga 
michalski carbonell mitchell 
eds 

machine learning artificial intelligence approach 
palo alto ca tioga 
michalski carbonell mitchell 
eds 
machine learning artificial intelligence approach 
vol 

san mateo ca morgan kaufmann 
michalski kodratoff 

research machine learning progress classification methods directions 
kodratoff michalski eds 
machine learning artificial intelligence approach vol 

san mateo ca morgan kaufmann 
minsky papert 

perceptrons computational geometry 
cambridge ma mit press 
mozer smolensky 

skeletonization technique trimming fat network relevance assessment 
touretzky ed 
neural information processing systems vol 

san mateo ca morgan kaufmann 
nadal 

study growth algorithm feedforward network 
international journal neural systems 

newell 

physical symbol systems 
cognitive science 
nilsson 

mathematical foundations learning machines 
new york mcgraw hill 
pagallo haussler 

algorithms learn dnf discovering relevant features 
proceedings th international workshop machine learning 
san mateo ca morgan kaufmann 
quinlan 

induction decision trees 
machine learning 
rosenblatt 

perceptron probabilistic model information storage organization brain 
psychological review 
marchand 

learning activating neurons new approach learning neural networks 
complex systems 
rumelhart hinton williams 

learning internal representations error propagation parallel distributed processing explorations microstructure cognition vol 
foundations 
cambridge ma mit press 
rumelhart hinton mcclelland 

general framework parallel distributed processing 
parallel distributed processing explorations microstructure cognition vol 
foundations 
cambridge ma mit press 
rumelhart 

parallel distributed processing 
lecture ieee international conference neural networks san diego ca 


learning object centered representations 
ph thesis university wisconsin madison 
shannon 

mathematical theory communication 
shannon weaver eds 
mathematical theory communication urbana university illinois press 
neuron learn synapse 
touretzky ed 
neural information processing systems vol 
san mateo ca morgan kaufmann 
tsotsos 

analyzing vision complexity level 
behavioral brain sciences 
uhr 

connectionist networks defined generally show power increased 
connection science journal neural computing artificial intelligence cognitive research press 
valiant 

theory learnable 
communications acm 
werbos 

regression new tools prediction analysis behavioral sciences 
ph thesis harvard university 
wolpert 

mathematical theory generalization part los alamos nm los alamos national laboratory 
wolpert 

mathematical theory generalization part ii 
los alamos nm los alamos national laboratory 
widrow hoff 

adaptive switching circuits 
ire convention record part 
