view em algorithm justifies incremental sparse variants radford neal dept statistics dept computer science university toronto toronto ontario canada www cs toronto edu radford geoffrey hinton department computer science university toronto toronto ontario canada www cs toronto edu hinton 
em algorithm performs maximum likelihood estimation data variables unobserved 
function resembles negative free energy show step maximizes function respect model parameters step maximizes respect distribution unobserved variables 
perspective easy justify incremental variant em algorithm distribution unobserved variables recalculated step 
variant shown empirically give faster convergence mixture estimation problem 
variant algorithm exploits sparse conditional distributions described wide range variant algorithms seen possible 

expectation maximization em algorithm finds maximum likelihood parameter estimates problems variables unobserved 
special cases algorithm date back decades grown generality widespread applicability discussed dempster laird rubin 
scope algorithm applications evident book mclachlan krishnan 
radford neal geoffrey hinton em algorithm estimates parameters model iteratively starting initial guess 
iteration consists expectation step finds distribution unobserved variables known values observed variables current estimate parameters maximization step re estimates parameters maximum likelihood assumption distribution step correct 
shown iteration improves true likelihood leaves unchanged local maximum reached uncommon cases 
step algorithm may partially implemented new estimate parameters improving likelihood distribution step necessarily maximizing 
partial step results true likelihood improving 
dempster refer variants generalized em gem algorithms 
sub class gem algorithms wide applicability maximization ecm algorithms developed meng rubin generalized meng van 
cases partial implementation step natural 
unobserved variables commonly independent influence likelihood parameters simple sufficient statistics 
statistics updated incrementally distribution variables re calculated sense immediately re estimate parameters performing step unobserved variable utilizes new information immediately speeding convergence 
incremental algorithm general lines investigated nowlan 
incremental variants em algorithm previously received formal justification 
view em algorithm seen maximizing joint function parameters distribution unobserved variables analogous free energy function statistical physics viewed terms divergence 
step maximizes function respect distribution unobserved variables step respect parameters 
csisz ar viewed em light 
viewpoint justify variants em algorithm joint maximization function performed means process lead maximum true likelihood 
particular justify incremental versions algorithm effect employ partial step sparse view em algorithm variants versions iterations update part distribution unobserved variable pertaining values winner take versions early iterations distributions unobserved variables restricted single value probability 
include brief demonstration showing incremental algorithm speeds convergence simple mixture estimation problem 

general theory suppose observed value random variable value variable data wish find maximum likelihood estimate parameters model assume problem easily solved directly corresponding problem known tractable 
simplicity assume finite range case results generalized 
assume joint probability parameterized 
marginal probability 
observed data wish find value maximizes log likelihood log 
em algorithm starts initial guess maximum likelihood parameters proceeds iteratively generate successive estimates repeatedly applying steps step compute distribution range gamma 
step set maximizes log 
delta denotes expectation respect distribution range note preparation generalization standard algorithm expressed slightly non standard fashion 
step algorithm seen representing unknown value distribution values step performing maximum likelihood estimation joint data obtained combining known value operation assumed feasible 
shown dempster em iteration increases true log likelihood leaves unchanged 
models algorithm converge local maximum exceptions 
monotonic improvement guaranteed radford neal geoffrey hinton gem algorithm partial maximization performed step simply set value log greater log gamma equal convergence reached 
order sense corresponding idea partially performing step view em algorithm variants steps seen maximizing increasing function 
show local maximum occurs local maximum occurs 
contemplate wide variety algorithms maximizing means maximizing incremental algorithms step updates factor corresponding data item 
function defined follows log gammae log entropy distribution note defined respect particular value observed data fixed 
simplicity assume zero finite restriction essential 
need assume continuous function conclude continuous function apart change sign function analogous variational free energy statistical physics provided physical states taken values energy state gamma log 
relate kullback liebler divergence follows gammad jjp lemmas state properties corresponding wellknown facts statistical physics boltzmann distribution states minimizes variational free energy free energy related log partition function 
correspond properties kullback liebler divergence non negative zero identical distributions 
lemma fixed value unique distribution maximizes 
furthermore varies continuously 
view em algorithm variants proof 
maximizing respect constrained requirement solutions possible easily show slope entropy infinite points moving slightly away boundary increase maximum occur critical point subject constraint lagrange multiplier 
maximum gradient respect components normal constraint surface log gamma log gamma follows proportional 
normalizing unique solution 
varies continuously follows immediately assumption 
lemma log 
proof 
log log gamma log log gamma log log log iteration standard em algorithm expressed terms function follows step set maximizes gamma 
step set maximizes 
theorem iterations equivalent 
proof 
steps iterations equivalent follows directly lemma 
steps equivalent follows fact entropy term definition equation depend 
em iterations expressed form clear algorithm converges values locally maximize radford neal geoffrey hinton ignoring possibility convergence saddle point 
theorem shows general finding local maximum yield local maximum justifying standard algorithm variants steps performed partially algorithms maximization done respect simultaneously 
theorem local maximum local maximum 
similarly global maximum global maximum proof 
combining lemmas see log particular 
show local maximum need show near 
see note existed varies continuously near contradicting assumption local maximum proof global maxima analogous restriction nearby values 
assumptions continuity unnecessary result 

incremental algorithms typical applications wish find maximum likelihood parameter estimate number independent data items 
observed variable decomposed unobserved variable 
joint probability factored 
data items identically distributed need assume 
incremental variant em algorithm exploits structure justified basis theorem 
note independent restrict search maximum distributions factor form maximum 
write form log incremental algorithm iteration maximize starting guess parameters guess distribution view em algorithm variants consistent step choose data item updated 
set gamma 
takes time 
set maximizes gamma gamma 
step set maximizes equivalently maximizes log 
data items selected updating step scheme gives preference data items stabilized 
step algorithm requires looking single data item written appears step requires looking components avoided common case inferential import complete data summarized vector sufficient statistics incrementally updated case models exponential family 
letting vector sufficient statistics standard em iteration implemented follows step set gamma 
detail set gamma 
step set maximum likelihood similarly iteration implemented sufficient statistics maintained incrementally starting initial guess may may consistent subsequent iterations proceed follows step choose data item updated 
set gamma 
takes time 
set gamma 
set gamma gamma gamma step set maximum likelihood iteration steps take constant time independent number data items 
cycle iterations visiting radford neal geoffrey hinton data item take slightly time iteration standard algorithm progress distributions variable partial steps utilized immediately held distributions unobserved variables 
nearly fast convergence may obtained intermediate variant algorithm step recomputes distributions data items fewer 
intermediate variant pure incremental algorithm reduces amount time spent performing steps 
note algorithm iteration save value computed contribution may removed new value computed 
requirement generally onerous 
incremental update potentially lead problems cumulative round error 
necessary accumulation avoided ways fixed point representation addition subtraction exact example recompute non incrementally infrequent intervals 
incremental variant em algorithm somewhat similar investigated nowlan 
variant maintain strictly accurate sufficient statistics 
uses statistics computed exponentially decaying average visited data points iterations form step select data item updating 
set gamma 
set fl gamma step set maximum likelihood fl decay constant 
algorithm converge exact answer fl kept fixed value 
empirically converge vicinity correct answer rapidly standard em algorithm 
data set large redundant expect appropriate value fl algorithm faster incremental algorithm forget date statistics rapidly 

demonstration mixture model order demonstrate incremental algorithm speed convergence applied simple mixture gaussians problem 
algorithm iteration tested 
view em algorithm variants gaussian mixture model observed variables realvalued unobserved variables binary indicating gaussian distributions corresponding observed variable generated 
joint probability density parameters ff oe oe follows ff oe oe gamma ff oe gamma exp gamma gamma oe ff oe gamma exp gamma gamma oe problem vector sufficient statistics data item gamma gamma gamma maximum likelihood parameter estimates ff oe gamma oe gamma synthetically generated sample points distribution ff oe gamma oe 
applied standard algorithm incremental algorithm data 
initial parameter values ff oe gamma oe 
incremental algorithm single iteration standard algorithm performed initialize distributions unobserved variables 
necessarily best procedure done avoid arbitrary selection starting distributions affect comparison standard algorithm 
incremental algorithm visited data points 
algorithms converged identical maxima ff gamma oe gamma oe 
special measures control round error incremental algorithm unnecessary case bit floating point numbers 
rates convergence algorithms shown log likelihood plotted function number passes pass iteration standard algorithm iterations incremental algorithm 
case pass visits data point 
seen incremental algorithm reached level half passes standard algorithm 
unfortunately pass incremental algorithm required twice computation time pass standard algorithm due primarily computation required perform step visiting data point 
cost greatly reduced radford neal geoffrey hinton 
comparison convergence rates standard em algorithm solid line incremental algorithm dotted line 
log likelihood shown vertical axis number passes algorithm horizontal axis 

convergence rates algorithm exponentially decayed statistics fl dashed line fl dotted line 
comparison performance incremental algorithm solid line reproduced 
view em algorithm variants intermediate algorithm step recomputes distributions data points 
rate convergence algorithm virtually indistinguishable pure incremental algorithm time required pass greater standard algorithm producing substantial net gain speed 
algorithm iteration tested 
initialization procedure elaboration decayed statistics computed initial standard iteration order initialize iterations 
runs algorithm shown done fl fl 
shown run incremental algorithm 
run fl converged optimal point rapidly incremental algorithm run fl converged poor point 
results indicate may scope improved algorithms combine fast convergence guarantees stability convergence true maximum incremental algorithm provides 

sparse algorithm sparse variant em algorithm may advantageous unobserved variable take possible values small set plausible values non negligible probability observed data current parameter estimate 
substantial computation may saved case freezing probabilities implausible values iterations re computing relative probabilities plausible values 
infrequent intervals probabilities values recomputed new set plausible values selected may differ old set due intervening change parameter estimate 
procedure designed guaranteed increase iteration ensuring stability iterations may decrease detail sparse algorithm represents follows set plausible values frozen probabilities implausible values frozen total probability plausible values relative probabilities plausible values updated iteration 
radford neal geoffrey hinton iterations sparse algorithm go follows step set gamma gamma gamma 
takes time 
set gamma gamma step set maximizes 
easily shown step selects maximize gamma 
suitable models restricted step take time proportional size independent values full range method useful model step done efficiently discussed 
occasion sparse algorithm performs full iteration follows step set gamma non negligible 
set gamma 
set gamma 
set gamma step set maximizes 
decisions values non negligible probability various heuristics 
take probable values predetermined take values needed account predetermined fraction total probability 
choice affect speed convergence stability algorithm bad choice subsequent iterations decrease problems independent observations data item treated independently separate set plausible values distributions expressed terms quantities efficient implementation step probably necessary model simple sufficient statistics 
contribution values frozen probabilities computed full iteration performed saved step combination statistics plausible values view em algorithm variants gaussian mixture problem provides example potential usefulness sparse algorithm 
components mixture data point typically non negligible probability having come components means nearby 
freezing small probabilities distant components avoids continual recomputation quantities negligible effect course algorithm 
note sparse incremental variants em easily applied combination 

variants incremental sparse algorithms variants em justified viewing terms maximizing example employ wide variety standard optimization methods find maximum respect jointly 
view provide insight em procedures 
example winner take variant em algorithm may obtained constraining distribution assign zero probability value 
distribution course represented single value assigned probability 
obviously variant algorithm general converge unconstrained maximum need find value maximizes computational advantages variant early stages maximizing switching variant capable finding true maximum winner take variant converged 
known means clustering algorithm seen light incremental winner take version em algorithm applied gaussian mixture problem variances mixing proportions fixed 
winner take method estimating hidden markov models speech recognition 
instance guaranteed increase iteration lead regard methods completely ad hoc appear sensible seen terms maximizing don find unconstrained maximum 
wray buntine bill byrne mike jordan jim kay andreas stolcke mike titterington comments earlier version 
supported natural sciences engineering research council canada ontario information technology research centre 
geoffrey hinton burns fellow canadian institute advanced research 
radford neal geoffrey hinton csisz ar 
information geometry alternating minimization procedures editors results estimation theory related topics statistics decisions supplement issue 
dempster laird rubin 
maximum likelihood incomplete data em algorithm discussion journal royal statistical society vol 
pp 


interpretation em algorithm mixture distributions statistics probability letters vol 
pp 

mclachlan krishnan 
em algorithm extensions new york wiley 
meng rubin 
extensions em algorithm discussion bernardo berger dawid smith editors bayesian statistics oxford clarendon press 
meng van 
em algorithm old sung fast new tune discussion journal royal statistical society vol 
pp 

nowlan 
soft competitive adaptation neural network learning algorithms fitting statistical mixtures ph 
thesis school computer science carnegie mellon university pittsburgh 
