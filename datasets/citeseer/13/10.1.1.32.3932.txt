emergent control planning autonomous vehicle appear proceedings fifteenth annual conference cognitive science society lisa gary mcgraw douglas blank department computer science center research concepts cognition indiana university bloomington indiana cs indiana edu gem cogsci indiana edu blank cs indiana edu connectionist network trained reinforcement control autonomous robot vehicle simulated robot 
show appropriate sensory data architectural structure network learn control robot simple navigation problem 
investigate complex problem examine plan behavior emerges 

autonomous agents autonomous agent abstractly defined mapping sequence sensory inputs appropriate action response percepts 
agent autonomous extent behavior determined immediate inputs past experience built control russell wefald 
interested investigating cognitive capabilities autonomous agents 
believe cognitive behavior emerge reactive situated activity autonomous agents 
consensus exists design autonomous agents 
relatively direct coupling perception action control distributed decentralized importantly dynamic interaction environment agent maes 
consensus exists best method implement design features 
connectionist networks easily accommodate design features believe effective mechanisms controlling autonomous agents 
focuses exploring connectionist designs controlling simple navigation autonomous vehicle 
conclude applying successful design features difficult problem examine plan behavior emerges 

methodology connectionist controller number implementation questions resolved 
sorts sensory abilities controller need simply react effectively environment second sort memory training subtasks enable controller produce complex responses environment 
third controller trained behavior emerges specified explicitly 
examine implementation questions testing network controllers real simulated robot simple environment experimental variables type sensory data type training subtasks amount memory 
train network controllers reinforcement learning algorithm converts measures goodness reward punishment specific teacher signals 
environment called rectangular box theta feet light corner 
reinforcement training problems investigate involve coordination motor activity information supplied sensors 
navigation problem refer avoid move reinforces robot moving avoiding walls 
difficult problem light food adds goal state order simulate hunger reinforces robot periodically seeking avoiding light avoiding walls moving 
holding problem constant varying sensory ability determine extent addition sensors helps robot succeed problem 
similarly evaluate utility contextual memory different training subtasks autoassociation prediction 

carbot autonomous robot autonomous vehicle robot called carbot modified toy car theta inches controlled programmable mini board designed martin 
carbot inexpensive build primarily primitive sensors lasers video sonar 
controls forward backward motion steering 
robot types physical sensors digital touch sensors front back bumpers analog light sensors near back 
light sensors directed degrees side carbot 
control networks carbot controlled remote connectionist network communicates mini board 
network gathers input data sensors determines set motors time step 
shows standard network experiments 
discrete sets input units sensors context memory 
set represents previous state motors units motor 
motor unit represents spin direction rear motor determines direction motion forward backward 
second unit designates state motor 
third unit represents spin direction front motor determines direction turning left right 
note order turn carbot motors running back motor provides movement front motor steers 
fourth unit designates state front motor 
digital analog context sensor banks motors motors copy connections copy connections fig 

standard control network 
bold arrows indicate units fully connected 
note experiments conducted context units sensor units 
set units input layer represent state digital touch sensors 
digital sensors front back 
front sensors side carbot sense side collisions moving forward 
units represent state analog light sensors values range due ambient light values rarely fall 
right sensor left 
context units experiments 
activations hidden layer previous time step copied context units directly 
simple recurrence context units allows network limited short term memory past states elman 
refer simple recurrent networks srns feed forward networks context memory 
output units standard network determine activity motors time step 
show recurrent connection output motor units input motor units robot physical sensors tell motors doing 
network controls motors information time step easily copied input layer considered additional type sensor 
digital analog context sensor banks motors motors prediction sensors auto association inputs fig 

add prediction network additional output units representing predicted sensor information required 
add auto association output units representing current state motors sensors required 
complex network investigated shown includes prediction auto association 
part research involved determining effect adding specific training subtasks controller network auto association prediction 
shows subtasks included 
general autoassociation forces network pay attention inputs learn duplicate input activations output layer 
prediction may help network build complex model environment instance avoid punishment predicting may hit wall time step 
portion research involved determining effect varying size contextual memory 
reinforcement training autonomous agent problems typically defined terms goals specific input output pairs type reinforcement procedure required learning 
example avoid move problem suppose carbot just bumped wall triggering front sensors 
action moves away wall clears sensors rewarded action persists bumping wall punished 
necessarily right action situation known priori 
experiments control networks trained modified version complementary reinforcement back propagation crbp learning algorithm ackley littman 
back propagation learning requires precise error measures output produced network 
crbp provides exact error measures reward punishment signals follows 
forward propagation input values produces real valued search vector activations interpreted probability associated random bit takes value 
probabilities binary output vector stochastically produced 
rewarded learning push network vector error measure gamma backpropagated 
punished learning push network away vector appropriate direction clear 
crbp chooses push network directly complement error measure gamma gamma 
way rewarded outputs occur punished outputs tend produce complement output vector similar situations 
crbp designed static problems 
carbot problems temporally extended needed modify algorithm 
best knowledge application crbp continuous problem 
original version crbp uses learning rate times greater reward punishment reflect informativeness associated errors 
carbot domain system gets rewarded simply moving environment reward punishment ratio larger static problems making large disparity learning rates infeasible 
side step problem empirically determined reward learning rate times punishment learning rate worked dynamic domain 
experiments reported reward learning rate punishment learning rate momentum training motor outputs 
additional outputs explicit target available auto association prediction trained fixed learning rate 
evaluating global behavior running preliminary tests carbot global metric provide measure behavior percent time punished 
measure graphs shown statistical analyses 
statistical analyses reported analysis variance anova testing 
post hoc significance comparisons done tests 
important keep mind temporal nature experimental problems 
analyzing particular time slice behavior useful 
behavior large range time slices considered 
avoid move networks tended converge learn successful strategy cycles run 
chose consider cycles training phase 
cycles run learning continued performance phase 
gathered data performance phase order evaluate carbot behavior 
training real world robot physically move world bumping things took long time train test particular controller network approximately half hours cycle run 
get time problem decided simulate behavior robot software 
aware simulators clean real world real robot able test correspondence simulator realworld behavior robot 
series brain transplant experiments verify sorts networks worked simulator robot 

carbot simulator simulator program controller networks robot 
empirically determined average turning radius distance traveled carbot 
averages simulator adding small amounts random noise heading position analog sensor readings time step 
test accuracy simulator trained networks simulator robot robot simulator compared behavior terms percent time punished 
controller networks transplant tests provided surprising result 
robot behavior trained network trained simulator robot superior simulator behavior 
suspect robot movements noisy way simulator 
robot movement occasionally noisy simulator systematically adds noise time step 
interestingly added noise beneficial training 
just useful runners train weights legs run competition useful noise simulation training robot testing 
investigation benefits noisy simulator needed 
compare controllers trained simulator noise controllers trained noise 
simulated robot behavior close actual behavior carbot warrant simulator research 
methodology simulator test hypotheses develop useful architectures apply back robot 
saves hours typical cycle run simulator takes minute speedup factor 

experiments simulator ran groups experiments simulator 
experimental variable described trials run 
trial network controller investigated initialized random set weights run cycles 
performance evaluated cycles trial producing average percent punishment 
averages averaged trials 
applied anova composite averages order determine significance variable 
large amount variation trials particular variable standard deviations ranged 
believe primary source variation random initial weights 
sets random weights tended quite bad quite 
unfortunately large variations results may tended obscure significant effects experiments 
avoid move problem recall problem robot rewarded avoiding walls constantly moving 
possible successful strategies simple navigation problem 
easiest solution continually turn direction forming circular path avoids walls 
limited size solution impossible carbot turning radius larger smallest dimension frequent result larger environments reported 
simple solution oscillate step forward step backward proved frequent solution experiments reported 
carbot constraints keep moving avoid walls impetus explore environment develop complex patterns behavior 
baselines determine baseline behavior simulator controller network set motors randomly 
network trained learning 
resulted average punishment 
determine baseline behavior network architectures ran sets tests srns 
types networks access sensor data learn 
wanted see network controllers random weights perform 
average punishment rates ffn model srn model 
differences non learning network baselines vs simulator baseline significant 
interesting shows untrained network controllers perform worse random 
fortunately learning alleviates 
varying sensory data discover effect addition sensor data controlling ability tested types networks 
starting ffn srn context units systematically added combinations sensors analog digital motor 
depicts results variations 
shows average convergence histories types networks curve trials type 
kind graph gives sense learning progresses time 
data points graphs calculated finding percentage punishments cycle bins 
ffn srn srn ffn ffn da ffn srn srn da ffn md ffn srn ma ffn ma srn mda ffn mda srn md srn cycles punishments training phase performance phase fig 

convergence histories types networks investigate utility sensory input 
curve represents average performance networks 
right curve label denoting category network 
labels convention digital sensors analog sensors motor sensors 
percentages punishment bin plotted connected form curve 
noted divided run phases training phase performance phase 
phases marked graph 
statistical analysis run averages performance phases 
seen sensory input networks divided groups 
starting top graph group types network failed converge 
networks types srn ffn inputs analog inputs 
group networks motor sensors including digital analog 
successful group includes networks motor sensors 
types networks best srn motor sensors 
statistical analysis significant difference srn ffn models sensory input tests 
average srns punished time punished time 
significant interaction architecture types inputs 
apparent convergence graph 
analysis confirmed networks access motor sensors far outperformed networks motor sensors 
pairwise comparisons networks motor inputs significantly better input digital input analog input 
motors better digital analog 
significant difference digital analog combined better analog 
interesting motors analogs significantly better motors 
intuitively expected perceptual input available better controller 
may true increasingly complex problems clearly problem just having access previous motor settings informative 
recall motor sensor values provided recurrence output layer input layer see 
simple step memory probably motor sensors useful sensors 
varying training subtasks discover effect adding training subtasks controlling ability tested types networks 
srn provided sensory data motors analogs 
shows convergence histories architecture networks 
best networks bottom graph prediction units 
srn auto srn srn auto pred srn pred punishments cycles training phase performance phase fig 

convergence histories types networks determine utility training subtasks 
networks prediction units pred autoassociation auto 
appears substantial advantage networks prediction significance networks compared standard deviations greater 
comparison changed level comparison twolevel comparison networks prediction vs networks prediction result significant average time punished vs 
forcing network auto association pay attention perceptual input improve performance 
having predict subsequent input extremely useful learning navigation control 
learning mimic input static problem predicting input temporal problem 
appears network context memory effectively training subtask explicitly depends temporal information 
varying contextual memory discover effect size contextual memory controlling ability tested types networks varying memory size units 
networks access available perceptual data including motors 
units units units units units units units punishments cycles training phase performance phase fig 

convergence histories types networks determine utility contextual memory 
shows convergence histories types network tested memory utility 
graph shows certain memory size units memory useful 
significant pairwise results size vs size size vs size 
may point large memory disadvantageous resources explore question 
experiments pairwise comparisons srns units context memory significantly better context memory shown units graph 
result somewhat surprising 
avoid move problem require temporal information expected access temporal information significant benefit learning control 
result explained findings input experiments 
noted earlier motor inputs really form recurrence output layer input layer 
provide information previous time step temporal nature 
means ffn model strictly feed forward 
summary avoid move experiments revealed contrary intuitions better simple navigation problem 
sensory input past motor states informative followed digital touch sensors analog light sensors 
types input better motor sensors 
respect training subtasks prediction significant benefit auto association useful 
contextual memory size better may limit improvement 
set experiments apply design insights difficult problem 
light food problem believe reasoning abilities planning arise developmentally concrete activity chapman agre connectionist autonomous agent controller able exhibit plan behavior complex problem 
problem goal unit added input layer 
positive value goal indicated carbot seek light placed corner maximum light reading obtained 
happened goal unit switched negative value indicating carbot avoid light minimum light reading obtained 
successful avoidance switched goal back seek mode 
seek mode carbot rewarded sum light sensor readings increased relative previous time step 
avoid mode carbot rewarded sum light sensor readings decreased relative previous time step 
carbot concurrently trained avoid move problem 
experiments focused type srn model 
controller networks context memory size prediction auto association provided available sensory inputs 
trials run 
intuitively light food problem difficult avoid move problem 
baseline experiments proved true 
learning controller networks light food punished average time avoid move srns punished significant difference 
difficulty problem reflected amount training required 
cycles successful network controller receiving punishment time average performance 
cycles total best network improved performance average 
training phases general trend observed distribution punishments networks 
illustrate describe trend network 
initial training phase punishment resulted sensor hits moving seeking light incorrectly avoiding light incorrectly 
note easier avoid light seek positions environment satisfy minimum light requirement satisfy maximum requirement 
second training phase distribution punishments substantially different sensor hits moving seeking light incorrectly avoiding light incorrectly 
controller successful seeking light avoiding light 

emergent planning basic strategies adopted controller networks trained light food problem 
avoid mode move away light orient carbot away light 
opposite strategy orient carbot light move light 
see examples strategies actual run network 
fig 

path simulated carbot units inches 
light located origin 
direction arrows indicate carbot current heading 
numbers path refer steps sequence motion 
occurred avoid mode occurred 
note satisfied goals steps 
enact strategies avoid mode carbot moved backward away light steps alternated moving forward turning left backward turning right facing away light steps 
seek mode alternated moving backward turning right forward turning left facing light steps 
moved forward light steps 
principal component principal fig 

path network internal states corresponding actions 
shaded regions highlight visited areas 
carbot orienting light seek mode network alternates regions carbot orienting away light avoid mode network alternates examine network implemented strategies ran principal components analysis hidden layer activations cycles trained network 
shows path carbot shows path network internal states points time 
modes carbot behavior seeking avoiding light distinct network internal transitions 
addition phases mode orient move forward move backward orient away evident 
interesting aspect regions compact reflects fact seeking light constrained task avoiding light 
summary behavior produced network controllers plan number ways 
controllers learned associate goals sequences primitive actions occurred time 
second behavior easily described hierarchically terms see 
controller flexibly react environmental conditions maintaining strategy 
example wall encountered move light phase seek strategy carbot suspends current task heading light backing away wall returns moving forward light 
light food avoid light go away light backward orient away light backward left forward right seek light orient light forward left backward right go light forward fig 

hierarchical view carbot behavior 
respects behavior plan planning traditionally conceived 
controllers consistently anticipate avoid punishment attempt minimize resources produce optimal behavior number complexity strategies exhibit minimal 
spite deficiencies believe multi step procedural behavior emerged light food problem interestingly plan 
results lend empirical weight argument complex behavior result low level interaction autonomous agent environment 

sven anderson amy barley laura dave chalmers mike gasser jim marshall mcauley jonathon mills john cathy rogers andy strauss support research 
andreas stolcke cluster principal component analysis program 
ackley littman ackley littman 

generalization scaling reinforcement learning 
editor advances neural information processing systems pages 
morgan kaufmann san mateo ca 
chapman agre chapman agre 

reasoning emergent concrete activity 
georgeff lansky editors reasoning actions plans proceedings workshop pages 
morgan kaufmann los altos ca 
elman elman 

finding structure time 
cognitive science 
maes maes 

guest editorial designing autonomous agents 
robotics autonomous systems 
martin martin 

mini board technical 
mit media lab cambridge ma 
russell wefald russell wefald 

right thing studies limited rationality 
mit press cambridge ma 

