learner evaluation usefulness statistical phrases text categorization maria facultad universidad de buenos aires buenos aires argentina mail dd stan matwin dept science university ottawa mail stan site ca fabrizio sebastiani istituto di dell informazione cell inf nazionale delle ricerche pisa italy mail fabrizio iei pi cnr investigate usefulness grams document indexing text categorization tci call gram set word stems say occurs document sequence words appears word removal stemming consists exactly ofthe stems order 
previous researches investigated grams variant context specific learning algorithms obtained general answers usefulness tc investigate usefulness grams intc independently ofany specific learning algorithm 
applying feature selection pool grams checking grams score high selected top grams 
report results experiments various feature selection measures varying values performed benchmark 
report results actual ofthe selected grams context ofa linear classifier induced means rocchio method 
key issue retrieval ir content text management applications docu author carried visiting department computer science university ottawa supported universidad de buenos aires 
ment indexing task constructing internal representation ofa text amenable interpretation document management algorithms ii compactly capture meaning choice representation format text depends regards meaningful textual units problem lexical semantics meaningful natural language rules combination ofthe meanings units convey problem compositional semantics 
traditionally ir concentrated issue disregarded issue assuming representation document may obtained simply account frequently word appears document collection disregarding syntactic semantic pragmatic contexts occurrences 
rise called bag words approach indexing text represented vector rj number occur document collection represents loosely speaking word contributes semantics weights computed frequency collection consideration 
variants ofthe bag approach obtained word stems disregarding frequency issues simply binary assignment presence absence setof words approach 
speak neutral expression indicate vector words stems characteristics ofa document decide representation 
course possible choices counts feature limited current text processing technology extracted fully automated scalable way text 
principle best identify features concepts document deals problems document tackles pieces reach knowledge extraction technology 
phrase indexing ir past number researchers expressed bag set words approach tried notions feature time semantically richer technically feasible 
particular number authors investigated phrase indexing phrases addition individual words features 
linguistic sense phrase textual unit usually larger word smaller full sentence examples noun phrases nuclear dis pos dog bill clinton examples verb phrases playing ice hockey went tos ol 
term syntactic phrase denoting phrase grammar ofthe language consideration 
syntactic phrases indexing interesting idea phrases come closer individual words stems expressing structured concepts phrases smaller degree constituent words mutual disambiguation ect 
words hand drill ambiguous hand cards hands oil drilling pronunciation drill hand drill constituent words creates context unambiguous interpretation ofthe phrases index terms document contains phrase ranked higher document just contains constituent words unrelated contexts current natural language processing technology special part speech tagging parsing allows individuation performed degree robustness 
unfortunately number researches investigated usefulness indexing syntactic phrases ir obtained discouraging results see section 
reason indexing languages phrases superior semantic qualities inferior statistical qualities respect indexing languages single words 
instance phrase nuclear dis os definitely denotes interesting articulated concept occurs frequently document collection consideration impact terms ofe ectiveness 
situation worsened fact concept may triggered related linguistically di erent units os nuclear dis os nuclear usually considered standpoint frequency di erent unit similar underlying concept recognized 
syntactic phrase denotes interesting concept asc tall telling phrase di cult call termhood problem 
number attempted find way ofthese problems understanding notion phrase statistical sense syntactic sense 
call statistical phrase sequence words occur contiguously text statistically interesting way 
statistical phrases number syntactic ones may recognized means robust computationally demanding algorithms ect syntactic variants factored uninteresting phrases tall tend filtered interesting ones 

ofcourse inherent statistical nature disadvantage ofa non null error rate phrases going recognized non phrases going incorrectly recognized phrases 
deals assessing value phrases document indexing context text categorization tcd activity learning classify natural language texts topical categories pre specified set 
previous researches investigated impact phrases tc context learning algorithms obtained general answers usefulness tout court 
want analyze problem learner independent way aim indication ofthe usefulness statistical phrases independent learning algorithm 
order extract word sequences corpus documents assess value direct way running classification experiments test collection indirect way scoring sequences means ofa number evaluation 
organized follows 
section briefly introduce basic notions categorization 
section define precisely notion phrase call gram section describe learner independent method evaluation grams 
section describes results obtained applying method reuters standard research 
section discuss number direct experiments conducted running rocchio classifier learning algorithm gram representations aimed assessing results indirect experiments confirmed field tests started running similar experiment ripper system results ready printing time 
section describes related phrase indexing ir tc section concludes 
text categorization text categorization known text classification topic spotting activity building means learning ml techniques automatic text classifiers programs capable natural language texts thematic categories predefined set 
frequently approach building text classifier categories independent classifiers capable document classified category fori term gram text processing literature quite di erent senses 
sense indicate set words occur sequentially text 
second sense indicate set occur sequentially text may part word sequence words occurring contiguously sense texts resulting ocr asian languages dealt 
general assumption document principle belong zero categories assumption verified reuters benchmark experiments 
techniques discuss straightforwardly adapted case document belongs process requires availability ofa corpus preclassified documents documents known 
general inductive process called learner automatically builds classifier category learning characteristics training 
classifier built ectiveness capability take right categorization decisions may tested applying test set te tr checking degree decisions ofthe automatic classifier encoded corpus 
feature selection classifier induction methods computationally hard computational cost function ofthe ofthe vectors represent documents 
importance able vectors shorter usually number tens 
selection techniques select original set ofr features subset ofr features useful compactly representing meaning ofthe documents value called usually techniques consist scoring feature means evaluation fef selecting ther features highest score 
feature selection beneficial tends reduce overfitting phenomenon classifier tends better classifying data trained classifying data 
functions tradition decision information theory intc illustrated table 
third column ofthis table probabilities interpreted event space indicates probability random document feature occur belongs category estimated counting occurrences training set 
column function refers specific category order assess value ofa feature global category independent sense weighted average avg maximum fmax max category specific values usually computed 
function denoted mathematical form document frequency df tk log log chi square odds rat tk table feature evaluation functions literature 
formula cardinality ofthe training set 
bw grams start precisely characterizing mean statistical phrases 
definition number contexts see section detailed discussion 
definition gram unigram word stem 
gram alphabetically ordered sequence gof unigrams 
say gram occurs manifests document sequence words appears aft word removal stemming instance inform gram bigram possible manifestations text expressions information retrieval retrieval information informative retrieval retrieving information retrieved information retrieving information retrieves information retrieve information 
inform retriever 
note evident examples word removal stemming alphabetical ordering ect factoring notion gram number syntactic semantic variations 
morphosyntactic variations note noun phrases expressions verb phrases expressions sentences expressions manifestations gram 
semantic variations note noun phrases di erent meanings case give rise gram 
defining grams way hypothesis various syntactic expressions may convey concept seen form conflation 
types conflation generalization perform means grams problems 
particular grams defined su er generalization may seen fact example refer concept examples generalization may seen fact expression ing information arguably refers concept examples recognized 
note quite obviously mere contiguous occurrence oftwo words text guarantee refer complex concept 
instance text whatis recurs itis illus dialogue little harmonic labyrinth ing variations contains recur dialog harmon harmon labyrinth var 
arguably ofthese conveys articulated concept case consecutive occurrence words indicative ofa strong semantic relationship 
clear grams indexing purposes possible presence ofa method filtering interesting grams non interesting ones 
filtering necessary number grams occur collection high 
fact number gram occurrences increases linearly gram occurrence gram occurrences number di erent grams increases average gram occurs frequently average gram 
possible filters frequency occurrence considerations 
surprising may expect interesting bigram inform di erent occurrence patters uninteresting occasional bigram illus recur 
classifier ind ends evaluation usefulness statistical phrases text categorization method usefulness grams purposes consists generating grams occur corpus score means ofa fef ofthe type discussed section rank score received 
usefulness grams determined frequently grams appear top ofthis ranked list 
order precise introduce notion penetration level grams 
definition training setof documents andr numberof di erent unigrams occur 
define penetration level grams fef ther top grams tr grams 
purpose ofthis definition best described example 
suppose di erent unigrams training ifwe apply fef ofthese unigrams reduction factor obtain unigrams considers valuable 
suppose di erent bigrams intr order compute penetration level apply ofthe grams check ofthe top grams bigrams 
higher worthwhile bigrams look worthwhile looks extract 
worthwhile chosen fef reduction factor chosen 
ifwe repeat experiment di erent di erent reduction factors jr averaging results way get fairly clear picture promising bigrams look purposes invoking single learning algorithm means results arguably going valid regardless ofthe specific learning algorithm chosen 
method ofcourse applicable value pros cons approach moving discussion experimental results obtained means possible approach evaluation grams tc possible alternative approach consists generating subset grams grams selected particular statistical filter heuristics document indexing checking di erence ectiveness classifier exhibits respect standard bag case 
method doubt advantage ofa better computational ciency instance heuristics grams composed valuable unigrams certain frequency characteristics generated allows substantially reduce computation time needed generate grams completely avoids computation time needed score 
practical applications may feasible method 
drawback ofthis method experimental results obtained going dependent chosen heuristics chosen classifier learning algorithm 
method chosen abstracts away aspects 
aspect needs discussion concerning want emphasize 
method relies generic heuristics studied founded statistical information theory 
method relies application ofa range obtain results biased fef 
sense real object ofthis grams application devising cient algorithm extracting 
foundational nature want assess principle grams interesting applications worth devise algorithm 
purpose clear need analyze grams just generated selective heuristics 
reason need perform analysis general possible way specific learning algorithm widest possible spectrum 
experiments performed number experiments order test usefulness grams mentioned learner independent method 
experiments reported limited case 
experimental setting experiments reuters ut distribution corpus currently widely benchmark text categorization research reuters consists ofa set news stories partitioned modapte split adopted training set documents test set documents 
documents average length words word removal labelled categories average number document ranging minimum maximum 
number examples category ranges minimum maximum 
definition reuters contains unigrams bigrams total uni bigrams 
run experiments set categories training example commonly subsets 
full set categories harder includes categories positive instances inducing reliable classifiers obviously haphazard task experiments discussed section words removed list provided reuters corpus may downloaded experimentation purposes www re see discussion right subset categories 
pages 
punctuation removed letters converted lowercase number removal performed 
experimental results table report results penetration levels bigrams applying described table varying reduction factors 
chosen turned best performers thorough comparative experiments 
results evident fef penetration level decreasing function reduction factor 
surprising small reduction factor features selected unigrams tend available selection number available selection tends substantially una ected originally huge number 
instance total unigrams bigrams times bigrams unigrams 
selecting say top features means avg unigrams bigrams chosen means unigrams bigrams available selection 
means times bigrams unigrams higher proportion earlier means chances selected feature bigram higher 
evident studied may partitioned groups df avg ig avg max avg ofthe group display similar behaviour 
incidentally confirms results pedersen experiment involving di erent collections shown df avg ig highly correlated conjectured pattern general 
third observation penetration levels high 
means ifwe define bigrams definition statistical characteristics employed preferable unigrams rated high 

lists various reduction factors average score obtained feature selected fef 
entry lists average score bigrams increase average score obtained switching df avg ig cv fh avg max reduction features factor ig avg max table penetration level grams computed di erent di erent reduction factors 
penetration level grams computed di erent di erent reduction factors 

order correctly interpret results note row includes sub rows reporting results unigrams uni bigrams cases respectively numberof 
instance interpret row note reducing set unigrams reduction factor yields features number obtained reducing set uni bigrams reduction factor 
results listed incf show achieve high penetration levels see table achieve high increase average score ofa feature 
confirm penetration levels reasonable way compute contribution grams quality ofa feature set 
combined results indicate df avg ig avg conservative allow bigrams enter top scoring feature set conversely max avg liberal 
direct experiments evaluation methodology experiments follow classification ectiveness measured terms ofthe classic ir notions pr recall re adapted case categorization 
precision wrt pr defined probability ifa random document dx categorized deemed positive example decision correct true positive 
follows tp tn fp fn denote numbers positives true negatives false positives false negatives respectively 
wrt re defined probability ifa random document dx ought categorized decision taken 
estimates re indicated pr re obtained obvious way counting occurrences test set 
category relative values may turn averaged obtain pr re values global category set alternative methods microaveraging indicated superscript pr re obtained globally summing individual decisions pr tp tp fp tp tp fp re tp tp fn tp tp fn macroaveraging indicated superscript precision recall evaluated locally category globally averaging results ofthe di erent categories pr pr tp tp fp re re tp tp fn experiments evaluated microaveraged macroaveraged precision recall 
measure ofe ectiveness combines contributions pr re wellknown function defined pr re pr re 
similarly researchers parameter value places equal emphasis pr re 
experimental results table compares ectiveness uni bigrams linear classifier induced rocchio method table di erent reduction factors 
rocchio parameters set see section full discussion rocchio method 
term weighting obtained means ofthe standard ltc variant ofthe tfidf function tfidf tf log tr tr tr denotes number tr occurs tf log 
denotes number occurs weights normalized cosine reduction average average average factor score document category ig ig ig ig ig ig avg avg avg avg avg avg max max max max max max table average score ofa feature average number feature occurs average number categories feature occurs computed various di erent reduction factors 
entry lists score unigrams case upper sub row uni bigrams case lower sub row percentage increase 
reduction micro micro micro macro macro macro fef factor recall precision recall precision ig ig ig ig ig ig ig ig avg avg avg avg avg avg avg avg max max max max max max max max table kfi fhv unigram uni bigram ectiveness ofa rocchio classifier di erent di erent reduction factors 
normalization kj tfidf tfidf set features resulting feature selection 
conventions formatting table similar discussed table 
particular recall entry consists oftwo listing performance rocchio classifier unigram representation upper sub row uni bigram representation lower sub row representations number ultimately means value purposes measured second sub row reports better result magnitude ofthese improvements 
results show bigrams contribute categorization ectiveness ofthe rocchio classifier cases witness improvement ectiveness cases loss performance 
bigrams bring performance improvement seldom significant best improvement obtained macroaveraged max 
bigrams cause deterioration performance significant worst deterioration obtained microaveraged 
sense unexpected results indicate particularly penetration levels increases average scores high quality ofthe feature set increases 
improvements evenly distributed microaveraged macroaveraged cases 
may observe 
improvements achieved low high reduction factors 
instance reduction factor tends associated performance gains reduction factor invariably brings ectiveness losses 

loss ectiveness introduced bigrams higher achieved high penetration levels 
instance cases bigrams improve performance obtained ig cases df avg avg yielded smallest penetration levels table smallest increases average score table 
produced high penetration levels increases average score perform badly case avg achieve 

increases number feature occurs average number categories feature occurs associated increase performance definitely clear pattern 
observations especially indicate excessive expense may detrimental ectiveness ifthe total score ofthe top feature set increased letting bigrams 
may indicate important unigrams pushed ofthe top set bigrams duplicate information carried existing unigrams 
instance inform inform may selected top set inform pushing unigram quite unrelated remaining features 
inherent weakness ofthe filtering approach feature selection fact feature evaluated independently features 
principle better approach wrapper approach feature selection feature sets evaluated approach impractical tc presence sets choose computationally infeasible 
eliminating potentially informative unigrams selecting bigrams drawback increases pairwise stochastic dependence terms situation odds principles underlying text classifiers currently including rocchio 
methods designed handle situations maximum entropy 
maximum entropy combines feature selection classifier somewhat similarly bayesian methods 
confronted words occur frequently situation ect mentioned duplication maximum entropy avoids occurrence significant predictor 
empirical applications reported mixed performance maximum entropy 
domains improvement reported respect bayesian classifiers classification accuracy noted 
kantor lee report similarly mixed results information retrieval task 
related phrase indexing closely related problem automatic term recognition atr terminology subfield linguistics investigates identification extraction texts linguistic units characterise specialised domains 
excellent review research draw distinction research emphasizes fact linguistic expression qualifies term syntactic point research emphasizes termhood fact linguistic expression qualifies term semantic point 
distinction drawn syntactic statistical phrases ir similar 
related information retrieval syntactic statistical phrases ir dates back early see review early 
fagan thorough experimental comparison standard indexing syntactic phrase indexing statistical phrase indexing performed 
experiments fagan syntactic phrases yield small ectiveness improvements notwithstanding fact sophisticated linguistic technique employed phrase extraction 
importantly statistical phrases obtained simple method improved performance lot syntactic phrases 
lewis investigated idea extracting syntactic phrases clustering order endow resulting indexing language better statistical properties resulted significant ectiveness improvements 
mitra investigated impact syntactic statistical phrases ir 
research shows di erence ectiveness negligible significant overlap sets identified methods ofthe union ofthe sets intersection 
shown phrase indexing gives little benefits low recall levels benefits tend increase high recall levels 
important observation applications intc recall level usually parameter learnt validation set means tc recall level maximizes performance automatically chosen system 
statistical phrases exactly equivalent bigrams consider grams di erence empirical statistical filter place bigrams occurring documents considered 
results concerning statistical phrases essentially confirmed study moffat tried non phrases obtaining substantially di erent results 
related text categorization quite researchers investigated usefulness phrase indexing ir purposes relatively done tc context 
number syntactic statistical phrases purposes provide explicit comparisons performance phrases 
syntactic phrases lewis study effects syntactic phrase indexing tc context 
reported context ofa nave bayes classifier yields significantly lower ectiveness standard set words indexing regardless syntactic phrases successively clustered similarly 
remarked lewis phrase indexing language consisted phrases di erent works including phrases added unigram indexing language 
dumais report having noted benefit syntactic phrases variety classifiers context experimentation 
furnkranz showed syntactic phrases yield precision improvements low recall levels confirming results obtained mitra ir context 
statistical phrases mladenic grobelnik extracted grams means ofa fast incom plete algorithm relies document frequency statistical filter 
nave bayes classifier applied corpus pages grams give significant benefits respect single words case grams provide additional benefit 
furnkranz uses algorithm similar extract grams 
reuters ut ripper significant improvement performance grams longer grams reduce classification performance dataset usenet newsgroup articles grams utility negative contribution grams confirmed 
di erence experiments experiment apart obvious issue learner independence stemming alphabetical ordering 
important di erence discussed section stemming alphabetical ordering allow factor significant number syntactic semantic di erences linguistic expressions 
di erence research works discussed section comparing ectiveness deriving standard indexing deriving phrase indexing keep number fixed bigrams substitute unigrams vector representations works bigrams added unigrams representation 
chosen ir intc dimensionality feature space important parameter see section ofthis comparison di erent representation schemes significant numbers 
investigated usefulness bigrams text categorization performing study assessing indications ofthis study confirmed real text categorization experiments 
think approach sheds light role tc role previously published experiments learner dependent issues 
study uses definition grams standard ir contexts evaluated tc experiments 
learner independent study showed feature evaluation functions routinely text categorization experiments tend score bigrams higher unigrams select unigram feature selection tasks giving rise high bigram penetration levels 
indicate value added selecting fixed number features pool contains unigrams bigrams 
hypothesis high penetration level conducive improving ectiveness completely confirmed 
particular experiments showed bigram penetration level high ectiveness may decrease easy conjecture due elimination unigrams part partly duplicate information carried existing unigrams 
think issue duplication result insertion central understanding significant penetration levels part go par classifier ectiveness improvements 
main direction plan carry 
ts luigi making available text classification software environment greatly simplified experimental making available ripper system collaboration implementation gram extraction software 
irene making meet 
apte fred damerau weiss 
automated learning rules text categorization 
acm transactions systems 
eric brill 
simple rule part speech tagger 
proceedingsof anlp ence applied natural language processing pages trento 
cf buckley amit singhal mandar mitra 
query zoning correlation smart ellen voorhees donna harman editors proceedingsof trec th text ence gaithersburg 
kenneth 
stochastic parts program noun phrase parser unrestricted text 
proceedingsof anlp ence applied natural language processing pages 
william cf yoram singer cn learning methods text categorization 
acm transactions systems 
fred damerau 
evaluating domain oriented multi word terms texts inf processing management 
susan dumais john platt david heckerman mehran sahami 
inductive learning algorithms representations text categorization 
georges french luc editors proceedingsof cikm th acm international conf ence knowledge management pages bethesda 
ac press new york 
tal laurent 
automatic natural acquisition ofa terminology 
quantitative linguistics 
joel fagan 
experiments automatic phrase document retrieval syntactic non syntactic methods 
phd thesis department fh science university ithaca 
joel fagan 
ectiveness ofa approach automatic phrase indexing document retrieval 
american society fi inf science 
william frakes 
stemming algorithms 
william frakes ricardo baeza yates editors inf retrieval data structures algorithms pages 
prentice hall 
norbert fuhr stephan hartmann gerhard gerhard lustig michael tzeras 
air rulebased multistage indexing system large subject fields 
andre editor proceedingsof riao rd ence inf par ordinateur pages barcelona es 
elsevier science publishers amsterdam nl 
johannes furnkranz 
study gram features text categorization 
technical report tr fur artificial intelligence wien 
johannes furnkranz tom mitchell ellen 
case study linguistic phrases text categorization www 
proceedings st aaai workshop text categorization pages madison 
luigi fabrizio sebastiani maria simi 
experiments selection negative evidence automated text categorization 
jose thomas baker editors proceedingsof ecdl th european conf ence research advanced digital libraries number lecture notes incf science pages lisbon pt 
springer verlag heidelberg de 
george john ron kohavi karl pfleger 
irrelevant features subset selection problem 
proceedingsof icml th international ence machine learning pages new brunswick 
bin 
methods term recognition review 
terminology 
paul kantor jung lee 
testing maximum entropy principle information retrieval 
american inf jq science 
lam dik lee 
feature reduction neural network text categorization 
cfw frederick lochovsky editors proceedingsof dasfaa th ieee ence database advanced advanced application pages tw 
society press los alamitos 
david lewis 
evaluation clustered representations text categorization task 
nicholas belkin peter ingwersen mark pejtersen editors proceedingsof sigir th acm ence research development retrieval pages dk 
ac press new york 
david lewis 
representation learning mk retrieval 
phd thesis department cf fi science university amherst 
david lewis bruce term clustering phrases 
proceedingsof sigir th acm ence research development retrieval pages bruxelles 
mandar mitra cra fi wq buckley amit singhal cdf analysis syntactic phrases 
proceedingsof riao th ence recherche inf par ordinateur pages montreal ce 
extended version forthcoming nk processing management 
mladenic 
feature subset selection text learning 
incf nedellec andc rouveirol editors proceedingsof ecml th european ence machine learning number lecture notes incf vw science pages de 
springer verlag heidelberg de 
mladenic marko grobelnik 
word sequences features text learning 
proceedingsof erk seventh electrotechnical computer ence pages ljubljana sl 
kamal nigam john la erty andrew maximum entropy text classification 
proceedingsof ijcai workshop inf qn filtering stockholm se 
robert schapire yoram singer 
boostexter boosting system text categorization 
machine learning 
robert schapire yoram singer amit singhal 
boosting rocchio applied text filtering 
bruce alistair mo van rijsbergen ross wilkinson justin zobel editors proceedingsof sigir st acm ence research development retrieval pages melbourne au 
ac press new york 
hinrich schutze david hull jan pedersen 
comparison document representations routing problem 
edward fox peter ingwersen fidel editors proceedingsof sigir th acm ence research development retrieval pages seattle 
ac press new york 
fabrizio sebastiani 
machine learning automated text categorisation 
technical report iei istituto di dell informazione cell inf nazionale delle ricerche pisa 
submitted publication acm computing surveys 
andrew alistair mo 
statistical phrases vector space information retrieval 
proceedingsof sigir nd acm international ence research development retrieval pages berkeley 
tzeras stephan hartmann 
automatic indexing bayesian inference networks 
robert korfhage rasmussen peter willett editors proceedingsof sigir th acm ence research development retrieval pages pittsburgh 
ac press new york 
yiming yang jan pedersen 
comparative study feature selection text categorization 
douglas fisher editor proceedings icml th ence machine learning pages nashville 
morgan kaufmann publishers san francisco 
joe zhou 
phrasal terms real world ir applications 
tomek strzalkowski editor natural retrieval pages 
kluwer academic publishers dordrecht nl 

