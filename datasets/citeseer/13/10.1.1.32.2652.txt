letter communicated alexandre pouget mutual information fisher information population coding nicolas brunel jean pierre nadal laboratoire de physique statistique de ecole normale sup rieure paris cedex france context parameter estimation model selection quite direct link fisher information informationtheoretic quantities exhibited 
give interpretation link standard framework information theory 
show context population coding mutual information activity large array neurons stimulus neurons tuned naturally related fisher information 
light result consider optimization tuning curves parameters case neurons responding stimulus represented angular variable 
natural framework study neurons communicate transmit information nervous system information theory see blahut cover thomas 
years information theory neuroscience motivated large amount linsker barlow mitchison bialek rieke de van van hateren atick nadal parga 
asks informal sense information spike train single neuron population neurons provides external stimulus 
example high activity ca hippocampal neuron may tell precision rat environment 
information theory provides mathematical tools measuring information selectivity signals characterized probability distribution spike train neuron population characterized probability distribution conditioned signal 
mutual information signal neural representation measure statistical dependency signal spike train 
laboratory associated cnrs ens universities paris paris 
neural computation massachusetts institute technology nicolas brunel jean pierre nadal related domain belongs information theory field statistical parameter estimation 
typically sample observations drawn distribution depends parameter set parameters wishes estimate 
cramer rao inequality tells mean squared error unbiased estimator underlying parameter lower bounded inverse quantity defined fisher information blahut 
means fisher information measure estimate parameter observation probability law 
sense information quantity 
spite similar intuitive meanings quantities explicit relationship fisher information information theoretic quantities derived clarke barron rissanen limit large number observations 
link exhibited context parameter estimation clarke barron case statistically independent identically distributed observations 
generalized broader context framework stochastic complexity result refined minimum description length criterion model selection rissanen 
goal article show framework information theory link manifests naturally context neural coding limit large number neurons coding low dimensional stimulus population coding mutual information activities neuronal population stimulus equal mutual information stimulus efficient gaussian estimator appropriate conditions detailed section 
efficient means variance estimator reaches cramer rao bound 
variance related fisher information equality provides quantitative link mutual fisher informations 
equality shown hold single cell case gaussian noise vanishing variance 
mutual information stimulus efficient gaussian estimator reaches mutual information stimulus neuronal activities asymptotically 
light relationship fisher mutual information examine section issues related population codes neurons coding angular variable triangular bell shaped tuning curve 
neurons common neural structures 
cells taube muller anterior thalamic nuclei taube rat tuned head direction 
cells mt cortex maunsell van essen monkey tuned direction mutual information fisher information population coding perceived motion 
cells motor cortex monkey georgopoulos massey tuned direction arm 
study case array neurons firing poisson process response angular stimulus frequency defined tuning curve neuron interval duration cases poisson processes considered reasonable approximations firing process cortical neurons see koch 
calculate fisher information arbitrary density preferred angles 
address question optimization tuning curves making link mutual information fisher information 
optimal density preferred angles maximizes mutual information calculated function distribution angles section 
shown seung sompolinsky fisher information large limit diverges tuning width neurons goes zero 
show section finite tuning width stems optimization criteria consider finite system small number spikes emitted population 
illustrate results triangular tuning curves section 
general framework parameter estimation population coding 
general context parameter estimation wishes estimate parameter set observations xi xi discrete continuous 
may characterize model expected description stochastic process generating observations xi 
simplest case xi independent realizations random variable xi 
may case necessary true process belongs family consideration true value parameter 
context sensory coding specifically population coding see seung sompolinsky stimulus 
angle information stimulus contained activities xi population large number neurons 
simplest case xi represents activity ith neuron output layer feedforward network lateral connection probability density function factorized pi xi 
nicolas brunel jean pierre nadal pi xi neuron dependent activity xi neuron input stimulus takes value 
task neural system obtain estimate stimulus value problem particular case parameter estimation exists true value generated observed activity cramer rao bound 
general find different algorithms computing estimate observation chosen estimator algorithm unbiased xp variance estimator denotes integration sum case discrete state vector bounded cramer rao bound see blahut fisher information ln 
multidimensional parameter equation replaced inequality covariance matrix fisher information matrix expressed terms second derivatives ln blahut 
simplicity restrict discussion case scalar parameter consider straightforward extension multidimensional case section 
efficient estimator saturates bound 
maximum likelihood ml estimator known efficient large limit 
mutual information fisher information main result 
give interpretation cramer rao bound terms information content 
note fisher information see equation information quantity 
terminology comes intuitive interpretation bound knowledge information stimulus limited bound 
qualitative statement turned quantitative statement clarke mutual information fisher information population coding barron rissanen 
give different presentation standard information theoretic point view relevant sensory coding point view parameter estimation model selection 
consider mutual information observable stimulus 
defined naturally context sensory coding random quantity generated characterizes environment 
mutual information defined blahut log 
measures statistical dependency input output considered mutual information multiplicative constant satisfying set fundamental requirements shannon weaver 
suppose exists unbiased efficient estimator 
mean variance 
amount information gained computation estimator entropy estimator pr ln pr entropy 
smaller entropy gaussian distribution variance 
implies ln 
processing increase information see blahut pp 
information conveyed equal conveyed estimator 
efficient estimator means ln 
nicolas brunel jean pierre nadal limit distribution estimator sharply peaked mean value particular implies entropy estimator identical entropy stimulus 
righthand side inequality equal plus terms order defined ln 
expression term entropy stimulus ln 
discrete distribution information gain resulting perfect knowledge 
second term equivocation due gaussian fluctuations estimator mean value 
limit estimator 
inequality see equation equation gives essence link mutual information fisher information 
results elementary application simple fundamental theorem information processing cramer rao bound 
cramer rao bound understood statement information content strictly larger 
way extract information 
inequality fact equality ln ln 
fact equality hold obvious 
cramer rao bound tell knowledge cumulants variance obtained 
estimator nongaussian distribution inequality strict give example section discuss case single output cell 
large limit exists efficient estimator maximum likelihood relevant probability distributions close gaussian distributions expect equation true limit 
case proved rissanen framework stochastic complexity suitable restrictive hypotheses 
mutual information fisher information population coding appendix show completely different techniques equation holds provided conditions satisfied 
derivatives ln respect stimulus order 

cumulants respect distribution ag bg order nn meaning second condition value cumulants decrease sufficiently rapidly particular true xi independent model holds general case xi correlated provided conditions hold show explicitly appendix example correlated xi 
extensions remarks 
multiparameter case model selection 
straightforward extend equation case dimensional stimulus derive equality equation fisher information matrix defined blahut jij ln quantity multidimensional case ln ln 
det second term equal entropy gaussian covariance matrix averaged 
large limit gets equality 
note formulas meaningful general context parameter estimation priori random variable 
bayesian framework clarke barron natural introduce prior distribution parameter space 
typically distribution chosen possible takes account prior knowledge constraint parameter space 
tells localized parameter space observation data framework mdl minimum description length rissanen natural prior maximizes mutual information realizing shannon capacity 
maximizing nicolas brunel jean pierre nadal respect finds optimal input distribution square root fisher information multidimensional case expression replaced det 
corresponds stimulus distribution neural system best adapted 
biased estimators 
preceding discussion easily extended case biased estimators estimators 
cramer rao bound case reads dm 
form bias variance compromise 
write inequality similar equation replacing dm limit estimator sharply peaked mean value log dm 
inserting inequality terms dm cancel 
bound equation valid known efficient estimator biased 
cramer rao bound understood bound discriminability psychophysics characterizing performance discrimination task see green swets 
discussed seung sompolinsky equality efficient estimator properly normalized respect bias dm case single neuron 
continuous neuron vanishing output noise 
consider case single neuron characterized scalar output deter mutual information fisher information population coding function input stimulus plus noise possibly stimulus dependent variance deterministic functions parameter giving scale variance noise random variable arbitrary necessarily gaussian distribution zero mean unit variance 
interested low noise limit 
difficult write fisher information mutual information limit vanishing gets sufficiently regular log entropy distribution fisher information finds log 
dz log log dz noise distribution normal distribution log integral equation equal 
easily check agreement general result see equation 
optimization transfer function 
maximization mutual information respect choice transfer function studied case stimulus independent additive noise nadal parga 
expression mutual information equation computed nadal parga 
new link fisher information 
mutual information maximized chosen equalization rule absolute value derivative equal activity uniformly distributed nicolas brunel jean pierre nadal min max values 
general case depends stimulus maximum reached defined satisfies equalization rule dx arbitrary parameters define min max values 
interesting case relevant analysis poisson neuron large time limit see subsection 
case maximum reached square root satisfies equalization rule 
fact mutual information related fisher information case single neuron vanishing noise means maximizing information transfer identical minimizing variance reconstruction error 
fact different qualitative lines reasoning known lead equalization rule related information transfer output uniform distribution see related reconstruction error 
slope transfer function large possible order minimize error constraint bounded leads compromise 
large error tolerated rare events 
shown formal link approaches link mutual fisher information 
poisson neuron 
related case single neuron emitting spikes poisson process section consider population neurons 
probability observing spikes interval stimulus perceived exp 
frequency assumed deterministic function tuning curve stimulus 
stimulus drawn randomly distribution frequency distribution 
mutual information fisher information population coding information processing ability model neuron studied great detail stein 
results interest follows 
short times mutual information stimulus cell activity order stein log mean frequency 
easily check duration fact long times information increases log large time limit gets stein log 
expression gets optimal tuning curve uniformly distributed extreme values min max 
analyze result view relationship fisher mutual information 
making change variable equation rewrite mutual information large times precisely defined equation fisher information associated single neuron 
result understood way 
limit large distribution number emitted spikes divided tends gaussian mean variance properties spiking neuron similar neuron having continuous activity gaussian random variable zero mean unit variance 
particular case equation 


nicolas brunel jean pierre nadal population direction selective spiking neurons fisher information 
illustrate main statement section context population coding 
consider large number neurons coding scalar stimulus angle 
equation tells compute mutual information calculate fisher information 
activities xi neurons independent pi xi fisher information written xi pi xi integration xi pi xi 
restrict case neurons firing poisson process rate response stimulus represent tuning curve neuron assumptions single maximum preferred stimulus tuning curve depends distance current stimulus preferred periodic function distance function 
locations preferred stimuli neurons independently identically distributed variables interval density 
model neurons fire poisson process information contained spike trains interval duration fully contained number spikes xi emitted neuron interval 
poisson process law pi xi xi xi 
exp 
equations easily calculate fisher information large replace sum average distribution preferred stimuli tn mutual information fisher information population coding triangular tuning curve corresponding minimal frequency min hz max hz receptive field half width degrees preferred angle degrees 
isotropic distribution recover result seung sompolinsky 
understand fisher information depends parameters tuning curve redefine min max min min max minimal maximal frequency width tuning curve decreasing function preferred stimulus stimuli far preferred stimulus terms parameters tn max min az particular case triangular tuning curve 
min max min 
shown 
considered detail 
tuning curve uniform distribution preferred stimuli fisher information simple form tn max min ln max 
min nicolas brunel jean pierre nadal noted seung sompolinsky fisher information diverges different extreme cases maximal frequency max goes infinity tuning width goes zero 
functions fisher information diverges value min max optimization fisher information respect parameters ill defined problem additional constraints 
note cases equation relating fisher information mutual information longer valid 
defined optimization problem optimization respect distribution preferred orientations 
considered section 
show finite size effects transform problem optimization fisher mutual information respect tuning width defined problem 
numerical estimates quantities inserting real data taube equation 
optimization distribution preferred orientations 
ask distribution preferred orientations optimizes mutual information obviously optimal depend distribution orientations 
optimizing equation respect subject normalization constraint gives ct defined 
condition satisfied 
optimal distribution preferred stimuli convolved quantity proportional fisher information matches distribution stimuli 
course particular case obtain ropt 
note equation valid unbounded stimulus values 
result equation specific optimization mutual information 
different results obtained say maximization average fisher information minimization average inverse 
fact optimum mean fisher information linear 
mutual information fisher information population coding left sd reconstruction error single spike function 
right mutual information spike stimulus function note minimizing sd reconstruction error case different maximizing mutual information 
finite size effects case single spike 
seen fisher information large limit diverges tuning width goes zero 
investigate property specific large limit study case finite number neurons short time interval single spike emitted population response stimulus 
situation clear optimal estimator stimulus ml estimate case preferred stimulus neuron emitted spike 
finite cramer rao bound general saturated calculate directly performance estimator 
simple exercise calculate standard deviation sd error estimate triangular tuning curve equation sd error min max min min max min minimum 
show sd reconstruction error single spike function max min 
minimum degrees sd error degrees 
nicolas brunel jean pierre nadal mutual information hand max max log max min min log min max min min log min max min min maximum positive width maximizes different width minimizes sd reconstruction error shown 
case general nongaussian tuning curves 
case half width maximizing mutual information degrees 
note wide range spike brings bits information stimulus 
finite optimal stems constraint minimizing error small number spikes emitted neuronal array 
implies largest receptive fields useful short times rough estimate possible smaller receptive fields useful larger times accurate estimate obtained 
application analysis empirical data 
section experimental data taube 
show equation estimate fisher mutual information conveyed large populations neurons angular stimulus case head direction rat 
taube 
shown rats tuning curves fitted triangular tuning curves distribution preferred orientations consistent uniform distribution 
determined distribution parameters tuning curve max signal noise ratio snr max min recorded neurons 
data indicate parameters important variability neuron neuron 
equation case inhomogeneities replaced tn pr max max ln 
pr max joint probability parameters max 
mutual information fisher information population coding left minimal reconstruction error cramer rao bound full curve dashed curve neurons data taube 
function time 
right mutual information full curve dashed curve data equation 
global constraints may expect neuron contribute way information max ln constant 
imply width increases max 
taube 
shows trend higher firing rate cells wider directional firing ranges 
insert distributions parameters measured taube 
equation estimate minimal reconstruction error done head direction output neurons interval duration shown left part 
assume number neurons large mutual information conveyed population estimated equation 
shown right part 
case neurons error small degree ms interval small proportion selective neurons emitted spike 
note degree order magnitude error typically perceptual discrimination tasks see pouget thorpe 
interval activity population neurons carries bits stimulus 
doubling number neurons duration interval divides minimal reconstruction error increases mutual information bit 
nicolas brunel jean pierre nadal article exhibited link fisher information mutual information context neural coding 
link derived context bayesian parameter estimation clarke barron context stochastic complexity rissanen 
shown result rissanen applies population coding number neurons large compared dimension stimulus 
derivation link uses completely different techniques 
result mutual information neural activities stimulus equal stimulus ideal gaussian unbiased estimator variance equal inverse fisher information 
result true independent observations correlated activities see rissanen appendix 
important context neural coding noise different cells cases correlated due common inputs lateral connections 
result implies limit large number neurons maximization mutual information leads optimal performance estimation stimulus 
considered problem optimizing tuning curves maximizing mutual information parameters defining tuning curves optimization choice preferred orientations widths tuning curves 
simple model considered optimal value width zero seung sompolinsky 
shown finite size effects necessarily lead nonzero optimal value independent decoding scheme 
discussed detail case dimensional stimulus angle 
similar relationship mutual information fisher information matrix holds dimensionality stimulus long remains small compared number neurons 
straightforward consider general case optimization tuning curves 
zhang ginzburg mcnaughton sejnowski computed fisher information matrix dimensional stimuli 
results imply optimal tuning curve parameters depend strongly dimensionality stimulus 
briefly discussed cases finite number neurons short time limit 
case maximization mutual information leads general different results minimization variance reconstruction error networks number input output continuous neurons ruderman 
currently working limits aspects remain clarified 
addressed problem decoding 
asymptotic limit maximum likelihood ml decoding optimal 
pouget zhang showed simple recurrent network perform computation ml estimate 
suggests optimal performance mutual information fisher information population coding point view information content decoding reached simple cortical architecture 
appendix goal derive equation compute mutual information random variables working large limit 
recall seen set observations related measurement unknown parameter set responses neurons stimulus 
mutual information defined ln 
equation denotes integration 
define ln 
hypothesis 
derivatives respect stimulus order large limit 

cumulants order xg limit 
yg order nn large properties verified factorized models see equations cases xi correlated variables show appendix 
large limit allows saddle point method bhattacharya rao parisi computation integrals particular computation fact appear sharply peaked probable value maximum likelihood ml estimator 
standard cumulant expansions integration equivocation part eventually lead announced result equation 
distribution written exp ng 
nicolas brunel jean pierre nadal large integral dominated maxima integrand 
defined solutions satisfy 
denoted resp 
resp 
second partial derivative respect 
assume single global maximum 
taylor expansion 
standard saddle point techniques find qm qm exp gm 
note ml estimator 
mutual information integration 
start expression mutual information ln xq ln term entropy input distribu tion 
second term written xq ln ln 
mutual information fisher information population coding expression part entropy replace qm equation leading ln ln term equation written exp ln 
exp exp computed saddle point method exp exp ln qm ln putting mutual information written ln exp ln 
interesting compare equations 
equation term entropy ln stimulus distribution second term equivocation equation average logarithm variance estimator 
mutual information integration difficulty perform equation trace apply saddle point method directly number integration variables precisely equal number exponential large 
difficulty circumvented small compared auxiliary integration variables way integration nicolas brunel jean pierre nadal xi done exactly 
fact large perform integration variables leading order relation order deal valid arbitrary function integral representation delta function dy exp similarly order deal introduce conjugate variables 
function write exp putting get ln ln exp exp iy recall exp ng 
cumulant expansion exp exp 
cumulant expansion valid cumulants order law exp ng decrease sufficiently rapidly sufficient condition assumption cumulants order 
order nn 
mutual information fisher information population coding identities obtained deriving twice respect gets note fisher information equal nj order assumption 
terms ln exp task integrate remaining auxiliary variables fact deduced schwartz inequality integrations simple gaussian integrations leading ln exp integration gaussian weight centered width going zero goes infinity lim exp 
nicolas brunel jean pierre nadal fact fisher information nj obtain ln ln announced result equation 
conditions see validity calculation satisfied xi independent equations satisfied correlated 
discuss cases 
conditional independence activities 
case independent neurons model equation easily check cumulant expansion order gives terms order case gi xi ai ai gi xi cumulant expansion reads exp exp log exp ai exp ai equation holds iy gi xi 
ai 
gi gi gi gi gi gi 
correlated neurons 
conditions cumulants imply xi independent qualitative meaning convey order independent observations 
see mutual information fisher information population coding give example correlated activities conditions satisfied 
consider simple model 
xi expressed terms independent random variables xi mi 
independent invertible matrix statistically independent variables arbitrary factorized case recovered diagonal 
case gaussian orthogonal equation principal component decomposition show case invertible arbitrary satisfies conditions 
obvious result see equation holds change variables recovers case independent activities 
apply equation 
det independent fisher information associated equal associated equation holds 
second check directly conditions hold 
model ln det ln xi cumulants respect pdf equal cumulants respect factorized pdf holds 
acknowledgments alexandre pouget sophie interesting discussion sid wiener drawing data taube 
attention 
grateful alexandre pouget peter latham pointing mistake earlier version article referees comments helped improve article significantly 
atick 

information theory provide ecological theory sensory processing 
network 
barlow mitchison 

finding minimum entropy codes 
neural comp 
bhattacharya rao 

normal approximation asymptotic expansions 
new york wiley 
nicolas brunel jean pierre nadal bialek rieke de van 

reading neural code 
science 
blahut 

principles practice information theory 
reading ma addison wesley 
clarke barron 
information theoretic asymptotics bayes methods 
ieee trans 
information theory 
cover thomas 

information theory 
new york wiley 
georgopoulos massey 

relations direction dimensional arm movements cell discharge primate motor cortex 
neurosci 
green swets 

signal detection theory psychophysics 
new york wiley 


simple coding procedure enhances neuron information capacity 

linsker 

self organization perceptual network 
computer 
maunsell van essen 

functional properties neurons middle temporal visual area macaque monkey 
selectivity stimulus direction speed orientation 
neurophysiol 
nadal parga 

nonlinear neurons low noise limit factorial code maximizes information transfer 
network 
parisi 

statistical field theory reading ma addison wesley 
pouget thorpe 

models orientation identification 
connection science 
pouget zhang 

statistically efficient estimations cortical lateral connections 
jordan petsche eds advances neural information processing systems pp 

cambridge ma mit press 
rissanen 

fisher information stochastic complexity 
ieee trans 
information theory 
ruderman 

designing receptive fields highest fidelity 
network 
seung sompolinsky 

simple models reading neural population codes 
usa 
shannon weaver 

mathematical theory communication 
urbana il university illinois press 


parameter extraction population codes critical 
neural comp 
koch 

highly irregular firing cortical cells inconsistent temporal integration random 
neurosci 
stein 

information capacity nerve cells frequency code 
biophys 

taube 

head direction cells recorded anterior thalamic nuclei freely moving rats 
neurosci 
taube muller 

head direction cells recorded freely moving rats 
description quantitative analysis 
neurosci 
mutual information fisher information population coding van hateren 

theoretical predictions spatiotemporal receptive fields fly experimental validation 
comp 
physiology 
zhang ginzburg mcnaughton sejnowski 

interpreting neuronal population activity reconstruction unified framework application hippocampal place cells neurophysiol 
received november accepted february 
