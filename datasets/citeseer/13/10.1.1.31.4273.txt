appear tesauro touretzky leen eds advances neural information processing systems mit press cambridge ma 
growing neural gas network learns topologies bernd fritzke institut fur neuroinformatik ruhr universitat bochum bochum germany incremental network model introduced able learn important topological relations set input vectors means simple hebb learning rule 
contrast previous approaches neural gas method martinetz schulten model parameters change time able continue learning adding units connections performance criterion met 
applications model include vector quantization clustering interpolation 
unsupervised learning settings input data available information desired output 
goal learning situation 
possible objective dimensionality reduction finding low dimensional subspace input vector space containing input data 
linear subspaces property computed directly principal component analysis iteratively number network models sanger oja 
kohonen feature map kohonen growing cell structures fritzke allow projection non linear discretely sampled subspaces dimensionality chosen priori 
depending relation inherent data dimensionality dimensionality target space information topological arrangement input data may lost process 
astonishing reversible mapping high dimensional data lower dimensional spaces structures exist general 
asking structures look allow reversible mappings directly leads possible objective unsupervised learning described topology learning high dimensional data distribution find topological structure closely reflects topology data distribution 
elegant method construct structures competitive hebbian learning chl martinetz 
chl requires vector quantization method 
martinetz schulten propose neural gas ng method purpose martinetz schulten 
briefly introduce discuss approach martinetz schulten 
propose new network model chl 
contrast mentioned chl ng combination model incremental constant parameters 
leads number advantages previous approach 
competitive hebbian learning neural gas chl martinetz assumes number centers successively inserts topological connections evaluating input signals drawn data distribution 
principle method input signal connect closest centers measured euclidean distance edge 
resulting graph subgraph delaunay triangulation fig 
corresponding set centers 
subgraph fig 
called induced delaunay triangulation limited areas input space 
induced delaunay triangulation shown optimally preserve topology general sense martinetz 
centers lying input data submanifold vicinity develop edges 
useless purpose topology learning called dead units 
centers placed regions differs zero 
done vector quantization vq procedure 
martinetz schulten proposed particular kind vq method mentioned ng method martinetz schulten 
main principle ng input signal adapt nearest centers decreasing large initial small final value 
large initial value causes adaptation movement input signal large fraction centers 
adaptation range decreased nearest center input signal adapted 
adaptation strength underlies similar decay schedule 
realize parameter decay define total number adaptation steps ng method advance 
delaunay triangulation induced delaunay triangulation ways defining closeness set points 
delaunay triangulation thick lines connects points having neighboring voronoi polygons thin lines 
basically reduces points having small euclidean distance set points 
induced delaunay triangulation thick lines obtained masking original delaunay triangulation data distribution shaded 
centers connected common border voronoi polygons lies partially region 
closely adapted martinetz schulten data distribution run ng algorithm distribute certain number centers chl generate topology 
possible apply techniques concurrently martinetz schulten 
case method removing obsolete edges required motion centers may edges invalid generated earlier 
martinetz schulten edge aging scheme purpose 
note chl algorithm influence outcome ng method way adaptations ng distance input space network topology 
hand ng influence topology generated chl moves centers 
combination ng chl described effective method topology learning 
problem practical applications may determine priori suitable number centers 
depending complexity data distribution wants model different numbers centers may appropriate 
nature ng algorithm requires decision advance result satisfying new simulations performed scratch 
propose method overcomes problem offers number advantages flexible scheme center insertion 
growing neural gas algorithm consider networks consisting ffl set units nodes 
unit associated vector vectors regarded positions input space corresponding units 
ffl set connections edges pairs units 
connections weighted 
sole purpose definition topological structure 
possibly infinite number dimensional input signals obeying unknown probability density function 
main idea method successively add new units initially small network evaluating local statistical measures gathered previous adaptation steps 
approach growing cell structures model fritzke topology fixed dimensionality 
approach described network topology generated incrementally chl dimensionality depends input data may vary locally 
complete algorithm model call growing neural gas 
start units random positions 
generate input signal 

find nearest unit second nearest unit 
increment age edges emanating 
add squared distance input signal nearest unit input space local counter variable kw gamma 
move direct topological neighbors fractions ffl ffl respectively total distance deltaw ffl gamma deltaw ffl gamma wn direct neighbors 
connected edge set age edge zero 
edge exist create 

remove edges age larger amax results points having emanating edges remove 
term neighbors denotes units topological neighbors graph opposed units small euclidean distance input space 
step hebbian spirit correlated activity decide insertions 

number input signals generated far integer multiple parameter insert new unit follows ffl determine unit maximum accumulated error 
ffl insert new unit halfway neighbor largest error variable ffl insert edges connecting new unit units remove original edge ffl decrease error variables multiplying constant ff 
initialize error variable new value error variable 
decrease error variables multiplying constant 
stopping criterion net size performance measure fulfilled go step 
described method 
adaptation steps input signals lead general movement units areas input space signals come 
insertion edges nearest second nearest unit respect input signal generates single connection induced delaunay triangulation see fig 
respect current position units 
removal edges necessary get rid edges longer part induced delaunay triangulation points moved 
achieved local edge aging nearest unit combined age re setting edges exist nearest second nearest units 
insertion removal edges model tries construct track induced delaunay triangulation slowly moving target due adaptation vectors 
accumulation squared distances adaptation helps identify units lying areas input space mapping signals units causes error 
reduce error new units inserted regions 
simulation results give simulation results demonstrate general behavior model 
probability distribution fig 
proposed martinetz schulten demonstrate non incremental neural gas model 
seen model quickly learns important topological relations complicated distribution forming structures different dimensionalities 
second example fig 
illustrates differences proposed model original ng network 
final topology similar models intermediate stages quite different 
models able identify clusters distribution 
growing neural gas model growing neural gas network adapts signal distribution different dimensionalities different areas input space 
shown initial network consisting randomly placed units networks input signals applied 
network shown necessarily final growth process principle continued indefinitely 
parameters simulation ffl ffl ff amax 
continue grow discover smaller clusters particular example 
discussion growing neural gas network able explicit important topological relations distribution input signals 
advantage ng method martinetz schulten incremental character model eliminates need pre specify network size 
growth process continued user defined performance criterion network size met 
parameters constant time contrast models heavily rely decaying parameters ng method kohonen feature map 
noted topology generated chl optional feature neural gas competitive hebbian learning growing neural gas uses competitive hebbian learning ng chl network martinetz schulten author growing neural gas model adapt clustered probability distribution 
shown respective initial states top row number intermediate stages 
number units ng model final number units growing neural gas model 
bottom row shows distribution centers adaptation steps edges previous row shown 
center distribution similar models intermediate stages differ significantly 
method ng model essential component direct completely local adaptation insertion centers 
probably proper initialization new units interpolation existing ones possible constant parameters local adaptations 
possible applications model clustering shown vector quantization 
network perform particularly situations neighborhood information edges implement interpolation schemes neighboring units 
error occuring early phases determined insert new units generate topological look table different density different dimensionality particular areas input data space 
promising direction research combination supervised learning 
done earlier growing cell structures fritzke growing neural gas described fritzke 
crucial property kind application possibility choose arbitrary insertion criterion 
feature original growing neural gas 
results new supervised network model incremental radial basis function network promising investigating currently 
fritzke 

fast learning incremental rbf networks 
neural processing letters 
fritzke 

growing cell structures self organizing network unsupervised supervised learning 
neural networks 
fritzke 

supervised learning growing cell structures 
cowan tesauro alspector editors advances neural information processing systems pages 
morgan kaufmann publishers san mateo ca 
kohonen 

self organized formation topologically correct feature maps 
biological cybernetics 
martinetz 

competitive hebbian learning rule forms perfectly topology preserving maps 
icann international conference artificial neural networks pages amsterdam 
springer 
martinetz schulten 

neural gas network learns topologies 
kohonen simula kangas editors artificial neural networks pages 
north holland amsterdam 
martinetz schulten 

topology representing networks 
neural networks 
oja 

simplified neuron model principal component analyzer 
journal mathematical biology 
sanger 

optimality principle unsupervised learning 
touretzky editor advances neural information processing systems pages 
morgan kaufmann san mateo ca 
