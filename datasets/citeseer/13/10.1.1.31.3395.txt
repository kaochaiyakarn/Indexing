trec spoken document retrieval track success story john garofolo cedric ellen voorhees national institute standards technology bureau drive mail gaithersburg md usa john garofolo cedric ellen voorhees nist gov describes nist text retrieval conference trec years designing implementing evaluations spoken document retrieval sdr technology broadcast news domain 
sdr involves search retrieval excerpts spoken audio recordings combination automatic speech recognition information retrieval technologies 
trec sdr track provided infrastructure development evaluation sdr technology common forum exchange knowledge speech recognition information retrieval research communities 
sdr track declared success provided objective proof technology successfully applied realistic audio collections combination existing technologies objectively evaluated 
design implementation sdr evaluations results summarized 
plans trec sdr track thoughts track evolve discussed 
trec national institute standards technology sponsors annual text retrieval conference trec designed encourage research text retrieval realistic applications providing large test collections uniform scoring procedures forum organizations interested comparing results voorhees 
conference tip iceberg 
trec primarily evaluation task driven research program 
trec research task common evaluation just prior conference 
results evaluations published nist trec workshop notebook conference proceedings 
sites participating evaluations meet trec discuss approaches evaluation results plan trec research tasks 
years conference contained main task set additional tasks called tracks 
main task investigates performance systems search static set documents new questions 
task similar researcher library collection known questions asked known 
tracks focus research problems related main task retrieving documents written variety languages questions single language cross language retrieval retrieving documents large gb document collections retrieval performance humans loop interactive retrieval 
taken tracks represent majority research performed trecs keep trec research program encouraging research new areas information retrieval 
trecs trec trec included spoken document retrieval sdr track 
spoken document retrieval motivation developing technology provide access non textual information fairly obvious 
large multi media collections assembled 
explosive growth internet enabled access wealth textual information 
access audio information specifically spoken audio archives limited audio manually indexed transcribed 
true human generated transcripts available radio television broadcasts greater body spoken audio recordings legacy radio television broadcasts recordings meetings conferences classes seminars remains virtually inaccessible 
trec spoken document retrieval sdr track created address problems 
sdr provides content retrieval excerpts archives recordings speech 
chosen area interest trec potential navigating large multi media collections near believed component speech recognition information retrieval technologies usable sdr domains 
sdr technology opens possibility access large stores previously audio archives paves way development access technologies multimedia collections containing audio video image data formats 
voorhees practice sdr accomplished combination automatic speech recognition information retrieval technologies 
speech recognizer applied audio stream generates time marked transcription speech 
transcription may phone word lattice probability network best list multiple individual transcriptions typically best transcript probable transcription determined recognizer 
transcript indexed searched retrieval system 
result returned query list temporal pointers audio stream ordered decreasing similarity content speech pointed query garofolo 
typical sdr process shown 
recognized transcripts broadcast news speech recognition engine broadcast news audio recording corpus ir search engine ranked document list topic query temporal index typical sdr process trec sdr background evaluation retrieval output optical character recognizer ocr run confusion track trec explore effect ocr errors retrieval kantor 
track showed possible implement evaluate retrieval corrupted text 
implementing track nist members trec community thought interesting implement similar experiment automatic speech recognition asr 
trec workshop researchers nist trec community led karen jones university cambridge met discuss possibility applying information retrieval techniques output speech recognizers 
nist natural language processing information retrieval group supporting evaluation retrieval technologies auspices trec nist spoken natural language processing group working darpa automatic speech recognition asr community evaluating speech recognition technology radio television broadcast news 
broadcast news evaluation task accelerated progress recognition real data technology producing transcripts reasonable accuracy investigation downstream application uses sdr 
darpa asr community access hour corpus broadcast news recordings collected linguistic data consortium ldc asr training graff time provided data collection sufficiently large sdr 
nist spoken natural language processing group natural language processing information retrieval group joined forces develop plan creation research track trec investigate new hybrid technology 
primary goal track bring speech information retrieval communities promote development sdr technologies track progress development 
track foster research development large scale near real time continuous speech recognition technology retrieval technology robust face input errors 
importantly track provide venue investigating hybrid systems may effective simple stove pipe combinations 
track encourage cooperation synergy groups complementary speech recognition information retrieval expertise 
trec sdr known item retrieval evaluation design year sdr track truly getting speech ir communities exploring feasibility implementing evaluating sdr technology 
trec sdr evaluation designed easy entry straight forward implementation 
common evaluation sdr technology evaluation considered experimental 
main trec task focussing ad hoc retrieval multiple relevant documents single topics decided sdr track employ known item retrieval task simulates user seeking particular half remembered document collection 
goal known item retrieval task generate single correct document topic set relevant topics adhoc task 
approach simplified topic selection process eliminated need expensive relevance assessments 
thought time sdr ad hoc retrieval task produce results poor evaluate discourage participation voorhees 
early decided evaluation measure effectiveness sdr systems individual asr ir components 
evaluation included complementary runs set topics different sets transcriptions broadcast news recordings test collection retrieval perfect human transcribed transcriptions baseline retrieval ibm asr generated transcriptions speech retrieval recordings requiring asr ir components run permitted evaluation effectiveness retrieval algorithms spoken language collection removing asr factor 
likewise baseline condition permitted comparison effectiveness retrieval algorithms asr produced transcripts 
speech run permitted evaluation full sdr performance 
transcripts contributed ldc formatted hub style utf format files broadcast garofolo 
baseline recognizer transcripts contributed ibm 
baseline shared recognized transcripts human transcripts perfect 
hub training quality transcripts generally believed contain wer 
stored sgml formatted files included story boundaries record word including start times 
broadcast recordings digitally sampled bit samples linear pcm encoded khz 
sampling rate single monophonic channel stored nist files 
approach served purposes allowed different asr ir sites join create pipelined systems components mixed matched separately evaluated 
permitted retrieval sites access asr systems participate limited way implementing baseline retrieval tasks 
participation level sites implementing recognition retrieval deemed full sdr participation level sites implementing retrieval deemed quasi sdr 
artificial simplify implementation evaluation sites human annotated story boundaries story id test conditions 
permitted simplified document approach implementation evaluation 
nist developed test topics half designed nist group exercise classic ir challenges 
half designed snlp group exercise challenges speech recognition part problem 
half speech topics designed target stories easy speech scripted speech recorded studio conditions native speakers noise music background 
half speech topics designed target stories difficult recognize speech speech speech telephone channels non native speakers speech noise music background 
variety topics permit examine detail effect speech recognition accuracy retrieval performance 
important differences broadcast news stories document ir collections 
broadcast news stories extremely short regard number words 
trec sdr collection average number words story stories containing words 
full text ir collections tend documents words usually order magnitude larger 
stories sdr collection annotated filler non topical transitional material 
filtered collection remove commercials sports summaries weather reports stories 
decided leave filler segments test collection keep large possible 
final filtered broadcast news collection stories 
collection represented sizable corpus speech recognition previous test corpora hours small retrieval testing orders magnitude smaller current ir test collections 
test specifications documentation trec sdr track archived www nist gov speech sdr txt 
test results test participants months complete evaluation 
thirteen sites site combinations participated sdr track 
performed full sdr carnegie mellon university cmu asr eth zurich glasgow university sheffield university asr ibm royal melbourne institute technology sheffield university university massachusetts dragon systems asr 
remaining sites performed quasi sdr city university london dublin city university national security agency university maryland 
see trec sdr participant papers goal track evaluate retrieval performance formal evaluation recognition performance 
full sdr sites encouraged submit best transcripts nist examine relationship recognition performance retrieval accuracy 
word error rate ibm baseline recognizer 
mean story word error rate bit lower 
mean story word error rate measured recognizers fell 
error rates substantially higher obtained hub asr tests 
difference primarily due factors transcriptions scoring sdr asr performance created asr training material put rigorous verification nist employs hub evaluation test data 
likewise generic orthographic mapping file 
orthographic mapping file maps alternate representations certain words contractions common format prior scoring 
custom version file created hub test set minimizes number alternative representation confusion errors 
order process hour collection sites chose faster accurate recognizers hub tests 
initially believed retrieval results sdr track quite poor 
devised scoring metrics mean rank mean reciprocal rank gave systems partial credit finding target stories lower ranks voorhees 
happily surprised find systems performed quite 
fact chose percent retrieved rank primary metric garofolo 
retrieval rates high transcript condition sites showed small degradation retrieval recognizers 
generally higher degradation retrieval baseline recognizer transcripts due high error rate high number vocabulary oov words 
results evaluation retrieval conditions shown 
trec sdr retrieval rate rank systems modes best run percent retrieved rank best performance test conditions achieved university massachusetts system dragon systems recognition full sdr obtained retrieval rate condition baseline recognizer condition speech condition allan 
fact umass system missed topic speech condition condition 
analysis errors systems particular topics showed general easy recognize topic set yielded best performance evaluation conditions difficult recognize topic set yielded substantially degraded performance 
difficult query topic subset yielded greater performance degradation 
interesting note systems difficulty retrieving stories difficult recognize topic subset transcriptions indication factors transcribed speech recognition errors influence retrieval performance 
far variance topic effect sweeping 
trec sdr percent retrieval rank averaged systems topic subset examine effect recognition error rate retrieval examined performance baseline recognizer results 
topic sorted mean rank retrieval systems target story word error rate story 
sorting appears show increasing trend poorer retrieval performance recognition errors increase 
trec baseline condition mean retrieval rank sorted baseline recognizer story word error rate interestingly plot retrieval transcripts shows similar trend indicating stories difficult recognize may difficult retrieve recognized perfectly 
hypothesis complexity language difficult stories greater easy recognize stories 
trec condition mean retrieval rank sorted baseline recognizer story word error rate statistical analysis variance showed little data eliminate large proportion confounding unexplained factors garofolo 
evaluation provide multiple recognizer transcript sets retrieval sites run help clarify relationship recognition retrieval performance 
sdr evaluation showed successfully implement evaluation sdr technology existing component technologies worked known item task small audio collection 
test participants agreed test collection enlarged order magnitude real performance issues surface 
agreed known item task provided insufficient evaluation granularity 
evaluation retrieval performance played significant role sdr performance recognition performance 
difficult limited evaluation paradigm collection 
trec sdr ad hoc retrieval evaluation design trec set address inadequacies trec sdr track 
access large audio collection true retrieval evaluation able double size sdr collection additional broadcast news corpus collected ldc hub asr training 
importantly decided give known item retrieval paradigm implement classic trec ad hoc retrieval task 
ad hoc retrieval test systems posed topics attempt return list documents ranked decreasing similarity topic 
documents evaluated relevance team human assessors 
trec keep evaluation tractable nist pools top documents output evaluated systems judges documents 
systems get evaluated documents documents judged 
exhaustive approach assumes different systems relevant documents included pool 
traditional trec ad hoc track provided forms information topic title short query form usually single sentence phrase descriptive narrative giving rules judging relevance 
limited size sdr collection decided simplify sdr topics single short form 
required runs fully automatic 
trec sdr test collection contained hours audio usable stories filtering similar mean median story length compared trec collection 
trec participants human annotated story boundaries story ids 
removed story boundary detection technical challenge permitted nist standard trec document trec eval scoring software evaluate results test 
team nist trec assessors created test topics averaging words length collection 
test topics created find reports fatal air crashes 
topic economic developments occurred hong kong incorporation chinese people republic 
topic accurately examine effect recognition performance retrieval decided add new optional evaluation condition cross recognizer retrieval retrieval systems run sites recognized transcripts 
permit tightly control recognizer effect analyses provide information regarding relationship recognizer performance retrieval performance 
encouraged sites running best recognition submit recognizer transcripts nist sharing participants 
permit sites explore effect different recognizers permitted full sdr site run retrieval primary secondary recognizer 
baseline recognizer nist created local instantiation carnegie mellon university sphinx iii recognizer 
sphinx iii ran nearly times real time nist unix workstations nist realized take nearly years computation complete single recognition pass hour collection 
nist learned inexpensive clusters pc linux systems nasa beowulf project beowulf set create cluster recognition system 
final system incorporated scheduling server computational nodes 
cluster enormous computational power enrich spectrum recognizers evaluation nist chose create baseline recognizer transcript sets 
set created optimal version sphinx recognizer benchmarked word error rate hub test set pallett sdr test collection 
enabled time benchmark difference performance recognizer running hub sdr asr tests 
second set created lowered pruning thresholds benchmarked word error rate sdr collection 
trec full sdr sites required implement baseline speech input retrieval conditions quasi sdr sites required implement baseline retrieval conditions 
test specifications documentation trec sdr track archived www nist gov speech sdr sdr htm 
test results trec sdr participants months implement recognition portion task 
month implement required retrieval tasks additional month implement optional cross recognizer retrieval task 
sites restricted hardware number processors apply implementing evaluation 
eleven sites site combinations participated second sdr track 
performed full sdr att carnegie mellon university group cmu university cambridge dera dera royal melbourne institute technology mds sheffield university shef netherlands organization tpd tu delft tno university massachusetts dragon systems asr umass 
remaining sites performed quasi sdr carnegie mellon university group cmu national security agency nsa university maryland umd 
see trec sdr participant papers addition nist baseline recognizers best transcripts additional recognizers submitted nist scoring sharing cross recognizer retrieval condition 
recognizers covered wide range error rates provided spectrum material cross recognizer retrieval condition 
shows word error rate mean story word error rate submitted recognizer transcripts 
sdr traditional recognition metrics dragon att nist shef nist word error rate word error rate wer trec sdr test set word error rate wer mean story word error rate submitted recognized transcripts cross system significance best recognition results obtained university cambridge htk recognition system test set word error rate mean story word error rate johnson 
circled mean story word error rate points considered statistically different performance 
sdr asr error rates significantly higher hub general error rates significantly improved previous year faster speeds required recognize larger test collection 
retrieval run required produce rank ordered list id top stories topic 
top ids lists merged create pools human assessment 
trec assessors read transcriptions topic pool stories evaluate stories relevance 
retrieval runs scored standard trec eval text retrieval scoring software 
trec ad hoc tasks primary retrieval metric sdr evaluation mean average precision map mean average precision scores topics run 
average precision equivalent area underneath recall precision graph voorhees 
trec sdr track contained retrieval conditions retrieval human closed quality transcripts baseline retrieval nist cmu sphinx asr transcripts baseline retrieval nist cmu sphinx sub optimal asr transcripts speech retrieval participant recognizer speech retrieval participant secondary recognizer cross recognizer cr retrieval participants recognizer transcripts results required test conditions baseline baseline speech speech shown 
full sdr participants required implement retrieval conditions 
quasi sdr participants required implement retrieval conditions 
retrieval condition att cmu cmu cu htk dera mds nsa shef tno umass umd mean average precision retrieval condition trec sdr mean average precision map required retrieval conditions retrieval conditions university massachusetts system allan achieved best mean average precision 
systems performed surprisingly conditions 
surprising run best recognizer run evaluation outperformed run 
attributed excellent performance new approach implemented document expansion contemporaneous newswire texts employed runs run singhal 
interesting condition trec sdr cross recognizer retrieval cr condition participating systems ran retrieval submitted recognizer produced transcript sets addition human recognizer transcript sets 
experiment gave recognition retrieval data points examine effect recognition performance retrieval performance 
sites university cambridge dera royal melbourne institute technology mds sheffield university participated cr experiment 
mean story word error rate asr metric mean average precision map retrieval metric plotted recognition retrieval performance curve systems 
retrieval vs recognition mean story wer dera mds shef mean corr coef ref cu htk att dragon shef dera dera trec sdr cross recognizer results mean average precision vs mean story word error rate shows gentle fairly linear drop map recognition transcripts increasing 
calculated correlation coefficient metrics determine correlated retrieval performance 
average correlation coefficient systems significant correlation 
explored word error rate metrics see find better predictor retrieval performance 
hypothesis metric useful developing asr systems retrieval purposes 
explored metrics ir methods filter unimportant words retrieval word filtered word error rate stemmed word filtered word error rate garofolo 
surprisingly metrics turned slightly correlated mean average precision word error rate 
effective approaches ir customized asr scoring trec sdr data explored reported johnson singhal 
implementing trec sdr track administering evaluation named entity ne tagging broadcast news 
ne evaluation involved identification people locations organizations broadcast news asr transcripts przybocki 
fortune gte bbn hand annotated data sdr evaluation named entity tags miller 
hypothesis named entities identify key content carrying words spoken documents focussed asr metric words obtain better predictor retrieval performance measuring error rate words 
re scored asr systems named entity word error rate plotted asr metric mean average precision done mean story word error rate 
retrieval vs recognition named entity mean story wer ne mean average precision dera mds shef mean corr coef ref cu htk dragon dera dera shef att trec sdr cross recognizer results mean average precision vs named entity mean story word error rate plot showed nearly linear relationship named entity asr performance retrieval performance mean correlation coefficient systems 
significantly plot accurately positioned problematic nist recognizer systematically increased errors longer probably content carrying words 
systems named entity metric showed higher correlation mean average precision word error rate garofolo 
things equal finding tells asr system recognizes named entities accurately provide best input retrieval 
trec learned successfully implement evaluate ad hoc sdr task 
new cross recognizer condition able investigate relationship recognition performance retrieval performance 
near linear relationship word error rate mean average precision recognition content word word error metrics named entity word error rate provided better predictors retrieval performance word error rate 
twice size predecessor number stories hour collection far small usefulness technology 
evaluating systems artificial human annotated story boundaries 
trec sdr large audio collection evaluation design linguistic data consortium began collecting large radio television corpus darpa topic detection tracking tdt program 
contrast trec tracks tdt program concerned detecting processing information continuous stream occurs online manner fiscus 
tdt corpus collected support tdt program contains news recordings abc cnn public radio international voice america 
exception broadcasts began early march sources sampled evenly month period january june 
corpus contains contemporaneous newswire corpus containing articles new york times associated press 
time sampled broadcast news sources parallel text corpus hour tdt corpus perfectly suited sdr track 
unfortunately high quality human transcriptions closed quality transcriptions 
transcription quality prevented reasonably evaluating recognition performance entire collection selected hour randomly selected story subset collection detailed transcription ldc 
high quality transcripts permit perform sampled evaluation asr performance 
permitted evaluate error rate closed quality transcriptions roughly wer television closed sources wer radio sources quickly transcribed commercial transcription services fisher 
error rates significant television closed error rates approach error rates state ofthe art broadcast news recognizers 
sdr participants hub participants intended hub asr systems contained training data january overlapped month tdt corpus 
eliminate possibility training test cross contamination eliminated january data sdr collection 
final collection contained hours audio collected february trec filtering track works online retrieval task similar tdt 
june 
collection contained stories order magnitude larger hour trec sdr collection 
believe deployed sdr systems operate archive search modality 
efficient means implement system employ online recognition recognition performed continuous basis audio recorded retrospective retrieval entire collection queried formed 
contrast tdt type system performs online retrieval audio recognized 
modalities recognition adaptation techniques adjust changes collection language time 
traditional hub style broadcast news recognizers employed static pre trained language models 
recognizer real time longitudinal application language news fixed language model recognizer diverge resulting increasing error rates time 
recognizers incapable recognizing new words words important retrieval 
conversely computational expense performing recognition retrospective recognition time retrieval impossible realistically large collections 
real sdr application audio recorded months years recognizer re trained periodically accommodate changes language new words 
support modality defined online recognition mode supported evolving rolling language models recognition systems periodically retrained test epoch 
full sdr sites permitted traditional pre trained recognition system continuously adaptive recognition system contemporaneous newswire text days prior day recognized adaptation 
sites free choose retraining period strategy liked long didn look ahead time performed recognition garofolo 
realizing cmu sphinx recognizer far slow recognize trec collection nist set find faster baseline recognizer 
nist added spoke hub broadcast news asr evaluation systems run times real time fast single processor 
spoke dubbed xrt encouraged development fast broadcast news recognizers suffered little degradation recognition accuracy xrt cousins pallett 
gte bbn offered nist linux instantiation fast byblos rough ready recognizer operated xrt baseline sdr tdt tests kubala 
bbn gave nist basic language modeling toolkit 
computational power nist recognition cluster speed bbn recognizer nist set create complementary baseline recognizer transcript sets 
set traditional hub fixed language model 
recognizer benchmarked wer hub test set wer hub test set wer sdr hour subset 
nist created adaptive rolling language model version sdr contemporaneous newswire texts periodic look back language model training 
details regarding recognizer provided 

system benchmarked wer hour sdr subset 
difference performance insignificant 
nist statistical tests showed significantly different recognizer 
small decrease word error significant decrease oov rate recognizer 
oov rate percentage test set words included recognizer vocabulary correctly recognized 
oov rate fixed recognizer 
oov rate adaptive recognizer relative improvement 
addition baseline speech cross recognizer retrieval conditions trec optional story boundaries unknown su condition added trec 
condition difference story density explained large proportion short cnn stories trec collection 
average story length trec collection words 
permitted sites explore sdr operate broadcasts knowledge human annotated topical boundaries 
condition accurately represented real sdr application challenge 
new ad hoc paradigm created support su condition document previous evaluations 
natural unit audio recordings time documents words 
decided su systems output ranked list time pointers 
tdt program investigating technology story segmentation want require sdr systems find topical boundaries audio recordings 
decided require emit single time pointing hot spot mid point topical section 
approach allowed map emitted times known stories traditional document retrieval evaluation software 
approach focussed new interesting problem making existing evaluation infrastructure permitting comparison runs story boundaries known runs weren known 
keep task clean required full sdr sites implementing su option required run recognizers knowledge story boundaries 
maximal recognizers cr task nist devised script story boundaries su asr transcripts 
new su condition pose challenges scoring 
biggest issue time pointers mapped commercials fillers stories treated 
nist decided implement mapping algorithm severely penalize generation time pointers 
pointers mapped known story id 
duplicate story id commercials fillers mapped dummy id automatically scored non relevant 
results scored usual trec eval 
story boundary known sk collection excluded commercials segments included su collection direct comparisons conditions possible 
su evaluation give idea difficult technical challenge su condition pose 
team nist assessors created ad hoc topics evaluation 
goal creating trec topics devise topics relevant documents collection appropriately challenge retrieval systems 
prior coming nist assessors told review news half come possible topics 
assessors tested putative topics transcripts trec sdr collection nist prise search engine 
topic retrieve documents top considered inclusion test 
assessors required refine broaden narrow replace topic retrieve appropriate number relevant documents prise 
assessors created approximately topics 
topics similar subjects considered malformed excluded yield final test set containing topics 
test specifications documentation trec sdr track archived www nist gov speech sdr sdr htm 
test results trec sdr participants approximately half months implement recognition portion task month half implement required retrieval tasks 
order give participants maximum possible amount time run recognition retrieval period overlapped recognition period month 
site recognized transcripts submitted nist checked filtered formatted distributed cross recognizer retrieval condition 
retrieval sites weeks perform cr task 
nist limited time assessment pre cr retrieval results construct pools assessment took place parallel cr test 
trec sites restricted hardware number processors apply implementing evaluation 
sites site combinations participated third sdr track 
performed full sdr att carnegie mellon university cmu university cambridge cu htk limsi limsi sheffield university sheffield consortium tno 
remaining sites performed quasi sdr state university ny buffalo cedar ibm ibm royal melbourne institute technology mds university massachusetts umass 
see trec participant publications trec sdr track contained retrieval conditions retrieval human closed quality transcripts baseline retrieval nist bbn byblos fixed language model asr transcripts baseline retrieval nist bbn byblos adaptive language model asr transcripts speech retrieval site recognizer speech retrieval site secondary recognizer cross recognizer cr retrieval site recognizer transcripts baseline boundaries unknown baseline boundaries unknown speech boundaries unknown speech boundaries unknown cross recognizer boundaries unknown full sdr sites required run retrieval conditions 
quasi sdr sites required run retrieval conditions 
cr story boundaries unknown conditions optional 
benchmarked performance speech recognizer transcripts contributed full sdr sites sharing cross recognizer condition hour hub style transcribed subset sdr collection 
summary results shown 
sdr speech recognition results hr 
hub style transcribed subset limsi nist nist att shef cmu wer trec sdr speech recognition performance results test set word error rate mean story word error rate cross system significance word error rate word error rates surprisingly low considering enormous size test collection orders magnitude larger test sets hub asr tests 
graph shows results test set word error rate mean story word error rate 
systems produced transcripts word error rates 
fairly impressive considering speed systems run process large collection 
interesting note scores generally lower comparable scores trec asr systems run fast speeds 
best asr results obtained university cambridge htk recognizer wer johnson trec 
exception alternative pass cambridge system nist system recognizer transcripts significantly similar performance respect wer nist statistical significance software 
shows results scoring original closed style transcripts transcribed hub style transcripts 
speech recognition performance retrieval performance quite 
trec ad hoc tests quite bit variation performance particular topics 
sample trec sdr test topics illustrate variation topic nuclear waste stored new mexico 
average map systems runs relevant stories 
topic get income save spend 
average map systems runs relevant stories topic percentage population prison countries 
average map systems runs relevant stories shows results non cross recognizer retrieval conditions 
best results baseline recognizer retrieval conditions obtained system map respectively singhal trec 
best result speech input retrieval condition obtained university cambridge system map johnson trec 
sheffield university achieved best performance baseline speech input story boundary unknown conditions map respectively 
sdr site retrieval results test condition att cedar cmu cu htk ibm limsi mds sheffield tno umass trec sdr mean average precision map required non cross recognizer retrieval conditions individual test conditions useful contrasting effect binary variables human transcripts vs asr transcripts story boundaries known vs story boundaries unknown 
interesting results cross recognizer retrieval conditions contain multiple recognition performance retrieval performance data points examine effect recognition performance retrieval performance 
sites participated story boundaries known cross recognizer cr retrieval condition university cambridge limsi sheffield university 
sites ran retrieval sets submitted recognizer transcripts 
adding retrieval results closed quality transcripts gives recognition retrieval data points system 
shows graph retrieval performance vs recognition performance story boundaries known cross recognizer retrieval condition 
cmu recognizer data point removed extreme outlier 
graph shows retrieval performance degrades little transcripts increasing word error rates retrieval fairly robust recognition errors 
hypothesis redundancy key words spoken documents permits relevant documents retrieved substantial number words mis recognized 
trec assumed robustness due small collection size expected recognition retrieval performance drop steeper larger trec collection 
appear case 
compare average cross system slope recognition retrieval performance curve trec trec find identical trec vs trec 
individual systems different relative retrieval performance systems slopes appears relatively flat 
system achieved best cr performance shallow recognition retrieval performance slope singhal trec 
sdr retrieval vs recognition wer att cu htk limsi sheffield att slope slope limsi slope sheffield slope mean slope mean slope trec sdr story boundaries known cross recognizer retrieval condition results showing mean average precision vs word error rate sites participated story boundaries unknown cross recognizer retrieval condition university cambridge sheffield university consortium 
results condition shown 
sdr retrieval vs recognition su condition wer sheffield tno slope sheffield slope tno slope mean slope trec sdr story boundaries unknown cross recognizer retrieval condition results showing mean average precision vs word error rate story boundaries known cr condition relative performance retrieval systems differed recognition retrieval performance slopes relatively flat average slope 
university cambridge system achieved best performance johnson trec 
retrieval scores significantly lower comparable cr scores indicates unknown story boundaries pose greater difficulties retrieval systems 
part difficulty explained difference test data 
story boundaries known systems transcripts commercials filler segments removed story boundaries unknown systems process entire broadcasts 
difficult compare results penalization duplicates scoring 
recognition results trec extremely encouraging 
saw recognition error rates fall recognition systems faster tackle large trec collection 
results retrieval systems quite 
factors conclude technology robust larger spoken document collections improved significantly trec 
adaptive recognition systems effectively recognize speech data collected time comparable static systems 
cross recognizer retrieval conditions multiple recognition retrieval data points showed near linear relationship recognition errors retrieval accuracy retrieval performance degradation slope increasing recognition errors relatively gentle 
sdr technology applied evaluated conditions story boundaries unknown 
trec sdr plans discussion trec sdr community decided stabilize sdr track upcoming year minor changes 
significant story boundaries unknown condition mandatory participants 
test collection new set test topics developed 
story boundaries unknown condition effective audio signal information transcriptions speaker changes noise changes volume changes music prosody encourage development common non lexical information exchange format store share information 
encourage sdr participants year share data addition asr transcripts cross recognizer retrieval condition 
test specifications documentation trec sdr track available www nist gov speech sdr sdr htm 
trec sdr track sdr track enormous success regard primary goals bringing speech recognition information retrieval research communities explore feasibility implementing evaluating retrieval spoken audio recordings 
certainly shown technology implemented evaluated trec known item ad hoc tasks 
implemented evaluated reasonably large audio collections conditions story boundaries unknown 
fact progress occurred quickly conclude sdr solved problem 
useful non lexical information harnessed audio signal 
explored traditional text retrieval modalities automatically transcribed speech haven tackled challenging problems question answering spoken queries mis recognition single word cause catastrophic failure technology 
traditional sdr task redundancy words collection protected truly facing issues 
issues explore conquer regard general problem multi media information retrieval 
discussion regarding trec sdr track suggestions evaluations revolving audio domain circulated including passage retrieval multi lingual cross lingual sdr sdr question answering interactive sdr name 
problems tackled text basis trec possible exception question answering additional information learned audio collections somewhat limited 
fairly idea kinds problems asr introduces text retrieval model behavior text retrieval domains asr running full blown evaluations 
challenge broadening true multi media information retrieval domain require text retrieval speech recognition video image processing 
multi media sources come different forms need integrated threaded 
threading doubt require natural language processing knowledge engineering 
enormous problem require collaboration different technology communities 
sdr brought research communities 
require involvement 
taken task virtually impossible 
sense break constituent components component combinations incrementally integrated 
accordingly believe binary ternary technology development evaluation projects undertaken explore tractable lower level challenges undertake full 
approach core signal processing technologies speech recognition speaker identification face object identification scene tracking incrementally integrated higher level information processing technologies 
eventually capability create robust multi media information system technologies emerge 
year nist interested creating retrieval track explore information contained video signal 
video corpus including audio explore integration speech recognition video processing retrieval applications 
scratched surface audio processing speech recognition great deal information words encoded audio signal 
new domains integrated technologies course require development new evaluation methods formats tools 
greatest challenges overcome developing new technology research task 
research tasks nist created evaluation programs significant lengthy discussion debate regarding development metrics scoring protocols 
metrics taken granted today mean average precision word error rate discussion 
need build component technology measures system measures multi media systems technologies take shape 
possibilities quite exciting done 
nist trec sdr tracks sponsored part defense advanced research projects agency darpa 
authors karen jones university cambridge guidance development sdr track 
donna harman david pallett nist support sdr track stanford nist help implementing baseline speech recognition systems 
sue johnson university cambridge help refining test specifications evaluation protocols 
ibm contribution baseline speech recognizer transcripts trec sdr carnegie mellon university contribution sphinx iii recognizer trec sdr special gte bbn contribution support linux byblos rough ready fast recognizer trec sdr 
trec sdr participants participation track success 
disclaimer mention commercial products commercial organizations information imply recommendation endorsement national institute standards technology imply products mentioned necessarily best available purpose 
bibliographical beowulf project nasa center excellence space data information sciences gsfc nasa gov linux beowulf reviewed 
graff liberman tdt text speech corpus proc 
darpa broadcast news workshop march 
fiscus doddington garofolo nist topic detection tracking evaluation proc 
darpa broadcast news workshop february 
fisher re investigation tdt transcription error rates personal conversation 
garofolo fiscus fisher design preparation hub broadcast news benchmark test corpora proc 
darpa speech recognition workshop february 
garofolo voorhees stanford jones trec spoken document retrieval track overview results proc 
trec darpa speech recognition workshop february 
garofolo voorhees stanford lund trec spoken document retrieval task overview results proc 
trec nov 
garofolo john cedric voorhees ellen trec spoken document retrieval track overview results proc 
trec nov 
due time constraints referenced created 
riao trec sdr overview trec proceedings graff wu macintyre liberman broadcast news speech corpus proc 
darpa speech recognition workshop february 
johnson moore jones woodland cambridge university spoken document retrieval system proc icassp vol 
pp march kantor voorhees trec confusion track comparing retrieval methods scanned text information retrieval press 
kubala liu srivastava makhoul integrated technologies indexing spoken language communications acm volume page feb 
miller schwartz weischedel stone named entity extraction broadcast news proc 
darpa broadcast news workshop march 
pallett fiscus przybocki preliminary broadcast news benchmark tests proc 
darpa speech recognition workshop february 
pallett fiscus martin przybocki broadcast news benchmark test results english non english proc 
darpa broadcast news transcription understanding workshop february 
pallett fiscus garofolo martin przybocki broadcast news benchmark test results english non english word error rate performance measures proc 
darpa broadcast news workshop february 
przybocki fiscus garofolo pallett hub information extraction evaluation proc 
darpa broadcast news workshop march 
singhal pereira document expansion speech retrieval proc 
sigir 
voorhees garofolo jones trec spoken document retrieval track proc 
darpa speech recognition workshop february 
voorhees garofolo jones trec spoken document retrieval track trec notebook nov 
voorhees harman overview seventh text retrieval conference trec proc 
trec november 
voorhees harman overview sixth text retrieval conference trec information processing management vol 
pp january 
trec sdr participant publications trec nist gov pubs trec proceedings html renals thisl spoken document retrieval system university sheffield uk cook robinson proc 
trec nov 
allan callan croft ballesteros byrd swan xu inquery battle trec proc 
trec nov 
crestani sanderson lalmas short queries natural language spoken document retrieval experiments glasgow university proc 
trec nov 
fuller kaszkiel ng wilkinson zobel mds trec report proc 
trec nov 
sheridan wechsler eth trec routing chinese cross language spoken document retrieval proc 
trec nov 
oard document translation cross language text retrieval university maryland proc 
trec nov 
siegler slattery seymore jones hauptmann witbrock experiments spoken document retrieval cmu proc 
trec nov 
singhal choi hindle pereira trec sdr track proc 
trec nov 
smeaton quinn 
ad hoc retrieval thresholds french monolingual retrieval document glance high precision triphone windows spoken documents proc 
trec nov 
walker robertson jones jones okapi trec automatic ad hoc vlc routing filtering proc 
trec nov 
trec sdr participant publications trec nist gov pubs trec proceedings html renals cook robinson retrieval broadcast news documents thisl system proc 
trec nov 
allan callan sanderson xu inquery trec proc 
trec nov 
franz roukos audio indexing broadcast news trec sdr proc 
trec nov 
kraaij van leeuwen tno trec site report sdr filtering proc 
trec nov 
fuller 
kaszkiel ng wu zobel kim robertson wilkinson trec ad hoc speech interactive tracks mds csiro proc 
trec nov 
henderson crystal text retrieval semantic forests trec proc 
trec nov 
johnson moore jones woodland spoken document retrieval trec proc 
trec nov 
nowell experiments spoken document retrieval dera proc 
trec nov 
oard trec experiments university maryland proc 
trec nov 
siegler berger hauptmann witbrock experiments spoken document retrieval cmu proc 
trec nov 
singhal choi hindle lewis pereira trec proc 
trec nov 
trec sdr participant publications trec nist gov pubs trec proceedings html ellis renals robinson thisl sdr system trec proc 
trec nov 
allan callan feng malin inquery trec proc 
trec nov 
franz ward ad hoc cross language spoken document information retrieval ibm proc 
trec nov 
fuller kaszkiel ng wilkinson wu zobel rmit csiro ad hoc web interactive speech experiments trec proc 
trec nov 
gauvain de lamel adda limsi sdr system trec proc 
trec nov 
han nagarajan srihari srikanth trec experiments suny buffalo proc 
trec nov 
kraaij hiemstra trec language technology information retrieval proc 
trec nov 
johnson spark jones woodland spoken document retrieval trec cambridge university proc 
trec nov 
siegler jin hauptmann cmu spoken document trec analysis role term frequency tf proc 
trec nov 
singhal abney collins hindle pereira trec proc 
trec nov 
