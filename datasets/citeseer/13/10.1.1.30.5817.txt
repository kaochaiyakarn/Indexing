clustering learning tasks selective cross task transfer knowledge sebastian thrun joseph sullivan november cmu cs school computer science carnegie mellon university pittsburgh pa author affiliated computer science department iii university bonn germany part research carried 
research sponsored part national science foundation award iri wright laboratory aeronautical systems center air force materiel command usaf advanced research projects agency arpa number 
views document author interpreted necessarily representing official policies endorsements expressed implied nsf wright laboratory united states government 
keywords clustering concept learning knowledge transfer lifelong learning machine learning object recognition supervised learning increased interest machine learning methods learn learning task 
methods repeatedly outperform conventional single task learning algorithms learning tasks appropriately related 
increase robustness approaches methods desirable reason relatedness individual learning tasks order avoid danger arising tasks unrelated potentially misleading 
describes task clustering tc algorithm 
tc clusters learning tasks classes mutually related tasks 
facing new thing learn tc determines related task cluster exploits information selectively task cluster 
empirical study carried mobile robot domain shows tc outperforms counterpart situations small number tasks relevant 
exciting new developments field machine learning algorithms gradually improve ability learn applied sequence learning tasks 
motivated observation humans encounter learning task lifetime successfully improve ability learn researchers proposed algorithms able acquire domain specific knowledge re learning tasks 
example context face recognition methods developed improve recognition accuracy significantly learning recognize face learning transferring face specific invariances acquired face recognition tasks 
similar results context object recognition robot navigation chess reported 
technically speaking underlying learning problem stated follows 

training data current learning task 
training data previous learning tasks 
performance measure find hypothesis maximizes performance current th learning task 
notice item list data previous learning tasks called support data support tasks appear usual formulation machine learning problems 
support data indirectly related carry different class labels 
utilize data mechanisms required acquire re domain specific knowledge order guide generalization knowledgeable way 
date available variety strategies transfer domain specific knowledge multiple learning tasks ffl learning internal representations artificial neural networks ffl learning distance metrics ffl learning re represent data ffl learning invariances classification ffl learning algorithmic parameters choosing algorithms :10.1.1.44.2898
pair support tasks compute performance gain task knowledge transferred task 
arrange tasks small number clusters maximizing performance gain task cluster 

new task arrives determine similar task cluster 
selectively transfer knowledge task cluster 
table tc general scheme task clustering 
ffl learning domain models 
approaches demonstrated reduce sample complexity number available support tasks sufficiently large equally importantly appropriately related 
means tasks related currently understood 
relatedness learning tasks seen independently particular learning algorithm 
tasks appropriately related algorithm may unrelated 
support tasks improve generalization accuracy provide set learning tasks known appropriately related respect learning algorithm 
places burden programmer knowledgeable learning algorithm relation learning tasks 
weaken requirement desirable design algorithms discover relation multiple learning tasks new task arrives identify strongly related tasks transfer knowledge 
describes tc task clustering algorithm 
tc transfers knowledge selectively 
general principle outlined table 
impose structure space learning tasks tasks grouped clusters related tasks 
relatedness defined effect transferring domain knowledge learning task performance task improves knowledge transferred task related step 
performance gain step clusters tasks bins bin relatedness maximal 
result tasks transfer mechanism produces mutual leverage grouped 
new task arrives step algorithm table determines related task cluster 
transfers knowledge single cluster task clusters employed 
tc implementation instantiates general scheme shown table 
lowest level learning tc uses nearest neighbor knn generalization 
knn memorizes training data explicitly interpolates query time 
tc transfers knowledge tasks adjusting distance metric determining proximity data points knn see :10.1.1.44.2898
determine task relatedness tc inspects effect generalization accuracy task distance metric optimally adjusted task step 
clusters tasks maximize accuracy gain clusters step 
results clustering tc determines optimal distance metric task cluster 
new task arrives step tc inspects distance metrics cluster picks works best 
clustering strategy enables tc handle multiple classes tasks characterized different distance metric 
elucidate tc practice reports results series experiments carried mobile robot domain 
key results empirical study 
sample complexity knn reduced significantly distance metric learned previous related tasks 

tc reliably succeeds partitioning task space hierarchy meaningful related tasks 

selective transfer significantly improves results cases support tasks relevant hurt performance support tasks appropriately related 
remainder organized follows 
section describes tc algorithm detail 
empirical results obtained mobile robot domain described section 
section discusses strengths weaknesses approach outlines open questions 
tc algorithm nearest neighbor nearest neighbor knn known method fitting functions review briefly 
suppose approximate function delta finite potentially noisy set input output examples knn approximates delta searching set nearest neighbors returns average output value 
specifically suppose query point know value 
knn searches nearest data points fi fi fi fi fi fi fi dist dist distance metric dist delta delta 
knn estimates averaging notice discrete output spaces case experiments reported result knn truncated 
generalization properties knn depend choice distance metric 
popular choice euclidean distance metric dist euclid gamma superscript refer th component vector 
notice euclidean metric weighs data dimension equally 
ideas tc algorithm uses globally weighted version dist euclid dist gamma vector parameters determine relative weight input dimension :10.1.1.44.2898
obviously different vectors knn interpolate differently 
vector determines knn generalizes 
learning shown empirically effective way transfer knowledge multiple learning tasks 
determining optimal distance metric task set tasks discussed possesses optimal distance metric 
optimal metric finite sample set minimizes error data assuming data generated unknown probability distribution 
strictly speaking estimate optimal estimate generalization error data adjust minimize error 
practice error computed exactly tc uses slightly different criterion determining set training examples tc minimizes distance examples belong class time maximizes distance examples belong different ones 
words obtained minimizing expression ffi xy dist ffi xy gamma component constrained lie argmin denote parameter vector minimizes henceforth called optimal task optimal 
dist corresponding optimal distance metric 
minimizing infra class distance dist simultaneously maximizing inter class distance dist dist focuses relevant input dimensions particular learning task 
monotonic gradient descent 
notice optimized simultaneously multiple learning tasks 
ae ng denote subset support tasks 
argmin optimal parameter vector dist corresponding distance metric task set subscript indicates computed training data th learning task 
principle suffices bound 
practice data set optimal considers extremely small number input dimensions happen discriminative training set 
particularly case input space high dimensional training data scarce case experiments 
prevent type fitting beneficial constrain lie interval include 
clustering tasks steps table tc arranges different learning tasks disjoint set task clusters 
incentive expected gain generalization accuracy task uses task optimal distance function 
specifically tc generates task transfer matrix entry pair learning tasks value expected generalization accuracy task optimal distance metric dist element computed cross validation follows 
firstly optimal distance metric dist task determined minimizing em 

secondly nearest neighbor applied task dist estimate generalization accuracy sample set th support task repeatedly split training testing set 
measured test set averaged splits 
denote disjoint decomposition support tasks clusters delta delta delta ng 
functional ja measures averaged estimated generalization accuracy obtained support task uses optimal distance metrics tasks cluster 
maximizing clusters tasks way averaged generalization accuracy maximal assuming distance metric transferred cluster 
notice clusters defines optimal distance metric argmin obtained minimizing ea cf 

distance metrics task cluster form basis selective transfer described section 
remains discussed maximize general optimizing functional defined pairwise cost matrix understood combinatorial data clustering problem various algorithms exist see example 
experiments set support tasks sufficiently small globally optimal computed explicitly 
larger incomplete methods applied 
selective transfer task cluster defines optimal distance metric 
optimizing single metric tasks cluster knowledge transferred task cluster 
particular interest situations new learning task arrives may related previous learning tasks 
principal strategies relate new learning task set existing ones 

non incremental time new task arrives set tasks including new re clustered 
achieve optimal results require tasks clustered arrival single training example computationally expensive 

incremental repeatedly clustering task space leave existing clusters unchanged sort new task related task cluster 
strategy faster avoids combinatorial problem clustering tasks 
task clusters incrementally approach necessarily optimal 
current implementation tc employs non incremental incremental strategy clustering tasks 
data collection learning task finished tc clusters tasks bins described 
appropriate value known clusters generated 
unknown tc builds complete hierarchy tasks clustering task space clusters 
worst case total delta task clusters experiments including reported tc generated fewer clusters 
new task arrives tc determines related task cluster uses cluster optimal distance metric 
glance determine related cluster minimize fold cross validation error distance metrics determining appropriate 
experiments adopted different strategy consistently slightly better 
determine related task cluster tc algorithm maximizes ratio inter class infra class distance ffi xy gamma dist delta ffi xy dist gamma value divides distance instances different classes distance instances fall class 
gives measure classes separated particular distance metric dist new task arrives tc selects distance metric dist maximizes uses nearest neighbor generalization 
summarize tc algorithm tc transfers knowledge multiple learning tasks learning distance metric tasks nearest neighbor generalization 
focus transfer related tasks tc clusters support tasks space sets related tasks 
facing new learning task distance metric transferred related task cluster 
experimental results setup tc algorithm evaluated empirically mobile robot xavier shown 
experiments employed color camera top robot sonar proximity sensors arranged ring robot 
camera pointed slightly downward 
practical objective drove development tc algorithm design robust computationally efficient learning algorithms applied variety perceptual tasks specifically context mobile robot control 
collected databases sonar camera snapshots persons landmarks objects locations 
database consisted snapshots color camera images sonar scans examples shown figures 
datasets constructed particular person front robot 
different persons wore different clothes recognition involved spotting certain colors 
databases contained generic sensor readings robot laboratory hallway respectively 
seventh database contained images sonar scans blue trash bin final databases consisted snapshots situations robot facing open closed door respectively 
illustrates variations database 
collecting data attention paid color camera pan tilt head range finder sonar ring structured light bump detectors oe radio link oe computers mobile robot xavier 
image far away person landmark occured 
keep size data manageable sampled image matrix rgb color triplets shown 
experiments reported snapshots represented values sonar scans normalized unit interval 
test tc different conditions defined families learning tasks 
task family described table consisted various tasks involving recognition people landmarks locations 
reasons sect 
thirteenth tasks recognition open versus closed doors testing task th learning task twelve tasks support tasks 
tasks somewhat similar sensors address recognition similar features assembled second family tasks 
task family consisted tasks task family see table tasks time input dimensions permuted ran person joseph person sean person lisa person sebastian laboratory wean hallway wean xx open door closed door examples databases image sonar scan 
open door closed door variations data 
permutation 
resulting tasks called second permuted version tasks time tasks permuted differently 
tasks called testing task table chosen facilitate comparison results obtained task families 
task family contained tasks potentially related testing tasks 
tasks mutually related unrelated task tasks related task mutually related 
see transfer suffer unrelated tasks 
clustering tasks sect 
value mn estimated fold cross validation training set examples dataset testing set examples dataset 
distance metric sect 
optimized gradient descent iterated steps step size delta delta red green blue sampling images red green blue values 
momentum 
convergence consistently observed earlier epochs 
results sensitive learning parameters 
observe fitting tasks distance metrics optimized task test task 
observation crucially depended existence bounds distance parameters absence bounds performance training task increased slightly usually decreased significantly task 
experimental results reported test set results performance measured data points part training set 
averaged experiments different sets training examples 
illustrate effect transfer tasks compare optimal distance metric equally weighted euclidean distance metric serves uninformed default metric absence support tasks 
appropriate diagrams show confidence bars true value 
performance graphs show generalization accuracy testing set accuracy testing task 
non selective transfer elucidate benefit limits regular transfer conducted experiments support tasks computing optimal distance metric 
transfer understood special case tc algorithm number clusters set 
key empirical result shown 
thin curve diagram shows generalization accuracy equally weighted euclidean distance metric testing task function number training examples 
curve illustrates accuracy nearest neighbor absence support tasks 
thick curve depicts generalization accuracy optimal distance metric support tasks 
seen graphs data set joseph sean lisa seb 
lab hallway trash bin open closed door hallway door door 
joseph sean gamma 
joseph lisa gamma 
joseph sebastian gamma 
sean lisa gamma 
sean sebastian gamma 
lisa sebastian gamma 
joseph trash bin gamma 
sean trash bin gamma 
lisa trash bin gamma 
hallway trash bin gamma 
lab hallway gamma 
door door gamma gamma 
door open closed gamma table task family twelve tasks support tasks thirteenth testing task 
optimal distance metric shows significantly better results equally weighted metric illustrating benefit transferring knowledge tasks 
ways quantify results 

relative generalization accuracy 
generalization error averaged different number training examples shown 
optimal distance metric infers average error equally weighted distance metric 
fixed number training examples expect optimal distance metric cut error roughly half 

relative sample complexity 
second quantity measures reduction sample complexity 
shows result statistical comparisons generalization accuracies equally weighted versus optimal metric different number training examples 
white region transfer training examples generalization error equally weighted metric thin vs optimal metric bold best possible support tasks mean error average generalization error optimal metric equally weighted euclidean metric sample complexity sample complexity equally weighted metric vs optimal metric generalization curves average generalization error testing task 
statistical comparison error equally weighted optimal distance metric varying numbers training examples 
weight grey area equally weighted optimal distance metric superior confidence level 
dark region methods generalize equally 
transfer training examples generalization error equally weighted metric thin vs optimal metric bold support tasks mean error average generalization error optimal metric equally weighted euclidean metric sample complexity sample complexity equally weighted metric vs optimal metric transfer task family notice optimal distance metric slightly better equally weighted metric 
equally weighted distance metric outperforms optimal confidence 
large grey version opposite case 
methods equally generalization accuracies differ significantly level 
notice average optimal distance metric uses number training examples required equally weighted euclidean metric 
example average generalization accuracy optimal distance metric applied training examples euclidean distance metric training examples available 
interesting notice boundary approximately linear 
observation suggests reduction sample complexity depends weakly actual number training examples 
summarize generalization error optimal distance metric inferred equally weighted euclidean metric requires samples required metric reaching certain level generalization accuracy 
results apply task family positive impact knowledge transfer depends crucially fact support tasks sufficiently related test task 
task family majority tasks unrelated fails produce similar effects 
seen optimal distance metric slightly better equally weighted counterpart 
particular generalization error reduced sample complexity reduced clearly improvement corresponding values 
shown sections selectively transferring knowledge right cluster tasks improve results greatly 
clustering tasks shows normalized version transfer matrix task family 
row depicts particular task including testing task benefits knowledge transferred task white boxes indicate generalization accuracy task improves optimal distance metric task equally weighted distance metric 
black boxes indicate opposite case meaning tasks anti related 
size box visualizes magnitude effect 
task family tasks related testing task unrelated notably anti related row 
diagram test support task test performance task task transfer matrix test support task test performance task task transfer matrix task transfer matrix task family task family white values show indicate error task reduced optimal distance metric task 
black values indicate opposite tasks anti related 
relation individual tasks testing tasks depicted 
interesting task family shows tasks particular anti related testing task 
words respective optimal distance metrics hurt performance testing tasks 
shows non permuted tasks related testing task showing exists opportunity synergy knowledge transfer 
figures show different clusters task families different values task hierarchy task family depicted illustrates second key result empirical study tc manages discover meaningful tasks clusters 
early starting clusters major task families encoding grouped 
example partitions tc generates task clusters 
comparison worst partitioning groups tasks bins 
different task types clustered separate clusters 
finding illustrates tc manages find meaningful clusters discover structure inherently exists different tasks 
second interesting result depicted tc groups exactly level task hierarchy best worst best tasks involving people task hierarchy task family hierarchy obtained clustering task space different numbers clusters right part diagram depicts corresponding value difference best worst partitioning 
tasks rely input encoding 
clusters task family differences different tasks subtle interesting note tasks involving recognition people form similar subgroup particularly involving different people cf 

tasks involve recognition person arranged different clusters 
selective transfer figures summarize results obtained tc algorithm task family clusters 
notice results directly level best worst best regular encoding permutation group ii permutation group task hierarchy task hierarchy task family notice early task hierarchy separates different task types 
related task cluster identified clusters available 
compared shown 
dashed curves figures left bars figures show task cluster corresponding generalization accuracies test task cluster optimal distance metric 
experiments clusters appropriately related testing task leads results better obtained equally weighted distance function 
known advance tc task cluster adjust distance metric 
information available 
results obtained tc algorithm shown figures solid curve diagrams right bars diagrams 
board tc manages guess best task cluster considerably generalizes 
learning generalization accuracy tc slightly worse best task cluster due fact certain clusters training examples generalization error task clusters dashed optimal metric solid tc support tasks mean error task clusters average generalization error individual task clusters left tc right optimal metric equally weighted euclidean metric sample complexity sample complexity equally weighted metric vs optimal metric selective transfer task family task clusters 
results clearly superior obtained transfer 
notice difference different task clusters 
clusters training examples generalization error task clusters dashed optimal metric solid tc support tasks mean error task clusters average generalization error individual task clusters left tc right optimal metric equally weighted euclidean metric sample complexity sample complexity equally weighted metric vs optimal metric selective transfer task family task clusters 
clusters training examples generalization error task clusters dashed optimal metric solid tc support tasks mean error average generalization error individual task clusters left tc right optimal metric equally weighted euclidean metric sample complexity sample complexity equally weighted metric vs optimal metric selective transfer task family complete task hierarchy 
clusters training examples generalization error task clusters dashed optimal metric solid tc support tasks mean error task clusters average generalization error individual task clusters left tc right optimal metric equally weighted euclidean metric sample complexity sample complexity equally weighted metric vs optimal metric selective transfer task family task clusters 
selective transfer yields results transfer tasks related 
probability tc picks wrong task cluster 
example training examples test task example open door closed door tc picks experiments correct task cluster 
experiments tc selects task cluster task cluster task cluster situation changes training data arrives 
training examples tc correctly guesses best task cluster experiments patterns reliably identifies best cluster 
illustrates tc error manages identify relevant tasks 
unknown tc uses complete task hierarchy pool potentially related tasks partitioning specific number clusters seen task hierarchy task family contains total task clusters consists single task clusters cluster contains tasks clusters containing task 
results applying tc complete task hierarchy depicted 
expected performance tc task hierarchy worse performance clusters 
results clearly superior approach forced transfer knowledge tasks 
performance results illustrate third key result empirical study selective transfer superior transfer situations tasks irrelevant 
example tc achieves average generalization error test task knowledge transferred selectively support tasks 
relatively speaking average error observed approach cf 
right bar considerably close theoretically optimal value cf 
right bar 
relative improvement sample complexity significant sample complexity test set tc transfers knowledge selectively compared counterpart 
tc compared equally weighted distance metric nearest neighbor absence support sets tc uses samples reach level generalization accuracy generalization accuracy average inferred equally weighted distance metric 
results remarkably close achieved knew advance support sets appropriately related 
results shown figures appears number task clusters weakly impacts results 
experiments shown case long 
smaller values number task clusters insufficient tc performance degrades regular nearest neighbor equally weighted distance metric 
sake completeness show results tc task family recall tasks related transfer mechanisms produced satisfactory results 
interesting finding tc identifies clusters tasks significantly differs relatedness testing task small number tasks determine optimal distance metric tc generalizes approximately approach 
approach benefits fact data adjust distance metric 
practical utility results ability learn recognize people landmarks locations useful context robot navigation human robot interaction 
tc algorithm particularly designed facilitate reliable fast recognition concepts 
addition small sample complexity due knowledge transfer tc works fast practice relies nearest neighbor generalization 
nearest neighbor memorizes training examples avoiding need long training times 
efficient retrieval tree algorithms exist facilitate fast access memorized data see 
particular testing task distinction open closed doors useful context mobile robot navigation 
shows xavier robot navigates occupancy map approach originally developed rhino mobile robot project university bonn 
occupancy maps assume world static handle failures arise dynamics real world environments 
typical failure situation shown 
door previously open suddenly locked 
xavier navigation system detects failure quite inefficiently moves door circles front door internal occupancy map corrected 
practice may quite time consuming previous evidence door open overridden new evidence indicates presence obstacle 
recognize upcoming plan failure earlier navigation routines augmented learned door status recognizer result task door door 
training examples door recognition task easily constructed plan failures robot turns easily detected post fact 
shown artificial neural network able reliably trials detect closed doors enabling robot change path accordingly 
example shown 
reduce ratio example xavier robot leaving room 
twodimensional occupancy map bird eye view robot environment constructed previously sonar sensor information 
bright values correspond free space dark values occupied regions 
example plan failure 
xavier find part path blocked correct occupancy map different route 
correction timeconsuming 
failure anticipation learning recognize closed doors 
robot turns earlier 
false alarms robot motion direction changed consecutive sensor readings indicated presence closed door high confidence 
neural network approach suffered enormous training times prohibited systematic evaluation approach 
expect tc approach faster easier handle practice 
discussion describes instantiation results method selective transfer knowledge multiple learning tasks 
tc algorithm employs nearest neighbor algorithm transfers knowledge adjusting distance metric tasks re 
transfer knowledge selectively tc clusters tasks bins related tasks 
relatedness defined context tc knowledge transfer mechanisms 
facing new learning task tc determines related task cluster selectively transfers knowledge form cluster 
experimental comparison conducted mobile robot perceptual domain shown 
tasks appropriately related tc transfer mechanisms successfully reduces sample complexity 
example task family tc consumes training examples uninformed distance metric equal weights requires 

tc clustering mechanisms manages discover meaningful task clusters build hierarchies tasks 
example task family tc groups different task types separate clusters clusters available 
tasks involve recognition people fall different clusters 

tasks appropriately related task family selectively transferring knowledge related task cluster improves results significantly 
example task family tasks appropriately related testing task selective transfer requires amount training data required corresponding transfer mechanisms 
key assumption tc approach existence groups tasks tasks related group 
tc benefits appropriate number task clusters known 
tc particularly appropriate small known number distinct problem categories problem category tc transfer mechanisms successfully reduces sampling complexity 
empirical results provide evidence tc somewhat robust respect choice number task clusters tc managed reduce sample complexity task family complete task hierarchy employed contains clusters possible values 
little known cases class boundaries smoother 
cases task boundaries smoother smoother arbitration schemes weighting impact task cluster proportion delta superior 
second limitation current implementation arises fact space partitions searched exhaustively 
clearly complexity exhaustive search prohibits global optimization large values view principal limitation tc algorithm heuristic stochastic optimization methods certainly applicable 
learning tasks arrive task clusters may learned incrementally determining cluster membership task arrives 
little known concerning results depend fact partitioning represents global minimum experiments reported tc applied analyze images sonar measurements 
comparing images sonar scans pixel pixel certainly effective strategy analyzing images 
simple encoding chosen mainly interest rely little possible prior domain specific knowledge 
tc currently implemented unable discover dependencies level individual pixels learns distance value input dimension 
applying tc algorithm sophisticated image encodings applying algorithms discover sophisticated commonalities multiple tasks surveyed subject ongoing research 
reader may notice general scheme outlined table may applicable approaches transfer knowledge multiple learning tasks 
course approaches computationally infeasible scheme requires order comparisons involving repeated experiments transfer tasks 
main limitations tc arises fact task clustering pairwise comparisons 
tc capture effects transfer arise tasks involved 
remains seen pairwise comparisons prevent tc finding useful clusters different application domains 
full evaluation transfer subsets tasks requires time exponentially number tasks tc time requirements quadratic 
appears feasible design incremental strategies time requirements linearly delta efficient current implementation tc small 
key difference tc approach previous approaches lies tc ability transfer knowledge selectively 
weighting previous learning tasks equally learning bias new tc structures space learning tasks reasons relatedness 
light experimental findings conjecture tc approach scales better diverse application domains domains learning tasks just single type 
abu mostafa method learning hints 
advances neural information processing systems edited hanson cowan giles 
morgan kaufmann san mateo ca pp 

ahn 
brewer psychological studies explanation learning 
investigating explanation learning edited dejong 
kluwer academic publishers boston dordrecht london 
atkeson locally weighted regression robot learning 
proceedings ieee international conference robotics automation edited sacramento ca pp 

baxter canonical metric vector quantization 
university department mathematics statistics australia 
submitted publication 
baxter learning internal representations 
proceedings conference computation learning theory edited appear 
beymer shashua poggio example image analysis synthesis 
massachusetts institute technology artificial intelligence laboratory november 
memo 
buhmann data clustering learning 
handbook brain theory neural networks edited arbib 
books mit press pp 

buhmann burgard cremers fox hofmann schneider thrun mobile robot rhino 
ai magazine vol 
caruana multitask learning knowledge source inductive bias 
proceedings tenth international conference machine learning edited utgoff 
morgan kaufmann san mateo ca pp 

franke scattered data interpolation tests methods 
mathematics computation vol 
pp 

friedman flexible metric nearest neighbor classification 
department statistics linear accelerator center stanford university stanford 
ca november 
hastie tibshirani discriminant adaptive nearest neighbor classification 
dept statistics biostatistics stanford university stanford ca december 
submitted publication 
hertz krogh palmer theory neural computation 
addison wesley pub 
redwood city california 
hild waibel multi speaker speaker independent architectures multi state time delay neural network 
proceedings international conference acoustics speech signal processing ieee edited pp 
ii 
jordan rumelhart forward models supervised learning distal teacher 
cognitive science vol 
pp 

edelman generalizing single view face recognition 
cs tr department applied mathematics computer science weizmann institute science rehovot israel january 
moore efficient memory learning robot control 
trinity hall university cambridge england 
moore hill johnson empirical investigation brute force choose features smoothers function approximators 
computational learning theory natural learning systems volume edited hanson judd petsche 
mit press 
moravec sensor fusion certainty grids mobile robots 
ai magazine vol 
pp 

moses ullman edelman generalization changes illumination viewing position upright inverted faces 
cstr department applied mathematics computer science weizmann institute science rehovot israel 
sullivan mitchell thrun explanation neural network learning mobile robot perception 
symbolic visual learning edited ikeuchi veloso 
oxford university press 
sullivan thrun robot improves ability learn 
carnegie mellon university school computer science pittsburgh pa september 
submitted publication 
pratt transferring previously learned back propagation neural networks new learning tasks 
rutgers university department computer science new brunswick nj may 
appeared technical report ml tr 
rendell layered concept learning dynamically variable bias management 
proceedings ijcai 
pp 

sejnowski rosenberg nettalk parallel network learns read aloud 
jhu eecs johns hopkins university 
sharkey sharkey adaptive generalization transfer knowledge 
proceedings second irish neural networks conference edited belfast silver mercer model consolidation retention transfer neural net task knowledge 
proceedings inns world congress neural networks edited washington dc pp 
volume iii 
stanfill waltz memory reasoning 
communications acm vol 
pp 

suddarth holden symbolic neural systems hints developing complex systems 
international journal machine studies vol 
sutton adapting bias gradient descent incremental version delta bar delta 
proceeding tenth national conference artificial intelligence aaai aaai edited aaai press mit press menlo park ca pp 

thrun explanation neural network learning lifelong learning approach 
kluwer academic publishers boston ma 
appear 
thrun exploration model building mobile robot domains 
proceedings icnn ieee neural network council edited san francisco ca pp 

thrun learning th thing easier learning 
advances neural information processing systems edited touretzky mozer 
mit press cambridge ma appear 
thrun lifelong learning case study 
cmu cs carnegie mellon university school computer science pittsburgh pa november 
utgoff machine learning inductive bias 
kluwer academic publishers 

