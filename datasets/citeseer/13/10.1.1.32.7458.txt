reinforcement learning function approximation converges region geo rey gordon cs cmu edu algorithms approximate reinforcement learning known converge 
fact counterexamples showing adjustable weights algorithms may oscillate region converging point 
shows popular algorithms oscillation worst happen weights diverge converge bounded region 
algorithms sarsa algorithm known td gammon program 
convergent online algorithms td learning parameters linear approximation value function markov process way known extend convergence proofs task online approximation state value action value function general markov decision process 
fact known counterexamples proposed algorithms 
example tted value iteration diverge markov processes learning linear function approximators diverge states updated xed update policy sarsa oscillate multiple policies di erent value functions 
similarities sarsa learning value iteration suppose convergence properties identical 
case learning diverge exploration strategies proves iterates trajectory sarsa converge probability xed region 
similarly value iteration diverge exploration strategies proves iterates trajectory converge probability xed region 
question convergence behavior sarsa open theoretical questions reinforcement learning sutton identi es particularly important pressing opportune 
covers sarsa trajectory algorithm exploration policy may change single episode learning 
policy may change episodes value function may change single episode 
episodes agent enters terminal state 
considers episodic tasks discounted task transformed equivalent episodic task algorithms apply non episodic tasks 
earlier describes convergence behavior stable sense exist bounded regions probability eventually enters leaves markov decision processes may converge single point 
proofs extend easily sarsa 
unfortunately bound practical guarantee loose provides little reason believe sarsa produce useful approximations state action value functions 
important reasons 
best result available algorithms 
second bound rst step proving stronger results 
practice happens initial exploration period di erent policies greedy case strategy prove tighter bounds 
results similar ones developed independently 
algorithms sarsa algorithm rst suggested 
algorithm popularized td gammon backgammon playing program 
fix markov decision process nite set states nite set actions terminal state initial distribution step reward function transition function ftg 
may discount factor specifying trade rewards ones 
results carry 
transition reward functions may stochastic long successive samples independent markov property reward bounded expectation variance 
assume states reachable positive probability 
de ne policy function mapping states probability distributions actions 
policy sample trajectory sequence states actions step rewards rule selecting state choose action 
choose onestep reward 
choose new state 
repeat 
assume policies proper agent reaches probability matter policy follows 
assumption satis ed trivially 
reward trajectory sum step rewards 
goal nd optimal policy policy average generates trajectories highest possible reward 
de ne best total expected reward achieve starting state performing action acting optimally 
de ne max 
knowledge combination determine optimal policy 
sarsa algorithm maintains approximation write refer approximation 
assume full rank linear function parameters convenience notation write tack arbitrary action trajectories terminal state 
seeing proof cover td gammon program td gammon uses nonlinear function approximator represent value function 
interestingly proof extends easily cover games backgammon addition mdps 
extends cover sarsa 
trajectory fragment sarsa algorithm updates notation means parameters represent adjusted gradient descent reduce error preselected learning rate new old convenience assume remains constant single trajectory 
standard assumption sequence learning rates xed start learning satis es 
consider trajectory version sarsa 
version changes policies trajectories 
trajectory selects greedy policy current function 
state greedy policy chooses action arg max probability selects uniformly random actions 
rule ensures matter sequence learned functions state action pair visited nitely 
greedy policies essential 
just need able nd region contains approximate value functions policy considered bound convergence rate td 
compare sarsa update rule learning max sarsa update rule maximizing learning update rule di erence appears agent takes exploring action greedy current function 
algorithm maintains approximation write assume full rank linear function parameters held xed 
seeing trajectory fragment sets update ignores chosen greedy greedy policy analysis need assume consider nitely policies policy remains xed trajectory 
leave open question updates happen immediately transition trajectory 
pointed di erence ect convergence updates single trajectory cause change means subsequent updates ected 
decaying zero terms neglected 
change policies trajectory argument longer hold small changes cause large changes policy 
result result weights sarsa converge probability xed region 
proof result intuition sarsa consider di erent policies time trajectory follow td update rule policy 
td update general conditions norm contraction converge xed point applied repeatedly causes sarsa converge point just consider di erent policies take steps di erent xed points di erent trajectories 
crucially general conditions xed points bounded region 
view sarsa update rules contraction mappings plus bounded amount 
observation standard convergence theorems show weight vectors generated sarsa diverge 
theorem markov decision process satisfying assumptions bounded region sarsa algorithm acting produces series weight vectors probability converges similarly bounded region algorithm acting produces series weight vectors converging probability proof lemma shows sarsa updates written form positive de nite current learning rate var kw depend currently greedy policy 
represent manner described lemma transition probabilities step costs result current policy 
course di erent depending sarsa 
positive de nite sarsa updates norm contractions small kept policy xed changing trajectory standard results lemma guarantee convergence 
intuition de ne nonnegative potential function show average updates tend decrease long small starts large compared apply lemma assumption keep policy constant changing trajectory write write smallest eigenvalue real positive positive de nite 
write aw update direction step take kw rj jw aw aw aw kw descent direction sense required lemma 
easy check lemma variance condition 
lemma shows converges probability means converge probability pick arbitrary vector de ne max kw uk suciently large constant argument reaches weaker converge probability sphere radius centered see note descent direction inside sphere rh descent condition satis ed trivially 
outside sphere rh kw uk kw uk rh jw jw kw kw uk kak kw positive term larger negative kw large 
choose large descent condition satis ed 
variance condition easy check 
lemma shows rh lipschitz 
lemma shows converges probability means converge probability sphere radius centered done nitely policies sarsa consider pick choose large argument holds policies simultaneously 
choice update policy decreases average long small update sarsa lemma applies 
lemma corollary 
statement lemma lipschitz continuous function exists constant kf wk lipschitz condition essentially uniform bound derivative lemma di erentiable function bounded rj lipschitz continuous 
suppose sequence satis es random vectors independent 
suppose descent direction sense jw rj 
suppose ks jw jw rj nally constants satisfy probability 
proving lemma 
transformation mdp xed policy markov chain standard 
lemma update sarsa single trajectory written form new old old constant matrix constant vector depend currently greedy policy current learning rate 
furthermore positive de nite constant var kwk 
proof consider markov process state state action pair transition goes state action reward state probability transition state hs ai reward state hs probability transition js 
represent value function way represented function words representation hs ai representation 
de nitions easy see td acting produces exactly sequence parameter changes sarsa acting xed policy 
ajs state visited nitely 
write transition probability matrix markov process 
entry row hs ai column hs equal probability step hs start hs ai 
de nition 
nonnegative entries row sums equal 
write vector hs element ajs probability start state take action write identity matrix 
demonstrated vector expected visitation frequencies element corresponding state action expected number times agent visit state select action single trajectory policy 
write diagonal matrix diagonal 
write vector expected rewards component corresponding state action 
write jacobian matrix notation sutton showed expected td update new jw old old xw old considered case rewards zero transitions nonterminal terminal states argument works equally general case nonzero rewards allowed 
take 
furthermore sutton showed long agent reaches terminal state probability words long proper long state visited positive probability true states reachable nonzero probability choosing action matrix strictly positive de nite 
seen sutton equations sources variance update direction variation number times transition visited variation step rewards 
visitation frequencies step rewards bounded variance independent 
enter update ways set terms bilinear step rewards visitation frequencies set terms bilinear visitation frequencies weights set terms constant variance 
policy xed independent visitation frequencies set terms variance proportional kwk constant total variance bounded kwk 
similar simpler argument applies 
case de ne states transition matrix element probability landing step start step follow 
write vector starting probabilities 
de ne assumed policies proper policy considered positive probability reaching state update matrix strictly positive de nite 
lemma gradient function max kwk lipschitz continuous 
proof inside unit sphere derivatives uniformly zero 
outside rh wd kwk kwk rd kwk kwk ww kwk norm rst term norm second terms multiple norms add 
norm inside unit sphere outside 
boundary unit sphere rh continuous directional derivatives direction bounded argument 
rh lipschitz continuous 
andrew moore anonymous reviewers helpful comments 
supported part darpa contract number part nsf award number dms 
opinions author re ect government agencies 
sutton 
learning predict methods temporal di erences 
machine learning 
geo rey gordon 
stable function approximation dynamic programming 
technical report cmu cs carnegie mellon university 
baird 
residual algorithms reinforcement learning function approximation 
machine learning proceedings twelfth international conference san francisco ca 
morgan kaufmann 
geo rey gordon 
chattering sarsa 
internal report 
cmu learning lab 
available www cs cmu edu 
sutton 
open theoretical questions reinforcement learning 
fischer simon editors computational learning theory proceedings eurocolt pages 
de van roy 
existence xed points approximate value iteration temporal di erence learning 
journal optimization theory applications 
gavin rummery niranjan 
line learning connectionist systems 
technical report cambridge university engineering department 
tesauro 
td gammon self teaching backgammon program achieves master level play 
neural computation 
jaakkola jordan singh 
convergence stochastic iterative dynamic programming algorithms 
neural computation 
ya 

adaptation training algorithms 
automation remote control 
translated 
kemeny snell 
finite markov chains 
van nostrand reinhold new york 
