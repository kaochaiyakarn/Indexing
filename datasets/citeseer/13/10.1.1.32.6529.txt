inference learning hybrid bayesian networks kevin murphy report 
ucb csd january computer science division eecs university california berkeley california survey literature methods inference learning bayesian networks composed discrete continuous nodes continuous nodes multivariate gaussian distribution mean variance depends values discrete nodes 
brie consider hybrid dynamic bayesian networks extension switching kalman lters 
report meant summarize known sucient level detail enable implement algorithms dwelling 
update algorithm described report due lau see ch 
implemented part bayes net toolbox numerically unstable 
di erent algorithm xes problem described lj 
material added appropriate 
discuss bayesian networks bns jen node discrete continuous scalar vector valued joint distribution nodes conditional gaussian cg lw lau instantiation discrete nodes distribution continuous nodes form xjy represents multivariate gaussian normal density :10.1.1.18.5213
note discrete nodes continuous parents model 
general kind bn exact inference algorithms known 
related review article rg :10.1.1.30.5555
start discussing represent conditional probability distribution node joint distribution encodes 
give example hybrid bn discussing inference learning techniques 
representing local conditional probability distributions discrete nodes conditional distribution discrete node parents pa speci ed means table called conditional probability table cpt entries ijk pr 
denotes th possible value instantiation pa 
clearly require ijk note ways specifying conditional distribution discrete node require fewer parameters full table noisy pea causal interaction models mh decision trees default tables fg updated august :10.1.1.18.5213
supported number awarded prof russell www cs berkeley edu bayes bnt html continuous nodes conditional distribution continuous node parents pa speci ed gaussian function mean linear function parents covariance xed 
standard linear regression model 
shall start assuming parents real valued scalars shall consider vector case nally case parents discrete 
scalar case node parents kp conditional distribution jx kp exp pa ki ki weights regression coecients arcs coming node parents 
equivalently may write pa ki white noise random variable 
alternatively consider model don subtract parents means pa ki colored noise term 
vector case imagine simple extension scheme node vector 
case conditional distribution jx kp exp ki 
view multivariate generalized linear regression ex ki ex ki bn vector valued nodes 
matrices 
scalar bn corresponding case solid arcs coecients correspond rst rows bx 
similarly dotted arcs coecients correspond second rows example gx hx write avoid profusion subscripts 
note expand vector components yielding equivalent scalar network shown 
vector notation compact 
discrete parent case node discrete parents continuous parents di erent mean covariance weight matrix value xjy note case sense subtract parents means may depend discrete parents 
continuous parent discrete child case discrete node continuous parents threshold unit probit logistic distribution 
unfortunately exact inference case intractable continuous nodes observed 
possible approximation see mur 
characterizing corresponding joint distribution stated represents continuous nodes represents discrete nodes joint distribution speci multivariate gaussian parameters 
show compute parameters function local parameters node 
scalar case start considering scalar case sk 
compute 
construct diagonal matrix containing variances node diag containing standard deviations diag 
construct matrix th column contains weight vector node assume nodes numbered topologically upper triangular 
rewrite equation vector form follows sw wn vector noise terms 
innovations residuals di erences due noise values realized predicted linear model def sw strictly upper triangular invertible may write de ned def var var var var su su du result holds equation 
compute global mean 
equation subtract parents means just local means stacked 
see consider example parent jx equation need traverse graph topological order compute 
simple example jx vector case case block diagonal block upper triangular 
example ordered nodes global matrix weights 



represents value happens connect represents value topological ordering 
conditional independence properties gaussian graphical models section show rest ij inverse covariance matrix called precision matrix joint distribution rest means nodes whi 
represent joint distribution nodes exp jxj normalizing constant ensure 
called moment characteristics distribution 
expanding quadratic form collecting terms rewrite follows 
exp kx kalman filter represented dynamic bayesian network dbn 
hidden state variables observation variables noise terms state evolution sensor models implicit fact distributions jq jq gaussian 
nodes noise variables 
state variables discrete model called hidden markov model hmm 
discrete node added model switching kalman lter 
exponential family terminology called canonical characteristics related moment characteristics follows log jxj write equation scalar form exp ij dawid theorem states jz joint density factored fx prove equation 
example hybrid dbns switching kalman lters dynamic bayesian network dw bn model temporal stochastic process 
created specifying network structure parameters consecutive time slices unrolling static network required size 
example show represent linear dynamical system subject gaussian noise 
represents hidden state system time assumed evolve linear equation white noise random vector distribution stationary 
set parameters represents observation vector time assumed linear function hidden state uncorrelated fw 
set parameters task computing probability hidden state past observations pr jy called ltering classical algorithm invented kalman 
task computing probability hidden state observations pr jy yn called smoothing classical algorithm invented rauch 
see bsf bsl details 
kalman lter developed tracking point objects planes missiles 
reasonable represent state position velocity missile single node want track complicated objects people represent complex internal spatial structure object entire network node limb 
jointly gaussian rv replaced entire subnetwork encodes jointly gaussian rv 
resulting network equivalent various matrices sparse 
claim easier exploit conditional independence assumptions learning possibly speeding inference encoded graphically bayes net encoded implicitely sparse matrix 
imagine dynamical system di erent modes represent means discrete variable shown 
example set parameters plane cruising called jump linear system corresponding inference algorithm switching kalman lter 
state evolution equation sensor model equation brie discuss computational issues involved performing inference hybrid dbns section 
inference shall discuss perform inference hybrid networks join tree algorithm ls jen works undirected markov trees :10.1.1.18.5213
similar results derived directed trees pea ps aa dm 
start reviewing discrete case show generalize handle gaussian networks nally hybrid networks ls lw lau ole lau :10.1.1.18.5213
abc bc bcd cd cde de def original dag 
show di erent moralized triangulated graphs 
dotted arcs denote arcs introduced moralization 
dashed arcs denote arcs introduced triangulation 
join tree produced 
squares denote separators ellipses denote cliques 
di erent triangulations correspond elimination orders respectively 
example rst ordering eliminate ensure neighbors lower ordering mutually connected adding edge 
similarly eliminating connect pure discrete case associate potential function clique joint probability variables evidence 
discrete case potential functions represented multidimensional tables discuss continuous case 
associate potential separator separator intersection cliques ends arc separator attached write complete joint probability distribution pr prs set cliques set separators joint universe xn 
example referring assuming evidence pr pr pr pr pr pr pr pr ajb pr bjc pr cjd pr follows separation implies independence property undirected graphical models separate moralized triangulated graph pr cjd pr pr continuing way pr bjc pr pr nally pr ajb pr pr 
suppose evidence arrives node observe value need update potentials re ect fact 
variable nd clique contains parents call clique representative say assigned 
example representative 
update prc assigning zero probability combinations inconsistent 
discrete case just set table entries discuss continuous case 
gives pr pr evidence event 
need change potentials 
idea clique sends message neighbors absorb update potential re ect new piece information way shall explain shortly 
clique node received messages neighbors bar may send send message 
way clique eventually gets updated global consistency restored 
centralized version message passing protocol follows pick root pivot node inducing directionality tree 
rst pass nodes send messages receiving children postorder second pass root sends messages leaves preorder 
rst pass called collect evidence phase second pass called distribute evidence phase 
denote evidence subtree rooted denote rest evidence rst pass clique potential contains pr second pass clique potential contains pr pr 
passes recover posterior marginal family nding clique contains marginalizing variables clique 
compute marginal set variables contained clique see xu 
tree caterpillar chain structure algorithm identical forwards backwards algorithm hmms shj :10.1.1.18.5213:10.1.1.150.82
things remain speci ed initialize update clique potentials 
initialize potential clique multiplying cpts variables assigned example assign get pr pr pr bja pr cja pr 
similarly assign get pr pr djb 
separators initialized 
forward pass backward pass result clique potential contains joint probability member variables 
absorption update process best illustrated example 
referring suppose observe compute pr 
update potential pr write pr pr pr follows conditional probability pr xed constant independent evidence pr potential separator computed marginalization pr pr 
conditional probability computed pr pr pr summary neighbors separator absorbs perform steps calculate pr ns pr give new potential pr give new potential pr pr prs require prs value set pr 
tree satis es requirement called supportive 
pure gaussian case discrete case potential clique represented table 
gaussian case potential represented gaussian function moment canonical form 
turns operations easier express terms canonical characteristics easier express terms moment characteristics 
initialization lauritzen spiegelhalter algorithm clique potential initialized product conditional distributions nodes assigned clique 
node assigned exactly clique contain family 
forwards backwards pass tree clique potential joint distribution member variables 
call virgin potentials 
ready incorporate evidence 
unfortunately may able represent initial potential initial forwards backwards pass tree moment form 
reason mean may depend values variables assigned clique node assigned clique initial potential form jy mean depends represent initial potentials canonical characteristics convert moment form necessary section 
vector node conditional distribution form xjz exp exp log set canonical characteristics log log generalizes result lau vector case 
scalar case log bb canonical characteristics compute initial potentials clique multiplying potentials associated variable assigned clique 
unfortunately convert canonical characteristics moment characteristics full rank invertible 
easy see scalar case contains outer product rank 
entering evidence observe continuous variable takes speci value modify potentials cliques separators contain dimensionality reduced 
clique contain new potential exp hx kxy exp hx kxy generalizes equation lau vector case 
compute analogous result moment characteristics follows 
start just considering quadratic form kxy expanding kxy kxy kxy kxy kxy def ax rule called completing square ax yield log log multiplication division discrete case multiplication division update potentials new evidence arrives pr pr prs separator clique 
notice prs pr js really computing distribution multiplying new information 
de ne multiplication division gaussian case terms canonical characteristics follows 
multiply xn extend domain xn adding zeros appropriate dimensions compute support new function intersection previous supports 
division similar de ne 
marginalization potential set variables 
compute potential subset variables marginalizing denoted having dimension having dimension shown completing square nice properties multidimensional gaussians 
dy log log jk log log jk moment case things simpler 
simply extract components relate change constant normalizes new distribution 
hybrid case change hybrid case potential functions continuous discrete nodes 
essentially set canonical moment characteristics value discrete nodes 
operations go marginalization 
marginalize continuous nodes proceed section value discrete nodes 
marginalize discrete nodes mean variance depend just sum appropriate constants value called strong marginalization 
mean variance depend get mixture gaussians simpli ed kept list terms 
arrange things integrate continuous nodes discrete nodes depend write 
achieved ensuring continuous nodes eliminated discrete ancestors 
node elimination ordering called strong triangulation 
unfortunately case hybrid dbns need eliminate nodes discrete ancestors clashes desire eliminate nodes slice eliminate slice 
don strong triangulation number mixture components exponential length sequence 
standard approach see tsm bsl kim wh collapse mixture components 
corresponds computing weak moments give correct mean variance pr yji pr jji yji ji pr jji var yji var yji ji var yji ji lauritzen lau shows best approximation kl sense :10.1.1.18.5213
learning section discuss nd maximum likelihood estimates parameters associated node 
assume set training examples example assigns value node network called fully observable case 
section address issue values variables unknown 
assume parameters node independent nodes maximize separately 
terms joint distribution depend involve parents just need compute sucient statistics family 
discrete linear gaussian mixtures linear gaussian distributions exponential family deg bun lau size sucient statistics need keep equal size parameter vector independent :10.1.1.18.5213
fully observable case discrete case discrete variable parameter vector ijk pr just table numbers 
sucient statistics ijk number times event pa occurs training set 
ijk pr pa pr pa ijk ij ij ijk mle ijk ijk ij gaussian case approach adopt model joint distribution node parents forming family compute sucient statistics nd mle parameters 
discuss simplest case general results see mur 
sucient statistics seeing examples def qn def qn simple update sucient statistics see example xn compute parameters node sucient statistics family linear regression follows 
represent child parents conditional density jx jx jx local parameters node zz zy broken individual blocks parent 
hybrid case exact posterior distribution hybrid potential mixture gaussians 
approximated single gaussian performing weak marginalization 
general arbitrarily bad approximation may replacing multimodal distribution unimodal 
suppose error introduced step 
results bk bk show hybrid dbn total error function mixing rate markov chain independent alternative approach learning hybrid dbns taken gh maximize exact lower bound likelihood produced considering tractable approximation original structure :10.1.1.18.5213:10.1.1.30.5334
partially observable case observe value node training case longer closed form expression mle 
section investigate methods learning circumstances 
methods passes training data update parameters pass reach local maximum likelihood space batch methods 
easy convert incremental online versions update parameters seeing subset training set see nh incremental em bc incremental gradient descent 
em basic idea expectation maximization em algorithm ll missing values expected values expectation current set parameters expected sucient statistics ess computing mle 
parameters set mle values process repeats likelihood stops increasing proved em converge local maximum 
discrete case ess ijk pr pa jje pr pa pr gaussian case ess je qn je var xx hybrid case just compute bot kinds ess discrete parent value 
em algorithm detail 

choose random starting values parameters node 
broad covariance idea samples far mean assigned unduly low likelihood 

repeat reset ess node 
reset log likelihood 
training case update log likelihood log pr 
ii 
compute posterior marginal family evidence 
iii 
update ess family 
compute mle parameters family ess 

converges 
steps ii computed inference algorithms discussed earlier 
gradient descent possible compute expression gradient log likelihood xj gradient learning methods 
maintain constraints parameters 
particular continuous nodes remain symmetric positive de nite discrete nodes ijk lie inside unit cube surface ijk 
best way maintain constraints problem solve problem unconstrained space convert back 
cpt entries learn parameters softmax function bc 
learn parameters unconstrained set em technically rst order method better nominally faster gradient methods conjugate gradient quasi newton xj :10.1.1.18.5213
primarily em avoids line search iteration expensive requires computing log likelihood points line evaluation requiring call inference engine 
see techniques levenberg marquardt approximate hessian don require line search perform 
priors compute map estimate avoid tting little training data priors compute map estimates ml estimates 
suitable prior discrete nodes dirichlet prior simple intuitive interpretation terms pseudo counts just imagine seen certain number ijk cases event pa add real counts 
vector gaussian case things little complicated 
simpler associate prior distribution family parameters bx node 
suitable prior normal wishart gh deg 
important takes lot data ensure positive de nite 
aa agogino :10.1.1.18.5213
inference message propogation topology transformation vector gaussian continuous networks 
uai 
bc baldi chauvin 
smooth learning algorithm hidden markov models 
neural computation 
boutilier friedman goldszmidt koller :10.1.1.18.5213
context speci independence bayesian networks 
uai 
bk boyen koller 
approximate learning dynamic models 
nips 
bk boyen koller 
tractable inference complex stochastic processes 
uai 
binder koller russell kanazawa 
adaptive probabilistic networks hidden variables 
machine learning 
bsf bar shalom fortmann 
tracking data association 
academic press 
bsl bar shalom li 
estimation tracking principles techniques software 
artech house 
bun buntine 
operations learning graphical models 
ai research pages 
cowell dawid lauritzen spiegelhalter 
probabilistic networks expert systems 
springer 
deg degroot 
optimal statistical decisions 
mcgraw hill 
dm driver 
implementation continuous bayesian networks sums weighted gaussians 
uai pages 
dw thomas dean michael wellman 
planning control 
morgan kaufmann 
edwards 
graphical modelling 
springer 
fg friedman goldszmidt :10.1.1.18.5213
learning bayesian networks local structure 
uai 
gh geiger heckerman 
learning gaussian networks 
uai volume pages 
gh ghahramani hinton :10.1.1.18.5213:10.1.1.30.5334
switching state space models 
technical report crg tr dept comp 
sci univ toronto 
jen jensen :10.1.1.18.5213
bayesian networks 
ucl press london england 
jensen jensen 
uence diagrams junction trees 
uai 
kim 
kim 
dynamic linear models markov switching 
econometrics 
computational scheme reasoning dynamic probabilistic networks 
uai 
lau lauritzen 
propagation probabilities means variances mixed graphical association models 
jasa december 
lau lauritzen :10.1.1.18.5213
graphical models 
oup 
lj lauritzen jensen 
stable local conditional gaussian distributions 
technical report dept math 
sciences uni 
ls lauritzen spiegelhalter 
local computations probabilities graphical structures expert systems 
stat 
soc 

lw lauritzen wermuth 
graphical models associations variables qualitative quantitative 
annals statistics 
mh meek heckerman 
structure parameter learning causal independence causal interaction models 
uai pages 
mur murphy 
fitting conditional gaussian distribution 
technical report berkeley dept comp 
sci 
mur murphy 
variational approximation bayesian networks discrete continuous latent variables 
uai 
nh neal hinton 
new view em algorithm justi es incremental variants 
jordan editor learning graphical models 
mit press 
ole olesen 
causal probabilistic networks discrete continuous variables 
ieee trans 
pattern analysis machine intelligence 
pea pearl 
probabilistic reasoning intelligent systems networks plausible inference 
morgan kaufmann 
ps peot shachter 
fusion propogation multiple observations belief networks 
arti cial intelligence 
rg roweis ghahramani :10.1.1.30.5555
unifying review linear gaussian models 
neural computation 
shj smyth heckerman jordan :10.1.1.18.5213:10.1.1.150.82
probabilistic independence networks hidden markov probability models 
technical report msr tr microsoft research 
sk shachter 
gaussian uence diagrams 
managment science 
tsm titterington smith makov 
statistical analysis nite mixture distributions 
wiley 
wh mike west je harrison 
bayesian forecasting dynamic models 
springer 
whi whittaker 
graphical models applied multivariate statistics 
wiley 
xj xu jordan :10.1.1.18.5213
convergence properties em algorithm gaussian mixtures 
neural computation 
xu xu 
computing marginals arbitrary subsets marginal representations markov trees 
ai 

