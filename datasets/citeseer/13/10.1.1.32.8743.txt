sequential particle lter method static models nicolas laboratoire de statistique crest timbre paris cedex france fr december particle lter methods complex inference procedures combine importance sampling monte carlo schemes order consistently explore sequence multiple distributions interest 
purpose article show methods er ecient estimation tool static setups case jy posterior distribution interest preliminary exploration partial posteriors jy computing time savings possible 
complete black box algorithm proposed independent markov models 
method shown possibly challenge common estimation procedures terms robustness execution time especially sample size important 
classes examples discussed illustrated numerical results mixture models discrete generalized linear models 
key words generalized linear models hastings metropolis importance sampling importance sub sampling mcmc mixture models parallel processing particle lter methods 
mcmc monte carlo markov chains methods common tool nowadays bayesian inference handle large variety models 
unfortunately dynamic setups sequence posterior distributions involved mcmc techniques fail er quick manageable resolution need generate di erent chain run posterior take account previous generations alternatively authors developing ecient methods importance sampling iterative strategies 
context inference easily reused draw inference proper reweighting operation 
methods usually referred particle lter methods doucet 
purpose show methods improve estimation static setups single posterior distribution jy involved 
fact initiated concern common simulation procedures hardly implementable huge datasets consist numerous iterations requiring complete browsing observations iteration needs access compute sample 
internal structure clearly timeconsuming 
propose alternative structure provides signi cant computational savings performing preliminary explorations partial distributions jy rst inference drawn rst observations second time inference updated importance sampling take account observations 
strategy iterated times nally infer sample stages required browse small part observations sample 
sense endow static model arti cial dynamics observations considered arrive sequentially distinct times smoothly incorporated previous inferences 
dynamics particle lter applied sequence partial posterior distributions jy observations supposed drawn parametric family fp open space 
notation refer sequence observations partial posterior distribution jy denoted stand complete posterior distribution 
structured follows 
section recalls main properties particle lter methods 
section presents importance sub sampling iss scheme consists importing sampling applied partial posteriors section details isis algorithm importance sub sampling iterated scheme particle lter method dedicated estimating expectations fh cases observations independent markov 
section gives examples applications generalized linear models mixture models 
particle filters importance sampling particle system sequence weighted random variables particle weight targets distribution interest sense lim fh surely holds measurable fh exists 
particles drawn instrumental distribution weights proportional give particle system target operation called importance sampling 
ratio known multiplicative constant cancelled denominator resulting estimate biased consistent fh 
central limit theorem valid particles independently drawn general hypotheses instance obtained reversible markov chain stationary law madras 
sensible consider quantity measure eciency importance sampling instrumental speak clearly measures close estimation fh possible 
quantity normalized dividing fh order remove variability due studied phenomena 
de ne fh note normalized measure eciency directly related carpenter 
de nition ective sample size sample size required attain precision particles directly drawn target distribution ective sample size order quantity 
properties sucient conditions niteness geweke sup fh iterated applications importance sampling major advantage particle systems exibility simple reweighting operation shifts target particle system liu chen gilks 
ratio called incremental weight 
dynamic setting distribution interest evolving time reweighting scheme iterated unfortunately reweighting phase introduces bit variability estimates moving away typically particles signi cant weights get numerous mass considered distribution leave support initial distribution refers distribution particles initially drawn 
words may dangerously increase successive reasonable values 
particle system suffers progressive degeneracy 
reweighting steps alternated resampling steps particle replaced number replicates may equal 
new particles assigned weight equal 
determined various ways common multinomial selection gordon 
new particle drawn multinomial distribution random selection keep estimates unbiased 
random 
deterministic selection stands integer part 
selection preserve estimates avoid extra variability induced random selection remains asymptotically valid doucet see 
stressed resampling protect degeneracy just saves calculation time getting rid particles insigni cant weight 
resampling arti cially conceals system replacing high weights numerous replicates unique particle introducing high correlations particles 
liu chen doucet reviews methods related iterations reweighting resampling steps 
gilks proposed add resampling step rejuvenation step 
resampled particles stage moved markov chain transition kernel stationary distribution 
operation change system target may strongly reduces system identical replicates single particle replaced new fresh values 
eciency rejuvenation step obviously relies sensible choice assessing eciency kernel may dicult task practice 
particle lter methods refer algorithms provide consistent inferences sequence distributions iterating steps 
reweighting compute 
resampling resample 
selection scheme 

move draw transition kernel stationary distribution results convergence algorithms doucet 
importance sub sampling iss aim estimate parameter xed dimension observations drawn static model contrast dynamic models parameters may vary observations posterior distribution interest jy 
possible endow static model dynamical structure 
suppose rst observations available rst 
inference procedures managed subsample involving instance simulations posterior jy generally particle system target jy 
assume new observations available 
provided large jy jy similar take advantage rst inference results jy shorten calculation cost incorporating new data proper reweighting particles incremental weight jy jy jy call particular case importance sampling importance subsampling iss 
give important properties highlight interest iss 
provided jy bounded clearly weak condition bounded ensures nite fh exists 
particle system introduced importance sub sampling approximately evaluated asymptotic result theorem regularity conditions listed appendix integrals exist proof appendix 
see large relative precision iss depends proportion new data proportion relative precision may attained sucient amount particles depend important cases reweighting step operates new data avoiding second complete browse rst part data observations independent jy markov exists integer jy jy jy jy cases reweighting step quick update particle system possible provided related likelihoods jy computable 
cases iss interesting calculation jy may intensive grows 
assume observations markov independent adopt common notation cases jy jy independent case stand 
isis algorithm isis algorithm importance sub sampling iterative scheme particle lter method devoted iterated applications importance sub sampling 
consists iterations steps 
initialization generate particle system targets initial distribution 
reweighting compute jy 
resampling resample particle system 
selection scheme see 

move generate transition kernel stationary distribution 
loop back 
algorithm stops particle system targets distribution interest jy 
complete de nition algorithm features speci ed subsections choice transition kernel move determination new observations handle step 
practical choice particles needed achieve precision 
speci cations calibrated important considerations 
robustness estimates avoid volatile results algorithm check eciently restrict system degeneracy stage 

low execution time alternatively need avoid extra computational costs may induced instance severe settings regard degeneracy high frequency move steps 

black box feature isis algorithm internal machinery depend considered model 
user just supply likelihood calculation routine needed reweighting step particle system initialized instance prior de nes true probability distribution 
subsections devoted important features isis algorithm appear clear advantages estimation procedures 
move 
eciency move step critical computationally demanding step requires complete browsing past observations steps 
mcmc techniques usually involve numerous applications transition kernel single particle 
contrary move step particle lter methods consist single application kernel large set particles 
choice kernel rely di erent criteria usually mentioned mcmc settings theoretical convergence instance ensure particles move eciently single application kernel 
assessing correctly system really easy task practice example hastings metropolis hm kernel may assess rejuvenation acceptance rate considering accepted particles new particles introduced system greater number new particles better 
assume proposed value generated random walk random walk high acceptance rates arti cially achieved imposing low variance random step case identical replicates single particle replaced equivalent number particles distinct close values highly correlated 
system remains high detectable longer 
sensible select transition kernel depends weakly previous value moved particle 
case independent hastings metropolis kernel ihm proposed particle generated independently instrumental distribution moved particle depends previous value acceptance prob ability 
ihm kernel acceptance rate far reliable eciency indicator 
rejuvenation performed independent kernel browsing past subsample needed computation appears acceptance probability 
provided ihm kernel high acceptance rates obtained instrumental distribution close target distribution sole information get particle system provide current stage estimate expectation fh particular estimates gf give rough localization mass instrumental distribution reasonable choice simple space dimension important easily take account correlations components model dependent ful lls black box requirement asymptotically justi ed tends normal distribution 
point essential said rejuvenation step intensive fortunately gets ecient time 
nite range complex posterior distribution signi cantly di er gaussian distribution featuring instance local modes thick distribution tails 
case note posterior distribution tails easily reduced proper instance density decreases nity exp get thinner gaussian tails replacing model take time user deriving distinct instrumental distribution tails order considered target distribution 
appeal black box approach 
instrumental distribution chose suspected hardly move particles highly target distribution 
see settings method lead satisfactory results see 
reasons see convenient purpose instrumental distribution greater eciency expected re ned case restricted instrumental distributions 
new observations handle step 
clear increasing functions support progressively shrinks increases comparatively initial support added observation bit particle system 
algorithm smoothly incorporate new observations reweighting step transitive operation 
increase amount added observations regularly checking degeneracy process degeneracy level attained 
derive subsection degeneracy criterion stopping rule incorporation observations 
notice ord cautious criterion imply high frequency move steps said steps intensive 
due particular choice instrumental distribution see information draw particle system intermediary stage summarized estimates quantities localize region high probability explored precisely adjunction new points 
need check variance estimates suciently low prevent instrumental distribution moving step missing target 
preliminary note kullback leibler distance normal distributions 
ln taylor expansion order 
small quantity acceptable indicator similarity particular considered ecient instrumental law hastings metropolis kernel indicator roughly measure point kernel may keep comparable eciency replace suppose stand respectively 
case unimodal setting argue optimal instrumental distribution moving restricted class gaussian distributions assume previous subsection acceptable choice 
moments directly available replace may unable move eciently far quantities computed directly 
order magnitude respectively write derive degeneracy indicator quantity adopt corresponding criterion increase arbitrary threshold numerical applications 
de ned indicator rst unidimensional setting sake simplicity 
replacing variances norms natural generalization multidimensional case quantity directly estimated contrary variance terms easily evaluated successive reweighting move steps joint distribution particles quite intractable 
address issue propose replace variances empirical approximations 
notice rst particle system unique particles generated accepted move replicated particles resampled replicates particles move accepted 
particle system represented sequence pairwise distinct properly weight particle system practice new system corresponds previous weight times number replicates 
denote corresponding normalized weights assume rst approximation couples independent random variables 
variance estimated fh approximation gf deduce obviously new indicator rough empirical 
assumption independence far reality mentioned independent hastings metropolis kernel move step strongly limits dependence distinct particles 
kernel moved particle conditionally fact successfully moved independent construction 
new value particle set drawn independently stage nearly independent completely true drawn instrumental distribution depended moments computed particle system dependence clearly weak 
words assume main part estimates variance due particles replication reweighting step move step successful provides brand new particle 
theorem indicates level degeneracy induced reweighting step asymptotically proportion new points apply criterion stage assume constant level eciency move steps expect number new points increase geometric speed 
move steps ecient increases see incorporation may faster 
particles required 
keep mind particle lter methods justi ed asymptotically number particles 
tempting run isis algorithm highest possible values imply tremendous computational cost 
practice choice determining proper calibration 
aim nd problem reasonable value computational terms leads robust estimates 
instance run various times algorithm measure variability obtained results number particles independent case observations shu ed run check results depend order incorporation observations 
note numerical applications faced case degeneracy high estimates variability long exceeded reasonable bound confronted complex posterior distributions mixture posterior distributions parameter space high dimension 
computational considerations parallel processing issue isis algorithm explores parameter space signi cantly distinct way compared estimation procedures tracks numerous particles smoothly browsing sample observations estimation algorithms track single particle numerous iterations requiring complete browsing dataset particle referring single parameter value varying iteration 
distinct exploration strategy strongly limits number accesses dataset bring important computational savings obviously sample size large 
fact isis algorithm able roughly localize region interest drawing information small part sample exploring precisely competitors need perform computations sample achieve localization 
distinct exploration structure strongly facilitates parallel processing implementation 
parallel processing refers possibility share execution time algorithm various processors computers 
computational savings may signi cant ideally factor processors provided considered algorithm divided routines running independently 
case numerous data exchanges processors may strongly slow processing cancel saving 
estimation procedures hardly processed parallel consist sequence dependent iterations iteration time divided independent computations corresponding results need gathered order initiate iteration time 
frequent exchanges processors occur 
contrary isis algorithm tracks numerous particles reasonable amount iterations 
parallel processing easily implementable partition particle system subsets dedicate computations particles processor partitioning occur distinct stages reweighting step processor computes incremental weights particles move step processor moves particles 
resampling step intensive really need partitioning 
steps reweighting move results gathered moving step 
isis algorithm ability take account new data nearly geometric speed see frequency move steps quickly decreases grows 
frequency required information exchanges processors decreases exponential speed making parallel processing reweighting move steps quickly highly ecient 
economies scale run properly isis algorithm need supplied routine computing partial likelihood jy value 
fact isis algorithm call intensively routine particle added observation reweighting step order compute corresponding incremental weights move step order compute acceptance probabilities kernel 
computational feasibility isis algorithm directly depends execution time routine 
certain cases jy may appear rst sight complex likelihood particularly expressed integral 
economies scale fact easily achievable practitioner keep mind cases evaluations jy cleverly performed lower time times time needed evaluate quantity 
illustrative examples likelihood involves costly function computational terms 
store preliminary evaluations rst derivatives grid points replace computation appropriate taylor approximation 
likelihood involves integral dz function di ers evaluation 
integral evaluated amount evaluations integrand 
may stored rst time computed reused necessary 
numerical recipes applied generalized linear models see show computational savings far anecdotal 
economies scale may workable cases appeal isis algorithm approach 
isis latent variable context suppose considered model involves latent variable structure say observations may related unobserved zg models inference obtained methods gibbs sampler em algorithm explore augmented space parameters latent variables successive moves dimension augmented space dimensions keep values 
fact greater dimension augmented space may dicult explore eciently 
algorithms imply greater adaptation cost terms practitioner time intermediary movements latent variables dimensions involve speci model gibbs sampler instance practitioner derive model distinct scheme perform intermediary latent variable simulations 
words algorithms black boxes isis algorithm 
sample size important complete browsing sample iteration stressed costly cases involves iteration storages recalls latent variables values 
large databases may dicult handle methods age great computational power availability 
settings algorithm recommended provided marginal likelihood dz computed 
examples generalized linear models generalized linear models mccullagh nelder relate independent observations covariates exponential density form exp canonical parameter determined linear relation known function called link function 
row vectors covariates form full rank matrix may discrete continuous 
generalized linear models usually estimated mcmc techniques gibbs sampler appears times natural choice especially latent variable structure exhibited binary regression instance 
simulating conditional distributions constitute gibbs sampler feasible restrictive cases probit model albert chib 
cases en vn table estimates simulated probit model gibbs sampler may er inference arti cial distribution approximates distribution interest 
contrary isis algorithm handle directly generalized linear model yjx computable 
exponential nature density model leads regular unimodal posterior distribution making gaussian instrumental distribution isis algorithm able move eciently particles early stages see 
rst consider probit model yjx case yjx algorithm perform numerous evaluations function 
order get substantial economies scales reduce execution time implement taylor approximation method log computing pro table logarithm transforms products sums 
method provides approximate absolute error performing sums products initial evaluations grid points 
note absolute error correspond relative error magnitude 
estimated probit model covariates 
observations sample simulated true model parameter 
covariates simulated independent standard normal distributions rst row constant 
table gives resulting estimates numbers parenthesis give standard deviation results runs 
see reasonable number particles estimates relatively robust 
figures give evolution acceptance rate elapsed execution time acceptance rate execution time amount data incorporated 
stated move steps frequency decreases quickly move steps correspond vertical parts time plot acceptance rates close early stages 
note adapting algorithm logit model immediate replaced model gibbs sampler implemented directly albert chib 
nally consider generalized dimensions probit model jz easily shown jx dt stands standard gaussian density possible values likelihood takes similar form 
computing likelihood economies scales achieved see 
integral may evaluated classical integration methods romberg integration require multiple evaluations integrand grid points need evaluated evaluations may accelerated taylor approximation 
computation time yjx may remains costly making parallel processing implementation strongly recommended model 
mixtures mixture models take form lg unobserved independent 
aim estimate discarded 
example take 
constraint parameters needed fully identify model 
celeux 
showed exploration unconstrained posterior distribution manageable lead satisfactory results approach compatible algorithm transition kernel gaussian instrumental distribution hardly move target distribution 
equivalent modes 
latent variables discrete likelihood jy easily computable sum possible states 
diculties mixture models related calculation issues previous example complexity posterior jy involves great number local modes large dimension space making standard algorithms unable converge 
re ned methods gibbs sampler theoretically converges region highest probability lead trap modes practice robert mengersen 
concern see algorithm able manage target distribution move steps performed normal instrumental distribution see 
constraint chosen standard errors log order lower posterior distribution tails see 
correct choice prior distribution sensitive issue mixture modeling see instance 
aim address problem restrained simple reasonable prior uniform distribution compact set corresponding constraints adapted considered sample examples took min max log histogram density solid line isis estimate true density dashed line simulated sample densities corresponding isis estimates mixture models resp 
components stamps dataset log min jy 
algorithm rst applied simulated data 
contrasts density corresponding isis estimate true density simulated data mixture model components 
rst density clearly ts data 
algorithm applied known dataset rst analysed sommer consists observed stamp thicknesses mexican issue various numbers mixture components see 
results appear quite satisfactory agreement authors sommer robert mengersen recommend component mixture modeling 
isis algorithm quick ecient method evaluating estimates general context 
theoretically handle model independent markov observations 
practice seen cope complex multidimensional posterior distributions mixture posterior distributions 
black box sense main adaptation cost problem reduces supplying likelihood calculation routine adapted considered model gibbs sampler instance 
advantage isis algorithm computational eciency rival algorithms need complete browsing observations iteration isis algorithm strongly limits number access large parts sample 
isis algorithm may outperform estimates evaluation methods large datasets computational terms 
isis algorithm allows parallel processing implementation clear advantage competitors 
bene ted valuable discussions christian robert judith rousseau 
rst part ph thesis partially supported eu tmr network erb ct program research kitchen workshop de lans january 
dedicate wife months 
appendix proof theorem parameter space open space realizations model 
simplify notation summarize dependence observations single subscript instance refer likelihood log likelihood jy 
need conditions ful lled considered observations supposed belong corresponding set probability 
existence mle arg max convergence 
positive de niteness matrix convergence fisher information matrix 
existence 

lim sup sup 
sup bounded compact set bound depend observations 
assumed times continuously di erentiable 
rst give versions laplace expansion applied containing likelihood 
full proof result customized needs arguments reused 
laplace expansion johnson tierney 
schervish 
lemma laplace expansion det proof 
take small 
see condition assume instance replace de ne log 
gd gd gd second integral clearly exponentially negligible gd replaced gd 
perform taylor expansion log jr sup nm bound corresponds condition sup log 
write nr bounded quantity instance 
log de nite positive large condition holds 
expf log nr expf expf log nr 
leading term expansion available expf gd det replace show expf gr expf gd 
expf gr tn expf expf ktk log dt ktk log ktk ktk sup log write expf gr tn expf ktk ktk ktk ktk dt 
take integer max ktk time kt ktk ktk ktk ktk ktk ktk ktk increasing function 
tn expf ktk ktk ktk ktk dt expf ktk ktk ktk dt second integral nite small condition take small needed clearly bounded converges condition 
expf gr show way expf gr starting get expf gr apply result terms form 
lemma exist 
proof 

assume rst 
lemma det det 
asymptotic expansion third integral performed similar arguments lemma 
provided neighborhood log write expf gd expf expf expf expf log log nr bounded function 
check expf gr 
kind variable change previous lemma 
second term clearly bounded exhibit bound kind expf ktk expf ktk ktk dt get det det combining integrals det det det converges 
case write ae get result 
deduce lemma proof theorem 
theorem regularity conditions listed appendix conditions integrals exist proof 
known fh fh show 

assume rst case multidimensional notice cov see result obtained smart combination terms asymptotic expansion lemma 
particular nally get result easily generalized multidimensional case computing cross terms cov albert chib 

bayesian analysis binary response data 
amer 
statist 
assoc 
carpenter cli ord 

improved particle lter nonlinear problems 
iee proc radar sonar navigation 
celeux robert 

computational inferential diculties mixture posterior distributions 
amer 
statist 
assoc 
doucet 

convergence sequential monte carlo methods 
technical report cambridge university cued tr 
doucet de gordon 

editors sequential monte carlo methods practice 
springer verlag new york appear january 
geweke 

bayesian inference econometric models monte carlo integration 
econometrica 
gilks 

moving target monte carlo inference dynamic bayesian models 
roy 
stat 
soc 
appear 
gordon salmond smith 

novel approach nonlinear non gaussian bayesian state estimation 
amer 
statist 
assoc 

sommer 

mixtures multimodal densities 
amer 
statist 
assoc 
johnson 

asymptotic expansions associated posterior distributions 
ann 
math 
stat 


convergence accuracy gibbs sampling conditional distributions generalized linear models 
ann 
stat 
liu chen 

sequential monte carlo methods dynamic systems 
amer 
statist 
assoc 
madras 

importance sampling families distributions 
ann 
applied prob 
mccullagh nelder 

generalized linear models 
chapman hall 
robert mengersen 

issues mixture modelling bearing mcmc algorithms 
comp 
stat 
data anal 
schervish 

theory statistics 
springer verlag 
tierney kass kadane 

fully exponential laplace expectations variances nonpositive functions 
amer 
statist 
assoc 

