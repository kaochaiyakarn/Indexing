kernel pca feature extraction de noising non linear regression roman mark girolami leonard andrzej cichocki computational intelligence research unit school information communication technologies university paisley paisley pa scotland mail ci ci paisley ac uk computational sciences division nasa ames research center mo ett field ca mail mail arc nasa gov laboratory advanced brain signal processing brain science institute riken shi japan mail cia brain riken go jp propose application kernel principal component analysis pca technique feature selection high dimensional feature space input variables mapped gaussian kernel 
extracted features employed regression problems chaotic mackey glass time series prediction noisy environment estimating human signal detection performance brain event related potentials elicited task relevant signals 
compared results obtained kernel pca linear pca data preprocessing steps 
human signal detection task report superiority kernel pca feature extraction linear pca 
similar linear pca demonstrate de noising original data appropriate selection various non linear principal components 
theoretical relation experimental comparison kernel principal components regression kernel ridge regression insensitive support vector regression provided 
key words feature extraction principal components non linear regression kernel functions de noising human performance monitoring 
real world applications appropriate preprocessing transformations high dimensional input data increase performance algorithms 
general exist correlations input variables dimensionality reduction called feature extraction allows restrict entire input space sub space lower dimensionality 
study proposed kernel principal component analysis pca method feature selection high dimensional feature space dimension 
allows obtain features nonlinear principal components higher order correlations input variables addition extract nonlinear components number data points assuming 
kernel pca computation standard linear pca feature space input data mapped nonlinear function 
compute canonical dot product space kernel function 
kernel trick allows carry algorithm support vector regression svr expressed terms dot products space selected features train insensitive svr see detailed description kernel principal components regression kpcr models estimate desired input output mappings 
techniques perform linear regression feature space di erent cost functions 
whilst insensitive cost function svr robust noise distributions close uniform case gaussian noise best approximation regression provides quadratic cost function 
applying quadratic cost function svr leads kernel ridge regression krr 
krr kpcr shrinkage estimators designed deal near linear dependence regressors see 
results large variances covariances squares estimators regression coecients dramatically uence ectiveness regression model 
give theoretical basis kpcr highlight relation krr 
noisy environments linear pca widely de noising technique 
discard nite variance due noise projection data main principal components 
technique applied feature space main nonlinear principal components computed kernel pca 
number nonlinear principal components extracted kernel pca substantially higher number data points 
nearly advantageous especially situation dimensionality input data points signi cantly smaller number data points data structure spread 
case decreasing input dimensionality projecting input data main linear principal components may lead loss signi cant amounts information 
hand believe spreading information data structure nonlinear principal components give potential discarding noisy part data mainly contained 
data sets chaotic mackey glass time series human evoked related potentials erps compared kpcr krr svr techniques 
demonstrate selection subset nonlinear principal components kpcr achieve superior similar results compared krr case kpcr nal linear model feature space signi cantly smaller 
erps data set results suggest superiority kernel pca feature extraction linear pca cases 
addition performance kpcr krr models quadratic loss function slightly superior svr 
suggests particular data set gaussian type noise regression models quadratic loss function preferable 
section presents kernel pca technique linear regression models high dimensional kernel de ned space 
problem de noising data set kernel space addressed 
section construction data sets employed described 
section discusses results 
section concludes 
methods kernel pca multi layer svr pca problem high dimensional feature space formulated diagonalization sample estimate covariance matrix centered nonlinear mappings input variables centralization mapped data appendix assuming svr model insensitive cost function 

diagonalization represents transformation original data new coordinates de ned orthogonal eigenvectors nd eigenvalues non zero eigenvectors satisfying eigenvalue equation cv realizing solutions lie span mappings sch olkopf derived equivalent eigenvalue problem denotes column vector coecients symmetric gram matrix elements ij normalizing solutions corresponding non zero eigenvalues matrix translates condition 
compute projection th nonlinear principal component select rst nonlinear principal components directions describe desired percentage data variance dimensional sub space feature space allows construct multi layer support vector machines preprocessing layer extracts features regression classi cation task 
study focus regression problem 
generally svr problem see de ned determination function approximates unknown desired function form unknown bias term vector unknown coe cients 
regularized risk functional compute unknown coecients svr err fy obtained outputs regularization constant control trade complexity accuracy regression model vapnik insensitive loss function 
shown regression estimate minimizes risk functional form lagrange multipliers 
combining kernel pca preprocessing step svr yields multilayer svr form components vectors de ned 
practice choice appropriate kernel function dicult 
study polynomial kernel rst order employed 
performing linear svr dimensional sub space advantage linear svr ordinary linear regression possibility large variety loss functions suit di erent noise models vapnik proposed insensitive function robust noise distributions close uniform provides sparse solution regression problem 
case gaussian noise best approximation regression provides squares method quadratic loss function form discuss methods loss function section 
feature space regularized squares regression models near linear dependence regressors serious problem dramatically uence usefulness regression model 
results large variances covariances leastsquares estimators regression coecients 
produce estimates regression coecients large absolute value 
values signs estimated regression coecients may change considerably di erent data samples 
ect lead regression model ts training data reasonably general bad generalization model occur 
fact close relation argument stressed authors shown choosing attest function feature space smoothing properties selected kernel function lead smooth function input space 
exist methods deal case discuss ridge regression rr principal component regression pcr approaches 
theoretical basis techniques input space discuss parallel kernel de ned feature space kpcr krr 
kernel principal component regression consider standard regression model feature space vector observations dependent variable matrix regressors th row vector mapped observation dimensional feature space vector regression coecients vector error terms elements equal variance independent 
assume regressors zero mean 
proportional sample covariance matrix kernel pca performed extract eigenvalues corresponding eigenvectors fv projection th nonlinear principal component 
projection original regressors principal components rewrite bw matrix transformed regressors matrix th column eigenvector columns matrix orthogonal squares estimate coecients de ned sense penalizing high values regression coe cients estimate 
diag 
results obtained principal components projection original regressor variables equivalent obtained squares original regressors 
fact express estimate original model corresponding variance covariance matrix cov avoid problem pcr uses principal components 
clear uence small eigenvalues signi cantly increase variance estimate 
pcr simply deletes principal components corresponding small values eigenvalues principal components may appear 
penalty pay decrease variance regression coecient estimate bias nal estimate 
serious problem introduced bias signi cant ect comparison high variance estimate 
elements corresponding deleted regressors zero unbiased estimate achieved 
rst nonlinear principal components create linear model orthogonal regressors feature space formulate kpcr model fc shown removing principal components variances small eliminate large variances estimate due 
orthogonal regressors corresponding principal components large correlation dependent variable deletion undesirable experimentally demonstrated 
di erent strategies selecting appropriate orthogonal regressors nal model see ref 

considered covariance ation criterion model selection kpcr novel alternative methods cross validation 
kernel ridge regression krr technique deal assuming linear regression model solution achieved minimizing rr regularization term 
squares estimate biased variance decreased see 
similar kpcr case express variance covariance matrix estimate cov see contrast kpcr variance reduction krr achieved giving weight small eigenvalue principal components factor 
practice usually know explicit mapping computation high dimensional feature space may numerically intractable 
dual representation linear rr model authors derived formula estimation weights linear rr model feature space 
non linear krr 
fact express nal krr model dot product form gram matrix consisting dot products ij vector dot products new mapped input example vectors training set identity matrix 
worth noting solution rr problem feature space derived dual representation regularization networks see techniques derived gaussian processes 
see including possible bias term model leads penalization term 
case regression classi cation tasks reason penalize shift constant 
overcome add extra bias term linear regression model ectively means new kernel form solution take form unknown coecients fc solving system linear equations new vector ones 
positive de nite kernel change estimate new new terms 
recall solution svr assuming linear regression model feature space leads non linear regression model 
fact authors shown quadratic loss function case svr transforms general quadratic optimization problem nding estimate weights solution linear equations 
technique removing bias term centralize regression problem feature space assume sample mean mapped data targets zero 
lead regression problem bias term 
centralization individual mapped data points done centralization gram matrix vector described appendix solution modi cation form observed approaches provide results 
summing analogy pcr rr input data space connection regularized linear regression models feature space corresponding kpcr krr established 
methods belong class shrinkage estimators shrink ordinary squares solution directions low data spread directions larger data spread 
ectively means achieve desired lower variance estimated regression coecients cost biased estimate 
whilst kpcr project data mainly principal components corresponding larger eigenvalues krr giving weight smaller eigenvalues 
cases faced model selection problem selection non linear principal components kpcr setting regularization term krr respectively 
kpcr straightforward model selection criteria choosing rst principal components describing prede ned amount variance 
methods advantageous noisy environments noise spread corresponding small eigenvalues 
hypothesize situations represent mainly noisy part signal kpcr pro table due data projected 
discuss topic de noising pca section 
pca de noising white additive noise change covariance matrix investigated signal adding diagonal matrix corresponding variances individual noise components diagonal 
case isotropic noise lead increase eigenvalues computed clear signal 
signal noise ratio suciently high assume noise mainly ect directions principal components corresponding smaller eigenvalues 
allows discard nite variance due noise projection data principal components corresponding higher eigenvalues 
nonlinear transformation measured signal consisting signal additive noise noise certain directions 
discarding nite variance due noise lead higher loss signal information deal balance noise reduction information loss 
investigated situation case noisy mackey glass time series nonlinearity induced gaussian kernel 
left see noise increases variance directions smaller eigenvalues decreases variance main signal components 
infer uniform smearing investigated signal directions induced 
cutting directions smaller eigenvalues provide level noise reduction loss information main signal direction appear 
data sample construction chaotic mackey glass time series chaotic mackey glass time series de ned di erential equation ds dt bs 
data generated second order runge kutta method step size 
training data test data range 
generated time series added noise normal distribution di erent levels corresponding ratios standard deviation noise clean mackey glass time series 
human signal detection performance monitoring event related potentials erps performance data earlier study 
male navy technicians experienced operation display systems performed signal detection task 
technician trained stable level performance tested multiple blocks trials separate days 
blocks separated minute rest intervals 
set trials performed subject 
inter trial intervals random duration mean range 
entire experiment computer controlled performed inch color crt display 
triangular symbols minutes arc di erent luminance contrasts constant eccentricity degrees visual angle 
symbol designated target non target 
blocks targets contained central dot non targets 
association symbols targets alternated blocks prevent development automatic processing 
single symbol trial randomly selected position degree annulus 
fixation monitored infrared eye tracking device 
subjects required classify symbols targets non targets button presses indicate subjective con dence point scale button mouse 
performance measured linear composite speed accuracy con dence 
single measure pf derived factor analysis performance data subjects validated subjects 
computational formula pf pf accuracy con dence reaction time standard scores accuracy con dence reaction time mean variance distributions subjects 
pf varied continuously high fast accurate con dent responses low slow inaccurate uncon dent responses 
erps recorded midline frontal central parietal electrodes fz cz pz referred average ltered digitally bandpass hz decimated nal sampling rate hz 
baseline ms adjusted zero remove dc set 
vertical horizontal eog recorded 
epochs containing artifacts rejected eog contaminated epochs corrected 
furthermore trial detection response con dence rating subject excluded corresponding erp 
block trials running mean erp computed trial 
running mean erp average erps window included current trial plus preceding trials maximum trials average 
trial window minimum artifact free erps required compute erp 
fewer available running mean trial excluded 
running mean artifact free erps 
trial window corresponds task time 
pf scores trial averaged running mean window applied erps excluding pf scores trials erps rejected 
prior analysis running mean erps clipped extend time zero stimulus onset time ms post stimulus total time points 
results carried gaussian kernels kx yk determines width gaussian function 
gaussian kernel possesses smoothness properties suppression higher frequency components case priori knowledge regression problem prefer smooth estimate :10.1.1.127.1519
chaotic mackey glass time series noisy chaotic mackey glass time series compared kpcr regressors extracted kernel pca preprocessing krr 
regression models trained predict value time inputs time 
training data partitions constructed moving sliding window training samples steps samples 
window sizes samples samples respectively 
created partitions size samples partitions size samples 
estimated variance clean training set estimate repeated simulations width range step size 
xed test set size data points see section experiments 
regularization parameter krr estimated cross validation training data partitions validation set 
fact nd value cross validation steps 
order estimated ner structure values range order taken estimate optimal value 
performance regression models predict clean time series evaluated terms normalized root mean squared error nrmse 
best results test set averaged individual runs summarized table 
compared results noisy time series dependence width gaussian kernel 
table signi cant di erences noted kpcr krr methods results suggest especially lower level noise kpcr method provides slightly better results smaller variance di erent training data partitions 
relatively small width gaussian kernel observed best performance kpcr test set suggests time series prediction problem kernel pca preprocessing step mainly local correlations data points attractor taken account 
increasing value leads faster decay eigenvalues see potential loss ner data structure due smaller number nonlinear principal components describing percentage data variance 
increasing levels noise tendency increase optimal value parameter coincides intuitive assumption smearing local structure 
signi cant di erence prediction accuracy clean noisy mackey glass time series gives rise question possible suciently reduce level noise kernel space due violation additive uncorrelated essence noise introduced nonlinear transformation 
may potentially stronger ect main principal components see left 
deal trade noise reduction associated signal information loss 
method kpcr krr table comparison approximation errors nrmse prediction di erent sizes mackey glass training set 
values represent average simulations case training points simulations case training points respectively 
corresponding standard deviation parentheses 
represents ratio standard deviation added gaussian noise underlying time series 
kpcr computed training points rst nonlinear principal components corresponding case respectively 
kpcr computed training points rst nonlinear principal components 
solution eigenvalue problem numerically unstable dealing matrix higher dimensionality case 
noisy mackey glass time series observed best performance kpcr achieved main nonlinear principal components 
simply gives rise possibility reduced training data set compute main eigenvalues eigenvectors simply project remaining training data points extracted nonlinear principal components 
experiments compared performance kpcr training data set size estimate main nonlinear principal components approach principal components estimated rst half training data set 
right compare main eigenvalues estimated rst data points computed data points 
small di erence suggest rst half training data set suciently describe sub space feature space generated nonlinear transformation time series 
table compare performance approaches 
observe signi cant degradation performance reduced training data set estimate main principal components 
table see reducing number eigenvectors case clean mackey glass leads signi cant decrease performance nrmse compared results table best performance achieved eigenvectors nrmse 
conjecture case clean mackey glass principal components corresponding small eigenvalues may improve performance adding noise time series principal components negatively affected achieve better results removal 
similar previous discussion leads signal information loss 
extraction smaller subspace nonlinear principal components desired avoid problem direct diagonalization high dimensional gram matrix approaches iterative estimation principal components 
successfully expectation maximization approach kernel pca iteratively estimates subspace main principal components 
human signal detection performance monitoring desired output pf linearly normalized range 
trained models erps tested remaining data 
described results setting parameters average runs di erent partition training testing data 
consistent previous results reported validity models measured terms normalized mean squared error nmse method kpcr kpcr table comparison approximation errors nrmse kpcr method training data points kpcr estimate eigenvectors eigenvalues kpcr method rst half training points kpcr case rest training points projected estimated eigenvectors 
values represent average simulations 
corresponding standard deviation parentheses 
represents ratio standard deviation added gaussian noise underlying time series 
rst nonlinear principal components corresponding case respectively 
terms proportion data pf correctly predicted tolerance test proportion correct tpc case 
performance svr krr methods trained data preprocessed linear pca input space compared results achieved kpcr features extracted kernel pca step compared technique trained selected nonlinear principal components svr technique trained data points pca preprocessing 
parameters values svr models 
case krr regularization term estimated cross validation training data set validation set 
cross validation strategy applied mackey glass time series 
results achieved subject erps erps erps erps erps depicted figures 
figures see consistently better results features extracted kernel pca subjects superior results achieved kernel pca exist approaches selection best subset principal components criterion amount variance described selected principal components 
case linear pca sample covariance matrix estimate principal components 
representation observed remaining subjects 
subject performance features selected linear pca slightly better 
step individual subjects selected results gaussian kernel width krr linear pca preprocessed data kpcr kernel pca preprocessing achieved minimal nmse test set 
boxplot lines lower quartile median upper quartile values whisker plot individual subjects depicted 
boxplots suggest di erences results subjects sign test wilcoxon matched pairs signed ranks test tested hypotheses direction size di erences pairs 
subjects values indicate statistically signi cant di erence results achieved linear pca kernel pca preprocessing steps 
alternative hypothesis regarding superiority leads values 
tests subjects show statistically signi cant di erence results values alternative wilcoxon test superiority leads higher value subject 
note subject smallest number erps available 
indicates weakest results highest variance individual runs 
result suggests number erps subject insucient model desired dependencies erps subject performance 
case dimension matrix feature space lower input dimensionality exploit advantage kernel pca improve performance components feature space number available input space 
demonstrate kernel pca preprocessing step feature space increase performance 
contrary subjects performance method slightly superior 
remaining subjects di erence insigni cant 
case subject number data points input dimensionality svr provides superior results methods considered utilize kernel pca preprocessing 
experiments compared svr krr kpcr methods data set subjects 
split data set erps di erent training erps testing erps data pairs 
training data set cross validation estimate parameters svr krr respectively 
case svr direct solution quadratic optimization problem nd coecients replaced svmtorch algorithm designed deal large scale regression problems 
case kpcr eigenvectors eigenvalues estimated approach em steps 
results reported main nonlinear principal components 
gaussian kernel width 
table summarizes performance individual methods 
see slightly better performance achieved kpcr krr models comparison svr 
results achieved individual subjects results table suggest data set gaussian type noise regression models quadratic cost function preferable 
method nmse tpc kpcr krr svr svmtorch table comparison nmse tpc prediction errors test set model subjects erps 
values represent average di erent simulations 
kernel pca method feature extraction investigated selected features regression problem 
performance monitoring data set half cases demonstrated kernel regression methods nonlinear kernel pca preprocessing step provide signi cantly superior results data preprocessed linear pca 
case indication superiority linear pca observed suciency data representation case questionable 
contrast training odd numbered blocks trials testing numbered blocks trials data pair study created di erent training testing data partitions random sampling blocks trials 
kernel regression models data partitions achieved approximately twice level improvement terms tpc 
quite signi cant improvement biomedical application 
data setting representation discrete wavelet transforms erps reported objective 
shown reduction number nonlinear principal components reduce noise 
similar investigated mackey glass time series prediction task exploited especially situation low dimensional input data spread directions noise reduction projection lower number linear principal components leads information loss 
solution eigenvalue problem numerically dicult case high number data samples 
noisy mackey glass time series demonstrated estimation main eigenvalues eigenvectors sucient smaller data representation 
implies possibility signi cantly reduce computation memory requirements deal large scale regression problems 
situations methods iterative estimation eigenvalues eciently 
data sets employing kpcr selected nonlinear principal components demonstrated comparable performance krr svr techniques 
computational cost approach comparable kernel pca estimation regression coecients requires diagonal matrix inversion order extracted regressors linearly independent advantageous subset selection techniques linear regression 
various strategies see deciding nonlinear principal components delete regression model improve performance proposed kpcr model feature space acknowledgments authors professor colin fyfe helpful discussions comments 
rst author funded research project objective measures depth university paisley glasgow western nhs trust partially supported agency science 
data obtained navy oce naval research pe monitored joel davis harold hawkins 
dr supported nasa aerospace operations systems program nasa intelligent systems program 
sch olkopf smola aj uller kr 
nonlinear component analysis kernel eigenvalue problem 
neural computation 
principal component analysis 
springer verlag new york vapnik nature statistical learning theory 
springer new york smola aj sch olkopf tutorial support vector regression 
technical report nc tr neurocolt technical report series 
cristianini shawe taylor support vector machines 
cambridge university press girolami lj 
kernel pca feature extraction potentials human signal detection performance 
proceedings conference sweden springer pp 
saunders gammerman vovk ridge regression learning algorithm dual variables 
proceedings th international conference machine learning frank friedman jh 
statistical view chemometrics regression tools 
technometrics montgomery dc peck ea 
linear regression analysis nd edn 
john wiley sons smola aj sch olkopf uller kr 
connection regularization operators support vector kernels 
neural networks girolami lj 
kernel principal component regression covariance ation criterion model selection 
technical report cis university paisley 

note principal components regression 
applied statistics tibshirani knight covariance ation criterion adaptive model selection 
statist soc girosi jones poggio priors stabilizers basis functions regularization radial tensor additive splines 
technical report memo mit williams 
prediction gaussian processes linear regression linear prediction 
jordan mi eds 
learning inference graphical models 
kluwer 
wahba splines models observational data volume series applied mathematics edn 
siam philadelphia pontil poggio regularization networks support vector machines 
advances computational mathematics suykens jak vandewalle sparse approximation squares support vector machines 
ieee international symposium circuits systems iscas 
lj cichocki kernel principal component regression em approach nonlinear principal components extraction 
technical report cis university paisley 
lj mj 
feature extraction erps wavelets application human performance monitoring 
brain language lj kramer af arnold ja 
event related potentials indices display monitoring performance 
biological psychology lj 
estimation human signal detection performance erps feed forward network model 
computer intensive methods control signal processing curse dimensionality 
birkhauser boston 
williamson rc smola aj sch olkopf generalization performance regularization networks support vector machines entropy numbers compact operators 
technical report nc tr neurocolt royal holloway college 
girolami expectation maximization approach nonlinear component analysis 
appear neural computation 
bengio support vector machines large scale regression problems 
technical report idiap golub gh van loan chf 
matrix computations 
john hopkins university press london appendix section assumed dealing centralized data feature space 
practical computation centralization data leads modi cation form requirement centralized data transformed change matrix nk nk matrix elements 
similarly change test matrix test elements test ij fx fx testing training points respectively 
centralization matrix test test test test matrix entries left eigenvalues computed embedded mackey glass time series transformed kernel space 
di erent levels noise added represents ratio standard deviation noise signal respectively solid line dots dash dotted line 
right comparison eigenvalues computed solid line dash dotted line data samples 
symbols nt nt block confidence rating detection response 
level symbols display input device con guration symbols task relevant stimuli signal detection task 
fz sequence trials cz time ms pz running mean erps sites fz cz pz subject rst running mean erps 
comparison results achieved noisy mackey glass time series kpcr solid krr dashed methods 
di erent training sets size data points 
performance di erent widths gaussian kernel compared normalized root mean squared error nrmse terms 
top 
bottom 
represents ratio standard deviation added gaussian noise underlying time series 
subject tpc svr subject nmse svr subject tpc svr subject nmse svr subject tpc svr subject nmse svr comparison results achieved subjects svr data preprocessed linear pca svr respectively 
cases principal components describing variance 
performance di erent widths gaussian kernel compared terms test proportion correct tpc normalized mean squared error nmse 
subject tpc kpcr krr subject kpcr krr subject tpc kpcr krr subject kpcr krr subject tpc kpcr krr subject kpcr krr comparison results achieved subjects kpcr krr data preprocessed linear pca krr respectively 
cases principal components describing variance 
performance di erent widths gaussian kernel compared terms test proportion correct tpc normalized mean squared error nmse 
subject tpc svr subject svr subject svr subject svr subject svr subject nmse svr comparison svr subjects nonlinear principal components case 
performance di erent widths gaussian kernel compared terms test proportion correct tpc normalized mean squared error nmse 
nmse subjects boxplots lines lower quartile median upper quartile values whisker plot subjects performance krr preprocessing step left hand boxplots compared kpcr data preprocessed kpca right hand boxplots terms normalized mean squared error nmse 
boxplots computed results di erent runs widths gaussian kernel methods achieved minimal nmse test set 

