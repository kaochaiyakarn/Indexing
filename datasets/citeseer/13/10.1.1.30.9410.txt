arcing classifiers leo breiman statistics department university california berkeley shown combining multiple versions unstable classifiers trees neural nets results reduced test set error 
effective bagging breiman modified training sets formed resampling original training set classifiers constructed training sets combined voting 
freund schapire propose algorithm basis adaptively resample combine acronym arcing weights resampling increased cases misclassified combining done weighted voting 
arcing successful bagging test set error reduction 
explore arcing algorithms compare bagging try understand arcing works 
introduce definitions bias variance classifier components test set error 
unstable classifiers low bias large range data sets 
problem high variance 
combining multiple versions bagging arcing reduces variance significantly partially supported nsf 
classification regression methods unstable sense small perturbations training sets construction may result large changes constructed predictor 
subset selection methods regression decision trees regression classification neural nets unstable breiman 
unstable methods accuracy improved perturbing combining 
generate multiple versions predictor perturbing training set construction method 
combine multiple versions single predictor 
instance ali generates multiple classification trees choosing randomly best splits node combines trees maximum likelihood 
breiman adds noise response variable regression generate multiple subset regressions averages 
label perturb combine designate group methods 
effective methods bagging breiman 
bagging perturbs training set repeatedly generate multiple predictors combines simple voting classification averaging regression 
training set consist cases instances labeled 
put equal probabilities case probabilities sample replacement bootstrap times training set forming resampled training set cases may appear may appear 
construct predictor repeat procedure combine 
bagging applied cart gave dramatic decreases test set errors 
freund schapire proposed algorithm designed drive training set error rapidly zero 
algorithm run far past point training set error zero gives better performance bagging number real data sets 
crux idea start resample form training set 
sequence classifiers training sets built increase cases frequently 
termination combine classifiers weighted simple voting 
refer algorithms type adaptive resampling combining arcing algorithms 
honor freund schapire discovery denote specific algorithm arc fs discuss theoretical efforts relate training set test set error appendix better understand stability instability bagging arcing section define concepts bias variance classifiers 
difference test set misclassification error classifier minimum error achievable sum bias variance 
unstable classifiers trees characteristically high variance low bias 
stable linear discriminant analysis low variance high bias 
illustrated examples artificial data 
section looks effects arcing bagging trees bias variance 
main effect bagging arcing reduce variance 
arcing usually better bagging 
arc fs complex things behavior puzzling 
variance reduction comes adaptive resampling specific form arc fs 
show define simpler arc algorithm denoted arc accuracy comparable arc fs 
appear opposite poles arc spectrum 
arc demonstrate arcing works specific form arc fs algorithm adaptive resampling 
freund schapire compare arc fs bagging data sets conclude arc fs small edge test set error rates 
tested arc fs arc bagging real data sets bagging get results favorable arcing 
section 
arc fs arc finish dead heat 
data sets little better significantly better bagging 
look arcing bagging applied postal service digit data base 
results arcing exciting turns great classifier cart procedure usually get close lowest achievable test set error rates 
furthermore arc classifier shelf 
performance depend tuning settings particular problems 
just read data press start button 
neural net standards fast construct 
section gives results experiments aimed understanding arc fs arc 
algorithm distinctive different signatures 
generally arc fs uses smaller number distinct cases resampled training sets successive values highly variable 
successive training sets arc fs rock back forth convergence final set 
back forth rocking arc convergence final 
variability may essential ingredient successful arcing algorithms 
instability essential ingredient bagging arcing improve accuracy 
nearest neighbors stable breiman noted bagging improve nearest neighbor classification 
linear discriminant analysis relatively stable low variance section experiments show bagging arcing effect linear discriminant error rates 
sections contain remarks mainly aimed understanding bagging arcing 
reason bagging reduces error fairly transparent 
clear general terms arcing works 
dissimilar arcing algorithms arc fs give comparable accuracy 
possible arcing algorithms intermediate fs arc give better performance 
experiments freund shapire drucker cortes quinlan indicate arcing decision trees may lead fast generally accurate classification methods indicate additional research aimed understanding workings class algorithms high pay 

bias variance classifier classification output variable 
class label 
training set form 
class labels 
method construct classifier predicting values 
assume data training set consists iid selections distribution misclassification error defined pe denote pe expectation pe denote dx dx minimum misclassification rate bayes classifier argmax misclassification rate pe max dx 
define aggregated classifier ca argmax 
aggregation voting 
consider independent replicas 
construct classifiers 
determine classification ca having multiple classifiers vote popular class 
definition unbiased ca 
unbiased replications picks right class class 
classifier unbiased necessarily accurate classifier 
instance suppose class problem 
unbiased correct classification 
bayes predictor probability correct classification 
unbiased ca optimal 
set unbiased call unbiased set 
complement called bias set denoted define definition bias classifier bias variance var leads fundamental decomposition pe pe bias var note aggregating classifier replacing ca reduces variance zero guarantee reduce bias 
fact easy give examples bias increased 
bias set large probability pe may significantly larger pe 
defined bias variance properties bias variance non negative 
variance ca zero 
deterministic depend variance zero 
bias zero 
proofs immediate definitions 
variance expressed var max dx 
bias similar integral clearly bias variance non negative 
ca variance zero 
deterministic zero variance 
clear zero bias 
terms bias variance classification somewhat misleading properties commonly associated bias variance predicting numerical outcomes regression see geman bienenstock doursat 
important aspect definition variance component classification error eliminated aggregation 
bias may larger aggregated classifier 
friedman contains thoughtful analysis meaning bias variance class problems 
simplifying assumptions definition boundary bias point shown points negative boundary bias classification error reduced reducing variance class probability estimates 
boundary bias negative decreasing estimate variance may increase classification error points negative boundary bias exactly points defined unbiased set 
definitions bias variance classification dietterich kong kohavi wolpert tibshirani instability bias variance breiman pointed prediction methods unstable small changes training set cause large changes resulting predictors 
listed trees neural nets unstable nearest neighbors stable 
linear discriminant analysis lda stable 
unstable classifiers characterized high variance 
changes classifiers differ markedly aggregated classifier ca 
stable classifiers change replicates ca tend variance small 
procedures cart high variance average right largely unbiased optimal class usually winner popularity vote 
stable methods lda achieve stability having limited set models fit data 
result low variance 
data adequately represented available set models large bias result 
examples illustrate compute bias variance cart examples 
consist artificially generated data computed replicated 
example classes equal probability training sets cases 
waveform dimension class data 
described cart book breiman code generating data uci repository ftp address ftp ics uci edu directory pub machine learning databases 
ii twonorm dimension class data 
class drawn multivariate normal distribution unit covariance matrix 
class mean 
class mean 

iii dimension class data 
class drawn equal probability unit multivariate normal mean 
unit multivariate normal mean 

class drawn unit multivariate normal mean 

iv ringnorm dimension class data class multivariate normal mean zero covariance matrix times identity 
class unit covariance matrix mean 
monte carlo techniques compute bias variance distributions 
training sets size generated cart tree grown 
additional set size generated distribution aggregated classifier computed point set 
bayes predictor analytically computed point assigned bias set complement 
results averaging set table 
table bias variance error cart data set pe bias variance error waveform twonorm ringnorm problems difficult cart 
instance twonorm optimal separating surface oblique plane 
hard approximate multidimensional rectangles cart 
ringnorm separating surface sphere difficult rectangular approximation 
difficult separating surface formed continuous join oblique hyperplanes 
examples cart low bias 
problem variance 
explore sections methods reducing variance combining cart classifiers trained perturbed versions training set 
trees grown default options cart splitting continues far possible 
special parameters set done optimize performance cart data sets 

bias variance arcing bagging ubiquitous low bias tree classifiers variances reduced accurate classifiers may result 
general direction reducing variance indicated classifier ca 
classifier zero variance low bias 
specifically problems bias 
nearly optimal 
recall generating independent replicates constructing cart classifier replicate training sets letting classifiers vote popular class 
possible real data generate independent replicates training set 
imitations possible 
bagging simplest implementation idea generating quasi replicate training sets bagging breiman 
define probability nth case training set sample times distribution 
equivalently sample replacement 
forms resampled training set 
cases may appear may appear 
called bootstrap sample denote distribution iid repeat sampling procedure getting sequence independent bootstrap training sets 
construct corresponding sequence classifiers classification algorithm applied bootstrap training sets 
multiple classifiers vote class 
point ca really depends underlying probability training sets drawn ca ca 
bagged classifier ca 
hope approximation ca considerable variance reduction result 
arcing arcing complex procedure 
multiple classifiers constructed vote classes 
construction sequential construction st classifier depending performance previously constructed classifiers 
give brief description freund schapire arc fs algorithm 
details contained section 
start construction probability distribution cases training set 
training set constructed sampling times distribution 
probabilities updated depending cases classified 
factor defined depends misclassification rate smaller larger nth case misclassified put weight bp case 
define weight 
divide weight sum weights get updated probabilities round sampling 
fixed number classifiers constructed weighted voting class 
intuitive idea arcing points selected replicate data sets misclassified 
troublesome points focusing adaptive resampling scheme arc fs may better neutral bagging approach 
results bagging arc fs run artificial data set described 
procedure consisted generating replicate training sets size 
training set bagging arc fs run times cart classifier 
training set gave classifier bagged classifier 
additional member monte carlo set generated results aggregating bagged classifiers computed member set compared bayes classification 
enabled bias variance computed 
results table compared cart results 
table 
bias variance data set cart bagging arcing waveform bias var twonorm bias var bias var ringnorm bias var bagging arcing reduce bias bit major contribution accuracy large reduction variance 
arcing better bagging better variance reduction 
effect combining classifiers 
experiments bagging arcing combinations tree classifiers 
natural question happens classifiers combined 
explore ran bagging waveform twonorm data combinations trees 
run consisted repetitions 
run training set test set generated prescribed number trees constructed combined test set error computed 
errors averaged repetitions give results shown table 
standard errors table test set error combinations data set waveform arc fs bagging twonorm arc fs bagging arc fs error rates decrease significantly combination reaching rates close bayes minimums waveform twonorm 
standard comparison linear discriminant analysis optimal twonorm 
error rate averaged repetitions 
bagging error rates decrease combinations decreases smaller arc fs 

arcing algorithms section specifies arc algorithms looks performance number data sets 

definitions arc algorithms 
algorithms proceed sequential steps user defined limit steps termination 
initialize probabilities equal 
step new training set selected sampling original training set probabilities 
classifier resampled training set constructed updated depending misclassifications step 
termination classifiers combined weighted arc fs unweighted arc voting 
arc fs algorithm boosting theorem freund schapire 
arc ad hoc invention 
arc fs specifics kth step current probabilities sample replacement get training set construct classifier 
ii run classifier nth case classified incorrectly zero 
iii define updated st step probabilities sp steps 
combined weighted voting having weight log 
revisions algorithm necessary 
equal great original freund schapire algorithm exits construction loop 
better results gotten setting equal restarting 
equals zero making subsequent step undefined set probabilities equal restart 
referee pointed updating definition arc fs leads interesting result 
weight st step equally divided points misclassified kth step correctly classified 
arc specifics arc fs ii run classifier number misclassifications nth case 
iii updated step probabilities defined steps 
combined unweighted voting 
training set selected sampling probabilities set generated way 
tree construction test set pruning 
eliminating need cross validation pruning classification trees grown pruned cpu time takes trees grown pruned fold crossvalidation 
true bagging 
arcing bagging applied decision trees grow classifiers relatively fast 
parallel bagging easily implemented arc essentially sequential 
arc devised 
testing arc fs suspected success lay specific form adaptive resampling property increasing weight placed cases frequently misclassified 
check tried simple update schemes probabilities 
update form tested waveform data 
best arc 
higher values tested improvement possible 
experiments data sets 
experiments moderate sized data sets larger ones bagging breiman plus handwritten digit data set 
data sets summarized table 
table data set summary data set training test variables classes heart breast cancer ionosphere diabetes glass soybean letters satellite shuttle dna digit data sets heart data uci repository 
brief descriptions breiman 
procedure data sets sets consisted iterations steps select random training set set aside test set 
ii run steps arc fs arc remaining data 
iii get error rates test set 
error rates computed iii averaged iterations get final numbers shown table 
note arc fs soybean data set frequently causing restarting 
larger data sets came separate test training sets 
arcing algorithms generate classifiers digit data combined final classifier 
test set errors shown table 
table test set error data set arc fs arc bagging cart heart breast cancer ionosphere diabetes glass soybean letters satellite shuttle dna digit larger data sets statlog project michie compared classification methods 
results arc fs ranks best barely edged place dna 
arc close 
digit data set famous postal service data set preprocessed le cun result grey scale images 
data set test bed adventures classification bell laboratories 
best layer neural net gets error rate 
layer network gets 
hastie tibshirani deformable prototypes get error 
smart metric nearest neighbors gives lowest error rate date simard 
classifiers specifically tailored data 
interesting support vector machines described vapnik shelf require specification parameters functions 
lowest error rates slightly 
arcing algorithms cart requires reading training set arc fs gives accuracy competitive hand crafted classifiers 
relatively fast 
trees constructed arc fs digit data took hours cpu time sparc 
reprogramming get hour cpu time 
looking test set error results little choose arc fs arc 
arc slight edge smaller data sets arc fs little better larger ones 

properties arc algorithms experiments carried smaller sized data sets listed table plus artificial waveform data 
arc fs arc lengthy step runs data set 
run information various characteristics gathered 
information better understand algorithms similarities differences 
arc fs arc probably stand opposite extremes effective arcing algorithms 
arc fs constructed trees change considerably step 
arc changes gradual 
preliminary results resampling equal probabilities training set cases appear resampled data set put way data 
adaptive resampling weight cases data 
table gives average percent data arc algorithms constructing classifier sequence steps 
third column average value beta arc fs algorithm constructing sequence 
table percent data data set arc arc fs av 
beta waveform heart breast cancer ionosphere diabetes glass soybean arc data ranges 
arc fs uses considerably smaller fractions data ranging breast cancer data set cases tree 
average values beta surprisingly large 
instance breast cancer data set misclassification training set case lead amplification unnormalized weight factor 
shuttle data leads extreme results 
average data constructing arc fs tree sequence average value beta 
variability signature variability characteristic differed significantly algorithms 
signature derived follows run kept track average value step runs equal bagging average values 
standard deviation computed 
gives plots standard deviations vs averages data sets algorithm 
upper point cloud graph corresponds arc fs values lower arc values 
graph soybean data set shown frequent restarting causes arc fs values anomalous 
arc fs standard deviations generally larger average increase linearly average 
larger volatile contrast standard deviations arc quite small increase slowly average 
range arc fs times larger arc 
note modulo scaling shapes point sets similar data sets 
mysterious signature step runs kept track number times nth case appeared training set number times misclassified training set 
algorithms frequently point misclassified probability increases frequently training set 
intuitively obvious graphs 
data set number times misclassified plotted vs number times training set 
plots arc behave expected 
arc fs 
plots rise sharply plateau 
plateau change misclassification rate vs rate training set 
fortunately mysterious behavior rational explanation terms structure arc fs algorithm 
assume iterations constant equal experiments values bk moderate sd mean values large 
proportion times nth case misclassified 
kr kr max set indices small positive cardinality small increasing number misclassifications cases accurately classified training sets drawn misclassification rates increase get close 
illustrate shows misclassification rates function number iterations cases twonorm data discussed subsection 
top curve case consistently large 
lower curve case vanishingly small 
number cases accurately classified training sets drawn characterized lower values misclassification rate small 
cases cluster axes 
insight provided 
percentile plot proportion training sets cases twonorm data iterations 
cases small number training sets 
rest uniform distribution proportion training sets 
hard classify points get weight 
explore question twonorm data 
ratio probability densities classes point depends value vector coordinates 
smaller closer ratio densities difficult point classify 
idea underlying arc algorithms valid probabilities inclusion resampled training sets increase decreases 
plots average iterations vs arc algorithms 
av generally increases decreasing relation noisy 
confounded factors able pinpoint 

linear discriminant analysis isn improved bagging arcing 
linear discriminant analysis lda fairly stable low variance come surprise test set error significantly reduced bagging arcing 
test bed data sets table 
ionosphere soybean eliminated class covariance matrix singular full training set ionosphere bagging arc fs training sets soybean 
experimental set similar section 
leave test set repetitions run linear discriminant analysis test set errors averaged 
repeated repetition combinations linear discriminants built bagging arc fs 
test set errors combined classifiers averaged 
results listed table 
table linear discriminant test set error 
data set lda lda bag lda arc restart freq 
heart breast cancer diabetes glass recall arc fs construction restarted equal 
column table indicates restarting occurred 
instance heart data average occurred times 
contrast runs combining trees restarting encountered soybean data 
frequency restarting consequence stability linear analysis 
procedure stable cases tend misclassified changing training sets 
weights increase weighted training set error 
results indicate linear discriminant analysis generally low variance procedure 
fits simple parametric normal model change replicate training sets 
illustrate monte carlo estimation bias variance synthetic data 
recall bias variance cart 
lda 
problem lda usually bias wrong consistently wrong simple model hope low bias variety complex data sets 
improved bagging aggregate classifier depends distribution samples selected number selected 
letting dependence implicit denote ca ca 
mentioned bagging replaces ca ca hope approximation produce variance reduction 
best discrete estimate distribution usually smoother spread interesting question better approximation produce 
check possibility simulated data sets described section 
training set drawn distributions replaced spherical normal distribution centered bootstrap training set iid drawn smoothed distribution 
values tried sd normal smoothing best adopted 
results table 
table smoothed estimate bagging test set errors data set bagging bagging smoothed arcing waveform twonorm ringnorm pe values smoothed estimates show better approximation lower variance 
limits estimate unknown underlying distribution training set 
aggregated classifiers smoothed approximations variances significantly zero doubt efforts refine estimates push lower 
note better approximation bagging arcing 
arcing training set error arcing transparent bagging 
freund schapire designed arc fs drive training set error rapidly zero remarkably 
context arc fs designed gives clues ability reduce test set error 
instance suppose run arc fs exit construction loop training set error zero 
test set errors number steps exit loop averaged iterations leave smaller data sets table compared results table 
smaller data sets kept track step training set error non zero 
averages figures parentheses column table 
values show training set error drops zero invariably stays zero 
ran bagging data sets table exiting loop training error zero kept track average number steps exit average test set error repetitions leave 
numbers table soybean restarting problems 
table test error exit times arc fs data set error steps exit heart breast cancer ionosphere diabetes glass letters satellite shuttle dna table test error exit times bagging data set error steps exit heart 
breast cancer ionosphere diabetes glass results delineate differences efficient reduction training set error test set accuracy 
arc fs reaches zero training set error quickly average tree constructions 
accompanying test set error higher bagging takes longer reach zero training set error 
produce optimum reductions test set error arc fs run far past point zero training set error 
non random arcing important question understanding arcing happens randomness taken 
definition arc fs section remains randomly sampling probabilities probabilities fed directly classifier weight case 
cart modified weights run minimum node size data sets test set error estimation methods get table results 
results table compared random arcing 
table test set error arc fs data set random non random heart breast cancer ionosphere diabetes glass soybean letters satellite shuttle dna digit results show randomness important element arcing ability reduce test set error 
sharply differentiates arcing bagging 
bagging random sampling critical weights remain constant equal 
remarks arcing classifier expressible aggregated classifier approximation distributions successive training sets drawn change constantly procedure continues 
arc fs algorithm successive form multivariate markov chain suppose stationary distribution dp suspect true referee doubts 
probability training sets drawn original training set distribution cases 
steadystate unweighted voting class gets vote dp 
clear steady state probability structure relates error reduction properties arcing 
importance suggested experiments 
results table show arcing takes longer reach minimum error rate bagging 
error reduction properties arcing come steady state behavior longer reduction time may reflect fact dependent markov property arc fs algorithm takes longer reach steady state bagging independence successive bootstrap training sets law large numbers sets quickly 
steady state behavior arcing algorithms relates drive training set error zero iterations unknown 
complex aspect arcing illustrated experiments done date 
diabetes data set gives higher error rate single run cart 
quinlan experiments tree structured program similar cart compared arc fs bagging classifiers 
data sets examined experiments arc fs test set error larger 
occur bagging 
understood arc fs causes infrequent degeneration test set error usually smaller data sets 
conjecture may caused outliers data 
outlier consistently probability sampled continue increase arcing continues 
start appearing multiple times resampled data sets 
small data sets may warp classifiers 
arc fs arcing algorithms reduce test set error methods cart point accurate available shelf classifiers wide variety data sets 
freund schapire discovery adaptive resampling embodied arc fs creative idea lead interesting research better understanding classification works 
arcing algorithms rich probabilistic structure challenging problem connect structure error reduction properties 
clear optimum arcing algorithm look 
arc fs devised different context arc ad hoc 
concepts bias variance suitable explanation bagging arcing effectively reduce variance bias variance setting give convincing explanation arcing works 
better understanding arcing functions lead improvements 

acknowledgments am indebted yoav freund giving draft papers referred article yoav freund robert schapire informative email interchanges help understanding boosting context trevor hastie making available preprocessed postal service data harris drucker responded generously questioning nips subsequent comparing arc fs bagging convinced arcing needed looking tom dietterich comments draft david wolpert helpful discussions boosting 
associate editor referees interesting constructive comments resulted improved 
particular indebted referees suggesting experiment non random arcing reported section ali learning probablistic relational concept descriptions thesis computer science university california irvine breiman bagging predictors machine learning pp 
breiman heuristics instability model selection annals statistics pp 
breiman friedman olshen stone classification regression trees chapman hall dietterich kong error correcting output coding corrects bias variance proceedings th international conference machine learning pp 

drucker cortes boosting decision trees advances information processing systems vol 
pp 

freund schapire decision theoretic generalization line learning application boosting 
appear journal computer system sciences 
freund schapire experiments new boosting algorithm machine learning proceedings thirteenth international conference pp 
friedman bias variance loss curse dimensionality appear journal knowledge discovery data mining 
geman bienenstock doursat neural networks bias variance dilemma 
neural computations pp 
hastie tibshirani handwritten digit recognition deformable prototypes unpublished ms ftp stat stanford edu pub hastie zip ps kearns valiant learning boolean formulae finite automata hard factoring technical report tr harvard university aiken computation laboratory kearns valiant cryptographic limitations learning boolean formulae finite automata 
proceedings annual acm symposium theory computing acm press pp 

kohavi wolpert bias plus variance decomposition zero loss functions machine learning proceedings thirteenth international conference pp 
le cun boser denker henderson howard hubbard jackel handwritten digit recognition back propagation network advances neural information processing systems vol pp 
michie spiegelhalter taylor machine learning neural statistical classification ellis horwood london quinlan bagging boosting proceedings aaai national conference artificial intelligence pp 

schapire strength weak learnability machine learning pp 
simard le cun denker efficient pattern recognition new transformation distance advances neural information processing systems vol 
pp tibshirani bias variance prediction error classification rules technical report statistics department university toronto vapnik nature statistical learning theory springer appendix boosting context arc fs freund schapire designed arc fs drive training error rapidly zero 
connected training set property test set behavior ways 
structural risk minimization see vapnik 
idea bounds test set error terms training set error bounds depend vc dimension class functions construct classifiers 
bound tight approach contradictory consequence 
stopping soon training error zero gives complex classifier lowest vc dimension test set error corresponding stopping rule lower continue combine classifiers 
table shows hold 
second connection concept boosting 
freund schapire devised arc fs context boosting theory see schapire named adaboost 
follow freund setting definitions assume input space vectors unknown function defined space input vectors assigns class label input vector 
problem learn classifying method called exist integer training set consisting drawn random distribution dx input space corresponding 
classifier constructed probability greater random vector having distribution dx 
classifying method called integer training set consisting drawn random distribution dx input space corresponding 
classifier constructed probability random vector having distribution dx 
note low error input space just training set small test set error 
concept weak learning introduced kearns valiant left open question weak strong equivalent 
question termed boosting problem equivalence requires method boost low accuracy high accuracy 
schapire proved boosting possible 
boosting algorithm method takes converts 
freund proved algorithm similar arc fs boosting 
freud schapire apply results freund conclude adaboost boosting 
boosting assumptions restrictive 
instance overlap classes bayes error rate positive weak strong learners 
overlap classes easy give examples input spaces weak learners 
boosting theorems really say weak learner virtually real data situations arcing bagging overlap classes weak learners exist 
freund boosting theorem applicable 
particular applicable examples simulated data examples real data sets freund schapire quinlan 
may connection ability arcing algorithms rapidly drive training set error zero steady state test set reduction rooted boosting context 
vs av resampling probabilities waveform heart breast cancer ionosphere diabetes glass 
vs 
times training set waveform heart breast cancer ionosphere diabetes glass number trees combined proportion times cases proportion training sets percentile percentile plot proportion training sets cases av arc fs average vs arc av 
