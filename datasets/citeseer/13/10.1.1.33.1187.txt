probabilistic latent semantic analysis appear artificial intelligence uai stockholm thomas hofmann eecs department computer science division university california berkeley international computer science institute berkeley ca hofmann cs berkeley edu probabilistic latent semantic analysis novel statistical technique analysis mode occurrence data applications information retrieval filtering natural language processing machine learning text related areas 
compared standard latent semantic analysis stems linear algebra performs singular value decomposition occurrence tables proposed method mixture decomposition derived latent class model 
results principled approach solid foundation statistics 
order avoid overfitting propose widely applicable generalization maximum likelihood model fitting tempered em 
approach yields substantial consistent improvements latent semantic analysis number experiments 
learning text natural language great challenges artificial intelligence machine learning 
substantial progress domain strong impact applications ranging information retrieval information filtering intelligent interfaces speech recognition natural language processing machine translation 
fundamental problems learn meaning usage words data driven fashion text corpus possibly linguistic prior knowledge 
main challenge machine learning system address roots distinction lexical level said written semantical level intended referred text utterance 
resulting problems twofold word may multiple senses multiple types usage different context ii semantically related words different words may similar meaning may certain contexts denote concept weaker sense refer topic 
latent semantic analysis lsa known technique partially addresses questions 
key idea map high dimensional count vectors ones arising vector space representations text documents lower dimensional representation called latent semantic space 
name suggests goal lsa find data mapping provides information lexical level reveals semantical relations entities interest 
due generality lsa proven valuable analysis tool wide range applications 
theoretical foundation remains large extent unsatisfactory incomplete 
presents statistical view lsa leads new model called probabilistic latent semantics analysis plsa 
contrast standard lsa probabilistic variant sound statistical foundation defines proper generative model data 
detailed discussion numerous advantages plsa subsequent sections 
latent semantic analysis count data occurrence tables lsa principle applied type count data discrete dyadic domain cf 
:10.1.1.46.2857
prominent application lsa analysis retrieval text documents focus setting sake concreteness 
suppose collection text doc uments fd dn terms vocabulary fw wm ignoring sequential order words occur document may summarize data theta occurrence table counts ij denotes term occurred document particular case called matrix rows columns referred document term vectors respectively 
key assumption simplified bag words vector space representation documents cases preserve relevant information tasks text retrieval keywords 
latent semantic analysis svd mentioned key idea lsa map documents symmetry terms vector space reduced dimensionality latent semantic space 
mapping restricted linear singular value decomposition svd occurrence table 
starts standard svd sigmav orthogonal matrices diagonal matrix sigma contains singular values lsa approximation computed setting largest singular values sigma zero sigma rank optimal sense matrix norm 
obtains approximation sigmav sigmav notice document document inner products approximation sigma think rows sigma defining coordinates documents latent space 
original high dimensional vectors sparse corresponding low dimensional latent vectors typically sparse 
implies possible compute meaningful association values pairs documents documents terms common 
hope terms having common meaning particular synonyms roughly mapped direction latent space 
probabilistic lsa aspect model starting point probabilistic latent semantic analysis statistical model called aspect model :10.1.1.46.2857
aspect model latent variable model occurrence data associates unobserved class variable fz observation 
joint probability model theta graphical model representation aspect model asymmetric symmetric parameterization 
defined mixture wjd wjd wjz zjd virtually statistical latent variable models aspect model introduces conditional independence assumption independent conditioned state associated latent variable corresponding graphical model representation depicted 
cardinality smaller number documents words collection acts bottleneck variable predicting words 
worth noticing model equivalently parameterized cf 
djz wjz perfectly symmetric entities documents words 
model fitting em algorithm standard procedure maximum likelihood estimation latent variable models expectation maximization em algorithm 
em alternates coupled steps expectation step posterior probabilities computed latent variables ii maximization step parameters updated 
standard calculations cf 
yield step equation zjd djz wjz djz wjz step formulae wjz zjd djz zjd zjd discussing algorithmic refinements study relationship proposed model lsa detail 
spanned sub simplex simplex embedding kl divergence projection sketch probability sub simplex spanned aspect model 
probabilistic latent semantic space consider class conditional multinomial distributions vocabulary call factors 
represented points gamma dimensional simplex possible multinomials 
convex hull set points defines gamma dimensional sub simplex 
modeling assumption expressed conditional distributions wjd document approximated multinomial representable convex combination factors wjz mixing weights zjd uniquely define point spanned sub simplex 
simple sketch situation shown 
despite discreteness introduced latent variables continuous latent space obtained space multinomial distributions 
dimensionality sub simplex gamma opposed maximum gamma complete probability simplex performs dimensionality reduction space multinomial distributions spanned sub simplex identified probabilistic latent semantic space 
stress point clarify relation lsa rewrite aspect model parameterized matrix notation 
define matrices jz jz sigma diag joint probability model written matrix product sigma comparing svd observations outer products rows reflect conditional independence plsa ii factors correspond mixture components aspect model iii mixing proportions plsa substitute singular values 
crucial difference plsa lsa objective function utilized determine optimal decomposition approximation 
lsa frobenius norm corresponds implicit additive gaussian noise assumption possibly transformed counts 
contrast plsa relies likelihood function multinomial sampling aims explicit maximization predictive power model 
known corresponds minimization cross entropy kullback leibler divergence empirical distribution model different type squared deviation 
modeling side offers important advantages example mixture approximation occurrence table defined probability distribution factors clear probabilistic meaning 
contrast lsa define properly normalized probability distribution may contain negative entries 
addition obvious interpretation directions lsa latent space directions plsa space interpretable multinomial word distributions 
probabilistic approach take advantage established statistical theory model selection complexity control determine optimal number latent space dimensions 
choosing number dimensions lsa hand typically ad hoc heuristics 
comparison computational complexity suggest advantages lsa ignoring potential problems numerical stability svd computed exactly em algorithm iterative method guaranteed find local maximum likelihood function 
experiments computing time em significantly worse performing svd cooccurrence matrix 
large potential improving run time performance em line update schemes explored far 
topic decomposition polysemy briefly discuss elucidating examples point reveal advantage plsa lsa context words 
generated dataset cluster abstracts documents clustering trained aspect model latent classes 
pairs factors visualized 
pairs selected factors highest probability generate words segment matrix line power respectively 
sketchy characterization factors probable words reveals interesting topics 
particular notice term select particular pair different meaning topic factor segment refers image region phonetic segment second factor 
ii matrix denotes rectangular table numbers material embedded enclosed 
iii line refer line image line spectrum 
segment segment matrix matrix line line power power imag speaker robust constraint alpha power load segment speech matrix cell line redshift spectrum texture recogni part match line omega vlsi color signal matrix mpc power tissue train plane cellular imag systolic brain hmm linear input slice source condition design high redshift complex cluster 
perturb segment mri segment root format standard volume sound group recogn model implement selected factors factor decomposition 
displayed word stems probable words class conditional distribution wjz top bottom descending order 
document jd segment segment jd segment medic imag problem field imag base proper segment digit imag segment medic imag need applic estim object classif abnorm shape contour detec segment exist segment specif medic imag remain crucial problem 
document jd segment segment jd consid signal origin specif problem segment signal relat segment address wide applic field report method hidden markov model hmm hmm state correspond signal signal determin viterbi algorithm forward algorithm observ train estim hmm train applic signal problem experi perform unknown speaker 
abstracts exemplary documents cluster collection latent class posterior probabilities segment word probabilities segment 
iv power context radiating objects astronomy electrical engineering 
shows abstracts exemplary documents pre processed standard word list stemmer 
posterior probabilities classes different occurrences segment indicate factors pair generated observation 
displayed estimates conditional word probabilities segment jd see correct meaning word segment identified cases 
implies segment occurs frequently document overlap factored representation low identified polysemous word relative chosen resolution level dependent context explained different factors 
aspects versus clusters worth comparing aspect model statistical clustering models cf 
:10.1.1.46.2857
clustering models documents typically associates latent class variable document collection 
closely related approach distributional clustering model thought unsupervised version naive bayes classifier 
shown conditional word probability probabilistic clustering model wjd pfc wjz pfc zg posterior probability document having latent class simple implication bayes rule posterior probabilities concentrate probability mass certain value increasing number observations length document 
means algebraically equivalent conceptually different yield fact different results 
aspect model assumes document specific distributions convex combination aspects clustering model assumes just cluster specific distribution inherited documents cluster 
clustering models class conditionals wjz distributional clustering model posterior uncertainty cluster assignments induces averaging class conditional word distributions wjz 
capture complete vocabulary subset cluster documents factors focus certain aspects vocabulary subset documents 
example factor explain fraction words occurring document explain words assign zero probability words taken care factors 
model fitting revisited improving generalization tempered em far focused maximum likelihood estimation fit model document collection 
likelihood equivalently perplexity quantity believe crucial assessing quality model clearly distinguish performance training data unseen test data 
derive conditions generalization unseen data guaranteed fundamental problem statistical learning theory 
propose generalization maximum likelihood mixture models known annealing entropic regularization term 
resulting method called tempered expectation maximization tem closely related deterministic annealing 
starting point tem derivation optimization principle 
pointed em procedure latent variable models obtained minimizing common objective function helmholtz free energy aspect model fi gammafi logp wjz log variational parameters define conditional distribution fi parameter analogy physical systems called inverse computational temperature 
notice contribution negative expected log likelihood scaled fi 
case zjd minimizing parameters defining wjz amounts standard step em 
fact straightforward verify posteriors obtained minimizing fi 
general determined djz wjz fi djz wjz fi perplexity refers log averaged inverse probability unseen data 
latent space dimensions plsa em tem lsa latent space dimensions perplexity plsa lsa perplexity results function latent space dimensionality med data rank lob data rank 
plotted results lsa dashed dotted curve plsa trained tem solid curve trained early stopping em dotted curve 
upper baseline unigram model corresponding marginal independence 
star right plsa denotes perplexity largest trained aspect models 
shows effect entropy fi dampen posterior probabilities closer uniform distribution decreasing fi 
somewhat contrary spirit annealing continuation method propose inverse annealing strategy performs em iterations decreases fi performance held data deteriorates 
compared annealing may accelerate model fitting procedure significantly factor gamma test set performance heated models worse achieved carefully annealed models 
tem algorithm implemented way 
set fi perform em early stopping 

decrease fi jfi perform tem iteration 

long performance held data improves non negligible continue tem iterations value fi goto step 
perform stopping fi decreasing fi yield improvements 
experimental results experimental evaluation focus tasks perplexity minimization document specific unigram model noun adjective pairs ii automated indexing documents 
evaluation lsa table average precision results relative improvement baseline method cos tf standard test collections 
compared lsi plsi results obtained combining plsi models plsi 
lsi indicates performance gain achieved baseline result dimensions reported case 
med cran cacm cisi prec 
impr 
prec 
impr 
prec 
impr 
prec 
impr 
cos tf lsi plsi plsi plsa task demonstrate advantages explicitly minimizing perplexity tem second task show solid statistical foundation plsa pays applications directly related perplexity reduction 
perplexity evaluation order compare predictive performance plsa lsa specify extract probabilities lsa decomposition 
problem trivial negative entries prohibit simple re normalization approximating matrix followed approach derive lsa probabilities 
data sets evaluate perplexity performance standard information retrieval test collection med document ii dataset noun adjective pairs generated tagged version lob corpus 
case goal predict word occurrences parts words document 
second case nouns predicted conditioned associated adjective 
reports perplexity results lsa plsa med lob datasets dependence number dimensions probabilistic latent semantic space 
plsa outperforms statistical model derived standard lsa far 
med collection plsa reduces perplexity relative unigram baseline factor lsa achieves factor reduction 
sparse lob data plsa reduction perplexity reduction achieved lsa 
order demonstrate advantages tem trained aspect models med data standard em early stopping 
seen curves difference em tem model fitting significant 
strategies tempering early stopping successful controlling model complexity em training performs worse inefficient available degrees freedom 
notice methods possible train high dimensional models continuous improvement performance 
number latent space dimensions may exceed rank occurrence matrix choice number dimensions merely issue possible limitations computational resources 
information retrieval key problems information retrieval automatic indexing main application query retrieval 
popular family information retrieval techniques vector space model vsm documents 
utilized straightforward representation untransformed term frequencies standard cosine matching function detailed experimental analysis 
representation applies queries matching function baseline term matching method written pp pp latent semantic indexing lsi original vector space representation documents replaced representation low dimensional latent space similarity computed representation 
queries documents part original collection folded simple matrix multiplication cf 
details 
experiments considered linear combinations original similarity score weight derived latent space representation weight gamma 
ideas applied probabilistic latent semantic indexing plsi conjunction plsa model 
precisely low dimensional representation factor space zjd med recall cran recall cacm recall cisi recall cos lsi plsi cos lsi plsi cos lsi plsi cos lsi plsi precision recall curves term matching lsi plsi test collections 
utilized evaluate similarities 
achieve queries folded done plsa fixing wjz parameters calculating weights tem 
advantage statistical models vs svd techniques allows systematically combine different models 
optimally done bayesian model combination scheme utilized simpler approach experiments shown excellent performance robustness 
simply combined cosine scores models uniform weight 
resulting method referred plsi empirically performance robust different non uniform weights weight combination original cosine score 
due noise reducing benefits model averaging 
notice lsa representations different form nested sequence true statistical models expected capture larger variety reasonable decompositions 
utilized medium sized standard document collection relevance assessment med document abstracts national library medicine ii cran document abstracts aeronautics cranfield institute technology iii cacm abstracts cacm journal iv cisi abstracts library science institute scientific informa beta beta beta perplexity beta average precision perplexity average precision function inverse temperature fi aspect model left right 
tion 
condensed results terms average precision recall recall levels gamma summarized table corresponding precision recall curves 
additional details experimental setup plsa models trained tem data set held data 
plsi report best result obtained models lsi report best result obtained optimal dimension exploring dimensions step size 
combination weight cosine baseline score coarsely optimized hand med cran cacm cisi 
experiments consistently validate advantages plsi lsi 
substantial performance gains achieved data sets 
notice relative precision gain compared baseline method typically interesting intermediate regime recall 
particular plsi works cases lsi fails completely problems lsi accordance original results reported 
benefits model combination substantial 
cases uniformly combined model performed better best single model 
sight effect model averaging selecting correct model dimensionality 
experiments demonstrate advantages plsa standard lsa restricted applications performance criteria directly depending perplexity 
statistical objective functions perplexity log likelihood may provide general yardstick analysis methods text learning information retrieval 
stress point ran experiment med data perplexity average precision monitored simultaneously function fi 
resulting curves show striking correlation plotted 
proposed novel method unsupervised learning called probabilistic latent semantic analysis statistical latent class model 
argued approach principled standard latent semantic analysis possesses sound statistical foundation 
tempered expectation maximization powerful fitting procedure 
experimentally verified claimed advantages achieving substantial performance gains 
probabilistic latent semantic analysis considered promising novel unsupervised learning method wide range applications text learning information retrieval 
acknowledgments author jan puzicha andrew mccallum mike jordan joachim buhmann tishby nelson morgan jerry feldman dan gildea andrew ng sebastian thrun tom mitchell stimulating discussions helpful hints 
supported daad fellowship 
bellegarda 
exploiting local global constraints multi span statistical language modeling 
proceedings icassp volume pages 
jurafsky 
better integration semantic predictors statistical language modeling 
proceedings icslp 
appear 
deerwester dumais furnas landauer 
harshman 
indexing latent semantic analysis 
journal american society information science 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal statist 
soc 

foltz dumais 
analysis information filtering methods 
communications acm 
hofmann 
probabilistic latent semantic indexing 
proceedings sigir 
hofmann puzicha jordan :10.1.1.46.2857
unsupervised learning dyadic data 
advances neural information processing systems volume 
mit press 
landauer dumais 
solution plato problem latent semantic analysis theory acquisition induction representation knowledge 
psychological review 
neal hinton 
view em algorithm justifies incremental variants 
jordan editor learning graphical models pages 
kluwer academic publishers 
pereira tishby lee 
distributional clustering english words 
proceedings acl pages 
rose gurewitz fox 
deterministic annealing approach clustering 
pattern recognition letters 
salton mcgill 
modern information retrieval 
mcgraw hill 
saul pereira 
aggregate mixed order markov models statistical language processing 
proceedings nd international conference empirical methods natural language processing 
