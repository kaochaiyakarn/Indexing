march lu tp nd revision finding embedding dimension variable dependences time series hong pi carsten peterson department theoretical physics university lund lund sweden submitted neural computation neural computation general method ffi test establishes functional dependencies sequence measurements 
approach calculating conditional probabilities vector component distances 
imposing requirement continuity underlying function obtained values conditional probabilities carry information embedding dimension variable dependencies 
power method illustrated synthetic time series different time lag dependencies noise levels sunspot data 
virtue method preprocessing data context feed forward neural networks demonstrated 
applicability tracking residual errors output units stressed 
lu se carsten lu se behaviour dynamical system modeled analyzing time series record certain system variables 
artificial neural networks ann model systems attracted attention 
success models relies heavily identifying underlying structure time series advantageous know advance embedding dimension relevant inputs noise level 
devise simple easy method continuity requirements statistical measures identifying essential properties time series record 
language time series approach applies continuous function mapping problem 
time series wide range behaviour ranging entirely random uncorrelated completely deterministic 
reality extremes 
existing approaches determine dependencies entropy measures kolmogorov farmer elaborate autocorrelation measures russell grassberger procaccia brock savit green 
approach roots philosophy aims determining embedding dimension pinning sensitivity various variables establishing noise levels 
brock 
devised method bds test test null hypothesis sequence numbers iid independently identically distributed random numbers 
test developed savit green conditional probability approach degree variable dependence may quantified 
method merit brevity message gives ambiguities 
inspired savit green propose method ffi test different viewpoint exploits definition function continuity 
definition easily connected behaviour conditional probabilities gives clear signatures apart ambiguities arising insufficient statistics respect variable dependences embedding dimensionality 
knowledge proposed method exist literature despite conceptual simplicity 
existing approaches mentioned general aim establishing invariant fractal dimensional measures 
numerical implementations usually involve box counting algorithms 
meaningful interpretations outcome algorithms box size reduced rely heavily scaling assumption 
estimate embedding dimension viewed byproduct estimating fractal dimensions algorithms 
attempt establish invariant nonlinearity measure 
aim pick variable dependences identify minimum embedding dimension directly data 
exploiting properties continuous functions need scaling assumption 
traditional line approach leads naturally bds statistic tests null hypothesis iid sequence ffi test tests hypotheses extremes iid deterministic map 
large class models particular neural network models time series prediction system identification problems existence function mapping inherently assumed 
ffi test provides measures truthfulness assumptions gives estimate successful models reproducing sequence data 
strength traditional approaches lacking 
successful explorations different maps noise variety time lag dependencies 
underlying dynamics sunspot series studied 
relevance method feed forward network training illustrated sunspot series shown feeding network established minimum embedding dimension vectors gives rise state art generalization performance 
method track residual dependencies output errors multilayer perceptron mlp 
general formulation consider discrete time system manifested time series interested knowing exists continuous map relating values past ones possible identify state equation gamma gamma gammad noise term represents part originates insufficient dimension measurements real noise 
general decrease system completely deterministic vanish entirely exceeds minimum embedding dimension min map sequence measurements wants know minimum embedding dimension min sensitivity respect dependent variables estimate size noise 
variable dependence mean primary dependence induced ones 
gamma gamma say gamma primary dependent variable induced dependence gamma interest 
variables primary dependence denoted irrelevant 
approach problem constructing conditional probabilities embedding spaces various dimensions time series eq 
represented series points dimensional space gammak distances th components vectors defined jz gamma set positive numbers ffl ffi ffi ffi construct joint series assumed bounded stationary take real complex values 
probabilities data ffl ffi pair ffl ffi ffi pair ffi pair total number vector pairs ffl ffi ffi number pairs satisfying corresponding distance constraints 
freely notation ffi ffi ffi ffi set ffi ffi form conditional probabilities fflj ffi fflj ffi ffl ffi ffi fflj ffi expected vary different conditions 
important observations 
completely random time series ffl fflj ffi fflj ffi identity understood statistical sense holds choice positive ffl ffi 

continuous map exists eq 
intrinsic noise ffl exists ffi ffl fflj ffi ffi ffi ffl smallest integer eq 
holds identified min gamma 

presence noise fflj ffi longer saturate ffl smaller width deltar max noise 
eq 
direct consequence definition function continuity states ffl exists ffi conditions jz gamma ffi jz gamma ffi guarantee jz gamma ffl 
presence noise jz gamma jf gamma gamma ffi smaller deltar jr gamma justifies statement number 
assume flat noise distribution extending gammar standard deviation oe get deltar max oe gives upper limit estimate oe knowing deltar max fflj ffi vary function ffi fixed ffl 
ffi conditions effect 
fflj ffi ffi ffl 
ffi fflj ffi increase monotonically saturate behaviour shown schematically fig 

approach savit green identity fflj ffi ffi ffl gamma fflj ffi ffi ffl establishes variable dependence identity violated 
ambiguities arise irrelevant variables induce sizable changes gamma gamma ffi ffl 
effect occurs due nonuniform curvatures trajectories 
reason examine maxima ffl max ffi fflj ffi fflj ffi ffiffi ffl saturation maxima increases singles irrelevant variables 
maxima ffl change ffl provides basically information need see fig 

ffl measures dynamics modeled terms variables 
quantify dependence variables convenient define dependability index ffl ffl gamma gamma ffl gamma ffl average ffl dffl ffl gamma ffl dffl gamma ffl dffl ffl gamma gamma ffl dffl gamma ffl similar index defined savit green different definition pd ffl normalization 

fflj ffi function ffi fixed ffl 

maxima ffl function ffl 
saturation observed presence noise saturation deviates ffl deltar max noise free deterministic map ffl saturates ffl variable considered irrelevant inclusion condition raise conditional probability higher plateau 
approach savit green negative indices statistical significance complicate issue identifying dependent variables 
case shown 
negative indices arise statistical fluctuations 
expect negative largely due limited statistics maximization procedures 
statistical problems easily clarified inspecting saturation affected treating variable irrelevant 
far formalism assumed infinite amount data 
limited statistics low ffi values large may give rise picture crisp fig 

estimate errors standard estimator deltap fflj ffi gamma ffi error expression entirely adequate correlations exists time series 
serves purpose signal statistically unreliable region crossed 
theoretically fflj ffi expected smooth monotonically decreasing function ffi 
generally flat near ffi plateau extends finite ffi ffl finding approximate maximum difficult provided reasonable amount statistics probe plateau region 
cases limited statistics advantageous set irrelevant variable inactive means condition ffi omitted computing fflj ffi removing unnecessary restrictive conditions way improved statistics obtained affecting evaluation higher dimensions 
implementation issues give systematic procedure read key properties fflj ffi data set 
propose scheme 
compute ffl 

starting compute fflj ffi find maxima ffl 

evaluate dependability index 
optional variable elimination th variable irrelevant set inactive 

increment repeat steps ffl saturates 

identify ffl begins saturate point ffl ffl begins deviate 
minimum embedding dimension noise width estimated ffl option dependability index eq 
modified ffl ffl gamma ffl gamma ffl gamma nearest active variable summations eq 
restricted active variables 
follows ffl ffi expressed units oe standard deviation data set 
obtain map ffl ffi go data set statistics time ffl ffi changed discretize ffl ffi plane record statistics bin sum contents bins progressively ffl ffi obtained grid ffl ffi values going data 
adopted approach sake computing speed discretized ln ffl ln ffi plane theta bins ln gamma ln ffl ln ln gamma ln ffi ln 
lower cut ffl min ffl integrations eq 

parameters critical outcome method 
possible design algorithms 
applications apply method synthetic time series sunspot data problems 
logistic map generate time steps jx gamma gamma gamma giving oe 
iterative noise consists random numbers uniformly distributed gammar 
data sets generated oe respectively 
order keep series bounded constrained 
fig 
show fflj ffi versus ffi ffl various observe saturation fflj ffi case expected dimensional noise free map 
conditional probabilities saturate case indicates presence noise 
fig 
maximized conditional probability ffl shown function ffl various dimensions noise free data ffl saturates entire ffl range 
noisy data ffl saturates approximately saturation value starts fall implying noise level delta max oe oe consistent generated noise level 
calculating dependability indices gives data noisy data 
map generate time steps gamma gamma gamma gamma gamma gamma gamma giving oe 
usual map dependences stretched larger lags noise additively applied 
data sets oe 
variable elimination option 
obtain gamma 
dependences gamma gamma emerge large values indicating noise free map value signalling presence noise 
sunspot data priestley contains annual averaged sunspot activities points 
limited statistics data set 
resolution limit extrapolate ffl behaviour set ffl 
reason explore different input representations linear logarithmic different sensitivities may give rise complete picture 
variable elimination option shown fig 
cases 
fig 
see probability conditional variables gamma gamma gamma gamma gamma approximately saturates logarithmic representation fig 
variables gamma gamma gamma gamma show relevant 
probing smaller ffl statistics clarify situation 
results fig 
approximately determine embedding dimension gamma gamma gamma gamma gamma gamma important variables find sunspot data masked large noises amplitude order oe 
impact neural network learning feeding mlp redundant variables avoided fitting noise increases difficulty learning may give rise poor generalization 
weigend 
mlp layer hidden nodes trained sunspot series data 
authors experiment different number time lags inputs conclude basis generalization performance optimal number reflects embedding dimension 
ffi test results guide relevant input variables 
procedures weight elimination weigend 
trained mlp sigmoidal hidden units linear output unit various number input units 
gamma gamma gamma gamma gamma gamma inputs find mlp ratio mean square error variance test data 
performance achieved nowlan hinton sun spot series variance oe 
oe value quoted weigend 
order compare established results 
sophisticated algorithms 
fig 
show learning curves input network ones network lag variables weigend 

interesting notice training network weight elimination weigend 
find large connections hidden nodes inputs gamma gamma gamma consistent ffi test findings 
ffi test powerful comes analyzing residual error output units mlp learning 
perfect training ffi test identify residual series independent random series 
case test singles relevant input units information fully extracted network 
summary devised general method ffi test identifying dependencies continuous functions 
limited linear correlations determines embedding dimensions dependencies noise levels fairly accurately cases low statistics 
automated procedures setting bin sizes cutoffs error analysis feasible 
conditional probabilities approach sight appears similar savit green 
conceptually distinct 
grassberger procaccia correlation integral ffi ffl 
contrast method fundamental property functional continuity ffi 
savit green approach mainly problems pi peterson induced dependences show index nonuniform curvature function map saturation measure indicate critical embedding dimension indication quantify noise level 
able extract information unambiguously region fflj ffi maximized 
similarities kolmogorov entropy method kolmogorov methods examine behaviour certain set conditional probability distributions 
evaluating entropy high dimensional joint probability distribution requires huge amount statistics contrast method 
indebted richard bringing savit green attention 
brock scheinkman lebaron 
test independence correlation dimension university wisconsin preprint 
farmer 
information dimension probabilistic structure chaos 
russell hanson ott 

dimension strange attractors phys 
rev lett 

grassberger procaccia 
measuring strange attractors physica 
kolmogorov 
dokl 
akad 
nauk sssr entropy unit time metric invariant automorphisms math 
rev 
nowlan hinton 
simplifying neural networks soft neural computation 
pi peterson 
published 
priestley non linear non stationary time series analysis academic press 
savit green 
time series dependent variables physica 
weigend huberman rumelhart 
predicting connectionist approach international journal neural systems 
logistic map fflj ffi function ffi noise free noisy data 
curves represent solid dashed dash dotted dotted respectively 
ffl function ffl 
curves corresponds noise free data notation points noisy data 
upper curve displays fall ffl unity enlarged scale 
fflj ffi sunspot data estimated errors shown function ffi ffl various dimensions marked curve embedding gamma embedding ln gamma 
plateau fail rise omitted 
learning curves sun spot data shown versus training epochs network network 
solid lines training set lower dotted lines test set upper dotted lines test set ii 
large fluctuations curves filtered 

