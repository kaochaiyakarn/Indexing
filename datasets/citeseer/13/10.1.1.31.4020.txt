smoothing local regression principles methods william cleveland loader bell laboratories mountain avenue murray hill nj usa 
summary local regression old method smoothing data having origins graduation mortality data smoothing time series late th century early th century 
new local regression continues rapid pace 
review history local regression 
discuss basic components chosen local regression practice weight function parametric family fitted locally bandwidth assumptions distribution response 
major theme choices represent modeling data different data sets deserve different choices 
describe polynomial mixing method enlarging polynomial parametric families 
introduce approach adaptive fitting assessment parametric localization 
describe approach design adaptive procedures automatically chooses mixing degree mixing polynomials cross validation chooses bandwidth comment efficacy asymptotics provide guidance methods local regression 
keywords nonparametric regression loess bandwidth polynomial order polynomial mixing ridge regression statistical graphics diagnostics modeling modeling local regression approach fitting curves surfaces data smoothing fit value parametric function fitted observations neighborhood spencer henderson macaulay kendall kendall stuart stone cleveland katkovnik friedman stuetzle hastie tibshirani hastie loader fan 
underlying model local regression observations response tuples observations independent variables form design space model 
distribution including means unknown 
practice model data means making certain assumptions aspects distribution example common distributional assumption constant variance 
supposed function approximated locally member parametric class frequently taken polynomials certain degree 
refer parametric localization 
carrying local regression parametric family just global parametric fitting ask family fit locally globally 
parametric localization fundamental aspect distinguishes local regression smoothing methods smoothing splines wahba regression splines knot selection friedman wavelets donoho johnstone notion implicit methods variety ways 
estimation estimation arises modeling simple 
fitting point define neighborhood metric dimensional design space independent variables 
neighborhood assume approximated member chosen parametric family 
example family quadratic polynomials gamma gamma estimate parameters observations neighborhood local fit fitted function evaluated want incorporate weight function gives greater weight neighborhood close lesser weight 
criterion estimation depends assumptions distribution example suppose approxi mately gaussian constant variance sense base estimation squares 
parametric family consists quadratic polynomials minimize gamma gamma gamma gamma gamma gamma case pleasant case 
parameter estimates simple closed form expression numerical solution fitting equations straightforward 
provided neighborhood size depend response resulting estimate linear function leads simple distribution theory mimics closely distributions parametric fitting intervals tests invoked cleveland devlin 
possibilities specification bandwidth point design space 
choice simply constant 
fixed bandwidth selection parameter case neighborhood 
note fitting say quadratic polynomials locally gets large local regression fit approaches global quadratic fit 
change function values example nearest neighbor bandwidth selection case parameter ff 
ff multiply ff round integer take neighborhood distance kth closest ff distance furthest multiplied ff number numeric independent variables involved fit 
fixed bandwidth selection bandwidth parameter gets large local fit approaches global parametric fit 
possibilities parametric family fitted locally 
common polynomials degree describe polynomial mixing enlarges families polynomials replacing usual integer degree continuous parameter mixing degree 
successfully fit curves surfaces selecting single parametric family example take fitting locally 
single value bandwidth parameter ff nearest neighbor parameter 
adaptive procedure selects degree locally function selects bandwidth locally selects locally 
modeling data local regression practice choose weight function bandwidth parametric family fitting criterion 
choices depend assumptions behavior fourth choice depends assumptions aspects distribution words parametric fitting modeling data 
need rely fully prior knowledge guide choices 
data 
graphical diagnostic tools residual dependence plots spread location plots residual quantile plots cleveland 
model formal model selection criteria mallows cross validation stone 
example cleveland devlin select bandwidth parameter nearest neighbor fitting 
modeling usually comes trade variance bias 
applications strong inclination small variance applications strong inclination small bias 
advantage model selection criterion automated 
disadvantage easily go wrong give poor answer particular application 
advantage graphical diagnostics great power see space independent variables bias occurring variability greatest 
allows decide relative importance 
example underestimating peak surface curve quite undesirable accept lower variance result peak distortion 
disadvantage graphical diagnostics labor intensive excellent picking small number model parameters practical adaptive fitting practical matter requires automated selection criterion 
example friedman stuetzle cross validation choose bandwidth locally describe adaptive procedures crossvalidation 
final adaptive fit hand critical subject graphical diagnostics study performance 
important implication statements choices tailored data set practice choices represent modeling data 
widely accepted global parametric regression variety choices example parametric family fitted form distribution response rely knowledge mechanism generating data model selection diagnostics graphical diagnostic methods choices 
true smoothing 
cleveland presents examples modeling process 
example application nitrogen automobile engine fitted equivalence ratio fuel compression ratio engine 
show reasonable local parametric family added assumption fitted linear 
quite easy achieve conditionally parametric fit simply ignore defining weights gamma 
addition residual diagnostic plots show distribution errors strongly means abandon squares fitting criterion methods robust estimation 
examples illustrate explain modeling process 
local regression 
local regression strengths discussed detail hastie loader adapts bias problems boundaries regions high curvature 
easy understand interpret 
methods developed provide fast computation independent variables 
simplicity tailored different distributional assumptions 
having local model just point estimate enables derivation response adaptive methods bandwidth polynomial order selection straightforward manner 
require smoothness regularity conditions required methods boundary kernels 
estimate linear response provided fitting criterion squares model selection depend response 
singly provides strong reason favor local regression smoothing methods smoothing splines regression splines knot selection wavelets various modified kernel methods 
combination issues combine local regression attractive 
contents history local regression reviewed section 
polynomial mixing described section 
sections choices required carrying local regression discussed 
section discusses approach making judgments efficacy methods local regression 
section discusses design weight function choice straightforward just main desiderata apply applications underlying dependence described continuous single weight function serve applications 
section discusses fitting criteria little say considerations making distributional assumptions global parametric regression 
section discusses bandwidth selection choice local parametric family issues complex potential paths followed particular application inevitably rely data help choose path 
help convey salient points section examples section 
section adaptive methods chooses bandwidth locally function chooses mixing polynomial degree locally function section discuss asymptotic theory particular determined asymptotic theory current state 
section summarizes drawn 
historical review early local regression natural extension parametric fitting natural local regression arose independently different points time different countries th century 
setting early univariate equally spaced setting simple performing smoothers developed computationally feasible hand calculation 
discussion subsection set 
th century arose studies mortality sickness rates smoothed function age 
reports smoothing danish early methods published years 
gram published doctoral dissertation local polynomial fitting uniform weight function weight function tapers zero 
focused local cubic fitting binomial coefficients weights 
reports united states de forest local polynomial fitting smoothing data 
de forest investigated optimization problem similar studied henderson describe shortly 
de forest focused local cubic fitting 
britain smoothing begun published method local quadratic fitting 
method received discussion eventually method spencer popular computationally efficient performance 
spencer developed smoothers different bandwidths 
point rule yields smoothed values gamma rep resentation gamma notation means take symmetric weighted moving average length weights gamma gamma take unweighted moving sums lengths divide 
resulting fit gamma symmetric values gamma gamma gamma gamma gamma gamma note crucial properties 
smoother exactly reproduces cubic polynomials 
second smoothing coefficients smooth function decay smoothly zero ends 
third smoothing carried applying sequence smoothers simple done facilitate hand computation 
achieving properties remarkable spencer spent considerable effort deriving summation formulas various lengths satisfy properties 
ask put summation formula spencer point rule category local fitting 
answer provided interesting henderson weighted local cubic fitting 
weight function gammam henderson showed local cubic fit written gammam oe oe cubic polynomial coefficients property smoother reproduces data cubic 
symmetric oe quadratic 
henderson showed converse coefficients cubic reproducing summation formula fc sign changes formula represented local cubic smoothing weights cubic polynomial oe oe example spencer term summation formula take oe gamma weight function gamma times values henderson considered problem obtaining smoothest possible fit subject reproduction 
smoothness measured sum squares third differences smoother weights equivalently sum squares third differences fit 
closed form solution smoother coefficients gamma gamma gamma am gamma gamma gamma gamma gamma gamma am term weights add 
result previous paragraph summation formula equivalent local cubic fitting neighborhood weight function gamma gamma gamma gamma jkj gamma 
large amounts weight function gamma asymptotic optimality problems type considered muller equally spaced solved exactly henderson finite samples 
known henderson british smoothing research community included accomplished applied mathematicians whittaker henderson invented smoothing splines 
influence resulted movement local fitting methods time series literature 
example book smoothing time series macaulay shows local fitting methods applied purpose economic series 
macaulay reported earlier local fitting developed methods smoothing seasonal time series 
macaulay book turn substantial influence major milestone local fitting methods 
began bureau census 
series computer programs developed smoothing seasonal adjustment time series culminating method young 
represented earliest uses computer intensive statistical methods life early 
widely penetrate standard statistics literature time methods empirically emanating fully specified statistical model 
standard seasonal trading day adjustment economic time series widely today 
time series fitted additive components trading day component described parametric function day week variables fitted parametric regression trend component fitted smoothing seasonal component fitted smoothing 
developers known decades backfitting algorithm friedman stuetzle iteratively estimate components 
iterations nested inside iterations provide robust fitting identifying outliers modifying data corresponding outliers 
inception employed semi parametric additive models backfitting robust estimation decades methods commonplace statistics 
early literature smoothing local fitting focused independent variable equally spaced values intuition built smoothing methods remain valid smoothing function scattered multivariate measurements 
invoke intuition 
modern modern view smoothing local regression origins kernel methods introduced density estimation setting rosenblatt parzen regression setting nadaraya watson 
new view extended smoothing function single independent variable equally spaced measurements smoothing function scattered measurements independent variables 
kernel methods special case local regression kernel method amounts choosing parametric family consist constant functions 
recognizing weaknesses local constant approximation general local regression enjoyed late stone cleveland katkovnik 
method branches scientific literature example numerical analysis lancaster 
furthermore early smoothing assumption near gaussian distribution modern view extended smoothing distributions 
brillinger formulated general approach cleveland katkovnik developed robust smoothers 
tibshirani hastie substantially extended domain smoothing distributional settings logistic regression developed general fitting algorithms 
extension new settings continues today fan gijbels loader 
local regression continued 
major thrust application smoothing multidimensional cases 
numerous approaches taken cleveland devlin apply local linear quadratic fitting directly multivariate data 
friedman stuetzle local linear regression basis constructing projection pursuit estimates 
hastie tibshirani local regression additive models 
methods substantial differences data requirements types surface successfully modeled graphical diagnostics help decisions crucial cases 
accompanying modern current smoothing new pursuit asymptotic results 
began earliest papers rosenblatt stone grew greatly intensity muller fan ruppert wand 
section comment role asymptotics local regression 
polynomial mixing common choice local parametric family polynomials degree change degree say degree applications represents substantial change results 
example find degree fitting distorts peak interior configuration observations independent variables degree removes distortion results undue boundary 
cases find wishing compromise 
polynomial mixing provide compromise 
idea global parametric fitting mallows 
description local regression cleveland hastie loader 
mixing degree nonnegative number 
integer mixed fit simply local polynomial fit degree suppose integer integer 
mixed fit simply weighted average local polynomial fits degrees weight gamma weight 
easy see amounts local ridge regression polynomial degree zero ridge parameters monomial terms term degree 
choose single mixing degree build adaptive method letting vary approaches sections 
look guidance making choices 
sections turn making choices necessary carry local regression weight function bandwidth fitting criterion local parametric family 
look guidance choices 
answer emphasized treat choices degree bandwidth modeling data formal model selection criteria graphical diagnostics provide guidance 
happens process build knowledge base tends provide models 
sense statement behavior data performance selection methods 
quite definitive theorem process get guidance approach new set data 
development methods parametric regression long history model selection criteria diagnostic methods common parametric models fitted regression data daniel wood 
credit researchers parametric regression methods part mainstream statistical practice 
reporting works practice process greatly strengthened parametric fitting 
attention needs data arise practice assessment methods works practice important part early smoothing literature discussed section 
coming sections draw early literature 
selecting weight function suppose data analyzed continuous suppose higher derivatives continuous necessary produce estimates reproduce discontinuity 
case want consider weight functions peaked decay smoothly increases 
alternative rectangular weight function boxcar 
boxcar observations distance receive weight away receive weight 
results noisy estimate changes observations abruptly switch smoothing window 
smooth weight function results smoother estimate 
widely appreciated early smoothing literature 
example macaulay writes smooth weight diagram leads smoothness resulting graduation smoothing means weighted unweighted moving average amounts distributing observation region long weight diagram shape weight diagram 
second want weight function nonzero bounded interval example approaching zero gets large 
reason computational speed simply ignore observations zero weight 
constraints choice critical examples cases smooth weight function loess fitting procedure cleveland cleveland devlin ae gamma juj juj juj cases locally approximated polynomials helpful consider quite different weight functions 
example case discontinuous sided kernels employed mcdonald owen loader 
selecting fitting criterion turns virtually global fitting procedure localized 
local regression proceed rich collection distributional assumptions global parametric fitting 
methods making choice proceed global parametric fitting 
simplest case discussed gaussian objection squares lack robustness estimates quite sensitive heavy tailed residual distributions 
data suggest distributions robust fitting procedures example iterative loess cleveland cleveland devlin implements local 
robust fitting discussed detail tsybakov 
approach taken local bayesian regression 
error distributions lead fitting criteria brillinger hastie tibshirani 
example double exponential distribution leads local regression 
interesting extension generalized linear models example binary data 
suppose gamma locally weighted likelihood gamma gamma log log gamma 
sensible cases model natural parameters case local polynomials transform quantities interest 
density estimation observe xn density 
loader uses local likelihood gamma log gamma gamma du model density natural polynomials log 
see jones discussion generalizations 
selecting bandwidth local parametric family section discuss bandwidth local parametric family choices interact strongly 
change parametric family dramatic effect sensible choice bandwidth 
goal choosing bandwidth local parametric family produce estimate smooth possible distorting underlying pattern dependence response independent variables 
words want little bias possible small variance possible 
usually need strike balance right model selection tools graphical diagnostics applications find parametric family select bandwidth satisfy needs analysis 
methods bandwidth specification considered section nearest neighbor fixed 
simple easily implemented 
section consider adaptive bandwidth selection 
ordinary polynomials mixed polynomials parametric family degrees ranging single degree 
section consider adaptive selection mixing degree 
applications particularly smooth provide sufficient flexibility get fits 
bandwidth selection nearest neighbor bandwidths widely local regression stone cleveland fan gijbels 
reason simple 
fixed bandwidth estimate dramatic swings variance due large changes density data design space leading unacceptably noisy fits extreme cases empty neighborhoods lead undefined estimate 
boundary regions play major role bandwidth choice 
suppose independent variable distributed uniformly 
estimating bandwidth cover half data bandwidth estimating data side fitting point boundary case 
bandwidth boundary interior point clearly results high variability 
practice situation worse data density may sparse boundary regions example gaussian distribution 
situation worse dimensions data design space lie boundary 
boundary region problem known early researchers smoothing 
example kendall writes local polynomials expect nearer tails reliable trend point measured error reducing power point 
fitted curve said tends wag tail 
fixed bandwidth selection provide fits cases nearest neighbors appear perform better applications variance issue 
course nearestneighbor bandwidth selection fail model data specific examples 
usually nearest neighbor selection fails fixed bandwidth selection needed remedy problem adaptive methods 
take section 
stated earlier approach set data notion bandwidths range small large 
asymptotic assumption bandwidth going zero quite large bandwidths important practice 
example applications nearest neighbor bandwidth selection get best fits small values ff say smaller case time series goal track high frequency component 
applications best fits provided large values ff say higher 
reaction large bandwidth probably global parametric function fits data 
necessarily 
large bandwidths provide large number degrees freedom substantial flexibility fits 
example loess fitting local fitting weight function nearest neighbor bandwidth selection bandwidth parameter ff polynomial fitting degree independent variables degrees freedom fit roughly ff number degrees freedom global parametric fit polynomial degree cleveland grosse 
global parametric degrees freedom multiplied ff loess fit means example ff global parametric degrees freedom multiplied 
polynomial degree choice polynomial degree mixed polynomial degree ordinary polynomial degree bandwidth bias variance trade 
higher degree generally produce biased variable estimate lower degree 
asserted local polynomials odd degree beat degree degree beats degree degree beats degree forth 
applications sensible think local regression models ranging small neighborhoods provide local fits large neighborhoods provide nearly global fits infinite neighborhoods result globally parametric fits 
words globally parametric fitting limiting case local regression 
obvious rule fitting degrees sensible ruling degrees global polynomial fitting 
degree infrequently proves best choice practice degree zero locally constant fitting 
widely appreciated early smoothing literature 
term moving average moving average positive weights 
fact standard methods preserved quadratic cubic polynomials context mean degrees higher 
macaulay writes moving averages away boundary example simple moving average applied data underlying trend second degree parabolic type falls parabola 
little thought experimentation quickly convince reader long restrict positive weights moving average weighted unweighted exactly fit mathematical curve straight line 
exactly point macaulay holds local regression generally 
applications single degree polynomial fitting chosen careful modeling data seldom leads locally constant fitting 
general problem locally constant fitting reproduce line special cases example data away boundaries 
reducing lack fit tolerable level requires quite small bandwidths producing fits rough 
polynomial degree greater zero typically increase bandwidth large amount introducing intolerable bias despite increased number monomial terms fitting result far smoother neighborhood size far larger 
fact local linear fitting fails provide sufficiently local approximation rapid change slope example local minimum maximum local quadratic fitting better 
reason early smoothing literature methods devised preserved quadratic functions peaks valleys common practice 
examples section examples demonstrate modeling data choices weight function fitting criterion bandwidth parametric family 
cross validation graphical diagnostics guide modeling 
ozone wind speed data example measurements variables ozone concentrations wind speed ground level days new york city may september year 
take cube roots ozone concentrations distribution residuals model cube root ozone function wind speed 
fit choices weight function nearest neighbor bandwidth selection squares polynomial mixing 
shows cross validation sum absolute deviations mixing degrees values ff equal percentage steps 
rough approximation referred equal percentage steps ff tend degrees freedom fits fixed mixing degree change equal percentage steps 
shows fits 
column contains fits value ff 
top row mixing fit minimum cross validation score remaining rows show fits degrees 
shows residuals fits 
superposed plot loess smooth local linear fitting ff 
residual plots provide exceedingly powerful diagnostic nicely complements selection criterion cross validation 
diagnostic plots show lack fit locally opportunity judge lack fit knowledge mechanism generating data knowledge performance smoothers fitting 
mixing degree score cross validation scores mixed fits function ozone wind speed data pronounced dependence response independent variable radical changes curvature particular peaks valleys 
expect locally linear family provides reasonable approximation 
diagnostics cross validation show case 
local constant fitting small ff needed capture dependence ozone wind speed introducing undue distortion 
ff plot residuals suggests lack fit smallest values wind speed left boundary large slope 
local constant fitting capture linear effect boundary 
distortion occur right boundary local slope zero local constant fitting cope zero slope 
despite distortion ff cross validation criterion suggests local constant fitting best fit larger values variability fit increases rapidly 
local constant fit quite noisy far noisier reasonable 
words satisfactory fit ff 
increase ff get smoother fit local constant fit introduces degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree mixing fit mixing fit mixing fit mixing fit wind speed mph fits nearest neighbor bandwidths ff equal percentage steps local fitting methods 
top row shows fits mixing method cross validation remaining rows show local polynomial fits degrees 
degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree mixing fit mixing fit mixing fit mixing fit wind speed mph cube root ozone cube root residual plots fits previous 
major distortion bad cross validation judges worst case minimum cross validation score occurs close 
ff minimum cross validation score fit quite smooth residual plot suggests lack fit 
example data turn data generated 
study smoothers useful addition real data data true model known 
take gamma sin ffl 
consider smoothing fixed nearest neighbor selection local polynomial fitting degrees 
compare smooths 
real set data cross validation case know model compute true mean square error summed criterion gamma gamma fk oe tr vector values vector values hat matrix smoother 
final expression equation decomposes mean square error bias variance 
compare different degrees smoothing example local constant versus local quadratic smoothers placed equal footing 
get meaningful results bandwidth higher order fit variable biased 
gain useful insight equivalent amounts smoothing methods consideration 
consider equivalent degrees freedom smooth define tr 
particularly convenient factor oe variance component 
plot displays bias variance tradeoff confounded meaningless bandwidth effects 
shows results nearest neighbor fixed bandwidths 
degrees ranging local constant local cubic considered 
points right plots represent small bandwidths fits little bias substantial variance 
fits values smoothing parameters minimize displayed 
local constant fitting clearly unsatisfactory fixed nearest neighbor bandwidths 
smooths noisy small bandwidths properly track boundary effects 
local linear fitting improved boundary behavior enables larger bandwidths smoother curve results 
example fairly modest curvature local quadratic cubic fits perform better fitted curves substantially noisy 
visually difference fixed nearest neighbor fits higher orders 
best fit local quadratic fit nearest neighbor bandwidths fixed bandwidth selection performs example 
plots tr nearest neighbor bandwidths left fixed bandwidths right fitting degrees 
note entirely scale left 
important reiterate exclusive reliance practice global criterion similar unwise global criterion provide information contributions bias variance coming design space 
applications high bias high variance region may serious 
example careful look right panels reveals degrees minimum maximum values nearly zero residuals fits points nearly interpolating data results unacceptably high variance 
nearest neighbor bandwidths go way relieving problem accounts advantage nearest neighbors observed local quadratic fitting 
example suggests boundaries tend dominate fixed vs nearest neighbor comparison 
asymptotic comparisons provide useful guidance case 
example gasser steinmetz asymptotic characterization nearest neighbor bandwidth proportional reciprocal density design space example ff oe oe standard normal density 
left point gamma ff local linear fit local cubic alpha local cubic local quadratic alpha local quadratic local linear alpha local linear local constant alpha local constant local fits minimize mean square error nearest neighbor bandwidths left fixed bandwidths right 
solid curves estimates dashed curves true function 
ting 
asymptotic characterization yields observations receive weights 
clearly approximate reality observations receive non zero weights weights small 
ozone data ozone wind speed data studied earlier just variables multivariate data set independent variables temperature solar radiation cleveland 
full dataset daily measurements wind speed temperature radiation ozone concentration 
cleveland appropriate fit models conditionally parametric plot data shown degrees variety bandwidths 
local constant fitting vastly inferior degrees 
local linear minimum larger local quadratic 
local quadratic slightly beats local cubic 
minimum local quadratic fitting occurs ff pattern quite flat quadratic fitting minimum fit fairly noisy cleveland elected larger ff 
plot environmental data trivariate predictors 
approach adaptive fitting assessing localization previous sections discussed smoothing procedures parametric family fitted locally change bandwidth selection parameters fixed nearest neighbor smoothing change data sets particularly amount curvature different broad regions design space quite different sense adaptive methods 
adaptive bandwidth methods suggested past friedman stuetzle 
varying parametric family locally common fan gijbels promising 
general approach adaptive fitting assessment parametric localization 
section parametric localization describe basic approach guides local regression approximation true locally parametric function 
assessment localization design methods adaptive bandwidth selection cleveland loader adaptive parametric family selection cleveland hastie loader 
important principle 
particular neighborhood fit expected perform locally fitted parametric function adequately approximates true function neighborhood 
object notion basis possible fit fitted parametric function provides poor approximation places thought clear degenerate cases despite poor approximation 
example equally spaced data away boundary locally constant fitting provide fit linear solely due serendipitous result local linear fitting case happens local constant fitting 
assess fit equally neighborhood 
weight assessment weight function compute fit words greater tolerance poor performance positions far positions close notion weighted assessment parametric localization approach details carrying reasonably straightforward 
fit simply take automatic selection criterion picking global parameter ff nearest neighbors changes 
applying criterion smoother apply criterion parametric function fitted second weight criterion neighborhood weights local fit examples illustrate process 
local polynomial mixing assessment localization develop adaptive method choosing mixing degree polynomial mixing 
method discussed detail cleveland hastie loader 
section polynomial mixing selected single mixing degree cross validation sum absolute deviations 
chose value fitted mixed polynomial get fit studied single observation deletion effect gamma gamma ii ii th diagonal element hat matrix mixed polynomial smoother 
value ii comes hat matrix weighted squares operator fits polynomial notation diagonal element show dependence localize procedure provide adaptive fitting method 
select candidate carry weighted crossvalidation mixed polynomial fitted criterion gamma gamma ii ii th diagonal element hat matrix weighted squares operator fits polynomial compute localized criterion grid values mixing degree mixing degree choose mixing degree minimizes criterion 
illustrate method example 
data sickness rates ages reported spencer 
study percentage change rates smoothing differences natural log rates function age 
fits shown 
column value nearest neighbor bandwidth parameter 
values increase logarithmically 
top row adaptive fit mixing degree chosen locally local method 
remaining rows show fits integer degrees words local cubic fitting local constant fitting 
shows residuals fits superposed plot loess smooth local linear fitting ff 
shows cross validation score function age bandwidths shows selected values mixing degree function age 
displays quite informative 
residual plots show bandwidth smoother just begins introduce distortion bias fit 
degree smallest bandwidth ff slight distortion largest ages 
degree begins ff degree begins ff degree adaptive fit begins ff 
bandwidth degree produce fit distortion revealed residuals plots fit smallest bandwidth noisy 
degrees distortion free fits bit noisy fits smooth distortion quite minor degree ff degree ff degree ff 
saving grace comes degree produce smooth fit major distortion 
locally adaptive mixed fit ff get excellent fit distortion requisite smoothness 
course fit attractive property parametric family chosen automatically 
fit interesting property years just near age pattern begins nonlinear behavior local mixing method switches degree fitting degree fitting 
adaptive bandwidth selection assessment localization develop adaptive method choosing bandwidth local linear polynomial fitting 
method discussed detail cleveland loader 
bandwidth consider localized goodness fit criterion gamma oe tr gamma gamma diag values local polynomial fitted estimated localized version tr tr gamma tr oe gamma wx gamma design matrix oe estimate oe non adaptive smooth small bandwidth 
roughly fitting point choose bandwidth small 
specifically choose interval distance kth nearest small results rejection goodness fit test low significance level 
interval searched right local minimum 
algorithm carried vertices tree manner similar loess cleveland grosse split rule suggested loader 
suppose tree cell vertices adaptive procedure produces bandwidths vertices 
cell requires refinement bandwidth small relative length cell specifically new vertex added gamma min 
function estimate constructed cubic interpolant local fits local slopes vertices 
implementation involves minor modification existing algorithms code includes multivariate extensions rectangular triangular cells 
computation straightforward product local fit rely higher order fits taylor series expansions estimate bias 
top panel shows data set observations response function examples considered degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree mixing fit mixing fit mixing fit mixing fit mixing fit age years fits nearest neighbor bandwidths ff equal percentage steps local fitting methods 
top row shows fits adaptive mixing method remaining rows show local polynomial fits degrees 
degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree degree mixing fit mixing fit mixing fit mixing fit mixing fit age years residual plots fits previous 
age years score cross validation scores adaptive mixing fits 
age mixing degree mixing degrees adaptive mixing fits 
donoho johnstone fan gijbels 
equally spaced denoted increases estimate oe gamma gamma gammay gamma gamma second panel fit truth indistinguishable apart small amounts roughness discontinuities 
fit appears better previously obtained problem sharper reproduction breaks noise 
residual plot third panel shows large residuals discontinuities clear indication fit perfect 
mentioned earlier proper modeling discontinuous functions requires appropriate choice weight function basis functions loader sensibly proceed data practice 
plot fourth panel shows bandwidths knots determined tree algorithm 
theory theory minimal assumptions local regression squares criterion results estimate linear provided model selection depend response 
take fixed 
standard practice estimation sampling distributions conditional conditioning sensible practice argued cox 
exact expressions bias variance easily obtained 
development similar katkovnik 
simplicity treat case independent variable local polynomial fitting degree continuous second derivative taylor theorem enables write gamma gamma gamma gamma 
local regression wx gamma column vector fitting functions design matrix rows diagonal matrix weights oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo residuals oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo oo bandwidth oo adaptive bandwidth selection 
top panel bottom graph shows data adaptive local linear fit residuals local bandwidths fitted knots 
gamma 
note particular just mathematical statement obvious exactly fitting functions reproduced exactly 
local linear fitting linear functions reproduced exactly 
particular gamma 
gamma fits degree higher retain terms taylor series leading precise bias estimates 
variance simple closed form expression var oe kl oe wx gamma wx gamma expressions exact asymptotics 
fits terms bias expansion involving gone finite samples values design space bizarre 
expressions easily computed compared various estimates specific examples useful assessing variance estimate practice 
asymptotic analysis performance comparison estimates tractable necessary assumptions section 
emphasize stone additional assumptions required retain properties local regression 
asymptotic theory results stated independent variable new 
tsybakov muller derive local regression similar expressions kernel regression density estimation known longer 
ruppert wand derive similar results multivariate predictors 
design assumptions describing design behaves function common assumption random design independent random variables density 
simply device taken signal data analyses model data random fixed design gamma gamma du 
fitting odd order polynomials degree gamma var oe nh gamma points design density continuous non zero 
constants depending weight function letting gamma dx 
gamma gamma dx gamma gamma assuming support gamma 
fitting order polynomials degree gamma var oe nh gamma points differentiable 
shown difference successive orders bias term 
lower boundary point typically defined point satisfying delta right continuous fan gijbels 
example lower boundary upper boundary 
boundary regions asymptotic results modified 
suppose boundary point hv fixed order fitting gamma variance expansions odd order bias modified constants forms similar integrals taken gammav 
biggest difference bias order fitting bias 
obvious difference usually asymptotics perform 
studies simulated data previously fitting local constant local cubic 
asymptotics suggest variance fitted degrees freedom orders local constant local linear local quadratic local cubic 
problem note asymptotics weakness setting 
consider problem boundary bias 
definition boundaries design density compact support bounded away support 
definition case boundaries 
saw earlier substantial boundary effects problem 
local constant linear fitting asymptotics fine 
standard deviation approximation slightly conservative cases 
note departure asymptotic theory local linear method variable local constant near boundaries 
local quadratic cubic fitting asymptotics perform poorly 
bias approximations completely misleading peaks zeros having little relation reality 
limitations asymptotics noted various settings 
example rosenblatt discussion optimal kernels states arguments usually asymptotic character mistake take literally finite sample point view 
goes challenges standard approach asymptotics suggesting bandwidth fixed gives realistic assessment performance estimates 
clear message caution realistic problems modeled asymptotic framework asymptotic approximations quite poor 
example mistake consider procedures equivalent purely basis having asymptotic expansions asymptotics justify smoothing procedures 
unfortunately asymptotics kernel smoothing literature past years 
sweeping drawn solely asymptotics cases procedures labeled optimal real justification 
examples sections bandwidth choice polynomial degree 
bandwidth decade effort expended devising methods automatically select bandwidth assessing close selected bandwidth theoretically optimal bandwidth little regard possibility optimal bandwidth may inadequate 
major focus plug methods hall directly estimates bias variance asymptotic approximations choosing bandwidth attempting minimize pointwise mse global criterion mise 
key problem bias estimation 
heavily biased estimate bias estimation reasonable problem 
local linear estimate considered bias estimation essentially amounts estimating curvature accomplished local quadratic cubic fit higher order kernels usually employed literature 
plug algorithm quite criterion close selected bandwidth optimal bandwidth 
criterion quality bandwidth selector quality resulting estimate 
available information curvature go directly estimate just bandwidth selector 
resulting smoother heavily biased bias difficult estimate 
problem plug methods local cubic local quadratic local linear local constant local cubic local quadratic local linear local constant biases top panel standard deviations bottom panel asymptotic approximations 
clear local quadratic cubic fits bias estimation essentially amounts estimating fourth order derivatives data contains little information 
case asymptotic expressions poor 
quote katkovnik attempts select theoretical precision estimates productive require assigning type information function noise usually exist apriori 
weakness plug methods seen occasionally acknowledged comparison assumptions optimal rates convergence 
bandwidth selectors second order kernel estimates usually derived assumption derivatives assumption best possible rate convergence gamma stone 
second order kernel method achieves rate better gamma regardless bandwidth selector difference small advantage going higher order methods convincingly demonstrated examples 
plug bandwidth selector local constant local linear regression suggest comparison fit local quadratic regression variance measured equivalent degrees freedom tr pointwise bandwidth selectors kl 
asymptotically local quadratic method improve local linear existence second derivative assumed 
improvement observed practice simple examples studied 
degree fit question provoked discussion relative merits successive odd orders local constant versus local linear fitting local quadratic versus local cubic fitting 
comparison quite different interior points boundary regions seen examples asymptotics 
consider case local constant versus local linear fitting 
noted earlier local constant fitting bias size boundaries interior points 
local linear fitting bias 
similar comparison holds higher orders fitting order bias fitting order bias boundaries interior points 
observation argument favor odd order fitting imply odd orders boundary effects 
variance far ignored 
local polynomial fitting variable boundary regions variance increase pronounced odd orders shown example 
conclude boundary regions dominate comparison 
local constant fitting boundary bias caused slope widely recognized unacceptable local linear usually preferable 
examples support 
comparison local quadratic local cubic clear cut 
summary detailed discussions major embedded 
simply 
carrying local regression parametric family just global parametric fitting ask family fit locally globally 
parametric localization 
important aspects local regression weight function bandwidth selection local parametric family fitting criterion 
choices aspects represent modeling data guided automatic selection criteria graphical diagnostics 
polynomial mixing provides continuous progression polynomial fits local constant fitting proceeding continuously higher orders 
early smoothing literature decades twentieth century provides number important insights choices spencer henderson macaulay 
example nearly literature applications weight function needed smooth local constant fitting inadequate smoothers needed reproduce exactly just asymptotically quadratic 
despite important widely intuition theoretical smoothing research early focused locally constant fitting ignored higher order polynomial fitting papers fan hastie loader appeared 
fortunately success local polynomial methods practice stone cleveland friedman stuetzle cleveland devlin hastie tibshirani established local constant fitting standard approach data analysis 
bandwidth specifications fixed nearest neighbor attractive property typically radical swings variance smooth 
typically method works radical changes smoothness function case adaptive methods helpful 
introduce assessment parametric localization general approach adaptive fitting 
discuss methods adaptive selection mixing degree adaptive bandwidth selection 
leads far better adaptive procedures pointwise bias estimation methods originated kernel literature 
asymptotics smoothing give indicators works number observations finite 
principal problem finite samples boundary regions important bandwidths large 
unfortunately asymptotic results interpreted face value 
drawn literature fit odd order polynomial better lowest order fixed bandwidth smoothing better nearest neighbor hold finite samples 
brillinger 

discussion stone 
ann 
statist 

cleveland 

robust locally weighted regression smoothing scatterplots 
amer 
statist 

cleveland 

visualizing data 
hobart press summit nj books hobart com 
cleveland devlin 

locally weighted regression approach regression analysis local fitting 
amer 
statist 

cleveland hastie loader 

adaptive local regression automatic selection polynomial mixing degree 
preparation 
cleveland grosse 

computational methods local regression 
statist 
computing 
cleveland grosse 

local regression models 
statistical models chambers hastie editors pages 
chapman hall new york 
cleveland loader 

computational methods local regression th century 
preparation 
cox 

problems connected statistical inference 
ann 
math 
statist 

daniel wood 

fitting equations data 
wiley new york 
de forest 

methods interpolation applicable graduation irregular series 
annual report board smithsonian institution 
de forest 

additions memoir methods interpolation applicable graduation irregular series 
annual report board smithsonian institution 
donoho johnstone 

ideal spatial adaptation wavelet shrinkage 
biometrika 
fan 

local linear regression smoothers minimax efficiencies 
ann 
statist 

fan gijbels 

variable bandwidth local linear regression smoothers 
ann 
statist 

fan gijbels 

censored regression local linear approximations applications 
amer 
statist 

fan gijbels 

adaptive order polynomial fitting bandwidth bias reduction 
unpublished manuscript available ftp stat unc edu 
fan gijbels 

data driven bandwidth selection local polynomial fitting variable bandwidth spatial adaptation 
royal statist 
soc ser 

friedman multivariate adaptive regression splines 
ann 
statist 

friedman stuetzle 

projection pursuit regression 
amer 
statist 

friedman stuetzle 

smoothing scatterplots 
technical report orion dept statistics stanford university 
gasser th 
steinmetz 

unifying approach nonparametric regression estimation 
amer 
statist 

gram 

uber der methode der 
math 

hall sheather jones marron 

optimal data bandwidth selection kernel density estimation 
biometrika 


applied nonparametric regression 
oxford university press oxford 
hastie loader 

local regression automatic kernel carpentry discussion 
statist 
science 
hastie tibshirani 

generalized additive models 
chapman hall london 
henderson 

note graduation adjusted average 
soc 
amer 

henderson 

new method graduation 
soc 
amer 



trio little known early discoveries life insurance mathematics oppermann thiele gram 
inter 
stat 
rev 


local bayesian regression 
unpublished manuscript 
jones 

locally parametric nonparametric density estimation 
unpublished manuscript 
katkovnik ya 

linear nonlinear methods nonparametric regression analysis 
soviet automatic control 
kendall 

time series 
oxford university press oxford 
kendall stuart 

advanced theory statistics vol 

hafner new york 
lancaster 

surfaces generated moving squares methods 
mathematics computation 
lancaster 

curve surface fitting 
academic press london 
loader 

change point estimation local regression 
unpublished manuscript available ftp netlib att com 
loader 

computing nonparametric function estimates 
preparation 
loader 

local likelihood density estimation 
ann 
statist 
appear 
macaulay 

smoothing time series 
national bureau economic research new york 
mallows 

comments technometrics 
mallows 

discussion tukey 
technometrics 
mcdonald owen 

smoothing split linear fits 
technometrics 
uller 

smooth optimum kernel estimators densities regression curves modes 
ann 
statist 

uller 

weighted local regression kernel methods nonparametric curve fitting 
amer 
statist 

nadaraya 

estimating regression 
theor 
probab 
appl 

parzen 

estimation probability density function mode 
ann 
math 
statist 

rosenblatt 

remarks nonparametric estimates density function 
ann 
math 
statist 

rosenblatt 

curve estimates 
ann 
math 
statist 

ruppert wand 

multivariate locally weighted squares regression 
ann 
statist 

young 

variant census method ii seasonal adjustment program 
technical bureau census 


discussion donoho royal statist 
soc ser 

spencer 

graduation rates sickness mortality 
inst 
act 

spencer 

graduation sickness table hypothesis 
biometrika 


kernel estimate regression function likelihood models 
amer 
statist 



mathematical statistics early states 
ann 
statist 



smoothing bias density derivative estimation 
amer 
statist 

stone 

consistent nonparametric regression discussion ann 
statist 

stone 

optimal rates convergence nonparametric estimators 
ann 
statist 

stone 

cross choice assessment statistical predictions discussion 
statist 
soc 

tibshirani hastie 

local likelihood estimation 
amer 
statist 

tsybakov 

robust reconstruction functions local approximation method 
prob 
inform 
trans 

watson 

smooth regression analysis 
ser 

wahba 

spline functions observational data 
siam philadelphia 
whittaker 

new method graduation 
proc 
edinburgh math 
soc 



explanation new method adjusting mortality tables observations modification theory 
inst 
act 

