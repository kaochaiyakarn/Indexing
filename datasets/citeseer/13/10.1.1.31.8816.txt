kernel methods survey current techniques colin campbell department engineering mathematics bristol university bristol bs tr united kingdom kernel methods increasingly popular tool machine learning tasks involving classification regression novelty detection 
exhibit generalisation performance real life datasets approach properly motivated theoretically 
relatively free parameters adjust architecture learning machine need experimentation 
tutorial survey subject principal focus known models kernel substitution support vector machines 

support vector machines svms successfully applied number applications ranging particle identification face identification text categorisation engine knock detection bioinformatics database marketing 
approach systematic properly motivated statistical learning theory 
training involves optimisation convex cost function false local minima complicate learning process 
approach benefits example model constructed explicit dependence informative patterns data support vectors interpretation straightforward data cleaning implemented improve performance 
svms known class algorithms idea kernel substitution broadly refer kernel methods 
tutorial introduce subject describing application kernel methods classification regression novelty detection different optimisation techniques may required training 
tutorial exhaustive approaches kernel pca density estimation considered 
thorough treatments contained books cristianini shawe taylor vapnik classic textbook statistical learning theory edited volumes 
fig 

perpendicular distance separating hyperplane hyperplane closest points support vectors called margin 
examples support vectors opposite sign 
learning support vectors 
introduce subject outlining application support vector machines simplest case binary classification 
perspective statistical learning theory motivation considering binary classifier svms comes theoretical bounds generalisation error 
quote relevant theorem note important features 
firstly upper bound generalization error depend dimension space 
secondly error bound minimised maximising margin fl minimal distance hyperplane separating classes closest datapoints hyperplane 
consider binary classification task datapoints having corresponding labels sigma decision function sign delta dataset separable data correctly classified delta 
clearly relation invariant positive rescaling argument inside sign function define canonical hyperplane delta closest points side delta gamma closest side 
separating hyperplane delta normal vector clearly jjwjj margin projection gamma vector see 
delta delta gamma means margin fl jjwjj maximise margin task minimise jjwjj subject constraints delta learning task reduced minimisation primal lagrangian delta gamma ff delta gamma ff lagrangian multipliers ff 
derivatives respect back primal gives wolfe dual lagrangian ff ff gamma ff ff delta maximised respect ff subject constraint ff ff far haven second feature implied generalisation theorem mentioned bound depend dimensionality space 
dual lagrangian notice datapoints appear inside inner product 
get better representation data map datapoints alternative higher dimensional space called feature space replacement delta oe delta oe functional form mapping oe need known implicitly defined choice kernel oe delta oe inner product feature space feature space pre hilbert inner product space 
suitable choice kernel data separable feature space despite non separable original input space kernel substitution provides route obtaining nonlinear algorithms algorithms previously restricted handling linearly separable datasets 
example data parity spirals problem nonseparable hyperplane input space separated feature space defined rbf kernels giving rbf type network gamma gammax oe choices kernel function possible delta tanh fix delta defining polynomial feedforward neural network classifiers 
class mathematical objects kernels general includes scores produced dynamic alignment algorithms example 
suitable kernels satisfy mathematical condition mercer theorem 
binary classification choice kernel learning task involves maximisation lagrangian ff ff gamma ff ff subject constraints 
associated karush kuhn tucker kkt conditions delta gamma ff ff delta gamma satisfied solution 
optimal values ff decision function sign ff bias feature dual formulation primal constraints gamma max gamma ff min ff henceforth refer solution ff hypothesis modelling data 
maximal margin hyperplane feature space points lie closest hyperplane ff points fig 

multi class classification problem reduced series binary classification tasks 
support vectors 
points ff 
means representation hypothesis solely points closest hyperplane informative patterns data 
problems involve multiclass classification number schemes outlined broadly similar performance 
simplest schemes directed graph learning task reduced binary classification node 
soft margins allowing training errors 
real life datasets contain noise svm fit noise leading poor generalisation 
effect outliers noise reduced introducing soft margin schemes currently 
error norm learning task box constraint ff second error norm learning task addition small positive constant leading diagonal kernel matrix control trade training error generalisation ability chosen means validation set 
effect soft margins illustrated ionosphere dataset uci repository 
fig 

left generalisation error percentage axis versus axis right generalisation error percentage axis versus axis soft margin classifiers error norms respectively 
uci ionosphere dataset rbf kernels oe samplings data 
justification soft margin techniques comes statistical learning theory readily viewed relaxation hard margin constraint 
error norm prior introducing kernels introduce positive slack variable delta gamma task minimise sum errors addition jjwjj min delta readily formulated primal objective function ff delta gamma ff delta gamma gamma lagrange multipliers ff 
derivatives respect give gamma ff ff gamma ff gamma back primal objective function obtain dual objective function 
gamma ff gamma ff constraint ff replaced ff patterns values ff referred non bound ff ff said bound 
error norm find bias decision function final kkt condition 
non bound pattern follows gamma ff assuming sigma 
optimal value experimentation validation set readily related characteristics dataset model 
alternative approach svm shown solutions error norm obtained maximising ff gamma ff ff subject ff ff ff lies range 
fraction training errors upper bounded provides lower bound fraction points support vectors 
formulation conceptual meaning soft margin parameter transparent 
error norm primal objective function ff delta gamma ff delta gamma gamma ff 
obtaining derivatives respect substituting primal objective function noting dual objective function maximal obtain dual objective function kernel substitution ff ff gamma ff ff gamma ff gives dual objective function hard margin learning substitution 
real life datasets imbalance amount data different classes significance data classes quite different 
example detection mri scans may best allow higher number false positives improved true positive detection rate 
relative balance detection rate different classes easily shifted introducing asymmetric soft margin parameters 
binary classification error norm ff ff gamma gamma gamma gamma error norm 
linear programming approach classification 
quadratic programming possible derive kernel classifier learning task involves linear programming 
formulated directly feature space involves min ff subject ff gamma ff 
minimising ff obtain solution sparse relatively fewer datapoints 
furthermore efficient simplex column generation implementations exist solving linear programming problems practical alternative conventional qp svms 
linear programming approach evolved independently qp approach svms see linear programming approaches regression novelty detection possible 
novelty detection 
real world problems task classify detect novel abnormal instances 
novelty abnormality detection potential applications problem domains condition monitoring medical diagnosis 
approach model support data distribution having find real valued function estimating density data 
simplest level objective create function positive regions input space data predominantly lies negative 
approach find hypersphere minimal radius centre contains data novel test points lie outside boundary hypersphere 
technique outline originally suggested vapnik novelty detector tax duin authors real life applications 
effect outliers reduced slack variables allow datapoints outside sphere task minimise volume sphere number datapoints outside min subject constraints gamma gamma controls tradeoff terms 
primal objective function ff gamma fl gamma ff gamma delta gamma delta delta ff fl 
kernel substitution dual formulation amounts maximisation ff ff gamma ff ff respect ff subject ff ff bound examples occur ff correspond outliers training process 
having completed training process test point declared novel gamma ff ff ff gamma computed finding example non bound setting inequality equality 
alternative approach developed scholkopf 
suppose restricted attention rbf kernels case data lie region surface hypersphere feature space oe delta oe 
objective separate region surface region containing data 
achieved constructing hyperplane maximally distant origin datapoints lying opposite side origin delta 
kernel substitution dual formulation learning task involves minimisation ff ff ff subject ff ff determine bias find example say non bound ff fi nonzero ff determine gamma ff support distribution modelled decision function sign ff models parameter neat interpretation upper bound fraction outliers lower bound fraction patterns support vectors 
scholkopf provide experimental fig 

solution input space hyperplane minimising ff equation 
hard margin rbf kernels trained oe 
evidence favour approach including highlighting abnormal digits usps handwritten character dataset 
method works types kernel 
earlier scheme novelty detection error norm case constraint ff removed addition kernel diagonal 
model scholkopf origin feature space plays special role 
effectively acts prior class abnormal instances assumed lie 
repelling away origin consider attracting hyperplane datapoints feature space 
input space corresponds surface wraps data clusters achieved linear programming task min ff subject ff ff ff bias just treated additional parameter minimisation process unrestricted sign 
handle noise outliers fig 

left linear ffl insensitive loss function versus gamma delta gamma right quadratic ffl insensitive loss function 
introduce soft boundary min ff subject ff gamma constraints 
method successfully detection abnormalities blood samples detection faults condition monitoring ball bearing cages 
regression 
real valued outputs learning task theoretically motivated statistical learning theory 
constraints gamma delta gamma ffl delta gamma ffl allow deviation ffl eventual targets function delta modelling data 
visualise band tube size sigma gamma fl hypothesis function points outside tube viewed training errors 
structure tube defined ffl loss function 
minimise jjwjj penalise 
account training errors introduce slack variables types training error 
slack variables zero points inside tube progressively increase points outside tube loss function 
general approach called ffl sv regression common approach sv regression 
linear ffl loss function task minimise min jjwjj subject gamma delta gamma ffl delta gamma ffl slack variables positive 
kernel substitution dual objective function ff ff ff gamma ff gamma ffl ff ff gamma ff gamma ff ff gamma ff maximised subject ff ff ff ff similarly quadratic ffl loss function gives rise min jjwjj subject giving dual objective function ff ff ff gamma ff gamma ffl ff ff gamma ff gamma ff ff gamma ff ffi ij maximised subject 
function modelling data ff gamma ff compute bias considering kkt conditions regression 
linear loss function prior kernel substitution ff ffl gamma delta ff ffl gamma delta gamma ff gamma ff gamma ff gamma ff conditions see ff ff slack variables non zero examples correspond points outside ffl insensitive tube 
find bias non bound example ff gamma delta gamma ffl similarly ff obtain gamma delta ffl 
bias obtained example best compute average points margin 
apart formulations possible define loss functions giving rise different dual objective functions 
addition specifying ffl priori possible specify upper bound fraction points lying outside band find ffl optimising primal objective function jjwjj jy gamma ffl acting additional parameter minimise 
classification novelty detection possible formulate linear programming approach regression min ff ff subject gamma ffl gamma ff gamma ff ffl minimising ff approximately minimises number support vectors favours sparse hypotheses smooth functional approximations data 
approach kernel need satisfy mercer condition 
algorithmic approaches far methods considered involved linear quadratic programming 
linear programming implemented column generation techniques packages available cplex 
quadratic programming applicable techniques including quasi newton conjugate gradient primal dual interior point methods 
certain qp packages readily applicable minos loqo 
methods train svm rapidly disadvantage kernel matrix stored memory 
small datasets practical qp routines best choice larger datasets alternative techniques 
split categories techniques kernel components evaluated discarded learning working set methods evolving subset data 
category obvious approach sequentially update ff approach kernel ka algorithm 
binary classification soft margin bias simple gradient ascent procedure ff initially ff subsequently sequentially updated ff fi fi fi ff gamma ff fi heaviside step function 
optimal learning rate readily evaluated sufficient condition convergence jk 
decision function method easy implement give quick impression performance svms classification tasks 
equivalent hildreth method optimisation theory generalised case soft margins inclusion bias 
fast qp routines especially small datasets 
chunking decomposition 
sequentially updating ff alternative update ff parallel subset chunk data stage 
qp routine optimise lagrangian initial arbitrary subset data 
support vectors retained datapoints ff discarded 
new working set data derived support vectors additional datapoints maximally violate storage constraints 
chunking process iterated margin maximised 
course procedure may fail dataset large hypothesis modelling data sparse ff non zero say 
case decomposition methods provide better approach algorithms fixed size subset data ff remainder kept fixed 
decomposition sequential minimal optimisation smo 
limiting case decomposition sequential minimal optimisation smo algorithm platt ff optimised iteration 
smallest set parameters optimised iteration plainly constraint ff hold 
remarkably parameters optimised rest kept fixed possible derive analytical solution executed numerical operations 
method consists heuristic step finding best pair parameters optimise analytic expression ensure lagrangian increases monotonically 
hard margin case easy derive maximisation respect additive corrections ff ff ff ff 
soft margin care taken avoid violation constraints leading bounds corrections 
smo algorithm refined improve speed generalised cover tasks classification regression novelty detection 
due decomposition learning task speed probably method choice training svms 
model selection 
apart choice kernel indeterminate choice kernel parameter oe 
kernel parameter cross validation sufficient data available 
model selection strategies give reasonable estimate kernel parameter additional validation data 
attempt theorem stating generalisation error bound reduced margin fl increased 
theorem gives upper bound radius smallest ball containing training data 
optimum possible show fl ff ff values ff optimum 
rbf kernels data lies surface hypersphere oe delta oe bound written ff estimate oe sequentially training svms dataset successively larger values oe evaluating bound ff case choosing value oe bound minimised 
method give reasonable estimate data spread evenly surface hypersphere poor data lie flat ellipsoid example radius influenced largest deviations 
refined estimates take account distribution data 
approach theoretically rescale data feature space compensate uneven distributions 
complex strategy lines proposed scholkopf leads algorithm performed practice small number datasets 
economical way training data leave procedure 
example consider scheme proposed joachims 
approach number leave errors norm soft margin svm bounded jfi ff gj ff solutions optimisation task upper bound determine ff gamma 
value kernel parameter leave error estimated quantity system retrained datapoints left bound determined ff solution 
kernel parameter incremented decremented direction needed lower bound 
method worked classification text 
techniques kernel representations 
far considered methods linear quadratic programming 
shall consider approaches may utilise general nonlinear programming techniques 
particular consider approaches issues improve generalisation performance standard svms create hypotheses sparse 
dual input space datapoints hyperplanes separating hyperplane points 
version space space hypotheses points consistent data space bounded set hyperplanes representing data 
svm solution viewed centre largest hypersphere version space support vectors correspond examples hyperplanes tangentially touching 
version space centre largest inscribed hypersphere appear best choice 
better choice bayes point approximately centre mass version space 
bayes point machines construct hypothesis centre version space choice justified theoretical arguments addition having geometric appeal 
fig 

centre mass version space centre largest inscribed sphere theta elongated version space approach centre mass determined billiard algorithm version space traversed uniformly estimate centre mass repeatedly updated 
large majority datasets version space diverges bpm outperforms svm statistically significant levels 
artificial examples elongated version spaces generalisation error bpm half svm 
current implementations number drawbacks algorithm slow execution better mechanisms soft boundary imitating soft margin need implementation include bias 
centre mass version space alternative hypothesis lies centre space easier compute 
achieved repulsive potentials phi ff favouring points centre version space 
example min phi ff ln ff subject ff basis analytic center machine 
gradient hessian readily evaluated algorithm appears perform practice achieving test error dataset svm gave example 
bayes point machine may exhibit generalisation disadvantage hypothesis dense nearly datapoints ff appear final hypothesis 
ideally derive kernel classifiers regression machines give sparse hypotheses minimal number datapoints 
effective means obtaining sparse hypotheses remains object research excellent scheme relevance vector machine tipping 
function ff weights ff model data bayesian prior defined parameters favouring smooth functions 
bayes rule posterior weights obtained marginal likelihood evidence 
iterative maximisation evidence suggests suitable kernel values pruning creating eventual hypothesis sparse number datapoints 
experiments show approach give hypotheses percent available data 
approach considered general applied wide range machine learning tasks generate possible learning machine architectures rbf networks feedforward neural networks appropriate choice kernel 
kernel methods practice 
subject development expected develop important tool machine learning applications 
burges 
tutorial support vector machines pattern recognition 
data mining knowledge discovery 
campbell bennett 
linear programming approach novelty detection 
appear advances neural information processing systems vol 
mit press 
chapelle vapnik 
model selection support vector machines appear advances neural information processing systems ed 
solla leen 
muller mit press 
cortes vapnik 
support vector networks 
machine learning 
cristianini campbell shawe taylor 
dynamically adapting kernels support vector machines advances neural information processing systems ed 
kearns solla cohn mit press 
cristianini shawe taylor 
support vector machines kernel learning methods cambridge university press 

cristianini campbell 
kernel algorithm fast simple learning procedure support vector machines 
th intl 
conf 
machine learning morgan kaufman publishers 
guyon matic vapnik 
discovering informative patterns data cleaning 
fayyad piatetsky shapiro smyth uthurusamy editors advances knowledge discovery data mining mit press 
cf www com isabelle projects svm html haussler 
convolution kernels discrete structures uc santa cruz technical report ucs crl 
herbrich graepel campbell 
bayesian learning reproducing kernel hilbert spaces submitted machine learning 
herbrich th 
graepel campbell 
proceedings esann publications belgium 
haussler 
probabilistic kernel regression models proceedings conference ai statistics 
joachims estimating generalization performance svm efficiently 
proceedings seventeenth international conference machine learning 
morgan kaufmann 

keerthi bhattacharyya murthy 
improvements platt smo algorithm svm classifier design 
tech report dept csa india 
luenberger 
linear nonlinear programming 
addison wesley 
mangasarian 
linear nonlinear separation patterns linear programming 
operations research 
alpaydin 
support vector machines multiclass classification proceedings international workshop artifical neural networks idiap technical report 
mercer 
functions positive negative type connection theory equations 
philos 
trans 
roy 
soc 
london 
mika ratsch weston scholkopf 
muller 
fisher discriminant analysis kernels 
proceedings ieee neural networks signal processing workshop pages 
nash 
linear nonlinear programming 
mcgraw hill new york ny 
opper haussler 
generalisation performance bayes optimal classification algorithm learning perceptron 
physical review letters 
osuna girosi 
reducing run time complexity support vector machines scholkopf burges smola ed advances kernel methods support vector learning mit press cambridge ma 
platt 
fast training svms sequential minimal optimisation 
scholkopf burges smola ed advances kernel methods support vector learning mit press cambridge ma 
platt cristianini shawe taylor 
large margin dags multiclass classification advances neural information processing systems ed 
solla leen 
muller mit press 
shawe taylor cristianini 
margin distribution soft margin 
smola scholkopf schuurmans eds advances large margin classifiers chapter mit press 

bayesian learning perceptron 
phd thesis centrum belgium 
scholkopf bartlett smola williamson 
support vector regression automatic accuracy control 
niklasson oden ziemke editors proceedings th international conference artificial neural networks perspectives neural computing berlin springer verlag 
scholkopf burges smola 
advances kernel methods support vector machines 
mit press cambridge ma 

scholkopf smola 
muller 
kernel principal component analysis 
scholkopf burges smola 
advances kernel methods support vector machines 
mit press cambridge ma 

scholkopf platt shawe taylor smola williamson estimating support high dimensional distribution 
microsoft research technical report msr tr 
scholkopf shawe taylor smola williamson 
kernel dependent support vector error bounds ninth international conference artificial neural networks iee conference publications 
scholkopf smola williamson bartlett 
new support vector algorithms 
appear neural computation 
smola scholkopf 
tutorial support vector regression 
neurocolt tr 
smola scholkopf schuurmans eds advances large margin classifiers mit press 
tax duin 
data domain description support vectors proceedings esann ed 
verleysen facto press brussels 
tax duin 
support vector data description applied machine vibration analysis 
boasson eds proc 
th annual conference advanced school computing imaging nl june 
tipping 
relevance vector machine 
sara solla todd leen klaus robert muller editors advances neural information processing systems 
cambridge mass mit press 

analytic center machine 
machine learning appear 
vapnik chapelle 
bounds error expectation svms submitted neural computation vapnik 
nature statistical learning theory springer 
vapnik 
statistical learning theory 
wiley 
campbell cristianini 
controlling sensitivity support vector machines 
proceedings international joint conference artificial intelligence ijcai stockholm sweden 
rau 
statistical mechanics learning rule 
reviews modern physics 
watkins 
dynamic alignment kernels technical report ul royal holloway csd tr 
weston watkins 
multi class support vector machines proceedings esann ed 
verleysen facto press brussels 
weston gammerman stitson vapnik vovk watkins 
support vector density estimation 
scholkopf burges smola 
advances kernel methods support vector machines 
mit press cambridge ma 

www ics uci edu mlearn mlrepository html 
