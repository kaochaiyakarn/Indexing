advances neural information processing systems nips pages denver colorado december approximate learning dynamic models xavier boyen computer science dept stanford ca xb cs stanford edu daphne koller computer science dept stanford ca koller cs stanford edu inference key component learning probabilistic models partially observable data 
learning temporal models inference phases requires complete traversal potentially long sequence furthermore data structures propagated procedure extremely large making process demanding 
describe approximate inference algorithm monitoring stochastic processes prove bounds approximation error :10.1.1.119.6111
apply algorithm approximate forward propagation step em algorithm learning temporal bayesian networks 
provide related approximation backward step prove error bounds combined algorithm 
show em inference algorithm faster em exact inference degradation quality learned model 
extend analysis online learning task showing error resulting restricting attention small window observations 
online em learning algorithm dynamic systems show learns faster standard offline em 
real life situations faced task understanding underlying dynamics complex stochastic process limited observations state time 
inductive learning task decision concerns representation learned hypotheses 
hidden markov models hmms played largest role representation learning models stochastic processes 
increasing structured models stochastic processes factored hmms dynamic bayesian networks dbns :10.1.1.16.2929
structured decomposed representations allow complex processes large number states encoded smaller number parameters allowing better generalization limited amounts data :10.1.1.129.4770
furthermore natural structure processes easier human expert incorporate prior knowledge domain structure model improving inductive bias 
parameter structure learning algorithms dynamic models probabilistic inference crucial component 
inference routine called multiple times order fill missing data expected value current hypothesis resulting expected sufficient statistics construct new hypothesis 
inference step times iterates entire sequence 
behavior problematic important respects 
settings may access entire sequence advance 
second various structured representations stochastic processes admit effective inference procedure 
messages propagated exact inference algorithms include entry possible state system number states exponential size model rendering type computation infeasible smallest problems 
describe analyze approach helps address problems 
proposed new approach approximate inference stochastic processes approximate distributions admit compact representation maintained propagated :10.1.1.119.6111
showed empirically approach allows orders magnitude speedup task monitoring complex stochastic process small cost terms accuracy 
proved accumulated error arising repeated approximations remains bounded indefinitely time 
analysis relied novel result showing transition stochastic process contraction relative entropy kl divergence 
apply approach parameter learning task 
application completely straightforward algorithm associated analysis applied forward propagation messages inference learning algorithms require propagation information entire sequence :10.1.1.119.6111
provide analysis error accumulated approximate inference process backward propagation phase inference 
analysis quite different contraction analysis forward phase 
combine results prove bounds error expected sufficient statistics relayed learning algorithm stage 
empirical results practical dbn illustrating performance approximate learning algorithm 
show speedups obtained easily discernable loss quality learned hypothesis 
theoretical analysis suggests way dealing problematic need reason entire sequence temporal observations 
contraction results show legitimate ignore observations far 
compute accurate approximation belief state considering small window observations 
idea construct efficient online learning algorithm 
show converges hypothesis faster standard offline em algorithm settings favorable 
preliminaries model dynamic system specified tuple hb qi represents qualitative structure model appropriate parameterization 
dbn instantaneous state process specified terms set state variables xn encodes network fragment specifies time variable set parents parents example fragment shown 
parameters define conditional probability table ip parents 
denote transition matrix stochastic process transition probability state state note concept defined dbn case matrix represented implicitly parameters 
goal learn model stochastic process partially observable data 
simplify discussion focus problem learning parameters known structure em expectation maximization algorithm discussion applies equally contexts 
em iterative procedure searches space parameter vectors local maximum function probability observed data describe em algorithm task learning hmms extension dbns straightforward 
em algorithm starts initial random parameter vector specifies current estimate transition observation matrices process em algorithm computes expected sufficient statistics ess compute expectation 
case hmms ess average joint distributions variables time gamma variables time new parameter vector computed ess simple maximum likelihood step 
steps iterated appropriate stopping condition met 
entire sequence computed simple forward backward algorithm 
response observed time likelihood vector probability observing response state 
forward messages propagated follows gamma delta theta theta outer product 
backward messages fififi propagated fififi delta fififi theta estimated belief time simply theta fififi suitably renormalized similarly joint belief proportional gamma theta fififi theta theta 
message passing algorithm obvious extension dbns 
unfortunately feasible small dbns 
essentially messages passed algorithm entry possible state time dbn number states exponential number state variables rendering explicit representation infeasible cases 
furthermore highly structured processes admit compact representation messages 
belief state approximation described new approach approximate inference dynamic systems avoids problem explicitly maintaining distributions large spaces :10.1.1.119.6111
maintain belief state distribution current state computationally tractable representation distribution 
propagate time approximate belief state transition model condition evidence time 
approximate resulting time distribution admits compact representation allowing algorithm continue 
dbns example considered belief state approximations certain subsets correlated variables belief state approximated independent 
case approximation step consists simple projection relevant marginals factored representation time approximate belief state 
showed errors arising repeated approximation accumulate unboundedly stochasticity process attenuates effect 
dbn approximation showed factor reduction running time achieved cost small error 
results directly applicable learning task belief state precisely forward message forward backward algorithm 
apply approach theta delta forward step guarantee approximation lead big difference sufficient statistics 
technique fully resolve computational problem associated inference task learning backward propagation phase just expensive forward phase 
want apply type approximate inference algorithm backward forward maintain propagate compactly represented approximate backward message fififi order guarantee bounds accumulated error simply extend analysis backward message :10.1.1.119.6111
approach turns problematic 
bounds relative entropy error forward backward messages bounds error follow 
solution turns alternative notion distance combines additively bayesian updating albeit cost weaker contraction rates 
definition positive vectors dimension 
projective distance defined id proj max ln delta delta 
note projective distance upper bound relative entropy 
lemma id proj id proj gamma gamma id proj fififi fififi 
results show projective distance contracts messages propagated stochastic transition matrix direction lemma min fi deltat delta delta define delta 
id proj gamma delta id proj gamma gamma id proj fififi fififi gamma delta id proj fififi fififi 
show approximations introduce large error expected sufficient statistics remain close correct value 
theorem ess computed exact inference approximation 
forward resp 
backward approximation step guaranteed introduce resp 
ffi projective error id proj ffi id kl ffi applied ideas task parameter learning dbns approximation scheme messages represented factored way marginals specified clusters variables :10.1.1.119.6111
clique tree algorithm propagate messages forward backward 
example compute fififi fififi generate clique tree dbn incorporate fififi 
fififi obtained reading relevant marginals tree fififi implicitly defined product 
tested algorithms task learning parameters bat network shown traffic monitoring 
training set fixed sequence slices generated correct network distribution 
test metric average log likelihood slice fixed test sequence slices 
experiments conducted different random starting points parameters experiments 
ran em different types structural approximations evaluated quality model iteration algorithm 
different structural approximations exact propagation ii clustering state variables iii clustering iv variable separate cluster 
results random starting point shown 
see impact xdot xdot stopped stopped slice slice evidence iteration average log likelihood dbn exact em em bat dbn 
structural approximations batch em 
severe structural approximation learning accuracy negligible 
starting points approximate algorithms tracked exact algorithm closely 
runs difference peak log likelihood reached different algorithms 
phenomenon remarkable especially view substantial savings caused approximations sun ultra ii computational cost learning min iteration exact case vs min iteration clustering min iteration 
line learning analysis gives tools address important problem learning dynamic models need reason entire temporal sequence 
consequence contraction result effect approximations done far away sequence decays exponentially time difference 
particular effect approximation ignores observations far limited 
inference time slice small window observations result fairly accurate 
precisely assume time considering window size view uniform message bad approximation fififi propagate approximate backward message error approximation attenuate exponentially quickly 
fififi message process update id proj fififi fififi decrease exponentially insights experimented various online algorithms small window approximations 
online algorithms approach ess updated exponential decay data cases parameters updated correspondingly 
main problem frequent parameter updates online setting require recomputation messages computed old parameters 
long sequences computational cost scheme prohibitive 
algorithms simply leave forward messages unchanged assumption time slices parameters close new ones current message fairly similar updated value 
contraction result tells old parameters far back sequence negligible effect message 
tried schemes update backward messages 
dynamic approach backward message computed slices closer messages recomputed frequently parameters changed iteration average log likelihood dbn batch em dynamic static static time slice average log likelihood dbn dynamic static static static temporal approximations batch setting online setting 
cached messages older parameters 
closest messages updated parameter update update approach closest realistic alternative full update backward messages 
static approach long window slices recompute messages window ends current parameters compute messages entire window 
static approach short window slices 
static approach lookahead past evidence compute joint beliefs 
case precisely technique kalman filters online learning process parameters 
minimize computational burden tests conducted structural approximation 
running time various algorithms sec slice batch em dynamic static static static 
evaluated temporal approximations line batch setting 
batch experiments step sequence 
results including results batch em shown 
terms accuracy peak see static method equivalent batch em dynamic method 
accuracy static approach somewhat lower 
methods overfit fairly badly due small amount training data 
terms running time static approximation reaches peak half time needed static 
algorithms reach highest accuracy static far fastest 
second set experiments focused real online learning task single long sequence slices 
results displayed 
static approach indistinguishable terms accuracy dynamic approach 
static approach converges slowly seen achieve accuracy algorithms 
static approach ultimately converges accuracy dynamic static approaches slowly 
results show short window allows rapid convergence right answer frequent updates short windows significantly better infrequent updates longer ones 
extensions suggested simple structural approximations inference algorithm step 
results show severe structural approximations negligible effects accuracy learning 
advantages approximate inference learning setting pronounced case pure inference small errors caused approximation negligible compared larger ones induced learning :10.1.1.119.6111
techniques provide feasible approach learning structured models complex dynamic systems resulting advantages generalization ability incorporate prior knowledge 
results online learning task showing small time window suffices doing learning 
comparable variational approach approximate inference applied learning factored hmms :10.1.1.16.2929
done direct empirical comparison algorithms track exact em closely improvement accuracy negligible 
algorithm simpler may faster 
importantly applicability wider dbn models structure complex factored hmms online learning 
obvious extension results integration ideas structure learning algorithm dbns 
integration completely trivial structural approximation algorithm designed produce expected sufficient statistics required learning parameters structure 
structure learning algorithm set sufficient statistics required changes constantly structure approximation need flexible 
believe resulting algorithm able learn structured models real life complex systems 
li 
note coefficient ergodicity column allowable nonnegative matrix 
linear algebra applications 
boyen koller :10.1.1.119.6111
tractable inference complex stochastic processes 
proc 
uai 
appear 
cover thomas 
elements information theory 
wiley 
dean kanazawa 
model reasoning persistence causation 
comp 
int 
dempster laird rubin 
maximum likelihood incomplete data em algorithm 
journal royal statistical society 
forbes huang kanazawa russell 
bayesian automated taxi 
proc 
ijcai 
friedman murphy russell 
learning structure dynamic probabilistic networks 
proc 
uai 
appear 
ghahramani jordan :10.1.1.16.2929
factorial hidden markov models 
nips 
kalman 
new approach linear filtering prediction problems 
basic engineering 
lauritzen spiegelhalter 
local computations probabilities graphical structures application expert systems 
roy 
stat 
soc 
neal hinton 
view em algorithm justifies incremental sparse variants 
jordan editor learning graphical models 
kluwer 
rabiner juang 
hidden markov models 
ieee acoustics speech signal processing 
zweig russell 
speech recognition dynamic bayesian networks 
proc 
aaai 
appear 
