model hierarchical clustering shivakumar vaithyanathan byron dom ibm almaden research center harry rd san jose ca approach model hierarchical clustering formulating objective function bayesian analysis 
model organizes data cluster hierarchy specifying complex feature set partitioning key component model 
features unique distribution cluster common distribution clusters 
cluster subsets features common distribution correspond nodes clusters tree representing hierarchy 
apply general model problem document clustering multinomial likelihood function dirichlet priors 
algorithm consists stage process perform flat clustering followed modified hierarchical agglomerative merging process includes determining features common distributions merged clusters 
regularization induced marginal likelihood automatically determines optimal model structure including number clusters depth tree subset features modeled having common distribution node 
experimental results synthetic data real document collection 
years seen significant interest modelbased clustering assumed data generated mixture underlying probability distributions components interpreted cluster 
complete description model clustering 
extensions generative models incorporating hierarchical agglomerative algorithms studied 
algorithms operate merging clusters resulting likelihood maximized 
efficient algorithms model hierarchical clustering special cases gaussian generative models described fraley 
addition model hac algorithm multinomial mixture model developed 
rest hac version hac likelihood setting described 
particular concentrating multinomial mixture models 
hierarchical clustering algorithms literature include describes scheme characterize text collections hierarchically deterministic annealing algorithm 
model latent variables clustering documents base hierarchy additional latent variables define intermediate nodes 
additional latent variables called abstraction nodes model conditional probabilities words 
regularization achieved maximizing likelihood separate validation data set 
seen model differs considerably described 
part feature set modeled intermediate nodes hierarchy choice features part model selection 
dramatically reduce number adjustable parameters model especially context high dimensional data 
second regularization achieved integrating parameter values bayesian fashion 
allows model utilize entire data set model building heldout validation set required 
stage algorithm described uses sufficient statistics computed stage merging process computationally expensive 
probabilistic model developed orga data elements natural hierarchy 
nodes different levels hierarchy determined modeling features common children particular node 
common features modeled having distribution clusters idea features having common distribution clusters useful discriminating clusters 
call common features noise features node discriminatory features useful 
analysis leads natural hierarchical model lends nicely real problems automatic taxonomy generation large collection documents 
formal definition noise useful features provided subsequent sections 
passing note document clustering rich area approaches section describes hierarchical model derives objective function marginal likelihood describes model 
section describe approximate schemes solve objective function 
section discusses experimental set results evaluation 
section describes area 
models unsupervised learning previous addressed problem flat non hierarchical partitional data element belongs cluster clustering 
describe model general terms extend case hierarchical partitional clustering 
model flat partitional clustering key aspect previous model partitioning features sets associated subspaces noise useful definitions 
noise features statistical distribution clusters 
useless discriminators clusters 
useful features hand different distribution cluster potentially useful discrimination 
previous model feature types precluding possibility features having distribution clusters 
having said add true certain sense 
principle features identical observed statistics clusters resulting identical parameter estimates 
despite identical values treated distinct bayesian prior distribution charged distributions regularization purposes 
issue discussed length sequel 
flat clustering model probability data conditioned model structure omega gamma written general terms dj omega gamma omega gamma psi omega gamma complete data set full feature representation 
omega represents model structure specified partition complete feature set number clusters assignment individual data items fd clusters 
represents projected noise subspace psi set clusters indexed represents data elements associated cluster projected useful subspace specific probability model associated cluster applied general model form problem document clustering multinomial distributions bayesian marginal likelihood computed dirichlet distributions priors resulting objective function dj omega gamma models document clustering defer discussion developed general form corresponding hierarchical model 
model hierarchical partitional clustering extend flat clustering model case cluster hierarchy represented tree nodes correspond clusters nodes leaves decomposed clusters recursively leaf clusters reached 
simple example cluster hierarchy diagrammed 
note model similar discussed 
see depth treatment bayesian statistical paradigm 
oeae oeae oeae oeae oeae delta delta delta oeae oeae delta delta delta oeae diagram simple taxonomy showing feature set partitions extension need extend notation 
various symbols sets functions parameters indexed hierarchy node apply 
ffl useful noise features associated hierarchy node definition required 
discussed 
index value corresponds root node contains 
ffl psi psi children node hierarchical model extends noise useful concept general case alluded 
case certain features common distribution proper subset clusters 
model cluster subsets candidates common distributions 
common feature distribution associated node hierarchy applies node descendants entire sub tree beneath node 
associated features represented note node ancestor node parent define gamma complement denoted useful features node construct hierarchical model corresponding equation incorrect uai proceedings version 
corrected 
substitute extended notation yielding dj omega gamma omega gamma psi omega gamma recursively expand omega gamma terms omega gamma nk omega gamma psi uk omega gamma non leaf node hierarchy expand similarly omega gamma nk omega gamma psi uk omega gamma completely expanded form dj omega gamma entire hierarchy obtained recursively applying leaves reached quite complicated complex cluster hierarchies specific forms substituted 
marginal likelihood hierarchy final forms dj omega gamma various components omega gamma bayesian marginal likelihoods 
underlying distribution example multinomial address application document clustering associated prior distribution dirichlet case multinomials 
distributions characterized set realvalued parameters represent bayesian paradigm treated random variables distribution referred prior represent omega gamma single component distribution specified values omega marginal likelihood formed integrating joint distribution omega gamma dj omega gamma omega gamma domain dj omega gamma dj omega gamma model various terms expanded way recursive hierarchical partitioning feature set described subsection 
expansion complete leaves hierarchy 
multinomial model hierarchical document clustering general approach construct model hierarchical partition set documents start treating document bag words common practice ignoring information sequence terms times term appears 
term counts features 
treatment referred information retrieval parlance vector space model 
assume probabilities occurrence terms independent times terms terms occur document 
despite obvious incorrectness assumption text commonly produced quite satisfactory results 
implication assumptions statistical distribution terms counts multinomial 
model single cluster probability observing single document composed words term lexicon term counts ft jj mg dj ft gamma delta multinomial coefficient probability word th term lexicon 
identify set subsection 
probability collection independent documents fd ji population single cluster described product terms dj ft notation augmented obvious way document index obtain marginal likelihood collection documents integrate product prior domain simplex defined normalization constraint 
stated choice prior dirichlet distribution gamma ff gamma ff ff gamma ff ff fff referred hyperparameters 
integration results expression product terms instances known distribution 
marginal likelihood written gamma ff gamma ff gamma ff gamma ff document clustering experiments describe section set ff prior results uniform prior 
model flat set clusters applying multinomial dirichlet models set clusters feature set partition yields expression 
dj omega gamma gamma fl fl gamma fl gamma fl gamma fl gamma fl gamma fl fl theta gamma fi gamma fi mn gamma fi gamma fi theta gamma oe gamma oe gamma oe jd gamma jd theta gamma ff gamma ff mu gamma ff gamma ff total useful noise term counts respectively total count noise term jd total number documents cluster total count useful term cluster fl fi oe ff hyperparameters associated various counts 
complete derivation provided 
writing implicitly assumptions noise useful features conditionally independent parameter sets independent 
model hierarchical partition set documents equation representation flat set clusters 
cluster hierarchy model fully expanded sense significantly complicated containing term noise distribution omega gamma fp nk omega gamma term useful feature distribution leaf cluster omega gamma assume uniform priors parameter spaces 
optimization algorithm task optimization algorithm finding model structure omega arg max omega dj omega gamma maximizes equation fp fully recursively expanded model forms appropriate application substituted multinomials document clustering application 
extremely difficult task closed form expressions integrals case multinomials 
due enormous number cluster hierarchies possible objects 
reasonably efficient albeit possibly suboptimal approach consisting phase approach explained detail 

perform flat clustering number clusters noise feature space determined optimizing equation 

form hierarchy clusters modified hierarchical agglomerative clustering mhac algorithm 
flat clustering algorithm perform flat clustering algorithm described detail 
review briefly 
algorithm finds maximum solution omega gamma equation models subsection em algorithm distributional clustering heuristic reduce feature partition search space 
bayesian marginal likelihood provides regularization criterion allows natural number clusters determined automatically 
modified hierarchical agglomerative clustering algorithm associated set clusters obtained flat clustering scheme partition feature set subsets partition part optimal model structure omega equation 
features global noise features sense having common distribution clusters 
hierarchy model embodies model structure omega includes possibility features common distributions proper sub sets clusters 
find associated hierarchy associated featureset partitions modified version hierarchical agglomerative clustering algorithm denote mhac 
hac algorithm starts singleton clusters data point cluster computationally expensive algorithm 
noted lower parts dendrogram created provide useful information 
overcome problem generated initial set clusters minimum spanning tree series heuristics 
resultant set clusters input regular hac algorithm 
different approach generate initial set clusters em algorithm merge mhac algorithm 
mhac perform cluster merges provide increase marginal likelihood hierarchy equation 
merge clusters performed finding feature sub set modeled noise features pair clusters consideration 
noise features modeled single set parameters clusters consideration potentially increasing 
algorithm stops merges results increase 
complete algorithm em mhac algorithm shown 
repeat steps cluster left increase identify clusters merging provides largest increase clusters compute increase identifying features noise merge clusters identified step compute sufficient statistics new cluster sum sufficient statistics clusters mhac algorithm need perform search feature space obtain local set noise features 
optimally accomplish search feature space obtain local subset search possible ij combinations available features 
computationally intractable resort greedy algorithm evaluates feature time stops addition new feature provide increase marginal likelihood 
accomplish sorting features increasing order delta maximum likelihood estimate parameter feature delta difference clusters considered algorithm keeps adding feature time sorted list marginal likelihood starts decreasing 
worthwhile note important aspects rationale heuristic features similar ml estimates clusters best candidates modeling single distribution mhac algorithm 
initial value obtained result flat clustering algorithm re compute complete merge operation interested change 
compute change theorem proved appendix states difference dependent sufficient statistics cluster wise term counts clusters locally computed accessing actual data document wise term counts clusters 
fact exploited maximum likelihood implementation 
theorem models multinomials dirichlet priors merging clusters change lml determined projections clusters subspaces spanned features chosen part merging process common noise clusters 
difference lml independent clusters merged cluster log omega omega omega substituting models gives expression difference log gamma ff gamma ff gamma ff jn log gamma ff gamma ff gamma ff gamma ff notation similar simple generalization denotes total count feature subset terms cluster cardinality set represented jn experiments section describe design experiments evaluate approach synthetic real world data 
note equivalent hierarchy root children clusters flat cluster data sets synthetic data generated synthetic data sets assumptions developing model objective function 
defined structures shown figures structure defined noise features intermediate node associated parameters described section 
oeae oeae oeae oeae delta delta delta oeae oeae delta delta delta oeae structure cluster hierarchy synthetic data experiments 
oeae oeae oeae oeae delta delta delta oeae oeae delta delta delta oeae oeae delta delta delta oeae structure cluster hierarchy synthetic data experiments 
addition generating data distribution matched model assumptions placed constraints limit number experiments 
ffl data points objects associated terminal nodes associated internal nodes 
ffl total number features 
ffl noise features intermediate levels generated constraint total probability associated useful features terminal node cluster 
probabilities sampled different numbers data points tables distribution structures shown figures 
real world data trec document collection applied models algorithms real world data set consisting collection documents 
important practical problem today automatic creation meaningful taxonomies large collections documents 
document collection topics trec collection 
topics form leaf nodes taxonomy member domains domains form higher level taxonomy 
description experiments data sets mhac algorithm find best estimate omega gamma step application em algorithm data structures clustered data sets different numbers clusters inclusive 
numbers clusters ran em algorithm different initial seeds seeds generated method suggested 
optimum number clusters selected clustering gave highest marginal likelihood 
clusters mhac algorithm hierarchy built starting bottom 
experiments interested evaluating algorithm ability 
discover right number leaf clusters right assignments data clusters 
note objective function capable dealing situation data exists intermediate nodes ignored aspect 
note generate noise features root node precluding search noise features stage 
information searches reader referred described 
uncover hierarchical structure imposed 

discover appropriate noise features level hierarchical structure 
compute normalized mutual information nmi discovered hierarchy original expert hierarchy leaves intermediate level 
nmi corresponds perfect agreement class cluster labels nmi corresponds correlation 
evaluating taxonomy generated approach real world data difficult 
trec taxonomy chose levels 
mhac algorithm creates hierarchy merging clusters time creating dendrogram 
create appropriate second level clusters correspond trec taxonomy need cut dendrogram 
compute nmi measures cut mhac generated dendrogram appropriate place 
nmi discovered hierarchy expert taxonomy levels 
nmi discovered hierarchy expert taxonomy levels simple version mhac merge clusters decrease marginal likelihood experimental results results synthetic data results experiments synthetic data shown tables 
tables information number features modeled noise features level nmi leaves intermediate nodes number merges mhac algorithm uncovered 
em algorithm ran synthetic data different values clusters ranging data sets number clusters ran algorithm different starting points chose run highest marginal likelihood 
synthetic structures em algorithm right number clusters 
nmi leaf normalized mutual information true assignment data clusters assignment data clusters em algorithm 
value referred nmi leaf table 
nmi leaf computed manner 
consider data set complete description approach provided 
note mhac algorithm performs merges 
leaf clusters mhac algorithm perform merges find intermediate nodes 
noting mhac performs merges long mhac increases merges performed find table sample size mhac algorithm able perform single merge 
sample sizes resulted maximum possible merges 
merges compute nmi discovered intermediate clusters original assignment data intermediate clusters 
referred nmi inter table 
similar analysis maximum merges performed data set 
mhac size node node nmi nmi actual actual leaf inter na table results synthetic data corresponding 
mhac size node node node nmi nmi act 
act 
act 
leaf int 
table results synthetic data corresponding 
results real document collection real world data set performed feature extraction step features extracted described 
feature selection performed distributional clustering algorithm described 
feature selection left total tokens 
enable appropriate comparison trec taxonomy clustered documents clusters different starting points initial partitions chose run gave highest marginal likelihood 
clusters merged versions mhac algorithm feature selection feature selection mhac 
version feature selection simply merged clusters decrease marginal likelihood 
results experiments see table discussed subsection 
level nmi nmi mhac fs mhac level level table discussion seen tables em mhac algorithm able find structure easily data set opposed second 
importantly em right number clusters structures sets samples 
partly explained fact probability associated useful features leaf nodes forced greater 
number useful features leaf level small difficult em recover correct number leaf nodes may resort approximations 
pursued set experiments 
nmi greater intermediate level indicating confusion assignments data leaf nodes em possibly caused noise features intermediate level 
important factor nmi second level structure perfect sample sizes greater 
case structure mhac fs algorithm single merge merges result increase mlt structure samples algorithm performed wrong merges 
reflected drastic drop value nmi data set shown table 
real world data set note provides slight improvement mhac 
added advantage mhac fs algorithm model provides list common features intermediate node application taxonomies labels describe intermediate nodes 
conducted informal user study users randomly chosen documents original collection documents 
set users required classify documents hierarchy generated em mhac completely set asked classify documents expert hierarchy trec hierarchy 
users evaluated number misclassifications amount time taken classify documents 
users average took minutes classify documents hierarchy generated em mhac lower accuracy classification 
total time taken classify documents trec taxonomy minutes 
summary introduced new model arranging objects natural hierarchy 
key idea exploited fact certain features may common distribution clusters having different distribution 
model applications including important problem automatic taxonomy generation 
objective function associated model rigorous bayesian analysis derived closedform bayesian marginal likelihood hierarchy certain assumptions 
provide novel algorithm find approximate solution objective function 
algorithm works stages stage flat set partitions generated modified version hac algorithm model intermediate nodes features common children 
applied model synthetic datasets real world data set 
despite encouraging results research possibilities remain open model algorithms 
extensions hierarchical model include complicated structures directed acyclic graphs dags need investigated 
experiments uniform prior priors need investigated 
algorithm issues need addressed 
example need overcome mhac limitation merging clusters noise features modeled multiple children 

plan investigate extensions 
authors yael ravin roy byrd free textract timely help provided 
banfield adrian raftery 
hierarchical model clustering large datasets 
biometrics 
bernardo 
posterior distributions bayesian inference 
journal royal statistical society 
bernardo smith 
bayesian theory 
wiley new york 
isbn 
roy byrd yael ravin john prager 
lexical assistance information retrieval user interface 
proceedings symposium document analysis information retrieval 
douglas cutting david karger jan pederson john tukey 
scatter gather cluster approach browsing large document collections 
sigir 
chris fraley 
algorithms model gaussian hierarchical clustering 
siam journal scientific computing 
robin hanson john stutz peter cheeseman 
bayesian classification correlation inheritance 
proceedings twelfth international joint conference artificial intelligence pages san francisco ca 
morgan kaufman 
thomas hofmann 
cluster abstraction model unsupervised learning topic hierarchies text data 
international joint conference artificial intelligence 
marina meila david heckerman 
experimental comparison clustering initialization methods 
technical report msr tr microsoft research division 
christian 
hierarchical model clustering large datasets 
technical report tr university washington seattle department statistics 
yael ravin nina wacholder 
extracting names natural language text 
technical report rc ibm research division 
sahami 
machine learning improve information access 
technical report department computer science stanford university 
shivakumar vaithyanathan byron dom 
generalized model selection unsupervised learning high dimensions 
solla leen muller editors proceedings neural information processing systems 
mit press november 
shivakumar vaithyanathan byron dom 
model selection unsupervised learning applications document clustering 
dzeroski editors proceedings sixteenth international conference machine learning pages san june 
morgan kaufman 
proof theorem analysis merging process oeae oeae oeae oeae delta delta delta oeae diagram simple cluster hierarchy analyze merging process 
prove theorem analyze merging process associated simple hierarchy shown 
initial feature set partition associated initial flat clustering consisting independent clusters 
simplicity assume features effect merging process 
due fact merging process greedy decision features global noise features revisited 
follows adopt simplifying notation 
cluster feature subspace omega gamma note specific model identity holds nonoverlapping subsets composing 
proof holds just models models satisfying 
merge 
initially merging marginal likelihood clusters ju 
ju merge 
jn 
ju 
ju difference lml simply log ratio quantities 
simplify ratio decompose ju ju 
remember fact combined decomposition rule gives ju jn 
ju similar expression ju 
substituting expressions likelihood ratio associated merge cancelling terms yields 
jn 
jn 
jn proves theorem go consider second merge 
certain restriction associated 
second merge 
simply applying analysis led second merge yields 
jn 
jn 
jn pointed part merging process determining associated feature partition 
various possibilities tried giving highest ml merged cluster chosen 
possible partition chosen merging original clusters flat clustering 
higher level clusters 
included merge feature partitions allowed 
perform forward greedy merging process feature set selected part 
merge fixed point merging process 
implication features comprising ineligible inclusion general merge considered point hierarchy features included useful feature set part previous lower level merges ineligible included noise feature set associated higher level merge 
