reinforcement learning non markov environments steven whitehead gte laboratories road waltham ma long ji lin school computer science carnegie mellon university pittsburgh pa october techniques reinforcement learning rl build systems learn perform non trivial sequential decision tasks 
date focussed learning tasks described markov decision processes mdps 
mdps useful modeling wide range control problems important problems inherently non markov 
refer hidden state tasks arise information relevant identifying state environment hidden missing agent internal representation 
important types control problems resist markov modeling system high degree control information collected sensors active vision system limited set sensors provide adequate information current state environment 
surprisingly traditional rl algorithms primarily principles mdps perform hidden state tasks 
article examines approaches extending rl hidden state tasks 
generalized technique called consistent representation cr method described 
method unifies approaches lion algorithm algorithm cs ql restricted class problems call adaptive perception tasks 
general memory algorithms subject restriction 
memory algorithms quite different detail share common feature derives internal representation combining immediate sensory inputs internal state maintained time 
relative merits methods considered conditions useful application 
article concerned techniques building systems learn control 
specifically interested sequential control tasks 
tasks control unfolds time series control actions generated autonomous control system 
sequential control controller agent choosing control action take account action immediate effect impact states 
examples sequential control range simple pole balancing complex human behavior 
interested learning reasons 
interested systems adapt changing conditions changing tasks 
system behavior completely determined ahead time useful learn new task adapt changes environment 
learning simplify design relieving developer burden specifying full controller 
deriving optimal controller carefully analyzing domain priori impossible job cases may efficient install initially suboptimal controller learning optimized 
article focus reinforcement learning paradigm 
central concept underlying reinforcement learning formulate control tasks optimization problems providing system state dependent payoffs rewards penalties 
scenario objective system learn state dependent control policy maximizes measure payoff received time 
early examples reinforcement learning include minsky maze running automata samuel checker player michie chamber pole balancer 
examples include sutton adaptive heuristic critic sutton temporal difference methods holland bucket brigade watkins learning :10.1.1.132.7760
addresses wide range issues including modularity incremental planning efficient credit assignment intelligent exploration efficient representations neural implementations 
respect sequential control attention traditionally focussed learning control markov decision processes 
described formally section markov decision process intuitively corresponds control task point time controller description representation external environment specifies information relevant optimal decision making 
total information assumption called markov assumption important reasons 
markov assumption important theoretical development rl focusing markov decision processes allowed researchers apply classical mathematics stochastic processes dynamic programming :10.1.1.132.7760
second existing reinforcement learning methods depend markov assumption credit assignment perform badly assumption violated 
important control problems naturally easily formulated markov decision processes 
non markov tasks commonly referred hidden state tasks occur possible relevant piece information hidden missing controller representation current situation 
hidden state tasks arise naturally context autonomous learning robots 
example robot internal representation defined solely largely immediate sensor readings circumstances sensors provide information needed uniquely identify state environment respect task decision problem facing embedded controller non markov 
hidden state tasks natural consequence active selective perception 
active perception agent degree control allocation sensory resources controlling visual attention selecting visual processing modules 
control sense environment efficient task specific way 
control properly maintained data generated sensors may say important current state environment agent internal representation ambiguous 
follows agent learn control sensors periods time internal representation inadequate 
decision task non markov 
techniques applying reinforcement learning non markov decision processes central focus article 
describe generalized technique called consistent representation cr method learn control systems active perception 
principal idea underlying cr method split control phases perceptual phase overt phase 
perceptual phase system performs sensing sensor configuration actions order generate adequate read markov representation current external state 
overt stage representation select overt action actions change state external environment 
systems consistent representation method learn overt actions needed perform task perceptual actions needed construct adequate task specific representation environment 
cr method unifies algorithms lion algorithm cs ql algorithm 
limited tasks agent identify point time proper control sensors current state environment respect task 
refer limited class tasks adaptive perception tasks 
general memory algorithms restricted involving adaptive perception described 
simplest approach augments systems sensory inputs delay line achieve crude form short term memory :10.1.1.45.8935
approach successful certain speech recognition tasks 
alternative method predictive distinctions approach system learns predict sensory inputs environmental observables uses internal state predictive model drive action selection 
third approach uses recurrent neural network combination classical reinforcement learning methods learn state dependent control policy utility function directly :10.1.1.45.8935
methods described analyzed detail 
remainder article organized follows 
section provides basic review concept reinforcement learning theory markov decision processes 
section discusses sources non markov processes considers difficulties cause traditional reinforcement learning methods 
section presents consistent representation method technique dealing adaptive perception reviews examples technique 
section considers delay line method predictive distinctions method recurrent neural networks dealing hidden state tasks general 
comparisons drawn approaches preference conditions specified 
section discusses methods broader context scalability drawn section 
review elementary concepts getting details non markov decision problems consistent representation method techniques dealing hidden state useful establish context reviewing models techniques prevalent reinforcement learning literature 
turn brief description model agent environment interaction widely reinforcement learning 
review markov decision processes learning algorithm popular reinforcement learning community 
unfortunately thorough review markov decision processes reinforcement learning general scope article 
focus primarily qlearning difficulties caused non markov decision processes 
algorithms literature suffer similar fate 
complete review markov decision processes learning reader may wish consult 
review reinforcement learning general see 
modeling agent environment interaction illustrates model agent environment interaction widely reinforcement learning research 
model agent environment represented synchronized finite state interacting discrete time cyclical process 
point time series events occur 

agent sensory system measures properties environment constructs description current state environment 

internal representation agent embedded controller chooses motor command perform 

motor command transformed agent motor interface action world 

action issued agent current state environment transition new state generates reward 

reward passed back agent 
state actions sensory system motor system embedded controller internal representation motor commands agent reward simple model agent environment interaction 
environment modeled discrete time discrete state markov decision process 
agent directly sense state environment state process actions map directly process model actions 
environment automaton representing environment modeled markov decision process called controllable markov process 
formally markov decision process described tuple set possible states set possible actions state transition function reward function 
environment time point occupies exactly state accepts point single action usually assumed discrete finite 
dynamics state transitions modeled transition function maps state action pairs new states theta 
transition function generally probabilistic 
random variable denoting state time denote state action performed time respectively 
typically specified terms set transition probabilities prob rewards generated environment determined reward function maps states scalar valued payoffs rewards penalties 
general reward function probabilistic random variable denoting reward received time notice mdp effects actions terms state immediate reward received depend current state 
process models type said memoryless satisfy markov property 
markov property fundamental model environment implies knowledge current state precisely information needed optimal control maximizing reward generated time 
may possible devise action selection strategies base decisions additional observations environment strategies possibly outperform best decision strategies depend knowledge current state 
agent agent automaton consists components sensory interface motor interface embedded controller 
sensory interface implements mapping set external states set possible internal representations motor interface implements mapping internal motor commands external actions embedded controller responsible generating control actions 
time step receives input sensors generates action command interpreted motor interface 
controller receives rewards generated environment 
rewards feedback learning 
cases sensory motor interface controller environment explicitly modeled 
case embedded controller possible experiments occur simulation boundary external world agent internal representation easily blurred 
direct access state environment issues actions directly world 
sensory system explicitly modeled implemented typically corresponds fixed set sensors carefully chosen provide precisely information relevant control 
circumstances mapping internal states external states functional decision problem facing embedded controller markov 
perceptual aliasing said occur mapping internal state space external state space functional 
case non markov decision process obtains 
shall talk perceptual aliasing length section 
remainder section shall neglect sensory motor interface assume controller accesses directly 
policies objective control way specify agent behavior terms control policy prescribes state action perform 
formally policy function states actions denotes action performed state reinforcement learning agent objective learn control policy maximizes measure total reward accumulated time 
principle number reward measures prevalent measure discounted sum reward received time 
sum called return defined time return fl fl called temporal discount factor constant reward received time process may stochastic agent objective find policy maximizes expected return 
fixed policy define value function expected return process begins state follows policy 
agent objective find policy state maximizes value function 
find max important property mdps defined guaranteed exist 
particular optimality theorem dynamic programming guarantees discrete time discrete state markov decision process exists deterministic policy optimal 
furthermore policy optimal satisfies relationship max motor commands mapping actions markov model task 
called action value function defined return agent expects receive starts state applies action follows policy 
intuitively equation says policy optimal state policy specifies action maximizes local action value 
arg max max mdp set action values equation holds unique 
values said define optimal action value function mdp 
mdp completely specified priori including transition probabilities reward distributions dynamic programming techniques compute optimal policy directly 
interested learning assume state space set possible actions known priori statistics governing unknown 
circumstances agent compute optimal policy directly explore environment learn effective control policy trial error 
learning learning incremental reinforcement learning method 
representative reinforcement learning simple elegant mathematically founded widely 
purposes learning useful illustrating difficulties caused non markov decision problems 
reinforcement learning algorithms similar credit assignment techniques td methods understanding difficulties caused non markov decision problems learning goes long way understanding weaknesses algorithms :10.1.1.132.7760
detailed treatment learning see 
learning agent estimates optimal action value function directly uses derive control policy local greedy strategy mandated equation 
simple learning algorithm shown 
step algorithm initialize agent action value function agent estimate optimal action value function 
prior knowledge task available information may encoded initial values initial values arbitrary uniformly zero 
agent initial control policy established 
done assigning action locally maximizes action value 
arg max ties assumed broken arbitrarily 
initialization agent enters main control learning cycle 
agent senses current state selects action perform 
time action set initial values action value function uniformly zero max repeat forever current state select action execute usually consistent occasionally alternate 
example choose follow probability choose random action 
execute action state reward received 
update action value estimate state action pair gamma ff ff flu 
update policy arg max simple version step learning algorithm 
action specified policy occasionally agent choose random action 
agent executes selected action notes immediate reward resulting state action value estimate state action pair updated 
particular estimate obtained combining immediate reward utility estimate state max 
sum flu called step corrected estimator unbiased estimator definition max 
step estimate combined old estimate weighted sum gamma ff ff flu ff learning rate 
agent control policy updated equation cycle repeats 
limit state action pair tried occasionally choosing action random particularly simple mechanism exploring environment 
exploration necessary guarantee agent eventually learn optimal policy 
examples sophisticated exploration strategies see 
infinitely learning rate decreases proper schedule learning guaranteed converge optimal policy finite markov decision processes 
example simple example consider maze task illustrated 
problem agent free roam bounded dimensional maze 
move principle directions left right pass barriers 
actions deterministic 
agent longitude latitude sensors accurately locate position maze 
task navigate cell labeled 
agent goal small positive reward generated time enters goal cell 
states agent receives reward 
facilitate exploration entering goal cell agent new random location maze 
way temporal evolution process resembles sequence repeated trials 
state space task determined possible values location sensors 
case maze size theta total distinct cells occupied barrier 
choice possible actions state agent estimate total theta action values 
agent initially ignorant underlying structure environment position reward accurately estimate optimal action value function 
circumstances particularly simple approach initialize action values zero 
case agent initial performance random assuming ties broken choosing randomly useful change function occurs agent encounters goal state 
point action value state action pair immediately proceeded goal state increased 
subsequent trials action values state action pairs incrementally increased lead reward states states high utility 
way reward information backed accurate estimate optimal action value function obtained optimal control policy learned 
figures show examples policies learned trials respectively ff 
trials reward information propagated states trials action values nearly state effected reward generated goal 
non markov tasks model depicted widely reinforcement learning literature usefully applied number simple adaptive control problems 
key assumption model decision problem facing embedded controller markov 
important tasks naturally formulated model lead naturally non markov models 
particular non markov decision tasks arise time possible simple reinforcement learning task maze policy learned trials policy trials 
filled arrows indicate cells action values effected reward goal state 
controller uncertain state external environment 
section describe ways non markov decision problems arise context autonomous robot learning 
describe problems caused non markov decision problems learning particular reinforcement learning general 
active perception active perception refers idea intelligent agent actively control sensors order sense represent information relevant immediate ongoing activity 
system sensors matched intended control task clear relatively simple control tasks set fixed matched sensors may adequate 
active perception differentiates approaches perceptual organization agent behavior scaled include variety complex tasks come go time 
case body information relevant point time changes different phases task unfolds agent moves task 
example items immediate interest boy playground varies depending playing tag baseball soccer bat second playing 
diverse time dependent information needs active perception paradigm efficiency considerations mandate active approach sensing 
human vision example active perception 
people efficiently move eyes foveate objects behavioral significance register peripheral objects lower resolution 
similarly aimed developing vision systems robots seen shift active sensing active perception relevant adaptive control reinforcement learning reasons 
agents equipped active sensory systems pose interesting important adaptive control problems 
particular robot active sensory motor system build controller reinforcement learning techniques effectively learns control perception action 
second adaptive control coupled active sensing provides opportunity overcome limitation previous model right agent sensory system generates internal representation uniquely identifies state environment point time 
assumption implies designer system knows task ahead time identify relevant state variables choose sensors sensing procedures measure efficiently 
learning system active perception may possible equip system flexible sensory system learn extract relevant bit information dynamic task specific representation simultaneously learns control 
tasks involve active perception lead naturally non markov decision problems improper control sensors leads internal representations fail encode information relevant decision making 
point illustrated schematically shows basic structure system active world embedded controller selective sensory system motor system task specific representation overt acts sensory acts agent simple model structure system active perception 
perception 
key feature diagram arrow feeds embedded controller back sensory system 
represents control information commands modulate sensory mapping 
case sensory system thought implementing series sensory mappings states external model bits internal representation 
control signals select modify sensory mapping 
controller initially know appropriate sensing strategy may course learning adopt sensory mappings neglect relevant information phenomenon call perceptual aliasing 
formally perceptual aliasing occurs state internal representation maps represents states external markov model task 
perceptual aliasing causes embedded decision problem non markov circumstances impossible define set state transition reward probabilities internal state space accurately capture dynamics external process 
define class adaptive perception tasks control problems agent active sensory system properly configured uniquely identify state markov model external task 
shall assume sensory systems tasks changing configuration sensory system way effects state external environment 
kinds hidden state tasks course active perception source non markov decision problems 
non markov task arises relevant information missing agent internal representation 
situated system depends solely largely immediate sensory inputs decision making reason relevant piece information hidden sensors resultant control problem non markov 
lin provides example consider packing task involves steps open box put gift close seal :10.1.1.45.8935
agent driven current visual precepts accomplish task facing closed box agent know gift box decide seal open box 
case occlusion gift lid prevents immediate perception vital piece information 
hidden state tasks arise temporal features velocity acceleration important optimal control included system primitive sensor set 
effects learning straightforward application reinforcement learning non markov decision problems cases fails yield optimal policy cases results bad performance 
perceptual aliasing non markov decision problems result cause problems existing reinforcement learning 
dynamic programming methods existing reinforcement learning algorithms aim learn optimal policy deterministic 
markov processes guaranteed deterministic optimal policy optimal policies non markov processes frequently non deterministic 
second learning algorithms learning ahc bucket brigade adapt control policies maximizing local evaluation function action value function learning state utility function ahc 
non markov decision problems accurate estimates local evaluation functions obtained states perceptually aliased 
leads localized errors policy function 
temporal difference methods temporal credit assignment spreads estimation errors state space infecting policy actions non aliased states :10.1.1.132.7760
illustrate problem concretely examine effects applying learning simple non markov decision problem 
consider task shown 
task external decision problem state space containing states fs gg actions ae fl rg deterministic transition function shown 
goal external task associating single action state optimal policy involves associating state non trivial probability distribution possible actions 
illustration difficulties caused perceptual aliasing 
transition diagrams simple decision task transition diagram external decision problem transition diagram internal perceived decision problem interpreted sensory motor system perceptual aliasing 
enter goal state agent receives fixed reward 
non goal states yield zero reward 
optimal value function external markov task denoted exponentially decreasing function distance goal 
re fl gamma distance steps state goal 
optimal policy corresponds choosing action minimizes distance goal 
case optimal policy requires agent moving right opportunity 
notice optimal solution path trial traces trajectory monotonically increasing time optimal policy corresponds performing gradient ascent result illustrated plots versus time trial begins state time follows optimal trajectory time 
applied directly problem learning algorithm described easily learn optimal policy 
introduce perceptual aliasing sensory mapping see happens 
consider internal decision problem results agent system implements perceptual mapping fixed states gets mapped internal state fs represents world state 
motor mapping fl map respectively 
transition diagram internal decision problem shown 
notice decision problem non markov effects actions independent past depend hidden external state internal state reads 
note fixed optimal policy task apply action step learning learn optimal policy task 
particular agent policy initialized optimal policy controller fixed system follows optimal policy probability chooses random action system run long series trials adequate learn optimal value action value functions observed 
value action value estimates respectively expected returns state take values corresponding values external decision problem 
fact estimated action value function converge true sampled average returns observed 
follows update action value function agent uses step estimator enforces local constraints values estimated 
learning rate gradually decreased time external utility time steps actual external utility versus time optimal trajectory estimated utility time steps estimated utility versus time optimal trajectory plots utility versus time agent traverses state fl utility external decision problem utility estimates internal decision problem obtained step learning algorithm 
table utility action value functions estimated step learning algorithm true sampled utility action value functions 
estimated functions match true sampled values obtained satisfying local constraints imposed corrected step estimator 
estimated utility denoted respectively sampled utility action values denoted values shown fl 
action value function estimated agent converges values satisfy local relationships fl flu flu flu flu flu flu flu flu flu flu flu flu flu max fl 
equation fraction times application state results states respectively 
similarly equation fraction times application state results states respectively 
trials state values utility action value functions converge values shown table 
shown table sampled utility action values respectively obtained measuring averaging returns received trials step estimator 
notice sampled averages match optimal values external decision task states case notice utility action values estimated learning state match external sampled utility action values 
discrepancy arises estimates states internal task directly indirectly dependent utility estimate indistinguishable internal action value estimates constrained consequently inaccurate 
inaccurate utility estimates turn get propagated back states state space 
observation utility function learned measured internal decision problem longer increases monotonically system traverses optimal solution trajectory 
anomaly shown graphically plots function time system follows optimal trajectory plot shows utility aberration occurs system encounters point environment state true expected return fl 
indistinguishable internal representation internal decision system overestimates expected return 
similarly estimation error occurs second time encountered environment state case underestimates expected return 
relax hold decision policy allow system adapt find optimal policy unstable 
system unable find optimal policy moves away 
general system oscillate policies finding stable 
instability understood considering effect utility estimation errors policy 
recall qlearning system locally adjusts policy order maximize expected return 
running agent fixed policy trials releasing policy value state changed system tends take actions move back forward 
large utility value state acts attractor nearby states causes change local policy away optimal 
intuitive way understand problem consider local sits see utilities neighbors 
point view looks desirable system execute leads step goal 
hand choosing action leads leaves system steps goal 
point view going average better going perceive perceptual aliasing going directly returns real external world state reach directly 
problem distinguish represented erroneously markov assumption effects actions depend current perceived state 
errors utility function unstable running average expected returns 
policy changes rarely visited aberration disappear 
unfortunately soon policy changed begins encountered frequently aberration reappears 
system oscillates policy policy unable converge stable 
consistent representation methods years seen development rl algorithms deal active perception 
lion algorithm learns control visual attention primitive deictic sensory motor system cs ql algorithm learns efficient task specific sensing trees algorithm learns extract task relevant bits large input vector 
section review algorithms turn 
consistent representation method generalized approach adaptive perception unifies algorithms 
unified view summarizes basic assumptions limitations algorithms suggests new algorithms extend combine pieces basic architecture novel ways 
lion algorithm lion algorithm reinforcement learning algorithm specifically designed address adaptive perception task 
learn simple manipulation task modified blocks world 
distinguishing feature task agent equipped sensory motor system provides partial access environment 
learn task agent learn focus visual attention relevant objects select appropriate motor commands 
details task follows 
task learning task organized sequence trails 
trial agent pile blocks 
pile consists random number blocks ranging arranged arbitrary stacks 
blocks distinguishable color may red green blue 
pile contains single green block 
agent goal simply pick green block quickly possible 
note methods differ supervised feature selection methods rely presentation preclassified samples 
algorithms operate explicit supervision context embedded reinforcement learning task 
robot achieves goal trial time limit expires receives fixed positive reward receives reward 
dynamics environment block grasped uncovered agent hand empty 
cases necessary unstack blocks reach goal 
task effects block manipulating actions completely deterministic 
differentiates task blocks world problems reinforcement learning tasks agent sensory motor system 
assuming sensory system provides complete objective description object scene system equipped deictic sensory motor system provides controller ability flexibly access limited amount information scene time 
deictic sensory motor system selective perception implemented markers 
conceptually marker corresponds focus attention 
practice markers establish frames perception overt action 
sensory side placing marker object environment brings information object view internal representation 
motor side marker placement select targets overt manipulation 
specification sensory motor system agent 
system employs markers called action frame marker attention frame marker 
sensory side system generates bit input vector point time 
bits represent local marker specific information color shape marker bound object 
bits detect relational properties vertical horizontal alignment detect spatially non specific properties presence absence red scene 
moving markers object object agent multiplex wide range information relatively small input bit register 
listed right hand side internal motor commands supported sensory motor system 
commands partitioned groups related action frame marker related attention frame marker 
groups contain commands controlling marker placement 
actions index objects primitive features color spatial relationship top stack 
action frame marker additional commands manipulating blocks 
grasp object command causes system grasp possible object marked action frame marker 
similarly place object action frame command causes system place held object location marked action frame 
decision problem facing agent embedded controller non markov improper placement system markers fails multiplex relevant information agent internal representation 
point illustrated shows different external world states corresponding different states markov model task improper placement markers generate internal representation 
red scene green scene blue scene object hand action frame color red green 
action frame shape block table action frame stack height 
action frame table action frame hand attn frame color red green 
attn frame shape block table attn frame stack height 
attn frame table attn frame hand frames vertically aligned frames horizontally aligned peripheral features local features relational properties grasp obj action frame place obj action frame move action frame red move action frame green move action frame blue move action frame stack top move action frame stack bottom move action frame table action frame commands move attn frame red move attn frame green move attn frame blue move attn frame stack top move attn frame stack bottom move attn frame table attention frame commands sensory inputs internal motor commands specification deictic sensory motor system ii whitehead 
system markers action frame marker attention frame marker 
system bit input vector overt actions perceptual actions 
values registered input vector effects internal action commands depend bindings markers sensory motor system objects environment 
world state world state internal rep example perceptual aliasing block stacking domain 
case world states different utilities optimal actions generate internal representation 
control tackle non markov decision problem lion algorithm adopts approach attempts select overt manipulative actions action values internal states markov 
accomplish lion algorithm breaks control stages 
control cycle perceptual stage performed 
perceptual stage sequence commands moving attention frame marker executed 
called perceptual actions cause sequence input vectors appear input register 
values temporarily buffered short term memory 
perceptual actions change state external environment buffered input corresponds representation current external state 
perceptual actions selected care internal states markov encode information relevant selecting optimal action 
perceptual stage completed overt stage begins 
overt stage action changing state external environment selected 
called overt actions correspond commands action frame marker 
guide selection overt action lion algorithm maintains special action value function defined internal state overt action pairs 
overt action value function notice moving action frame marker object changes state external environment changes set objects effected grasp place commands 
world states perceptual actions overt action consistent internal states internal states graphical depiction lion algorithm 
large super graph depicts overt control cycle large nodes correspond world states arcs correspond overt actions 
subgraphs embedded large node depict perceptual cycles nodes corresponding internal representations current world state arcs corresponding perceptual actions 
special action values non markov states suppressed ideally equal zero action values markov states allowed take nominal values 
action value function lion algorithm overt stage selects overt action simply examining action values buffered internal state choosing action maximal 
non markov states tend suppressed action values selected action tends correspond maximal action markov internal state 
illustrates stage control cycle graphically 
learning special learning rule learn overt action value function 
learning procedure operates follows 
internal state maximal identified lion 
action value state updated standard rule step learning viz 
equation 
error term updating rule lion state update action values remaining buffered states 
done accurate action value learned markov state changes action values non markov states cease 
buffered state tested see non markov 
state tests positive action value reset zero 
simple procedure identify potentially non markov states 
rule simply examines sign error term step learning rule sign difference state current action value estimate constructed step delay 
action values initially zero task deterministic rewards positive non markov states due perceptual aliasing tend regularly overestimate action values show negative error markov states tend monotonically approach optimal action value positive error 
non markov states detected monitoring sign estimation error 
learning rule perceptual stage simpler 
perceptual control perceptual action value function estimated internal state pairs 
perceptual stage perceptual actions selected choosing action maximizes action value current input bit vector internal state 
perceptual action value function updated perceptual stage standard step learning rule overt utility internal state accounted 
non markov states tend suppressed overt action values perceptual actions tend lead markov internal states tend higher action values 
see details 
discussion lion algorithm able learn block manipulation task described 
learns perceptual control strategy focuses attention frame marker green block learns overt control policy moves action frame marker needed unstack covering blocks 
detailed experimental results 
assumptions exploited lion algorithm applicable tasks meet restrictions 
effects actions deterministic 
positive rewards allowed 
external state exist configuration sensory system generates internal state markov 
cs ql machine learning aimed learning classification tasks focuses predictive power piece information neglects account cost obtaining 
tan recognized learn classification procedures efficient necessary learning algorithm explicitly account cost sensing 
develops cost sensitive learning algorithms classification tasks cs id cs ibl respectively 
cs ql stands cost sensitive learning resulted combined ideas cost sensitive learning reinforcement learning 
cs ql reinforcement learning subtle interactions cause markov states overestimate action values 
leads suppression markov states 
states tend bounce back eventually stabilize 
detailed discussion technique detecting non markov states see 
agent learns overt actions needed perform task learns efficient procedure classifying current state environment respect task 
cs ql lion algorithm share basic control cycle 
cs ql control decomposed stage process sensing perceptual control action overt control 
sensing model cs ql considerably different 
deictic sensory motor system cs ql adopts sensing model agent equipped set atomic sensing tests 
sensing test provides specific piece information external environment 
learning perceptual control policy lion algorithm cs ql constructs classification tree internal nodes correspond sensing operations branches correspond test results leaves correspond states agent internal representation 
cs ql agent learned adequate classification tree leaf tree markov leaf represents unique state markov model task 
classification tree learned incrementally 
initially tree consists single root node 
non markov leaves detected expanded converted internal nodes attaching sensing operations 
new leaf nodes result introduce new distinctions representations 
tree expanded markov representation achieved 
expanding node cs ql simply selects expensive sensing operation remain attach target leaf 
heuristic favoring low cost tests tends explore inexpensive sensing procedures may generate efficient trees 
incorporating sophisticated selection method accounts cost discriminatory power sensing test see algorithm efficient classification trees result 
detect non markov leafs cs ql uses overestimation principle employed lion algorithm 
cs ql limited deterministic domains 
cs ql demonstrated successfully simulated robot navigation task similar shown 
navigation tasks studied reinforcement learning robot automatically provided knowledge current position 
employ sensors gather information local surround deduce position surrounding geographic structure 
robot sensing operations allow detect properties empty barrier cup nearby cells maze 
cost sensing cell assumed proportional distance robot 
accumulating features nearby cells system successfully identify position maze 
example classification tree learned cs ql state descriptors shown 
note tests atomic composed meaningful way 
left state cost state cost state cost left state cost state cost cup obstacle obstacle obstacle obstacle obstacle simple example cs ql grid world learned mapping state descriptions states learned optimal decision policy learned cost sensitive classification tree 
reproduced permission 
algorithm algorithm third technique developed address kind adaptive perception task 
lion algorithm cs ql development specifically motivated desire minimize cost sensing need control active sensory system 
algorithm developed mitigate problems caused availability information 
particular chapman kaelbling tried apply learning learn simple align shoot subtask context general video game domain called amazon learning system overwhelmed shear volume information generated sensory system 
subtask involves aligning agent target orienting firing weapon 
point time agent sensory system generates bits input 
information results internal state space containing states 
bits input irrelevant specific task just interfere learning introducing unnecessary distinctions internal representation 
hand bits specifically relevant necessarily known ahead time stages game irrelevant bits vitally important 
algorithm developed learn control policy generalization irrelevant information input 
algorithm works identifying bits input vector significant important control 
similar cs ql incrementally grow classification trees 
start single root node assuming information relevant construct tree structure classification circuit recursively splitting nodes values sensory inputs 
cs ql information split nodes tree corresponds results sensing acts tests algorithm nodes split values bits input 
cs ql leaves algorithm tree define agent internal state space 
cs ql algorithm associate cost sensing reading bit 
sets algorithm apart cs ql lion algorithm method uses detect non markov internal states 
cs ql lion algorithm monitor sign estimation error detect non markov states method limited deterministic tasks 
algorithm uses general statistical test 
general leaf classification tree shown bits input vector tested traversing tree node leaf statistically relevant predicting rewards 
detect leaf non markov algorithm uses student test find statistically significant bits 
time agent experiences variety state bit give leaf situations classified leaf divided blocks 
block corresponds situations bit question bit 
data occurrence reward immediate collected block 
sets data student test determine probable distinct distributions gave rise 
sufficient sampling probability estimate threshold bit deemed relevant leaf split 
insight provided algorithm statistical methods test bit relevance consequently detect non markov states 
specific algorithm limited test assumes underlying distributions compared gaussian 
clearly case general reward distributions arbitrary 
problem mitigated comparing distributions cumulative rewards central limit theorem tend normality number summed increases 
algorithm guaranteed detect bits relevant higher order pairings 
bit relevance apparent isolation 
additional memory sensing required gather statistics relevance testing 
difficulties limitations minor price pay method extends stochastic domains 
algorithm successfully demonstrated orient shoot task 
particular significantly outperform alternative approach error backpropagation neural network 
see details discussion difficulties encounter 
consistent representation method algorithms described vary considerably detail share basic approach 
refer common framework consistent representation cr method 
key features cr method 
time step control partitioned stages perceptual stage followed action overt stage 

perceptual stage aims generate internal representation markov 

action stage generates external state modifying actions effort maximize cumulative reward 
learning occurs control stages 
action stage traditional reinforcement learning techniques 
techniques impose markov constraint internal state space 
constraint turn drives adaptation perceptual stage perceptual stage constantly monitors internal representation non markov states 
perceptual process modified eliminate 

assumed external state identified immediate sensory inputs 
term consistent representation derived fact strictly necessary internal state space absolutely markov 
particular sufficient state markov respect predicting rewards necessarily states 
slightly weaker concept partially markov markov respect reward associated term consistent 
see discussion distinction 
illustrates architectural embodiment cr method 
major components include selective sensory motor interface perception module controller module representation monitor 
line perception module sensory motor interface represents perceptual control selection acts 
line controller module sensory motor interface represents overt acts 
controller perceptual modules adaptive 
reward environment received controller representation monitor 
representation monitor detects non markov states provides feedback perception module 
correspondence components architecture previous algorithms follows 
lion algorithm assumes deictic system includes commands moving perceptual attentional markers cs ql assumes sensory motor interface consists set discrete sensing acts algorithm assumes binary input vector individual bits selected 
identification procedure implemented perception module takes form perceptual policy lion algorithm form binary classification tree cs ql algorithm 
internal representation generated lion algorithms corresponds subset input bit vectors cs ql algorithm defined leaves classification tree 
lion cs ql algorithm form learning overt control 
representation monitoring lion cs ql algorithm overestimation technique algorithm relies general statistical method 
relating lion cs ql algorithms common framework cr method useful reasons 
promotes cross fertilization ideas specific algorithms 
instance statistical methods algorithms incorporated lion cs ql yield algorithms function stochastic domains 
second structure provided cr method highlights shared assumptions limitations suggests extensions overcome 
particular fundamental assumption algorithms external states identified point time immediate sensor inputs 
assumption techniques inappropriate interesting tasks require memory keep track information reason perceptually inaccessible 
general hidden state tasks memory approaches subject section 
version lion algorithm developed feedback external supervisor detect non markov states 
external supervision dramatically improves perceptual overt learning 
task specific rep world selective sensory system motor system overt acts agent perceptual control overt control rep analysis consistency feedback sensor acts selected sensory info reward state action basic architecture system consistent representation method 
control accomplished stages perceptual stage followed overt stage 
goal perceptual stage generate markov task dependent internal state space 
goal overt control maximize discounted reward 
control stages adaptive 
standard reinforcement learning algorithms overt learning perceptual learning driven feedback generated representation analysis module monitors internal state space non markov states 
memory methods obvious approach dealing inadequate perception non markov decision problems allow agent memory past 
memory help agent identify hidden states differences memory traces distinguish situations immediate perception appear identical 
problem huge volume information available past agent decide remember encode 
approaches problem discussed literature 
approach agent keeps sliding window history approach agent builds state dependent predictive model environmental observables :10.1.1.45.8935
addition approaches section describes new third approach learns history sensitive control policy directly reinforcement 
memory architectures depicts memory architectures reinforcement learning domains 
architectures neural network net trained temporal difference methods incrementally learn action value function function 
window architecture relying immediate sensory inputs sensations define internal representation architecture uses immediate sensations sensations time steps actions represent current state 
words window architecture allows direct access information past sliding window 
called window size 
window architecture simple straightforward 
architecture choose window size may difficult advance 
hand selected window size small internal representation may sufficient define state space markov 
hand em input generalization problem may arise window size chosen large window necessarily large capture relevant information sparsely distributed time 
circumstances excessive amounts training may required neural network accurately learn action value function generalize irrelevant inputs 
spite problems window architecture worthy study kind time delayed neural network ahs useful speech recognition tasks architecture establish baseline comparing methods 
window architecture sort brute force approach memory 
alternative distill small set contextual features large volume information past 
historical context agent current sensory inputs define internal representation 
context features constructed correctly resultant internal state space markov standard rl methods learn optimal control utility net action utility model sensation action history features memory utility net sensation action sensation payoff net sensation action history features memory current sensation sensations actions memory architectures reinforcement learning non markov domains window architecture recurrent architecture recurrent model architecture 
policy 
recurrent recurrent model architectures illustrated basic idea 
differ way construct context features 
window architecture architectures principle discover utilize historical information depend sensations arbitrarily deep past practice difficult achieve 
recurrent neural networks elman networks provide approach constructing relevant context features 
illustrated input units elman network divided groups immediate sensory input units context units 
context units encode compressed representation relevant information past 
units function kind memory encode aggregate previous network states output network depends past current inputs 
recurrent architecture uses recurrent network estimate function directly 
predict action values correctly recurrent network called recurrent net learn contextual features enable network output units hidden units input units context units feedback elman network 
distinguish different external states generate immediate sensory inputs 
recurrent model architecture consists concurrent learning components step prediction module simply model qlearning module 
prediction module responsible learning predict immediate sensory inputs rewards result performing action 
agent immediate inputs completely code state external environment model learn context features accurately predict effects action environment 
assume accurate predictive model learned models context features extracted markov state space generated learning component defining inputs internal state space conjunction agent immediate sensory input context features 
follows time state environment completely determined new state representation action taken 
general predictive model trained predict new sensory inputs immediate reward 
see consider packing task steps put gift open box seal box opened place box proper bucket depending color gift box 
suppose reward box placed correct bucket 
note agent required know gift color order predict sensory inputs box opened sealed 
model predicts sensations may set context features adequate control features may encode information color 
recurrent recurrent model architectures learn context features gradient descent mean square method error back propagation differ important way 
learning predictive model goal minimize errors actual predicted sensory inputs rewards 
case environment provides needed training information consistent time long environment change 
recurrent qlearning goal minimize errors temporally successive predictions action utility values see equation 
case error signals computed partly information environment partly agent current estimate optimal action value function 
term changes time carries little useful information early stages learning error signals may general weak noisy inconsistent time 
practical viability recurrent architecture uncertain 
having introduced architectures worthwhile note combinations approaches possible 
example combine architectures inputs recurrent net include just current sensory input inputs actions 
combine architectures 
instance approach share context units model network network context features learned prediction errors networks 
possibilities article concerned basic architectures 
investigation needed see combinations result better performance basic versions 
network training non recurrent nets window recurrent model architectures trained straightforward combination temporal difference methods connectionist back propagation algorithm :10.1.1.132.7760
combination successfully applied solve nontrivial reinforcement learning problems 
training model recurrent model architecture slightly complicated 
recurrent networks trained recurrent version backpropagation algorithm called back propagation time bptt unfolding time 
bptt observation recurrent network spanning steps converted equivalent feed forward network duplicating network times 
recurrent network unfolded back propagation directly applied 
net recurrent architecture trained bptt temporal difference 
detailed network structures implementation see :10.1.1.45.8935
simulation results subsection presents experimental results study memorybased architectures applied series non markov decision tasks 
study gained insight behavior architectures better understanding relative merits conditions useful application 
detailed descriptions simulation results :10.1.1.45.8935
actions walk left walk right pick binary inputs left cup right cup left collision right collision reward cup picked possible initial states task cup collection task 
task cup collection simple cup collection task 
task requires learning agent pick cups located space 
agent actions walking right cell walking left cell pick 
agent executes pick action pick cup cup located agent current cell 
agent sensory input includes binary bits bits indicating cup immediate left right cell bits indicating previous action results collision left right 
action attempting move agent space cause collision 
cups placed far apart agent picks cup see 
act optimally agent remember location second cup 
task non trivial reasons agent sense cup front agent gets reward cups picked agent operates cup sight especially picking cup 
experiment trial begins possible initial states shown 
restriction simplifies task avoiding perceptual aliasing onset trial history information available 
memory architectures tested cup collection task 
experiment repeated times time successfully learned optimal control policy trials 
window size 
interesting observation recurrent model architecture learned perfect model trials 
instance agent seen cup steps model normally able predict appearance cup 
imperfect model prevent learning learning optimal policy 
experiment revealed lessons ffl architectures worked simple cup collection problem 
ffl recurrent model architecture just partially correct model may provide sufficient context features optimal control 
news perfect model difficult obtain 
task task random features task simply task random bits agent sensation 
random bits simulate difficult predict irrelevant features accessible learning agent 
real world features difficult predict fortunately relevant task solved 
example predicting going rain outside difficult matter task pick cups inside 
ability handle difficult predict irrelevant features important learning system practical 
simulation results summarized follows random features gave little impact performance window architecture recurrent architecture noticeable negative impact recurrent model architecture observed 
system recurrent model architecture exhibited optimal performance course trials 
apparently stabilize optimal policy optimal policy sub optimal policies 
observed model tried vain reduce prediction errors random bits 
possible explanations poorer performance compared obtained random sensation bits 
model fail learn context features needed solve task effort wasted trying learn predict random bits 
second activations context units shared model network net change representation context features model part simply destabilize trained net change significant 
explanation ruled optimal policy times 
test second explanation fixed model point learning allowed changes net 
setup agent optimal policy stuck 
experiment revealed lessons ffl recurrent architecture economic recurrent model architecture sense try learn context feature appear relevant predicting action values 
ffl potential problem recurrent model architecture changes representation context features model part may cause instability net part 
task task control errors noise uncertainty prevail real world 
study capability architectures handle noise added control errors agent actuators pole balancing problem 
time executed action effect environment 
random bits removed 
runs window architecture successfully optimal policy runs suboptimal policies 
contrast recurrent architecture learned optimal policy little instability recurrent model architecture optimal policy trials policy optimal sub optimal ones due changing representation context features happened task 
find way model example gradually decreasing learning rate able obtain stable optimal policy 
lessons learned experiment ffl architectures handle small control errors degree 
ffl architectures recurrent scale best presence control errors 
task pole balancing pole balancing problem system objective apply forces base movable cart order balance pole attached cart hinge 
problem studied widely reinforcement learning literature 
practical interest resemblance problems aerospace missile guidance robotics biped balance locomotion 
theoretical interest difficult credit assignment problem arises due sparse reinforcement signals 
particular formulations problem system receives non zero reinforcement pole falls 
instance simulations system receives penalty gamma pole tilt exceeds degrees vertical 
traditional pole balancing task system sensory inputs include position velocity cart angular position velocity pole 
information completely characterizes state system yields control problem markov 
experiments cart position pole angle 
yields non markov decision problem order learn adequate control policy system construct contextual features resembling velocities cart pole 
experiment policy considered satisfactory pole balanced steps test trials pole starts angle sigma sigma sigma degrees 
maximum initial pole angle pole balanced indefinitely degrees 
training phase pole angles cart positions generated randomly 
initial cart velocity pole velocity set 

input representation straightforward real valued input unit pole angle cart position 
table shows number trials taken architecture satisfactory policy learned 
numbers average results best runs 
satisfactory policy trials 
method window recurrent recurrent model trials lesson learned experiment ffl recurrent architecture suitable architecture cup collection tasks outperformed architectures pole balancing task 
discussion experiments provide insight performance memory architectures 
section considers task characteristics may useful determining architecture may preferred 
features parameters task effect applicability architectures ffl memory depth 
important problem parameter length time agent remember previous inputs order generate internal representation markov 
example memory depth task evidenced fact window agent able obtain optimal control window size 
memory depth pole balancing task 
note learning optimal policy may require larger memory depth needed represent policy 
ffl payoff delay 
cases payoff zero goal state define payoff delay problem length optimal action sequence leading goal 
parameter important influences difficulty learning 
payoff delay increases learning accurate function increasingly difficult due increasing difficulty credit assignment 
ffl number context features learned 
general perceptual aliasing agent faces context features agent discover difficult task 
general predicting sensations model requires context features predicting net turn requires context features representing optimal policies 
consider task example 
binary context features required determine optimal actions cup front second cup right hand side left hand side 
perfect function requires features cups picked far far second cup 
perfect model task requires features perfect function 
perfect model task requires features current state random number generator perfect function task requires extra features 
important note need perfect function perfect model order obtain optimal policy 
function just needs assign value action response situation relative values correct order model just needs provide sufficient features constructing function 
architecture characteristics problem parameters understand architectures best suited particular types problems 
consider key advantages disadvantages architecture problem parameters influence importance characteristics 
ffl recurrent model architecture 
key difference architecture recurrent architecture learning context features driven learning action model function 
strength approach agent obtain better training data action model function making learning reliable efficient 
particular training examples action model sensation action sensation payoff 
quadruples directly observable step agent takes environment 
contrast training examples function sensation action utility 
triples directly observable agent estimate training utility values changing approximation true action value function 
second strength approach learned features dependent environment independent reward function action model may trained predict rewards sensations result features reused agent different reward functions goals learn achieve 
ffl recurrent architecture 
architecture suffers relative disadvantage learn indirectly observable training examples offsetting advantage need learn context features relevant control problem 
context features needed represent optimal action model superset needed represent optimal function 
easily seen noticing optimal control action principle computed action model look ahead search 
cases features necessary predicting utilities needed predict completely state number context features learned recurrent architecture smaller number needed recurrent model architecture 
ffl window architecture 
primary advantage architecture learn state representation recursively recurrent network architectures 
recurrent networks typically take longer train non recurrent networks 
advantage offset disadvantage history information limited features directly observable fixed window captures bounded history 
contrast recurrent network approaches principle represent context features depend sensations arbitrarily deep agent history 
competing advantages architectures imagine preferred architecture different types problems ffl expect advantage window architecture greatest tasks memory depths smallest example pole balancing task 
ffl expect recurrent model architecture advantage directly available training examples important tasks payoff delay longest example pole balancing task 
situations indirect estimation training values problematic recurrent architecture 
ffl expect advantage recurrent architecture need learn features relevant control pronounced tasks ratio relevant irrelevant context features lowest example cup collection task random features 
recurrent model architecture acquire optimal policy long just relevant features learned drive learning irrelevant features may cause problems 
representing irrelevant features may limited context units sacrifice learning relevant features 
secondly seen experiments recurrent model architecture subject instability due changing representation context features change improves model deteriorate function needs re learned 
tapped delay line scheme window architecture uses widely applied speech recognition turned quite useful technique 
expect control tasks speech recognition important difference tasks 
major task speech recognition find temporal structure exists sequence speech phonemes 
reinforcement learning agent look temporal structure generated actions 
actions generated randomly case early learning find sensible temporal structures action sequence improve action selection policy 
discussion principle memory architectures described previous section applicable non markov tasks general 
raises question usefully applied adaptive perception tasks 
example memory architectures learn control deictic sensory motor system similar described 
writing question remains open 
principle memory architectures 
experimental studies tell sure preliminary results cast shadow doubt 
particular neural network backpropagation tested adaptive perception task described section agent learn efficiently select relevant features bit input 
algorithm instance cr method able learn task backpropagation network 
apparently neural network difficulty dealing noise introduced irrelevant bits input 
similar kinds difficulties expected arise controlling active sensory system system access tremendous volume irrelevant information 
fair noted network experiments preliminary simple version backpropagation algorithm 
sophisticated methods momentum terms updating rule may yield better results 
may possible extend cr method deal general hidden state problems 
simple approach lines extend agent selective sensory system include remembered sensory motor events 
selecting bits information current sensory input system select bits memory trace previous inputs actions 
approach similar window architecture memory trace maintained differs relatively small amount information selected point time 
scheme possible devise rules updating memory trace way preserve relevant memories dropping irrelevant ones 
architectures combine features cr method memorybased architectures may useful 
example problem cr method currently stands system uses information previous state environment trying identify current state 
sense system re identifies state environment starting scratch action 
knowledge state action considerably reduce effort required identify current state environments transitions states tend local predictable 
rediscovering state action agent merely verify current state worst case identify outcome limited number possibilities 
addition exploring variations architectures assess scalability algorithms 
algorithms derived desire extend reinforcement learning markov decision problems problems involve active perception hidden state 
extent successful 
tasks explored remain painfully simple compared scale problems required truly autonomous intelligent behavior 
issues addressed achieve scalability include ffl learning bias reinforcement learning viewed kind search space possible control policies 
search biased appropriate direction learning proceed quickly 
approach introducing bias learning agent allow interact intelligent agents performing similar tasks 
agents serve role models advice instructors critics supervisors general strongly bias agent learning 
simple versions methods demonstrated context reinforcement learning produced significant improvements learning time 
needed 
ffl intelligent credit assignment credit assignment fundamental problem reinforcement learning viz agent received payoff parts agent responsible generating payoff system changed improve performance 
rl algorithms solve problem making incremental changes system course trials 
take long time 
additional knowledge causal structure environment available efficient credit assignment methods developed see example idea 
ffl increased state space action space complexity date reinforcement learning problems small compared control problems facing real robotic systems 
example walking robot may require precise continuous information dozens sensors may need control dozens effectors 
combinatorics associated problems quickly overwhelm simplest rl methods 
real problems robot walking suffer kind severe temporal credit assignment problem control administered fine grain feedback learning arrives relatively course grain 
delay combined increased scale multi dimensional tasks leads tasks impractical existing techniques see promising direction 
ffl multi purpose behavior source complexity arises consider agents coordinate behavior order achieve multiple goals 
circumstances agent internal state space may increase exponentially number possible goals necessary develop methods managing explosion carefully 
see direction 
course issues stand current technology development intelligent autonomous agents reinforcement learning panacea 
autonomy afforded reinforcement learning methods play important role 
ubiquity perceptual aliasing non markov decision tasks autonomous control issues central 
intelligent control systems deal information limitations imposed sensors 
inadequate information available agent sensors agent actively control sensors order select relevant features internal decision problem faces necessarily non markov 
learning control tasks difficult traditional reinforcement learning methods typically yield poor performance 
article approaches dealing non markov decision problems 
consistent representation method proposed approach dealing tasks involve control selection active sensory system 
cr method control partitioned phases perceptual control phase aim identify current state environment overt control phase aims control state environment 
instances method lion algorithm algorithm cs ql described examples uses 
major assumption cr method state environment identified point time appropriately controlling selecting aspects sensory system 
assumption prevents applied tasks relevant state information temporarily hidden view 
tasks memory methods appropriate 
different memory architectures described window recurrent model recurrent 
window architecture uses tapped delay line maintain fixed length history sensory motor events 
recurrent model architecture constructs predictive model external environment internal state conjunction sensory inputs drive control 
architecture uses recurrent neural network learn action value function non markov task directly 
recurrent network encode state information time steps internal state resolve ambiguities caused inadequate sensory input 
architectures demonstrated series hidden state tasks conditions useful application discussed 
methods described article preliminary demonstrated relatively simple tasks extensively tested compared complicated domains 
algorithms represent significant advance traditional reinforcement learning algorithms address non markov tasks 
modest algorithms serve stepping stones sophisticated capable methods dealing ubiquitous problems hidden state 
philip agre 
dynamic structure everyday life 
phd thesis mit artificial intelligence lab 
tech report 
philip agre 
dynamic structure everyday life 
cambridge university press cambridge forthcoming 
david aha 
incremental instance learning independent graded concept descriptions 
proceedings sixth international workshop machine learning ithaca ny 
morgan kaufmann 
john isaac weiss amit bandyopadhyay 
active vision 
international journal computer vision 
bachrach 
connectionist modeling control finite state environments 
phd thesis university massachusetts department computer information sciences 
bajcsy allen 
sensing strategies 
france robotics workshop univ pennsylvania philadelphia pa november 
dana ballard 
animate vision 
technical report department computer science university rochester 
barto bradtke singh 
real time learning control asynchronous dynamic programming 
technical report university massachusetts amherst ma 
andrew barto richard sutton charles anderson 
neuron elements solve difficult learning control problems 
ieee trans 
systems man cybernetics smc 
bellman 
dynamic programming 
princeton university press princeton nj 
bertsekas 
dynamic programming deterministic stochastic models 
prentice hall 
booker 
triggered rule discovery classifier systems 
proceedings third international conference genetic algorithms june 
david chapman leslie pack kaelbling 
learning delayed reinforcement complex domain 
proceedings ijcai 
technical report tr 
chrisman 
reinforcement learning perceptual aliasing predictive distinctions approach 
proceedings tenth national conference artificial intelligence pages 
aaai press mit press 
jeffery clouse paul utgoff 
teaching method reinforcement learning 
proceedings ninth international conference machine learning 
morgan kaufmann 
elman 
finding structure time 
cognitive science 
john holland 
escaping brittleness possibilities general purpose learning applied parallel rule systems 
machine learning artificial intelligence approach 
volume ii 
morgan kaufmann san mateo ca 
leslie kaelbling 
learning embedded systems 
phd thesis stanford university 
long ji lin 
self improving reactive agents case studies reinforcement learning frameworks 
proceedings international conference simulation adaptive behavior september 
long ji lin 
programming robots reinforcement learning teaching 
proceedings ninth national conference artificial intelligence pages 
aaai press mit press 
long ji lin :10.1.1.45.8935
memory approaches reinforcement learning non markov domains 
technical report dept computer science carnegie mellon university pittsburgh pa may 
long ji lin 
self improving reactive agents reinforcement learning planning teaching 
machine learning 
long ji lin mitchell :10.1.1.45.8935
memory approaches reinforcement learning non markovian domains 
technical report cmu cs school computer science carnegie mellon university 
sridhar mahadevan jonathan connell 
automatic programming behavior robots reinforcement learning 
research report rc ibm watson research center december 
michie chambers 
boxes experiment adaptive control 
dale michie editors machine intelligence pages 
oliver boyd edinburgh 
marvin minsky 
theory neural analog reinforcement systems application brain model problem 
phd thesis princeton university 
andrew moore 
variable resolution dynamic programming efficiently learning action maps multivariate real values state spaces 
proceedings eighth international conference machine learning pages 
quinlan 
induction decision trees 
machine learning 
rick riolo 
empirical studies default sequences rules learning classifier systems 
phd thesis dept computer science engineering university michigan 
ross 
stochastic dynamic programming 
academic press new york ny 
david 
rumelhart 
hinton ronald 
williams 
learning internal representations error propagation 
parallel distributed processing explorations microstructure cognition vol 
psychological biological models mcclelland rumelhart eds pages 
mit press ma 
samuel 
studies machine learning game checkers 
feigenbaum feldman editors computers thought pages 
krieger fl 
schmidhuber 
reinforcement learning markovian non markovian environments 
touretzky editor advances neural information processing systems pages 
morgan kaufmann 
jurgen schmidhuber 
making world differentiable self supervised fully recurrent neural networks dynamic reinforcement learning planning non stationary environments 
technical report report revised technische universitat munchen 
satinder singh 
transfer learning compositions sequential tasks 
proceedings eighth international workshop machine learning pages 
morgan kaufmann 
george 
statistical methods 
iowa state university press ames iowa 
richard sutton 
temporal credit assignment reinforcement learning 
phd thesis university massachusetts amherst 
coins tech report 
richard sutton :10.1.1.132.7760
learning predict method temporal differences 
machine learning 
richard sutton 
integrating architectures learning planning reacting approximating dynamic programming 
proceedings seventh international conference machine learning austin tx 
morgan kaufmann 
ming tan 
cost sensitive reinforcement learning adaptive classification control 
proceedings ninth international conference artificial intelligence 
ming tan 
cost sensitive robot learning 
phd thesis carnegie mellon university 
tesauro 
practical issues temporal difference learning 
machine learning 
thrun moller 
planning adaptive world model 
lippmann editors advances neural information processing systems 
morgan kaufmann 
sebastian thrun 
efficient exploration reinforcement learning 
technical report cmu cs school computer science carnegie mellon university 
shimon ullman 
visual routines 
cognition 
visual cognition pinker ed 
waibel 
modular construction time delay neural networks speech recognition 
neural computation 
chris watkins 
learning delayed rewards 
phd thesis cambridge university 
christopher watkins peter dayan 
learning 
machine learning 
steven whitehead 
scaling reinforcement learning 
technical report tr computer science dept university rochester 
steven whitehead 
reinforcement learning adaptive control perception action 
phd thesis department computer science university rochester rochester ny november 
steven whitehead dana ballard 
active perception reinforcement learning 
neural computation 
proceedings seventh international conference machine learning morgan kaufmann june 
steven whitehead dana ballard 
learning perceive act trial error 
machine learning 
tech 
report department computer science university rochester 
steven whitehead dana ballard 
study cooperative mechanisms faster reinforcement learning 
tr computer science dept university rochester feburary 
steven whitehead jonas karlsson josh tenenberg 
learning multiple goal behavior task decomposition dynamic policy merging 
robot learning 
mit press cambridge ma forthcoming 
ronald williams 
reinforcement learning connectionist networks 
technical report technical report ics institute cognitive science university california 
ronald williams 
reinforcement learning connectionist systems 
technical report nu ccs college computer science northeastern university boston ma 
lambert wixson dana ballard 
learning efficient sensing sequences object search 
aaai fall symposium november 

eye movements vision 
plenum press 
richard yee sharad saxena paul utgoff andrew barto 
explaining temporal differences create useful concepts evaluating states 
proceedings ninth national conference artificial intelligence 

