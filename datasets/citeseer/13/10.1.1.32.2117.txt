ieee 
personal material permitted 
permission reprint republish material advertising promotional purposes creating new collective works resale redistribution servers lists reuse copyrighted component works obtained ieee 
ieee transactions neural networks vol 
may self organization massive document collection teuvo kohonen samuel kaski krista lagus honkela antti saarela authors neural networks research centre helsinki university technology espoo finland 
mail websom websom hut fi january draft ieee transactions neural networks vol 
may article describes implementation system able organize vast document collections textual similarities 
self organizing map som algorithm 
feature vectors documents statistical representations vocabularies 
main goal scale som algorithm able deal large amounts high dimensional data 
practical experiment mapped patent abstracts node som 
feature vectors dimensional vectors stochastic figures obtained random projections weighted word histograms 
keywords data mining exploratory data analysis knowledge discovery large databases parallel implementation random projection self organizing map som textual documents 
simple searches browsing self organized data collections locating documents basis keywords simple search expressions commonplace task nowadays 
formulating effective search queries difficult scanning lists search results apparent meaningful order may tiresome 
user friendly method data exploration exemplified called hypertext approach links provided document related data 
world wide web links created manually individual users quality linking varies greatly documents 
quality improved extensive human labor constructing organized collections yahoo hierarchical directory internet resources www yahoo com 
immense help document collections databases automatically organized meaningful way 
especially interactive exploration document collection user looks individual documents time greatly aided ordering documents contents 
context related documents residing nearby help understanding true meaning individual texts finding information highest interest 
furthermore interpreting results searches easier results grouped january draft ieee transactions neural networks vol 
may similarity content returning matches list hits 
exist possibilities organize document collection graph hierarchical structure 
article describes organization documents represented points dimensional plane geometric relations image points documents represent similarity relations 
representations called maps 
document maps add value text retrieval providing meaningful visual background results searches 
background helps making sense retrieved matches provides cues selecting interesting ones 
addition may index image points information derived document collection indexing utilized perform searches collection 
organized collections data facilitate new dimension retrieval possibility locate pieces relevant similar information user explicitly looking 
tasks methods constitute field called exploratory data analysis knowledge discovery databases colloquially called data mining 
scope exist classical methods exploratory data analysis multivariate analysis able form illustrative dimensional projections distributions items high dimensional data spaces 
multidimensional scaling mds frequently applied version called sammon projection 
large amounts data items mappings computationally heavy 
considerable interest devoted neural network methods selforganizing map som approximate unlimited number input data items finite set models 
advantage achieved som mapping say multidimensional scaling som computed representative subset old input data new input items mapped straight similar models re computation mapping 
basic projection methods organize textual data items documents contents described statistically kind metric feature vectors 
instance collection words document described january draft ieee transactions neural networks vol 
may histogram serve input feature vector basis document collection organized 
developed som methodology tool especially exploring document collections various searching tasks 
method called websom textual document collection organized graphical map display provides overview collection facilitates interactive browsing 
browsing focused locating interesting documents map content addressing 
existed attempts apply som organization texts word histograms regarded input vectors 
order avoid dimensionalities growing large vocabularies limited manually 
classify masses natural texts unavoidable refer large vocabulary say words 
exist possibilities reduce dimensionalities histogram vectors essentially losing accuracy classification 
representation histogram vectors eigenvectors latent semantic indexing described sec 
iii 
clustering words semantic categories done earlier websom publications :10.1.1.57.3859
reduction dimensionality histogram vectors random projection method done 
article describes final phases major project launched 
phases development decided summer experiment demonstrate scalability som method 
documents mapped publications increased database documents 
exist sources freely available information size 
order lead useful application decided corpus patent abstracts available cd roms electronic media 
corpus consisted japanese european patents 
interesting compare scalability method algorithms variants som 
took years group january draft ieee transactions neural networks vol 
may develop final software method possible construct alternative search methods dimension benchmarking 
system operate real time fit medium sized computers develop shortcut computing solutions obviously methods 
ii 
self organizing map self organizing map som unsupervised learning neural network method produces similarity graph input data 
consists finite set models approximate open set input data models associated nodes neurons arranged regular usually dimensional grid 
models produced learning process automatically orders dimensional grid mutual similarity 
original som algorithm recursive regression process 
regression ordered set model vectors space observation vectors recursively gamma arg min gamma kg index regression step regression performed presentation sample denoted 
scalar multiplier called neighborhood function smoothing kernel grid 
subscript defined eqn model called winner matches best 
comparison metric usually selected euclidean metrics form change accordingly 
samples stochastic continuous density function probability having multiple minima zero 
discrete valued variables multiple minima may occur cases selected random winner 
neighborhood function taken gaussian ff exp gamma kr gamma oe ff learning rate factor decreases monotonically regression steps vectorial locations display grid january draft ieee transactions neural networks vol 
may oe corresponds width neighborhood function decreasing monotonically regression steps 
practice computational reasons truncated kr gamma exceeds certain limit 
thought som algorithm derivable objective function describes average quantization error 
study shown different point density model vectors obtained 
original som algorithm computationally lightest variants 
aspect decisive large implementation 
attempt accelerate computation som batch map principle turned computationally effective 
implementation method batch map 
assuming convergence ordered state true require expectation values equal 
words stationary state fh gamma deltag means expectation value simplicity consider regarded time invariant steps iteration 
special case finite number batch respect solved write explicit solution subscript right hand side depends solve iteratively 
starting coarse approximations utilized find indices 
basis approximate values improved approximations computed applied computed substituted 
optimal solutions usually obtained small amount input data relation map size may happen models denominator zero 
corresponds left side zero applied models iteration step old value retained 
january draft ieee transactions neural networks vol 
may iteration cycles discrete valued indices settled longer changed iterations 
convergence proof slightly different batch map 
formulate batch map som principle way reduced familiar computational steps classical vector quantization smoothing numerical values dimensional grid 
particular implementation steps 
set closest model 
set called voronoi set 
number samples falling called step 
initialize proper method step 
finite set fx data samples compute step vector quantization regarded average step 
carry smoothing step ji ji alternate steps regarded stationary 
iii 
statistical models documents automatic classification documents described set features 
purpose assign documents prescribed classes selection features optimized maximum classification accuracy cf 

goal unsupervised classification classes known priori documents clustered detailed topical similarities 
demonstrated som map free text natural language documents textual contents describable statistical models word shown random choice possible 
faster convergence obtained initial values roughly ordered 
monitor classification accuracy respect major patent classes subsections order able compare different algorithms 
accuracy indirect relative measure comparing different algorithms 
january draft ieee transactions neural networks vol 
may histograms compressed forms 
series earlier works replaced word histograms histograms formed word clusters self organizing semantic maps 
system called websom 
phases explained honkela 
certain reasons expounded led abandon semantic maps encode word histograms newly developed projection methods 
results reported article totally new developments reason call system websom 
overview websom system depicted fig 

review attempts describe textual contents documents statistically 
primitive vector space model basic vector space model stored documents represented real vectors component corresponds frequency occurrence particular word document 
obviously provide different words weights information taken correspond significance power discrimination topics 
weighting word known inverse document frequency idf weighting schemes idf inverse number documents word occurs 
documents topical classification contains relevant information words weighted shannon entropy set document classes see sec 
detailed description 
method order utilize additional information contained patent classification 
weighted word histogram viewed feature vector describing document 
main problem vector space model large vocabulary sizable collection free text documents vast dimensionality model vectors kept main memory 
size vocabulary course reduced automatically manually selecting subset containing important words criterion 
difficult problem find suitable subset words january draft ieee transactions neural networks vol 
may 
construction user interface select labels automatically characterize map regions create necessary parts operation interface map images html files databases 
construction document vectors preprocess text construct document vector weighted word histogram reduce dimensionality random mapping text documents 
alternative operations user interface browsing nodes content addressable search create document vector search document way stage 
return best matching map locations keyword search visualize results indexed search map 
construction large map initialize models smallest som teach small map repeat desired map size estimate larger map smaller initialize pointers data models fine tune map parallel batch map algorithm document vectors largest map user interface fig 

overview construction operation websom system 
represents essential characteristics documents 
latent semantic indexing lsi attempt reduce dimensionality document vectors essentially losing information contained full vocabulary forms matrix column corresponds word histogram document column january draft ieee transactions neural networks vol 
may document 
space spanned column vectors decomposed ordered set factors matrix computation method called singular value decomposition svd 
decomposition property factors minimal influence matrix 
omitted document vector formed histogram remaining factors smaller dimensionality possible retained original histograms 
method called latent semantic indexing lsi 
randomly projected histograms shown earlier dimensionality document vectors reduced radically simpler method lsi random projection method essentially losing power discrimination documents :10.1.1.57.3859
experimental results prove sec 
iii table hand computation random projections orders magnitude lighter lsi discussed sec 
iii consider original document vector weighted histogram rectangular random matrix elements column assumed normally distributed vectors having unit length 
form document vectors projections rn shown pairwise similarity projection vectors measured inner products average similarity corresponding original document vectors error inversely proportional 
demonstrated experimentally dimensionalities exceeding cf 
table classification accuracy practically vector space method decreasing dimensionality document vectors time needed classify document decreased radically 
suggested random projection similar methods reducing computational complexity lsi 
january draft ieee transactions neural networks vol 
may histograms word category map original version websom reduction dimensionality document vectors carried letting words free natural text clustered neighboring grid points special som :10.1.1.57.3859
input word category map consisted triplets adjacent words text taken moving window word vocabulary represented unique random vector :10.1.1.57.3859
abandoned word category map eliminate haphazard process words categorized better accuracy document classification achieved straightforward random projection word histograms 
validation random projection method small scale preliminary experiments describing new encoding documents preliminary experimental results motivate idea 
table compares projection methods discussed model vectors case dimensional 
final implementation selected dimensionality cf 
sec 
computationally feasible preliminary experiments reported section dimensional vectors historical reasons able compare results earlier works dimensional vectors 
material smaller scale preliminary experiment patents corpus abstracts available 
patents sampled random equal number patents subsections patent classification system included 
texts preprocessed explained sec 
remaining automatically extracted vocabulary consisted words word forms 
document maps consisting units computed document collection document mapped grid points map documents represented minority class grid point counted classification errors experiment goal organize data set structure documents retrieved easily 
measured accuracies refer classification original data 
note january draft ieee transactions neural networks vol 
may table classification accuracies documents cent different projection matrices figures row averages test runs different random elements matrix 
accuracy standard deviation due different randomization vector space model gamma lsi gamma normally distributed classification accuracy cent reported row table refers classification carried classical vector space model full dimensional histograms document vectors 
practice kind classification orders magnitude slow large text collections 
dimensionality reduction lsi resulted accuracy full histograms 
reported second row 
accuracy obtained random projection factorial decomposition lsi needs computed close 
construction random projections word histograms pointers consider want simplify projection matrix order speed computations 
thresholding matrix elements sparse matrices 
experiments reported table ii 
rows meaning second row originally random matrix elements thresholded gamma third row exactly randomly distributed ones generated column elements zeros fourth row number ones fifth row number ones respectively 
results supposed give idea formation task different pattern recognition task goal classify new items learning attention paid training data 
january draft ieee transactions neural networks vol 
may table ii classification accuracies documents cent different projection matrices figures averages test runs different random elements matrix 
accuracy standard deviation due different randomization normally distributed thresholding gamma ones column ones column ones column random projection reserve memory array accumulator document vector array weighted histogram permanent address pointers locations array locations array matrix element equal form product fast pointers summing components vector indicated ones method project ready histograms pointers word text construction lowdimensional document vectors 
scanning text hash address word formed word resides hash table elements array say address pointers stored corresponding hash table location incremented weight value word 
weighted randomly projected word histogram obtained way may optionally normalized 
computing time needed form histograms way small scale experiments cent usual matrix product method 
due fact histograms projections contain plenty zero elements 
computational complexity random projection pointers january draft ieee transactions neural networks vol 
may number documents average number different words document original dimensionality 
due construction hash table complexity computing actual projections assuming hashing operation takes constant time 
contrast computational complexity lsi known ld resulting dimensionality 
estimates hold short texts sparse histogram vectors 
iv 
rapid construction large document maps som algorithm capable organizing randomly initialized map 
initialization regular closer final state asymptotic convergence map order magnitude faster 
introduce speed methods reasonable approximation initial state formed stationary state som algorithm reached effectively combination various shortcut methods 
fast distance computation word histograms plenty zeros pointer method random projection zeros predominant projected document vectors 
document vectors normalized mapped som inner products model vectors 
zero valued components vectors contribute inner products possible tabulate indices non zero components input vector consider components computing distances 
related complex methods proposed computing euclidean distances sparse vectors 
model vectors stored original high dimensional format memory capacity low dimensional models 
estimation larger maps carefully constructed smaller ones suggestions increasing number nodes som construction cf 
new idea estimate january draft ieee transactions neural networks vol 
may initial values model vectors large map basis asymptotic values model vectors smaller map 
consider rectangular dimensional som array dimensional input vectors 
probability density function pdf input selected uniform rectangular domain zero outside characteristic shrink distribution model vectors respect borders support pdf inside array model vectors assumed uniformly distributed cf fig 

arbitrary number grid points som array rectangular hexagonal amount shrinkage easily estimated 
consider input arbitrary higher dimensionality arbitrary pdf continuous smooth 
relative shrinkage relative local differences new model vectors similar uniform case cf fig 

consider pdf uniform dimensional rectangular area 
area approximated set vectors fm fm superscript refers dense lattice sparse lattice respectively 
sparse vectors lie straight line dimensional signal plane dense vector approximated linear expression ff fi gamma ff gamma fi ff fi interpolation extrapolation coefficients 
dimensional vector equation unknown scalars ff fi solved 
illustration relations codebook vectors see fig 

consider nonuniform smooth pdf space arbitrary dimensionality som lattices topology different density ideal example 
true pdf arbitrary may assume lattices true codebook vectors planar 
perform local linear estimation true codebook vectors dense lattice basis true codebook vectors sparse lattice interpolation extrapolation coefficients 
january draft ieee transactions neural networks vol 
may fig 

illustration relations model vectors sparse solid lines dense dashed lines grid 
partial grids shown 
shall interpolated terms closest sparse models respectively 
practice order linear estimate accurate respective indices codebook vectors closest signal space line 
ff fi solved node separately obtain wanted interpolation extrapolation formula ff fi gamma ff gamma fi notice indices refer topologically identical lattice points 
interpolation extrapolation coefficients dimensional lattices depend topology neighborhood function phase learning 
best results stiffness sparse dense map relative width final neighborhoods referred diameter array equal 
rapid fine tuning large maps addressing old winners 
assume middle training process som smoothly ordered asymptotically stable 
assume model vectors changed iteration training 
training input time may clear new winner vicinity old 
training vectors expressed january draft ieee transactions neural networks vol 
may linear table pointer corresponding old winner location stored training vector map unit corresponding associated pointer searched local search new winner neighborhood located unit suffice fig 

new winner location identified associated pointer input table replaced pointer new winner location 
significantly faster operation exhaustive winner search som 
search immediate surrounding said location best match edge searching continued surrounding preliminary best match winner middle units search domain 
order ensure matches globally best full search winner som performed intermittently 
new winner old training vectors pointers som winner fig 

finding new winner vicinity old old winner directly located pointer 
pointer updated 
suggested similar speedup method search tree structure 
initialization pointers size number grid nodes maps increased stepwise learning estimation procedure discussed section iv initial pointers data vectors increase estimated quickly utilizing formula increasing map size equation 
winner map unit inner january draft ieee transactions neural networks vol 
may product data vector largest inner products computed rapidly expression ff fi gamma ff gamma fi refers model vectors large map sparse map respectively 
expression interpreted inner product dimensional vectors ff fi gamma ff gamma fi irrespective dimensionality necessary winner search speeded restricting winner search area dense map corresponds neighborhood winner sparse map 
especially fast subset albeit subset covers map possible triplets allowed 
parallelized batch map algorithm batch map algorithm introduced section ii facilitates efficient parallel implementation 
iteration compute pointer best matching unit input 
old value pointer assumed close final value case pointer initialized properly obtained previous iteration relatively organized map need perform exhaustive winner search discussed 
model vectors change stage winner search easily implemented parallel dividing data different processors shared memory computer 
pointers computed previous values model vectors needed longer 
means defined computed recursive expressions nodes defined pointers associated extra memory needed keep old values computing new values 
new values model vectors computed 
computation implemented parallel done memory reserved model vectors subset new values model vectors held suitably defined buffer 
january draft ieee transactions neural networks vol 
may saving memory reducing representation accuracy memory requirements reduced significantly coarser quantization vectors 
common adaptive scale components model vector representing component bits 
dimensionality data vectors large statistical accuracy distance computations sufficient shown earlier studies 
sufficient accuracy maintained computation suitable amount noise added new value model vector quantizing 
performance evaluation new methods numerical comparison traditional som algorithm section introduced methods speeding computation large soms 
verify quality resulting maps comparable maps constructed traditional som algorithm 
smaller scale tests carried collection patent abstracts sec 
iii 
shall performance indices measure quality maps average distance input closest model vector called average quantization error separability different classes patents resulting map called classification accuracy 
classes subsections patent classification system 
computed sets maps traditional som algorithm new methods respectively compared quality 
computing sets parameter values preliminary experiments guarantee results 
model vectors maps traditionally computed set initialized values spaced evenly subspace spanned dominant eigenvectors data set 
map computed som algorithm eqns 
total number iterations map unit width height neighborhood kernel decreased rapidly slowly learning 
january draft ieee transactions neural networks vol 
may second set maps small maps consisting units computed som algorithm iterations map unit 
final large maps estimated small ones pointers winning units input sample initialized iterations batch map algorithm carried 
seen table iii quality resulting maps comparable time needed shortcut methods tenth traditional algorithm 
time measured sgi computer parallelization programs 
table iii comparison new shortcut methods traditional som algorithm 
figures averages test runs different random matrices encoding documents error margins standard deviations 
classification accuracy quantization error time traditional som sigma sigma sigma shortcut methods sigma sigma sigma comparison computational complexity large maps difference computation times marked table iii deduced computational complexities table iv 
complexity computation traditional som algorithm dn complexity full winner search dn number iterations multiple number map units guarantee sufficient statistical accuracy resulting map 
complexity new method term dm stems computation small map 
second term dn results vq step eqn 
batch map algorithm winners sought vicinity old winners described sec 
iv 
assumed search neighborhood having size independent sufficient 
term draft ieee transactions neural networks vol 
may tational complexity refers estimation pointers cf 
sec 
iv smoothing step eqn batch map computation 
initialization pointers carried time units larger map need searched input 
smoothing step average neighbors map unit computed desired keep stiffness map approximately constant number map units increased size neighborhood fraction number map units 
may estimated account speedup obtained random projection total speedup factor construction large maps order dimensionality original input vectors largest experiment table iv computational complexity methods 
denotes number data samples number map units small map dimensionality input vectors 
assumed number map units final map chosen proportional number data samples 
computational complexity traditional som dn shortcut methods dm dn document map electronic patent abstracts largest websom map far selected data base patent abstracts available electronic form written english 
patents granted european japan patent offices stored databases page database patent abstracts japan 
average length text words 
size som models neurons 
sec 
computed largest map stages 
complexity stage dn 
january draft ieee transactions neural networks vol 
may preprocessing raw patent abstracts extracted titles texts processing 
removed non textual information 
mathematical symbols numbers converted special dummy symbols 
vocabulary contained different words base forms 
words converted base form stemmer 
words occurring times corpus set common words stopword list words removed 
remaining vocabulary consisted words 
omitted abstracts words remained 
formation statistical models reduce dimensionality histograms random projection method sec 
iii 
final dimensionality selected random pointers word columns projection matrix 
words weighted shannon entropy distribution occurrence subsections patent classification system 
subsections patent classification system total examples subsections agriculture transportation chemistry building engines electricity cf 
fig 

weight measure distribution word subsections 
weights calculated follows probability randomly chosen instance word occurring subsection number subsections 
shannon entropy gamma log weight word defined max gamma max log formation document map final map constructed successively enlarged stages dimensional document vectors input 
map increased twice sixteen fold fold 
smallest unit map constructed original som algorithm learning steps 
enlarged estimated maps cf 
sec 
iv fine tuned batch map iteration cycles 
order asymptotic form map smooth regular final january draft ieee transactions neural networks vol 
may neighborhood size radius grid spacings 
choices parameter values map sizes training lengths training process 
earlier experiences experiments smaller subsets document collection 
monitored classification accuracy cf 
sec 
iv stage computation trials variations smaller maps fine tuning largest map carried took weeks 
final classification accuracy compatible results obtained smaller maps fine structures clustering manifested best largest map 
newest versions programs process computation document map takes weeks processor sgi computer 
moment provide exact values real processing time time developed programs carrying computations 
amount main memory required mb 
forming user interface automatically took additional week computation 
time includes finding keywords label map forming www pages exploring map indexing map units keyword searches 
results order get idea quality organization final map measured different subsections patent classification system separated map 
map node labeled majority subsections node abstracts belonging subsections considered misclassifications resulting accuracy purity nodes 
noted subsections overlap partially patent may subclasses belong different subsections 
result corresponded accuracies obtained different runs smaller maps computed subsets document collection 
distribution patents final map visualized fig 

january draft ieee transactions neural networks vol 
may chemistry building engines pumps electricity fig 

distribution sample subsections patent classification system document map 
gray level indicates logarithm number patents node 
exploration document map document map user series html pages enable exploration map clicking point map display mouse links document database enable reading contents documents 
map large subsets viewed zooming 
largest maps zooming levels reaching documents 
provide guidance exploration automatic method utilized selecting keywords characterize map regions 
keywords regarded kind landmarks map display serve navigation cues exploration map provide information january draft ieee transactions neural networks vol 
may topics discussed documents respective map area 
content addressable search example interface map provided form field user type query description interest form short document 
query preprocessed document vector formed exactly manner stored documents prior construction map 
resulting vector compared model vectors map units best matching points marked circles map display better match larger circle 
locations provide starting points browsing 
example utilizing content addressable search map patent abstracts shown fig 

map patent abstracts performing search takes seconds total 
keyword search example conventional keyword search mode provided finding starting points browsing 
building map word indexed map units contain word 
search description matching units index best matches returned displayed circles map seconds starting search 
example performing keyword search depicted fig 

example shows advantage visual map traditional searching 
performing search best matches contain different kinds material different aspects may relevant query 
traditional search engine matches returned list organized relevance 
map relevance information portrayed size circle symbol marking match furthermore different aspects topic may different clusters areas map 
user familiar map display may immediately help selecting interesting subset matches 
january draft ieee transactions neural networks vol 
may patent laser ablation includes laser source optical ablation system ablation region changing device 
cornea shape imaging device images desired shape optical zone cornea superimposed radiation optical ablation system 
content addressable search laser surgery cornea 
fig 

content addressable search utilized find information laser surgery cornea eye 
best matching locations marked circles 
zooming area reveals small cluster map units contains patent abstracts cornea eye surgical operations 
abstracts concerned description interest laser surgery cornea best matching units 
january draft ieee transactions neural networks vol 
may keyword search color display fig 

keyword search mode utilized find information color displays 
best matching units marked display circles size indicates goodness match 
seen map display matches distributed tight clusters different regions map 
clusters partial contents sample matching unit shown 
closer inspection units reveals different aspects color displays 
unit features considerable number abstracts color filters building lcd displays finds technology related displaying colors printing documents descriptive words lists map unit automatic keyword selection method introduced 
user probably printing mind formulating query concentrate clusters 
january draft ieee transactions neural networks vol 
may vi 
demonstrated similarity graphs large free form defective collections english texts produced contemporary computers 
similarity graphs appear especially suitable interactive data mining exploration tasks user know domain vague idea contents full text database examined 
emphasis scalability methods relating large text collections 
novel contributions article 
new application order magnitude larger previous 
new method forming statistical models documents 
new fast computing methods shortcuts 
experiments computational complexity reduced factor orders magnitude compared straightforward solution 
wish european patent office national board patents registration finland help patent collection academy finland financial support 
january draft ieee transactions neural networks vol 
may tukey exploratory data analysis addison wesley reading ma 
young householder discussion set points terms mutual distances vol 
pp 

torgerson multidimensional scaling theory method vol 
pp 

kruskal wish multidimensional scaling tech 
rep sage university series applications social sciences park ca 
de leeuw heiser theory multidimensional scaling handbook statistics krishnaiah kanal eds vol 
pp 

north holland amsterdam 
wish multidimensional scaling applications handbook statistics krishnaiah kanal eds vol 
pp 

north holland amsterdam 
young multidimensional scaling encyclopedia statistical sciences kotz johnson reads eds vol 
pp 

wiley new york 
sammon jr nonlinear mapping data structure analysis ieee transactions computers vol 
pp 

kohonen self organizing formation topologically correct feature maps biol 
cyb vol 
pp 

kohonen clustering taxonomy topological maps patterns sixth int 
conf 
pattern recognition munich germany october pp 

kohonen self organizing maps springer berlin second extended edition 
lin soergel marchionini self organizing semantic map information retrieval proc 
th 
ann 
int 
acm sigir conf 
information retrieval pp 

scholtes unsupervised learning information retrieval problem proc 
ijcnn int 
joint conf 
neural networks singapore vol 
pp 
ieee service center piscataway nj 
merkl tjoa representation semantic similarity documents maps application artificial neural network organize software libraries proc 
fid general assembly conf 
congress int 
federation information documentation 
honkela kaski lagus kohonen newsgroup exploration websom method browsing interface tech 
rep helsinki university technology laboratory computer information science espoo finland 
kaski honkela lagus kohonen creating order digital libraries self organizing maps proc 
world congress neural networks september san diego california pp 

lawrence erlbaum inns press mahwah nj 
kohonen kaski lagus honkela large level som browsing newsgroups proc 
icann int 
conf 
artificial neural networks bochum germany july von der malsburg von seelen sendhoff eds lecture notes computer science vol 
pp 

springer berlin 
lagus honkela kaski kohonen self organizing maps document collections new approach interactive exploration proc 
second int 
conf 
knowledge discovery data mining simoudis han fayyad eds pp 

aaai press menlo park california 
january draft ieee transactions neural networks vol 
may kohonen exploration large databases self organizing maps proc 
icnn int 
conf 
neural networks pp 
pl pl 
ieee service center piscataway nj 
kaski honkela lagus kohonen websom self organizing maps document collections neurocomputing vol :10.1.1.57.3859
pp 

lagus honkela kaski kohonen websom textual data mining artificial intelligence review press 
kohonen kaski lagus honkela saarela self organization massive text document collection kohonen maps oja kaski eds pp 

elsevier amsterdam 
kohonen comparison som point densities different criteria neural computation vol 
pp 

kohonen new developments learning vector quantization self organizing map symp 
neural networks alliances perspectives osaka japan int 
information institute 
cheng convergence ordering kohonen batch map neural computation vol 
pp 

gersho asymptotically optimal block quantization ieee transactions information theory vol 
pp 

gray vector quantization ieee assp magazine pp 
april 
makhoul gish vector quantization speech coding proceedings ieee vol 
pp 

kohonen kangas laaksonen som pak self organizing map program package report helsinki university technology laboratory computer information science jan 
koller sahami optimal feature selection machine learning proc 
thirteenth int 
conf 
icml saitta ed 
pp 
morgan kaufmann 
chen internet categorization search self organizing approach journal visual communication image representation vol 
pp 

murtagh neural networks information extraction astronomical information retrieval astronomy vol 
pp 

lin map displays information retrieval journal american society information science vol 
pp 

merkl text classification self organizing maps lessons learned neurocomputing vol 
pp 

chen nunamaker jr information visualization collaborative computing ieee computer pp 
august 
ritter kohonen self organizing semantic maps biol 
cyb vol 
pp 

honkela kaski lagus kohonen websom self organizing maps document collections proc 
workshop self organizing maps espoo finland june pp 

helsinki university technology neural networks research centre espoo finland 
salton mcgill modern information retrieval mcgraw hill new york 
deerwester dumais furnas landauer indexing latent semantic analysis journal american society information science vol 
pp 

january draft ieee transactions neural networks vol 
may kaski data exploration self organizing maps acta mathematics computing management engineering series march sc 
tech thesis helsinki university technology finland 
kaski dimensionality reduction random mapping fast similarity computation clustering proc 
ijcnn int 
joint conf 
neural networks vol 
pp 

ieee service center piscataway nj 
drineas frieze kannan vempala vinay clustering large graphs matrices proc 
th acm siam symposium discrete algorithms san francisco ca pp 

acm 
papadimitriou raghavan tamaki vempala latent semantic indexing probabilistic analysis proc 
seventeenth acm sigact sigmod sigart symposium principles database systems june seattle wa pp 

acm press 
kohonen self organization large document collections state art proc 
icann th int 
conf 
artificial neural networks niklasson bod en ziemke eds vol 
pp 

springer london 
chen scalable self organizing map algorithm textual classification neural network approach thesaurus generation cc ai communication cognition artificial intelligence vol 
pp 

rodrigues almeida improving learning speed topological maps patterns proc 
int 
neural networks conference dordrecht netherlands pp 
kluwer 
progress tree structured self organizing map proc 
ecai th european conf 
artificial intelligence cohn ed new york pp 
john wiley sons 
fast deterministic self organizing maps proc 
icann int 
conf 
artificial neural networks fogelman gallinari eds france vol 
ii pp 
ec 
kohonen things haven heard self organizing map proc 
icnn int 
conf 
neural networks pp 
ieee service center piscataway nj 
koskenniemi level morphology general computational model word form recognition production ph thesis university helsinki department general linguistics 
lagus kaski keyword selection method characterizing text document maps proc 
icann ninth int 
conf 
artificial neural networks vol 
pp 

iee press london 
january draft ieee transactions neural networks vol 
may dr teuvo kohonen professor academy finland affiliated helsinki university technology 
founding president european neural network society 
research area self organization introduced widely known unsupervised learning algorithm called self organizing map 
author textbooks monographs self organizing maps springer nd ed 
author edited books 
samuel kaski received sc 
tech degree computer science helsinki university technology finland 
currently professor computer science laboratory computer information science neural networks research centre helsinki university technology 
main research areas neural computation data mining 
krista lagus received sc 
degree computer science helsinki university technology finland 
research associate neural networks research centre helsinki university technology 
main research interests related neural networks especially self organizing maps application natural language processing data mining 
received sc 
degree technical physics helsinki university technology 
main research interests related neural networks emphasis self organizing maps application data mining 
research associate neural networks research centre helsinki university technology 
honkela undergraduate student university oulu finland studies computer engineering 
research done research assistant neural networks research centre helsinki university technology 
currently works oy utilizing self organizing maps textual data mining 
january draft ieee transactions neural networks vol 
may undergraduate student helsinki university technology studies information technology 
working research assistant neural networks research centre helsinki university technology 
antti saarela student helsinki university technology studies information technology 
currently doing master thesis abb industry oy neural nets image analysis defect classification 
january draft 
