machine learning 
fl kluwer academic publishers boston 
manufactured netherlands 
reinforcement learning replacing eligibility traces satinder singh singh psyche mit edu dept brain cognitive sciences massachusetts institute technology cambridge mass richard sutton rich cs umass edu dept computer science university massachusetts amherst mass received november revised may editor leslie kaelbling 
eligibility trace basic mechanisms reinforcement learning handle delayed reward 
introduce new kind eligibility trace replacing trace analyze theoretically show results faster reliable learning conventional trace 
kinds trace assign credit prior events occurred conventional trace gives greater credit repeated events 
analysis conventional replace trace versions offline td algorithm applied undiscounted absorbing markov chains 
show methods converge repeated presentations training set predictions known monte carlo methods 
analyze relative efficiency monte carlo methods 
show method corresponding conventional td biased method corresponding replace trace td unbiased 
addition show method corresponding replacing traces closely related maximum likelihood solution tasks mean squared error lower long run 
computational results confirm analyses show applicable generally 
particular show replacing traces significantly improve performance reduce parameter sensitivity mountain car task full reinforcement learning problem continuous state space feature function approximator 
keywords reinforcement learning temporal difference learning eligibility trace monte carlo method markov chain cmac 
eligibility traces fundamental mechanisms reinforcement learning handle delayed reward 
temporal difference td learning td algorithm sutton learning watkins 
td learning effect constructs internal reward signal delayed original external 
td methods eliminate delay completely fully markov problems rare practice 
problems delay remains action effective reward problems delay time td learning complete 
singh sutton general need second mechanism handle delay eliminated td learning 
second mechanism widely handling delay eligibility trace 
introduced klopf eligibility traces variety reinforcement learning systems barto sutton anderson lin tesauro peng williams 
systematic empirical studies eligibility traces conjunction td methods sutton theoretical results obtained authors dayan jaakkola jordan singh tsitsiklis dayan sejnowski sutton singh 
idea eligibility traces simple 
time state visited initiates short term memory process trace decays gradually time 
trace marks state eligible learning 
unexpectedly bad event occurs trace non zero state assigned credit accordingly 
conventional accumulating trace trace builds time state entered 
replacing trace hand time state visited trace reset regardless presence prior trace 
new trace replaces old 
see 
sutton describes conventional trace implementing credit assignment heuristics recency credit events frequency credit events occurred times 
new replacing trace seen simply discarding frequency heuristic retaining recency heuristic 
show simple change significant effect performance 
typically eligibility traces decay exponentially product decay parameter discount rate parameter fl fl 
conventional accumulating trace defined ae fle fle represents trace state time actual state time corresponding replacing trace defined times state visited conventional accumulating trace replacing trace 
accumulating replacing eligibility traces 
reinforcement learning replacing eligibility traces ae fle control problem state action pair separate trace 
state visited action taken state trace action reset traces actions reset zero see section 
problems large state space may extremely exact state recur think replacing traces irrelevant 
large problems require sort generalization states form function approximator 
states recur states features 
section show replacing traces significant difference problems large state space traces done feature feature basis state state basis 
rest structured follows 
section review td prediction algorithm prove variations accumulating replacing traces closely related monte carlo algorithms 
section main results relative efficiency monte carlo algorithms 
sections empirical return general case 

td monte carlo prediction methods prediction problem consider classical reinforcement learning optimal control 
markov chain emits transitions reward probability distribution dependent pre transition state post transition state state seek predict expected total cumulative reward emitted starting state chain reaches terminal state 
called value state function mapping states values called value function 
assume discounting fl markov chain reaches terminal state 
loss generality assume single terminal state value 
single trip starting state terminal state called trial 

td algorithms td family algorithms combine td learning eligibility traces estimate value function 
discrete state form td algorithm defined deltav ff gamma estimate time ff positive step size parameter eligibility trace state deltav increment estimate determined time value terminal state course defined singh sutton 
online td estimates incremented time step deltav 
offline td hand increments deltav set aside terminal state reached 
case estimates constant chain undergoing state transitions changes deferred trial 
third case updates deferred entire set trials 
usually done small fixed step size ff ff training set set trials convergence value estimates 
repeated presentations training paradigm rarely practice reveal telling theoretical properties algorithms 
example sutton showed td td converges conditions maximum likelihood estimate arguably best possible solution prediction problem see section 
convenience refer repeated presentations training paradigm simply batch updating 
section show batch versions conventional replace trace td methods equivalent monte carlo prediction methods 

monte carlo algorithms total reward particular visit state called return visit 
value state expected return 
suggests estimate state value simply averaging returns follow 
classically done monte carlo mc prediction methods rubinstein barto duff 
distinguish specific algorithms visit mc estimate value state average returns followed visits state 
visit mc estimate value state average returns followed visits state visit time trial state visited 
note algorithms form estimates entirely actual complete returns 
contrast td updates part existing estimates 
part td methods come closely approximate mc methods sections 
particular conventional accumulate trace version td comes approximate visit mc replace trace td comes approximate visit mc 
main points better understand difference replace accumulate versions td understanding difference mc methods 
naturally brings question focus section relative merits visit visit mc methods 
reinforcement learning replacing eligibility traces 
simple example help develop intuitions consider simple markov chain shown 
step chain stays probability goes terminate probability gamma suppose wish estimate expected number steps termination starting put form estimating value function say reward emitted step case equal expected number steps termination 
suppose data observed single trial generated markov chain trial lasted steps passing passing shown 
mc methods conclude trial 
assume methods know structure chain 
know experience shown 
visit mc method effect sees single traversal time visited traversal lasted steps estimate 
visit mc hand effect sees separate traversals steps steps steps step 
averaging effective trials visit mc estimates 
replace accumulate versions td may may form exactly estimates depending ff sequence move estimates directions 
particular corresponding offline td method starts trial estimates leave unchanged experiencing trial 
batch version td algorithms compute exactly estimates 
estimate better 
intuitively answer appears better 
trial observed took steps best estimate expected value 
event answer low 
sense point theoretical empirical analyses support intuition 
show fact answer unbiased answer answer visit mc conventional td biased statistical sense 
true process observed trial max likelihood model 
simple example markov prediction problem 
objective predict number steps termination 
singh sutton instructive compare estimates value function estimate optimal maximum likelihood sense 
data case set observed trials construct maximum likelihood model underlying markov process 
general model probability generating observed data highest 
consider simple example 
trial observed maximum likelihood estimate transition probability fraction actual transitions went way maximum likelihood estimate transition probability transitions observed estimated having probability 
maximum likelihood model markov chain shown 
define ml estimate value function value function exactly correct maximum likelihood model markov process exactly correct 
estimate equal correct answer estimate markov chain really estimate known certainty 
note ml estimate full observed data 
compute ml estimate simple example 
maximumlikelihood model chain shown exactly correct expected number time steps termination 
possible number steps compute probability occurring expected number ml pr gamma delta delta simple example ml estimate visit mc estimate 
general exactly closely related 
establish relationship general case section computing ml estimate general computationally complex 
number states maximum likelihood model markov chain requires memory computing ml estimate requires roughly computational operations 
td methods contrast memory computation step 
part computational considerations learning solutions interest ml estimate remains ideal generally unreachable practice 
ask closely various learning methods approximate ideal 
reinforcement learning replacing eligibility traces 
equivalence batch td mc methods subsection establish replace accumulate forms batch td equivalent respectively visit visit mc 
subsection proves similar equivalence offline td algorithms 
equivalence accumulate trace version batch td visit mc follows immediately prior results 
batch td gradient descent procedure known converge estimate minimum mean squared error training set sutton dayan barnard 
case discrete states minimum mse estimate state sample average returns visit state training set estimate computed visit mc 
showing equivalence replace trace batch td visit mc requires little 
theorem training set trials fixed ff ff batch replace td produces estimates visit mc 
proof considering updates estimates state need consider trials occurs 
trials occur estimates methods obviously unchanged 
index trials state occurs 
time state visited trial time terminal state reached 
represent replace td estimate value state passes training set gamma ts deltav ff gamma ts theta gamma ff gammav gamma ts ff theta gamma gamma ff ff return time trial 
turn implies singh sutton gamma ff ff theta gamma ff gamma ff gamma gamma ff ff gamma ff ff gamma gamma ff ff visit mc estimate 

equivalence offline td mc methods choice ff subsection establish replace accumulate forms offline td equivalent corresponding mc methods suitable choice step size sequence ff 
theorem offline replace td equivalent visit mc stepsize schedule ff number visits time proof considering updates estimates need consider trials occurs 
cumulative increment estimate result th trial occurs ts deltav ts ff theta gamma gamma gamma gamma gamma update offline replace td complete trial gamma gamma gamma just iterative recursive equation incrementally computing average visit returns fr reinforcement learning replacing eligibility traces theorem offline accumulate td equivalent visit mc step size schedule ff number visits entire trial containing time proof consider trials state occurs 
proof need time index visit state complicating notation somewhat 
time index th visit state trial total number visits state trial essential idea proof show offline td equation iterative recursive averaging equation time returns visit state ff step size parameter processing trial cumulative increment estimate result trial ts deltav ff gamma ts delta gamma gamma ts delta gamma delta delta delta ts ks delta gamma ff ks gamma gamma delta gamma accumulate trace estimate trial gamma ff ks gamma gamma ff ks compute sample average actual returns visit state including trial 
analytic comparison monte carlo methods previous section established close relationships replace accumulate td visit visit mc methods respectively 
better understanding difference mc methods hope better understand difference td methods 
accordingly section evaluate analytically quality solutions mc methods 
brief explore asymptotic correctness methods bias mc methods variance mean squared error mc methods relationship mc methods maximum likelihood estimate 
results section summarized table 
singh sutton 
asymptotic convergence subsection briefly establish asymptotic correctness td methods 
asymptotic convergence accumulate td general known dayan jaakkola jordan singh tsitsiklis peng 
main results appear carry replace trace case minimal modifications 
particular theorem offline online replace td converges desired value function conditions convergence offline online conventional td stated jaakkola jordan singh 
proof jaakkola jordan singh proved online offline td converges correct predictions natural conditions number trials goes infinity number time steps goes infinity online non absorbing case fl 
proof showing offline td estimator contraction mapping expected value 
show weighted sum step corrected truncated returns flr fl gamma fl better estimates expected value 
eligibility trace collects successive step estimators magnitude determines weighting 
td estimator gamma gamma gamma gamma gamma accumulating trace number time steps termination replacing trace number time steps revisit state weighted sum slightly different replace case contraction mapping expected value meets conditions jaakkola proofs convergence online offline updating 

relationship ml estimate simple example visit mc estimate ml estimate 
true general starting state assuming trials start state 
way thinking consider state just subset training trials include trials discard early part trial visited time 
consider remaining tails trials new training set 
reduced training set reinforcement learning replacing eligibility traces really mc methods see forming estimates refer ml estimate reduced training set reduced ml estimate 
subsection show reduced ml estimate equivalent general visit mc estimate 
theorem undiscounted absorbing markov chain estimates computed visit mc reduced ml estimates states trials 
proof visit mc estimate average returns visits state maximum likelihood model built partial experience rooted state sum probability making particular transition time step maximum likelihood model equal ratio number times transition number trials 
reduced ml estimate state equal visit mc estimate 
see appendix complete proof 
theorem shows equivalence visit mc reduced ml estimates 
visit mc general produces estimate different reduced ml estimate 

reduction state abstracted markov chain subsection introduce conceptual reduction arbitrary undiscounted absorbing markov chains state abstracted markov chain rest analyses mc methods 
reduction focusing state individually 
assume moment interested value state assume training trials start state loss generality change value state trial unaffected happens visit state trial 
markov chain trial produces sequences random sequence states fsg associated random sequence rewards frg 
partition sequence fsg contiguous subsequences just revisit subsequence starting th revisit denoted fsg subsequence special ends terminal state denoted fsg corresponding reward sequences similarly denoted frg frg markov property fsg independent fsg similarly frg independent frg useful means precise sequence states occurs visits play role visit mc visit mc estimates 
similarly precise sequence rewards frg matter sum rewards visits mc methods 
singh sutton purpose analysis arbitrary undiscounted markov chains reduced state chain shown lower part 
associated probabilities rewards require careful elaboration 
denote probabilities terminating looping respectively abstracted chain 
represent random rewards associated transition transition 
quantities efr ar ef gamma rt ar ef rt gamma rt analysis 
precise definitions quantities appendix 
visit mc fxg stand paired random sequence fsg frg 
visit mc estimate trial fxg fxg sk random number revisits state sum individual rewards sequence frg random total reward received visit state efr visit mc estimate trials fxg fxg fxg fxg fxg fxg fxg 
abstracted markov chain 
top typical sequence states comprising training trial 
sequence divided contiguous subsequences visits start state analyses precise sequence states rewards revisits matter 
considering value arbitrary undiscounted markov chains abstracted state chain shown lower part 
reinforcement learning replacing eligibility traces words simply average estimates sample trajectories fxg fxg fxg independent markov property 
visit mc visit mc estimate trial fxg fxg num fxg kr sk random number revisits state sequence fxg 
visit state effectively starts trial 
rewards occur th st visits state included times estimate 
visit mc estimate trials fxg fxg fxg fxg fxg fxg num fxg number revisits th trial fxg visit mc estimator visit mc estimator trials simply average estimates individual trials making analysis complex 
derive bias bias variance ar visit mc visit mc function number trials mean squared error mse bias ar 

bias results consider true value state 
bellman equation bellman pt rt vt gamma pt rt pt theorem visit mc unbiased bias gamma 
singh sutton proof visit mc estimate unbiased total reward sample path start state terminal state definition unbiased estimate expected total reward paths 
average estimates independent sample paths unbiased 
see appendix detailed proof 
theorem visit mc biased trials bias bias gamma bias pt proof see appendix 
way understanding bias visit mc estimate note method averages returns trial 
returns trial share rewards independent 
bias smaller trials observed returns different trials independent 
way understanding bias note visit mc estimate ratio random variables 
general expected value ratio ratio expected values numerator denominator 
corollary visit mc unbiased limit 

variance mse results theorem variance visit mc ar ar ar ar proof see appendix 
visit mc estimate sample average estimates derived independent trials variance goes terms variance due variance rewards third term variance due random number revisits state trial 
corollary mse visit mc mse bias ar ar pt ar reinforcement learning replacing eligibility traces theorem variance visit mc trial bounded ar gamma ar ar ar ar pt ar proof see appendix 
able obtain upper lower bounds variance mc 
single trial visit mc produces estimate closer zero estimate produced visit mc ar ar effect seen simple example visit mc estimator underestimated expected number revisits 
course low variance virtue 
example estimator returns constant independent data zero variance estimator 
greater importance low mean squared error mse corollary trial mse mse bias ar mse 
trial visit mc better visit mc terms variance mse 
eventually relative advantage reverses theorem exists ar ar 
proof basic idea proof component ar larger ar components ar fall rapidly gamma delta component ignored large see appendix complete proof 
corollary exists mse bias ar mse ar shows empirical example crossover mse 
data mc methods applied instance example task 
case crossover occurred trial 
general crossover occur early trial 
example non zero reward problem termination ar turn implies bias ar ar mse mse 
singh sutton average root mse trials visit mc visit mc 
empirical demonstration crossover mse example task shown 
transition probability 
data averages runs 

summary table summarizes results section comparing visit visit mc methods 
results unambiguously favor visit method visit method visit estimate unbiased related ml estimate 
hand mse results viewed mixed 
initially visit mc better mse visit mc 
implications unclear 
suggest seek combination estimators lowest mse 
mistake 
suspect visit estimate useful worse terms mse 
theoretical results consistent view remains speculation topic research 
table 
summary statistical results algorithm convergent unbiased short mse long mse reduced ml visit mc higher lower visit mc lower higher reinforcement learning replacing eligibility traces 
random walk experiment section empirical comparison replacing accumulating eligibility traces 
theoretical results limited case offline batch updating experiment online updating general random walk process shown 
rewards zero entering terminal states 
reward transition state transition state gamma 
discount factor fl 
initial value estimates states 
implemented online td kinds traces different values 
step size parameter held constant ff ff 
value ff values increments 
ff pair treated separate algorithm ran trials 
performance measure trial root mean squared error rmse correct predictions predictions trial states visited previous trial 
errors averaged trials separate runs obtain performance measure algorithm plotted figures 
random number generator seeded algorithms experienced exactly trials 
shows performance method function ff value kinds td method performed best intermediate value ff typically case learning algorithms 
larger value smaller ff value yielded best performance presumably eligibility trace multiplies step size parameter update equation 
critical results differences replace accumulate td methods 
replace td robust choice step size parameter accumulate td 
accumulate td unstable ff 
large accumulate td built large eligibility traces states revisited times termination 
caused large changes value estimates led instability 
summarizes data plot 
random walk process 
starting state steps taken left right equal probability state state entered terminating trial generating final non zero reward 
singh sutton average rmse accumulate traces replace traces 
performance replace accumulate td random walk task various values ff 
performance measure rmse state trial trials 
data averages runs 
average rmse best accumulate replace 
best performances accumulate replace td random walk task 
reinforcement learning replacing eligibility traces ting performance best ff best performance replace td better equal best performance accumulate td 
conclude problem studied replace td faster robust accumulate td 

mountain car experiment section describe experimental comparison replacing accumulating traces part reinforcement learning system solve control problem 
case methods learned predict value state state action pair approximate value function implemented set cmac neural networks action 
control problem moore mountain car task 
car drives mountain track shown 
objective drive past top mountain righthand side 
gravity stronger engine full thrust car accelerate steep slope 
way solve problem accelerate backwards away goal apply full thrust forwards building speed carry steep slope slowing way 
initially move away goal order reach long run 
simple example task things get worse get better 
control methodologies great difficulties tasks kind explicitly aided human designer 
reward problem gamma time steps car passed right mountain top 
passing top ends trial ends punishment 
reinforcement learning agent seeks maximize total reward prior gravity goal 
mountain car task 
force gravity stronger motor 
singh sutton termination trial 
drive goal minimum time 
time step learning agent chooses actions full thrust forward full thrust reverse thrust 
action effect gravity dependent steepness slope determines velocity position car 
complete physics mountain car task appendix reinforcement learning algorithm applied task sarsa algorithm studied rummery niranjan 
objective algorithm learn estimate action value function current policy action value gives state action expected return starting state action policy case mountain car task return simply sum reward negative number time steps goal reached 
details sarsa algorithm 
name sarsa comes quintuple actual events involved update 
algorithm closely related learning 
initially gamma actions cmac tiles 

start trial random state features greedy policy 

eligibility traces 
accumulate algorithm 
replace algorithm 
environment step take action observe resultant reward state 
choose action features terminal state greedy policy 

learn ff gamma 
loop terminal state go go 

sarsa algorithm mountain car task 
function greedy policy computes wa action returns action sum largest resolving ties randomly 
function features returns set cmac tiles corresponding state programming optimizations reduce expense iteration small multiple dependent number features typical time step 

reinforcement learning replacing eligibility traces watkins various simplified forms bucket brigade holland wilson appear 
identical td algorithm applied state action pairs states 
mountain car task continuous dimensional state space infinite number states 
apply reinforcement learning requires form function approximator 
set albus miller kraft action 
simple functions approximators repeated overlapping tilings state space produce feature representation final linear mapping 
case divided state variables position velocity car evenly spaced intervals partitioning state space regions boxes 
ninth row column added tiling offset random fraction interval leaving states uncovered 
repeated times different randomly selected offset 
example shows tilings superimposed state space 
result total theta theta boxes 
state particular time represented boxes tiling state resided 
think state representation feature vector features exactly non zero point time 
approximate action value function linear feature representation 
note representation state causes problem longer markov different nearby states produce exactly feature representation 
eligibility traces implemented feature feature basis 
corresponding feature traces action 
features treated essence states 
replace algorithms feature occurs traces reset action selected actions 
car position car velocity tiling tiling 
theta cmac tilings offset overlaid continuous dimensional state space mountain car task 
state exactly tile box feature tiling 
experiments tilings offset random fraction tile width 
singh sutton possibility course 
allow traces state action pair continue pair occurred 
keeping idea replacing traces mechanism approach chose appropriate way generalize idea visit mc control case state revisited longer matters action taken previous visit 
comparison possibilities extension 
greedy policy select actions 
initial weights set produce uniform optimistic initial estimate value state space 
see details 
applied replace accumulate sarsa algorithms task range values ff 
algorithm run trials trial passage randomly selected starting state goal 
algorithms sets random starting states 
performance measure run average trial length trials 
measure averaged runs produce results shown figures 
shows detailed results value ff summary showing best performance algorithm value 
interesting results evident data 
replace trace method performed better accumulate trace method values 
accumulate method performed particularly poorly relative trace method high values methods performance appeared best intermediate value 
results consistent random walk task previous section 
mountain car task accumulating traces best improved slightly traces worst steps trial averaged trials runs replace traces accumulate traces 
results mountain car task value ff 
data point average duration trials run averaged runs 
standard errors omitted simplify graph ranged 
reinforcement learning replacing eligibility traces average steps trial best accumulate replace 
summary results mountain car task 
value show performance best value ff 
error bars indicate standard error 
matically degraded performance 
replacing traces hand significantly improved performance longest trace lengths 
traces decay resulted significantly worse performance values tried including traces 
empirical experience needed trace mechanisms definitive drawn relative effectiveness particularly function approximators 
experiments provide significant evidence key points replace trace methods perform better conventional accumulate trace methods particularly long trace lengths long traces may help substantially best performance obtained traces infinite intermediate predictions targets actual sample returns 

variety analytical empirical evidence supporting idea replacing eligibility traces permit efficient experience reinforcement learning long term prediction 
analytical results concerned special case closely related classical studies monte carlo methods 
showed methods conventional traces biased replace trace methods unbiased 
mean squared error analysis mixed maximum likelihood analysis clearly favor replacing traces 
analytic results strongly support replace trace methods better inferences limited data conventional accumulate trace methods 
singh sutton hand analytic results concern special case quite different encountered practice 
desirable extend analyses case permit step size schedules 
analysis cases involving function approximators violations markov assumption useful steps 
empirical results treated realistic case including cases extensions listed 
results showed consistent significant large advantages replace trace methods accumulate trace methods trace methods generally trace methods 
mountain car experiment showed replace trace idea successfully conjunction feature function approximator 
clear extend replace trace idea kinds function approximators back propagation networks nearest neighbor methods sutton whitehead argued feature function approximators preferable online reinforcement learning 
empirical results showed sharp drop performance trace parameter approached corresponding long traces 
drop severe replacing traces clearly 
bears long standing question relative merits td methods versus true temporal difference methods 
appear replacing traces td methods capable competitors replace td method unbiased special case efficient conventional td theory practice 
cost losing theoretical advantages conventional td 
particular conventional td converges cases minimal error solution function approximators dayan shown useful non markov problems jaakkola singh jordan 
replace version td share theoretical guarantees 
methods appears achieve greater efficiency part relying markov property 
practice relative merits different methods may great significance 
empirical results suggest far better performance obtained function approximators create apparently non markov task 
replacing traces simple modification existing discrete state reinforcement learning algorithms 
cases state representation obtained appear offer significant improvements learning speed reliability 
acknowledgments peter dayan pointing helping correct errors proofs 
patient thorough reading participation attempts complete proofs invaluable 
course remaining errors 
tommi jaakkola providing central idea proof theorem anonymous reviewer pointing error original proof reinforcement learning replacing eligibility traces theorem lisa white pointing error 
satinder singh supported michael jordan brain cognitive sciences mit atr human information processing research siemens 
appendix proofs analytical results 
proof theorem visit mc reduced ml considering estimate assume trials start visit mc reduced ml methods ignore transitions prior number times state visited ij number times transition encountered 
jk average rewards seen transitions 
visit mc estimate trials start state jk jk identical jk jk total summed reward seen trials 
gamma rewrite jk gamma jk jk jk maximum likelihood model markov process trials transition probabilities ij ij expected rewards ij ij ml denote reduced ml estimate trials 
definition ml en fr en expectation operator maximum likelihood model trials payoff step ml jk theta rob rob rob jk jk rob probability transition th step maximum likelihood model 
show jk equal jk 
consider special cases case sk sk ss sk sm ms sk singh sutton sk ss nsm nms nm sk theta ss ss ss sk ss ij probability going state state exactly steps ss expected number revisits state current maximumlikelihood model 
case jk sj jk sm mj jk jk sj nsm nm jk theta sj sj sj jk sj sj expected number visits state sj satisfy recursions sj sj mj sj nsm nm show sj ns gamma ss ns ns gamma gamma showing quantities satisfy recursions 
sj sj nm nm gamma sj gamma gamma sj gamma gamma sj gamma gamma ss ss nms nm nm gamma ss gamma gamma nms gamma gamma gamma plugging values ss sj obtain jk jk ns gamma jk reinforcement learning replacing eligibility traces 
facts proofs theorems proofs theorems assume chain just states quantities efr ar ef gamma efr ar ef gamma rt interest analysis require careful elaboration 
set state sequences occur visits state including state head set state sequences occur final run including state 
termination probability st fsg probability sequence states product probabilities individual state transitions 
definition gamma reward probabilities defined follows qg pi fsg ss fsg fsg qg pi st fsg 
qp qg qp qg 
similarly ar qg gamma ar qg gamma rt rewards original markov chain deterministic functions state transitions single associated fsg rewards original problem stochastic set possible random associated fsg note individual rewards original markov chain deterministic ar ar greater zero stochastic different paths fact fxg fxg fxg fxg frg ff fxg number revisits state facts ir gamma gamma 
proof theorem visit mc unbiased show visit mc unbiased trial 
fxg fxg frg ff fxg pt kr rt singh sutton gamma gamma pt estimate trials sample average independent estimates unbiased trial estimate unbiased 

proof theorem visit mc biased single trial bias visit mc algorithm fxg fxg frg ft fxg kr rt rt bias gamma ps pt computing bias trials bit complex combinatorics getting revisits state trials denoted 
equivalently think number different ways factoring non negative integer additive factors order considered important gamma gamma gamma delta kn jk number different ways get kn factors order ignored note factors kn jk 
superscripts distinguish rewards different trials refers random total reward received th st visits start state second trial 
fxg ae num fxg oe kn frg jr fi fi fi fi fi kn factors kn jk rt factors kn jk reinforcement learning replacing eligibility traces factors kn jk show fact pt leads rt bias gamma ps pt proves visit mc algorithm unbiased limit 
proof define sum factorizations squared factors factors kn jk know facts principles fact gamma fact jb gamma kb fact gamma definition fact gamma gamma facts imply gamma singh sutton facts show substitution gamma solution recursion proving factors kn jk gamma factors kn jk factors kn jk 
proof theorem variance visit mc compute variance visit mc trial fxg ar fxg fxg gamma frg fxg gamma pt frg gamma delta gamma pt rt fi fi fi fi fi pt frg gamma gamma rt fi fi fi fi fi pt frg gamma gamma fi fi fi fi fi kr rt pt gamma pt gamma rt gamma gamma kr rt pt kv ar ar gamma pt ar ar term variance due variance number revisits state second term variance due random rewards 
visit mc estimate trials sample average independent trials ar ar reinforcement learning replacing eligibility traces 
proof theorem variance visit mc trial ar fxg fxg gamma eft fxg fxg fxg kr sk gamma frg ij gamma pt gamma gamma rt fi fi fi fi fi pt ar ar gamma pt ar ar note gamma gamma ar ar pt gamma ar ar ar ar 
proof theorem visit mc eventually lower variance mse central idea proof provided jaakkola personal communication visit mc estimate num fxg rewritten num fxg ef pt num fxg pt singh sutton pt easy show efp num fxg 
consider sequence functions fn ffi ffi tn ffi kn tn num fxg gamma kn pt gamma 
note ef tng ef fn 
ar ef fn gamma taylor expansion ffi ffi fi fi fi ffi 
ffi ffi fi fi fi ffi delta delta delta ar ae ffi ffi fi fi fi ffi ffi ffi fi fi fi ffi oe ae ffi ffi fi fi fi ffi delta delta delta oe gamma gamma delta prove ae fn ffi ffi fi fi fi ffi delta delta delta oe showing ef ffi ffi ffi 
term ignored ar decreases goal show large ar ar facts proof proved subsequently fact eff fact ae ffi ffi fi fi fi ffi oe ffi ffi fi fi fi ffi tn gamma kn fact ffi ffi fi fi fi ffi gamma tn kn fact ef fact ef kn tn pt gamma fact ef pt ar ar gamma reinforcement learning replacing eligibility traces pt fact gamma pt rt fact ae ffi ffi fi fi fi ffi oe npt ar ar gamma np gamma npt rt behavior variance visit mc estimate ar npt ar ar np gamma gamma comparing ar np ar ar np proves theorem 
ignoring higher order terms note ffi form ffi tn ffi kn denominator ffi ffi form ffi kn 
evaluating ffi denominator leaving numerator terms form constant 
example fact shows ffi ffi fi fi ffi tn gamma kn fact shows ffi ffi fi fi ffi gamma tn kn show ef 
tn kn sums independent mean zero random variables 
contains terms products random variables trials 
expectation term contains random variable trial exactly drops 
terms remain contain variables trials 
implies number terms survive constant function independent 
ef constant 
implies expected value terms taylor expansion corresponding singh sutton proof fact ae ffi ffi fi fi fi ffi oe ef gamma tn kn fact ef pt ar ar gamma rt pt rt pt ar ar pt rt similarly fact ef gamma tn kn gamma pt rt gamma gamma gamma gamma pt rt fact ef see ef ffi ffi fi fi ffi ps npt ar ps ar ps gammap np gamma ps npt appendix details mountain car task mountain car task continuous state variables position car velocity car start trial initial state chosen randomly uniformly allowed ranges gamma gamma 
mountain geography described altitude sin 
action takes values gamma corresponding forward thrust thrust reverse thrust 
state evolution simplified physics bound gamma cos reinforcement learning replacing eligibility traces bound gamma force gravity bound operation clips variable allowed range 
clipped way reset zero 
reward gamma time steps 
trial terminates position value exceeds 
notes 
arguably third mechanism managing delayed reward change representations world models dayan sutton 

previous sutton barto traces normalized factor gamma fl equivalent replacing equations gamma fl 
previous absorb linear normalization step size parameter ff equation 

time index assumed continue increasing trials 
example trial reaches terminal state time trial begins time 
reason estimate referred certainty equivalent estimate kumar varaiya 

theory possible get operations practical far complex applications 

algorithm identical td theoretical results td stationary prediction problems sutton dayan apply policy continually changing creating nonstationary prediction problem 

simple way assuring initial exploration state space 
values better learning system initially disappointed matter causes try variety things policy time deterministic 
approach sufficient task course advocate general solution problem assuring exploration 
albus 

brain behavior robotics chapter pages 
byte books 


computer algorithms design analysis 
reading ma addison wesley 
barnard 

temporal difference methods markov models 
ieee transactions systems man cybernetics 
barto duff 

monte carlo matrix inversion reinforcement learning 
advances neural information processing systems pages san mateo ca 
morgan kaufmann 
barto sutton anderson 

neuronlike elements solve difficult learning control problems 
ieee trans 
systems man cybernetics 
bellman 

dynamic programming 
princeton nj princeton university press 


theoretical comparison efficiencies classical methods monte carlo method computing component solution set linear algebraic equations 
meyer 
ed symposium monte carlo methods pages new york wiley 
dayan 

convergence td general machine learning 
singh sutton dayan 

improving generalization temporal difference learning successor representation 
neural computation 
dayan sejnowski 

td converges probability 
machine learning 
holland 

escaping brittleness possibilities general purpose learning algorithms applied parallel rule systems volume machine learning artificial intelligence approach chapter 
morgan kaufmann 
jaakkola jordan singh 

convergence stochastic iterative dynamic programming algorithms 
neural computation 
jaakkola singh jordan 

reinforcement learning algorithm partially observable markov decision problems 
advances neural information processing systems 
morgan kaufmann 
klopf 

brain function adaptive systems theory 
technical report air force cambridge research laboratories bedford ma 
kumar varaiya 

stochastic systems estimation identification adaptive control 
englewood cliffs prentice hall 
lin 

self improving reactive agents reinforcement learning planning teaching 
machine learning 
miller kraft 

cmac associative neural network alternative backpropagation 
proc 
ieee 
moore 

variable resolution dynamic programming efficiently learning action maps multivariate real valued state spaces 
machine learning proceedings eighth international workshop pages san mateo ca 
morgan kaufmann 
peng 

dynamic programming learning control 
phd thesis northeastern university 
peng williams 

incremental multi step learning 
machine learning proceedings eleventh international conference pages 
morgan kaufmann 
rubinstein 

simulation monte carlo method 
new york john wiley sons 
rummery niranjan 

line learning connectionist systems 
technical report cued infeng tr cambridge university engineering dept sutton 

temporal credit assignment reinforcement learning 
phd thesis university massachusetts amherst ma 
sutton 

learning predict methods temporal differences 
machine learning 
sutton 

td models modeling world mixture time scales 
machine learning proceedings twelth international conference pages 
morgan kaufmann 
sutton barto 

temporal difference model classical conditioning 
proceedings ninth annual conference cognitive science society pages hillsdale nj 
erlbaum 
sutton barto 

time derivative models conditioning 
gabriel moore 
eds learning computational neuroscience pages 
cambridge ma mit press 
sutton singh 

step size bias temporal difference learning 
eighth yale workshop adaptive learning systems pages new haven ct sutton whitehead 

online learning random representations 
machine learning proceedings tenth int 
conference pages 
morgan kaufmann 
tesauro 

practical issues temporal difference learning 
machine learning 
tsitsiklis 

asynchronous stochastic approximation learning 
machine learning 


note inversion matrices random walks 
math 
tables aids comput 
watkins 

learning delayed rewards 
phd thesis cambridge univ cambridge england 
wilson 
appear 
classifier fitness accuracy 
evolutionary computation 
