bagging predictors leo breiman technical report september partially supported nsf dms department statistics university california berkeley california bagging predictors leo breiman department statistics university california berkeley bagging predictors method generating multiple versions predictor get aggregated predictor 
aggregation averages versions predicting numerical outcome plurality vote predicting class 
multiple versions formed making bootstrap replicates learning set new learning sets 
tests real simulated data sets classification regression trees subset selection linear regression show bagging give substantial gains accuracy 
vital element instability prediction method 
perturbing learning set cause significant changes predictor constructed bagging improve accuracy 

learning set consists data ng class labels numerical response 
procedure learning set form predictor input predict 
suppose sequence sets fl consisting independent observations underlying distribution mission fl get better predictor single learning set predictor 
restriction allowed sequence predictors numerical obvious procedure replace average denotes expectation subscript denotes aggregation 
partially supported nsf dms 
predicts class jg method aggregating voting 
fk jg take argmax usually single learning set luxury replicates imitation process leading done 
take repeated bootstrap samples fl form numerical take class label vote form 
call procedure bootstrap aggregating acronym bagging 
fl form replicate data sets consisting cases drawn random replacement may appear repeated times particular fl replicate data set drawn bootstrap distribution approximating distribution underlying background bootstrapping see efron tibshirani 
critical factor bagging improve accuracy stability procedure constructing 
changes replicate produces small changes close 
improvement occur unstable procedures small change result large changes 
unstability studied breiman pointed neural nets classification regression trees subset selection linear regression unstable nearest neighbor methods stable 
unstable procedures bagging works 
section bag classification trees variety real simulated data sets 
reduction test set rates ranges 
section regression trees bagged reduction test set mean squared error data sets ranging 
section goes theoretical justification bagging attempts understand 
illustrated results section subset selection linear regression simulated data 
section gives concluding remarks 
discuss bootstrap replications useful bagging nearest neighbor classifiers bagging class probability estimates 
evidence experimental theoretical bagging push unstable procedure significant step optimality 
hand slightly degrade performance stable procedures 
literature flavor bagging 
particular averaging voting multiple trees 
buntine gave bayesian approach kwok carter voting multiple trees generated alternative splits heath voting multiple trees generated alternative oblique splits 
showed method coding class problems large number class problems increases accuracy 
commonality idea bagging 

bagging classification trees 
results bagging applied classification trees data sets waveform simulated heart breast cancer wisconsin ionosphere diabetes glass soybean heart data uci repository ftp ics uci edu pub machine learning databases 
data briefly described section 
testing done random divisions data set learning test set constructing usual tree classifier learning set bagging tree bootstrap replicates 
repeated times data set specifics section 
average test set rate single tree denoted bagging rate results table rates percent data set decrease waveform heart breast cancer ionosphere diabetes glass soybean waveform data known minimal attainable rate bayes rate 
base excess error drops 

data sets table gives summary data sets test set sizes 
table data set summary data set samples variables classes test set waveform heart breast cancer ionosphere diabetes glass soybean figures parentheses original data sets 
modified reasons described give numbers 
simulated waveform data data set randomly divided test set learning set 
instance glass data size learning set iteration gamma 
simulated waveform data learning set test set generated iteration 
brief descriptions data sets follows 
extended background available uci repository 
waveform simulated variable data cases classes having probability 
described breiman subroutine generating data uci repository subdirectory waveform 
heart data study referred opening paragraphs cart book breiman 
quote university california san diego medical center heart attack patient admitted variables measured hours 
include blood pressure age ordered binary variables summarizing medical symptoms considered important indicators patient condition 
goal medical study see chapter development method identify high risk patients survive days basis initial hour data 
data base studied olshen 
gathered project headed john ross jr elizabeth richard olshen instrumental obtaining data 
data variables 
variables high proportions missing data deleted cases missing values 
left complete cases deaths survivors 
equalize class sizes case death replicated times giving deaths total cases 
breast cancer data uci repository william wolberg university wisconsin hospitals madison see wolberg 
class data cases benign malignant 
variables consisting cellular characteristics 
subdirectory breast cancer wisconsin ionosphere radar data gathered space physics group johns hopkins university see 
cases variables consisting attributes pulse numbers 
classes type structure ionosphere bad structure 
subdirectory ionosphere diabetes data base gathered pima indians national institute diabetes kidney diseases 
see smith 
data base consists cases variables classes 
variables medical measurements patient plus age pregnancy information 
classes tested positive diabetes negative 
equalize class sizes diabetes cases duplicated giving total sample size 
subdirectory pima indians diabetes glass data base created central research establishment home office forensic science service reading 
case consists chemical measurements types glass 
cases 
soybean soybean learning set consists cases variables classes 
classes various types soybean diseases 
variables observation plants climatic variables 
categorical 
missing values filled 
subdirectory soybean soybean large data subdirectory glass 
computations runs procedure 
data set randomly divided test set learning set test sets sizes selected real data sets ad hoc chosen reasonably large 
simulated data test set size chosen comfortably large 
ii 
classification tree constructed selection done fold cross validation 
running test set tree gives rate iii 
bootstrap sample lb selected tree grown lb fold cross validation 
repeated times giving tree classifiers 
iv 
estimated class class having plurality 
proportion times estimated class differs true class bagging rate 
random division data repeated times reported averages iterations 

bagging regression trees 
results bagging trees data sets numerical responses 
boston housing ozone friedman simulated friedman simulated friedman simulated computing scheme similar classification 
learning test sets randomly selected bootstrap replications iterations 
results table mean squared test set error data set decrease boston housing ozone friedman friedman friedman 
data sets table summary data sets data set cases variables test set boston housing ozone friedman friedman friedman boston housing data known book kuh 
cases corresponding census tracts greater boston area 
variable median housing price tract 
predictor variables mainly socio economic 
data studies 
uci repository housing 
ozone ozone data consists readings maximum daily ozone hot spot los angeles basin predictor variables temperature humidity described breiman friedman subsequent studies 
eliminating variable missing values cases leaves data set complete cases variables 
friedman friedman data sets simulated data appear mars friedman 
data set independent predictor variables uniformly distributed 
response sin gamma ffl ffl 
friedman gives results model sample sizes 
sample size 
friedman examples taken simulate impedance phase shift alternating current circuit 
variable data gamma ffl tan gamma gamma ffl uniformly distributed ranges noise ffl ffl distributed oe oe oe oe selected give signal noise ratios 
example sample sizes 

computations real data sets divided random learning set test set simulated data sets learning set cases generated test set cases 
regression tree grown fold cross validation 
test set run gave mean squared error 
bootstrap replicates generated 
regression tree grown fold cross validation 
gave predictors 
predicted yb value taken av 
mean squared error yb true values procedure repeated times errors averaged give single tree error bagged error 
bagging works case independently drawn probability distribution suppose numerical predictor 
aggregated predictor take random variables having distribution independent average prediction error gamma define error aggregated predictor gamma inequality ez ez gives ey gamma ey gamma lower mean squared prediction error 
lower depends unequal sides 
effect instability clear 
change replicate sides nearly equal aggregation help 
highly variable improvement aggregation may produce 
improves 
bagged estimate distribution concentrates mass point called bootstrap approximation 
caught currents hand procedure unstable give improvement aggregation 
side procedure stable accurate data drawn 
cross point instability stability stops improving worse 
vivid illustration linear regression subset selection example section 
obvious limitation bagging 
data sets may happen close limits accuracy attainable data 
amount bagging improving 
illustrated section 
classification predictor predicts class label jg 
drawn distribution independent probability correct classification fixed jjy denote jjx pl averaged probability correct classification jjx jy jjx jjx dx px dx distribution 
arg max ijx arg max ijx jjx dx delta indicator function 
consider set fx arg max jjx arg max jjx arg max ijx jjx max jjx max jjx dx jjx dx highest attainable correct classification rate predictor arg max jjx correct classification rate max jjx dx sum jjx jjx max jjx 
px predictor far optimal 
nearly optimal 
aggregating transform predictors nearly optimal ones 
hand numerical situation poor predictors transformed worse ones 
behavior regarding stability holds 
bagging unstable classifiers usually improves 
bagging stable classifiers idea 

linear regression illustration 
forward variable selection subset selection linear regression gives illustration points previous section 
data form ng xm consists predictor variables popular prediction method consists forming predictors linear depends variables 
mg chosen designated predictor 
background see breiman spector 
common method constructing mg simulation forward variable entry 
variables xm xm fm form linear regression xm xm xm compute residual sum squares rss take xm minimizes rss linear regression xm xm 
forms variable selection best subsets backwards variants thereof 
clear unstable procedures see breiman 
variables competing inclusion mg small changes data cause large changes mg 

simulation structure simulated data section drawn model 
fi mxm ffl ffl 
number variables sample size 
drawn mean zero joint normal distribution ex ae ji iteration ae selected uniform distribution 
known subset selection nearly optimal large non zero fi performance poor small non zero fi bridge spectrum sets coefficients 
set coefficients consists clusters centered 
cluster form fi gamma jm gamma kj cluster center second third set coefficients respectively 
normalizing constant taken data 
non zero ffi mg 
non zero ffi mg non zero ffi mg relatively small 
set coefficients procedure replicated times 
data drawn model fi mxm ffl drawn joint normal distribution described 
ii 
forward entry variables done get predictors 
mean squared prediction error computed giving iii 
bootstrap replicates fl generated 
forward stepwise regression applied construct predictors averaged give bagged sequence 
prediction errors sequence computed 
computed mean squared errors averaged repetitions give sequences fe fe set coefficients sequences plotted vs 
discussion simulation results obvious best bagged predictor best subset predictor 
subset selection nearly optimal improvement 
substantial improvement 
illustrates obvious bagging improve optimal 
second point obvious 
note graphs point past bagged predictors larger prediction error 
explanation linear regression variables fairly stable procedure 
stability decreases number variables predictor decreases 
noted section stable procedure accurate 
higher values large reflect fact 
decreases instability increases cross point accurate 
concluding remarks 
bagging class probability estimates classification methods estimate probabilities jjx object prediction vector belongs class class corresponding estimated arg max jjx 
methods natural competitor bagging voting average jjx bootstrap replications getting pb jjx estimated class arg max pb jjx 
estimate computed classification example worked 
resulting rate virtually identical voting rate 
applications estimates class probabilities required classifications 
evidence far indicates bagged estimates accurate single estimates 
verify necessary compare estimates true values jjx test set 
real data true values unknown 
computed simulated waveform data reduce computing expression involving error functions 
waveform data simulation similar section learning test sets size bootstrap replications 
iteration computed average test set classes jp jjx gamma jjx jp jjx gamma jjx repeated times results averaged 
single tree estimates error 
error bagged estimates decrease 

bootstrap replicates 
experiments bootstrap replicates classification regression 
mean necessary sufficient simply reasonable 
sense fewer required numerical required increasing number classes 
answer important procedures cart running times large number bootstraps nominal 
neural nets progress slower replications may require days computing 
bagging dream procedure parallel computing 
construction predictor proceeds communication necessary cpu 
give ideas results connected number bootstrap replicates ran waveform data replicates simulation scheme section 
results table bagged rates 
bootstrap replicates rate rate clear getting improvement bootstrap replicates 
bootstrap replicates love labor lost 

bagging nearest neighbor classifiers nearest neighbor classifiers run data sets described section soybean data variables categorical 
random division learning test sets bootstrap replicates iterations run 
euclidean metric coordinate standardized dividing standard deviation learning set 
see table results table rates nearest neighbor data set waveform heart breast cancer ionosphere diabetes glass nearest neighbor accurate single trees data sets bagged trees accurate data sets 
cycles expended find bagging nearest neighbors change things 
simple computations show 
possible outcomes trial cases learning set trials probability nth outcome selected times approximately poisson distributed large probability nth outcome occur gamma 
nb bootstrap repetitions class problem test case may change classification nearest neighbor learning set bootstrap sample half nb replications 
probability probability number heads nb tosses coin probability heads nb nb gets larger probability gets small 
analogous results hold class problems 
stability nearest neighbor classification methods respect perturbations data distinguishes competitors trees neural nets 

bagging goes ways making silk purse sow ear especially sow ear 
relatively easy way improve existing method needs adding loop front selects bootstrap sample sends procedure back aggregation 
loses trees simple interpretable structure 
gains increased accuracy 
kuh 
regression diagnostics john wiley sons 
breiman 
heuristics instability model selection technical report statistics department university california berkeley 
breiman friedman olshen stone 
classification regression trees wadsworth 
breiman friedman 
estimating optimal transformations multiple regression correlation discussion journal american statistical association breiman spector submodel selection evaluation regression random case international review statistics buntine 
learning classification trees artificial intelligence frontiers statistics ed hand chapman hall london 
dietterich bakiri 
error correcting output codes general method improving multiclass inductive learning programs proceedings ninth national conference artificial intelligence aaai anaheim ca aaai press 
efron tibshirani 
bootstrap 
chapman hall 
friedman 
multivariate adaptive regression splines discussion annals statistics 
heath kasif salzberg 
dt multi tree learning method 
proceedings second international workshop multistrategy learning chambery france morgan kaufman 
kwok carter 
multiple decision trees uncertainty artificial intelligence ed 
shachter levitt kanal lemmer north holland 
olshen henning collins ross 
twelve month prognosis myocardial classification trees logistic regression stepwise linear discrimination proceedings berkeley conference honor jerzy neyman jack kiefer le cam olshen ed wadsworth 
smith dickson johannes 
adap learning algorithm forecast onset diabetes 
proceedings symposium computer applications medical care 
ieee computer society press 
wing hutton baker 
classification radar returns ionosphere neural networks 
johns hopkins apl technical digest 
wolberg mangasarian method pattern separation medical diagnosis applied breast proceedings national academy sciences volume december pp 

