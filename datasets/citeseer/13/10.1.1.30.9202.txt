machine learning proceedings fourteenth international conference nashville tn july appear 
efficient feature selection conceptual clustering mark devaney college computing georgia institute technology atlanta ga cc gatech edu ashwin ram college computing georgia institute technology atlanta ga ashwin cc gatech edu feature selection proven valuable technique supervised learning improving predictive accuracy reducing number attributes considered task 
investigate potential similar benefits unsupervised learning task conceptual clustering 
issues raised feature selection absence class labels discussed implementation sequential feature selection algorithm existing conceptual clustering system described 
additionally second implementation employs technique improving efficiency search optimal description compare performance algorithms 
choice attributes describing input crucial impact classes induced learner 
reason majority real world data sets inductive learning research constructed domain experts contain attributes expected relevant classification problem 
areas cloud data described aha bankert sufficient domain knowledge select relevant attributes exist cases data described attributes considered potentially relevant 
unfortunately presence information form additional descriptors usually generate corresponding increase performance classifier 
inductive learner treat attributes equally important presence irrelevant misleading information tend decrease effectiveness learning algorithms 
feature selection algorithms address concerns reducing number attributes describe training population order improve quality concepts produced 
algorithms search space possible input descriptions select performs best evaluation criteria 
applications feature selection supervised concept learning systems produced promising results complex real world data sets aha bankert 
describe preliminary investigation viability feature selection techniques paradigm unsupervised concept learning absence class labels compounds difficulty problem added complexity learning algorithms ideal candidate reducing size feature sets learning 
implementation fisher cobweb unsupervised concept learning system 
additionally implementation employs technique call attribute incrementation improve efficiency feature selection process 
evaluate systems terms potential improve predictive accuracy reducing descriptor size system potential reduce time required arrive reduced description sacrificing improvement predictive accuracy 
feature selection feature selection viewed search space potential descriptions training data 
process involves algorithm control exploration space potential subsets evaluation function judge quality subsets metric ultimate performance function learner evaluated aha bankert 
space feature subsets attributes size feature selection algorithms typically form search 
popular techniques hill climbing search known sequential selection may forward fss backward bss 
forward sequential selection learner begins empty descriptor set evaluates effect adding attribute time 
attribute results best performance measured evaluation function added description process repeats improvement 
similarly backward sequential selection begins full descriptor set repeatedly removes single attribute improvements performance 
complexity sequential algorithms data sets large attribute sets sequential selection algorithms require great deal processing time 
search description space guided evaluation function measures quality attribute subset 
john kohavi pfleger distinguish types evaluation functions filter models wrapper models 
wrapper models employ feedback performance function kohavi john typically classifier measuring performance feature subset classification accuracy internal testing set cross validation training data 
filter models employ measure intrinsic property data presumed affect ultimate performance classifier direct measure performance 
supervised learning wrapper models typically measure subset quality evaluating predictive accuracy class labels 
similarly filter models explicitly measure accuracy predicting class label evaluate subsets ability determine class label 
algorithms focus almuallim dietterich relief kira rendell examples methods 
ultimate performance function supervised concept learning typically accuracy learner predicting class labels previously unseen set testing instances 
traditional evaluative metrics measure efficiency concepts learned structural property concepts example 
unsupervised feature selection evidence supervised feature selection research wrapper models outperform filter models john kohavi pfleger aha bankert presumably induction algorithm evaluate feature subsets bias exists evaluation function performance function 
class labels unsupervised learning usually exist data evaluate final output system possible typical wrapper feature model approaches 
absence class labels predictive accuracy conceptual clustering usually measured average accuracy predicting values descriptors testing data 
approach feature selection conceptual clustering system employ wrapper approach average predictive accuracy attributes replacing predictive accuracy class labels 
evaluation function suggested techniques clustering algorithms 
unsupervised concept learning systems construct classes rules assign instances classes employ evaluation function guide process creating concepts 
functions supply quality measure set concepts intrinsic properties data suited evaluation function guide feature selection selection search 
taken approach employing evaluation function called category utility research gluck human category formation 
interesting aspect evaluation function blurs traditional wrapper filter model distinction wrapper model underlying learning algorithm guide descriptor search filter model evaluation function measures intrinsic property data type predictive accuracy 
category utility number conceptual clustering systems employed system fisher cobweb underlying concept learner feature selection task 
category utility cobweb cobweb conceptual clustering system represents concepts probabilistically hierarchical tree structure 
set siblings hierarchy referred partition category utility metric partition calculated ij jc gamma ij terms concepts partition attribute ij possible values attribute equation yields measure increase number attribute values predicted set concepts number attribute values predicted concepts 
term ij probability attribute value independent class membership obtained parent partition 
term weights values concept size division number concepts partition allows comparison partitions different sizes 
evaluation function useful symbolic attributes order evaluate algorithm data set containing continuous variables gennari algorithm replace innermost summations category utility equation oe ik gamma oe pi number classes partition oe ij standard deviation attribute class oe pi standard deviation attribute parent node 
cobweb constructs concept hierarchy incrementally operators 
training instance incorporated recursively processed tree operators applied determined category utility metric 
briefly partition hierarchy cobweb algorithm considers adding instance existing concept incorporate operator applying create operator construct new concept containing instance partition 
choice determined action results better category utility score partition process repeats leaf node reached create operator applied 
recursing algorithm compensates ordering effects input considering application additional operators merge split 
merge operator attempts combine nodes identified best host new training instance single concept effectively generalizing concepts partition 
split operator performs inverse operation attempting replace best host children specializing concept 
operations result increased category utility score partition best applied recursion continues partition 
implementation unsupervised feature selection algorithm generates set feature subsets forward backward direction training data runs cobweb subsets measures category utility partition children root resulting concept hierarchy retaining highest score description produced 
process repeated generating larger smaller subset current best description terminates higher category utility score obtained 
improving search efficiency feature selection potential high computational complexity large number attribute subsets may evaluated 
particularly true cobweb due expense repeatedly computing category utility metric 
immediate observation feature selection process described attribute selected addition removal concept structure reconstructed scratch new descriptor set 
cobweb generates set concepts descriptors making existing descriptor hierarchy 
attempt improve efficiency feature subset search employ technique refer concept formation 
attribute incremental learner adds removes attributes existing concept structure instances 
previous research devaney ram suggested usually efficient retain modify existing concept structure making relatively minor modifications data comprise throwing away anew scratch cobweb forced 
sequential feature selection problem ideal application technique large number small changes descriptor sets 
implemented attribute incremental concept learner cobweb referred attribute incremental concept creator created additional feature selection implementation learner 
algorithm takes input concept hierarchy new attribute add remove values attribute training instances 
single pass hierarchy partition visited recursively modified light change descriptors training data 
modification performed stages 
stage set descriptors concept node current partition modified reflect changed attribute set 
category utility score recalculated reflect change 
point algorithm tries restructuring hierarchy attempt improve category utility score 
done operators merge split cobweb different manner 
split operator repeatedly applied partition attempting split nodes partition 
splits results better category utility score performed process repeats new partition improvement 
algorithm measures effect merging possible pair nodes partition performs best repeats improvement category utility partition 
point algorithm continues partition terminates partitions visited 
algorithm shown table 
essentially performing sort hillclimbing search space possible concept representations cobweb scratch time descriptor set changes begins previous point space 
small changes representation occurs feature selection task new set concepts lies close prior able arrive new point quickly traveling far search space 
table algorithm func object root classification hierarchy update instance descriptions partition children root splits cu category utility ii node cu category utility partition resulting splitting iii cum maximum cu iv cum cu split node nm new partition cum cu merges cu category utility ii node cu ij category utility partition resulting merging nodes iii cum maximum cu ij iv cum cu merge nodes new partition 
cum cu evaluation goals initial investigation verify hypotheses predictive accuracy concepts induced cobweb improved feature selection process restrict set descriptors considered significant performance gains terms search time employing attribute incremental approach sacrificing improved predictive accuracy 
compare concept formation systems cobweb feature selection entire attribute set cobweb feature selection feature selection 
additional interesting baseline compare cobweb feature selection attributes defined relevant 
evaluations conducted real artificial data set uci machine learning database merz murphy 
real data cleveland clinic heart disease database artificial data led data set generator 
heart disease data contains total attributes including class label indicating presence heart disease integer scale 
data set interesting context feature selection research attributes considered irrelevant majority published experiments subset fourteen attributes aha 
led data contains donated medical center long beach cleveland clinic foundation robert ph algorithm subset size accuracy cobweb relevant cobweb cobweb fs fs cobweb bs bs table subset size accuracy heart disease algorithm subset size accuracy cobweb relevant cobweb cobweb fs fs cobweb bs bs table subset size accuracy led attributes binary attributes predict classes seventeen attributes random values 
data set way cross validation performed randomly selecting training sets instances testing sets instances total number available instances heart disease data generating training sets instances testing sets instances different random seeds led data 
algorithms run training data class label attribute masked concept learning final evaluation 
baseline comparison standard non feature selection version cobweb run training sets available attributes relevant attributes 
feature selection algorithms referred cobweb fs fs evaluated forward backward feature selection context 
training concepts produced algorithms evaluated measuring accuracy predicting class label previously unseen testing instances 
evaluations conducted heart disease database researchers aha accuracy measured treating output classifier binary indicator presence heart disease values treated identical 
performance task led domain predict class value 
shown tables feature selection algorithms arrived significantly reduced descriptor sets corresponding increase predictive accuracy 
focus research concerned improving computational efficiency feature selection algorithm subsets tried time seconds cobweb relevant cobweb cobweb fs fs cobweb bs bs table subsets evaluated total time heart disease algorithm subsets tried time seconds cobweb relevant cobweb cobweb fs fs cobweb bs bs table subsets evaluated total time led process 
attribute incremental approach algorithm efficiently adds removes attributes particularly useful task feature selection 
seen algorithm provides performance similar cobweb forward backward feature selection 
order measure efficiency gained approach measured amount time cpu seconds feature selection algorithms required arrive final attribute subset number subsets evaluated search 
results shown tables illustrate dramatic speedup obtained approach sequential feature selection 
research research described preliminary investigation applicability feature selection techniques unsupervised concept learning 
techniques proven valuable supervised concept learning domain unsupervised concept learning presents number additional complexities similar positive results demonstrated suggesting research order 
particular feature selection greatly reduced descriptor size improving performance respect classification accuracy 
ability crucial domains large number available attributes necessarily relevant particular classification task 
shown focusing relevant attributes increases efficiency classifier reducing number attributes considered classification gennari 
intend research claims running single user mode sun sparc workstation real artificial data sets exhibit traits feature selection designed take advantage 
area explored research paradigm attribute incremental concept formation improve performance feature selection process 
experimental evidence suggests technique greatly reduces time required search space potential descriptors sacrificing ultimate performance concepts 
complex data explored techniques improving search performance caching caruana freitag increasingly important 
empirical evaluations areas research include exploring non sequential search algorithms incorporating combination cobweb optimize efficiency performing search improving accuracy concepts obtained investigating potential applying approach conceptual clustering supervised concept learning algorithms 
authors wish doug fisher initially suggesting topic david aha suggesting heart disease database anonymous reviewers helpful feedback 
aha 

text file heart disease names location ftp ics uci edu pub machine learning databases heart disease 
aha bankert 

feature selection casebased classification cloud types empirical comparison 
aha ed 
case reasoning papers workshop 
menlo park ca aaai press 
aha bankert 

comparative evaluation sequential feature selection algorithms 
proceedings fifth international workshop artificial intelligence statistics ft lauderdale fl unpublished 
almuallim dietterich 

learning irrelevant features 
ninth national conference artificial intelligence pp 

mit press 
caruana freitag 

greedy attribute selection 
machine learning proceedings eleventh international conference san francisco ca morgan kaufmann 
devaney ram 

dynamically adjusting concepts accommodate changing contexts 
kubat widmer eds proceedings workshop learning context sensitive domains 


evaluation feature selection methods application computer security tech 
rep cse 
davis university california 
fisher 

knowledge acquisition incremental conceptual clustering 
machine learning 
gennari 

experimental study concept formation tech 
rep 
irvine university california department information computer science 
gennari 

concept formation attention 
proceedings thirteenth annual conference cognitive science society pp 

irvine ca lawrence erlbaum associates 
gluck 

information uncertainty utility categories 
proceedings seventh annual conference cognitive science society pp 

irvine ca lawrence erlbaum associates 
john kohavi pfleger 

irrelevant features subset selection problem 
machine learning proceedings eleventh international conference san francisco ca morgan kaufmann 
kira rendell 

feature selection problem traditional methods new algorithm 
tenth national conference artificial intelligence pp 

mit press 
kohavi john 
wrappers feature subset selection 
artificial intelligence journal 
forthcoming 
merz murphy 

uci repository machine learning databases www ics uci edu mlearn mlrepository html 
irvine ca university california department information computer science 
