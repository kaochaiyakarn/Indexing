sequential update bayesian network structure nir friedman university california computer science division berkeley ca nir cs berkeley edu moises goldszmidt sri international ravenswood ave ek menlo park ca moises erg sri com obvious need improving performance accuracy bayesian network new data observed 
errors model construction changes dynamics domains afford ignore information new data 
sequential update parameters fixed structure accomplished standard techniques sequential update network structure open problem 
investigate sequential update bayesian networks parameters structure expected change 
introduce new approach allows flexible manipulation tradeoff quality learned networks amount information maintained past observations 
formally describe approach including necessary modifications scoring functions learning bayesian networks evaluate effectiveness empirical study extend case missing data 
great deal effort developing methods learning bayesian networks data density estimation data analysis pattern classification see tutorial overview :10.1.1.52.2692
body includes theoretical experimental results concentrated batch learning methods 
setting total corpus data fully available learning algorithm outputs model multiple inspections data 
study problem sequential update bayesian networks 
problem different batch learning aspects learning procedure receives data read stream observations learning procedure output model observations seen far various time points possibly observation 
sequential update crucial capability building adaptable systems overcome errors initial model adapt changes underlying distribution 
especially interested sequential update situations learning procedure store past observations complete summary main memory 
examples situations include monitoring systems collect data extended periods time embedded systems limited memory capabilities 
memory constraints arise data mining applications usually involve massive amount data kept secondary storage 
applications repeated inspection data infeasible 
claim effective sequential update structure involves tradeoff quality learned networks amount information maintained past observations 
consider approaches sequential update 
approaches lie extremes spectrum 
third approach new allows flexible manipulation tradeoff 
naive approach stores previously seen data repeatedly invokes batch learning procedure new datum recorded 
approach information provided far essentially optimal terms quality networks induce 
approach requires vast amount memory store entire corpus data 
attempt avoid overhead storing previously seen data instances summarizing model learned far 
approach essentially assumes set data instances summarized distributed probability measure described current model 
approach call map reasons clear section 
unfortunately current model summary past data strongly bias learning procedure model 
result number iterations approach locks particular model stops adapting new data 
course suffices keep counts number times distinct case observed far 
impractical number distinct cases exponential number variables interest 
third approach call incremental provides middle ground extremes defined naive map approaches 
allows flexible choices tradeoff space quality induced networks 
incremental approach interleaves steps search process find models incorporation new data 
approach focuses resources keeping track just information decision search process 
basic strategy maintain set network candidates frontier process 
set contains networks deemed promising current time 
procedure keeps track required information needed evaluating candidates frontier 
shall see information maintained space efficient manner 
new data case arrives procedure updates information stored memory invokes search process check networks frontier deemed suitable current model 
assume current network frontier 
case adopts new model current model updates search frontier 
shall see amount space required procedure related size search frontier 
dynamics recording information data raises fundamental question respect scoring functions commonly evaluate different models 
scoring functions proposed literature assume alternative candidates evaluated respect training data 
incremental learning procedure start recording information different candidates different times 
propose modifications mdl score bayesian score deal complication 
empirically evaluate extended scoring functions conjunction incremental learning procedure 
examine extend methods deal incomplete data sequential update 
propose combination generalizations expectation maximization algorithm incremental em model selection em 
rest organized follows section briefly review current practice learning bayesian networks 
section describe approach incremental update develop necessary theoretical foundations 
section perform empirical evaluation methods described section introduce extension missing data 
conclude section discussion related summary main results plans 
learning bayesian networks batch method consider finite set fx xng discrete random variables variable may take values finite set denoted val 
capital letters variable names lowercase letters denote specific values taken variables 
sets variables denoted boldface capital letters assignments values variables sets denoted boldface lowercase letters val obvious way 
joint probability distribution variables subsets sets conditionally independent val val val 
bayesian network annotated directed acyclic graph encodes joint probability distribution set random variables formally bayesian network pair hg qi 
component directed acyclic graph vertices correspond random variables xn edges represent direct dependencies variables 
graph encodes set independence assumptions variable independent parents second component pair represents set parameters quantifies network 
contains parameter jpa pb pa possible value pa pa pa denotes set parents bayesian network defines unique joint probability distribution pb xn pb pa problem learning bayesian network stated follows 
training set fu un instances find network best matches common approach problem introduce scoring function score evaluates fitness networks respect training data search best network score 
main scoring functions commonly learn bayesian networks bayesian score principle minimal description length mdl :10.1.1.156.9918:10.1.1.29.6166
scores asymptotically equivalent sample size increases 
furthermore asymptotically correct probability equal learned distribution converges underlying distribution number samples increases 
mdl score described denote bde variant bayesian introduced heckerman denote bde details scores batch learning :10.1.1.29.6166
interest purposes understand information training data needed compute scores 
data complete instance assigns values variables interest scores attractive properties 
property fixed network structure closed form formula finding optimal parameters maximize score 
parameters extracted sufficient statistics structure understand notion sufficient statistics convenient introduce additional notation 
number instances vector numbers values omit superscript subscript clear context 
call vector nx sufficient statistics turns optimal choice parameters jpa function nx pa see example :10.1.1.52.2692
selection parameters closed form focus choosing best structure network 
parameters easily computed sufficient statistics 
second property decomposability score assigned structure 
means score assigned structure context dataset general form pa pa local score evaluating choice parents determined 
family set composed parents 
local score pa depends sufficient statistics family nx pa pa pa local score particular family 
direct benefit property computational 
evaluate effect score addition removal arc need recompute local score families affected 
purposes learning mdl bayesian score required information training data summarized set sufficient statistics 
statistics form nx pa choices pa set networks considered search 
sequential update bayesian networks sequential update bayesian networks line learning problem 
iteration procedure receives new data instance un produces hypothesis bn estimate perform required task prediction classification diagnosis instance un turn update network 
practice procedure generate new model number instances collected see section change spirit discussion 
update procedure evaluated cumulative loss step 
loss defined ways usually depending particular application 
example classification tasks natural measure loss number misclassified instances 
density estimation tasks intent measure procedure predicts instance log loss log pbn commonly 
start formally describing procedures sequential update define ends spectrum terms tradeoff storage requirements quality induced networks 
introduce incremental approach section 
naive map approaches naive approach sequential update consists storing observed data repeatedly invoking batch learning procedure un form estimate bn clearly procedure strategy uses observed information construct estimate consequently yield optimal results 
noted approach unreasonable space requirements long run 
needs store instances observed keep count number times distinct instantiation variables observed 
representation grows linearly number instances collected infeasible network expected perform long periods time 
example network alarm network part system monitoring intensive care patients 
example domain consists variables distinct instantiations 
clearly store counts possible instantiation observed data 
alternative approach motivated bayesian learning methodology 
recall bayesian analysis start prior probability possible hypotheses models quantifications compute posterior observations 
principle treat posterior prior iteration sequential process 
maintain belief state possible hypotheses observing un gamma receiving un compute posterior produce estimate bn store posterior current belief state 
methodology attractive property presence reasonable assumptions belief state time posterior seeing un initial prior belief state 
assumption structure network fixed conjugate priors efficiently represent posterior update iteration closed form formula 
approach infeasible attempt update structure network 
bde score assumptions allow compactly represent prior single network equivalent sample size 
unfortunately posterior compactly represented 
conjugate form prior essentially requires storing complete network equivalent storing counts possible assignments realize exact bayesian method resort approximation 
step find approximate maximum posteriori probability map network candidate 
candidate considered probable data seen far 
approximate posterior iteration map network prior network appropriate equivalent sample size 
words procedure uses network bn summary observations 
procedure space efficient need store new instances observed performed update map 
approach similar spirit proposed lam bacchus context mdl score 
unfortunately map model prior iteration learning loosing information strongly biasing learning process map model 
illustrate phenomena consider scenario consists variables case learning procedure basically choose models independent dependent suppose correlated observing say instances posterior probability model independent higher 
happen number training instances small determine observed correlations genuine correlations artifacts sampling noise 
map model represent prior iteration learning procedure 
suppose observe instances reconsider models 
stage prior strongly biased samples determined correlated update strategy map model selects evidence contained new instances weak 
phenomena pronounced equivalence sample size assigned prior grows 
incremental learning procedure section propose approach explores middle ground extremes discussed previous section 
naive approach keep possible data records equivalent representation map approach rely single network represent prior information 
basic component procedure module maintains set sufficient statistics records 
records allow update procedure select set possible networks update 
explaining approach detail introduce necessary notation 
suff denote set sufficient statistics suff nx pa ng 
similarly set sufficient statistics records nets set network structures evaluated records nets fg suff sg 
suppose choice structures established section order mdl bde score variants thereof evaluate need maintain set suff evaluate need maintain set suff 
suppose differ arc large overlap suff suff 
suff suff suff pa pa parent set easily keep track structures maintaining slightly larger set statistics 
see generalizes larger sets covers considerable subset search space recall greedy hill climbing search procedure works comparing current candidate neighbors 
neighbors networks change away arc addition deletion reversal extending argument see evaluate set neighbors maintaining bounded set sufficient statistics 
note consists sufficient statistics neighbors nets contains additional networks including networks add arcs distinct families note ae nx recovered ny nets contains networks simpler generalizing discussion 
approach applies search procedure define search frontier 
frontier consists networks compares iteration 
denote set networks 
choice determines sufficient statistics maintained memory 
set contain sufficient statistics needed evaluate networks new instance received general number new instances received procedure uses sufficient statistics evaluate select best scoring network frontier generally nets 
choice invokes search procedure determine frontier updates accordingly 
process may start recording new information may remove sufficient statistics memory 
main loop incremental procedure described follows set initial network 
initial search frontier suff suff 
forever read data un update record un mod arg maxg nets update frontier search procedure set suff suff 
compute optimal parameters output 
procedure focuses resources keeping track just information decision search space 
steps procedure performs decision 
decision procedure resources preparation iteration 
reallocation may involve removing sufficient statistics adding new ones 
instantiate procedure greedy hill climbing procedure frontier consists neighbors bn beam search hand maintain candidates set frontier neighbors candidates 
search procedures explore neighbors bn smaller search frontiers 
unresolved issue description procedure definition score 
explain section directly apply scores introduced section 
scoring functions sequential update ideally rely standard scoring functions reported literature score evaluate different structures procedure described 
unfortunately scores assumption evaluating candidates respect dataset 
assumption hold procedure 
may start collecting sufficient statistics records different families different times sufficient statistics family parent set summarize larger number instances sufficient statistics parent set effectively translates evaluation different datasets 
underlying problem general model selection problem compare models model evaluated respect training set model evaluated respect training set course problem meaningful assume sampled underlying distribution 
assumption clearly true case 
mdl bde scores inappropriate problem current form 
mdl score measures number bits required encode training data assume underlying distribution form specified model 
smaller description usually shorter regardless model problem occurs bde score 
score evaluates probability dataset assume underlying distribution form specified model 
smaller usually larger probability dataset product probabilityof instance previous ones 
term usually smaller probability decreases longer sequences 
course reset counters time start gathering new sufficient statistics record 
effect restarts learning process suffix training sequence 
discard useful information gathered earlier parts sequence 
alternatively adopt bayesian method compute 
terms degrees belief compare see candidate considered available evidence candidate 
unfortunately closed form effectively evaluate note sufficient statistics records summarize suffix sequence un subset 
normalizing factor course different 
evaluate jd jd closed form 
believe comparison models evaluated respect different related data fundamental problem requires principled solution pose open question 
intuitively want score assigns higher confidence families data 
follows proposal modifying existing mdl bde scores learning bayesian networks 
proposal satisfies basic correctness property furthermore experimental results show performs practice 
proposal best motivated mdl setting 
score casted information theoretic terms hd pa log pa hd pa empirical conditional entropy parents equal gamma pa pa log pa pa grows larger conditional entropy converges true conditional entropy underlying distribution pa gamma pa pa log pa quantity smallest number bits particular underlying distribution encode value know value pa :10.1.1.29.6166
divide get pa log pa words average encoding length instance approximates true encoding length achieved second term expression embodies redundancy optimal encoding known true distribution incurred particular choice model 
larger datasets redundancy average encoding decreases 
effect redundancy term captures amount confidence learned parameters increases confident models 
time confident models require parameters 
discussion suggests average encoding length instance measure compared data sizes different lengths 
define average mdl score mdl pa pa pa pa score measures average number bits needed represent instance values pa 
compare models different datasets score normalizes evaluation measure average effective compression rate instance 
average mdl score consistent original mdl score compare networks data 
compare models data scores choices 
see note case average mdl score assigned models original mdl score divided scores want ensured limit score choose right model 
precise 
define inherent error model respect distribution error min jjp jjp log pb cross entropy kullback leibler divergence pb standard measure distance probabilistic models statistics information theory 
show sufficient data average score prefer structures incur smaller errors 
lemma network structures evaluated respect datasets size respectively sampled underlying distribution error error go infinity mdl mdl probability 
suggest similar averaging bayesian score 
define average bde score bde pa bde pa pa pa modification motivated asymptotic equivalence bayesian score mdl score 
general result schwarz shows bde gammas mdl lemma average bayesian score asymptotically correct 
clear stage average score principled probabilistic justification 
experimental evaluation experiments reported section designed objectives mind 
show performance methods described previous section 
second evaluate tradeoff space learning procedure quality learned networks 
compared performance procedures described 
procedures take parameter number instances network structure reconsidered 
procedures ffl naive naive approach described section 
procedure stores instances seen far invokes greedy hill climbing search procedure instances select network 
ffl map map procedure described section 
procedure best network far prior update iteration consisting instances 
procedure greedy hill climbing select network 
ffl incremental procedure introduced section 
instances procedure uses greedy hill climbing search select network nets set networks evaluated currently stored statistics 
procedure uses neighbor frontier set described 
datasets experiments generated networks alarm network insurance network 
alarm networks contains variables insurance network contains variables 
network sampled training sets consisting instances 
results reported figures averages results running algorithms datasets 
procedures described update parameters quantifying current network candidate instance received 
set prior probability weak uniform prior equivalent sample size 
run tests values 
expected performance naive independent report result 
run naive mdl bde scores 
similarly run incremental modified scores introduced section 
evaluation quality induced networks log loss log pbn 
standard measure performance density estimation 
generated training sets existing networks measure close learned models generating distribution 
particular measured normalized loss log gamma log pbn log un pbn un probability generating distribution 
measure attractive properties 
relates loss incurred learning procedure optimal reached 
measures additional penalty approximation true model 
second normalized loss related cross entropy measure distance probabilistic models 
easy see cross entropy rewritten jjp bn log pbn average average normalized loss estimate cross entropy 
summarizes performance method terms average normalized loss 
expected naive better able information observed data cumulative way 
figures show significant difference mdl bde scores procedure 
anticipated section observe map normalized loss instances alarm bde naive map map map normalized loss instances insurance bde naive map map map normalized loss instances alarm bde naive incremental incremental incremental normalized loss instances insurance bde naive incremental incremental incremental normalized loss instances alarm mdl naive incremental incremental incremental normalized loss instances insurance mdl naive incremental incremental incremental performance results methods sequential update 
left column reports results alarm network right column reports results insurance network 
results top row map procedure bde score middle row incremental procedure bde score lower row incremental procedure mdl 
graphs display results naive method corresponding score point 
horizontal axis measures number instances seen 
vertical axis measures average normalized loss data point average window instances datasets 
dency lock current model prior received sufficient weight 
seen clearly results 
qualitative behavior map approach shows improvement stage equivalent sample size prior larger procedure locks particular structure performance levels 
case performance degraded stage 
unfortunately performance case falls range graphs 
course deal problem setting prior strength smaller number examples seen 
runs risk learning simple networks 
recall adopt complex networks data supports sufficiently large number samples 
currently process experimenting different strategies setting prior equivalent sample size 
described section expect incremental converge right distribution albeit slowly naive 
qualitative behavior see 
curves show scores consistent incremental procedure sound sense data observed procedure outputs networks closer performance golden model best possible sequential learning exhibit naive procedure 
expected runs larger needed time achieve level performance 
due fact procedure posts new sufficient statistics records instances needs instances start maintaining sufficiently complex statistics 
long run procedures larger value robust 
summarizes space usage procedures 
estimates optimistic count necessary data structures auxiliary ones 
naive map measure space needed store different instances 
incremental measure space needed store active sufficient statistics records 
expected naive requires memory needs store previously seen instances 
hand map requires constant amount memory stores instances time 
incremental allocated memory learned complex structure 
structure estimate stabilized memory usage procedure stabilized remained roughly constant 
memory usage incremental minimized prior knowledge limits space possible networks ordering constraints 
additionally heuristic estimates estimate arc additions promising example pairwise mutual information maintain corresponding records 
suspect scheme reduce space requirements large factor small penalty performance 
currently process experimenting extensions 
missing data discussion assumption data complete sense data instance un contains values variable unfortunately real life applications forced deal incomplete data 
source incompleteness may come noisy measurements domains attributes directly observed 
source difficulty learning incomplete data computational nature 
longer decompose probability data 
means score mdl bayesian written sum local terms measuring model probabilityof variable parents equation 
order evaluate optimal choice parameters candidate network structure perform nonlinear optimization expectation maximization em gradient descent 
focus em procedure 
standard em batch learning 
addition restricted induce parameters assumption fixed structure 
order adapt em sequential update problem need relax restrictions 
fortunately methods deal restrictions turn describe combination methods leads elegant learning procedure sequential update 
reasons space keep discussion high level refer interested reader relevant papers cited 
standard em procedure learning parameters iteratively monotonically improves current choice parameters fixed structure steps 
step current parameters computing expected value relevant sufficient statistics records suff 
computation evaluates pb nd pb sum possible completions assignments unobserved values 
second step replaced parameters estimated expected statistics 
second step essentially equivalent learning parameters complete data 
theoretical justification procedure shows proceeding manner iteration increase probability observed data sampled distribution represented structure induced parameters 
described procedure batch learning method retain training set 
neal hinton essentially show improve probability incremental update sufficient statistics 
approach new incoming data cases continuously recompute sufficient statistics 
intuitive justification approach sufficiently long sequence samples similar 
storing training data procedure relays new data 
second enhancement describe justifies updates parameters instance seen 
results procedure learning parameters storage instances alarm bde naive incremental incremental incremental map map map storage instances insurance bde naive incremental incremental incremental map map map storage instances alarm mdl naive incremental incremental incremental storage instances insurance mdl naive incremental incremental incremental space requirements methods sequential update 
upper row reports methods bde score lower methods mdl score 
left column reports results alarm network right columns reports results insurance networks 
horizontal axis measures number instances seen 
vertical axis measures average space needed store data retained procedure 
set initial network 
suff delta pb forever read data instance suff ff pb xjy update parameters new sufficient statistics 
output procedure designates confidence initial network ff decay parameter value 
usually set ff quite close 
decay parameter gradually decrease contribution old samples completed old parameters 
procedure guaranteed positive progress step average progress sufficiently long sequence procedure converge 
second restriction standard em deals learning parameters fixed structure 
friedman shows expected sufficient statistics evaluate alternative structures mdl score choose structures assigned higher score current model bound improve marginal score network respect observed data 
similar results apply bayesian score 
follows expected sufficient statistics search procedure evaluate new models 
intrinsic difficulty casting results incremental framework 
combining techniques get simple modification approach deals incomplete data set initial network 
initial search frontier suff suff 
delta pb forever read data instance ff pb xjy mod arg nets update frontier update suff suff 
compute optimal parameters output known em methods may reach sub optimal local minima 
standard techniques avoid minima example running em times different random starting points apply 
course run parallel executions procedure 
currently process evaluating effectiveness procedure 
discussion previous sequential update bayesian networks restricted updating parameters assuming fixed structure 
notable exceptions approaches buntine lam bacchus 
buntine method assumes total order variables maintains sufficient statistics possible parents node lattice structures 
imposes restrictions size lattices order bound amount information maintained 
unfortunately buntine provide reports experimental evaluations approach rigorous comparison difficult 
lam bacchus propose different approach modification mdl score 
essence current network iteration summary previously seen data 
similar spirit map approach described section 
performed experiments alarm network evaluation criteria objective sense log loss scoring resulting networks rigorous comparisons difficult 
incremental approach introduced opens new degrees flexibility various dimensions 
different search strategies different search frontiers achieved turn changes amount information maintained 
additionally actively considering different heuristics pruning number sufficient statistics stored memory 
heuristics rule statistics lead arc addition 
mentioned approach describe useful applications involve large amounts data 
applications include data mining problems monitoring problems 
note keeping set candidates readily available approach useful cases real time constraints model selection process decisions real time fashion 
topics currently exploring 
empirical evaluation methods situations underlying distribution drifts time 
goal characterize different parameters affect efficiency procedure tracking changes 
second topic involves learning presence incomplete data 
established foundations solution procedure described section 
process conducting experiments allow evaluate approach propose refinements needed 
results confirm consistency effectiveness scores introduced section principled derivation strictly probabilistic interpretation 
mentioned section pose open problem 
acknowledgments parts done nir friedman sri international 
nir friedman berkeley funded aro muri daah nsf fd 
beinlich suermondt chavez cooper 
alarm monitoring system 
euro 
conf 
ai medicine 

buntine 
theory refinement bayesian networks 
uai 
cooper herskovits 
bayesian method induction probabilistic networks data 
machine learning 
friedman 
learning belief networks presence missing values hidden variables 
ml 
friedman goldszmidt 
learning bayesian networks local structure 
uai 
geiger heckerman meek 
asymptotic model selection directed graphs hidden variables 
uai 
heckerman :10.1.1.52.2692
tutorial learning bayesian networks 
technical report msr tr microsoft research 
heckerman geiger chickering 
learning bayesian networks combination knowledge statistical data 
machine learning 
lam bacchus 
learning bayesian belief networks 
approach mdl principle 
comp 
int 
lam bacchus 
new data refine bayesian network 
uai 
lauritzen 
em algorithm graphical association models missing data 
comp 
stat 
data analysis 
neal hinton 
new view em algorithm justifies incremental variants 
unpublished manuscript 
russell binder koller kanazawa 
local learning probabilistic networks hidden variables 
ijcai 
schwarz 
estimating dimension model 
ann 
stat 
spiegelhalter lauritzen 
sequential updating conditional probabilities directed graphical structures 
networks 
