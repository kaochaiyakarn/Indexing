deterministic annealing clustering compression classification regression related optimization problems kenneth rose member ieee invited deterministic annealing approach clustering extensions demonstrated substantial performance improvement standard supervised unsupervised learning methods variety important applications including compression estimation pattern recognition classification statistical regression 
method offers important features ability avoid poor local optima applicability different structures architectures ability minimize right cost function gradients vanish case empirical classification error 
derived probabilistic framework basic information theoretic principles maximum entropy random coding 
application specific cost minimized subject constraint randomness shannon entropy solution gradually lowered 
emphasize intuition gained analogy statistical physics annealing process avoids shallow local minima specified cost limit zero temperature produces nonrandom hard solution 
alternatively method derived rate distortion theory annealing process equivalent computation shannon rate distortion function annealing temperature inversely proportional slope curve 
provides new insights method performance new insights rate distortion theory 
basic algorithm extended incorporating structural constraints allow optimization numerous popular structures including vector quantizers decision trees multilayer perceptrons radial basis functions mixtures experts 
experimental results show considerable performance gains standard structure specific application specific training methods 
concludes brief discussion extensions method currently investigation 
manuscript received november revised april 
supported part national science foundation ncr university california micro program act networks advanced computer communications cisco systems dsp group dsp software engineering fujitsu laboratories america general electric hughes electronics intel nokia mobile phone qualcomm rockwell international texas instruments author department electrical computer engineering university california santa barbara ca usa 
publisher item identifier 
keywords classification clustering compression deterministic annealing maximum entropy optimization methods regression vector quantization 
ieee ways motivate introduce material described 
place neural network perspective particularly learning 
area neural networks greatly benefited unique position crossroads diverse scientific engineering disciplines including statistics probability theory physics biology control signal processing information theory complexity theory psychology see 
neural networks provided fertile soil infusion occasionally confusion ideas meeting ground comparing viewpoints sharing tools approaches 
ill defined boundaries field neural networks researchers traditionally distant fields come realization attacking fundamentally similar optimization problems 
concerned basic optimization problem important variants derivative problems 
starting point problem clustering consists optimal grouping observed signal samples training set purpose designing signal processing system 
solve clustering problem seeks partition training set space defined minimizes prescribed cost function average cluster variance 
main applications clustering pattern recognition signal compression 
training samples unknown source application objective characterize underlying statistical structure identify components mixture case quantizer designed unknown source 
describes deterministic annealing da approach proceedings ieee vol 
november clustering extension appropriate constraints clustering solution attack large important set optimization problems 
clustering belongs category unsupervised learning problems training access input samples system design 
desired system output available 
complementary category supervised learning involves teacher provides training phase desired output input sample 
training system expected emulate teacher 
important supervised learning problems viewed problems grouping partitioning fall broad class cover 
include particular problems classification regression 
shall see methods described applicable certain problems strictly speaking involve partitioning 
design practical system take account complexity 
general restrict complexity allowed partitions 
typically done imposing particular structure implementing partition 
allowing arbitrary partition training set input space require partition determined prescribed parametric function complexity determined number parameters 
example vector quantizer vq structure implements voronoi nearest neighbor partition space complexity may measured number codevectors prototypes 
example partition obtained multilayer perceptron complexity determined number neurons synaptic weights 
evident design method normally specific structure case known techniques 
approach describe applicable large diverse set structures problems 
instructive simplest nontrivial problem instance order obtain unobstructed insight essentials 
start problem clustering quantizer design seek optimal partition prescribed number subsets minimizes average cluster variance mean squared error mse 
case need impose structural constraint 
voronoi partition optimal naturally emerges solution 
structurally constrained clustering interest wants impose different structure solution 
example tree structured vq lower quantizer complexity required 
having explicitly impose structure significant simplification problem easy 
documented basic clustering suffers poor local minima riddle cost surface 
variety heuristic approaches proposed tackle difficulty range repeated optimization different initialization heuristics obtain initialization heuristic rules cluster splits merges approach stochastic gradient techniques particularly conjunction self organizing feature maps 
substantial margin gains methodical principled attack problem demonstrated clustering classification regression related problems 
observation annealing processes physical chemistry motivated similar concepts avoid local minima optimization cost 
certain chemical systems driven low energy states annealing gradual reduction temperature spending long time vicinity phase transition points 
corresponding probabilistic framework gibbs distribution defined set possible configurations assigns higher probability configurations lower energy 
distribution parameterized temperature temperature lowered discriminating concentrating probability smaller subset low energy configurations 
limit low temperature assigns nonzero probability global minimum configurations 
known technique nonconvex optimization capitalizes physical analogy stochastic relaxation simulated annealing metropolis algorithm atomic simulations 
sequence random moves generated random decision accept move depends cost resulting configuration relative current state 
careful annealing schedule rate temperature lowered 
image restoration geman geman shown theory global minimum achieved schedule obeys number current iteration see derivation necessary sufficient conditions asymptotic convergence simulated annealing 
schedules realistic applications 
shown perturbations infinite variance cauchy distribution provide better ability escape minima allow principle faster schedules 
name suggests da tries enjoy best worlds 
hand deterministic meaning want wandering randomly energy surface making incremental progress average case stochastic relaxation 
hand annealing method aims global minimum getting greedily attracted nearby local minimum 
view da replacing stochastic simulations expectation 
effective energy function parameterized pseudo temperature derived expectation deterministically optimized successively reduced temperatures 
approach adopted various researchers fields graph theoretic optimization computer vision 
starting point early clustering deterministic annealing appeared 
strongly motivated physical analogy approach formally rose deterministic annealing principles information theory probability theory consists minimizing clustering cost prescribed levels randomness shannon entropy 
da method provides clustering solutions different scales scale directly related temperature parameter 
phase transitions design process phases correspond number effective clusters solution grows splits temperature lowered 
limitation number clusters imposed zero temperature hard clustering solution quantizer obtained 
basic da approach clustering inspired modifications extensions related numerous researchers including 
begins tutorial review basic da approach clustering goes significant extensions handle various partition structures hard supervised learning problems including classifier design piecewise regression mixture experts 
important theoretical aspect connection shannon rate distortion rd theory leads better understanding method contribution quantization yields additional contributions information theory 
currently investigated extensions notably hidden markov models speech recognition briefly discussed 
ii 
deterministic annealing unsupervised learning clustering clustering informally stated partitioning set data points subgroups homogeneous possible 
problem clustering important optimization problem large variety fields pattern recognition learning source coding image signal processing 
exact definition clustering problem differs slightly field field major tool analysis processing data priori knowledge distribution 
clustering problem statement usually mathematically precise defining cost criterion minimized 
signal compression commonly referred distortion 
denote source vector denote best reproduction codevector codebook denoting distortion measure typically necessarily squared euclidean distance expected distortion right hand side assumes source distribution may approximated training set independent vectors 
case clustering solution approximation expected distortion empirical distortion practically unavoidable 
sequel approximation obvious context repeating explicit statement 
specified terms codebook encoding rule selecting codevector best matches input vector 
virtually useful distortion functions convex poor local minima 
clustering nonconvex optimization problem 
exhaustive search find global minimum hopelessly impractical nontrivial distributions reasonably large data sets 
clustering problem appears diverse applications solution methods developed different disciplines 
communications information theory literature early clustering method suggested scalar quantization known lloyd algorithm max quantizer 
method generalized vector quantization large family distortion measures resulting algorithm commonly referred generalized lloyd algorithm gla 
comprehensive treatment subject areas compression communications see 
pattern recognition literature similar algorithms introduced including isodata means algorithms 
fuzzy relatives algorithms derived 
iterative methods alternate complementary steps optimization encoding rule current codebook optimization codebook encoding rule 
operating batch mode cost due entire training set considered adjusting parameters easy show iterative procedure monotone nonincreasing distortion 
convergence local minimum distortion fuzzy variant respectively ensured 
principled derivation deterministic annealing various earlier versions principled derivation da appeared 
derivation revised include insights provide natural foundation sections 
probabilistic framework clustering defined randomization partition equivalently randomization encoding rule 
input vectors assigned clusters probability call association probability 
viewpoint bears similarity fuzzy clustering data point partial membership clusters 
formulation purely probabilistic 
consider clusters regular sets exact membership outcome random experiment may consider fuzzy sets obtained equating degree membership association probability probabilistic model 
possible utilize da fuzzy regular clustering design 
tools methods fuzzy sets theory 
hand traditional framework clustering marginal special case association probabilities zero 
pattern recognition literature called hard clustering soft fuzzy clustering 
proceedings ieee vol 
november randomized partition rewrite expected distortion joint probability distribution conditional probability association probability relating input vector codevector limit association probabilities hard input vector assigned unique codevector probability identical traditional hard clustering distortion 
minimization respect free parameters immediately produce hard clustering solution advantageous fully assign input vector nearest codevector 
recast optimization problem seeking distribution minimizes subject specified level randomness 
level randomness naturally measured shannon entropy optimization conveniently reformulated minimization lagrangian lagrange multiplier 
clearly large values mainly attempt maximize entropy 
lowered trade entropy reduction distortion approaches zero minimize directly obtain hard nonrandom solution 
point instructive pause consider equivalent derivation principle maximum entropy 
suppose fix level expected distortion seek estimate underlying probability distribution 
objective characterize random solution gradually diminishing levels distortion minimal distortion reached 
estimate distribution appeal jaynes maximum entropy principle states probability distributions satisfy set constraints choose maximizes entropy 
informal justification choice agrees known constraints maintains maximum uncertainty respect 
chosen distribution satisfying constraints reduced uncertainty implicitly extra restrictive assumption 
problem hand seek distribution maximizes shannon entropy satisfying expected distortion constraint 
corresponding lagrangian maximize lagrange multiplier 
term nearest sense distortion measure necessarily euclidean distance 
equivalence derivation obvious simultaneously optimized solution configuration analyze lagrangian note joint entropy decomposed terms source entropy independent clustering 
may drop constant lagrangian definition focus conditional entropy minimizing respect association probabilities straightforward gives gibbs distribution normalization partition function statistical physics 
corresponding minimum obtained plugging back minimize lagrangian respect codevector locations gradients set zero yielding condition note derivative notation stands general gradients 
normalization condition rewritten centroid condition denotes posterior probability calculated bayes rule squared error distortion case takes familiar form expressions convey clearly centroid aspect result practical approximation general condition gibbs distribution 
rose deterministic annealing practical algorithm consists minimizing respect codevectors starting high value tracking minimum lowering central iteration consists steps fix codevectors compute association probabilities fix associations optimize codevectors 
clearly procedure monotone nonincreasing converges minimum 
high levels cost smooth mild assumptions shown convex implies global minimum 
tends zero association probabilities hard hard clustering solution obtained 
particular easy see algorithm known gla method limit 
intuitive notion workings system obtained observing evolution association probabilities 
infinite uniform distributions input vector equally associated clusters 
extremely fuzzy associations 
lowered distributions discriminating associations fuzzy 
zero temperature classification hard input sample assigned nearest codevector probability 
condition traditional techniques gla 
da viewpoint standard methods zero temperature methods 
easy visualize zero temperature system sense better optimum farther away data point exercises influence nearest codevector 
hand starting high slowly cooling system start data point equally influencing codevectors gradually localize influence 
gives intuition system senses settles better optimum 
important aspect algorithm seen view association probability expected value random binary variable take value input assigned codevector zero 
perspective may recognize known expectation maximization em algorithm step iteration 
step computes association probabilities expectation step second step minimizes maximization step 
note em algorithm applied level emergence em surprising choices distortion measure interpretation negative likelihood function 
example case squared error distortion optimization equivalent maximum likelihood estimation means normal mixture assumed variance determined important note general differentiable convex function unique minimum asymptotically high temperature precisely input sample uniformly associated set equidistant nearest representatives 
ignore pathologies encoding ties significance da 
necessarily assume underlying probabilistic model data 
distributions derived distortion measure 
compression applications particular distortion measure attempts quantify perceptual significance reconstruction error independent source statistics 
statistical physics analogy probabilistic derivation largely motivated analogies statistical physics 
section develop analogy indicate precisely method produces annealing process 
demonstrate system undergoes sequence phase transitions obtain insights process 
consider physical system energy distortion shannon entropy lagrangian central da derivation exactly helmholtz free energy system strictly speaking helmholtz thermodynamic potential 
lagrange multiplier accordingly temperature system governs level randomness 
note choice notation exception stands distortion agree traditional notation statistical mechanics emphasizes direct analogy 
fundamental principle statistical mechanics called principle minimal free energy states minimum free energy determines distribution thermal equilibrium 
achieved system reaches equilibrium point system governed gibbs canonical distribution 
chemical procedure annealing consists maintaining system thermal equilibrium carefully lowering temperature 
compare computational procedure da track minimum free energy gradually lowering temperature 
chemistry annealing ensure ground state system state minimum energy achieved limit low temperature 
method simulated annealing directly simulates stochastic evolution physical system 
derive free energy corresponding expectation deterministically quickly optimize characterize equilibrium temperature 
summary da method performs annealing maintains free energy minimum thermal equilibrium gradually lowering temperature deterministic minimizes free energy directly stochastic simulation system dynamics 
physical analogy 
shall demonstrate temperature lowered system undergoes sequence phase transitions consists natural cluster splits clustering model grows size number clusters 
phenomenon highly significant number reasons 
provides useful tool controlling size clustering model relating scale solution explained 
second phase transitions critical points process needs careful proceedings ieee vol 
november annealing case physical annealing 
critical temperatures computable shown 
information allows accelerate procedure phase transitions compromising performance 
sequence solutions various phases solutions increasing model size coupled validation procedures identify optimal model size performance outside training set 
considering case high temperature association probabilities uniform optimality condition satisfied placing codevectors point centroid training set determined case squared error distortion optimal sample mean training set 
high temperature codebook collapses single point 
say effectively codevector cluster entire training set 
lower temperature cardinality codebook changes 
consider effective codebook cardinality model size characterizing phases physical system 
system undergoes phase transitions model size grows 
analysis phase transitions fundamental obtaining understanding evolution system 
order explicitly derive critical temperatures phase transitions assume squared error distortion bifurcation occurs set coincident codevectors splits separate subsets 
mathematically existing solution critical temperature longer minimum free energy temperature crosses critical value 
natural define point hessian loses positive definite property notational complexity working large complex matrix motivates equivalent approach variational calculus 
denote perturbed codebook perturbation vector applied codevector nonnegative scalar scale magnitude perturbation 
rewrite necessary condition optimality choices finite perturbation variational statement optimality condition leads directly earlier condition 
require condition second order derivative choices finite perturbation bifurcation occurs equality achieved minimum longer stable 
applying straightforward differentiation obtain condition equality denotes identity matrix 
term rewritten compactly equation covariance matrix posterior distribution cluster corresponding codevector claim left hand side positive perturbations term positive 
part trivial second term obviously nonnegative 
show part observe term nonpositive exists positive probability matrix positive definite 
fact assume coincident codevectors point allow bifurcation 
show case exists perturbation second term vanish 
select perturbation satisfying perturbation second term equals zero 
term nonpositive construct perturbation second term vanishes 
strict inequality term positive choices finite perturbation derivation condition phase transition requires coincident codevectors simplicity ignore higher order derivatives checked mathematical completeness minimal practical importance 
result necessary condition bifurcation 
rose deterministic annealing posterior data distribution satisfying critical temperature determined largest eigenvalue words phase transitions occur temperature lowered twice variance principal axis cluster 
shown split separation codevectors principal axis 
summarize result form theorem 
theorem squared error distortion measure cluster centered codevector undergoes splitting phase transition temperature reaches critical value cluster principal component 
fig 
illustrates annealing process phase transitions simple example 
training set generated mixture randomly displaced equal variance gaussians centers marked high temperature effective cluster represented codevector marked center mass training set 
temperature lowered system undergoes phase transitions increase number effective clusters shown 
note partition random precise boundaries 
give curves contours equal probability typically belonging cluster 
fig 
give corresponding phase diagram describes variation average distortion energy note particular temperature reaches value corresponds variance isotropic gaussians get explosion preferred direction principal component split 
analysis phase transitions section rd theory deeper treatment condition explosion continuum codevectors special role gaussian distribution see 
mass constrained clustering mass constrained clustering approach preferred implementation da clustering algorithm detailed sketch algorithm section 
shall show annealing process described far certain dependence number coincident codevectors effective cluster 
weakness desirable eliminated leading method totally independent initialization 
start recalling central characteristic da approach matter codevectors thrown effective number emerges temperature 
number model size defines phase system 
example thousands codevectors single effective codevector high temperature 
split occurs result fig 

clustering various phases 
lines equiprobable contours 
cluster ah clusters clusters clusters clusters clusters clusters 

may differ somewhat depending number codevectors resulting subgroups 
clearly initial partition subgroups depends perturbation 
order fix shortcoming reformulate method proceedings ieee vol 
november fig 

phase diagram distribution shown fig 

number effective clusters shown phase 

terms effective clusters distinct codevectors 
assume unlimited supply codevectors denote fraction codevectors represent effective cluster coincident position notation partition function rewritten equivalently summation distinct codevectors 
probability association distinct codevector called tilted distribution free energy free energy minimized obvious constraint optimization performed unconstrained minimization lagrangian respect cluster parameters note started countable number codevectors distributed clusters effectively view possibly uncountable required rational 
may visualize mass codevectors divided effective clusters simply distribution codevector space 
notion inducing possibly continuous distribution codevector space provides direct link rate distortion theory pursued section 
optimal set codevectors satisfy left equality constraint independent positions get condition important distinction association probabilities tilted 
hand set minimizes satisfies yields expectation respect distribution obtain equality uses definition 
substituting see optimal distribution satisfy implicit 
equation equation solve optimizing equation arises kuhn tucker conditions rate distortion theory 
instructive point imply words optimal codevector distribution mimics training data set partition clusters 
distribution identical probability distribution induced codevectors encoding rule denoted aside note results perfect agreement estimation priors mixing coefficients parametric estimation mixture densities 
kept mind mass constrained algorithm applicable solving rose deterministic annealing simple vq clustering design problem minimum distortion ultimate objective 
mass constrained formulation needs codevectors effective clusters temperature 
process computationally efficient increases model size needed critical temperature reached 
mechanism implemented maintaining perturbing pairs codevectors effective cluster separate phase transition occurs 
possibility compute critical temperature supply additional codevector condition satisfied 
noted limit low temperature unconstrained da method mass constrained da method converge descent process gla basic isodata sum squared distances 
association probabilities da methods identical limit assign data point nearest codevector probability 
difference behavior intermediate mass constrained clustering method takes cluster populations account 
illustrated simple example fig 

preferred implementation da clustering algorithm conclude treatment basic clustering problem sketch preferred implementation clustering algorithm 
version incorporates mass constrained approach 
squared error distortion assumed simplicity description extendible distortion measures 
assumed objective find hard clustering solution number clusters 
set limits number codevectors minimum temperature initialize update convergence test satisfied go 
perform iteration 
cooling step 
check condition phase transition critical reached cluster fig 

effect cluster mass population intermediate data sampled normal distributions centers marked 
computed representatives marked 
clustering mass constrained clustering 

add new codevector increment go 
note test critical may replaced simple perturbation considered expensive high dimensions 
case keep codevectors location perturb update critical reached merged iterations 
phase transition move apart 
relation lloyd algorithm quantizer design easy see 
iteration generalization nearest neighbor centroid conditions 
relation maximum likelihood estimation parameters proceedings ieee vol 
november normal mixtures obvious 
treatment problem covariance matrix estimation da see 
algorithm sketch typical case vq design easy modify produce cluster analysis solutions 
particular fuzzy clustering solutions produced naturally sequence scales determined temperature 
simple approach combine algorithm cluster validation techniques select scale solution sequence 
easy produce hard clustering solutions different scales adding quick quenching phase produce required hard solutions processed validation 
illustrative examples illustrate performance mass constrained clustering consider example shown fig 

mixture gaussian densities different masses variances 
compare result da known gla method 
gla yields results depend initialization run times time different initial set representatives randomly extracted training set 
fig 
show best result obtained gla mse 
result obtained runs got trapped local optima mse 
fig 
show result obtained da 
mse solution course independent initialization 
process annealing illustrated fig 

mixture overlapping gaussian densities 
process undergoes sequence phase transitions increased 
show results phases 
equiprobable contours emphasize fuzzy nature results intermediate limit high mse 
repeated runs gla example yielded variety local optima mse 
extensions applications section consider direct extensions da clustering method 
consider extensions motivated compression communications applications including vq design transmission noisy channels entropy constrained vector quantization structurally constrained clustering addresses encoding storage complexity problem 
briefly discuss straightforward extensions constraints codevectors identify special cases approaches traveling salesman problem self organizing feature maps 
vector quantization noisy channels area source channel coding concerned joint design communication systems account distortion due compression transmission noisy channel 
particular case vq communications systems advantageous optimize quantizer account effects channel 
noisy channel specified transition probabilities denote probability decoder decides codevector encoder fig 

gla versus da best result gla runs random initialization mass constrained clustering 

transmitted index codevector aside may noted exist applications noise models model distortion due implicit simplifying assumption channel memoryless significant temporal dependencies reaching transmission codevector index 
rose deterministic annealing fig 

various phases annealing process 
random partition represented equiprobable contours emphasize fuzzy nature results intermediate clusters contours clusters clusters clusters 

hierarchical topological constraints real communication channel 
simple important observation noisy channel vq design problem fact identical regular vq design modified distortion measure measures expected distortion codevector selected encoder 
observation allows direct extension known vq design algorithms case 
long history noisy channel quantizer design 
basic method proposed scalar quantizers extended papers 
papers basically describe gla type methods alternate enforcing encoder centroid decoder optimality conditions 
similarly extend da approach noisy channel case 
write expected source channel distortion defines encoder random da design 
note exploit fact form markov chain 
alternatively may write entropy defined encoding probabilities probabilities proceedings ieee vol 
november control standard da derivation obtain optimal encoding probability temperature free energy optimizing respect parameters yields centroid rule simplifies squared error distortion phase transition analysis da process noisy channel vq case see 
shown da approach avoids local optima design outperforms standard design 
particular interest design process converges explicit error control coding ecc limit noisy channels 
limit certain encoding regions empty codevectors encoder 
available bit rate reserved channel protection 
note system finds optimal error correcting code need general satisfy algebraic constraints linearity typically required ecc design 
entropy constrained vq design variable length coding called entropy coding commonly reduce rate required transmit quantized signals 
results rates close quantized signal entropy fundamental limit 
basic vq design approach assumes fixed length coding simplifying optimization problem minimizing distortion codebook size 
interest derive method optimizing vq conjunction variable length coding 
optimization take account distortion rate costs selecting codevector 
fundamental extension vq problem obtained incorporation entropy constraint design produce quantizers optimized subsequent entropy coding 
earlier concerned scalar quantizers 
vq design method proposed chou 
refer paradigm entropy constrained vq 
cost function weighted cost determines penalty increase rate relative increase distortion 
lagrange multiplier imposing prescribed rate constraint minimizing distortion alternatively imposing prescribed distortion minimizing rate 
follows problem reverts regular vq problem modified cost function subject additional constraint observation leads directly algorithm chou 
incorporated da method design pointed buhmann 
free energy note mass constrained version da masses implicit modified distortion measure 
equivalent mass constrained operation special case resulting update rules derived free energy 
assumes simplicity squared error distortion measure 
encoding rule centroid rule mass rule da iteration identical standard algorithm 
structurally constrained vq design major stumbling block way vq applications problem encoding complexity 
size codebook grows exponentially vector dimension rate bits sample 
encoder find best codevector codebook complexity grows linearly codebook size exponentially dimension rate 
practical applications encoding complexity acceptable low complexity alternatives needed 
common approach reducing encoding complexity involves imposition structural constraint vq partition 
treestructured partition typical structure consisting nested decision boundaries represented decision tree 
sibling nodes tree define vq partitions region associated parent node 
rose deterministic annealing reproduction codebook tree structured vq fact set leaves 
role internal nodes provide mechanism fast encoding search 
encoding operation exhaustive search leaves 
starts root tree determines path leaf sequence local decisions 
layer decision restricted selecting descendants winning node previous layer 
encoding search grows linearly exponentially dimension rate 
design general harder optimization problem design regular vq 
typical approaches employ greedy sequential design optimizing local cost grow tree node layer time 
reason greedy nature standard approaches unstructured case optimal partition design step readily specified nearest neighbor rule tree structured case optimal partition determined solving formidable multiclass risk discrimination problem 
heuristically determined high level boundaries may severely constrain final partition leaf layer lower layers designed 
da approach clustering offers way optimize partition parameters define final partition directly escape shallow local minima traps 
noted new difficulty need impose structure partition 
earlier problem appealed principle minimum cross entropy minimum divergence known generalization principle maximum entropy 
minimum cross entropy provides probablistic tool gradually enforce desired consistency leaf layer quantization cost calculated rest tree imposing structural constraint partition limit zero temperature 
approach provided consistent substantial gains standard approaches 
method worked tests theoretical disadvantages 
alternate minimization cross entropy cost leaf layer ensured converge practice problem 
second undesired aspect lacks direct simplicity basic da 
developments da approach context supervised learning opened way simpler general way impose structure partition direct extension basic da approach clustering 
detailed description extension section supervised learning 
shall cover minimum required develop da approach design 
appropriate focus derivation theoretical flaws earlier approach 
simulation results exist time written 
illustrate annealing type gains achievable include simulation results earlier approach spite theoretical shortcomings approximates closely optimal annealing process achieves apparently globally optimal solutions nontrivial examples 
replace hard encoding rule randomized decision 
probability encoding input leaf codevector fact probability choosing entire path starting root tree leading leaf 
sequence decisions node path competes siblings 
probability choosing node parent chosen gibbs parent scale parameter 
selection nodes sequential layers viewed markov chain transition probabilities obey gibbs distribution 
note particular markov chain hard decision tree resulting partition corresponds standard 
defined randomized tree partition limit enforces desired structure solution 
wish minimize objective distortion specified level randomness 
define lagrangian helmholtz free energy distortion leaf layer entropy markov chain computed employing explicit gibbs form 
minimizing free energy tree parameters obtain optimal random tree temperature 
temperature lowered reduction entropy traded reduction distortion tree hard 
process involves sequence phase transitions tree grows similar case unconstrained vq design 
fig 
show performance da method mixture example sequence phase transitions manner tree grows 
fig 
show designed da outperforms unstructured vq designed standard methods 
demonstrates significant impact poor local optima cause worse degradation structural constraint 
tree structure focused important structures signal compression da design methods developed 
commonly structure particularly speech coding multistage vector quantizer see early da approach 
important structure trellis quantizer trellis vector quantizer da approach proposed 
choice gibbs distribution arbitrary explained fundamental general setting section iii 
point simply noted directly obtainable maximum entropy principle 
proceedings ieee vol 
november fig 

hierarchy tree structured solutions generated annealing method increasing source gaussian mixture components 
right associated tree structure 
lines equiprobable contours membership probability partition region 
denotes highest level decision boundary 

graph theoretic optimization problems deterministic annealing clustering algorithm throw codevectors data point natural cluster 
viewed process data association data point exclusively associated codevector stands preference codevector associated data point 
adding appropriate constraints easy incorporate lagrangian derivation encourage process obtain associations satisfy additional requirements embody actual data assignment problem rose deterministic annealing fig 

gaussian mixture example components 
best unconstrained gla solution random initializations data set 
typical unconstrained gla solution 
unbalanced tree structured da solution maximal depth 

wish solve 
allows exploiting da framework solving variety hard graph theoretic problems 
example applied famous traveling salesman problem da derivation identical elastic net method 
approach applied various data assignment problems module placement problem computer aided design cad graph partitioning 
variant different constraint yields da approach batch optimization self organizing feature maps 
details see 
iii 
deterministic annealing supervised learning section develop deterministic annealing approach problems supervised learning 
consider general supervised learning function approximation problem parametric class functions function selected best captures input output statistics training set 
learning literature parametric class functions viewed set transfer functions implementable system structure varying parameters weights multilayer perceptron 
ultimate performance evaluation commonly performed independent test set samples 
fundamental extension da supervised learning inclusion structural constraints led da method classification da method piecewise regression extended address regression mixture experts 
formulating problem general setting deriving da method solution show specializes various classical regression classification problems defining structure parametric form function system cost criterion 
demonstrate da approach results powerful technique classifier regression function design variety popular system structures 
problem formulation define objective supervised learning approximating unknown function observation limited sequence typically noise corrupted input output data pairs 
function approximation typically referred regression output continuous classification output discrete 
regression classification important tools diverse areas signal processing statistics applied mathematics business administration computer science social sciences 
concreteness problem formulation employ terminology notation regression problem 
show solution fact applicable classification related problems 
regression problem usually stated optimization expected cost measures regression proceedings ieee vol 
november function applied random vector approximates output joint distribution practice sample set reformulate cost distortion measure general squared error 
cost optimized searching best regression function parametric class functions 
shall restrict space partitioning regression functions 
functions called piecewise regression functions approximate desired function partitioning space matching simple local model region 
space partitioning regression functions constructed components parametric space partition structured partition parametric local model partition cell 
partition parameter set denoted may consist nodes decision tree codevectors prototypes vector quantizer weights multilayer perceptron denote set local model parameters subset parameters specifying model region write regression function denotes local model 
note implicit partition cells typically local parametric model simple form constant linear gaussian 
average regression error measured sample set regression function learned minimizing design cost measured training set ultimate performance evaluation generalization cost error measured test set 
mismatch design cost generalization cost fundamental difficulty subject current research statistics general neural networks particular 
known choices cost measured design decreases complexity size learned regression model allowed increase generalization cost start increase model size grows certain point 
general optimal model size favorable regime model sizes unknown prior training model 
search correct model size naturally undertaken integral part training 
techniques improving generalization learning inspired known principle occam razor essentially states simplest model accurately represents data desirable 
william occam causes multiplied necessity perspective learning problem principle suggests design take account measure simplicity parsimony solution addition performance training set 
basic approach penalty terms added training cost directly favor formation small model indirectly regularization smoothness constraints costs measure 
second common approach build large model training set attempt undo training retaining vital model structure removing extraneous parameters learned nuances particular noisy training set 
approach adopted pruning methods cart methods optimal brain surgeon context neural networks 
techniques provide way generating parsimonious models additional serious difficulty methods address directly severely limit generalization achieved learning 
difficulty problem optima cost surface easily trap descent learning methods 
designed regression function performs poorly result shallow local minimum trap typical recourse optimize larger model assumption model sufficiently powerful characterize data 
larger model improve design cost may result training set suboptimal performance outside training set 
clearly superior optimization method finds better models smaller size enhance generalization performance regression function 
conventional techniques parsimonious modeling control model size address optimization difficulty 
particular standard methods classification regression trees cart tree structured classification regression employ greedy heuristics growing phase model design lead poorly designed trees 
subsequent pruning phase restricted search parsimonious models choosing pruned subtrees initial potentially suboptimal tree 
techniques add penalty terms cost suffer problems local minima 
fact cases addition penalty term increase complexity cost surface exacerbate local minimum problem 
alternative approach consider da optimization technique regression modeling formulation problem simultaneously embeds search parsimonious solution optimal design cost 
basic derivation design objective minimization expected regression cost repeated convenience rose deterministic annealing partition parameters implicit partition local model parameters derivation assume local model parameters known fixed focus difficult problem optimizing partition 
partition structurally constrained selected family partitions assigning values set parameters note partition fact classifier assigns input label indicating partition cell belongs 
popular structures partition vector quantizer decision tree multilayer perceptron radial basis functions partitions 
operation distinct structures classifiers consistent general canonical maximum discriminant model input system produces competing outputs partition cell discriminant functions input assigned largest winning output 
uses winner take partition rule partition represented model albeit possibly complicated discriminant functions 
note discriminant functions case specified set parameters suppressed dependence notation 
employ maximum discriminant model develop general optimization approach regression 
specialize results specific popular structures learning costs give experimental results demonstrate performance 
write objective function maximization determines hard partition note particular winner take rule optimal sense specifically maximizing possible partitions captures decision rule 
derive da approach wish randomize partition similar earlier derivation problem clustering 
probabilistic generalization partition represented association probabilities corresponding entropy emphasized measures average level uncertainty partition decisions 
determine assignment distribution level randomness maximizes maintaining prescribed level subject result best probabilistic partition sense structural objective specified level randomness 
naturally revert hard partition maximizes employs winner take rule 
positive solution gibbs distribution lagrange multiplier controlling level entropy 
associations increasingly uniform revert hard partition equivalent application rule 
probabilistic generalization winner take rule satisfies structural constraint specified choice note obvious dependence parameter discriminant functions determined far formulated controlled way introducing randomness partition enforcing structural constraint 
derivation assumed model parameters produced form distribution prescribing choose values parameter set 
derivation consider ultimate goal minimizing expected regression cost remedy shortcomings 
apply basic principles da design similar treatment clustering need introduce randomness partition enforcing required structure explicitly minimize expected regression cost 
priori satisfying multiple objectives may appear formidable task problem greatly simplified restricting choice random partitions set distributions random partitions naturally enforce structural constraint explained earlier 
parameterized set determined implicit seek distribution minimizes expected regression cost constraining entropy subject solution yields best random partition model parameters sense minimum entropy level limit zero entropy get hard partition minimizes desired structure specified 
naturally reformulate minimization unconstrained lagrangian free energy lagrange parameter temperature emphasizes intuitively compelling analogy statistical proceedings ieee vol 
november physics parallel da derivation earlier sections 
virtually discussion analogy statistical physics appeared context clustering holds provides strong motivation da method 
conciseness shall elaborate analogy 
initialize algorithm practice simply chosen large critical temperature 
clear goal temperature maximize entropy partition 
distributions consequently uniform 
parameters local regression models regions effectively single global regression model 
temperature gradually lowered optimization carried temperature find partition parameters local model parameters minimize lagrangian lagrangian reduces regression cost forced entropy go zero randomized space partition obtain hard partition satisfying imposed structure 
practice anneal system low temperature entropy random partition sufficiently small 
annealing change partition parameters significantly 
fix partition parameters point jump quench perform zero entropy iteration partition training set hard partition rule optimize parameters local models minimize regression cost approach consistent ultimate goal optimizing cost constrained hard structured space partition 
brief sketch da algorithm follows 
initialize lower temperature go 
zero entropy iteration partition hard partition rule simulations exponential schedule reducing annealing schedules possible 
parameter optimization may performed local optimization method 
generality wide applicability da solution regression classification clustering section iii derived da method design regression function subject structural constraints partition 
section pause appreciate general applicability da solution 
show special cases problem defined include problems clustering vector quantization statistical classifier design 
special cases obtained specifying appropriate cost functions local models 
review number popular structures data compression neural networks show special cases general maximum discriminant structure directly handled da approach derived 
restate learning problem 
training set pairs wish design function takes estimates estimator function constructed partitioning input space fitting local model region 
learning cost defined set partition regions set parameters determine local models 
obvious direct interpretation regression problem applications function approximation curve fitting easy see important problem classifier design special case learning problem local model simply class label assigned region define distortion measure error indicator function function takes value arguments equal vanishes learning cost exactly rate misclassification error rate classifier 
statistical classifier design special case general learning problem considered albeit particularly difficult cost optimize due discrete nature 
important problem numerous hot applications conjunction various structures devote space sequel 
somewhat contrived important special case regression problem unsupervised clustering 
consider degenerate case local regression models constant 
words approximate training set piecewise constant function 
partition space regions region represented constant vector codevector minimize cost mse 
clearly vector quantization problem 
apply da regression method problem assume vector quantizer structure get exactly clustering approach derived directly simply earlier 
simpler derivation da clustering method possible vq structure emerges minimization clustering distortion need externally imposed case general da regression method 
derivation unnecessarily cumbersome clustering problems fact open door important clustering applications 
interested solving clustering problem imposing different structure 
typical motivation reducing encoding storage complexity may discerning hierarchical information certain structure better fits prior information underlying distribution 
considered detail tree structured clustering unsupervised learning section postpone complete description mechanism enforcing structure supervised learning section 
rose deterministic annealing fig 

vq classifier architecture 

structures consider applicability approach variety structures 
recall approach generally derived maximum discriminant partition structure defined general structure specialized specific popular structures vector quantizer nearest prototype classifier multilayer perceptron radial basis functions classifier 
important note known design methods structure specific da approach directly applicable virtually structures 
remainder section describe structures 
choice presentation applicability general design procedure evident 
detailed structure specific derivation see 
vq classifier vq structure shown fig 

partition specified parameter set th prototype associated class vq classifier maps vector class associated nearest prototype specifying partition regions region union voronoi cells distance measure classification 
consistency maximum discriminant classifier winner takes note trivially classification rule written choosing 
fig 

rbf classifier architecture 

radial basis functions rbf classifier rbf classifier structure shown fig 

classifier specified set gaussian receptive field functions set scalar weights connect receptive fields class outputs network 
parameter center vector receptive field width normalized representation rbf adopt network output class written form viewed probability mass function network output effectively average weights emanating receptive fields 
classifier maps vector class largest output multilayer perceptron mlp classifier mlp classifier structure shown fig 

restrict mlp structure binary output unit class 
classification rule mlp rbf output functions parametrized differently 
input passes layers neurons layer define output hidden unit layer convention layer zero input layer layer output layer avoid special notation thresholds define augmented output vector layer standard notation allowing replace thresholds synaptic weights multiply fixed input value unity 
weight matrix connects augmented outputs layer neurons layer activation function th layer vector valued function defined proceedings ieee vol 
november fig 

mlp classifier architecture 

scalar activation function neurons th layer 
experiments logistic activation function hidden layers linear activation function output layer 
activity level input th layer network operation described recursion formula experimental results general da method supervised learning specialized specific design problems tested 
particular results classifier design vq rbf mlp structures piecewise regression mixture experts regression 
exact equations iteration depend structure derived straightforward manner general design approach 
details specific da design refer classifier design piecewise regression mixture experts 
vq classifier design da approach vq classifier design compared learning vq lvq method 
note lvq refer narrowly design method structure call vq 
simulation result synthetic example da design achieved test set prototypes prototypes comparison lvq prototypes 
general mlp hidden units achieved 
complicated mixture examples possibly overlapping mixture components multiple classes da method consistently achieve substantial performance gains lvq 
example consider training data class problem involving overlapping mixture components dimensions shown figs 

vq classifiers prototypes class designed lvq da 
figs 
display data partitions formed methods 
figs 
display prototype locations partitions 
best lvq solution random initializations shown fig 
achieved 
note method failed distinguish component class upper left fig 
component class near lower right 
contrast da solution shown fig 
succeeds discriminating components achieves 
benchmark test data finnish phoneme data set accompanies standard lvq package 
training set consists vectors dimensions 
vector represents speech attributes extracted short segment continuous finnish speech 
vectors labeled phoneme uttered speaker 
classes phonemes training set 
lvq da approaches number prototypes associated particular class set proportional relative population class training set 
referred initialization standard lvq package 
experimental results shown table 
note da method consistently outperformed lvq entire range 
rbf classifier design da approach rbf design compared method moody darken md rbf method described tarassenko roberts tr rbf gradient method steepest descent rbf 
md rbf combines unsupervised learning receptive field parameters supervised learning minimize squared distance target class outputs 
primary advantage approach modest design complexity 
receptive fields optimized supervised fashion cause performance degradation 
tr rbf methods described optimizes rbf parameters approximate target class outputs squared error sense 
design complex md rbf achieves better performance model size number receptive fields classifier uses 
tr rbf design objective equivalent minimizing case back propagation effectively aims approximate bayes optimal discriminant 
direct descent rose deterministic annealing fig 

class gaussian mixture training set vq classifier design partition produced lvq lvq partition class prototype 
locations prototypes shown 
error rate 

may minimize right objective problems local optima may quite severe 
fact performance methods quite poor judicious initialization 
methods employed unsupervised learning phase described isodata clustering variance estimation model initialization 
steepest descent performed respective cost surface 
complexity design typically tr rbf rbf occasionally design faster rbf 
accordingly chosen best results random initializations techniques compared single da design run 
illustrate increasing may help improve performance test set compared da fig 

class gaussian mixture training set vq classifier design partition produced da da partition class prototypes shown 
error rate 

table error probability comparison da lvq methods design vq classifiers dimensional class finnish phoneme data set accompanies standard lvq package 
represents total number prototypes results reported dimensional dimensional mixture examples 
example da achieved point training set point test set units 
results near optimal bayes rate 
contrast method receptive fields achieved 
example da achieved proceedings ieee vol 
november table error probability comparison da design techniques rbf classification dimensional waveform data 
number receptive fields 
da compared tr rbf md rbf rbf gradient descent 
test set performance results confidence intervals half length table error probability comparison da known design techniques rbf classification dimensional noisy waveform data 
test set performance results confidence intervals half length near optimal method achieved comprehensive tests higher dimensional data performed 
examples reported dimensional waveform data dimensional noisy waveform data obtained uc irvine machine learning database repository 
duplicate experiments conducted split vectors equal size training test sets 
results tables demonstrate quite substantial performance gains methods performance quite close estimated bayes rate 
note particular methods perform quite poorly small need increase achieve training set performance comparable approach 
performance test set necessarily improve may degrade large mlp classifier design da approach designing mlp compared approaches standard back propagation bp algorithm gradient descent cost surface mlp 
bp weights initialized random numbers uniformly distributed 
total epochs batch gradient descent algorithm run minimize mse desired actual outputs mlp 
bp descends cost surface mismatched minimum objective 
performance dependent choice initial weights 
mlp performance bp improved bp solution initialization descending practice gains achieved mlp bp marginal optimization performance sensitively depends choice initialization 
performance tested dimensional class image segmentation data university california irvine machine learning database 
training set contains vectors test set contains vectors dimensional 
features represent various attributes block pixels 
classes correspond textures sky foliage cement window path grass 
se table error probability comparison bp mlp design approaches dimensional class segmentation data example 
test set performance results confidence intervals half length quence single hidden layer neural networks designed data set 
table summarizes results various hidden layer sizes 
networks designed da significantly outperformed approaches entire range network sizes 
important concern issue design complexity 
experiments da learning complexity roughly higher back propagation roughly mlp 
suggests potential performance improvement typical applications greatly outweigh somewhat higher design complexity da approach 
piecewise regression summarize experiments comparing performance da approach vq piecewise regression conventional piecewise regression approach cart 
note regular cart severely restricted partition constrained tree structured partition boundaries parallel coordinate axes 
restriction prevents regular cart exploiting dependencies features overcome adopting extension cart allows boundaries regions arbitrary linear hyperplanes 
extension allows better partitioning input space smaller approximation error complexity design method extended structure grows size training set 
consequently extended form cart impractical training set short 
section refer regular cart cart extended form cart 
implementation cart consists growing large full tree pruning root node breiman friedman olshen stone algorithm see 
sequence cart regression trees obtained pruning process 
known pruning phase optimal fully grown tree 
cart complexity da method linear size training set 
da algorithm optimizes parameters regression function simultaneously avoiding shallow local minima trap greedy methods 
comparisons models piecewise constant simplest example piecewise regression 
implementation da method annealing schedule experiment involves synthetic datasets regression input output dimensional 
input components uniformly distributed interval 
output sum normalized gaussian shaped functions rose deterministic annealing table mean squared approximation error measured test set model order best solutions produced cart da multimodal gaussian data sets individual center variance magnitude 
choosing different sets parameters centers variances magnitudes gaussians created number data sets consisting training validation set size test set size 
output samples corrupted zero mean gaussian noise variance 
compare design approaches da cart applied design regression functions dataset training validation sets validation select best model size generalization performance evaluated independent test sets 
experiments conducted different data sets 
table provides randomly selected subset results 
note case da compared standard cart cart complex training sets size 
clearly examples da demonstrates consistent improvements cart 
compare cart da data sets real world regression applications 
data taken statlib database extensively researchers benchmarking relative performance competing regression methods 
due unavailability sufficient data proper validation simply compare performance regression models versus model size 
benchmark problem concerned predicting value homes boston area variety parameters 
training set consists data homes 
output case median price home input consisting vector scalar features believed influence price 
objective minimize average squared error price prediction 
features different dynamic ranges normalized unit variance prior application da cart 
piecewise constant regression models different model sizes generated design methods 
table compares squared error predicting house price standard cart extended form cart performance proposed da method 
clearly da method substantially outperforms cart cart entire range model sizes 
example illustrates da find substantially better solutions design objective 
note cart outperforms cart cases despite fact cart available www lib stat cmu edu data sets 
table mean squared error housing price boston ma area 
comparison training set errors standard cart extension cart da method 
number regions allowed model table mean squared prediction error age adjusted mortality rate inhabitants various environmental factors 
comparison cart cart da 
number regions allowed model potentially powerful regression structure 
results indicative difficulties due local minima 
data set example taken environmental sciences 
problem concerned predicting age adjusted mortality rate inhabitants locality factors presumably influenced 
factors related levels environmental pollution locality measurements mainly social parameters 
data set numerous researchers early 
data available localities divided separate training test sets 
show performance training set 
table shows vq regression function designed da offers consistent substantial advantage cart entire range model sizes 
third regression data set drawn application food sciences 
problem efficient estimation fat content sample meat 
techniques analytical chemistry measure quantity directly slow time consuming process 
data set quick measurements food feed analyzer measures absorption electromagnetic waves different frequency bands corresponding fat content determined analytical chemistry 
suggested data providers divided data training set size test set size 
applied cart cart da training set different model sizes 
table compares mean squared approximation error obtained training test sets methods 
da significantly outperformed cart regression functions number regions input space 
fact prototype da produced regression function outperformed cart regression function regions 
excellent performance da method outside training set confirms expected generalization capabilities 
note cart method exhibits overfitting test set performance deteriorating proceedings ieee vol 
november table mean squared approximation error fat content meat sample measurements 
performance cart cart compared proposed da method inside tr outside te training set 
number represent data mixture experts mixture experts important type structures inspired mixture models statistics 
class includes structures known mixture experts hierarchical mixture experts hme normalized radial basis functions 
refer class generally mixture experts models 
suggested variety problems including classification control regression tasks 
define local expert regression function set model parameters local model regression function defined nonnegative weight association input expert effectively determines degree expert contributes model output 
literature weights called gating units obey prespecified parametric form 
impose leads natural interpretation weight association gating unit probability association 
effective compromise purely piecewise regression models discussed earlier cart global models mlp 
purely piecewise meant input space regions exclusive expert model 
effectively piecewise regression function composed patchwork local regression functions collectively cover input space 
addition partitioning input space model parameter set partitioned submodels active particular local input region 
contrast global models mlp single regression function fit data explicit partitioning input space subdivision parameter set 
exploits partition space produces function combines contributions various experts appropriate weighting 
da design approach controlling entropy association probabilities case probabilities part problem definition artificial addition avoid minima 
important note approach zero temperature entropic constraint simply disappears find solution minimizes cost regardless entropy 
annealing consists starting high temperature high entropy gradually allowing entropy drop optimal level cost minimized 
results compare da approach conventional design methods hme regression functions 
experiments performed popular benchmark data sets regression literature 
experiment compare average squared error obtained training set da design method alternative design methods 
comparisons repeated different network sizes 
network size refers number local experts mixture model 
case binary hme trees levels case regression functions number gaussian basis functions 
common implementation local models constant functions case linear functions hme case 
alternative design approaches comparing hme design algorithm gd gradient descent algorithm simultaneously optimize hme parameters squared error cost ml jordan jacobs maximum likelihood approach 
regression function compared da design approach gd algorithm enhanced version method suggested see details 
fair comparison take conservative worst case estimate complexity da approach greater competing methods fact complexity da higher factor experiments 
compensate complexity allow competing method generate results different random initializations best result obtained runs selected comparison da result 
regression function obtained da generally independent initialization single da run sufficed 
consider results real world examples piecewise regression subsection 
results boston home value prediction problem tables demonstrate mixture models da approach achieves significantly smaller regression error compared approaches variety network sizes 
results mortality rate example tables meat fat content results measurements obtained food feed analyzer tables 
results synthetic data 
training set generated uniform distribution unit square 
output scalar 
created different data sets functions specified rose deterministic annealing table comparison regression error obtained da gd algorithms design boston home value problem 
number gaussian basis functions table comparison regression error obtained da gd ml algorithms hme function design boston home value problem 
number leaves teh binary tree table comparison regression error obtained da gd algorithms design mortality rate prediction problem 
number gaussian basis functions table comparison regression error obtained da gd ml algorithms hme design mortality rate prediction problem 
number leaves binary tree table comparison regression error obtained da gd algorithms design fat content prediction problem 
number gaussian basis functions 
tr te refer training test sets 
function generate training set test set size 
designed hme regression estimates data set da competitive design approaches 
results shown tables show improved performance da method consistent results obtained benchmark sets 
iv 
rate distortion connection rate distortion theory branch information theory concerned source coding 
fundamental table comparison regression error obtained da gd ml algorithms hme function design fat content prediction problem 
number leaves binary tree 
tr te refer training test sets respectively table comparison regression error obtained da gd training tr test te sets design approximate functions denotes number basis functions table regression error obtained da gd ml training tr test te sets hme design approximate functions denotes number leaves binary tree results due shannon 
coding theorems provide asymptotically achievable bound performance source coding methods 
bound expressed rd function source curve separates region feasible operating points region attained coding system 
important extensions theory general classes sources originally considered shannon developed see 
explicit analytical evaluation function generally elusive examples sources distortion measures 
main approaches taken address problem 
develop bounds important example shannon lower bound useful difference distortion measures 
second main approach develop numerical algorithm blahut ba algorithm evaluate rd functions 
power second approach function proceedings ieee vol 
november approximated arbitrarily closely cost complexity 
disadvantage complexity may overwhelming particularly case continuous alphabets continuous vector alphabets complexity grow exponentially dimensions 
disadvantage course closed form expression obtained function simple happens exist 
shall restrict attention continuous alphabet sources 
rd curve obtained minimizing mutual information subject average distortion constraint 
formally stated continuous source alphabet random variable probability measure density reproduction alphabet problem optimizing mutual information random encoders subject distortion measure 
replacing minimization parametric variational equations see problem reformulated problem optimization reproduction density functional minimized positive parameter varied compute different points rd curve 
criterion easily recognizable continuous version free energy developed mass constrained da derivation 
intuition obtained realization 
particular computation rd function equivalent process annealing effective reproduction alphabet discrete grows sequence phase transitions efficient da method compute rd curve 
result importance rate distortion theory basic da approach 
detailed treatment relations rd theory da 
give superficial outline 
see clearly connection da derivation note objective optimization determine probability measure reproduction space may consider alternative mapping approach searching optimal directly searches optimal mapping unit interval assign lebesgue measure denoted equivalence approaches ensured theory general measures topological spaces see example ch 
ch 

minimize functional mapping replace direct optimization density defined reproduction space mapping codevectors probabilities space 
exactly mass constrained da method 
recall basic da derivation high temperature small matter codevectors thrown converge single point viewed effective codevector 
rd case continuum codevectors easy see collapse centroid source distribution 
reproduction support effective alphabet cardinality 
lower temperature output remains discrete cardinality grows sequence phase transitions exactly seen treatment da clustering 
approach shown rd problem reproduction random variable continuous shannon lower bound see tight case squared error distortion happens source gaussian sum gaussians 
surprising result rate distortion theory analytically solved cases exactly shannon lower bound tight led implicit assumption optimal reproduction random variable continuous 
noted result anticipated fix early unfortunately went relatively unnoticed 
da viewpoint obvious direct observation 
summarized theorem 
theorem shannon lower bound hold equality support optimal reproduction random variable consists isolated singularities 
support bounded case practice discrete finite 
practical problem rd computation see approaches ba da equivalence follows borel isomorphism theorem 
approaches substantially different computational complexity performance need discretize 
ba discretization means defining grid output space da discretize unit interval replace set indexes induce adaptive grid mapping 
fixed grid output space da effectively optimizes codebook codevectors respective masses 
difference approaches crucial output distributions discrete finite 
gives da theoretical capability producing exact solutions finite model complexity ba approach exact solutions limit infinite resolution 
mass constrained da algorithm section ii compute rd curve 
rose deterministic annealing known result rd theory parameter defined simply related slope convex rd curve gives new interpretation da approach clustering temperature parameter 
process annealing simply process rd computation started maximum distortion consists climbing rd curve optimally trading decrease distortion increase rate 
position curve determined temperature level specifies slope point 
process follows rd curve long available codevectors needed output cardinality 
number codevectors priori limited case standard vq design da separates attempts stay close possible rd curve reaching phase corresponding maximum allowed codebook size 
important aspect annealing process raised demonstrated rd analysis existence types continuous phase transitions 
type transition analyzed computed critical temperature 
kind mass growing cluster born zero mass gradually gains mass type phase transition difficult handle preliminary results exist point 
able ensure phase transitions detected ensured da finds global optimum 
note practice mass growing phase transition missed algorithm compensated corresponding splitting phase transition occurs shortly optimality regained 
da extensions section couple extensions da approach briefly mentioned 
important extension method design classifiers hidden markov models obvious applications speech recognition 
preliminary results appeared 
shown da applied time sequences implemented efficiently forward backward algorithm similar baum welch reestimation algorithm 
da method allows joint optimization classifier components directly minimize classification error separate modeling speech utterances maximum likelihood approach 
results far show substantial gains standard methods 
preliminary results suggest speech recognition may turn important application da 
progress includes extensions continuous speech robustness classifier advance application da problem generalized vector quantization 
extends vq problem handle joint quantization estimation 
observes random vector provides quantized value statistically related unobservable random vector course special case regular vq problem 
typical application noisy source coding referred remote source coding information theory literature 
application concerned need combine vq interpolation vectors sampled complexity reasons 
preliminary results showing substantial gains due da 
vi 
summary da useful approach clustering related optimization problems 
approach strongly motivated analogies statistical physics formally derived information theory probability theory 
enables escaping poor local optima plague traditional techniques slow schedules typically required stochastic methods 
solutions obtained da totally independent choice initial configuration 
main objectives derive da basic principles emphasize generality illustrate wide applicability problems supervised unsupervised learning demonstrate ability provide substantial gains existing methods specifically tailored particular problem 
problems addressed concerned data assignment supervised unsupervised learning basic problem clustering 
probabilistic framework constructed randomization partition principle maximum entropy level distortion equivalently minimum expected distortion level entropy 
lagrangian shown helmholtz free energy physical analogy lagrange multiplier temperature 
minimization free energy determines equilibrium yields solution temperature 
resulting association probabilities gibbs distributions parameterized probabilistic framework annealing introduced controlling lagrange multiplier annealing interpreted gradually trading entropy associations reduction distortion 
phase transitions identified process fact cluster splits 
sequence phase transitions produces hierarchy fuzzy clustering solutions 
critical temperatures onset phase transitions derived 
limit zero temperature da converges known descent method gla means standard implementations arbitrarily heuristically initialized 
consistent substantial performance gains obtained 
method extended variety related unsupervised learning problems incorporating constraints clustering solutions 
particular da methods derived noisy channel vq entropy constrained vq structurally constrained vq design 
additional constraints may applied address graph theoretic problems 
proceedings ieee vol 
november highly significant extension supervised learning problems 
da approach allowing imposition structures partition optimizing ultimate optimization cost 
extension enables da approach optimize complicated discrete costs large variety popular structures 
method performance demonstrated problem classification vector quantizer radial basis functions multilayer perceptron structures problem regression vq hierarchical mixture experts normalized radial basis functions 
examples da approach significantly outperformed standard design methods developed specific structure 
relations information theory particular rd theory discussed 
shown da method clustering equivalent computation rd function 
observation led contributions rate distortion theory insights workings da 
couple extensions currently investigation briefly introduced 
extension design hidden markov model classifiers 
extends da handle time sequences directly applicable important problem speech recognition 
extension concerned problem generalized vector quantizer design 
akaike new look statistical model identification ieee trans 
automat 
contr vol 
pp 
dec 
algorithm calculating capacity arbitrary discrete memoryless channel ieee trans 
inform 
theory vol 
pp 
jan 
gray design joint source channel trellis waveform coders ieee trans 
inform 
theory vol 
pp 
nov 
ball hall clustering technique summarizing multivariate data behav 
sci vol 
pp 
mar 
baum petrie soules weiss maximization technique occurring statistical analysis probabilistic functions markov chains ann 
math 
statist vol 
pp 

beni liu biased fuzzy clustering method ieee trans 
pattern anal 
machine intell vol 
pp 
sept 
berger rate distortion theory 
englewood cliffs nj prentice hall 
minimum entropy quantizers permutation codes ieee trans 
inform 
theory vol 
pp 
mar 
bezdek convergence theorem fuzzy isodata clustering algorithms ieee trans 
pattern anal 
machine intell vol 
pami pp 
jan 
snyder mean field annealing formalism constructing algorithms ieee trans 
neural networks vol 
pp 
jan 
blahut computation channel capacity functions ieee trans 
inform 
theory vol 
pp 
july 
principles practice information theory 
reading ma addison wesley 
breiman friedman olshen stone classification regression trees wadsworth statistics probability series 
belmont ca wadsworth 
buhmann vector quantization complexity costs ieee trans 
inform 
theory vol 
pp 
july 
gray jr gray speech coding vector quantization ieee trans 
acoustics speech signal processing vol 
pp 
oct 

chang gray gradient algorithms designing predictive vector quantizers ieee trans 
acoustics speech signal processing vol 
assp pp 
aug 
cherkassky lee lari self organizing network regression efficient implementation comparative evaluation proc 
int 
joint conf 
neural networks vol 
pp 

chou optimal partitioning classification regression trees ieee trans 
pattern anal 
machine intell vol 
pp 
apr 
chou gray vector quantization ieee trans 
acoustics speech signal processing vol 
assp pp 
jan 
optimal pruning applications tree structured source coding modeling ieee trans 
inform 
theory vol 
pp 
mar 
dempster laird rubin maximumlikelihood incomplete data em algorithm roy 
stat 
soc vol 
pp 

dony haykin neural network approaches image compression proc 
ieee vol 
pp 
feb 
duda hart pattern classification scene analysis 
new york wiley 
dunham gray joint source channel trellis encoding ieee trans 
inform 
theory vol 
pp 
july 
dunn fuzzy relative isodata process detecting compact separated clusters cybern vol 
pp 

durbin szeliski yuille analysis elastic net approach travelling salesman problem neural computation vol 
pp 

durbin willshaw analogue approach travelling salesman problem elastic net method nature vol 
pp 

study vector quantization noisy channels ieee trans 
inform 
theory vol 
pp 
july 
optimum quantizer performance class non gaussian memoryless sources ieee trans 
inform 
theory vol 
pp 
may 
vaishampayan performance complexity channel optimized vector quantizers ieee trans 
inform 
theory vol 
pp 
jan 
fix rate distortion functions squared error distortion measures proc 
th annu 
allerton conf 
communications control computers oct pp 

gallager information theory reliable communication 
new york wiley 
geiger girosi parallel deterministic algorithms mrfs surface reconstruction ieee trans 
pattern anal 
machine intell vol 
pp 
may 
geman geman stochastic relaxation gibbs distribution bayesian restoration images ieee trans 
pattern anal 
machine intell vol 
pp 
nov 
gersho optimal nonlinear interpolative vector quantizers ieee trans 
commun vol 
com pp 
sept 
gersho gray vector quantization signal compression 
boston ma kluwer 
gold rangarajan graduated assignment algorithm graph matching ieee trans 
pattern anal 
machine intell vol 
pp 
apr 
graepel burger obermayer phase transitions stochastic self organizing maps phys 
rev vol 
pp 

gray probability random processes ergodic properties 
new york springer verlag 
source coding theory 
boston ma kluwer 
gray multiple local minima vector quantizers ieee trans 
inform 
theory vol 
pp 
mar 
rose deterministic annealing hajek tutorial survey theory applications simulated annealing proc 
th ieee conf 
decision control pp 

harrison rubinfeld hedonic prices demand clean air environ 
economics management vol 
pp 

hartigan clustering algorithms 
new york wiley 
haykin neural networks comprehensive foundation nd ed 
cliffs nj prentice hall 
hinton revow pairs data points define splits decision trees neural inform 
processing syst vol 
pp 

hofmann buhmann pairwise data clustering deterministic annealing ieee trans 
pattern anal 
machine intell vol 
pp 
jan 
hu customized ecg beat classifier mixture experts proc 
ieee workshop neural networks signal processing pp 


hwang 
lay martin regression modeling back propagation projection pursuit learning ieee trans 
neural networks vol 
pp 
may 
jacobs jordan learning piecewise control strategies modular neural network architecture ieee trans 
syst man cybern vol 
pp 
mar apr 
jacobs jordan nowlan hinton adaptive mixtures local experts neural computation vol 
pp 

jaynes information theory statistical mechanics papers probability statistics statistical physics rosenkrantz ed 
dordrecht netherlands kluwer 
jordan jacobs hierarchical mixtures experts em algorithm neural computation vol 
pp 

kirkpatrick gelatt vecchi optimization simulated annealing science vol 
pp 

deterministic annealing density estimation multivariate normal mixtures phys 
rev vol 
pp 

kohonen statistical pattern recognition neural networks benchmarking studies proc 
ieee int 
conf 
neural networks vol 
pp 

construction vector quantizers noisy channels electron 
commun 
japan vol 
pp 

kurtenbach noisy channels ieee trans 
commun vol 
com pp 
apr 
linde gray algorithm vector quantizer design ieee trans 
commun vol 
com pp 
jan 
lloyd squares quantization pcm ieee trans 
inform 
theory vol 
pp 
mar 
luttrell hierarchical vector quantization image compression proc 
inst 
elect 
eng 
commun 
speech vision vol 
pp 

derivation class training algorithms ieee trans 
neural networks vol 
pp 
june 
macqueen methods classification analysis multivariate observations proc 
th berkeley symp 
math 
statistics probability 
martinetz schulten network vector quantization application time series prediction ieee trans 
neural networks vol 
pp 
july 
max quantizing minimum distortion ire trans 
inform 
theory vol 
pp 
mar 
mcdonald instabilities regression estimates relating air pollution mortality technometrics vol 
pp 

mclachlan basford mixture models inference application clustering 
new york marcel dekker 
metropolis rosenbluth rosenbluth teller teller equations state calculations fast computing machines chem 
phys vol 
pp 

miller information theoretic framework optimization applications source coding pattern recognition ph dissertation univ california santa barbara 
miller rao rose gersho global optimization technique statistical classifier design ieee trans 
signal processing vol 
pp 
dec 
miller rose improved sequential search multistage vector quantizer proc 
ieee data computers conf pp 

combined source channel vector quantization deterministic annealing ieee trans 
commun vol 
pp 
feb 
hierarchical unsupervised learning growing phase transitions neural computation vol 
pp 

miller rose chou deterministic annealing trellis quantizer hmm design baum welch reestimation proc 
ieee int 
conf 
acoustics speech signal processing vol 
pp 

moody darken fast learning locally tuned processing units neural computation vol 
pp 

ahmed chan training radial basis function classifiers neural networks vol 
pp 

rao design pattern recognition systems deterministic annealing applications speech recognition regression data compression ph dissertation univ california santa barbara 
rao miller rose gersho deterministic annealing approach parsimonious design piecewise regression models submitted publication 
generalized vq method combined compression estimation proc 
ieee int 
conf 
acoustics speech signal processing vol 
iv pp 

rao rose gersho design robust hmm speech recognizer deterministic annealing proc 
ieee workshop automatic speech recognition understanding santa barbara ca dec pp 

deterministic annealing approach discriminative hidden markov model design proc 
ieee workshop neural networks signal processing amelia island fl sept pp 

rao miller rose gersho mixture experts regression modeling deterministic annealing ieee trans 
signal processing vol 
pp 
nov 
ripley neural networks related methods classification roy 
stat 
soc ser 
vol 
pp 
nov 
gray greedy tree growing algorithm design variable rate vector quantizers ieee trans 
signal processing vol 
pp 
nov 
rissanen stochastic complexity modeling ann 
statist vol 
pp 

rose deterministic annealing clustering optimization ph dissertation california inst 
technol pasadena 
mapping approach rate distortion computation analysis ieee trans 
inform 
theory vol 
pp 
nov 
rose gurewitz fox deterministic annealing approach clustering pattern recognition lett vol 
pp 

statistical mechanics phase transitions clustering phys 
rev lett vol 
pp 

vector quantization deterministic annealing ieee trans 
inform 
theory vol 
pp 
july 
constrained clustering optimization method ieee trans 
pattern anal 
machine intell vol 
pp 
aug 
rose miller constrained clustering data assignment problems examples module placement proc 
ieee int 
symp 
circuits systems san diego ca may pp 

real analysis rd ed 
new york macmillan 
rumelhart hinton williams parallel proceedings ieee vol 
november distributed processing 
cambridge ma mit 
shannon mathematical theory communication bell syst 
tech 
vol 
pp 

coding theorems discrete source fidelity criterion ire nat 
conv 
rec pt 
pp 

shore johnson axiomatic derivation principle maximum entropy principle minimum cross entropy ieee trans 
inform 
theory vol 
pp 
jan 
statistical mechanics underlying theory elastic neural optimization network vol 
pp 

constrained nets graph matching quadratic assignment problems neural computation vol 
pp 

hartley nonconvex optimization fast simulated annealing proc 
ieee vol 
pp 
nov 
tarassenko roberts supervised unsupervised learning radial basis function classifiers proc 
inst 
elect 
eng visual image signal processing vol 
pp 

titterington smith makov analysis finite mixture distributions 
new york wiley 
ueda nakano deterministic annealing variant em algorithm proc 
neural information processing systems nov pp 

waterhouse robinson non linear prediction acoustic vectors hierarchical mixtures experts proc 
neural inform 
processing syst vol 
pp 

weigend srivastava nonlinear gated experts time series discovering regimes avoiding overfitting int 
neural syst vol 
pp 


wong clustering data melting neural computation vol 
pp 

yair zeger gersho competitive learning soft competition vector quantizer design ieee trans 
signal processing vol 
pp 
feb 
yuille generalized deformable models statistical physics matching problems neural computation vol 
pp 

zeger gersho vector quantizer design memoryless noisy channels proc 
ieee int 
conf 
communications philadelphia pp 

zhao shawe taylor neural network optimization generalization performance proc 
int 
conf 
artificial neural networks pp 

kenneth rose member ieee received sc 
summa cum laude sc 
cum laude degrees electrical engineering tel aviv university israel respectively ph degree electrical engineering california institute technology pasadena 
july july employed israel carried research areas image coding image transmission noisy channels general image processing 
january joined department electrical computer engineering university california santa barbara currently associate professor 
research interests information theory source channel coding pattern recognition image coding processing nonconvex optimization general 
dr rose recipient ieee communications society william bennett prize award 
rose deterministic annealing 
