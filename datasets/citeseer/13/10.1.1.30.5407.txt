algorithms bigram trigram word clustering sven martin jorg hermann ney lehrstuhl fur informatik vi rwth aachen university technology aachen germany 
presents analyzes improved algorithms clustering bigram trigram word equivalence classes respective results give detailed time complexity analysis bigram clustering algorithms 
improved implementation bigram clustering large corpora words clustered small number days hours 
extend clustering approach bigrams trigrams 
experimental results word training corpus 

word equivalence classes method improving word gram language models 
words grouped classes word belongs class 
word pair seen training quite corresponding class pair seen 
bigram trigram class models equations wn gamma wn jg wn deltap wn jg wn gamma wn gamma wn gamma wn jg wn deltap wn jg wn gamma wn gamma denotes unique class mapping words delta word membership probability delta order delta second order markov chain probability 
probabilities computed maximum likelihood estimation case delta wn jg wn gamma wn gamma wn wn gamma delta number occurrences event parentheses 
problem unseen events circumvented smoothing methods take away probability mass seen events redistribute unseen events 
basic smoothing method class models absolute discounting backing 
see discussion issue 
word equivalence classes considerably reduce number model parameters bigram case words classes vocabulary size number classes 
furthermore linear interpolation gram class gram word models may yield better performance model 

clustering algorithm word equivalence classes obtained clustering algorithm 
goal algorithm find word class word perplexity class model minimized 
trigram bigram clustering exchange means style algorithm works follows set initial mapping compute initial train set perplexity stopping criterion met word vocabulary remove class existing classes compute perplexity moved assign class best perplexity initialization method consider frequent gamma words words defines class 
remaining words assigned class improve perplexity go step move words counts larger prespecified threshold 
stopping criterion prespecified number iterations 
algorithm stops words moved 

bigram clustering essential aspect algorithm efficient computation perplexity 
inserting likelihood estimates 
delta log perplexity formula dropping factor gamma arrive bi delta log gamma delta delta log delta log delta log delta delta log class indexes word index bigram count class pairs unigram count class iteration sufficient compute differences perplexity 
consider terms formula affected moving word class gw class kw give update formulae removing word class gw gw gw gw gamma gw gw gw gamma gw gw gw gw gamma gw gammac gw application called sieve formula poincar sylvester 
similar operations moving class kw obviously count updates resulting perplexity computation performed time 
counts computed word consideration 
scanning occurrences considered word training corpus occurrence word visited exactly iteration 
delta exchange attempts iteration obtain time complexity delta delta clustering algorithm corpus length number iterations 
reduce time complexity special organization training corpus 
observed word pair store count 
visiting occurrence considered word running text collect bigram occurrences involve assuming direct access array counts obtain time complexity delta delta number different word pairs encountered training corpus 
usually far smaller different bigrams vs total bigrams wall street journal corpus cpu time substantially reduced 
large vocabularies direct access prohibitive due large memory requirements 
lists binary search 
bigrams word average resulting time complexity delta delta log delta 
trigram clustering trigram class model log perplexity takes form tri delta log gamma delta log gamma delta log delta log delta log delta delta log efficiently compute perplexity exploit sieve formula 
bigram counts updated way trigram counts obtain gw gw gw gamma gw gw similar formulae 
gw gw gw gw gw gamma gw gammac gw gw gw gw gw similar formulae 
gw gw gw gamma gw gw gw gw gw similar formulae 
gw gw gw gw gw gw gamma gw gw gammac gw gw gamma gw gw gw gw gw gamma gw gw gw gw gamma gw gammac gw gw gw gw gw similar formulae 
gw gw gamma gw gw similar formulae 
note counts left hand side formulae appear right hand side 
formulae valid unmodified counts right hand side 
computation possible time complexity resulting total time complexity delta delta clustering large number classes computationally expensive 
trigram clustering performed classes 
larger number classes clustering algorithm bigram clusters define trigram class models 
get counts word trigrams corpus case bigram clustering 
case bigram clustering lists binary search 
finding counts delta delta computationally expensive obtain time complexity delta delta log delta delta log delta number different word trigrams 
method applied large corpora method runs memory problems 

language model interpolation linear interpolation combine word trigram model wn gamma wn gamma class trigram model wn gamma wn gamma wn gamma wn gamma delta wn gamma wn gamma gamma delta wn gamma wn gamma method especially suitable models preferred 
allowing individual models included special cases 
interpolated model optimal worse best individual models 
word model singleton trigram described 

corpora clustering times evaluation models wall street journal task vocabulary words 
data bases task constructed training corpus words clustering event counting development corpus words smoothing interpolation parameter estimation test corpus words 
check model behavior different amounts training data smaller clustering word sets taken corpus sizes words 
composition training corpora test corpus described 
threshold separating words considered moving estimated bigram clustering clustering classes different threshold values iterations training corpora choosing threshold performing best development data 
threshold corpus 
trigram clustering due cpu costs thresholds estimated exactly 
type threshold affects perplexity resulting class model decreases cpu time effective size vocabulary reduced 
thresholds bigram clustering vocabulary considered clustering corpus corpus 
table shows cpu times iteration bigram clustering improved version algorithm sgi workstation 
times predicted time complexity formula 
table cpu seconds iteration improved bigram clustering algorithm sgi workstation classes clustering times trigram clustering directly comparable clustering performed different machines 
give idea computation times mention clustering classes corpus takes hours iteration sgi workstation clustering classes corpus takes days iteration machine 
bigram clustering performed words moved resulting iterations 
bigram class model constructed iteration experiments selected model performed best development corpus 
due cpu costs trigram clustering fine tuning possible iterations performed resulting classes 

results main series experimental results 
series tables summarizes perplexities class models test corpus parameter tuning 
second series tables shows results interpolation class word models test corpus interpolation parameter estimation 
clustering bigram trigram models constructed resulting classes smoothing parameters performed best development corpus selected experiments reported 
table shows measured perplexities bigram class models 
classes perplexity obtained word bigram corpus 
table class bigram perplexities classes word class trigram trigram clustering algorithm obtaining classes 
results summarized table 
table class trigram perplexities classes trigram clustering classes word larger numbers classes rely classes obtained bigram clustering operation 
training corpora merely getting trigram class counts 
see classes obtained trigram clustering operation better obtained bigram clustering operation applied procedure models classes 
results summarized table 
exception class model clustered corpus appears small trigram clustering model parameters perplexities classes obtained trigram clustering shown table slightly better obtained bigram clustering 
perplexities better bigram class models table considerably worse word trigrams 
finer tuning parameters improves performance case class bigrams 
table class trigram perplexities classes bigram clustering classes word tables summarize results interpolation class word models 
interpolation factor determined development corpus 
trained word models interpolation factor eq 
word models trained corpus 
generally seen word models considerably improved best result perplexity classes corpus table trained word models remain unchanged performance 
note increase perplexity classes training corpus table 
class number class models 
effect appears tables 
table perplexity interpolated class word bigram models classes word table perplexity interpolated class bigram word trigram models classes word table perplexity interpolated class word trigram models classes word table perplexity interpolated class word trigram models classes taken bigram clustering classes word 
word equivalence classes means building small effective language models cases improve word gram language models 
efficient implementation bigram clustering algorithm trigram clustering 

kneser ney improved clustering techniques class statistical language modelling proc 
european conference speech communication technology berlin pp 
september 

brown della pietra desouza lai mercer class gram models natural language computational linguistics vol 
pp 


kneser ney improved backing language modeling proc 
international conference acoustics speech signal processing detroit mi pp 
may 

ney essen kneser structuring probabilistic dependences stochastic language modelling computer speech language vol 
pp 


rosenfeld adaptive statistical language modeling maximum entropy approach school computer science carnegie mellon university ph thesis pittsburgh pa cmu cs 

ney wessel extensions absolute discounting language modeling proc 
european conference speech communication technology madrid september 

pfeifer fur teubner verlag stuttgart 
