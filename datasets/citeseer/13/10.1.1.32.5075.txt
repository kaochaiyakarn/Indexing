occam razor carl edward rasmussen department mathematical modelling technical university denmark building dk lyngby denmark carl imm dtu dk bayes imm dtu dk zoubin ghahramani gatsby computational neuroscience unit university college london queen square london wc ar england zoubin gatsby ucl ac uk www gatsby ucl ac uk bayesian paradigm apparently gives rise occam razor times large models perform 
give simple examples kinds behaviour 
views reconciled measuring complexity functions machinery implement 
analyze complexity functions linear parameter models equivalent gaussian processes find occam razor 
occam razor known principle parsimony explanations influential scientific thinking general problems statistical inference particular 
review consequences bayesian statistical models behaviour easily demonstrated quantified 
think build prior models explicitly favours simpler models 
see occam razor fact embodied application bayesian theory 
idea known automatic occam razor smith spiegelhalter mackay berger 
focus complex models large numbers parameters referred non parametric 
term refer models necessarily know roles played individual parameters inference primarily targeted parameters predictions models 
types models typical applications machine learning 
non bayesian perspective arguments put forward adjusting model complexity light limited training data avoid fitting 
model complexity regulated adjusting number free parameters model complexity constrained regularizers weight decay 
model complexity low high performance independent test set suffer giving rise characteristic occam hill 
typically estimator generalization error independent validation set control model complexity 
bayesian perspective authors take conflicting stands question model complexity 
view infer probability model different model sizes probabilities making predictions 
alternate view suggests simply choose large model sidestep problem model size selection 
note views assume parameters averaged 
example occam razor determine optimal number hidden units neural network simply hidden units possible computationally 
describe views detail 
view model size selection central quantities bayesian learning evidence probability data model jm computed integral parameters likelihood times prior 
evidence related probability model jy bayes rule jm jw dw jy jm uncommon prior models flat jy proportional evidence 
explains evidence discourages models select probable model 
possible understand evidence discourages models embodies occam razor interpretation 
evidence probability randomly selected parameter values model class generate data set models simple generate particular data set models complex generate possible data sets generate particular data set random 
view large models non parametric bayesian models statistical reason constrain models long prior reflects beliefs 
fact constraining model order number parameters small number usually fit prior beliefs true data generating process sense large models matter data pursue infinite limit example ought limit number basis functions function approximation priori don really believe data generated small number fixed basis functions 
consider models parameters handle computationally 
neal showed multilayer perceptrons large numbers hidden units achieved performance small data sets 
sophisticated mcmc techniques implement averaging parameters 
line thought model complexity selection task don need evaluate evidence difficult don need want occam razor limit number parameters model 
really ought average predictions models weighted probabilities 
evidence strongly peaked practical reasons may want select approximation 
models limit infinite number parameters simple model treated tractably 
examples gaussian process limit bayesian neural networks neal infinite limit gaussian mixture models rasmussen 
simple complex just right possible data sets left panel evidence function dimensional representation possible datasets 
evidence normalize complex models account datasets achieve modest evidence simple models reach high evidences limited set data 
dataset observed evidence select model complexities 
selection done just likelihood jw 
right panel neural networks different numbers hidden unit form family models posing model selection problem 
linear parameters models example fourier model simplicity consider function approximation class models linear parameters class includes known models polynomials splines kernel methods scalar output unknown weights parameters model fixed basis functions scalar vector input example number example fourier model scalar inputs form sin dx cos dx fa ad assuming independent gaussian prior weights exp scale precisions inverse variances weights order frequency easy show gaussian priors weights imply gaussian process priors functions covariance function corresponding gaussian process prior cos prior joint density finite set outputs gaussian order order order order order order order order order order order order model order top different model orders unscaled model 
mean predictions shown full line dashed dotted lines limit central mass predictive distribution student 
bottom posterior probability models normalised models 
probabilities models exhibit occam hill discouraging models small big 
inference fourier model data fx jn ng independent gaussian noise precision likelihood yjx exp 
analytical convenience scale prior proportional noise precision put vague gamma priors exp exp integrate weights noise get evidence function prior hyperparameters scale relative scales zz yjx wjc dw jaj exp choose vague priors setting 
scaling exponent scaling exponent scaling exponent scaling exponent functions drawn random fourier model order dark light different scalings limiting behaviour left right discontinuous brownian borderline smooth smooth 
diag tilde indicates duplication components 
optimize scale weights 
newton method 
choose relative scales 
answer question turns intimately related different views bayesian inference 
example illustrate behaviour model data generated step function changes corrupted independent additive gaussian noise variance 
note true function implemented exactly model finite order typically case realistic modelling situations true function realizable model said incomplete 
input points arranged points step occurring middle larger see 
choose scaling precisions independent frequency contributions normalizing sum inverse precisions achieve predictions depicted 
clearly see occam razor behaviour 
model order preferred 
say limited data support models complex 
way understanding note model order grows prior parameter volume grows relative posterior volume decreases parameters accurately specified complex model ensure agreement data 
ratio prior posterior volumes occam factor may interpreted penalty pay fitting parameters 
model easy draw functions random prior simply drawing values coefficients prior distributions 
left panel shows samples prior previous example 
increasing order functions get dominated high frequency components 
modelling applications prior expectations smoothness 
scaling precision factors achieve prior functions converges functions particular characteristics grows infinity 
focus scalings form different values scaling exponent 
example choose scaling get occam razor terms order model 
note predictions independent model order long order large 
note large models reasonable spurious dip data predicted high confidence 
choice scaling large models view appropriate 
course ought integrate unfortunately difficult 
order order order order order order order order order order order order model order scaling leading prior converges smooth functions 
occam razor see long model complex evidence flat 
notice predictive density model unchanged long sufficiently large 
discussion previous examples saw depending scaling properties prior parameters occam razor view large models view appropriate 
example unsatisfactory obvious choose scaling exponent gain insight meaning analysing properties functions drawn prior limit large useful consider expected squared difference outputs corresponding nearby inputs separated 


limit 

table computed limits various values characteristics functions 
example property smooth functions 
kind information may help choose values practical applications 
attempt infer characteristics function data 
show evidence depends scale model large order 
seen evidence maximum 
fact seeing occam razor 
time terms dimension model terms complexity functions priors implied different values large values correspond priors probability mass simple functions small values correspond priors allow complex functions 
note optimal setting exactly model 
scaling exponent log evidence max lim 

properties discontinuous 
brownian 
ln 
borderline smooth 
smooth left panel evidence function scaling exponent scale maximum 
table shows characteristics functions different values examples functions shown 
reviewed automatic occam razor bayesian models seen necessarily penalising number parameters process active terms complexity functions 
simplistic examples explanations behaviours rely basic principles generally applicable 
differing bayesian views attractive depends circumstances large model limit may computationally demanding may difficult analyse scaling properties priors models 
hand typical applications non parametric models large model view may convenient way expressing priors typically don seriously believe true generative process implemented exactly small model 
optimizing integrating continuous hyperparameters may easier optimizing discrete space model sizes 
whichever view take occam razor discouraging models 
supported danish research councils computational neural network center connect thor center 
geoff hinton asking puzzling question stimulated writing 
berger 
ockham razor bayesian analysis 
amer 
sci 
mackay 
bayesian interpolation 
neural computation 
neal 
bayesian learning neural networks lecture notes statistics new york springer verlag 
rasmussen 
infinite gaussian mixture model solla leen 
muller editors adv 

inf 
proc 
sys 
mit press pp 

smith spiegelhalter 
bayes factors choice criteria linear models 
roy 
stat 
soc 
