tagging chunking bigrams pla antonio molina prieto universitat polit de val departament de inform computaci cam de vera val upv es integrated system tagging chunking texts certain language 
approach stochastic nite state models learnt automatically 
includes bigram models nite state automata learnt grammatical inference techniques 
models involved system learnt automatically exible portable system 
order show viability approach results tagging chunking bigram models wall street journal corpus 
achieved accuracy rate tagging precision rate np chunks recall rate 
part speech tagging shallow parsing known problems natural language processing 
tagger considered translator reads sentences certain language outputs corresponding sequences part speech pos tags account context word sentence appears 
shallow parser involves dividing sentences non overlapping segments basis super cial analysis 
includes discovering main constituents sentences nps vps pps 
heads 
shallow parsing usually identi es non recursive constituents called chunks abney non recursive noun phrases base np base vp :10.1.1.11.8199
include determining syntactical relationships subject verb verb object shallow parsing follows tagging process fast reliable pre processing phase full partial parsing 
information retrieval systems information extraction text summarization bilingual alignment 
addition solve computational linguistics tasks disambiguation problems 
pos tagging approaches di erent approaches solving problem classi ed main classes depending tendencies followed establishing language model lm linguistic approach hand coded linguistic rules learning approach derived corpora labelled approximations hybrid methods proposed voutilainen 
linguistic approach expert linguist needed formalise restrictions language 
implies high cost dependent particular language 
nd important contribution voutilainen uses constraint grammar formalism 
supervised learning methods proposed brill learn set transformation rules repair error committed probabilistic tagger 
main advantage linguistic approach model constructed linguistic point view contains complex kinds knowledge 
learning approach extended formalism grams hmm 
case language model estimated labelled corpus supervised methods church weischedel corpus unsupervised methods cutting 
rst case model trained relative observed frequencies 
second model learned baum welch algorithm initial model estimated labelled corpora merialdo 
advantages unsupervised approach facility build language models exibility choice categories ease application languages 
nd machine learning approaches sophisticated lms decision trees rodr guez magerman memory approaches learn special decision trees daelemans maximum entropy approaches combine statistical information di erent sources ratnaparkhi nite state automata inferred grammatical inference pla prieto comparison di erent approaches dif cult due multiple factors consid ered language number type tags size vocabulary ambiguity di culty test set best results reported wall street journal wsj treebank marcus statistical language models accuracy ratio depending di erent factors mentioned 
linguistic approach results better 
example voutilainen accuracy reported certain ambiguities output remain unsolved 
works published brill wu set taggers combined order improve performance 
cases methods achieve accuracy halteren 
shallow parsing approaches early techniques carrying shallow parsing developed 
techniques classi ed main groups hand coded linguistic rules learning algorithms 
approaches common characteristic take sequence lexical tags proposed pos tagger input learning chunking processes 
techniques hand coded linguistic rules methods hand written set rules de ned pos terminals grammar 
works nite state methods detecting chunks accomplishing linguistic tasks abney 
works different grammatical formalisms constraint grammars voutilainen combine grammar rules set heuristics bourigault :10.1.1.14.6073
works usually small test set manually evaluated achieved results signi cant 
regular expressions de ned identi ed non recursive clauses non recursive nps english text 
experimentation brown corpus achieved precision rate clauses nps 
abney introduced concept chunk abney incremental partial parser abney :10.1.1.11.8199
parser identi es chunks base parts speech chooses combine higher level analysis lexical information 
average precision recall rates chunks respectively test set sentences 
incremental architecture nite state transducers french 
transducer performs linguistic task identifying segments syntactic structures detecting subjects objects 
system evaluated various corpora subject object detection 
precision rate varied 
recall rate varied 
parser described voutilainen identi ed maximal length noun phrases 
gave precision rate recall rate 
results criticised ramshaw marcus due inconsistencies apparent mistakes appeared sample voutilainen :10.1.1.53.2725
bourigault developed parser french grammatical rules heuristics bourigault :10.1.1.14.6073
achieved recall rate identifying maximal length terminological noun phrases give precision rate dicult evaluate actual performance parser 
learning techniques approaches automatically construct language model labelled bracketed corpus 
rst probabilistic approach proposed church 
method learnt bigram model detecting simple noun phrases brown corpus 
sequence parts speech input church program inserts probable openings endings nps viterbi dynamic programming algorithm 
church give precision recall rates 
showed np omitted small test pos tagging accuracy 
transformation learning tbl ramshaw marcus detect base np :10.1.1.53.2725
chunking considered tagging technique pos tagged inside basenp outside basenp inside basenp preceding word basenp 
approach resulted precision rate recall rate 
result automatically evaluated test set words extracted wsj treebank 
main drawback approach high requirements time space needed train system needs train templates combinations words 
works memory learning algorithm 
approaches construct classi er task storing set examples memory 
example de ned set features learnt bracketed corpus 
memory learning mbl algorithm daelemans takes account lexical pos information 
stores features word form pos tag words left focus word word right 
system achieved precision rate recall rate wsj treebank 
pos information performance decreased achieving precision rate recall rate 
memory sequence learning algorithm argamon learns substrings sequences pos brackets 
precision recall rates data ramshaw marcus :10.1.1.53.2725
simple approach cardie pierce called treebank approach ta 
technique matches pos sequences initial noun phrase grammar extracted annotated corpus 
precision achieved rule rank prune rules discarding rules score lower prede ned threshold 
uses longest match heuristic determine base np 
precision recall wsj treebank respectively 
dicult compare di erent approaches due various reasons 
uses di erent de nition base np 
evaluated di erent corpus di erent parts corpus 
systems evaluated hand small test set 
table summarizes precision recall rates learning approaches data extracted wsj treebank 
method np precision np recall tbl ta mbl mbl pos table precision recall rates di erent np parsers 
general description integrated approach tagging chunking propose integrated system combines di erent knowledge sources lexical probabilities lm chunks contextual lm sentences order obtain corresponding sequence pos tags shallow parsing su su su wn cn su certain input string wn 
system transducer composed levels upper represents contextual lm sentences lower chunks considered 
formalism levels nite state automata 
exact models bigrams smoothed backo technique katz order achieve full coverage language 
bigrams lms bigram probabilities obtained means slm toolkit tagging chunking lexical probabilities contextual lm 
wn su su 
wn cn decoding learning corpus lm chunks learning lm overview system 
sequences categories training set 
represented nite state automata 
learning phase 
models estimated labelled bracketed corpora 
training set composed sentences su su su wn cn su words part speech tags su chunks considered 
models learnt contextual lm smoothed bigram model learnt sequences part speech tags chunk descriptors su training corpus see 
models chunks smoothed bigram models learnt sequences partof speech tags corresponding chunk training corpus see 
lexical probabilities estimated word frequencies tag frequencies word tag frequencies 
tag dictionary built full corpus gives possible lexical categories pos tags word equivalent having ideal morphological analyzer 
probabilities possible tag assigned information account obtained statistics 
due fact word seen training seen possible categories compulsory apply smoothing mechanism 
case word previously seen probability assigned categories dictionary seen 
su contextual lm lm chunks integrated lm integrated language model tagging chunking 
categories smoothing called add applied 
renormalization process carried 
lms learnt regular substitution lower model upper 
way get single integrated lm shows possible concatenations lexical tags syntactical units transition probabilities include lexical probabilities see 
note models smoothed 
decoding process tagging parsing tagging shallow parsing process consists nding sequence states maximum probability integrated lm input sentence 
sequence compatible contextual syntactical lexical constraints 
process carried dynamic programming viterbi algorithm conveniently modi ed allow transitions certain states automata consuming symbols epsilon transitions 
portion dynamic programming trellis generic sentence integrated lm shown seen 
states automata reached compatible lexical constraints marked black circle state possible reach state transition automata lexical probability jc null 
transitions initial nal states models chunks allowed states marked white circle case symbol consumed 
cases transitions initial nal produce transitions successors dotted lines symbols consumed 
dynamic programing trellis built obtain maximum probability path input sentence best sequence lexical tags best segmentation chunks 
ci cj ck ci ck cm cn 
input output 
wn wn wn ci su wn ck wn su wn cn final state 
partial trellis programming decoding integrated lm 
experimental section describe set experiments carried order demonstrate capabilities proposed approach tagging shallow parsing 
experiments carried wsj corpus pos tag set de ned marcus considering np chunks de ned church models 
approach corpora changing language lexical tag sets kinds chunks done direct way 
corpus description 
portion wsj corpus words tagged penn treebank tag set bracketed np markers train test system 
tag set contained di erent tags 
words corpus ambiguous ambiguity ratio tag word ambiguous words 
experimental results 
order train models test system randomly divided corpora parts approximately words training words testing 
bigram models representing contextual information syntactic description np chunk lexical probabilities estimated training sets di erent sizes 
due fact morphological analyser english constructed tag dictionary lexicon training set test set 
dictionary gave possible lexical tags word corpus 
case test estimate lexical probabilities 
words big big big lex accuracy rate tagging wsj incremental training sets 
show results tagging test set terms training set size approaches simplest lex tagging process take contextual information account lexical tag associated word words precision recall np chunking results wsj incremental training sets 
tagging np chunking tagger accuracy precision recall big big lex big ideal assumed table tagging np chunking results taggers training set words 
appeared training set 
second method corresponds tagger bigram model big 
third uses integrated lm described tagging accuracy big big big close respectively language model lex tagging accuracy points lower 
trend cases increment size training set resulted increase tagging accuracy 
training words result stabilized 
show precision correct proposed np proposed np recall correct proposed np np rates np chunking 
results obtained integrated lm satisfactory achieving precision rate recall rate 
performance np chunker improves training set size increases 
obviously due fact model better learnt size training set increases tagging error decreases seen 
usual sequential process chunking sentence 
rst tag sentence integrated lm carry chunking 
case contextual probabilities taken account decoding process 
table show relevant results obtained tagging np chunking 
rst row shows result tagging chunking done integrated way 
rows show performance sequential process di erent taggers lex takes account lexical probabilities 
case tagging accuracy 
big bigram model achieved accuracy 
ideal simulates tagger accuracy rate 
tagged sentences wsj corpus directly 
results con rm precision recall rates increase accuracy tagger better 
performance sequential process big tagger slightly better performance integrated process big big 
think probably way combined probabilities di erent models 
system tagging chunking integrated language model uses homogeneous formalism nite state machine combine di erent knowledge sources lexical syntactical contextual models 
feasible terms performance terms computational eciency 
models involved learnt automatically data system exible portable changes language lexical tags kinds chunks direct way 
tagging accuracy big big big higher similar approaches 
tag dictionary including test set restrict possible tags unknown words assumption obviously increase rates tagging done quantitative study factor 
mentioned comparison approaches dicult due reasons ones de nitions base np sizes train test sets di erent knowledge sources learning process di erent 
precision np chunking similar statistical approaches section integrated process sequential process tagger bigrams 
recall rate slightly lower approaches integrated system similar sequential process 
sequential system error free input ideal performance system obviously increased precision recall 
results show uence tagging errors process 
studying results integrated process sequential process di erent 
testing adjustment factors models weighting di erent probability distribution improve results 
models bigrams trigrams stochastic regular model 
respect worked complex lms formalized nite state automata learnt grammatical inference techniques 
approach bene inclusion lexical contextual information lm 
acknowledgments partially supported spanish research project cicyt tic 
abney 

parsing chunks 
berwick abney eds 
principle parsing kluwer academic publishers dordrecht 
abney 

partial parsing finite state cascades 
proceedings esslli robust parsing workshop prague czech republic 
argamon dagan 

memory approach learning shallow natural language patterns 
proceedings joint th international conference computational linguistics th annual meeting association computational linguistics coling acl pages montr eal canada 


incremental finite state parsing 
proceedings th conference applied natural language processing washington usa 
bourigault 

surface grammatical analysis extraction terminological noun phrases 
proceedings th international conference computational linguistics pages 
eric brill jun wu 

classi er combination improved lexical disambiguation 
proceedings joint th international conference computational linguistics th annual meeting association computational linguistics coling acl pages montr eal canada 
brill 

transformation error driven learning natural language processing case study part speech tagging 
computational linguistics 
cardie pierce 

error driven treebank grammars base noun phrase identi cation 
proceedings joint th international conference computational linguistics th annual meeting association computational linguistics pages montr eal canada august 
church 

stochastic parts program noun phrase parser unrestricted text 
proceedings st conference applied natural language processing anlp pages 
acl 


statistical language modeling cmu cambridge toolkit 
proceedings eurospeech rhodes greece 
cutting kupiec pederson sibun 

practical part speech tagger 
proceedings rd conference applied natural language processing anlp pages 
acl 
daelemans zavrel 

mbt memory part speech tagger generator 
proceedings th workshop large corpora pages copenhagen denmark 
daelemans buchholz veenstra 

memory shallow parsing 
proceedings emnlp vlc pages university maryland usa june 


finding clauses unrestricted text finitary stochastic methods proceedings second conference applied natural language processing pages 
acl 
van halteren zavrel daelemans 

improving data driven tagging system combination 
proceedings joint th international conference computational linguistics th annual meeting association computational linguistics pages montr eal canada august 
katz 

estimation probabilities sparse data language model component speech recognizer 
ieee transactions acoustics speech signal processing 
magerman 

learning grammatical structure statistical decision trees 
proceedings rd international colloquium grammatical inference pages 
springer verlag lecture notes series arti cial intelligence 
marcus marcinkiewicz santorini 

building large annotated corpus english penn treebank 
computational linguistics 
horacio rodr guez 

partof speech tagging decision trees 
rouveirol editor lnai proceedings th european conference machine learning ecml pages chemnitz germany 
springer 
merialdo 

tagging english text probabilistic model 
computational linguistics 
pla prieto 

grammatical inference methods automatic part speech tagging 
proceedings st international conference language resources evaluation lrec granada spain 
ramshaw marcus 

text chunking transformation learning 
proceedings third workshop large corpora pages june 
ratnaparkhi 

maximum entropy part speech tagger 
proceedings st conference empirical methods natural language processing emnlp 
voutilainen 
developing hybrid np parser 
proceedings th conference applied natural language processing anlp pages washington dc 
acl 
voutilainen 

detector english noun phrases 
proceedings workshop large corpora 
acl june 
voutilainen 

syntax part speech analyzer 
proceedings th conference european chapter association computational linguistics eacl dublin ireland 
weischedel schwartz meteer ramshaw 

coping ambiguity unknown words probabilistic models 
computational linguistics 
