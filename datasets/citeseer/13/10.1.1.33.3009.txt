compilation techniques parallel systems rajiv gupta dept computer science univ arizona tucson az santosh pande dept elect 
comp 
engg computer science univ cincinnati ml cincinnati oh division computer science univ texas san antonio san antonio tx vivek sarkar ibm watson research center box yorktown heights ny past decades tremendous progress design parallel architectures compilers needed exploiting parallelism architectures 
summarize advances compilation techniques uncovering effectively exploiting parallelism various levels granularity 
describing program analysis techniques parallelism detected expressed form program representation 
compilation techniques scheduling instruction level parallelism discussed relationship nature compiler support type processor architecture 
compilation techniques exploiting loop task level parallelism shared memory multiprocessors summarized 
locality optimizations conjunction parallelization techniques achieving high performance machines complex memory hierarchies discussed 
provide overview compilation techniques distributed memory machines perform partitioning code data parallel execution 
communication optimization code generation issues unique compilers briefly discussed 
keywords parallelism dependence testing instruction level parallelism shared memory systems distributed memory systems 
driven need deliver high levels performance wide array applications researchers developing parallel architectures compiler parallelization techniques past decades 
parallelism different levels granularity including instruction level parallelism loop task level parallelism data level parallelism considered techniques 
summarize past developments compilation technology open issues currently addressed researchers 
basis automatic detection parallelism dependence analysis 
results analysis enables compiler identify code fragments executed parallel 
great deal effort spent developing dependence tests varying complexity precision 
dependence tests compilers conservative nature test prove absence dependence reports dependence 
scientific applications major focus automatic parallelization considered programs involving array loops 
results dependence testing non scientific programs involving pointer intensive computations far encouraging 
instruction level parallelism exploited modern day long instruction word machines 
architectures depend greatly compiler uncovering parallelism 
studies shown significant levels instruction level parallelism scientific non scientific codes detection exploitation requires significant problems addressed 
essential aggressively perform code reordering long code sequences extend branches uncover schedule instruction level parallelism 
second aggressive memory disambiguation techniques employed expose form parallelism 
effective exploitation instruction level parallelism requires combination architectural features compiler support 
shared memory multiprocessors exploit parallelism executing multiple threads control execute independent processors communicate shared memory 
global name space supported systems provides uniform view memory processors simplify task writing automatically generating parallel programs 
dependence analysis techniques detect schedule loop level parallelism shared memory machines 
experience parallelization non numeric codes shared memory machines quite limited 
achieve high levels performance systems addition task parallelization important compiler pay attention paid data locality placement 
multilevel memory hierarchies supported systems 
effort scale multiprocessors large number processors distributed memory systems developed 
task compiler mapping programs written shared memory model machines complicated task 
necessary detect parallelism partition computation parallel execution shared data partitioned mapped memories associated individual processors 
complexity tasks led new constructs data distribution languages shift complexity compiler user 
cost interprocessor communication significant compiler employ variety optimization techniques tolerate communication delays 
furthermore distribution arrays memories requires new address generation techniques 
remainder organized follows 
section dependence analysis techniques surveyed 
combination compiler support accompanying architectural support required exploit instruction level parallelism described section 
techniques exploiting loop task level parallelism shared memory multiprocessors discussed section 
section unique issues compiler faces advantage distributed memory machines discussed 
concluding remarks section 
program analysis representation multiple processor system traditional sequential computer programs redesigned efficiently coordinate parallel processors 
way substantial gain computational performance may achieved 
tasks redesigning sequential program hand designing explicitly parallel programs prove difficult major approach automatic detection incorporation parallelism original sequential program 
optimizing compilers exploit limited forms parallelism sequential programs 
extending capabilities parallelizing compilers designed automatically restructure programs execution parallel architectures 
automatic detection parallelism requires thorough efficient program analysis optimization identify independent tasks program candidates parallel execution 
early programming languages developed single processor machines semantics naturally reflected underlying architecture imposes explicit order program statements executed 
total ordering restrictive necessary preserve semantics program 
fact shown concept dependence portions original ordering absolutely necessary 
dependence partial order relation statements program 
statement said depend executed order preserve semantics original program 
statements program dependent may executed order parallel 
dependence constraints program arise data considerations control considerations 
particular statements data dependent access memory location writes statements control dependent execution statement conditional results 
section discuss program analysis techniques employed compilers detect dependences sequential programs 
provide background dependence analysis intermediate program representation 
review proposed data dependence analysis tests techniques 
discuss current trends directions dependence analysis 
dependence analysis types data dependence constraints usually defined ffl flow dependence 
variable assigned statement subsequently executed statement 
ffl anti dependence 
variable statement reassigned subsequently executed statement 
ffl output dependence 
variable assigned statement reassigned subsequently executed statement 
cases order execution original program needs preserved 
anti dependence output dependence arise reassignment variables eliminated variable renaming 
flow dependence inherent computation eliminated 
control dependences derived usual control flow graph 
scalar variables traditional data flow analysis find exact data dependence relations 
arrays problem complicated compiler examine subscript expressions discussed section 
abstraction compilers represent data dependence information data dependence graph 
nodes graph correspond statements program edges correspond data dependence constraints statements 
unified framework data control dependence representation introduced form program dependence graph pdg 
pdg allows transformations vectorization special treatment control dependence performed manner uniform control data dependences 
couple intermediate program representations represent control data dependences proposed 
loops statement executed times 
data dependences flow instance execution statement statement instance back statement 
data dependence relation statement instances different iterations loop called cross iteration dependence 
data dependence relation statement instances iteration loop called intra iteration dependence 
compiler represent repeated instance statement individually extended abstraction needed 
data dependence graph constructed node statement 
case node may represent instances statement 
similarly edges graph may represent instance instance data dependence constraints 
data dependence relations annotated information relative iterations related dependent instances occur 
information stored distance direction vectors 
major focus automatic detection parallelism loop level 
situation arose important scientific computing applications rely heavily loop computations 
general sequential loop transformed parallel contain cross iteration dependences 
loop parallelism depends part resolution array aliases 
underlying problem detecting multiple array element nest loops 
data dependence analysis techniques compare pair array attempt determine subscript expressions different iterations possibly evaluate value memory location 
existing techniques assumption array subscripts linear functions loop index variables 
computation data dependences arrays non linear subscripts extremely difficult problem 
data dependence testing multidimensional arrays reduces determining system linear equations equation dimension integer solution satisfies set linear inequality constraints 
variables linear equations loop index variables inequality constraints arise loop limits direction vector relationships 
single dimensional arrays equation tested 
testing multidimensional arrays say subscript position separable loop indices occur subscripts 
different subscripts contain loop index say coupled 
subscripts separable test subscript separately linear equations system independent 
known subscript subscript testing results testing time single linear equation integer solutions satisfy set linear inequality constraints 
method introduces conservative approximation case multidimensional arrays coupled subscripts 
data dependence tests data dependence problem equivalent integer linear programming efficiently solved general 
number data dependence tests proposed literature 
test different tradeoffs accuracy efficiency 
data dependence tests approximate conservative side dependence assumed independence proved 
way unsafe parallel program produced 
gcd test 
gcd test arguably basic dependence tests incorporated dependence tests initial screen dependences 
theorem elementary number theory states linear equation integer solution greatest common divisor coefficients left hand side lhs equation evenly divides constant term right hand side 
test simply checks integer divisibility 
condition hold integer solutions linear equation dependence exists 
condition apply dependence necessarily exist 
case gcd test returns answer 
gcd test limited major respects firstly noted inexact test 
necessary sufficient condition data dependence incapable proving dependences disproving 
furthermore consider single subscript multidimensional array time 
secondly terms lhs linear equation coefficient absolute value 
case gcd coefficients lhs integer divisibility condition met 
lastly inequality constraints taken consideration test provide information regarding dependence distances directions 
spite limitations speed simplicity gcd test widely tests parallelizing compilers 
extreme value test 
extreme value test referred banerjee bounds test intermediate value theorem 
widely test calculates possible minimum maximum values expression lhs linear equation achieve bounds variables involved 
minimum maximum expression known test checks constant rhs equation falls extreme values 
dependence exists 
fall range know real solution linear equation exits 
conclude dependence exists may fact integer solution equation 
inability distinguish real integer solutions extreme value test highly efficient inexact test 
addition providing answer dependence test extreme value test generate direction vector information 
gcd test extreme value test inexact test considers single subscript multidimensional array time 
requires loop limit information known compile time 
efficiency usefulness disproving dependences common tests parallelizing compilers 
subsequent research shown certain conditions satisfied coefficients linear equation extreme value test exact test necessary sufficient condition single dependence equations 
test 
test enhances extreme value test 
extreme value test unable distinguish real integer solutions test conclusively prove disprove existence integer solutions dependences large percentage cases missed extreme value test 
test arose observation real solutions predicted extreme value test fact integer solutions 
insight led development set conditions met meant linear expression achieved integer value minimum maximum values calculated extreme value test 
accuracy conditions state necessary sufficient relationship coefficients loop iteration variables range values assume order guarantee integer value extreme values achievable 
practice conditions met frequently test cases linear time exact test single dimensional array 
test thought performing extreme value test variable time 
variable considered accuracy conditions tested 
met test continues 
variables pass test case test conclusively disproved proved dependence 
hand variables meet conditions test simply reverts extreme value test meaning disprove dependences prove 
test applied subscript subscript basis case multidimensional array 
dependence disproved considering subscripts dependence 
individual tests produce answers dependence known exist subscripts separable 
test prove existence simultaneous integer solutions case coupled subscripts returns answer 
test inherits benefits extreme value test including efficiency ability provide direction vector information 
accurate combination extreme value gcd tests accurate 
weaknesses particularly inability precisely handle multidimensional array involving coupled subscripts reliance information known compile time 
lambda test 
lambda test addresses coupled multidimensional array 
determines intersection hyperplanes represent individual subscripts array intersects convex region formed loop limits direction vector constraints 
test proceeds examining set linear combinations intersection subscript hyperplanes called canonical solutions 
hyperplane combination intersection convex region determined extreme value test 
canonical solutions intersect region answer returned 
canonical solutions intersect region lambda test returns answer 
lambda test extreme value test distinguish real integer solutions 
lambda test efficient test 
direction vectors considered examines canonical solution variable involved array 
canonical solutions examined loop bounds direction vectors number variables array 
reliance compile time information exists lambda test test extreme value test 
generalized gcd test 
generalized gcd test applies ideas euclid gcd algorithm determine simultaneous integer solutions system linear equations 
linear algebra properties test begins constructing coefficient matrix coefficient matrix number variables system number equations 
unimodular transformation matrix converts upper triangular matrix series elementary row operations gaussian elimination 
integer solution td integer solutions exist dependence assumed 
upper triangular matrix simple back substitution easily determine solution exists 
echelon form coefficients zeros 
referred free variables 
matrix product tu allows calculation additional information constant dependence distances 
generalized gcd test considers integer solutions set linear equations take constraints loop limits consideration 
useful proving existence dependences disproving 
fourier motzkin variable elimination 
technique dependence analysis tests notably omega power tests discussed section 
determine existence real solutions system linear inequalities simply treats constraints dependence vectors loop limits additional inequalities 
proceeds systematically eliminating variables system linear inequalities 
example variable selected elimination inequalities system rewritten terms upper lower bounds lower bound compared upper bound new inequality derived involve inequalities involving deleted remaining inequalities terms variable 
process continues variables eliminated contradiction reached 
contradiction detected announces dependence exists 
answer returned indicating test unsure solution integer 
worst case number inequalities system grow exponentially variables eliminated 
practice inequalities produced redundant exponential growth usually occur 
omega test 
omega test combination remainder algorithm 
produces exact answers worst case exponential time complexity 
omega test begins employing derivation knuth remainder algorithm convert system linear equalities inequalities system involving linear inequalities 
initial conversion bound normalization gcd test employed detect system inconsistent 
test reports dependences exist 
extension standard determine resulting system linear inequalities integer solutions recall prove disprove existence real solutions 
intuitively elimination variable may viewed projecting dimensional polyhedron dimensional surface 
resulting real shadow contains integers original object contains integers test reports solutions exist 
converse necessarily true real shadow may contain integers original object contains integers 
refinement added omega test consists calculating subset real shadow called dark shadow corresponds area original object integer solutions definitely exist 
dark shadow non empty contains integers omega test reports dependences exist 
real shadow non empty dark shadow empty omega test begins essentially exhaustive search solution space recursively generating solving integer programming problems integer solutions disproved 
addition omega test simplify integer programming problems symbolic projection 
integer programming problem set protected variables reduced sub problems involving variables sub problems describe values variables produce integer solutions technique omega test produce data dependence distance vectors describe relationship loop iterations data dependence occurs 
furthermore omega test handle certain cases loop bounds unknown compile time 
power test 
power test combines generalized gcd test producing test takes loop limits direction vector constraints consideration 
initially generalized gcd algorithm produce transformation matrix upper triangular matrix described earlier integer matrix produced back substitution 
loop limits direction vector information add upper lower limits free variables matrix system 
system inequalities describe upper lower bounds power test performs free variables 
power test applicable cases information unknown compile time particular involving unknown loop limits 
power test utilizes shares exponential worstcase execution time terms number free variables generalized gcd test terminated 
additionally cases integer solutions occur near loop limits cases imprecision introduced floor ceiling calculations test return conservative answer fact integer solutions exist 
interprocedural analysis 
presence procedure calls raises important practical issues relation data dependence analysis interprocedural boundaries 
simple solution expand inline procedure perform dependence analysis resulting program 
major technical difficulty case necessary reflect resulting code effect aliasing formal actual parameters 
main drawback practice size resulting code large procedures expanded 
reason techniques interprocedural data dependence analysis developed 
directions limitation program analysis inability cope statically unknown information 
existing dependence tests handle loop bounds array subscripts symbolic nonlinear expressions 
presence symbolic nonlinear expressions dependence usually assumed 
certain cases symbolic program analysis techniques help overcome problem enable effective parallelization larger class applications 
symbolic analysis limitations simply necessary information obtained inferred compile time 
order realize full potential automatic detection parallelism static program analysis techniques may complemented run time dependence analysis parallelization 
instruction level parallelism parallelism studies shown scientific non scientific codes contain high amounts instruction level parallelism ilp 
studies shown possible achieve average instruction issue rates high instructions cycle spec benchmarks 
uncovering high levels ilp requires examination instructions large instruction window containing hundreds thousands instructions 
shown relationship extracted parallelism instruction window size quadratic 
words order double degree ilp detected size instruction window 
compiler constructs large instruction window examining instructions program paths extend branches acyclic code cyclic code loop iterations 
reordering instructions placing independent instructions proximity compiler enables generation parallel schedules 
context statically scheduled long instruction word vliw machines code reordering performed independent operations explicitly scheduled execute parallel assignment long instruction compiler 
case superscalar processors actual schedule determined dynamically 
instructions small hardware instruction window typically instructions issued order exploit ilp 
code reordering performed compiler large instruction window causes additional parallelism accessible small hardware instruction window superscalar processor 
section summarize innovations compiler technology deal global scheduling acyclic program regions 
discuss role speculation briefly outline challenges posed predicated execution supported modern processors 
relationship instruction scheduling task register allocation briefly discussed 
discuss innovations area cyclic scheduling achieved software pipelining algorithms 
various formulations cyclic scheduling problem briefly discussed 
degree ilp uncovered statically compiler dynamically hardware greatly influenced effectiveness memory disambiguation techniques employed 
discuss innovations area 
current trends processor design impact compiler technology discussed 
acyclic schedulers order uncover significant levels ilp instruction window employed compiler extend branches 
acyclic schedulers employ instruction window extends branches acyclic code segment 
variety global scheduling algorithms continue developed handle task scheduling acyclic code segments program 
known global scheduling techniques include trace scheduling percolation scheduling region scheduling superblock scheduling critical path reduction 
techniques characterized scope instruction window employ program representation rely nature code reordering transformations incorporate 
techniques able perform code motion branches including speculative code motion 

trace scheduling earliest best known global instruction scheduling technique pioneered fisher 
technique instruction window limited trace sequence consecutive basic blocks path program control flow graph 
trace extend loop boundaries 
data dependence graph trace constructed scheduler list scheduler generate instruction schedule 
course scheduling instructions may propagated merge points points control flow graph 
technique critical traces representing frequently executed paths constructed scheduled prior executed frequently 
doing ensures speculative code motion performed order generate schedule trace processed paths suffer speculative code motion frequently executed current trace 
trace construction driven profile data compiler time heuristics identifying biased conditional branches 
remainder scheduling techniques discussed offer improvements trace scheduling rely core transformations introduced trace scheduling 

percolation scheduling program transformation system certain advantages trace scheduler 
trace scheduler divides program transformation stages 
stage reorders code trace second bookkeeping stage modifies rest program preserve program semantics 
separation transformation process steps lead generation redundant code bookkeeping stage 
applying semantics preserving transformations step percolation scheduling avoids problem 

region scheduling program transformation system operates program dependence graph representation program 
instruction scheduling carried reordering code control dependence region performing code motions control dependent regions 
code reordering control dependence region preference results code growth speculative code motion harm program paths 
code reordering trace scheduling enables instructions moved loop boundaries 
redundant code generated trace scheduler bookkeeping avoided 
instruction schedule progressively improved code motion transformations improvements schedule identified 

superblock scheduling scheduling technique simplifies complexity compensation code generation associated trace scheduling 
technique instruction scheduling carried superblocks created tail duplication eliminates code merge points program 
code reordering performed resulting superblocks result code motion merge points compensation code generation simpler redundant code generated trace scheduler avoided 
simplification comes cost significant code growth 

critical path reduction technique program region scheduled multiple entry multiple exit acyclic region 
region exits classified categories frequently taken infrequently taken 
paths leading frequently taken exits include delay slots attempt reduce schedule length pushing statements paths infrequently taken exits 
transformation possible path frequently taken exit may contain statements dead paths may live paths 
essentially technique integrates partial dead code elimination optimization instruction scheduler 
predication 
techniques perform speculative code motion aggressively reorder code 
mechanism enabling additional code reorderings predicated execution 
ia architecture supports predicated execution instruction conditionally executed depending value predicate input 
control flow acyclic code fragment implemented branches entirely eliminated instructions 
resulting code may viewed simultaneously execute paths original acyclic code 
situations predication quite useful 
predication may eliminate unpredictable branches consequently reduce harmful effects branches execution 
speculatively issued load frequently causes cache program path load may possible avoid speculative issue path minimize cache misses 
limited situations benefits predication clear aggressive application far challenging task 
lengths paths acyclic code vary greatly predication extend execution times shorter paths 
demand register resources may increased point predication longer yields superior instruction schedules 
data flow analysis techniques analyze programs basic tasks uncovering data dependences performing optimizations explicit control flow 
new analysis framework developed accurately handle implicit control flow expressed predicated code 
interaction register allocation 
interaction instruction scheduling register allocation important issue vliw superscalar architectures exploit significant degrees ilp 
register allocation instruction scheduling somewhat conflicting goals 
order keep functional units busy instruction scheduler exploits ilp requires large number operand values available registers 
hand register allocator attempts keep register demand low holding fewer values registers minimize need generating spill code 
register allocation performed limits amount ilp available introducing additional dependences instructions temporal sharing registers 
instruction scheduling performed create schedule demanding registers available causing register allocator 
addition spill code generated incorporated schedule scheduling pass degrading performance schedule 
effective solution integrate register allocation instruction scheduling 
significance integration greatly increased programs register demands high likelihood spill code generation high programs 
compiling programs wide issue machines need integrating register allocation instruction scheduling greatest 
number approaches proposed integrating instruction scheduling register allocation 
approaches differ complexity techniques detecting excessive resource demands strategies reducing resource demands 

fly approach approach performs local register allocation extended basic blocks instruction scheduling 
uses fly measures register needs detect excessive register demands 
part program value program referred live range value 
order reduce register demands algorithm employs live range spilling places value memory entire duration value live range 
main weakness approach perform live range splitting places value memory part time keeps register remaining duration live range 
live range splitting far superior live range spilling 

parallel interference graph approach uses extended form register interference graph detect excessive register demands guide schedule sensitive register allocation 
reduction register demands achieved live range spilling 

unified resource allocation approach measure reduce paradigm registers functional units 
reuse dag representation registers functional units identifies excessive sets represent groups instructions parallel scheduling requires resources available 
excessive sets drive reductions excessive demands resources 
live range splitting reduce register demands serialization independent instructions performed reduce excessive functional unit demands 
excessive sets eliminated simple list scheduler generate final schedule 
software pipelining instruction window ilp extracted acyclic schedulers consists paths extend loop iterations 
acyclic schedulers exploit ilp code blocks different loop iterations 
approach uncover parallelism unroll loops transform parallelism loop iterations parallelism exists single loop iteration transformed loop 
transformed loop scheduled acyclic scheduler 
approach result substantial code growth 
software pipelining exploits ilp loop iterations causing significant code growth 
software pipelining technique overlaps execution operations different loop iterations exploits ilp loop iteration boundaries 
objective software pipelining generate schedule minimizes interval iterations initiated initiation interval 
software pipelining algorithm take account instruction latencies resource availability scheduling operations loop 
addition increase register demands met avoid generation spill code inside loops 
generally techniques software pipelining assume loop body pipelined loop contains branches 
rau glaser introduced modulo scheduling commonly framework software scheduling 
approach lower bound initiation interval established data dependences loop resource demands loop 
modulo scheduler searches schedule minimum initiation interval 
search fails initiation interval increased search performed 
process repeated schedule 
manner modulo scheduler finds schedule minimum initiation interval 
technique introduced numerous variants modulo scheduling developed 
sgi compiler employs form modulo scheduler 
sets upper lower limit initiation interval performs binary search interval 
employs branch bound algorithm order search schedule initiation interval 
extensive pruning performed order ensure search schedule efficient practice 
algorithms modulo scheduling essentially heuristics 
motivated search optimal algorithms researchers formulated software pipelining integer linear programming problem 
formulations allow non pipelined pipelined functional units arbitrary structural hazards 
integer linear programming np complete problem number standard packages solving problem exist employed compute optimal schedules 
study results optimal algorithms compared sgi production compiler form modulo scheduling observed heuristics modulo scheduling employed sgi compiler highly effective 
solutions discussed represent scheduling frameworks entirely distinct acyclic schedulers 
contrast circular scheduling technique builds existing basic block schedulers construct software 
software pipelining performed code motions move selected instructions top loop body bottom loop body 
doing essentially causes instruction current iteration moved loop body instruction iteration moved loop body 
loop body viewed circular list instructions instructions moved 
techniques implemented mips production compiler 
memory disambiguation serialization memory load store operations greatly limits ilp extracted program 
order uncover significant levels ilp effective memory disambiguation techniques employed 
absence dependences store load operations allows early issuing load operations leading better memory latency tolerance 
compile time disambiguation techniques dependence analysis discussed earlier 
discuss solutions memory disambiguation problem require hardware support 
store set mechanism highly effective hardware memory disambiguator proposed superscalar processors 
basic idea technique initially assume dependences stores loads 
loads freely speculated past preceding stores 
mispredictions observed dependences detected observed dependences saved hardware structure 
group stores observed address form store set load dependent store associated store set containing store 
load linked store set encountered speculated stores belonging store set 
approach load speculation inhibited dependences observed program execution 
contrast compile time conservative nature prove absence dependence assume 
ilp scheduled statically scheduled long instruction word machines extracted compiletime performance significantly limited due conservative dependence analysis 
discussed earlier numerous dependence tests developed handling arrays 
research dependence analysis techniques presence pointers yielded equally positive results 
extraction ilp non scientific code requires look alternatives dependence analysis 
ia architecture overcomes problem innovative solution combination hardware software support 
instruction set provides pair new instructions speculative load instruction load verify instruction 
load instruction speculatively moved earlier store instructions compiler indicated speculative load instruction 
store instructions executed speculative load result load instruction marked invalid 
result load instruction load verify instruction executed 
instruction checks validity value loaded speculative load result invalid load reissued 
loaded value valid load speculation performed successfully managed hide latency load operation 
candidates load speculation may address profiling 
directions extensive research performed code optimizations research ignores factors important ilp domain 
developing optimization algorithms take account path execution frequencies machine characteristics poses significant challenge 
number new processor architectures developed today continues increase rapid rate 
particular new application domains led development wide array special purpose embedded processors 
time general purpose architectures developed today find increasingly effective means detecting exploiting high degrees ilp non scientific code 
developments created new challenges compiler community 
code optimizations ilp 
effective application code optimizations ilp domain requires main issues addressed 
suitable strategy integrating code optimizations instruction scheduling developed 
approach considered researchers interleaves application optimizations scheduling actions 
approach suggests application optimizations prior instruction scheduling new optimization algorithms aware effects optimization may register functional unit demands 
second develop new optimization algorithms take advantage path execution frequencies machine characteristics 
example researchers developed algorithms eliminating redundant code dead code frequently executed program paths 
algorithms take advantage path execution frequencies utilize speculation predication features ia architecture 
researchers begun realize information available run time compiler ability extract ilp optimize code greatly increased 
fisher describes notion walk time techniques perform code optimizations compile time run time profile data collected program runs 
general purpose architectures 
design processor architecture significantly impacts demands placed compiler exploiting ilp 
discuss current trends processor design impact complexity ilp compilers 
interestingly architectures developed range rely heavily sophisticated compiler support perform increasing number traditional compile time tasks hardware 
ia architecture supports features exploiting high degree ilp relies sophisticated compiler exploit features 
allows compiler explicitly express instruction level parallelism machine code 
ia instruction bundle contains independent operations template encodes dependence information allowing independent instructions placed multiple bundles 
ia supports full predication speculation exposed compiler 
mentioned earlier ia supports form load speculation exploited proper compiler support 
compiler responsible exposing ilp speculation predication encoding parallelism instruction bundles 
architectures proposed hand support instruction sets similar conventional hand employ long instruction word type execute engine internally 
long instruction word schedules determined run time cached reuse 
approach lowers burden compiler may lead better instruction schedules schedules dynamic information 
caching instructions repeatedly reusing suggests architectures may prime candidates incorporating low level optimizations typically left compiler 
examples optimizations include copy propagation constant propagation 
architectures combine characteristics superscalar processors long instruction word machines creative ways proposed 
illustration approach attempt incorporate load value prediction long instruction word machines 
aggressive speculation value prediction proposed approach scale performance high issue widths 
combination compiler hardware support required allow statically scheduled long instruction machines take advantage value prediction 
value profiling identify loads values loaded accurately predicted 
compiler schedules early possible execution instructions dependent loads values provided value predictor run time 
explicit checks scheduled compiler detect mispredictions 
recovery mispredictions achieved executing compensation code 
compensation code explicitly generated compiler doing results higher misprediction penalty significant code growth 
order avoid drawbacks architecture proposed automatically generates compensation code run time executed dedicated execution unit 
special purpose architectures 
wide array products ranging video games cellular phones automobiles increasingly relying specialized processors embedded control 
embedded processor may simple controller high performance dsp processor 
trend area vliw processor designs issues compiler face significantly different compiler technology developed exploiting ilp retargeted handle certain important new complexities 
compilers produce compact efficient code exploits ilp meet high levels real time performance 
addition compiler contend specialized data paths addressing modes instructions domain specific needs embedded devices 
applications executing embedded system may able meet execution speed performance requirements exploiting ilp able deliver performance tight memory requirements 
issue code density extremely important systems 
code growth accompanies existing scheduling techniques may acceptable 
jain developed code motion framework exploit ilp tms 
framework limited basic block 
existing scheduling techniques deal issue complex data paths functional units typically dsps extended incorporate instruction selection techniques lead compact code :10.1.1.48.4459
liao addressed instruction selection issue embedded dsp processors 
method covering formulation detects opportunities combining neighboring simple instructions 
limited opportunities exist restricted combining neighboring simple instructions 
expose far opportunities combining instructions code motion obeying data dependencies 
architectures ti tms dsp family provide indirect addressing modes auto decrement arithmetic 
carefully laying data variables storage address arithmetic subsumed auto increment auto decrement resulting faster denser code 
order increase opportunities efficient auto increment auto decrement addressing modes optimizing dsp compilers spam compiler delay storage allocation variables code selection phase 
code generation sequence variable accesses known storage assignment performed separate code optimization pass 
storage assignment problem studied liao 
liao formulated simple offset assignment problem soa simplified storage assignment problem single address register 
modeled problem graph theoretic optimization problem similar showed soa problem equivalent maximum weighted path covering problem proved np complete 
rao derived sequence systematic program transformations applied solves problem efficiently cases 
number approaches instruction selection scheduling proposed improve code size address problem code compaction relation storage assignment 
shared memory parallelism section summarize past current state art directions area compiler technologies exploiting shared memory parallelism 
issues related distributed memory parallelism discussed section 
compiler technologies discussed previous section enable exploitation instruction level parallelism modern processors grouping independent instructions belong sequential thread control 
shared memory parallelism additional dimension parallelism exploited executing multiple threads control shared memory multiprocessor contains multiple processors connected shared global memory 
decades symmetric shared memory multiprocessors smps evolved kind experimental machines widely available commercial systems 
fact favorable price performance characteristics smps computer systems shipped smp configuration uniprocessor configuration 
advances vlsi technologies enable entire smp integrated single chip strengthen trend making smp configurations default compared uniprocessor configurations 
shared memory parallelism attractive programming model multiprocessors supports global address space enables global data uniformly accessed processor 
greatly simplifies task writing parallel program 
obtain correct parallel program programmer needs focus correct parallelization program control logic global address space shared memory multiprocessor ensures shared data uniformly accessible threads tasks parallel program 
simplified view shared memory parallelism inadequate obtain efficient parallel program important pay attention data locality placement performance reasons 
performance considerations apply modern uniprocessors deep memory hierarchies effort required tuning performance shared memory parallel program covered effort required tuning performance sequential version program 
ways exploiting shared memory parallelism shared memory multiprocessors 
explicit parallelism 
approach user writes explicitly parallel program 
multithreading common programming model smps posix threads java threads 
programming models explicit parallelism build language constructs higher level threads doall cobegin coend 
advantage explicit parallelism programmer control exactly parallelization occurs 
disadvantage writing explicitly parallel program requires effort error prone writing sequential program possibility introducing errors concurrent data access errors due data races coordination parallel threads errors due deadlocks 
general explicit parallelism moderately successfully non numeric applications cost programmer effort 

implicit parallelism 
approach user writes sequential program compiler performs automatic parallelization compiler automatically generates shared memory parallel program sequential program 
advantage implicit parallelism reduces programming effort parallelization 
disadvantage technologies automatic parallelization limited certain classes programs typically scientific programs structured loops dense matrix data accesses 
general implicit parallelism successful approach shared memory parallelization engineering scientific programs written fortran rest section organized follows 
provide brief summary history loop parallelization techniques shared memory parallelism 
impact data locality loop transformations shared memory parallelization discussed 
outline approaches statement level task level parallelization shared memory multiprocessors go loop parallelization 
discuss directions compiler technologies shared memory parallelism 
loop parallelization loop parallelization primary focus past current research shared memory parallelization 
goal loop parallelization enable multiple iterations loop execute concurrently multiple processors shared memory multiprocessor 
programs execution time spent loops loop parallelization sufficient fully utilize processors target multiprocessor 
bulk past research loop parallelization restricted parallelization counted loops fortran loops premature exits 
earliest loop parallelization due lamport introduced hyperplane method coordinate method parallel execution iterations set perfectly nested counted loops 
framework introduced notion dependence vectors paved way linear algebra describing loop transformations legality conditions loop parallelization 
framework provides solid theoretical foundation transforming parallelizing multiple loops loop nest supercomputer hardware era late early geared exploiting specific form loop parallelism vector parallelism 
optimizing compilers era focused efforts exposing vector parallelism 
legality rules vectorization follows easily general legality rules loop parallelization 
hard problem remains compute dependence vectors sufficient precision increase set loops automatically vectorized 
path improving precision dependence vectors lay improved data dependence testing 
number data dependence tests introduced result precise sets dependence vectors turn leads improvements vectorization 
addition number new loop transformations introduced loop distribution loop fusion loop tiling went linear algebra framework introduced essential obtaining performance vector instructions 
major step technology evolution vector computers vectorizing compilers shared memory multiprocessors 
cray architecture evolved cray xmp combination vector processing processor multiprocessing processors 
experimental commercial smps began appear 
initially appeared vectorizing compiler technologies simply redirected accomplish loop parallelization smp execution 
true simple loop nests clear coarse grain nature smp parallelism led fundamental requirements necessary vectorization 
important parallelize outer loops contain conditional control flow vectorization innermost loop parallelized 
second important parallelize loops containing calls 
problem addressed control dependence relationship second problem addressed advent interprocedural analysis parallelizing compilers 
stage primary metric evaluating effectiveness shared memory parallelization compiler set loops parallelized granularity parallelized loops coarse grain parallelism usually preferable fine grain parallelism shared memory multiprocessors reduced synchronization cost 
benchmark programs focused metric comparing different vectorizing parallelizing compilers number loops vectorized parallelized 
soon apparent simple count parallelized loops inadequate predictor performance real applications take account costs data movement data locality 
initially recognition importance data locality came vectorizing compilers realized importance vector register allocation 
awareness parallelizing compilers began target modern multiprocessors cache efficiency significant performance factor processor utilization 
section discusses impact data locality loop transformations shared memory parallelization 
locality optimizations early compiler shared memory parallelization focused speedup measurements absolute execution times 
performance measurements research publications speedup curves 
general belief number processors shared memory multiprocessors keep increasing hardware technology evolved scalable speedup primary metric evaluating performance shared memory parallelization 
line thinking similar approach taken designers parallel algorithms complexity analysis focused analyzing execution time parallel algorithms asymptotic function number processors account cost data movement 
advent processors caches deep memory hierarchies early showed focusing solely speedup simplistic view performance 
ratio access time main memory times access time level cache newer machines ratio continues increase widening performance gap processor clock speeds main memory access times 
implies performance difference due locality optimizations factor larger making comparable performance difference sequential code parallelized code shared memory multiprocessors 
essential consider locality optimization conjunction parallelization 
realization set vectorizing parallelizing compilers moved targeting multiprocessors caches cray newer multiprocessors contain caches 
program exhibits locality majority memory instructions data location accessed close proximity data location accessed memory instruction executed past 
locality said temporal data location accessed location accessed previous instruction locality said spatial 
adapt locality constraints vectorizing parallelizing compilers built began extending transformation heuristics effective targeting multiprocessors caches 
extensions conflicts transformations necessary improving locality transformations necessary improving parallelization 
step develop approaches systematically express optimizations locality parallelism common framework 
set loop transformations improve locality fall classes iteration reordering statement reordering transformations 
iteration reordering loop transformation changes order loop iterations executed legal leaves loop body unchanged iteration loop treated atomic entity 
commonly iteration reordering loop transformations loop interchange loop tiling 
properly selected transformations improve data locality bringing computations access block data 
statement reordering loop transformation rearranges statements loop bodies 
commonly statement reordering transformations loop distribution loop fusion loop unrolling 
iteration reordering statement reordering loop transformations mutually exclusive conjunction 
transformations referred high level high order transformations performed source code level level intermediate language close source code level 
addition performing high level loop transformations automatically optimizing compilers feasible programmers perform high level transformations hand 
fact transformations quite extensively hand coded implementations computationally intensive kernels 
contrast hand implementation feasible approach low level optimizations instruction scheduling programmer little knowledge exact instruction sequence generated optimizing compiler hardware scheduling characteristics instructions 
unfortunately past research focused attention introducing new loop transformations problem automatically selecting combinations transformations optimize locality parallelism 
automatic selection high level transformations hard problem high level transformations invertible poor choice transformations degrade performance just effectively choice improve performance 
contrast classical compiler optimizations non invertible constant propagation expressions replaced constants vice versa difference poor optimization choice optimization choice lies amount improvement deliver 
classical optimizations essential selection high level transformations driven cost models effectively determine improvements degradations performance occur 
best transformation choices may change multiprocessors cost models evolve long run perform high level transformations hand 
notable examples compilers tools perform automatic cost driven selection transformations optimizing locality parallelism sharedmemory multiprocessors kap vast preprocessors ibm xl fortran compilers rice university system 
statement level task level parallelization mentioned earlier bulk past compiler research shared memory multiprocessors focused parallelization loops 
research projects addressed problem automatically selecting non loop statement level task level parallelism programs 
task level parallelization viewed extension global instruction scheduling problem 
global instruction scheduling goal reorder instructions multiple basic blocks independent instructions come close proximity unit parallelism single instruction 
task level parallelism goal reorder instructions partition program independent tasks unit parallelism single task may contain multiple instructions general sequential control flow loops conditionals 
goal automatic task level parallelization deliver performance benefits shared memory parallelism applications loop intensive applications containing significant amounts computation non parallelizable loops 
key issues task level parallelization desired granularity parallelism determined task scheduling synchronization overheads target multiprocessor 
overheads high desirable tasks large possible primary drawback large tasks amount parallelism may small generally division tasks may balanced 
scheduling synchronization overheads low desirable form smaller tasks exhibit better load balance 
considerations depend target multiprocessor best task partitioning performed automatically compiler performance enhancing tool 
examples automatic task partitioning past partitioning programs written functional languages sisal id haskell imperative languages fortran 
data dependence analysis developed loop parallelization applicable task level parallelization 
key issue unique task level parallelization dealing global control flow 
sequential programs written assuming sequential flow control lead control dependences addition data dependences 
example decision execute region code depend runtime predicate values prior conditional branches 
need consider control dependences data dependences motivated program dependence graph variant past task level parallelization 
directions directions compiler technologies shared memory parallelism largely motivated important research problems 
exploiting shared memory parallelism loops traditionally amenable automatic parallelization 
problem growing importance due increased availability current computer systems smp configurations trend get stronger computer systems built single chip smps 
imperative smp parallelization widely applicable current optimizations instruction scheduling register allocation necessary exploiting performance today high uniprocessors 

exploiting shared memory parallelism new programming languages java 
past shared memory parallelization largely targeted scientific programs written procedural languages fortran object oriented programming languages java gaining popularity programming tasks including scientific engineering applications 
essential parallelizing compiler technologies target programs 
trend expanding set loops amenable automatic parallelization pursue aggressive storage duplication transformations array privatization construction array ssa form 
transformations enable parallelization removing storage related dependences anti output dependences 
care required performing data transformations costs duplicating storage may outweigh benefit parallelization 
trend enabling loops executed parallel perform run time parallelization 
simple instance run time parallelization compiler generates multi version code loops unable parallelize compile time 
conditional test evaluated run time choose parallel version sequential version loop 
test includes conditions need verification run time ensure parallel version correct selected 
inspector executor framework general instance run time parallelization suitable sparse matrix computations 
research efforts way explore dynamic compilation supporting general forms run time parallelization 
logical extension run time parallelization speculative parallelization 
run time parallelization decision making loop parallelization performed followed parallel execution loop decision execute loop parallel 
speculative parallelization iterations loop executed parallel prior checking run time checking performed parallel execution determine parallelization legal 
parallelization legal speculation correct action needed 
parallelization illegal fix necessary ensure updates side effects performed iterations executed order globally committed 
idea analogous optimistic concurrency control transaction processing 
examples systems perform speculative loop parallelization 
second important research problem outlined exploiting shared memory parallelism new programming languages java 
current automatic parallelization techniques applied subsets java pressing problem dealing explicit parallelism threads locks java programs due threads locks 
java programmer explicitly create threads common example create thread performs form asynchronous wait 
importantly java programmer easily introduce locks program synchronized keyword method block statements 
parallelism java explicit new compiler technologies necessary enable efficient parallel execution shared memory multiprocessors 
challenge optimizing compilers area take java program explicit threads locks input generate output equivalent efficient parallel program uses fewer thread locks 
promising direction escape analysis identify unnecessary synchronization operations eliminated 
direction optimize structure user defined virtual threads java programs preserve semantics program fewer physical threads 
research direction especially challenging requires ability analyze optimize explicitly parallel programs illustrated 
distributed memory parallelism shared memory systems typically scale contention memory increases number processors 
distinctive characteristic distributed memory parallel systems ability scale 
systems typically constructed scalable topology torus mesh hypercube 
processor local memory cost interprocessor communication higher accessing local memories 
desired model supported machines popular shared memory model 
task generating parallel program threads execute distributed address spaces falls compiler 
potential providing scalable shared memory performance possible main obstacle realizing full potential distributed memory systems complexity programming systems 
complexity arises requirement parallelism detected managed effectively compiler simultaneously manage distributed address spaces 
task entails distribution data local memories orchestrating communication processors execute application 
effectively managing communication critical obtaining high performance cost interprocessor communication higher accessing data local memories 
spatial parallelism applications fluid flow weather modeling image processing perfectly decomposable easy map distributed memory systems 
speedups increase linearly number processors applications data accesses limited local memory 
due complexities applications practical success achieved hand parallelization codes communication managed message passing libraries 
spite tremendous amount research semiautomatic parallelization applicability compiler techniques remains limited 
order simplify task compiler significant languages user provides data distribution 
approach worked quite regular numeric applications 
success automatic parallelization distributed memory limited appears compiler play important role addressing fundamental problems regard machines 
remainder section organized follows 
discuss language design aspects compiling distributed memory systems 
important discuss approaches historical perspective compilation techniques evolved data parallel languages 
look crucial issues data code partitioning 
important techniques examined open research directions 
issues relating communication optimization discussed 
code generation problems arise specifically context distributed memory systems discussed 
perspective potential general problems solved implication effective systems 
language support attempt reduce burden code data distribution compiler new constructs incorporated variety languages 
examine language support introduced create imperative data parallel languages high performance object oriented languages functional languages 
data parallel languages 
data parallel model derives parallelism observation updates individual elements large arrays independent 
observation exposes significant degrees parallelism problems 
data parallel languages require programmer specify data decomposition allocate computation accordingly 
data parallel programming significant approaches proposed solve tough problem data distribution 
languages style typically provide aggregate array operations enable data level parallelism expressed additional constructs specifying data distributions 
number research projects commercial compilers employed data parallel programming job compiler run time system data parallel language efficiently map programs parallel hardware 
typically implementation creates parallel single program multiple data spmd parallel program owner computes rule assigns computation assignment statement processor owns variable left hand side 
approach quite successful areas dense linear algebra codes nicely mapped unclear fraction scalable applications data parallel 
spdp workshop compiler optimizations scalable parallel computing suggested scalable parallel applications data parallel 
applications converted data parallel domain program transformations remains open question 
limited forms data distribution constructs initially supported 
new distribution patterns considered researchers broaden class applications handled 
increase parallelism data demands go attention paid need scale 
object oriented languages 
object oriented solution distributed memory systems mainly centered providing library support parallel programming distributed memory space 
hpc important approach 
mode program execution explicit spmd model copies program run different contexts multiple processors 
programming model similar split distribution data shared contexts synchronization accesses data managed programmer 
hpc differs split computation context multi threaded synchronization mechanisms thread groups extends sets thread groups running multiple contexts 
addition hpc provides template library support synchronization collective parallel operations reductions remote memory 
order efficiently exploit parallelism new analysis techniques developed approaches limit run time overhead considered 
functional languages 
functional model computation attempt providing implicitly parallel programming paradigm 
lack state functionality allows compiler extract available parallelism fine coarse grain regular irregular generate partial evaluation order program 
sisal attempts provide implicitly parallel programming model 
key aspects model compiler flexibility manage memory binding values 
leads interesting optimizations update arrays place avoiding communication needed 
severe limitation model high operation counts typical functional programs run time overheads 
lack memory binding blessing problem 
example generally possible compiler perform optimizations data layouts language semantics strict lot analysis needed ascertain safety optimizations 
issues significant communication results due non local data accesses 
data code distribution communication overhead major impact performance code data partitioning decisions critical importance 
discussed language solutions partitioning largely leave problem hands programmer 
section discuss automatic techniques determining data distribution patterns minimize communication overhead 
code data partitionings general carried independently tradeoff increasing parallelism decreasing interprocessor communication performed 
research described assumes data distribution stage compiler generates spmd code appropriate message passing operations 
discuss static partitioning methods creating low communication distributions partitioning alignment distribution tiling 
briefly discuss dynamic partitioning techniques 
discussion mainly limited manner automatic distribution techniques minimize communication overhead worth mentioning important transformations limiting need communication play important role 
example transformation privatization leads localization memory 
communication free partitioning 
class methods assumption interprocessor communication offsets benefit parallelization parallelization exploits large amounts parallelism 
goal techniques construct partitionings completely eliminate interprocessor communication 
analysis relationship iteration data spaces loops performed find communication free partitionings 
key results approach summarized 
cases methods assume affine array 
methods assume loop carried dependences iteration space partitioned traversed order 
methods classified loop level statement level level partitioning performed 

loop level partitioning huang sadayappan derived necessary sufficient conditions feasibility communication free iteration space data space partitioning 
ramanujam sadayappan developed method communication free hyperplane partitioning data spaces 
computation partitioning achieved owner computes rule 
chen sheu proposed communication free data allocation strategies allows data duplication allow data duplication 

statement level partitioning shih sheu huang derived necessary sufficient conditions existence communication free statement iteration space data space partitionings 
lim lam proposed affine processor mappings communication free statement iteration space partitioning maximizing parallelism 
main drawback communication free partitioning applicable narrow class loops 
general strategy ones proposed perform tradeoff parallelism communication :10.1.1.2.3763
alignment distribution 
order expand applicability data loop partitioning larger class loops approach developed consists distinct phases alignment distribution 
alignment phase maps data computations set virtual processors organized cartesian grid dimension 
distribution phase maps virtual processors physical processors 
reason separating alignment distribution alignment dependent program characteristics independent underlying architecture distribution involves balancing computation communication relative costs depend underlying architectural model 
number heuristic solutions suggested alignment problem 
approach trades parallelism codes allow decoupling issues parallelism communication relaxing appropriate constraint problem 
pingali proposed systematic step procedure determining alignment 
constraints data computation placement determined 
constraints left unsatisfied selected 
step relaxes overconstrained system non trivial solution involving parallelism 
final step remaining system constraints solved determine data computation placement 
alignment heuristics approach developed :10.1.1.2.3763
relaxation step techniques rely may feasible important problems image processing 
order achieve communication free parallel execution applications replicate data processors 
situations resort different partitioning solution relative costs communication computation 
new approach proposed partition iteration space determining directions maximally cover communication minimally sacrifice parallelism 
resulting partition iteration space non affine results approaches hyperplanes systems linear inequalities 
particular reason confine solutions affine partitions iteration spaces long iteration partitions spanned run time overhead 
limited cases shown underlying non affine iteration space partitions spanned generating integer lattice vectors complexity 
techniques primarily focus volume communication important orchestrate structure communication utilizes aggregation 
dion robert propose approach communication minimized advantage macro communications broadcasts gathers scatters reductions decomposing general affine communication simpler components 
addresses communication aggregation issues effectively 
linear algebraic framework developed heuristic proposed minimizes non local communication optimizes residual affine communication macro communication decomposition 
tiling 
transformation essentially partitions iteration space loop nest tile blocks 
focus loop partitioning compilers increasing cache locality data reuse 
tiling useful reducing aggregating communication distributed memory systems 
agarwal address problem deriving optimal tiling parameters minimal communication loops general affine index expressions 
assume tiles atomic consider doall loops factoring issues dependencies synchronizations arise ordering iterations loop 
ramanujam sadayappan address problem compiling perfectly nested loops multicomputers tiling 
consider tiles atomic allow synchronization execution tile 
approach partitioning iteration data space communication minimized 
hollander presents partitioning algorithm identify independent iterations loops constant dependence vectors 
proposes scheduling scheme dependent iterations 
demands communication aggregation inter tile latency conflicting 
order combine communication approaches consider tile execution atomic order minimize latency may better schedule non atomic execution tiles 
consider non atomic execution tiles minimize inter tile latencies 
apply technique loops dependence distances unknown compile time 
derive optimal tile sizes constraints dependencies inter tile latencies minimize loop completion time 
main focus tiling distributed memory systems minimize communication latency 
tiling control centric transformation 
pingali argue usefulness data centric view light locality propose concept data orient loop partitioning data control 
similar argument hold tiling distributed memory systems 
fact concept data useful define related automatically discovered localized compiler 
lead efficient loop partitioning methods remains seen 
dynamic data partitioning 
static data alignment distribution typically solves problem communication loop nest loop nest basis rarely intraprocedural scope 
inter loop nest level interprocedural boundaries require dynamic data redistribution 
hudak abraham proposed method selecting redistribution points locating significant control flow changes program 
chapman zima describe design distribution tool performance prediction methods possible uses empirical performance data pattern matching process 
anderson lam approach dynamic partitioning problem heuristic combines loop nests potentially different distributions way largest potential communication costs eliminated maintaining sufficient parallelism 
bixby kennedy kremer garcia formulated dynamic data partitioning problem form integer programming problem selecting number candidate distributions loop nest constructing constraints data relations 
schreiber gilbert pugh applied graph contraction methods dynamic alignment problem reduce size problem space examined 
hall kennedy tseng defined term reaching decompositions array sections reach function call site 
palermo banerjee develop techniques automatically determine data partitions beneficial specific sections program accounting redistribution overhead 
initially static data distribution determined entire program communication graph constructed static distribution 
graph split point determined computing maximal cut splits program parts better individual distributions may exist 
redistribution costs estimated parts determine redistribution performed parts 
communication optimizations message communication costs significant communication startup overheads tend high distributed memory machines compiler attempt reduce size number messages generated 
furthermore attempt maximize overlapping computation communication 
variety communication optimizations including ones listed developed 

message vectorization optimization involves grouping communication different elements array sending large message 
typically optimization achieved replacing sending shorter messages loop larger messages outer loop 
message vectorization leads reduction message start costs hiding communication latency 

collective communication collective communication involves replacing individual point point messages broad cast multi cast order achieve efficient communication mechanisms 
especially beneficial loops transpose arrays different alignments reductions 

elimination redundant communication post pass optimization eliminates redundant communication arising alignments owner computes rule 
array sections analysis required determine unmodified sections arrays need repeatedly communicated 

overlapping communication computation technique involves hoisting communication appropriate point communication maximally overlapped computation 
separating send receive points blocking receives minimized 
frameworks global array data flow analysis allow formulation communication optimization problems proposed 
sophisticated frameworks handle complex situations proposed 
give take framework generate communication presence indirection arrays 
framework extended exploit information array sections 
agrawal framework works interprocedurally handles irregular communication 
results encouraging techniques consider network properties orchestrating communication 
crucial issue communication communication interaction result congestion 
spmd programs generated compilers typically operate distinct computation communication phases 
likelihood congestion programs higher 
communication staggering optimizations developed avoid congestion 
problem current approaches important optimization overlapping computation communication rely communication latency information estimated statically 
may better rely dynamic scheduling communication guided compiler 
compiler determine criticality message relative messages 
example short synchronizing messages critical delay may delay processors 
compiler prioritize messages effective scheduling 
flexible communication models exercise control slack producers consumers developed 
code generation issues final phase compiling distributed memory systems involves solving main code generation problems communication generation satisfy non local data processor address calculation maps global name space local memory addresses 
communication generation 
order efficiently generate parallel spmd code compiler computes quantities array sections locally allocated processor set iterations executed processor non local data accessed processor iterations access non local data iterations access local data 
representation entities important issue researched 
representations proposed represent entities communication generation 

symbolic 
representation quantities represented sets integer tuples mappings integer tuples 
integer tuple may represent array indices loop iterations processor indices 
compute sets generate code primary approach focus common special cases precompute iteration communication sets symbolically specific forms data layouts computation partitionings 
li chen describe algorithms classify communication caused general patterns generate code realize patterns efficiently target machine 
typically compilers focus providing specific optimizations aimed cases considered common important 
principal benefits case strategies conceptually simple predictable fast running times provide excellent performance cases apply 

linear inequalities 
approach code generation communication optimization problem described collection linear inequalities representing integer sets mappings 
fourier motzkin elimination simplify resulting inequalities compute range values individual index variables enumerate integer points described inequalities 
code generation amounts generating loops iterate index ranges 
representations perform certain optimizations message aggregation index set related techniques 
adve developed approach integer sets exploits speed case approaches preserving generality inequalities approaches 
consequently aggressive optimizations undertaken 
address generation 
important issue code generation communication generation important issue map global address space local address space efficiently 
address calculation mechanisms different types data distributions access patterns terms strides subscript expressions developed 
addition address calculation efficient 
number researchers thoroughly investigated problem approaches differ terms domains solve applicability efficacy solutions 

applicability techniques 
number researchers developed techniques certain combinations data distributions strides 
koelbel derived techniques compile time address calculation block cyclic distributions non unit stride accesses containing single loop index variable 
macdonald provided simple solution restricted case block sizes number processors powers 
gupta derived virtual block virtual cyclic schemes 
banerjee discuss code generation regular irregular applications 
extending applicability methods broader range distributions access patterns run time mechanism address calculations needed 
chatterjee derived run time technique identifies repeating access pattern characterized finite state machine 
log log min pk algorithm involves solution linear diophantine equations 

efficiency address calculations 
block cyclic distribution regular accesses common cases hpf programs researchers focused developing fast address calculation solutions 
techniques rely integer lattices linear algebra diophantine equations 
solutions lattices tend closed form efficient 
linear algebra framework generate code fully parallel loops hpf 
midkiff technique uses linear algebra approach enumerate local accesses processor technique similar virtual block approach gupta 
van technique requires solution linear diophantine equations 
kennedy sethi derived pk algorithm address generation 
describes access sequences periodic skewing schemes lattices derived closed forms lattices 
ramanujam closed form expressions basis vectors cases 
closed form expressions basis vectors derived non unimodular linear transformation 
wang discuss experiments address generation solutions conclude strategy described ramanujam best strategy terms speed address calculation 
far approaches relied data distribution address calculation problem mainly attempted array local address calculation 
discussed earlier useful generate non affine iteration space partitions cases 
efficiency highly dependent iteration spaces spanned lattice vectors 
code generation techniques possibly developed address issue efficiency code generation determine non affine iteration space partitioning approaches viable 
directions important deficiencies limit distributed memory systems applicability limited application domains satisfactory performance 
supporting general run time communication model may possible handle application domains requiring regular communication 
obtain superior performance model may help handling communication effectively 
model enable effective latency hiding giving sufficient flexibility compiler deferring hard decisions run time 
allow static application communication optimizations 
mentioned earlier allowing compiler communicate criticality messages run time system problem statically estimating cost communication may avoided 
probably best reason distributed memory systems potential benefit scalability application domains performance somewhat limited 
research develop scalable code generation techniques ignored 
concluding remarks reviewed developments compilation technology parallel systems identified directions research 
processors today designed exploit increasing amounts ilp research compilation techniques great practical significance 
small scale shared memory machines common place expected grow rapidly 
research effective techniques parallelization memory hierarchy management extremely important 
availability powerful single processor machines connected high speed networks common today computing environment 
research compilation techniques distributed memory machines highly relevant 
developments compilation technology continue significantly impact computing environments 
adve mellor crummey integer sets data parallel analysis optimization acm sigplan conference programming language design implementation pages montreal canada 
agarwal kranz automatic partitioning parallel loops data arrays distributed shared memory multiprocessors ieee transactions parallel distributed systems sept 
agrawal interprocedural partial redundancy elimination application distributed memory compilation ieee transactions parallel distributed systems volume number july pp 

aho sethi ullman compilers principles techniques tools addison wesley 
aiken nicolau development environment horizontal microcode ieee transactions software engineering vol 
pages may 
allen kennedy automatic translation fortran programs vector form acm transactions programming languages systems pages vol 
october 
allen kennedy vector register allocation technical report tr rice university houston tx december 
altman optimal software pipelining functional unit register constraints ph thesis mcgill university montreal quebec 
amarasinghe parallelizing compiler techniques linear inequalities computer systems laboratory stanford university january 
amarasinghe lam communication optimization code generation distributed memory machines proc 
acm sigplan conference programming language design implementation albuquerque new mexico june 
irigoin scanning polyhedra loops proc 
third acm sigplan symposium principles practice parallel programming williamsburg va april 
coelho irigoin linear algebra framework static hpf code distribution scientific programming spring 
anderson lam global optimizations parallelism locality scalable parallel machines proc 
acm sigplan conference programming language design implementation pldi pages june 
araujo malik lee register transfer paths code generation heterogeneous memory register architectures proc :10.1.1.48.4459
rd acm ieee design automation conference pages june 
araujo malik instruction set design optimization address computation dsp architectures proc 
th international symposium system synthesis pages nov 
arnold gosling java programming language addison wesley 
august hwu mahlke framework balancing control flow predication proc 
th annual international symposium microarchitecture pages research triangle park north carolina december 
banerjee dependence analysis supercomputing kluwer academic publishers norwell massachusetts 
banerjee loop transformations restructuring compilers foundations kluwer academic publishers boston ma 
banerjee chandy gupta hodges iv holm lain palermo ramaswamy su paradigm compiler distributed memory multicomputers ieee computer oct 
optimizing stack frame accesses processors restricted addressing modes software practice experience feb 
bau pingali solving alignment elementary linear algebra proc 
th workshop languages compilers parallel computing volume lecture notes computer science ny 
springer verlag 
chapman zima vienna fortran proc 
scalable high performance computing conference williamsburg va april 
beckman gannon tulip portable run time system object parallel systems proc 
th international parallel processing symposium april 
berson gupta soffa resource framework integrating register allocation local global schedulers proc 
ifip wg working conference parallel architectures compilation techniques pages 
bixby kennedy kremer automatic data layout integer programming proc 
int conf 
parallel architectures compilation techniques montr eal canada aug 
blanchet escape analysis correctness proof implementation experimental results proc 
th annual acm symposium principles programming languages pages san diego ca january 
blume eigenmann nonlinear symbolic data dependence testing ieee transactions parallel distributed systems vol 
december 
bodik gupta soffa complete removal redundant expressions proc 
acm sigplan conference programming language design implementation pages montreal canada june 
bodik gupta partial dead code elimination slicing transformations proc 
acm sigplan conference programming language design implementation pages las vegas nevada june 
burke cytron interprocedural dependence analysis parallelization proc 
sigplan symposium compiler construction pages july 
caro generating multithreaded code parallel haskell symmetric multiprocessors ph thesis massachussetts institute technology 
chakrabarti gupta 
choi global communication analysis optimization proc 
acm sigplan conference programming language design implementation philadelphia pa may 
chandy kesselman cc declarative concurrent object oriented programming notation research directions concurrent object oriented programming mit press 

chapman zima automatic support data distribution distributed memory multiprocessor systems proc 
th workshop languages compilers parallel computing volume lecture notes computer science portland aug 
springer verlag 
chatterjee gilbert schreiber alignment distribution graph languages compilers parallel computing 
sixth international workshop number lncs 
springer verlag 
chatterjee gilbert long schreiber teng generating local addresses communication sets data parallel programs journal parallel distributed computing 
chen sheu communication free data allocation techniques parallelizing compilers multicomputers ieee transactions parallel distributed systems september 
emer memory dependence prediction store sets proc 
acm ieee th international symposium computer architecture pages barcelona spain july 
conte evolutionary compilation long instruction superscalar microarchitectures exploiting parallelism levels asplos wild crazy idea session 
cooper kennedy efficient computation flow insensitive interprocedural summary information proceedings acm sigplan symposium compiler construction june 
cooper kennedy torczon impact interprocedural analysis optimization rn programming environment acm transactions programming languages systems vol 
pages october 
cooper non local instruction scheduling limited code growth proc 
languages compilers tools embedded systems pages june 
culler dusseau goldstein krishnamurthy von eicken yelick 
parallel programming split proc 
supercomputing 
dantzig fourier motzkin elimination dual journal combinatorial theory vol 

davies wolfe kap advanced source source mark iia supercomputer proc 
int conf 
parallel processing pages st charles illinois august 
dion robert optimize residual communications 
special issue journal parallel distributed computing compilation techniques distributed memory systems nov 
ia architecture ieee computer pages july 
ebcioglu altman daisy dynamic compilation architectural compatibility proc 
international symposium computer architecture pages denver colorado june 
feo report sisal language project journal parallel distributed computing vol 
october pp 

ferrante ottenstein warren program dependence graph optimization acm transactions programming languages systems vol 
pages july 
fisher trace scheduling technique global microcode compaction ieee transactions computers vol 
pages july 
fisher walk time techniques catalyst architectural change ieee computer september 
freudenberger gross lowney avoidance suppression compensation code trace scheduling compiler acm transactions programming languages systems july 
friendly patel patt putting fill unit dynamic optimizations trace cache microprocessors proc 
st annual acm ieee symposium microarchitecture pages november 
fu jennings conte value speculation scheduling high performance processors proc 
international conference architectural support programming languages operating systems pages october 
gallivan jalby meier blas linear algebra parallel processor hierarchical memory technical report csrd rpt 
center supercomputing res 
dev illinois october 
garcia novel approach automatic data distribution proc 
workshop automatic data layout performance prediction houston tx apr 
zima superb experiences research proc 
workshop languages compilers run time environments distributed memory machines north holland amsterdam netherlands 
polychronopoulos intermediate representation programs control data dependences ieee transactions parallel distributed systems vol 
march 
golf kennedy tseng practical dependence testing proceedings sigplan conference programming language design implementation toronto canada june 
gong gupta melhem compilation techniques optimizing communication distributed memory systems proc 
international conference parallel processing st charles il august 
goodman 
hsu code scheduling register allocation large basic blocks proc 
acm supercomputing conf pages 
gupta banerjee methodology high level synthesis communication multicomputers proc 
acm international conference supercomputing washington dc july 
gupta midkiff schonberg seshadri shields wang ching ngo hpf compiler ibm sp proc 
supercomputing san diego ca december 
gupta kaushik 
huang sadayappan compiling array expressions efficient execution distributed memory machines journal parallel distributed computing february 
gupta soffa region scheduling approach detecting redistributing parallelism ieee transactions software engineering vol 
pages april 
gupta code optimization side effect instruction scheduling proc 
international conference high performance computing pages bangalore india december 
gupta bodik register pressure sensitive redundancy elimination proc 
international conference compiler construction lncs springer verlag pages amsterdam netherlands gupta berson fang resource sensitive profile directed data flow analysis code optimization proc 
th annual ieee acm international symposium microarchitecture pages research triangle park north carolina december 
gupta berson fang path profile guided partial redundancy elimination speculation proc 
ieee international conference computer languages pages chicago illinois may 
gupta berson fang path profile guided partial dead code elimination predication proc 
international conference parallel architectures compilation techniques pages san francisco california november 
polychronopoulos symbolic analysis parallelizing compilers acm transactions programming languages systems vol 
july 
hank hwu rau region compilation motivation proc 
th annual ieee acm international symposium microarchitecture 
kennedy give take balanced code placement framework proc 
acm sigplan conference programming language design implementation orlando florida june 
kennedy koelbel das saltz compiler analysis irregular problems fortran proc 
th workshop languages compilers parallel computing new haven ct august 
hatcher quinn data parallel programming mimd computers mit press cambridge ma 
havlak kennedy implementation interprocedural bounded regular section analysis ieee transactions parallel distributed systems july 
hennessy patterson computer architecture quantitative approach morgan kaufmann publishers 
high performance fortran forum high performance fortran language specification version 
technical report tr center research parallel computation rice university houston tx january 
hall kennedy tseng interprocedural compilation fortran mimd distributed memory machines proc 
supercomputing minneapolis mn nov 
kennedy tseng compiling fortran mimd distributed memory machines communications acm august 
kennedy 
tseng preliminary experiences fortran compiler proceedings supercomputing portland november 
hollander partitioning labeling loops unimodular transformations ieee transactions parallel distributed systems july 

huang sadayappan communication free hyperplane partitioning nested loops journal parallel distributed computing 
hudak abraham compiling parallel loops high performance computers partitioning data assignment remapping kluwer academic pub boston ma 

hwu technology outlook predicated execution ieee computer vol 
pages january 

hwu mahlke chen chang warter hank holm superblock effective technique vliw superscalar compilation journal supercomputing may 
ibm engineering scientific subroutine library guide document sc 
irigoin supernode partitioning proc 
fifteenth acm symposium principles programming languages 
ishikawa multiple threads template library technical report tr real world computing partnership september 
jain circular scheduling new technique perform software pipelining proc 
acm sigplan conference programming language design implementation pages toronto canada june 
jain pande code motion generating compact code embedded dsps workshop compiler architecture support embedded systems washington dec 
available publications link www uc edu compiler johnson gannon hpc experiments parallel standard template library technical report tr indiana university department computer science december 
johnson schlansker analysis techniques predicated code proc 
th annual international symposium microarchitecture pages december 
kandemir shenoy banerjee choudhary minimizing data synchronization costs way communication international conference parallel processing pp 

kennedy mckinley optimizing parallelism data locality proc 
acm international conference supercomputing july 
kennedy sethi linear time algorithm computing memory access sequence data parallel programs proc 
fifth acm sigplan symposium principles practice parallel programming santa barbara ca pages july 
kennedy combining dependence data flow analyses optimize communication proc 
th international parallel processing symposium santa barbara ca april 
kennedy sethi resource communication placement analysis proc 
ninth workshop languages compilers parallel computing san jose ca august 
knobe weiss optimization techniques simd fortran compilers concurrency practice experience october 
knobe sarkar array ssa form parallelization proc 
fifth acm symposium principles programming languages san diego california january 
ahmed pingali data centric multi level blocking proc 
ofthe sigplan acm conference programming language design implementation 
koelbel compile time generation communication scientific programs proc 
supercomputing albuquerque nm pages november 
kong test improved dependence test automatic parallelization vectorization ieee transactions parallel distributed systems special issue parallel languages compilers vol 
july 
knuth art computer programming vol 
algorithms addison wesley 
kuck associates kap ibm fortran user guide version document champaign il 
lamport parallel execution loops communications acm pages vol 
february 
lee midkiff padua concurrent static single assignment form constant propagation explicitly parallel programs proc 
th international workshop languages compilers parallel computing lncs springer verlag minneapolis mn august 
li chen compiling communication efficient programs massively parallel machines ieee transactions parallel distributed systems july 
li yew efficient interprocedural analysis program parallelization restructuring proceedings acm sigplan symposium parallel programming new haven ct july 
li yew zhu efficient data dependence analysis parallelizing compilers ieee transactions parallel distributed systems vol 
january 
liao storage assignment decrease code size acm transactions programming languages systems vol 
pages may 
liao instruction selection covering code size optimization proc 
international conference computer aided design 
lim lam communication free parallelization affine transformations proc 
th workshop languages compilers parallel computing august 
lipasti wilkerson shen value locality load value prediction proc 
th international conference architectural support programming languages operating systems pages cambridge massachusetts 
macdonald meltzer addressing cray research mpp fortran proceedings rd workshop compilers parallel computers vienna austria pages july 
mckinley carr 
tseng improving data locality loop transformations acm transactions programming languages systems vol 
pages july 
midkiff local iteration set computation block cyclic distributions proc 
international conference parallel processing 
vol 
ii pages august 
breach sohi dynamic speculation synchronization data dependences proc 
th international symposium computer architecture 
nair hopkins exploiting instruction level parallelism processors caching scheduled groups proc 
international symposium computer architecture pages denver colorado june 
gupta soffa value prediction vliw machines proc 
acm ieee th international symposium computer architecture atlanta georgia may 
norris pollock scheduler sensitive global register allocator proceedings supercomputing pages portland oregon 
gupta superscalar execution direct data forwarding proc 
international conference parallel architectures compilation techniques pages paris france october 
heine 
liao lam olukotun software hardware exploiting speculative parallelism multiprocessor stanford university computer systems lab technical report csl tr february 
padua wolfe advanced compiler optimizations supercomputers communications acm december 
palermo compiler techniques optimizing communication data distribution distributed memory multicomputers ph thesis dept electrical computer eng univ urbana il june 
palermo hodges iv banerjee dynamic data partitioning distributed memory multicomputers journal parallel distributed computing nov 
pande bali computation communication load balanced loop partitioning method distributed memory systems journal parallel distributed computing appear 
pande agrawal compilation techniques distributed memory systems guest editorial special issue journal parallel distributed computing compilation techniques distributed memory systems pages november 
pande compile time partitioning method doall loops distributed memory systems international conference parallel processing ieee computer society press 
vol 
iii pages 
pande ramanujam robert workshop challenges compiling scalable parallel systems th ieee symposium parallel distributed systems october 
pingali beck johnson dependence flow graphs algebraic approach program dependences proceedings acm symposium principles programming languages january 
banerjee wolfe gcd tests exact data dependence information journal parallel distributed computing vol 
february 
kong accuracy banerjee test journal parallel distributed computing special issue shared memory multiprocessors vol 
june 
pande empirical study test exact data dependence proceedings international conference parallel processing st charles il august 
kong direction vector test ieee transactions parallel distributed systems vol 
november 
pugh practical algorithm exact array dependence analysis communications acm vol 
august 
sadayappan compile time techniques data distribution distributed memory machines ieee transactions parallel distributed systems october 
sadayappan tiling multidimensional iteration spaces multicomputers journal parallel distributed computing 
ramanujam dutta code generation complex subscripts data parallel programs proc 
th workshop languages compilers parallel computing minneapolis mn springer verlag 
rao pande optimal task scheduling minimize inter tile latencies international conference parallel processing pages 
rau glaser scheduling techniques easily schedulable horizontal architecture high performance scientific computing proc 
th annual microprogramming workshop pages mass october 
rauchwerger padua lrpd test speculative run time parallelization loops privatization reduction parallelization proc 
acm sigplan conference programming language design implementation june 
van sips implementation framework hpf distributed arrays message passing parallel computer systems ieee transactions parallel distributed systems september 
pacific sierra research vast xl fortran user guide edition document number va santa monica ca 
rao pande storage assignment optimizations generate compact efficient code embedded dsps acm sigplan conference programming language design implementation atlanta 
gao lichtenstein software pipelining showdown optimal vs heuristic methods production compiler proc 
acm sigplan conference programming language design implementation pages philadelphia pennsylvania may 
optimizing cm fortran compiler connection machine computers journal parallel distributed computing november 
saltz crowley run time scheduling execution loops message passing machines journal parallel distributed computing vol 
april 
sarkar partitioning scheduling parallel programs multiprocessors pitman london mit press cambridge massachusetts 
sarkar automatic partitioning program dependence graph parallel tasks ibm journal research development vol 

sarkar thekkath general framework iteration reordering loop transformations proc 
acm sigplan conference programming language design implementation pages san francisco california june 
sarkar automatic selection high order transformations ibm xl fortran compilers ibm journal research development vol 
may 
sarkar analysis optimization explicitly parallel programs parallel program graph representation proc 
th international workshop languages compilers parallel computing lncs springer verlag minneapolis mn august 
vectorizing fortran compiler ibm journal research development vol 
pages march 
schlansker critical path reduction scalar programs th annual ieee acm international symposium microarchitecture nov 
schreiber gilbert pugh efficient distribution analysis graph contraction proc 
th workshop languages compilers parallel computing lecture notes computer science pages columbus oh aug 
springer verlag 

shih 
sheu 
huang statement level communication free partitioning techniques parallelizing compilers proc 
th workshop languages compilers parallel computing august 
snir communication software parallel environment ibm sp ibm systems journal 
spam research group 
spam compiler user manual sept 
www ee princeton edu spam 
srinivasan optimizing explicitly parallel programs ph thesis department computer science university colorado denver colorado 
hallaron gross generating communication array statements design implementation evaluation journal parallel distributed computing april 
subramanian pande efficient program partitioning compiler controlled communication proc 
fourth international workshop high level parallel programming models supportive environments appear 
optimization embedded dsp programs post pass data flow analysis proc 
international conference acoustics speech signal processing apr 
malik 
memory bank register allocation software synthesis asips proc 
international conference computer aided design pages 
ramanujam fast address sequence generation data parallel programs integer lattices proc 
languages compilers parallel computing lecture notes computer science pages springer verlag 
ramanujam efficient computation address sequences data parallel programs closed forms basis vectors journal parallel distributed computing november 
gupta soffa dataflow analysis driven dynamic data partitioning fourth workshop languages compilers run time systems scalable computers lncs springer verlag pages pittsburgh pa may 
traub culler schauser global analysis partitioning non strict programs sequential threads acm conference lisp functional programming san francisco ca june 
irigoin feautrier direct parallelization call statements proceedings sigplan symposium compiler construction pages july 
tu padua array privatization shared distributed memory machines proc 
nd workshop languages compilers run time environments distributed memory machines acm sigplan notices january 
wang chatterjee runtime performance parallel array assignment empirical study proc 
supercomputing pittsburgh pa november 
data organization parallel computers kluwer academic publishers 
wilson suif parallelizing optimizing research compiler sigplan notices vol 
pages december 
wolf lam loop transformation theory algorithm maximize parallelism ieee transactions parallel distributed systems vol 
pages october 
wolfe optimizing supercompilers supercomputers pitman london mit press cambridge massachusetts 
wolfe iteration space tiling memory hierarchies proc 
rd siam conf 
parallel processing scientific computing pages 
wolfe tseng power test data dependence ieee transactions parallel distributed systems vol 
september 
zima 
bast superb tool semi automatic mimd simd parallelization parallel computing 

