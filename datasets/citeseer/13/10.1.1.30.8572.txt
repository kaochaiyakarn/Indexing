technical report april statistics department university california berkeley ca 
bias variance arcing classifiers leo breiman statistics department university california berkeley ca leo stat berkeley edu shown combining multiple versions unstable classifiers trees neural nets results reduced test set error 
study concepts bias variance classifier defined 
unstable classifiers universally low bias 
problem high variance 
combining multiple versions variance reducing device 
effective bagging breiman modified training sets formed resampling original training set classifiers constructed training sets combined voting freund schapire propose algorithm basis adaptively resample combine acronym arcing weights resampling increased cases combining done weighted voting 
arcing successful bagging variance reduction 
explore arcing algorithms compare bagging try understand arcing works 
partially supported nsf 
classification regression methods unstable sense small perturbations training sets construction may result large changes constructed predictor 
subset selection methods regression decision trees regression classification neural nets unstable breiman 
unstable methods accuracy improved perturbing combining 
generating multiple versions predictor perturbing training set construction method combining versions single predictor 
instance ali generates multiple classification trees choosing randomly best splits node combines trees maximum likelihood 
breiman adds noise response variable regression generate multiple subset regressions averages 
generic perturb combine designate group methods 
effective methods bagging breiman 
bagging perturbs training set repeatedly generate multiple predictors combines simple voting classification averaging regression 
training set consist cases instances labeled put equal probabilities case probabilities sample replacement bootstrap times training set forming resampled training set cases may appear may appear 
construct predictor repeat procedure combine 
bagging applied cart gave dramatic decreases test set errors 
freund schapire proposed algorithm designed drive training set error rapidly zero 
algorithm run far past point training set error zero gives better performance bagging number real data sets 
crux idea start resample form training set sequence classifiers training sets built increase cases frequently 
termination combine classifiers weighted simple voting 
refer algorithms type adaptive resampling combining arcing algorithms 
honor freund schapire discovery denote specific algorithm arc fs discuss theoretical efforts relate training set test set error appendix 
better understand stability instability bagging arcing section define concepts bias variance classifiers appendix discusses definitions 
difference test set error classifier minimum error achievable sum bias variance 
unstable classifiers trees characteristically high variance low bias 
stable linear discriminant analysis low variance high bias 
illustrated artificial data 
section looks effects arcing bagging trees bias variance 
main effect bagging arcing reduce variance 
arcing usually better bagging arc fs complex things behavior puzzling 
variance reduction comes adaptive resampling specific form arc fs 
show define simpler arc algorithm denoted arc accuracy comparable arc fs 
appear opposite poles arc spectrum arc ad hoc demonstrate arcing works specific form arc fs algorithm adaptive resampling 
freund schapire compare arc fs bagging data sets conclude arc fs small edge test set error rates 
tested arc fs arc bagging real data sets bagging get results favorable arcing 
section 
arc fs arc finish dead heat 
data sets little better significantly better bagging 
look arcing bagging applied postal service digit data base 
results arcing exciting turns great classifier cart procedure get close lowest achievable test set error rates 
furthermore arc classifier shelf 
performance depend tuning settings particular problems 
just read data press start button 
neural net standards fast construct 
section gives results experiments aimed understanding arc fs arc 
algorithm distinctive different signatures 
generally arc fs uses smaller number distinct cases resampled training sets successive values highly variable 
successive training sets arc fs rock back forth convergence final set 
back forth rocking arc convergence final 
variability may essential ingredient successful arcing algorithms 
instability essential ingredient bagging arcing improve accuracy 
nearest neighbors stable breiman noted bagging improve nearest neighbor classification 
linear discriminant analysis relatively relatively stable low variance section experiments show bagging arcing effect linear discriminant error rates 
section contains remarks mainly aimed understanding bagging arcing 
reason bagging reduces error fairly transparent 
clear general terms arcing works 
dissimilar arcing algorithms arc fs arc give comparable accuracy 
possible arcing algorithms intermediate fs arc give better performance 
experiments freund shapire drucker cortes quinlan indicate arcing decision trees may lead fast universally accurate classification methods indicate additional research aimed understanding workings class algorithms high pay 

bias variance classifier order understand methods studied article function helpful define bias variance classifier 
terms originate predicting numerical outputs look defined regression 
bias variance regression terms bias variance regression come known decomposition prediction error 
training set 
numerical outputs multidimensional input vectors method neural nets regression trees linear regression applied data set construct predictor values 
assume training set consists iid samples distribution samples drawn distribution 
define squared error pe subscripts indicate expectation respect holding fixed 
pe expectation pe decompose 

define bias variance bias var get fundamental decomposition pe ee bias var point contribution error bias variance points bias variance 
generally point contributions positive 
decomposition useful understanding properties predictors 
usually family functions defined selected function having minimum squared error training set 
small instance set linear functions fairly nonlinear bias large 
selecting small set functions estimating small number parameters variance low 
large family functions set functions represented large neural net binary decision trees bias usually small variance large 
illuminating discussion problem context neural networks geman bienenstock doursat 
cure bias known linear regression practitioner enlarge size family add quadratic interaction terms doing bias decreased variance goes 
may partial high variance 
consider aggregated predictor 
definition bias zero variance 
approximate get predictor reduced variance 
see simple idea carries classification 
bias variance classification classification output variable 
class label 
training set form 
class labels 
method construct classifier predicting values 
assume data training set consists iid selections distribution error defined pe denote pe expectation pe denote dx dx minimum rate bayes classifier argmax rate pe max dx 
defining bias variance regression key ingredient definition aggregated predictor 
different definition useful classification 
define aggregated classifier ca argmax 
aggregation voting 
consider independent replicas 
construct classifiers 
determine classification ca having multiple classifiers vote popular class 
definition unbiased ca unbiased replications picks right class class 
classifier unbiased necessarily accurate classifier 
instance suppose class problem 
unbiased correct classification 
bayes predictor probability correct classification 
unbiased ca optimal 
set unbiased 
complement called bias set denoted define definition bias classifier bias variance var leads fundamental decomposition pe pe bias var note aggregating classifier replacing ca reduces variance zero guarantee reduce bias 
fact easy give examples bias increased 
bias set large probability pe may significantly larger pe 
defined bias variance properties bias variance non negative 
variance ca zero 
deterministic depend variance zero 
bias zero 
proofs immediate definitions 
variance expressed var max dx bias similar integral clearly bias variance non negative 
ca variance zero 
deterministic zero variance 
clear zero bias 
distinction bias variance regression classification point bias set variance set 
bias set variance zero 
variance set bias zero 
reflects difference classification regression 
classification get right wrong 
regression 
error continuous 
see appendix remarks definition bias variance 
instability bias variance breiman pointed prediction methods unstable small changes training set cause large changes resulting predictors 
listed trees neural nets unstable nearest neighbors stable 
linear discriminant analysis lda stable 
unstable classifiers characterized high variance 
changes classifiers differ markedly aggregated classifier ca 
stable classifiers change replicates ca tend variance small 
procedures trees high variance average right largely unbiased optimal class usually winner popularity vote 
stable methods lda achieve stability having limited set models fit data 
result low variance 
data adequately represented available set models large bias result 
examples illustrate compute bias variance cart examples 
consist artificially generated data computed replicated 
example classes equal probability training sets cases 
waveform dimension class data 
described cart book breiman code generating data uci repository 
pe ii twonorm dimension class data 
class drawn multivariate normal distribution unit covariance matrix 
class mean 
class mean 

pe 
iii dimension class data 
class drawn equal probability unit multivariate normal mean 
unit multivariate normal mean 

class drawn unit multivariate normal mean 

pe 
iv ringnorm dimension class data class multivariate normal mean zero covariance matrix times identity 
class unit covariance matrix mean 
pe 
monte carlo techniques compute bias variance 
results table 
table bias variance error cart data set bias variance error waveform twonorm ringnorm problems difficult cart 
instance twonorm optimal separating surface oblique plane 
hard approximate multidimensional rectangles cart 
ringnorm separating surface sphere difficult rectangular approximation 
difficult separating surface formed continuous join oblique hyperplanes 
examples cart low bias 
problem variance 
explore sections methods reducing variance combining cart classifiers trained perturbed versions training set 
trees grown default options cart 
special parameters set done optimize cart data sets 

bias variance arcing bagging ubiquitous low bias tree classifiers variances reduced accurate classifiers may result 
general direction reducing variance indicated classifier ca classifier zero variance low bias 
specifically problems bias 
nearly optimal 
recall generating independent replicates constructing multiple classifiers replicate training sets letting classifiers vote popular class 
possible real data generate independent replicates training set 
imitations possible 
bagging simplest implementation idea generating quasi replicate training sets bagging breiman 
define probability nth case training set sample times distribution 
equivalently sample replacement 
forms resampled training set 
cases may appear may appear 
called bootstrap sample denote distribution iid repeat sampling procedure getting sequence independent bootstrap training sets 
form classifiers training sets vote classes 
ca really depends underlying probability training sets drawn ca ca 
bagged classifier ca 
hope approximation ca considerable variance reduction result 
arcing arcing complex procedure 
multiple classifiers constructed vote classes 
construction sequential construction st classifier depending performance previously constructed classifiers 
give brief description freund schapire arc fs algorithm 
details contained section 
start construction probability distribution cases training set 
training set constructed sampling times distribution 
probabilities updated depending cases classified 
factor defined depends rate smaller larger nth case put weight bp case 
define weight 
divide weight sum weights get updated probabilities round sampling 
fixed number classifiers constructed weighted voting class 
intuitive idea arcing points selected replicate data sets 
troublesome points focusing adaptive resampling scheme arc fs may better neutral bagging approach 
results bagging arc fs run artificial data set described 
results table compared cart results 
table 
bias variance data set cart bagging arcing waveform bias var twonorm bias var bias var ringnorm bias var bagging arcing reduce bias bit major contribution accuracy large reduction variance 
arcing better bagging better variance reduction 
effect combining classifiers 
experiments bagging arcing combinations tree classifiers 
natural question happens classifiers combined 
explore ran bagging waveform twonorm data combinations trees 
run consisted repetitions 
run training set test set generated prescribed number trees constructed combined test set error computed 
errors averaged repetitions give results shown table 
standard errors average table test set error combinations data set waveform arc fs bagging twonorm arc fs bagging arc fs error rates decrease significantly combination reaching rates close bayes minimums waveform twonorm 
bagging error rates decrease markedly 
standard comparison linear discriminant analysis optimal twonorm 
error rate averaged repetitions 

arcing algorithms section specifies arc algorithms looks performance number data sets 

definitions arc algorithms 
algorithms proceed sequential steps user defined limit steps termination 
initialize probabilities equal 
step new training set selected sampling original training set probabilities 
classifier resampled training set constructed updated depending step 
termination classifiers combined weighted arc fs unweighted arc voting 
arc fs algorithm boosting theorem freund schapire 
arc ad hoc invention 
arc fs specifics kth step current probabilities sample replacement get training set construct classifier 
ii run classifier nth case classified incorrectly zero 
iii define updated st step probabilities sp steps 
combined weighted voting having weight log 
revisions algorithm necessary 
equal great original schapire algorithm exits construction loop 
better results gotten setting equal restarting 
happened frequently soybean data set 
equals zero making subsequent step undefined set probabilities equal restart 
arc specifics arc fs ii run classifier number nth case 
iii updated step probabilities defined steps 
combined unweighted voting 
training set selected sampling probabilities set generated way 
tree construction test set pruning 
eliminating need cross validation pruning classification trees grown pruned cpu time takes trees grown pruned fold crossvalidation 
true bagging 
arcing bagging applied decision trees grow classifiers relatively fast 
parallel bagging easily implemented arc essentially sequential 
arc devised 
testing arc fs suspected success lay specific form adaptive resampling property increasing weight placed cases frequently 
check tried simple update schemes probabilities 
update form tested waveform data 
best arc 
higher values tested improvement possible 
experiments data sets 
experiments moderate sized data sets larger ones bagging breiman plus handwritten digit data set 
data sets summarized table 
table data set summary data set training test variables classes heart breast cancer ionosphere diabetes glass soybean letters satellite shuttle dna digit data sets heart data uci repository 
brief breiman 
procedure data sets sets consisted iterations steps select random training set set aside test set 
ii run arc fs arc remaining data generating classifiers 
iii combine classifiers get error rates test set 
error rates computed iii averaged iterations get final numbers shown table 
larger data sets came separate test training sets 
arcing algorithms generate classifiers digit data combined final classifier 
test set errors shown table 
table test set error data set arc fs arc bagging cart heart breast cancer ionosphere diabetes glass soybean letters satellite shuttle dna digit larger data sets statlog project michie compared classification methods 
results arc fs ranks best barely edged place dna 
arc close 
digit data set famous postal service data set preprocessed le cun result grey scale images 
data set test bed adventures classification bell laboratories 
best layer neural net gets error rate 
layer network gets 
hastie tibshirani deformable prototypes get error 
smart metric nearest neighbors gives lowest error rate date simard 
classifiers specifically tailored data 
interesting sv machines described vapnik shelf require specification parameters functions 
lowest error rates slightly 
arcing algorithms cart requires reading training set arc fs gives accuracy competitive hand crafted classifiers 
relatively fast 
trees constructed arc fs took hours cpu time sparc 
reprogramming get hour cpu time 
looking test set error results little choose arc fs arc 
arc slight edge smaller data sets arc fs little better larger ones 

properties arc algorithms experiments carried smaller sized data sets listed table plus artificial waveform data 
arc fs arc lengthy runs data set generating sequences trees 
run information various characteristics gathered 
information better understand algorithms similarities differences 
arc fs arc probably stand opposite extremes effective arcing algorithms 
arc fs constructed trees change considerably construction 
arc changes gradual 
preliminary results resampling equal probabilities training set cases appear resampled data set put way data 
adaptive resampling weight cases data 
table gives average percent data arc algorithms constructing classifier sequence 
third column average value beta arc fs algorithm constructing sequence 
table percent data data set arc arc fs av 
beta waveform heart breast cancer ionosphere diabetes glass soybean arc data ranges 
arc fs uses considerably smaller fractions data ranging breast cancer data set cases tree 
average values beta surprisingly large 
instance breast cancer data set training set case lead amplification unnormalized weight factor 
shuttle data leads extreme results 
average data constructing arc fs tree sequence average value beta 
variability signature variability characteristic differed significantly algorithms 
signature derived follows run kept track average value run equal bagging average values 
standard deviation computed 
gives plots standard deviations vs averages data sets algorithm 
upper point cloud graph corresponds arc fs values lower arc values 
graph soybean data set shown frequent restarting causes arc fs values anomalous 
arc fs standard deviations generally larger average increase linearly average 
larger volatile contrast standard deviations arc quit small increase slowly average 
range arc fs times larger arc 
note modulo scaling shapes point sets similar data sets 
mysterious signature run kept track number times nth case appeared training set number times 
algorithms frequently point probability increases training set 
intuitively obvious graphs 
data set number times plotted vs number times training set 
plots arc behave expected 
arc fs 
plots rise sharply plateau 
plateau change rate vs rate training set 
fortunately mysterious behavior rational explanation terms structure arc fs algorithm 
assume iterations constant equal experiments values bk moderate sd mean values large 
proportion times nth case 
kr kr max set indices cardinality small increasing numbers cases accurately classified training sets drawn rates increase get close 
illustrate shows rates function number iterations cases twonorm data discussed subsection 
top curve case consistently large 
lower curve case vanishingly small 
number cases accurately classified training sets drawn characterized lower values rate small 
cases cluster axes 
insight provided 
percentile plot proportion training sets cases twonorm data iterations 
cases small number train sets 
rest uniform distribution proportion training sets 
hard classify points get weight 
explore question twonorm data 
ratio probability densities classes point depends value vector coordinates 
smaller closer ratio densities difficult point classify 
idea underlying arc algorithms valid probabilities inclusion resampled training sets increase decreases 
plots average iterations vs arc algorithms 
av generally increases decreasing relation noisy 
confounded factors able pinpoint 

linear discriminant analysis isn improved bagging arcing 
linear discriminant analysis lda fairly stable low variance come surprise test set error significantly reduced bagging arcing 
test bed data sets table 
ionosphere soybean eliminated class covariance matrix singular full training set ionosphere bagging arc fs training sets soybean 
experimental set similar section 
leave test set repetitions run linear discriminant analysis test set errors averaged 
repeated repetition combinations linear discriminants built bagging arc fs 
test set errors combined classifiers averaged 
results listed table 
table linear discriminant test set error 
data set lda lda bag lda arc restart freq 
heart breast cancer diabetes glass recall arc fs construction restarted equal 
column table indicates restarting occurred 
instance heart data average occurred times 
contrast runs combining trees restarting encountered soybean data 
frequency restarting consequence stability linear analysis 
procedure stable cases tend changing training sets 
weights increase weighted training set error 
results illustrate linear discriminant analysis generally low variance procedure 
fits simple parametric normal model change replicate training sets 
problem bias wrong consistently wrong simple model hope generally low bias 

remarks bagging aggregate classifier depends distribution samples selected number selected 
letting dependence implicit denote ca ca 
mentioned bagging replaces ca ca hope approximation produce variance reduction 
best discrete estimate distribution usually smoother spread interesting question better approximation produce 
check possibility simulated data sets described section 
training set drawn distributions replaced spherical normal distribution centered bootstrap training set iid drawn smoothed distribution 
values tried sd normal smoothing best adopted 
results table 
table smoothed estimate bagging test set errors data set bagging bagging smoothed arcing waveform twonorm ringnorm pe values smoothed estimates show better approximation lower variance 
limits estimate unknown underlying distribution training set 
aggregated classifiers smoothed approximations variances significantly zero doubt efforts refine estimates push lower 
note better approximation bagging arcing 
arcing arcing transparent bagging 
freund schapire designed arc fs drive training set error rapidly zero remarkably 
context arc fs designed gives clues ability reduce test set error 
instance suppose run arc fs exit construction loop training set error zero 
test set errors average number combinations exit loop table compared results table 
ran bagging data sets table exiting loop training error zero kept track average number combinations exit test set error 
numbers table soybean restarting problems 
table test error exit times arc fs data set error exit time heart breast cancer ionosphere diabetes glass letters satellite shuttle dna table test error exit times bagging data set error exit time heart breast cancer ionosphere diabetes glass results delineate differences efficient reduction training set error test set accuracy 
arc fs reaches zero training set error quickly average tree constructions 
accompanying test set error higher bagging takes longer reach zero training set error 
produce optimum reductions test set error arc fs run far past point zero training set error 
arcing classifier expressible aggregated classifier approximation distributions successive training sets drawn change constantly procedure continues 
arc fs algorithm successive form multivariate markov chain probably stationary distribution dp 
probability training sets drawn original training set distribution cases 
steady state unweighted voting class gets vote dp 
clear steady state probability structure relates error reduction properties arcing 
importance suggested experiments 
results table show arcing takes longer reach minimum error rate bagging 
error reduction properties arcing come steady state behavior longer reduction time may reflect fact dependent markov property arc fs algorithm takes longer reach steady state bagging independence successive bootstrap training sets law large numbers sets quickly 
steady state behavior arcing algorithms relates drive training set error zero iterations unknown 
know arcing derives power ability adaptive resampling reduce variance 
illustrated arc simple algorithm expressly show thing arcing explicit form arc fs general idea adaptive resampling really nice idea focusing cases harder classify 
arc fs better bagging votes right 
surmise votes right way hard points bagging votes wrong way 
complex aspect arcing illustrated experiments done date 
diabetes data set gives higher error rate single run cart 
freund schapire quinlan experiments tree structured program similar cart compared arc fs bagging classifiers 
data sets examined experiments arc fs test set error larger 
occur bagging 
understood arc fs causes infrequent degeneration test set error usually smaller data sets 
conjecture may caused outliers data 
outlier consistently probability sampled continue increase az arcing continues 
start appearing multiple times resampled data sets 
small data sets may warp 
arc fs arcing algorithms function reduce test set error wide variety data sets improve classification accuracy methods cart point best available shelf classifiers 
freund schapire discovery adaptive resampling embodied arc fs creative idea lead interesting research better understanding classification works 
arcing algorithms rich probabilistic structure challenging problem connect structure variance reduction properties 
clear optimum arcing algorithm look 
arc fs devised different context arc ad hoc 
better understanding arcing functions lead improvements 

acknowledgments am indebted yoav freund giving draft papers referred article yoav freund robert schapire informative email interchanges help understanding boosting context trevor hastie making available preprocessed postal service data harris drucker responded generously questioning nips subsequent comparing arc fs bagging convinced arcing needed looking tom dietterich comments draft david wolpert helpful discussions boosting area relevant papers published 
addresses obtained electronically 
ali learning probablistic relational concept descriptions thesis computer science university california irvine breiman bagging predictors press machine learning ftp stat berkeley edu users pub breiman breiman heuristics instability model selection press annals statistics ftp stat berkeley edu users pub breiman breiman friedman olshen stone classification regression trees chapman hall dietterich kong error correcting output coding corrects bias variance proceedings th international conference machine learning pp 
morgan kaufmann 
ftp ftp cs orst edu tgd papers ml ps gz drucker cortes boosting decision trees appear neural information processing morgan kaufmann ftp ftp edu pub drucker 
ps freund schapire decision theoretic generalization line learning application boosting 
www research att com orgs ssr people yoav www research att com orgs ssr people schapire freund schapire experiments new boosting algorithm appear machine learning proceedings thirteenth international conference july 
friedman bias variance loss curse dimensionality geman bienenstock doursat neural networks bias variance dilemma 
neural computations hastie tibshirani handwritten digit recognition deformable prototypes ftp stat stanford edu pub hastie zip ps kearns valiant learning boolean formulae finite automata hard factoring technical report tr harvard university aiken computation laboratory kearns valiant limitations learning boolean formulae finite automata 
proceedings annual acm symposium theory computing acm press 
kohavi wolpert bias plus variance decomposition zero loss functions ftp stanford edu pub ronnyk ps le cun boser denker henderson howard hubbard jackel handwritten digit recognition back propagation network touretzky ed 
advances neural information processing systems vol morgan kaufman michie spiegelhalter taylor machine learning neural statistical classification ellis horwood london quinlan bagging boosting appear proceedings aaai national conference artificial intelligence www cs su oz au quinlan schapire strength weak learnability machine learning simard le cun denker efficient pattern recognition new transformation distance advances neural information processing systems morgan kaufman tibshirani bias variance prediction error classification rules ftp toronto edu pub ps vapnik nature statistical learning theory springer appendix bias variance definitions part early flurry activity concerned definitions bias variance classifiers stimulated circulation draft 
draft different definition bias variance call definition 
bias classifier bias pe pe variance var pe pe definition variance proposed earlier dietterich kong 
defined bias pe arriving different decomposition pe 
notes variance defined negative 
kohavi wolpert criticized definition possibility variance negative grounds assign zero variance deterministic classifiers 
give different definition bias variance 
definition bias generally positive 
tibshirani defined bias way definition defined variance ca explored methods estimation bias variance terms 
considering various suggestions criticisms exploring cases variance defined definition negative formulated definition section 
gives correct intuitive meaning bias variance drawback negative variance 
additional support comes friedman 
ms contains thoughtful analysis meaning bias variance class problems 
simplifying assumptions definition boundary bias point shown points negative boundary bias classification error reduced reducing variance class probability estimates 
boundary bias negative decreasing estimate variance may increase classification error points negative boundary bias exactly points defined variance set 
appendix boosting context arc fs freund schapire designed arc fs drive training error rapidly zero 
connected training set property test set behavior ways 
structural risk minimization see vapnik 
idea bounds test set error terms training set error bounds depend vc dimension class functions construct classifiers 
bound tight approach contradictory consequence 
stopping soon training error zero gives complex classifier lowest vc dimension test set error corresponding stopping rule lower continue combine classifiers 
table shows hold 
second connection concept boosting 
freund schapire devised arc fs context boosting theory see schapire named adaboost 
follow freund setting definitions assume input space vectors unknown function defined space input vectors assigns class label input vector 
problem learn classifying method called exist integer training set consisting drawn random distribution dx input space corresponding 
classifier constructed probability greater random vector having distribution dx 
classifying method called integer training set consisting drawn random distribution dx input space corresponding 
classifier constructed probability random vector having distribution dx 
note low error input space just training set small test set error 
concept weak learning introduced kearns valiant left open question weak strong equivalent 
question termed boosting problem equivalence requires method boost low accuracy high accuracy 
schapire proved boosting possible 
boosting algorithm method takes converts 
freund proved algorithm similar arc fs boosting 
freud schapire apply results freund conclude adaboost boosting 
boosting assumptions restrictive 
instance overlap classes bayes error rate positive weak strong learners 
overlap classes easy give examples input spaces weak learners 
boosting theorems really say weak learner virtually real data situations arcing bagging overlap classes weak learners exist 
freund boosting theorem applicable 
particular applicable examples simulated data examples real data sets freund schapire quinlan 
may connection ability arcing algorithms rapidly drive training set error zer steady state test set reduction rooted boosting context 
vs av resampling probabilities waveform heart breast cancer ionosphere diabetes glass 
vs 
times training set waveform heart breast cancer ionosphere diabetes glass number trees combined proportion times cases proportion training sets percentile percentile plot proportion training sets cases av arc fs average vs arc av av 
