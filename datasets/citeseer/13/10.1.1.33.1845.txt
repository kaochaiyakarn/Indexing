power amnesia learning probabilistic automata variable memory length dana ron cs huji ac il yoram singer singer cs huji ac il naftali tishby tishby cs huji ac il institute computer science hebrew university jerusalem israel 
propose analyze distribution learning algorithm variable memory length markov processes 
processes described subclass probabilistic finite automata name probabilistic suffix automata psa 
hardness results known learning distributions generated general probabilistic automata prove algorithm efficiently learn distributions generated psas 
particular show target psa kl divergence distribution generated target distribution generated hypothesis learning algorithm outputs small high confidence polynomial time sample complexity 
learning algorithm motivated applications human machine interaction 
applications algorithm 
apply algorithm order construct model english language model correct corrupted text 
second application construct simple stochastic model coli dna 

statistical modeling complex sequences fundamental goal machine learning due wide variety natural applications 
noticeable examples applications statistical models human communication natural language handwriting speech statistical models biological sequences dna proteins 
kinds complex sequences clearly simple underlying statistical source generated natural sources 
typically exhibit statistical property refer short memory property 
consider empirical probability distribution symbol preceding subsequence length exists length memory length conditional probability distribution change substantially condition preceding subsequences length greater observation lead shannon seminal suggest modeling sequences markov chains order order memory length model 
alternatively sequences may modeled hidden markov models hmms complex distribution generators may capture additional properties natural sequences 
statistical models define rich families sequence distributions give efficient procedures dana ron yoram singer naftali tishby generating sequences computing probabilities 
models severe drawbacks 
size markov chains grows exponentially order low order markov chains considered practical applications 
low order markov chains poor approximators relevant sequences 
case hmms known hardness results concerning learnability discuss section 
propose simple stochastic model describe learning algorithm 
observed natural sequences memory length depends context fixed model suggest variant order markov chains order equivalently memory variable 
describe model subclass probabilistic finite automata pfa name probabilistic suffix automata psa 
state psa labeled string alphabet sigma 
transition function states defined string labels walk underlying graph automaton related sequence ends state labeled suffix sequence 
lengths strings labeling states bounded upper bound different states may labeled strings different length viewed having varying memory length 
psa generates sequence probability distribution symbol generated completely defined previously generated subsequence length mentioned probability distributions automata generate equivalently generated markov chains order description psa may succinct 
size order markov chains exponential estimation requires data length time exponential learning model assume learning algorithm sample consisting sample sequences single sample sequence generated unknown target psa bounded size 
algorithm required output hypothesis machine necessarily psa properties 
efficiently generate distribution similar generated sequence efficiently compute probability assigned distribution 
measures quality hypothesis considered 
mainly interested models statistical classification pattern recognition natural measure kullback leibler kl divergence 
results hold equally variation distance norms upper bounded kl divergence 
kl divergence markov sources grows linearly length sequence appropriate measure kl divergence symbol 
define ffl hypothesis hypothesis ffl kl divergence symbol target source 
particular hypothesis algorithm outputs belongs class probabilistic machines named probabilistic suffix trees pst 
learning algorithm grows suffix tree starting single root node adaptively adds nodes learning probabilistic automata variable memory length strings strong evidence sample significantly affect prediction properties tree 
show distribution generated psa equivalently generated pst larger 
converse true general 
characterize family psts converse claim holds general case pst exists larger pfa generates equivalent distribution 
contexts psas preferable psts preferable representation 
example psas efficient generators distributions probabilistic automata defined state space transition function exploited dynamic programming algorithms solving practical problems 
addition natural notion stationary distribution states psa psts lack 
hand psts succinct representations equivalent psas natural notion growing 
stated formally main theoretical result 
bound memory length target psa bound number states target psa known ffl ffi learning algorithm outputs ffl hypothesis pst confidence gamma ffi time polynomial sigmaj ffl ffi furthermore hypothesis obtained single sample sequence sequence length polynomial parameter related rate target machine converges stationary distribution 
despite intractability result concerning learnability distributions generated probabilistic finite automata described section restricted model learned pac sense efficiently 
shown far popular sequence modeling algorithms 
applications learning algorithm 
application apply algorithm order construct model english language model correct corrupted text 
second application construct simple stochastic model coli dna 
combined learning algorithm different subclass probabilistic automata algorithm part complete cursive handwriting recognition system 

related powerful popular model modeling natural sequences hidden markov model hmm 
detailed tutorial theory hmms selected applications speech recognition rabiner :10.1.1.131.2084
commonly procedure learning hmm sample maximum likelihood parameter estimation procedure baum welch method special case em expectation maximization algorithm 
algorithm guaranteed converge local maximum assured hypothesis outputs serve dana ron yoram singer naftali tishby approximation target distribution 
hope problem overcome improving algorithm finding new approach 
unfortunately strong evidence problem solved efficiently 
abe warmuth study problem training hmms 
hmm training problem problem approximating arbitrary unknown source distribution distributions generated hmms 
prove hmms trainable time polynomial alphabet size rp np 
sipser study problem exactly inferring ergodic hmm binary alphabet inference algorithm query probability oracle long term probability binary string 
prove inference hard algorithm inference exponentially oracle calls 
method information theoretic depend separation assumptions complexity classes 
natural simpler alternatives order markov chains known gram models 
noted earlier size order markov chain exponential want capture short term memory dependencies sequences substantial length sequences models clearly practical 
studies families distributions related ones studied algorithms depend exponentially polynomially order memory length distributions 
freund point result learning typical deterministic finite automata random walks membership queries extended learning typical 
unfortunately strong evidence indicating problem learning general hard 
kearns show efficiently learnable assumption efficient algorithm learning noisy parity functions pac model 
machines hypothesis representation probabilistic suffix trees psts introduced slightly different form tasks universal data compression 
strongest results brought attention completion tightly related result 
describes efficient sequential procedure universal data compression psts larger model class 
algorithm viewed distribution learning algorithm hypothesis produces pst psa applications 
willems show algorithm modified give minimum description length pst 
case source generating examples pst able show pst convergence limit infinite sequence length source 
vitter krishnan adapt version ziv lempel data compression algorithm get page prefetching algorithm sequence page accesses assumed generated pfa 
show page fault rate algorithm converges page fault rate best algorithm full learning probabilistic automata variable memory length knowledge source 
true page access sequences limit sequence length 
laird saul describe prediction algorithm similar spirit algorithm markov tree directed acyclic word graph approach data compression 
analyze algorithm formally applications algorithm 

overview organized follows 
section give basic definitions notation describe families distributions studied generated psas generated psts 
section discuss relation families distributions 
section learning algorithm described 
proofs regarding correctness learning algorithm section 
demonstrate applicability algorithm illustrative examples section 
example algorithm learn structure natural english text resulting hypothesis correcting corrupted text 
second example algorithm build simple stochastic model coli dna 
detailed proofs claims section concerning relation psas psts provided appendices technical proofs lemmas regarding correctness learning algorithm appendix 
preliminaries 
basic definitions notations sigma finite alphabet 
sigma denote set possible strings sigma 
integer sigma denotes strings length sigma denotes set strings length empty string denoted string sigma notations ffl longest prefix different denoted prefix def gamma ffl longest suffix different denoted suffix def gamma ffl set suffixes denoted suffix def fs lg feg 
string proper suffix suffix 
ffl strings sigma suffix shall say suffix extension ffl set strings called suffix free set suffix fsg 
dana ron yoram singer naftali tishby 
probabilistic finite automata prediction suffix trees 
probabilistic finite automata probabilistic finite automaton pfa tuple sigma fl finite set states sigma finite alphabet theta sigma transition function fl theta sigma symbol probability function initial probability distribution starting states 
functions fl satisfy conditions oe sigma fl oe 
assume transition function defined states symbols oe fl oe state symbol pairs 
extended defined theta sigma follows gamma prefix 
pfa generates strings infinite length shall discuss probability distributions induced prefixes strings specified finite length 
pm probability distribution defines infinitely long strings denote probability induced strings length shall drop superscript assuming understood context 
probability generates string sigma fl gamma 

probabilistic suffix automata interested learning subclass name probabilistic suffix automata psa 
automata property 
state psa labeled string finite length sigma set strings labeling states suffix free 
states symbol oe sigma oe labeled string labeled string suffix delta oe 
order defined set strings set suffix free property 
string labeling state symbol oe fl oe exists string suffix soe 
convenience point state denote string labeling state 
assume underlying graph defined delta delta strongly connected pair states directed path note definition assumed probability associated transition edge underlying graph non zero strong connectivity implies state reached state non zero probability 
simplicity assume aperiodic greatest learning probabilistic automata variable memory length common divisor lengths cycles underlying graph 
assumptions ensure ergodic 
exists distribution pi states state may start probability distribution state reached time grows infinity converges pi probability distribution pi unique distribution satisfying pi oe pi fl oe named stationary distribution ask state initial probability stationary probability pi 
noted assumptions needed learning single sample string learning sample strings 
sake brevity requirements cases 
subclass psas state labeled string length denoted psa 
example psa depicted 
special case automata case includes strings sigma example psa depicted 
automata described markov chains order states markov chain symbols alphabet sigma state transition probability depends states symbols traversed 
psa extended possibly larger equivalent psa states labeled strings sigma described markov chain order alternatively states psa labeled small subset sigma suffixes labeling states may shorter viewed markov chain variable order variable memory 
learning markov chains order psas states labeled sigma strings straightforward takes time exponential 
identity states strings labeling states known transition function uniquely defined learning automata reduces approximating symbol probability function fl 
general case psas states labeled strings variable length task efficient learning algorithm involved reveal identity states 

prediction suffix trees interested learning psas choose hypothesis class class prediction suffix trees pst defined section 
show section psa exists equivalent pst roughly size 
pst alphabet sigma tree degree sigmaj 
edge tree labeled single symbol sigma internal node exactly edge labeled symbol 
nodes tree labeled pairs fl dana ron yoram singer naftali tishby string associated walk starting node root tree fl sigma symbol probability function related require string labeling node tree oe sigma fl oe 
case pst generates strings infinite length consider probability distributions induced finite length prefixes strings 
probability generates string sigma pi fl gamma gamma string labeling deepest node reached walk corresponding gamma starting root example pst depicted probability string theta theta theta theta labels nodes prediction 
view definition requirement internal node exactly sigmaj sons may allowing omission nodes labeled substrings generated tree probability 
psts generate probability distributions similar fashion psas 
case psas symbols generated sequentially probability generating symbol depends previously generated substring bounded length 
cases simple procedure determining substring determining probability distribution symbol conditioned substring 
related differences psas psts 
psas generate symbol simply traversing single edge current state state symbol generated pst walk root tree possibly traversing edges 
implies psas efficient generators 
second difference psas substring state symbol state defined psts property necessarily hold 
current generating node pst symbol generated node necessarily uniquely defined depend previously generated symbols included string associated current node 
example assume tree leaves see appendix 
current generating leaf generates generating leaf depending symbol generated just prior 
psts psas described markov chains fixed finite order case psas description exponentially large 
shall want discuss structure pst ignore prediction property 
words interested string labels nodes values fl delta 
refer trees suffix trees 
introduce notations 
set leaves suffix tree denoted string labeling node denotes subtree rooted learning probabilistic automata variable memory length 
left psa 
strings labeling states suffixes corresponding 
bold edges denote transitions symbol dashed edges denote transitions 
transition probabilities depicted edges 
middle psa states labeled strings strings labeling states observed symbols state reached viewed representation markov chain order 
right prediction suffix tree 
prediction probabilities symbols respectively depicted nodes parentheses 
models equivalent sense induce probability distribution strings 
learning model learning model described motivated pac model learning boolean concepts labeled examples similar spirit introduced 
start defining ffl hypothesis pst respect psa 
definition 
psa pst 
pm pt probability distributions generate respectively 
say ffl hypothesis respect dkl jjp ffl dkl jjp def sigma log kullback leibler divergence distributions 
definition chose kullback leibler kl divergence distance measure distributions 
similar definitions considered distance measures variation quadratic distances 
note kl divergence bounds variation distance follows dkl jjp jjp gamma jj norm bounds norm bound holds quadratic distance 
note kl divergence distributions generated finite order markov chains proportional length strings divergence computed length longer order model 
obtain measure independent length necessary divide kl divergence length strings dana ron yoram singer naftali tishby learning algorithm psas maximum length strings labeling states target psa upper bound number states second assumption easily removed searching upper bound 
search performed testing hypotheses algorithm outputs runs growing values algorithm confidence security parameter ffi approximation parameter ffl 
analyze learning scenarios 
scenario algorithm access source sample strings minimal length independently generated second scenario single long sample string generated cases require output hypothesis pst probability gamma ffi ffl hypothesis respect drawback having pst hypothesis psa generally pfa prediction procedure tree somewhat efficient factor 
transition function defined order predict generate symbol walk root leaf reached 
mentioned earlier show appendix pst transformed equivalent pfa larger 
pfa differs psa way generates symbols 
show pst certain property defined appendix transformed equivalent psa 
order measure efficiency learning algorithm separate case algorithm sample consisting independently generated sample strings case single sample string 
case say learning algorithm efficient runs time polynomial sigmaj ffl ffi order define efficiency case need take account additional property model mixing convergence rate 
discuss parameter psas general 
psa rm denote theta stochastic transition matrix defined delta delta fl delta delta ignoring transition labels 
states symbol oe rm fl oe oe 
rm transition matrix ergodic markov chain 
rm denote time reversal rm rm pi rm pi pi stationary probability vector rm defined equation 
define multiplicative um um rm rm denote second largest eigenvalue um um 
learning algorithm receives single sample string allow length string running time algorithm polynomial sigmaj ffl ffi gamma um 
rationale roughly 
order succeed learning psa observe state stationary probability non negligible times learning probabilistic automata variable memory length algorithm identify state significant algorithm compute approximately symbol probability function 
independently generated sample strings easily bound size sample needed polynomial sigmaj ffl ffi chernoff bounds 
sample string string long ensure convergence probability visiting state stationary probability 
show convergence rate bounded expansion properties weighted graph related um generally algebraic properties um second largest eigenvalue 

emulation psas psts section show psa exists equivalent pst larger 
allows consider pst equivalent target psa convenient 
theorem psa sigma fl exists equivalent pst tm maximal depth delta jqj nodes 
proof sketch describe construction needed prove claim 
complete proof provided appendix tm tree leaves correspond strings leaf symbol oe fl oe fl oe 
ensures string suffix extension leaf tm symbol oe pm oejs oejs 
remains define symbol probability functions internal nodes tm functions defined tm generates strings related nodes probability node tree weight denoted def suffix 
words weight leaf tm stationary probability corresponding state weight internal node labeled string equals sum stationary probabilities states suffix equals sum weights leaves subtree rooted node 
weights nodes assign values fl internal nodes tree manner 
symbol oe fl oe suffix ws fl oe 
probability fl oe generating symbol oe string shorter state weighted average fl oe taken states correspond suffix extensions weight related state average corresponds stationary probability 
example probability distribution symbol generated tm fl delta 
probability distribution equivalent definition probability distribution symbol generated internal node tm symbol probability function equivalent symbol probability functions descendants remove descendants tree 
dana ron yoram singer naftali tishby example construction described proof theorem illustrated 
pst right constructed psa left equivalent 
note symbol probabilities related leaves internal nodes tree defined proof theorem 

learning algorithm start overview algorithm 
sigma fl target psa learn jqj theorem exists pst size bounded delta jqj equivalent sample statistics define empirical probability function delta construct suffix tree high probability subtree define hypothesis pst construction done follows 
start tree consisting single node labeled empty string add nodes reason believe tree 
node labeled string added leaf holds 
empirical probability symbol oe empirical probability observing oe oejs differs substantially empirical probability observing oe suffix 
note suffix string labeling parent node decision rule adding dependent ratio oejs 
add node ratio substantially greater 
suffices analysis due properties kl divergence need add node ratio smaller 
grow tree level level adding sons leaf tree exhibit behavior sample growing tree true leaf 
problem node belong tree symbol probability function equivalent parent node 
leaves pst differ parents redundant internal nodes property 
pst depicted illustrates phenomena 
example fl delta fl delta fl delta fl delta differ fl delta 
continue testing potential descendants leaves tree depth mentioned test strings belong branches empirical count sample small 
way avoid exponential grow number strings tested 
similar type branch bound technique various bounding criteria applied algorithms trees data structures cf 

set strings tested step denoted viewed kind potential frontier growing tree bounded size 
construction completed define adding nodes internal nodes full degree defining symbol probability function node probability functions defined string tree symbol oe fl oe bounded fl min parameter set subsequently 
done learning probabilistic automata variable memory length conventional smoothing technique 
bound fl oe needed order bound kl divergence target distribution distribution hypothesis generates 
scheme follows top approach start tree consisting single root node frontier consisting children incrementally grow suffix tree frontier alternatively bottom procedure devised 
bottom procedure start putting strings length significant counts setting tree nodes correspond strings trim starting leaves proceeding tree comparing prediction probabilities node parent node done top procedure 
schemes equivalent yield prediction suffix tree 
find incremental top approach somewhat intuitive simpler implement 
top procedure easily adapted online setting useful practical applications 
denote probability distribution generated formally define empirical probability function sample generated string roughly relative number times appears sample symbol oe oejs roughly relative number times oe appears give precise definition 
sample consists sample string length string length define 
gamma gamma symbol oe oejs gamma soe gamma sample consists sample strings length string length define 
gamma gamma symbol oe oejs gamma soe gamma simplicity assume sample strings length length polynomial sigma 
case sample strings dana ron yoram singer naftali tishby different lengths treated similarly strings long ignore parts 
course algorithm analysis refer parameters simple functions ffl sigmaj set follows ffl ffl fl min ffl sigmaj ffl sigmaj ffl ffl nl log fl min ffl nl log lj sigmaj ffl ffl ffl fl min ffl ffl log lj sigmaj ffl sigmaj size sample set analysis algorithm 
pseudo code describing learning algorithm illustrative run algorithm depicted 
algorithm learn psa 
initialize consist single root node corresponding foe oe sigma oe gamma ffl ffl 
pick remove exists symbol oe sigma oejs ffl fl min oejs ffl add node corresponding nodes path deepest node suffix jsj oe sigma oe deltas gamma ffl ffl add oe deltas 
initialize 
extend adding missing sons internal nodes 

labeling node fl oe oejs gamma min fl min longest suffix 
algorithm learn psa learning probabilistic automata variable memory length 
illustrative run learning algorithm 
prediction suffix trees created run algorithm depicted left right top bottom 
stage run nodes plotted dark grey nodes plotted light grey 
alphabet binary predictions bit depicted parenthesis node 
final tree plotted bottom right part built adding bottom left missing children 
note node labeled added final tree part intermediate trees 
happen probability string small 
dana ron yoram singer naftali tishby 
analysis learning algorithm section state prove main theorem regarding correctness efficiency learning algorithm learn psa described section 
theorem target psa security parameter ffi approximation parameter ffl algorithm learn psa outputs hypothesis pst probability gamma ffi 
ffl hypothesis respect 
number nodes sigmaj delta times number states algorithm access source independently generated sample strings running time polynomial sigmaj ffl ffi algorithm access sample string running time polynomial parameters gamma um 
order prove theorem show probability gamma ffi large sample generated typical typical defined subsequently 
assume algorithm fact receives typical sample prove theorem assumption 
roughly speaking sample typical substring generated non negligible probability empirical counts substring symbol substring far corresponding probabilities defined definition 
sample generated typical string sigma properties hold 
gamma ffl ffl 
gamma ffl ffl oe sigma oejs gamma oejs ffl fl min ffl ffl ffl fl min defined section 
lemma 
exists polynomial sigmaj ffl ffi probability sample sigmaj ffl ffi strings length generated typical gamma ffi 
exists polynomial sigmaj ffl ffi gamma um probability single sample string length sigmaj ffl ffi gamma um generated typical gamma ffi proof lemma provided appendix pst equivalent target psa defined theorem 
lemma prove claims 
claim show prediction learning probabilistic automata variable memory length properties hypothesis pst similar 
proof claim theorem showing kl divergence symbol small 
second claim give bound size terms implies similar relation second claim theorem 
lemma learn psa typical sample 
string ffl fl oe fl oe ffl longest suffix corresponding node 
sigmaj gamma delta jt proof sketch complete proofs claims provided appendix 
order prove claim argue sample typical exist strings falsify claim 
prove assuming exists pair reaching contradiction 
setting parameters ffl fl min show pair ratio fl oe fl oe bounded ffl 
reached contradiction 
show algorithm add longer suffix contradicting assumption longest suffix corresponding node order bound size show subtree suffices prove second claim transforming add sigmaj gamma siblings node prove subtree arguing construction add string correspond node follows decision rule add nodes proof theorem lemma probability gamma ffi algorithm receives typical sample 
second claim lemma sigmaj gamma delta jt jt delta jqj sigmaj delta delta jqj second claim theorem valid 
sigma prefix denote strings corresponding deepest nodes reached walk respectively 
particular denote probability distribution generated sigma log sigma delta log fl gamma fl gamma dana ron yoram singer naftali tishby sigma delta log fl gamma fl gamma sigma gamma ffl delta log fl gamma fl gamma sigma gamma ffl delta log fl gamma fl gamma term parenthesis equation bounded follows 
string worst possible ratio fl gamma fl gamma fl min total weight strings term equals total weight nodes weight ffl term bounded log fl min 
lemma ratio fl gamma fl gamma string second term parenthesis ffl 
total weight strings bounded second term bounded log ffl 
combining value ffl set section ffl nl log fl min get dkl jj delta ffl log fl min log ffl ffl straightforward implementation algorithm get rough upper bound running time algorithm order square size sample times implementation time add string perform complete pass sample count number occurrences sample symbol statistics 
lemma bound polynomial relevant parameters required theorem statement 
time efficient space efficient implementation bound running time algorithm size sample times string leaf keep set pointers occurrences string sample 
string want test extensions oes add need consider occurrences sample distribute accordingly strings added 
symbol sample single pointer pointer corresponds single string length running time algorithm order size sample times 
applications slightly modified version learning algorithm applied tested various problems correcting corrupted text predicting dna bases learning probabilistic automata variable memory length part speech disambiguation resolving :10.1.1.47.5713
exploring possible applications algorithm 
demonstrate algorithm correct corrupted text build simple model dna strands 

correcting corrupted text machine recognition systems speech handwriting recognizers recognition scheme divided independent stages 
stage low level model perform stochastic mapping observed data acoustic signal speech recognition applications high level alphabet 
mapping accurate get correct sequence high level alphabet assume belongs corresponding high level language 
common errors mapping occur sequences high level language corrupted 
effort building recognition systems devoted correct corrupted sequences 
particular optical handwriting character recognition systems stage employs natural language analysis techniques correct corrupted sequences 
done model high level language learned uncorrupted examples sequences language 
show psas order perform task 
applied learning algorithm bible 
alphabet english letters blank character 
removed served test set 
algorithm applied rest books accuracy parameters ffl order length training data 
resulted pst having nodes 
pst transformed psa order apply efficient text correction scheme described subsequently 
final automaton constitutes states length qu xe states symbols long shall 
indicates algorithm really captures notion variable memory needed order accurate predictions 
building markov chain order case clearly practical requires sigmaj states 
observed corrupted text 
estimation corrupting noise probability calculate state sequence probability created walk psa constitutes states random variable denotes event ith state passed random variable sigma oe denotes event ith symbol observed oe 
denote joint event sigma denote joint event assume corrupting noise independent states constitute walk dana ron yoram singer naftali tishby state sequence ml ml arg max gamma qj delta arg max gamma rj delta arg max gamma delta theta jx gamma gamma arg max log jx log log jx gamma gamma deriving equality monotonicity log function fact corruption noise independent states 
string labeling jx probability uncorrupted symbol probability noise process flipped 
note sum computed efficiently recursive manner 
maximization equation performed efficiently dynamic programming dp scheme 
scheme requires jqj theta operations 
jqj large approximation schemes optimal dp stack decoding algorithm employed 
similar methods possible correct errors insertions deletions symbols occur 
tested algorithm text corrupting ways 
altered letter including blanks probability 
second test altered letter probability changed blank character order test resulting model powerful cope non uniform noise 
result correction algorithm cases original corrupted texts depicted 
compared performance psa constructed performance markov chains order 
performance measured negative log likelihood obtained various models uncorrupted test data normalized observation symbol 
negative log likelihood measures amount statistical surprise induced model 
results summarized table 
entries correspond markov chains order entry corresponds psa 
order psa defined log sigmaj jqj 
empirical results imply psa reasonable size get better model data larger full order markov chain 
learning probabilistic automata variable memory length original text god called dry land earth gathering waters called seas god saw god said earth bring forth grass herb yielding seed fruit tree yielding fruit kind corrupted text god earth ibd oj waters re seas aed god saw ann god said tae earth bring forth tse tree kind corrected text god caused dry land earth gathering waters called sees god saw god said earth bring forth grass memb yielding fruit tree fielding fruit kind corrupted text corrected text god called dry land earth gathering called god saw took god said forth grass herb yielding seed thy fruit fruit kind 
correcting corrupted text 
table 
comparison full order markov chains versus psa markov model variable memory 
fixed order markov psa model order number states negative log likelihood 
building simple model coli dna dna alphabet composed nucleotides denoted dna strands composed sequences protein coding genes fillers regions named intergenic regions 
locating coding genes necessary prior dna analysis 
manually segmented data coli built different psas coding regions intergenic regions 
disregarded internal triplet structure coding genes existence start codons regions 
dana ron yoram singer naftali tishby models constructed different dna strands type lengths ranging bases thousands 
psas built small compared hmm model described psa models coding regions states psa models intergenic regions states 
tested performance models calculating log likelihood models obtained test data drawn intergenic regions 
cases log likelihood obtained psa trained intergenic regions higher log likelihood psa trained coding regions 
misclassifications log likelihood obtained second model higher occurred sequences shorter bases 
log likelihood difference models scales linearly sequence length slope close kl divergence markov models computed parameters psas depicted 
main advantage psa models simplicity 
log likelihood set substrings strand computed time linear number substrings 
property combined results mentioned indicate psa model performing tasks dna gene locating 
stress done preliminary step direction results obtained part complete parsing system better 
log likelihood difference sequence length 
difference log likelihood induced psa trained data taken intergenic regions psa trained data taken coding regions 
test data taken intergenic regions 
cases likelihood psa higher 
anders krogh david haussler letting coli dna data helpful discussions 
special supplying coli sequences dna experiments 
rubinfeld yoav freund helpful comments 
lee giles providing software plotting finite state machines 
research supported part israeli ministry science arts bruno goldberg endowment fund 
dana ron learning probabilistic automata variable memory length support fellowship 
yoram singer clore foundation support 

abe warmuth 
computational complexity approximating distributions probabilistic automata 
machine learning 

baum 
inequality associated maximization technique statistical estimation probabilistic functions markov chains 
inequalities 

baum petrie soules weiss 
maximization technique occuring statistical analysis probabilistic functions markov chains 
annals mathematical statistics 

bellman 
dynamic programming 
princeton university press 

blumer 
applications data compression 
editor sequences combinatorics compression security pages 
springerverlag 

cover thomas 
elements information theory 
wiley 

dempster laird rubin 
maximum likelihood incomplete data em algorithm 
royal stat 
soc 

fill 
eigenvalue bounds convergence stationary markov chains application exclusion process 
annals applied probability 

freund kearns ron rubinfeld schapire sellie 
efficient learning typical finite automata random walks 
proceedings th annual acm symposium theory computing pages 

sipser 
inference minimization hidden markov chains 
proceedings seventh annual workshop computational learning theory pages 


statistics language 
hudson editors encyclopedia linguistics information control pages 
pergamon press oxford england 



learning robust learning product distributions 
proceedings sixth annual workshop computational learning theory pages 

jelinek 
fast sequential decoding algorithm stack 
ibm res 
develop 

jelinek 
self organized language modeling speech recognition 
technical report ibm watson research center 

kearns mansour ron rubinfeld schapire sellie 
learnability discrete distributions 
th annual acm symposium theory computing 

krishnan vitter 
optimal prediction prefetching worst case 
technical report cs duke university 

krogh mian haussler 
hidden markov model finds genes coli dna 
technical report ucsc crl university california santa cruz 

kushilevitz mansour 
learning decision trees fourier spectrum 
siam journal computing 

laird saul 
discrete sequence prediction applications 
machine learning 

mihail 
conductance convergence markov chains combinatorial treatment expanders 
proceedings th annual conference foundations computer science 

nadas 
estimation probabilities language model ibm speech recognition system 
ieee trans 
assp 

rabiner 
tutorial hidden markov models selected applications speech recognition 
proceedings ieee 
dana ron yoram singer naftali tishby 
rissanen 
universal data compression system 
ieee trans 
inform 
theory 

rissanen 
complexity strings class markov sources 
ieee trans 
inform 
theory 

ron singer tishby 
power amnesia 
advances neural information processing systems volume 
morgan kaufmann 

ron singer tishby 
learnability usage acyclic probabilistic finite automata 
proc 
th annual conf 
computational learning theory 


maps genes sequences computers escherichia coli case study 
asm news 

schutze singer 
part speech tagging variable memory markov model 
proceedings acl nd 

shannon 
prediction entropy printed english 
bell sys 
tech 
jour 

singer tishby 
adaptive cursive handwriting recognition system 
technical report cs tr hebrew university 

vitter krishnan 
optimal prefetching data compression 
proceedings second annual symposium foundations computer science pages 

weinberger lempel ziv 
sequential algorithm universal coding finite memory sources 
ieee trans 
inform 
theory may 

willems shtarkov 
context tree weighting method basic properties 
ieee trans 
inform 
theory 
submitted publication 

ziv lempel 
compression individual sequences variable rate coding 
ieee trans 
inform 
theory sept 
appendix proof theorem theorem psa sigma fl exists equivalent pst tm maximal depth delta jqj nodes 
proof tm tree leaves correspond strings states 
leaf symbol oe fl oe fl oe 
ensures string suffix extension leaf tm tm generate symbol probability 
remainder proof dedicated defining symbol probability functions internal nodes tm functions defined tm generates strings related nodes tm probability node tree weight denoted defined follows def suffix words weight leaf tm stationary probability corresponding state weight internal node labeled string learning probabilistic automata variable memory length equals sum stationary probabilities states suffix 
note weight internal node sum weights leaves subtree particular 
weights nodes assign values fl internal nodes tree manner 
symbol oe fl oe suffix fl oe definition weights nodes clear node fl delta fact probability function output symbol required definition prediction suffix trees 
probability generates string node tm suffix state 
definition transition function suffix extension pm sum probability reaching chosen initial distribution delta starting states 
initial distribution stationary point probability state just pm suffix prove equals 
showing tree jsj prefix fl prefix 
follows simple inductive argument tm definition psas delta 
fl leaf tm tm suffix fl tm prefix fl prefix fl prefix follows substituting fl fl equation definition delta delta follows definition structure prediction suffix trees follows definition weights internal nodes 
leaf prefix fl prefix required 
internal node result equation get tm dana ron yoram singer naftali tishby tm prefix fl prefix prefix fl prefix left show resulting tree bigger times number states number leaves tm equals number states jl jqj 
internal node tm full degree probability tm generates string labeling leaf tree strictly greater number internal nodes bounded jqj total number nodes jqj 
particular true state symbol oe fl oe 
case simply bound total number nodes delta jqj 
appendix emulation psts section show pst exists equivalent pfa larger slight variant psa 
furthermore pst certain property defined denoted property emulated psa 
property string labeling node tree oe sigma pt oes state theorem observe property implies string pt oe sigma true simple reasoning 
node equality equivalent property 
longest prefix leaf pt delta jr oe delta jr oe pt delta pt oe pt equality follows definition pst 
learning probabilistic automata variable memory length theorem pst depth sigma exists equivalent pfa mt delta jl states 
furthermore property holds equivalent psa 
proof proof theorem psa defined equivalent suffix tree tm tree leaves correspond states automaton 
suffix tree natural dual procedure construct psa mt states correspond leaves problem construction able define transition function pairs states symbols 
exist state symbol oe state suffix soe 
solution extend larger tree subtree defined leaves easily verified equivalent requirement symbol oe leaf soe leaf subtree oe rooted oe suffix extension leaf oe 
case shall say covers children subtrees 
viewing way leaf longest prefix leaf internal node obtain adding nodes property holds 
symbol probability functions nodes defined follows 
node oe sigma fl oe fl oe 
new node gamma fl oe fl oe longest suffix deepest ancestor 
probability distribution generated equivalent generated equality directly follows property holds holds 
define sigma fl 
property holds define mt follows 
states mt leaves transition function defined usual psas state symbol oe oe unique suffix soe 
note number states times number leaves required 
true original leaf tree gamma prefixes added oe sigma fl oe fl oe pt 
noted necessarily ergodic 
follows construction string suffix extension leaf symbol oe pmt 
remains show string node pmt pt 
state mt denote probability generated assuming start state pmt mt pmt dana ron yoram singer naftali tishby pt sr pt equality follows definition psas equality follows definition delta equality follows series applications equality 
property may able define initial distribution states psa string node pmt pt 
define slight variant follows 
states mt leaves prefixes delta delta defined follows state symbol oe oe longest suffix soe 
mt structure prefix tree combined psa 
define fl delta delta empty string single starting state definition mt equivalent illustration constructions described 

left prediction suffix tree 
prediction probabilities symbols respectively depicted nodes parentheses 
right pfa equivalent pst left 
bold edges denote transitions symbol dashed edges denote transitions 
property holds pst equivalent psa defined circled part pfa 
initial probability distribution psa 
note states psa replaced node tree 
learning probabilistic automata variable memory length appendix proofs technical lemmas theorems lemma 
exists polynomial sigmaj ffl ffi probability sample sigmaj ffl ffi strings length generated typical gamma ffi 
exists polynomial sigmaj ffl ffi gamma um probability single sample string length sigmaj ffl ffi gamma um generated typical gamma ffi proof proving lemma recall parameters ffl ffl ffl fl min polynomial functions ffl sigmaj defined section 
sample strings start obtaining lower bound property typical sample holds 
sample strings generated independently may view state average value independent random variables 
variables range expected value 
variant hoeffding inequality get ffl ffl ln ffi probability gamma ffi gamma ffl ffl probability inequality holds state gamma ffi point assumptions sample strings generated independently length independence different strings bounding error 
assume random variables related restricted sample string expected value 
strings known longer careful analysis applied described subsequently case single sample string 
show appropriate second property holds probability gamma ffi 
string sigma lines refer appearances sample mean sense defined count appearances lth greater symbol sample string 
ith appearance sample symbol oe oejs random variable oe appears ith appearance 
state suffix extension state oe random variables fx oejs independent random variables expected value oejs 
total number times appears sample nmin ffl fl min ln ffl ffi nmin probability gamma symbol oe oejs gamma oejs ffl fl min suffix states symbol oe dana ron yoram singer naftali tishby oejs oejs oejs oejs recall ffl ffl fl min nffl 
state gamma ffl ffl satisfying ffl ffl oejs gamma oejs ffl fl min oe oejs gamma oejs ffl fl min required 
sample property required typical sample gamma ffl ffl state ffl ffl nmin probability gamma ffi second property typical sample holds strings states suffixes states 
string suffix extension state gamma ffl ffl nmin strings second property holds probability gamma ffi 
putting bounds ffl ffl ln ffi nmin ffl ffl probability gamma ffi sample typical 
single sample string case analysis somewhat involved 
view sample string generated walk markov chain described rm defined subsection 
may assume starting state visible contribution delta negligible 
shall need theorem gives bounds convergence rate stationary distribution general ergodic markov chains 
theorem partially mihail gives bounds convergence terms combinatorial properties chain 
markov chain convergence theorem state markov chain rm delta denote probability distribution states rm walk length starting state jr gamma um note simply applying markov inequality get probability gamma ffi gamma ffl ffl state 
ffi ffl ffl 
remains obtain lower bound true ffi ffl ffl 
bounding variance random variable related applying inequality 
learning probabilistic automata variable memory length ln gamma ffi ffl ffl delta ln um show satisfying ffi ffl ffl jr gamma ffi ffl ffl theorem assumption gamma gamma delta jr gamma um ffi ffl ffl um ffi ffl ffl gammat ln um ffi ffl ffl jr gamma ffi ffl ffl intuitively means integers gamma event th state passed walk length independent event ith state passed walk 
state satisfying ffi ffl ffl random variable iff ith state walk length definition case single sample string gamma gamma 
clearly ar gamma 
bound ar 
ar ar gamma ji ji gamma ffi ffl ffl gamma pick greater nt ffi ffl ffl ar ffi ffl ffl inequality jy gamma ffl ffl ffi probability holds ffi analysis second property required typical sample identical described case sample consisting strings 
dana ron yoram singer naftali tishby lemma learn psa typical sample 
string ffl fl oe fl oe ffl longest suffix corresponding node 
sigmaj gamma delta jt proof st claim assume contrary claim exists string labeling node ffl oe sigma fl oe fl oe ffl longest suffix simplicity presentation assume node labeled case suffix internal node son missing analysis similar 
easily show counter assumption false 
proper suffix prove 
counter assumption true added necessarily proper suffix longer contradicts fact longest suffix achieve lower bound ratio true symbol probabilities fl oe fl oe 
definition fl delta fl oe gamma min oejs analyze separately case fl oe fl min case fl oe fl min recall fl min ffl sigmaj 
fl oe fl min fl oe fl oe fl oe oejs delta gamma ffl fl oe fl oe delta gamma ffl gamma min 
ffl gamma ffl inequality follows assumption sample typical inequality follows definition fl oe inequality follows counter assumption choice fl min ffl ffl ffl get fl oe fl oe ffl fl oe fl min fl oe fl oe fl oe defined fl min learning probabilistic automata variable memory length fl oe fl oe fl oe fl oe ffl ffl 
counter assumption evidently false address case proper suffix show counter assumption true exists index added index fl sr oe 
ffl fl min index reason need deal prior case clarified subsequently 
case ffl ffl ffl fl oe fl sr oe ffl words fl oe fl oe delta fl oe fl oe delta delta fl sr gamma oe fl sr oe ffl inequality implies exist index gamma fl oe fl oe ffl show inequality implies added showing added compared oejs oejs ratio values ffl 
ffl necessarily gamma ffl ffl added choice index fl oe ffl fl min assume sample typical oejs ffl fl min 
ffl fl min means compared oejs oejs 
separate case fl oe fl min case fl oe fl min fl oe fl min oejs ffl fl min dana ron yoram singer naftali tishby oejs oejs ffl fl min ffl fl min ffl added hand fl oe fl min hold oejs oejs gamma ffl fl oe ffl fl oe gamma ffl ffl ffl gamma ffl ffl gamma ffl ffl inequality follows choice ffl ffl ffl 
contradicts initial assumption longest suffix added nd claim prove subtree claim follows directly transforming add sigmaj gamma siblings node suffices show add node assume contrary add node algorithm reason add exists symbol oe oejs ffl fl min oejs ffl suffix greater gamma ffl ffl sample string typical oejs fl min oejs oejs ffl fl min ffl oejs gamma ffl fl min fl min gamma ffl oejs gamma ffl ffl ffl greater ffl 
fl min oejs fl min oejs 
cases ratio greater tree contradicting assumption 
