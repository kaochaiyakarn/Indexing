machine learning fl kluwer academic publishers boston 
manufactured netherlands 
appears machine learning volume nos 
july august pages final draft prior line editors kluwer 
slight differences page numbers exist empirical comparison voting classification algorithms bagging boosting variants eric bauer cs stanford edu computer science department stanford university stanford ca ron kohavi ronnyk cs stanford edu blue martini software campus dr suite san mateo ca received oct revised sept editors philip chan salvatore stolfo david wolpert 
methods voting classification algorithms bagging adaboost shown successful improving accuracy certain classifiers artificial realworld datasets 
review algorithms describe large empirical study comparing variants conjunction decision tree inducer variants naive bayes inducer 
purpose study improve understanding algorithms perturbation reweighting combination techniques affect classification error 
provide bias variance decomposition error show different methods variants influence terms 
allowed determine bagging reduced variance unstable methods boosting methods adaboost arc reduced bias variance unstable methods increased variance naive bayes stable 
observed arc behaves differently adaboost reweighting resampling indicating fundamental difference 
voting variants introduced include pruning versus pruning probabilistic estimates weight perturbations backfitting data 
bagging improves probabilistic estimates conjunction pruning data 
measure tree sizes show interesting positive correlation increase average tree size adaboost trials success reducing error 
compare mean squared error voting methods non voting methods show voting methods lead large significant reductions mean squared errors 
practical problems arise implementing boosting algorithms explored including numerical instabilities underflows 
scatterplots graphically show adaboost instances emphasizing hard areas outliers noise 
keywords classification boosting bagging decision trees naive bayes mean squared error 
methods voting classification algorithms bagging adaboost shown successful improving accuracy certain classifiers artificial real world datasets breiman freund schapire quinlan :10.1.1.133.1040
voting algorithms divided types adaptively eric bauer ron kohavi change distribution training set performance previous classifiers boosting methods bagging 
algorithms adaptively change distribution include option decision tree algorithms construct decision trees multiple options nodes buntine buntine kohavi kunz averaging path sets sets extended sets alternatives pruning oliver hand voting trees different splitting criteria human intervention kwok carter error correcting output codes dietterich bakiri kong dietterich 
wolpert discusses stacking classifiers complex classifier simple uniform weighting scheme bagging 
ali provides review related algorithms additional chan stolfo wolpert 
algorithms adaptively change distribution include adaboost freund schapire arc breiman :10.1.1.32.8918
drucker cortes quinlan applied boosting decision tree induction observing error significantly decreases generalization error degrade classifiers combined 
elkan applied boosting simple inducer performs uniform discretization achieved excellent results real world datasets artificial dataset failed achieve significant improvements artificial datasets 
review voting algorithms including bagging adaboost arc describe large empirical study purpose improve understanding algorithms affect classification error 
ensure study reliable dozen datasets fewer instances instances 
organized follows 
section basic notation follow description base inducers build classifiers section 
naive bayes variants decision tree inducers unlimited depth level decision stump discretized level 
section describe main voting algorithms study bagging adaboost arc 
section describe bias variance decomposition error tool 
section describe design decisions study include defined set desiderata measurements 
wanted sure implementations correct describe sanity check previous papers voting algorithms 
section describe major set experiments bagging variants 
section detailed example adaboost works discuss numerical stability problems encountered 
describe set experiments boosting algorithms adaboost arc 
raise issues section conclude summary contributions section 

notation labeled instance pair hx yi element space element discrete space represent attribute vector empirical comparison boosting bagging variants attributes class label associated instance 
assume probability distribution space labeled instances 
sample set labeled instances fhx hx hx ym ig 
instances sample assumed independently identically distributed 
classifier hypothesis mapping deterministic inducer mapping sample referred training set containing labeled instances classifier 

base inducers base inducers experiments came families algorithms decision trees naive bayes 

decision tree inducers basic decision tree inducer called mc mlc topdown decision tree induction algorithm implemented mlc kohavi sommerfield dougherty 
algorithm similar quinlan exception unknowns regarded separate value 
algorithm grows decision tree standard methodology choosing best attribute evaluation criterion gain ratio 
tree grown pruning phase replaces subtrees leaves pruning algorithm uses 
main reason choosing algorithm familiarity ability modify experiments tight integration multiple model mechanisms mlc mc available web source form part mlc kohavi sommerfield dougherty 
original algorithm variants mc explored mc mc disc 
mc limits tree single root split shallow tree called decision stump iba langley 
root attribute nominal multi way split created branch unknowns 
root attribute continuous way split created threshold greater threshold unknown 
mc disc discretizes attributes entropy discretization kohavi sahami fayyad irani effectively allowing root split multiple thresholds 
mc disc similar classifier holte discretization step entropy compared favorably discretization previous kohavi sahami 
mc mc disc build weak classifiers mc disc powerful 
specifically multi class problems continuous attributes mc usually unable build classifier tree consists single binary root split leaves children 
eric bauer ron kohavi 
naive bayes inducer naive bayes inducer duda hart langley iba thompson called simple bayes domingos pazzani builds simple conditional independence classifier 
formally probability class label value unlabeled instance containing attributes ha delta bayes rule delta label values 
delta conditional independence assumption probability computed class prediction class largest posterior probability 
probabilities formulas estimated training set 
implementation part mlc kohavi sommerfield dougherty continuous attributes discretized entropy discretization kohavi sahami fayyad irani 
probabilities estimated frequency counts estimate laplace correction cestnik described kohavi becker sommerfield 
naive bayes classifier relatively simple robust violations independence assumptions 
performs real world datasets domingos pazzani kohavi sommerfield excellent handling irrelevant attributes langley sage 

voting algorithms different voting algorithms described 
algorithm takes inducer training set input runs inducer multiple times changing distribution training set instances 
generated classifiers combined create final classifier classify test set 

bagging algorithm bagging algorithm bootstrap aggregating breiman votes classifiers generated different bootstrap samples replicates 
shows algorithm 
bootstrap sample efron tibshirani generated uniformly sampling instances training set replacement 
bootstrap samples generated classifier built bootstrap sample final classifier built output class predicted sub classifiers ties broken arbitrarily 
bootstrap sample instance training set probability gamma gamma selected times instances randomly empirical comparison boosting bagging variants input training set inducer integer number bootstrap samples 


bootstrap sample sample replacement 



arg max predicted label output classifier 
bagging algorithm selected training set 
large gamma means bootstrap sample contains unique instances training set 
perturbation causes different classifiers built inducer unstable neural networks decision trees breiman performance improve induced classifiers correlated bagging may slightly degrade performance stable algorithms nearest neighbor effectively smaller training sets training classifier breiman 

boosting boosting introduced schapire method boosting performance weak learning algorithm 
improvements freund expanded freund adaboost adaptive boosting introduced freund schapire 
concentrate adaboost called adaboost freund schapire :10.1.1.133.1040
bagging adaboost algorithm generates set classifiers votes 
algorithms differ substantially 
adaboost algorithm shown generates classifiers sequentially bagging generate parallel 
adaboost changes weights training instances provided input inducer classifiers previously built 
goal force inducer minimize expected error different input distributions integer specifying number trials weighted training sets generated sequence classifiers built 
final classifier formed weighted voting scheme weight classifier depends performance training set build 
eric bauer ron kohavi input training set size inducer integer number trials 

instance weights assigned 


ffl weight weighted error training set 

ffl set bootstrap sample weight instance goto step step limited times exit loop 

fi ffl gamma ffl 
weight weight delta fi 
normalize weights instances total weight 

arg max log fi output classifier 
adaboost algorithm update rule steps mathematically equivalent update rule statement believe intuitive divide weight ffl gamma ffl see properties hold adaboost algorithm 
incorrect instances weighted factor inversely proportional error training set ffl 
small training set errors cause weights grow orders magnitude 

proportion misclassified instances ffl instances get boosted factor ffl causing total weight misclassified instances updating half original training set weight 
similarly correctly classified instances total weight equal half original weight normalization required 
adaboost algorithm requires weak learning algorithm error bounded constant strictly 
practice inducers provide guarantee 
original algorithm aborted error bound case fairly frequent multiclass problem empirical comparison boosting bagging variants simple inducers mc mc disc opted continue trials 
nondeterministic inducers neural networks common practice reset weights initial values 
deterministic inducers resetting weights simply duplicate trials 
decision generate bootstrap sample original data continue limit samples trial limit reached experiments trial succeeded samples 
implementations adaboost boosting resampling inducers unable support weighted instances freund schapire 
implementations mc mc mc disc naive bayes support weighted instances implemented boosting reweighting direct implementation theory 
evidence exists reweighting works better practice quinlan 
schapire freund bartlett lee suggests explanation success boosting fact test set error increase classifiers combined theoretical model implies 
specifically successes linked distribution margins training examples respect generated voting classification rule margin example difference number correct votes received maximum number votes received incorrect label 
breiman claims framework proposed gives results opposite expect schapire explanation arcing works 

arc term arcing adaptively resample combine coined breiman describe family algorithms adaptively resample combine adaboost calls arc fs primary example arcing algorithm 
breiman contrasts arcing family perturb combine bagging primary example 
breiman wrote testing arc fs suspected success lay specific form adaptive resampling property increasing weight placed cases frequently misclassified 
arc algorithm shown described breiman ad hoc invention accuracy comparable arc fs adaboost weighting scheme building final adaboosted classifier 
main point show strength derived adaptive reweighting instances final combination 
adaboost algorithm sequentially induces classifiers number trials instances weighted simple scheme weight instance proportional number mistakes previous classifiers fourth power plus 
final classifier built returns class predicted classifiers ties broken arbitrarily 
adaboost classifiers voted equally 
eric bauer ron kohavi input training set size inducer integer number trials 


instance weight set normalized total weight number misclassifications classifiers gamma 


arg max predicted label output classifier 
arc algorithm 
bias variance decomposition bias plus variance decomposition geman bienenstock doursat powerful tool sampling theory statistics analyzing supervised learning scenarios quadratic loss functions 
fixed target training set size conventional formulation decomposition breaks expected error sum non negative quantities intrinsic target noise oe quantity lower bound expected error learning algorithm 
expected error bayes optimal classifier 
squared bias bias quantity measures closely learning algorithm average guess possible training sets training set size matches target 
variance variance quantity measures learning algorithm guess fluctuates different training sets size 
classification quadratic loss function inappropriate class labels numeric 
proposals decomposing classification error bias variance suggested including kong dietterich kohavi wolpert breiman 
believe decomposition proposed kong dietterich inferior allows negative variance values 
remaining chose decomposition kohavi wolpert code available previous mirrors quadratic decomposition best 
yh random variable representing label instance hypothesis space yf random variable representing label instance target function 
shown error decomposed sum terms follows empirical comparison boosting bagging variants error gamma oe bias variance delta oe gamma yf yjx bias yf yjx gamma yh yjx variance gamma yh yjx estimate bias variance practice stage sampling procedure detailed kohavi wolpert 
test set split training set 
remaining data sampled repeatedly estimate bias variance test set 
process repeated multiple times improve estimates 
experiments conducted followed recommended procedure detailed kohavi wolpert making twice size desired training set sampling times 
process repeated times provide stable estimates 
practical experiments real data impossible estimate intrinsic noise optimal bayes error 
actual method detailed kohavi wolpert estimating bias variance generates bias term includes intrinsic noise 
experimental procedure computing bias variance gives similar estimated error holdout error estimation repeated times 
standard deviations error estimate run computed standard deviation outer runs assuming independent 
assumption strictly correct kohavi dietterich quite reasonable circumstances training sets small size average values 

experimental design describe desiderata comparisons show sanity check performed verify correctness implementation detail measured experiment 

desiderata comparisons order compare performance algorithms set desiderata comparison eric bauer ron kohavi error number instances waveform mc naive bayes number instances satimage mc naive bayes error number instances letter mc naive bayes number instances led mc naive bayes number instances segment mc naive bayes number instances segment 
learning curves selected datasets showing different behaviors mc naivebayes 
waveform represents stabilization instances satimage represents crossover mc improves naive bayes letter segment left represent continuous improvements different rates letter similar rates segment left led represents case algorithms achieve error rate large training sets segment right shows mc exhibited surprising behavior degrading training set size grew see text 
point represents mean error rate runs training set size tested holdout sample 
error bars show standard deviation estimated error 
vertical bar shows training set size chose rest desiderata 
note waveform small training set sizes high standard deviations estimates training set small large training set sizes high standard deviations test set small 
empirical comparison boosting bagging variants table 
characteristics datasets training set sizes 
files sorted increasing dataset size data set dataset training attributes classes size set size cont nominal credit german image segmentation segment hypothyroid sick euthyroid dna chess led waveform satellite image satimage mushroom nursery letter adult shuttle 
estimated error rate small confidence interval order reliable assessment algorithm outperforms dataset set datasets 
requires test set size large led choose files instances 
fourteen files satisfying requirement uc irvine repository blake keogh merz shown table 
room improving error training set size 
specifically may training set size large generated classifiers perform bayes optimal algorithm making improvements impossible 
example training thirds data mushroom dataset common practice usually results error 
problem challenging train fewer instances 
satisfy desideratum generated learning curves chosen files selected training set size point curve sloping indicating error decreasing 
avoid variability resulting small training sets avoided points close zero points standard deviation estimates large 
similarly training large datasets shrinks number instances available testing causing final estimate highly variable 
left half data test set 
selected learning curves training set sizes shown 
actual training set sizes shown table 
surprising case segment dataset mc error increased training set size grew 
theory behavior happen eric bauer ron kohavi induction algorithm wolpert schaffer time seen real dataset 
investigation revealed problem classes equiprobable dataset stratified 
strong majority training set implies non majority test set resulting poor performance 
stratified holdout appropriate cases mimicking original sampling methodology kohavi 
experiments relative performance mattered specifically stratify holdout samples 

voting algorithms combine relatively sub classifiers 
similar spirit learning curves possible voting algorithms reach asymptotic error rate reach fewer sub classifiers 
allowed vote classifiers done schapire 
slower variants need sub classifiers vote may just 
quinlan replicates breiman replicates freund schapire 
numbers graphs boosting error rates versus number trials sub classifiers chose vote sub classifiers 
decision limiting number sub classifiers important practical applications voting methods 
competitive important algorithms run reasonable time 
experience believe order magnitude difference reasonable orders magnitude unreasonable practical applications example relatively large run base inducer mc takes hour today take days voted version runs times slower trials 

sanity check correctness mentioned earlier implementations mc naive bayes inducers support instance weights algorithms 
results closer correspondence theory defining voting classifiers 
ensure implementation correct algorithmic changes cause significant divergence past experiments repeated experiments breiman quinlan implementation voting algorithms 
results showed similar improvements described previously 
example breiman results show cart bagging improving average error datasets relative gain bagging mc improved average error datasets relative gain 
likewise quinlan showed boosting datasets find replication experiment produced gain accuracy experiments boosting mc show gain datasets 
confirmed correctness methods 
empirical comparison boosting bagging variants 
runs measurements runs estimate error rates fell categories bias variance repeated holdout 
bias variance details section preferred method provided estimate error holdout size gave decomposition bias variance 
ran repeated times different seed time cases 
holdout generating error rates different numbers voting trials 
case bias variance decomposition vary time time penalty performing experiment bias variance decomposition varying number trials high 
second measuring mean squared errors 
bias variance decomposition classification extend mean squared errors labels classification tasks associated probabilities 
estimated error rates experimental methods differed cases especially smaller datasets 
difference error rates different induction algorithms tested methods similar 
absolute errors may large variance cases differences errors accurate algorithms trained tested exactly training test sets 
compare algorithms summarize information ways 
give decrease increase average absolute error averaged datasets assuming represent reasonable real world distribution datasets 
second give average relative error reduction 
algorithms errors ffl ffl decrease relative error ffl gamma ffl ffl example algorithm error algorithm error absolute error reduction relative error reduction 
average relative error average datasets relative error pair algorithms compared 
relative error breiman quinlan names ratio average ratio respectively 
note average relative error reduction different relative reduction average error computation involves averaging errors computing ratio 
relative reduction average error computed error averages supply explicitly stated avoid overload numbers 
computations error relative errors averages done high precision program 
presentation purposes show digit decimal point numbers may add exactly bias variance may error 

bagging algorithm variants comparison mc naive bayes bagging 
proceed variants bagging include pruning versus pruning eric bauer ron kohavi german segment hypothyroid sick euthyroid dna nominal chess led waveform satimage mushroom nursery letter adult shuttle mc bagged mc bagged mc pruning prob 
estimates bagged mc pruning prob 
estimates backfitting bias variance 
bias variance decomposition mc versions bagging 
cases reduction error due reduction variance waveform letter satimage shuttle examples bias reduction pruning disabled mushroom letter 
empirical comparison boosting bagging variants algorithms error mc bagged mc pruning bagging backfitting bias variance algorithms mc disc bagged mc disc bagged mc disc bias variance algorithms algorithms error naive bayes bagged naive bayes bagged naive bayes bias variance 
average bias variance datasets variants bagging induction algorithms 
mc mc disc improve significantly due variance reduction naive bayes stable inducer improves little expected 
classifications versus probabilistic estimates scoring 
shows decomposition datasets mc versions bagging explained 
shows average bias variance datasets mc naive bayes mc disc 

bagging error bias variance decomposition error bias variance applying bagging mc caused average absolute error decrease average relative error reduction 
important observations 
bagging uniformly better datasets 
case increase error 

waveform satimage letter error rate decreased dramatically relative reductions 
datasets relative error reduction 

average tree size number nodes trees generated bagging mc slightly larger trees generated mc nodes versus nodes 
average tree size averaged replicates dataset larger times averages larger 
comparison trees generated mc larger bagging larger 
adult dataset average tree size bagged mc nodes compared nodes mc 
fact bagged classifier contains trees 
hypothesize larger tree sizes due fact original instances replicated bagging samples implying stronger pattern reducing amount pruning 
hypothesis related hypotheses tested section 
eric bauer ron kohavi 
bias variance decomposition shows error reduction completely due variance reduction average variance decreased matching relative reduction 
average bias reduced average relative error reduction 
bagging algorithm mc reduced error small average reduction relative error 
bias decreased variance decreased 
attribute reduction bias slightly stronger classifier formed bagging 
low reduction variance expected shallow trees single root split relatively stable 
bagging algorithm mc disc reduced error average relative error reduction 
bias increased due waveform bias increased absolute error variance decreased 
hypothesize bias increase due inferior discretization bagging discretization access full training set sample containing unique instances 
bagging algorithm naive bayes reduced average absolute error average relative error reduction 

pruning effects described previous section warrant investigation larger average size trees generated bagging slight reduction bias bagging 
section deals unbounded depth decision trees built mc mc mc disc 
hypothesized larger trees generated mc bootstrap replicates pruning algorithm pruned larger trees initially grown 
verify hypothesis disabled pruning algorithm reran experiments 
mc default grows trees nodes pure split children contain instances 
unpruned trees mc average size unpruned trees bagged mc trees average size smaller 
averaged size trees generated mc bootstrap samples dataset smaller corresponding size trees generated mc 
postulate effect due smaller effective size training sets bagging contain unique instances original training set 
oates jensen shown close correlation training set size tree complexity reduced error pruning algorithm mc 
trees generated bootstrap samples initially grown smaller corresponding mc trees larger pruning invoked 
experiment confirms hypothesis structure bootstrap replicates inhibits reduced error pruning 
believe reason inhibition empirical comparison boosting bagging variants instances duplicated bootstrap sample reinforcing patterns pruned noise 
non pruned trees generated bagging significantly smaller bias pruned counterparts versus average relative error reduction 
observation strengthens hypothesis reduced bias bagging due larger trees expected pruning increased bias decreased variance 
non bagged unpruned trees similar bias 
bias reduced non pruned trees bagging mc mc variance trees generated mc grew dramatically increasing error 
variance bagging grows error remains 
average relative error decreased due decrease absolute error mushroom decrease note impressive decrease corresponds small change absolute error 

probabilistic estimates standard bagging uses predicted classes combined classifier predicts class frequently predicted sub classifiers built bootstrap samples 
mc naive bayes probabilistic predictions hypothesized information reduce error 
algorithm combining probabilistic predictions straightforward 
returns probability distribution classes 
probabilistic bagging algorithm bagging uniformly averages probability class sub classifiers predicts class highest probability 
case decision trees hypothesized unpruned trees give accurate probability distributions conjunction voting methods reason node class distribution children standard pruning algorithms prune children children predict class 
probability estimates needed children may accurate child having identifying perfect cluster training set 
single trees variance penalty incurred estimates nodes small number instances may large pruning help pazzani merz murphy ali hume brunk voting methods reduce variance voting multiple classifiers bias introduced pruning may limiting factor 
test hypothesis probabilistic estimates help reran experiments bagging naive bayes mc pruning disabled 
average error mc decreased average relative error decreased 
decrease due bias variance reduction 
bias reduced average relative decrease 
variance decreased average relative decrease 
eric bauer ron kohavi average error mc decreased significantly average relative error reduction 
reduction bias reduction variance average error mc disc decreased average relative reduction 
bias decreased average relative reduction compared increase bias non probabilistic version 
variance decreased average relative error reduction 
average error naive bayes decreased average relative error decreased 
bias decreased zero relative bias reduction variance decreased relative variance reduction 
results naive bayes insignificant 
expected error incurred naive bayes due bias term 
probability estimates generated usually extreme conditional independence assumption true cases causing single factor affect attributes probabilities multiplied assuming conditionally independent label friedman 
summarize seen error reductions family decision tree algorithms probabilistic estimates 
error reductions larger level decision trees 
reduction due decrease bias variance 
naive bayes unaffected probabilistic estimates 

mean squared errors practical applications important classify correctly give probability distribution classes 
common measure error task mean squared error mse squared difference probability class probability predicted 
test set assigns label probability measure mse gamma minus probability assigned correct label 
average mse mean squared error averaged entire test set 
classifier assigns probability correct label penalty zero penalty positive grows square distance 
classifier single prediction viewed assigning probability predicted class zero classes 
conditions average mse classification error 
note results slightly different error averages times holdout runs compared times holdout bias variance runs compute mse 
estimate bagging really better estimating probabilities ran inducer times holdout sample effective size bias variance experiments 
mc average mse decreased bagging pruning classification error bagging probability estimates average relative reduction 
mc run probability estimate mode frequency counts empirical comparison boosting bagging variants average mse apply estimate laplace correction leaves described kohavi becker sommerfield average mse decreased bagging significantly outperformed method reducing average relative mse 
naive bayes average mse went average relative reduction 
naive bayes run probability estimate mode mse average relative mse difference 
mc disc average mse went decrease average relative mse 
running mc disc probability estimate mode gave average mse benefit came bagging probability estimates 
children root usually tend give fairly accurate probability estimates albeit single attribute 
results see applying estimate laplace corrections classification mc significantly reduces average mse reduces significantly 
naive bayes main improvement results switching probabilities classification small benefit bagging 

backfitting data interesting variant bagging tried called weight aggregation 
method seeks repeatedly perturb training set bagging sampling adds gaussian noise weight mean zero standard deviation 
trial start uniformly weighted instances add noise weights induce classifier 
method nice property trade bias variance increasing standard deviation noise introduce instances weight decrease zero disappear increasing bias reducing variance 
experiments showed standard deviation method finishes head head best variant bagging error bagged mc pruning scoring errors 
differences significant 
results naive bayes similar 
successful variant tried mc uses method called backfitting described 
bagging creates classifiers unique instances training set 
improve probability estimates leaves decision tree algorithm second pass sub classifier construction feeding original training set decision tree changing structure 
estimates leaves expected accurate data 
bias variance run shows error bagging mc pruning scoring reduces error average relative error decreased 
bias variance decomposition shows bias backfitting backfitting variance eric bauer ron kohavi error number trials satimage test error bagged mc train error bagged mc test error bagged mc pruning prob 
estimates backfitting train error bagged mc pruning prob 
estimates backfitting number trials chess test error bagged mc train error bagged mc test error bagged mc pruning prob 
estimates backfitting train error bagged mc pruning prob 
estimates backfitting number trials dna nominal test error bagged mc train error bagged mc test error bagged mc pruning prob 
estimates backfitting train error bagged mc pruning prob 
estimates backfitting number trials nursery test error bagged mc train error bagged mc test error bagged mc pruning prob 
estimates backfitting train error bagged mc pruning prob 
estimates backfitting 
bagging graphs selected datasets showing different behaviors mc original bagging versus bagging 
graphs show test set training set errors number bootstrap samples replicates increases 
point shows error performance set classifiers created bagging algorithm far averaged trial runs 
satimage represents largest family versions track closely improvement happens trials 
chess represents case bagging initially better flattens bagging matches trials 
nursery examples bagging superior keep advantage 
note low training set errors implying decision trees overfitting 
reduced average relative variance decrease 
fact variance files remained reduced 
shows graphs error changes trials selected datasets represent different behaviors seen initial version bagging final backfitting version mc 
graphs show test set training set error number bootstrap samples increases 
case bagging significantly outperform bagging 

bagging shown series variants bagging improves performance slightly predecessor mc algorithm 
variants disabling pruning average probability estimates scores backfitting 
error decreased original bagging algorithm final empirical comparison boosting bagging variants version compared mc 
average relative decrease error due mushroom reduced relative decrease 
excluding mushroom average relative decrease error impressive algorithm performs initially 
mc original bagging algorithm achieves benefit reducing variance 
final version decreased variance slightly decreased bias 
bagging reduces error mc somewhat surprising initially expect variance level trees large thought bagging effect 
error decrease analyzing results showed change due bias reduction variance reduction 
similar effect true mc disc error reduced 
naive bayes successful datasets domingos pazzani friedman langley sage kohavi usually coupled feature selection included study starts inferior mc experiments error naive bayes versus error mc difference grows 
mc error decreased naive bayes went 
naive bayes extremely stable algorithm bagging variance reduction technique 
specifically average variance naive bayes bagging probability estimates decreased 
average bias bagging reduces 
mean squared errors generated bagging significantly smaller non bagged variants mc mc mc disc 
aware reported mean squared errors results voting algorithms past 
probability estimates crucial applications loss matrices bernardo smith significant differences indicate bagging promising approach 

boosting algorithms adaboost arc discuss boosting algorithms 
explore practical considerations boosting algorithm implementation specifically numerical instabilities underflows 
show detailed example boosting run emphasize underflow problems experienced 
show results experiments adaboost arc describe 

numerical instabilities detailed boosting example detail results experiments step detailed example adaboost run reasons get better understanding process second highlight important issue numerical instabilities underflows rarely discussed common boosting algorithms 
believe authors faced problems eric bauer ron kohavi corrected know exist example shows 
example domingos pazzani reported poor accuracy error sonar dataset naive bayes induction algorithm performed 
class problem predicting majority done better 
kohavi becker sommerfield reported accuracy error problem similar algorithm 
investigation discrepancy domingos kohavi revealed domingos naive bayes algorithm normalize probabilities attribute 
attributes multiplication creating zero probabilities 
numerical instabilities problem related underflows 
cases past observed problems entropy computations yield small negative results order gamma gamma bit double precision computations 
problem exacerbated sample small large weight instances occurs boosting 
instances small weights total weight training set vary depending summation order shuffling summing may result slightly different sum 
standpoint numerical analysis sums done smallest largest numbers reduce instability imposes severe burdens terms programming running time standard computations 
mlc defined natural weight instance sample unweighted instances total weight sample equal number instances 
normalization operations required boosting modified accordingly 
mitigate numerical instability problems instances weights gamma automatically removed 
initial implementation boosting cases instances removed due underflows described 
explore example boosting run show underflow problem help reader develop feel boosting process functions 
example training set size shuttle dataset 
mc algorithm relatively small test set error measured remaining instances 
description progress trials shown 
training set error uniformly weighted top left boosting trial misclassified instances 
update rule equation page shows instances re weighted weight weight update factor delta correctly classified instances weight halved gamma weight 
regular mc test set error classifier 
empirical comparison boosting bagging variants 
shuttle dataset projected discriminatory axes 
color shading denotes class instances cube sizes correspond instance weights 
picture shows adaboost trial progression top left top right middle left eric bauer ron kohavi 
second trial classifier trained weighted sample mistake single instance previously misclassified shown top right red close previous large red instance 
training set error single instance weigh exactly half total weight sample 
weight correctly classified instances approximately halved changing weights mistaken instances trial weights rest instances 
test set error classifier 
third trial middle left classifier mistakes instances correctly classified previous trials 
training set error delta 
weight instance incorrectly classified trial approximately halved incorrectly classified instances occupy half weight exactly 
instances weigh 
test set error classifier 
fourth trial middle right classifier mistakes training set 
training set error test set error 

fifth trial lower left classifier mistake instance weight 
training set error 
original implementation update rule recommended algorithm shown adaboost algorithm 
error small fi delta gamma multiplying weights fi prior normalization caused underflow minimum allowed weight gamma instances removed causing sixth trial zero training set error test set error 
newer implementation rest update rule equation page suffers underflow problems 

sixth trial lower right classifier mistakes test set error compared original decision tree classifier 
process stops 
classifier zero training set error gets infinite voting power 
final test set error adaboost algorithm 
example special training set error single classifier zero boosting trials 
sense interesting result single decision classifier built test set error relatively better original decision tree 
really ensemble single classifier 
update rule avoids normalization step mainly circumvents issue underflow early process underflows happen 
error empirical comparison boosting bagging variants close zero instances correctly classified trials reduced factor experiments trials weights reduced delta gamma minimum threshold 
rest algorithm sets instances weights falling minimum weight minimum weight 
runs significantly larger error example especially boosting trials underflow issue severe 
boosting implementations freund schapire singer maintain log weights modify definition fi small value divided number training examples added numerator denominator personal communication schapire 
issue deserves careful attention boosting experiments trials schapire 
require addressing issue carefully 

adaboost error bias variance shows absolute errors mc adaboosted mc decomposition bias variance 
shows average bias variance datasets bagging boosting methods mc naive bayes mc disc 
average absolute error decreased mc adaboosted mc average relative error reduction 
important observations 
average adaboost better promising bagging algorithm explored bagging versus 

bagging boosting uniformly better datasets error hypothyroid increased relative significant level standard deviation estimate 
error sick euthyroid similar increase relative significant level 
error led increased relative significant standard deviation estimate 
worth noting led attribute noise may contributing factor poor performance adaboost noted class noise quinlan 
repeated experiment noise level varying opposed original noise difference mc adaboosted mc increased noise level increased 
adaboost absolute differences noise noise noise noise 
error adult increased relative significant increase standard deviation estimate eric bauer ron kohavi german segment hypothyroid sick euthyroid dna nominal chess led waveform satimage mushroom nursery letter adult shuttle mc bagged mc pruning prob 
estimates backfitting boosted mc arc resample boosted mc adaboost bias variance 
bias variance decomposition mc bagging arc resample adaboost 
boosting methods arc adaboost able reduce bias bagging cases dna chess nursery letter shuttle 
increase variance hypothyroid sick euthyroid led mushroom adult 
empirical comparison boosting bagging variants algorithms algorithms mc bagged mc boosted mc arc boosted mc adaboost bias variance algorithms algorithms error mc disc bagged mc disc boosted mc disc arc boosted mc disc adaboost bias variance algorithms naive bayes bagged naive bayes boosted naive bayes arc boosted naive bayes adaboost bias variance 
average bias variance datasets bagging boosting variants inductions algorithms 
boosting algorithms outperform bagging differences noticeable naive bayes mc disc 
somewhat surprisingly arc resample superior adaboost naive bayes mc disc 

errors adult dataset census data postulate led noisy 

error segment dna chess waveform satimage mushroom nursery letter shuttle decreased dramatically relative reduction error 
letter relative error decreased mushroom relative error decreased 

average tree size number nodes adaboosted trees larger files waveform satimage shuttle 
hypothyroid grew sick euthyroid grew led grew adult grew 
top fact boosted classifier contains trees 
note close correlation average tree sizes improved performance 
datasets decrease average number nodes decision tree error decreased dramatically datasets increase average number nodes resulting classifiers error increased 

bias variance decomposition shows error reduction due bias variance reduction 
average bias reduced average relative reduction average variance reduced average relative reduction 
contrast initial version bagging reported reduced bias variance 
clear methods behave differently 
mc boosting runs failed get training set error trial especially multiclass problems 
mc disc failed get errors files led letter 
eric bauer ron kohavi cases versions averages purposes comparison 
mc disc average absolute error decreased average relative decrease 
compares favorably bagging reduced error far achieving performance achieved naive bayes mc 
bias decreased improvement 
variance reduced average relative improvement computed variance mc disc chess non zero adaboost version 
naive bayes average absolute error decreased decrease average relative error 
compares favorably bagging reduced error 
bias reduced average relative reduction variance increased 
increased variance caused different discretization thresholds real valued attributes discretized entropy method unstable 
shows progress trials selected datasets represent different behaviors seen adaboost bagging conjunction mc algorithm 
graphs show test set training set errors number trials replicates bagging increases 
mc training set error decreases approximately zero trial cases indicating severe overfitting test set error decrease zero 
show similar trial graphs naive bayes mc disc respectively 
classifiers training set error commonly reach zero 

arc error bias variance original implementation arc instance reweighting 
adaboost reweighting resampling past papers reweighting considered superior 
quinlan wrote better results adaboost compared freund schapire may due reweighting compared resampling 
wrote resampling negates major advantage enjoyed boosting bagging viz 
training instances produce constituent classifier 
initial experiments showed arc reweighting performed significantly worse breiman experiments tried resampling version arc performed better 
versions arc called arc reweight arc resample 
similar experiments adaboost change performance adaboost significantly direction 
indicates fundamental difference adaboost arc performs worse sampling done 
average error mc arc resample exactly adaboost average error rate average error empirical comparison boosting bagging variants error number trials nursery test error bagged mc pruning prob 
estimates backfitting train error bagged mc pruning prob 
estimates backfitting test error adaboosted mc train error adaboosted mc number trials led test error bagged mc pruning prob 
estimates backfitting train error bagged mc pruning prob 
estimates backfitting test error adaboosted mc train error adaboosted mc number trials waveform test error bagged mc pruning prob 
estimates backfitting train error bagged mc pruning prob 
estimates backfitting test error adaboosted mc train error adaboosted mc number trials adult test error bagged mc pruning prob 
estimates backfitting train error bagged mc pruning prob 
estimates backfitting test error adaboosted mc train error adaboosted mc 
trial graphs selected datasets showing different behaviors mc adaboost bagging 
graphs show test set training set errors number trials replicates bagging increases 
point average trial runs 
nursery represents cases adaboost outperforms bagging 
led represents common cases bagging outperforms adaboost 
waveform example algorithms perform equally 
adult example adaboost degrades performance compared regular mc 
note point corresponding trial adaboost performance mc algorithm 
bagging runs usually worse point training sets effectively smaller samples 
graphs error zero close zero trial hypothyroid mushroom shuttle training set error single classifier reach zero causing boosting process abort 
arc reweight significantly worse 
important observations 
arc resample superior arc reweight 

bias variance decomposition shows arc resample better arc reweight higher variance arc reweight 
adaboost bias arc resample bias arc reweight bias 
variances differences algorithms showed adaboost variance arc resample variance reweight variance 

compared bagging see variance higher adaboost versions arc bias lower bias bagging variants 
eric bauer ron kohavi number trials nursery test error boosted naive bayes train error boosted naive bayes error number trials satimage test error boosted naive bayes train error boosted naive bayes error number trials dna nominal test error boosted naive bayes train error boosted naive bayes 
trial graphs selected datasets showing different behaviors naive bayes adaboost 
graphs show test set training set errors number trials increases 
point average runs trials replicates 
nursery represents classical theoretical scenario test set error tracks training set error naive bayes simple classifier case fairly rare 
satimage example training set error test set errors decrease asymptote 
dna represents interesting example training set error went zero theory predicts test set error increased 
note axis ranges vary 

versions arc increased average tree size datasets waveform 

error rates arc resample higher mc hypothyroid sick euthyroid led adult 
exact datasets adaboost worse mc 
adaboost due noise 
mc disc failed get errors files led letter 
cases versions averages sake comparison 
average error arc resample arc reweight 
compares favorably adaboost error 
bias variance decomposition shows difference stems bias 
arc runs bias resampling reweighting respectively adaboost bias 
adaboost highest variance arc resample variance arc reweight variance 
naive bayes average absolute error arc resample average absolute error arc reweight adaboost error naive bayes error 
mc disc bias arc resample lowest followed arc reweight adaboost error 
variance naive bayes clear winner followed adaboost arc reweight arc resample 
mc disc arc resample slightly outperformed adaboost 
summarize arc resample superior arc reweight outperformed adaboost level decision trees naive bayes 
arc algorithms increased variance naive bayes decreased error due strong bias reductions 
empirical comparison boosting bagging variants number trials adult test error boosted mc disc train error boosted mc disc error number trials shuttle test error boosted mc disc train error boosted mc disc error number trials nursery test error boosted mc disc train error boosted mc disc 
trial graphs selected datasets showing different behaviors mc disc adaboost 
graphs show test set training set errors number trials increases 
point average runs trials replicates 
adult represents nice run training set test set error track improve final error rate best seen 
shuttle represents erratic behavior 
nursery represents failure change 
trials training set error happens weights change little fact trial fi 
note axis ranges vary 

boosting adaboost arc algorithms different behavior bagging differ 
important observations 
average adaboost arc resample better bagging datasets 
confirms previous comparisons breiman quinlan 

adaboost arc uniformly better bagging 
cases performance boosting algorithms degraded compared original non voted algorithms 
mc adaboost failures correlate average decision tree growth size relative original trees 
arc variants increased average tree size 

adaboost deal noise mentioned quinlan 

adaboost arc reduced bias variance decision tree methods 
algorithms increased variance naive bayes reduced error 

mc disc breiman ad hoc algorithm works better adaboost 

arc adaboost behave differently sampling applied 
arc resample outperformed arc reweight adaboost result 
fact arc works adaboost reinforces hypothesis breiman main benefit adaboost attributed adaptive reweighting 

boosting theory guarantees training set error go zero 
scenario happens mc naive bayes mc disc see 
cases observed errors close eric bauer ron kohavi trials nursery dataset mc disc described dna satimage 
expected boosting concentrates weight hard classify instances making problem harder 
cases requirement error bounded away unsatisfied 
improvement boosting trials apparently needed orders magnitude 

bagging significant difference combined decision tree classifier making probabilistic non probabilistic predictions calculating mean squared error mse adaboost significantly different 
adaboost optimizing classification error may biased probability estimator 
bagging disabling pruning reduced errors 
boosting increased 
probabilistic estimates final combination slightly worse classifications 
probably due fact reweighting done classification errors 
try reweight instances probabilistic predictions classifiers mentioned freund schapire 

study highlighted problems voting algorithms error estimation bias variance decomposition average tree sizes graphs showing progress trials 
new observations clarify behavior algorithms issues require investigation research 

main problem boosting robustness noise 
attempted drop instances high weight experiments show successful approach 
smaller sample sizes force theories simple tree sizes grow trials progress 
methods boosting algorithms robust dataset noisy 

unclear tree size affected different variants 
adaboost interesting correlation change average tree size error reduction arc resample similar correlation 
research try explain relations 

boosting stops classifiers achieves zero training set error 
classifier gets infinite voting power effectively single classifier 
better methods handling extreme situation 

case shuttle dataset single decision tree built significantly better original mc tree 
decision tree zero error training set voter 
situations true classifiers learned sample skewed distribution performs test set 
empirical comparison boosting bagging variants 
boosting bagging create complex classifiers overfit data 
domingos claims multiple trees simply implement bayesian approach shift learner bias machine learning bias statistical bias away commonly simplicity bias 
bias explicit 

bagging works pruning 
pruning decision trees method reducing variance introducing bias 
bagging reduces variance disabling pruning indirectly reduces bias 
error rate change pruning increased 
specifically cases pruning happen bagging 

wolpert discusses stacking generic method meta learning 
bagging arc uniform weighting adaboost uses complex weighting scheme 
possible stacking inducer help 
attempted stack naive bayes top base classifiers built adaboost bagging success 
better method combine classifiers devised 

boosting applied algorithms nearest neighbors 
surface standard interpretation counting highly weighted instance increasing weight instance helps classify neighbors classify 

probabilistic predictions sub classifiers 
quinlan voting strength ignores fact classifiers built skewed distributions 

voting techniques usually result incomprehensible classifiers easily shown users 
solution proposed kohavi kunz attempts build structured model affect bagging 
madigan richardson convert boosted naive bayes regular naive bayes allows visualizations becker kohavi sommerfield 
ways boosting comprehensible general models 
craven shavlik built single decision tree attempts classifications neural network 
quinlan notes parallel problems require testing attributes 
single tree problems large 

parallel environments bagging strong advantage built parallel 
boosting methods hand require estimated training set error trial generate distribution trial 
coarse grain parallelization hard 
efficient parallel implementations devised 
eric bauer ron kohavi 
provided brief review families voting algorithms perturb combine bagging boosting adaboost arc 
contributions include 
large scale comparison bagging bagging variants adaboost arc families induction algorithms decision trees naive bayes 
previous papers concentrated datasets performance bagging adaboost stellar 
believe gives realistic view performance improvement expect 
specifically best algorithm adaboost average relative error reduction mc mc disc naive bayes 
boosting algorithms generally better bagging uniformly better 
furthermore datasets adult voting algorithms helped knew learning curve graphs bayes optimal error lower 

decomposition error rates bias variance real datasets 
previous provided decomposition artificial datasets 
believe decomposition real world problems informative leads better understanding tradeoffs involved real world scenarios 
bagging error reduction conjunction mc due variance reduction mc unstable high variance 
mc mc disc bagging reduced bias significantly variance 
bagging little effect naive bayes 
boosting methods reduced bias variance 

evaluation variants voting algorithms including pruning probabilistic variants backfitting 
shown outperform original bagging algorithm 

mean squared error evaluation showed voting techniques extremely successful reducing loss metric 
believe voting techniques perform significantly better loss matrices fairly common real world applications 

discussion numerical instabilities require careful thought implementation boosting algorithms 

positive correlation increase average tree size adaboost trials success reducing error 

observation arc reweighting methods adaboost sampling step crucial 
error rates achieved bagging boosting algorithms surprisingly 
looking learning curves voting algorithms built classifiers small samples outperformed looked asymptotic error original algorithms 
example waveform stabilized error training set size instances empirical comparison boosting bagging variants adaboost error rate training set sizes instances 
letter domain lowest error learning curve instances adaboost error rate training set size 
learning tasks comprehensibility crucial voting methods extremely useful expect see significantly today 
acknowledgments authors worked silicon graphics done 
research conducted mlc available freely research purposes 
carla brodley cliff brunk tom dietterich eric eros clay kunz foster provost ross quinlan helpful comments draft 
yoav freund rob schapire clarifying issues adaboost algorithm 
special silicon graphics powerful servers including hegde steve whitney scalable networked servers division lori cray research subsidiary eugene yu manufacturing 
estimate cpu hours mips mhz study 
study easily done cpu origin dozens gigabytes ram 
remember cpu flurry 
notes 
original boosting algorithm weights instances total weight sample 
due numerical instabilities assign initial weight instance normalize number instances described section 
algorithmic difference 
ali 
learning probabilistic relational concept descriptions phd thesis university california irvine 
www ics uci edu ali 
becker kohavi sommerfield 
visualizing simple bayesian classifier kdd workshop issues integration data mining data visualization 
bernardo smith 
bayesian theory john wiley sons 
blake keogh merz 
uci repository machine learning databases 
www ics uci edu mlearn mlrepository html 
breiman 
heuristics instability model selection technical report statistics department university california berkeley 
breiman 
arcing classifiers technical report statistics department university california berkeley 
www stat berkeley edu users breiman 
breiman 
bagging predictors machine learning 
breiman 
arcing edge technical report technical report statistics department university california berkeley 
www stat berkeley edu users breiman 
buntine 
learning classification trees statistics computing 
buntine 
theory learning classification rules phd thesis university technology sydney school computing science 
eric bauer ron kohavi cestnik 
estimating probabilities crucial task machine learning aiello ed proceedings ninth european conference artificial intelligence pp 

chan stolfo wolpert 
integrating multiple learned models improving scaling machine learning algorithms 
aaai workshop 
craven shavlik 
learning symbolic rules artificial neural networks proceedings tenth international conference machine learning morgan kaufmann pp 

dietterich 
approximate statistical tests comparing supervised classification learning algorithms neural computation 
dietterich bakiri 
error correcting output codes general method improving multiclass inductive learning programs proceedings ninth national conference artificial intelligence aaai pp 

domingos 
bagging 
bayesian account implications heckerman mannila pregibon uthurusamy eds proceedings third international conference knowledge discovery data mining aaai press pp 

domingos pazzani 
independence conditions optimality simple bayesian classifier machine learning 
drucker cortes 
boosting decision trees advances neural information processing systems pp 

duda hart 
pattern classification scene analysis wiley 
efron tibshirani 
bootstrap chapman hall 
elkan 
boosting naive bayesian learning technical report department computer science engineering university california san diego 
fayyad irani 
multi interval discretization continuous valued attributes classification learning proceedings th international joint conference artificial intelligence morgan kaufmann publishers pp 

freund 
boosting weak learning algorithm majority proceedings third annual workshop computational learning theory pp 

freund 
boosting weak learning algorithm majority information computation 
freund schapire 
decision theoretic generalization line learning application boosting proceedings second european conference computational learning theory springer verlag pp 

appear journal computer system sciences 
freund schapire 
experiments new boosting algorithm saitta ed machine learning proceedings thirteenth national conference morgan kaufmann pp 

friedman 
bias variance loss curse dimensionality data mining knowledge discovery 
ftp stanford edu pub friedman curse ps geman bienenstock doursat 
neural networks bias variance dilemma neural computation 

estimation probabilities essay modern bayesian methods press 
holte 
simple classification rules perform commonly datasets machine learning 
iba langley 
induction level decision trees proceedings ninth international conference machine learning morgan kaufmann publishers pp 

kohavi 
study cross validation bootstrap accuracy estimation model selection mellish ed proceedings th international joint conference artificial intelligence morgan kaufmann pp 

robotics stanford edu ronnyk 
kohavi 
wrappers performance enhancement oblivious decision graphs phd thesis stanford university computer science department 
stan cs tr robotics stanford edu ronnyk ps empirical comparison boosting bagging variants kohavi becker sommerfield 
improving simple bayes th european conference machine learning poster papers pp 

available robotics stanford edu users ronnyk 
kohavi kunz 
option decision trees majority votes fisher ed machine learning proceedings fourteenth international conference morgan kaufmann publishers pp 

available robotics stanford edu users ronnyk 
kohavi sahami 
error entropy discretization continuous features proceedings second international conference knowledge discovery data mining pp 

kohavi sommerfield 
feature subset selection wrapper model overfitting dynamic search space topology international conference knowledge discovery data mining pp 

kohavi sommerfield dougherty 
data mining mlc machine learning library international journal artificial intelligence tools 
www sgi com technology mlc 
kohavi wolpert 
bias plus variance decomposition zero loss functions saitta ed machine learning proceedings thirteenth international conference morgan kaufmann pp 

available robotics stanford edu users ronnyk 
kong dietterich 
error correcting output coding corrects bias variance prieditis russell eds machine learning proceedings twelfth international conference morgan kaufmann pp 

kwok carter 
multiple decision trees levitt kanal lemmer eds uncertainty artificial intelligence elsevier science publishers pp 

langley iba thompson 
analysis bayesian classifiers proceedings tenth national conference artificial intelligence aaai press mit press pp 

langley sage 
scaling domains irrelevant features greiner ed computational learning theory natural learning systems vol 
mit press 
oates jensen 
effects training set size decision tree complexity fisher ed machine learning proceedings fourteenth international conference morgan kaufmann pp 

oliver hand 
pruning averaging decision trees prieditis russell eds machine learning proceedings twelfth international conference morgan kaufmann pp 

pazzani merz murphy ali hume brunk 
reducing misclassification costs machine learning proceedings eleventh international conference morgan kaufmann 
quinlan 
programs machine learning morgan kaufmann san mateo california 
quinlan 
comparing connectionist symbolic learning methods hanson rivest eds computational learning theory natural learning systems vol 
constraints prospects mit press chapter pp 

quinlan 
bagging boosting proceedings thirteenth national conference artificial intelligence aaai press mit press pp 

madigan richardson 
interpretable boosted naive bayes classification proceedings fourth international conference knowledge discovery data mining 
schaffer 
conservation law generalization performance machine learning proceedings eleventh international conference morgan kaufmann pp 

schapire 
strength weak learnability machine learning 
schapire freund bartlett lee 
boosting margin new explanation effectiveness voting methods fisher ed machine learning proceedings fourteenth international conference morgan kaufmann pp 

eric bauer ron kohavi wolpert 
stacked generalization neural networks 
wolpert 
relationship pac statistical physics framework bayesian framework vc framework wolpert ed mathematics generalization addison wesley 
