online bagging boosting stuart russell computer science division university california berkeley ca cs berkeley edu bagging boosting known ensemble learning methods 
combine multiple learned base models aim improving generalization performance 
date primarily batch mode ective online versions proposed 
simple online bagging boosting algorithms claim perform batch counterparts 
traditional supervised learning algorithms classify examples single model decision tree neural network 
ensemble learning algorithms varieties combine predictions multiple base models learned traditional algorithm 
bagging boosting known ensemble learning algorithms shown ective improving generalization performance compared individual base models :10.1.1.133.1040
theoretical analysis boosting performance supports results 
develop online versions algorithms 
online learning algorithms process training instance arrival need storage reprocessing maintain current hypothesis re ects training instances seen far 
algorithms advantages typical batch algorithms situations data arrive continuously 
useful large data sets secondary storage multiple passes required batch algorithms prohibitively expensive 
deal classi cation problem 
batch ensemble algorithms typically batch learning algorithm shall call generate base model 
rst requirement online ensemble algorithm online learning algorithm base models shall call online variants learning algorithms available 
lossless online algorithm output hypothesis training set identical corresponding batch algorithm 
lossless online algorithms available decision trees naive bayes models nearest neighbor classi ers 
lossless online algorithms decision trees naive bayes models experiments 
producing online versions bagging boosting requires way mirror speci techniques generating multiple distinct base models 
di culty algorithms appear require size training set unavailable meaningless online context 
example bagging works resampling original training set size produce bootstrap training sets size train base model 
online version trains base models online 
simulates bootstrap process sending copies new example update base model suitable poisson random variable 
simple trick yields learning behavior similar batch bagging 
describe online bagging algorithm give theoretical results section empirical results provided section 
boosting somewhat complex process generates series base models hm base model hm learned weighted training set weights determined classi cation errors preceding model hm speci cally examples misclassi ed hm weight training set hm weights misclassi ed examples constitute half total weight training set 
bagging type normalization appears require complete training set 
poisson sampling process approximate reweighting algorithm 
online boosting algorithm described detail section 
empirical results section 
topic online bagging boosting received little attention literature 
ensemble neural networks trained boosting online fashion method proposed discards substantial amounts data process drawing desired distribution data base models 
blocked online boosting algorithm proposed trains base models consecutive subsets training examples xed size process discards fraction data received 
algorithms directly comparable approach focuses reproducing advantages bagging boosting online setting 
online bagging algorithm proposed attempts simulate bootstrap process sending new training example update base model probability user xes advance 
experiments various probabilities online bagging algorithm performed better single decision tree 
proposes online boosting algorithm online version arc example weight update base model number previous base models currently misclassify example 
algorithm applied branch prediction problem computer architecture 
results suggest limited memory boosted ensemble greater number smaller decision trees generally superior fewer large trees 
potentially interesting parallels drawn approach winnow weighted majority algorithms 
algorithms xed set base models trained online combined weights depend training set performance base model 
performance shown best component model training sequence 
hand ensemble algorithms generally perform better component models 
comparing online bagging boosting see send identical training sequences base model base model diversity known aid ensemble performance built priori emerging data 
imagine hybrid approaches may case amortized analysis techniques applied algorithms 
online bagging training dataset size standard batch bagging creates base models trained bootstrap sample size created drawing random samples replacement original training set 
pseudocode original training set examples number base models learned bagging mg tm sample ith replacement hm tm return fh hm base model training set contains original training examples times binomial distribution 
distribution tends poisson distribution exp 
suggests perform bagging online follows training example algorithm base model choose example times update base model accordingly 
pseudocode set base models learned far latest training example arrive 
base model hm mg ensemble set 
times hm hm new instances classi ed way online batch bagging unweighted voting base models 
online bagging approximation batch bagging extent base model learning algorithms produce similar hypotheses trained similar distributions training examples 
rst prove original training set supplied bagging algorithms distributions number base models normally chosen trial error validation set 
training sets supplied base models batch online bagging converge size original training set grows nity 
de ne vector length ith element represents number times ith original training example included bootstrap training set mth base model batch bagging 
sampling replacement batch bagging algorithm done performing trials trial yields training examples equal probability drawn 
multinomial training examples equal success probability de ne online bagging version mentioned earlier online bagging training example chosen number times distribution 
training examples trials total number examples drawn distribution 
example equal probability drawn recast sampling online bagging algorithm performing trials trial yields training examples equal probability drawn 
multinomial 
theorem converges distribution 
proof probability generating function batch bagging algorithm sampling distribution multinomial mult xn xn generating function multinomial distribution mult xn xn generating function distribution oi exp 
online bagging sampling algorithm involves performing multinomial trials generating function online bagging sampling distribution oi mult xn exp xn furthermore standard result lim mult xn lim xn exp xn convergence generating functions implies convergence probabilities possible vector sampling methods converge distribution 
de ne resample function takes input original training set vector length ith element number times ith training example included bootstrap training set 
function returns actual bootstrap training set induced assume examples drawn randomly independently xed distribution 
sampling distributions batch online bagging induce distributions base hypotheses resample resample respectively 
batch bagged ensemble consists independent identically distributed draws resample 
ensemble consists draws resample 
show resample resample 
clearly true learning algorithms suppose return null hypothesis training set exactly examples examples probability receives examples tends 
intuitively need learning algorithm behaved sense having examples bootstrapped training set signi cant di erence learning algorithm output 
local learning algorithms nearest neighbor clearly behaved sense 
nearest neighbor base model returns classi cation new test example nearest neighbors bootstrap training set 
shown easily distribution nearest neighbors batch bagging converges online bagging 
simple contingency table learning behaved 
class cjx denominator just consider purpose classi cation 
de ne fraction examples form having attribute values class batch bagging draws bootstrap training sets multinomial means performs trials probability choosing example online bagging draws bootstrap training sets multinomial involves performing trials probability choosing example adaboost xn yn initialize ng 

call distribution dm 
get back hypothesis hm 
calculate error hm hm xn dm 
set abort loop 

set 
update distribution dm dm dm zm hm xn yn zm normalization constant chosen dm probability distribution 
output nal hypothesis fin hm log adaboost algorithm fore mult examples expected counts entry contingency tables online batch bagging classi cations new examples expectation online batch bagging :10.1.1.133.1040
working describing larger set learning algorithms behaved 
online boosting online boosting algorithm designed correspond batch boosting algorithm adaboost 
:10.1.1.133.1040
give pseudocode adaboost inputs set training examples yn base learning algorithm number base models generated 
explained earlier adaboost generates sequence base models hm weighted training sets training examples misclassi ed model hm half total weight model hm correctly classi ed examples remaining half weight 
online boosting algorithm pseudocode hm set base models learned hm set example weight 
base model hm mg ensemble 
set 

times hm hm 
hm correct label sc sc 
sc sw sw 
sw classify new examples mg calculate sw sc sw return hm log online boosting algorithm far latest training example arrive incremental learning algorithm takes current hypothesis training example input returns updated hypothesis 
online boosting algorithm similar online bagging algorithm base model misclassi es training example poisson distribution parameter associated example increased base model decreased 
example upper left corner point diagram rst training example 
example updates rst base model misclassi ed training weight increased rectangle represent taller 
example higher weight updates second base model correctly classi es weight decreases rectangle 
just adaboost algorithm gives examples misclassi ed stage half total weight stage correctly classi ed examples remaining half weight 
see examining adjustments shown item follows 
suppose sc sum values examples classi ed correctly base model stage sw sum discuss caveat point section 
weighted combination training examples 
illustration online boosting progress 
row represents example passed sequence base models updating time runs diagram 
base model depicted tree generated updating base model weighted training example 
rectangle represents training example height rectangle represents weight 
incorrectly classi ed examples 
stage boosting want sums scaled value just adaboost want nd factors scale sc sw half total weight respectively 
sum adaboost weights sum online algorithm number examples seen far 
get sc sc sw sw note expect sc sw means adaboost terminology examples weights algorithm works values treat weights 
weights correctly classi ed examples decrease weights incorrectly classi ed examples increase desired 
area concern adaboost example weight adjusted performance base model entire training set online boosting weight adjustment base model performance examples seen earlier 
see may issue consider running adaboost online boosting training set size 
adaboost rst base model generated examples tested say tenth training example 
online boosting generated rst examples tested tenth example 
clearly may expect di erent adaboost online boosting may di erent weights tenth example 
may turn lead di erent weights fraction correct number examples decision tree bagging online bagging adaboost online boosting learning curves car evaluation dataset tenth example algorithm 
intuitively want online boosting get mix training examples normalized error base model online boosting quickly converges adaboost 
rapidly convergence occurs similar weight adjustments similar performances 
experimental results section discuss experiments demonstrate online algorithms perform batch counterparts number training examples increases 
implemented online bagging online boosting decision trees naive bayes classi ers base models 
decision trees reimplemented lossless iti online algorithm batch online naive bayes algorithms essentially identical 
illustrate convergence batch online learning experimented car evaluation dataset uci machine learning repository 
dataset examples retained test set remaining examples training sets 
ran algorithm decision trees times number training examples account randomness ensemble algorithms 
results shown 
gure shows batch online bagging decision trees performing identically significantly better single decision tree 
adaboost performs signi cantly better single decision tree numbers examples 
online boosting rst performs comparably adaboost signi cantly better single decision trees maximum number examples 
note online boosting performance steadily closer adaboost number examples grows expects online algorithm compared batch version 
tested algorithms uci datasets varying sizes numbers attributes see table 
accuracies algorithms table table increasing order dataset size 
boldface entries represent cases ensemble algorithm signi cantly test outperformed single model italicized entries represent cases ensemble algorithm signi cantly relative single model 
batch algorithm accuracies averages runs cross validation 
tested online algorithms random orders training set generated batch algorithms 
order matters online boosting lossless learning algorithm 
tested bagging boosting decision trees smaller datasets iti algorithm proved expensive larger ones 
small promoters dataset adaboost algorithm ran seconds online boosting needed hours 
compares second online boosting naive bayes 
decision trees online boosting performed signi cantly worse adaboost promoters dataset signi cantly better balance comparably remaining datasets 
bagging online bagging performed noticeably better single decision trees breast cancer dataset 
naive bayes bagging online bagging performed noticeably better naive bayes expected stability naive bayes 
boosting online boosting performed comparably relatively small promoters dataset performances relative single naive bayes classi er consistently improved sizes datasets grew 
balance soybean datasets boosting algorithms performed signi cantly worse naive bayes 
breast cancer dataset adaboost performed signi cantly worse online boosting performed marginally worse 
car evaluation chess datasets adaboost online boosting performed signi cantly better naive bayes 
nursery dataset adaboost performed signi cantly better online boosting performed marginally better 
described online versions popular bagging boosting algorithms shown table sizes uci datasets experiments 
data set training test inputs classes set set promoters balance soybean large wi 
breast cancer german credit car evaluation chess mushroom nursery table results fraction correct batch online algorithms decision trees uci datasets dataset decision tree bagging online bagging adaboost online boosting promoters balance wi breast cancer car evaluation experiment online versions typically perform comparably batch counterparts 
algorithms low overhead quite suitable practical applications 
current empirical focuses testing large continuously arriving data streams 
shown batch online bagging identical large datasets provided base learning algorithm behaved certain sense 
theoretical tasks include characterizing tightly class learning algorithms convergence online ine bagging proved developing analytical framework online boosting 
investigating case lossy online learning ect ensemble performance 
leo breiman bin yu michael jordan joe hellerstein tumer useful discussions 
part done rst author nasa ames research center 
eric bauer ron kohavi 
empirical comparison voting classi cation algorithms bagging boosting variants 
machine learning sep 
blake keogh merz 
uci repository machine learning databases 
url www ics uci edu mlearn mlrepository html 
breiman 
bias variance arcing classi ers 
technical report department statistics university california berkeley 
breiman 
pasting small votes classi cation large databases line 
machine learning 
drucker schapire simard 
improving performance neural networks boosting algorithm 
hanson cowan giles editors advances neural information processing systems pages 
morgan kaufmann 
harris drucker 
boosting neural networks 
sharkey editor combining arti cial neural nets ensemble modular multi net systems pages 
springer verlag london 
alan fern robert givan 
online ensemble learning empirical study 
proceedings seventeenth international conference machine learning pages 
morgan kaufmann 
freund schapire :10.1.1.133.1040
experiments new boosting algorithm 
proceedings thirteenth international conference machine learning pages bari italy 
morgan kaufmann 
yoav freund robert schapire 
decisiontheoretic generalization line learning application boosting 
journal computer system sciences 
grimmett 
probability random processes 
oxford science publications new york 
table results fraction correct batch online algorithms naive bayes uci datasets dataset naive bayes bagging online bagging adaboost online boosting promoters balance soybean large wi breast cancer german credit car evaluation chess mushroom nursery littlestone 
learning quickly irrelevant attributes abound new linear threshold algorithm 
machine learning 
littlestone warmuth 
weighted majority algorithm 
information computation 
tumer 
linear order statistics combiners reliable pattern classi cation 
phd thesis university texas austin tx may 
berkman clouse 
decision tree induction ecient tree restructuring 
machine learning 
